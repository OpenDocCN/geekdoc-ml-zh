["```py\nignore_warnings = True                                        # ignore warnings?\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator, AutoLocator) # control of axes ticks\nplt.rc('axes', axisbelow=True)                                # set axes and grids in the background for all plots\nfrom scipy.stats import rankdata                              # to assist with plot label placement\nfrom sklearn.linear_model import LinearRegression             # fit the relationship between latent and training data slope \nseed = 13                                                     # random number seed\ncmap = plt.cm.tab20                                           # default colormap\nplt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\nif ignore_warnings == True:                                   \n    import warnings\n    warnings.filterwarnings('ignore') \n```", "```py\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef xavier(n_in, n_out):                                      # Xavier initializer function\n    limit = np.sqrt(6 / (n_in + n_out))\n    return np.random.uniform(-limit, limit)\n\ndef sigmoid(x):                                               # sigmoid activation\n    return 1 / (1 + np.exp(-x))\n\ndef initialize_parameters():                                  # initialize all weights and biases and build dictionaries of both\n    weights = {                            \n        'w14': xavier(3, 2),\n        'w24': xavier(3, 2),\n        'w34': xavier(3, 2),\n        'w15': xavier(3, 2),\n        'w25': xavier(3, 2),\n        'w35': xavier(3, 2),\n        'w46': xavier(2, 1),\n        'w56': xavier(2, 1),\n        'w67': xavier(1, 2),\n        'w68': xavier(1, 2),\n        'w79': xavier(2, 3),\n        'w89': xavier(2, 3),\n        'w710': xavier(2, 3),\n        'w810': xavier(2, 3),\n        'w711': xavier(2, 3),\n        'w811': xavier(2, 3),\n    }\n    biases = {                                                # biases (one per neuron, excluding input)\n        'b4': 0.0,\n        'b5': 0.0,\n        'b6': 0.0,\n        'b7': 0.0,\n        'b8': 0.0,\n        'b9': 0.0,\n        'b10': 0.0,\n        'b11': 0.0\n    }\n    return weights, biases \n\ndef forward_pass(input_vec, weights, biases):                 # forward pass of the autoencoder\n    I1, I2, I3 = input_vec.flatten()                               # input nodes (I1, I2, I3)\n    z4 = weights['w14'] * I1 + weights['w24'] * I2 + weights['w34'] * I3 + biases['b4'] # encoder\n    a4 = sigmoid(z4)\n\n    z5 = weights['w15'] * I1 + weights['w25'] * I2 + weights['w35'] * I3 + biases['b5']\n    a5 = sigmoid(z5)\n\n    z6 = weights['w46'] * a4 + weights['w56'] * a5 + biases['b6'] # bottlekneck\n    a6 = sigmoid(z6)\n\n    z7 = weights['w67'] * a6 + biases['b7']                   # decoder\n    a7 = sigmoid(z7)\n\n    z8 = weights['w68'] * a6 + biases['b8']\n    a8 = sigmoid(z8)\n\n    z9 = weights['w79'] * a7 + weights['w89'] * a8 + biases['b9']\n    a9 = z9  \n\n    z10 = weights['w710'] * a7 + weights['w810'] * a8 + biases['b10']\n    a10 = z10  # linear\n\n    z11 = weights['w711'] * a7 + weights['w811'] * a8 + biases['b11']\n    a11 = z11  # linear\n\n    return {                                                  # return all activations as a dictionary\n        'I1': I1, 'I2': I2, 'I3': I3,\n        'L4': a4, 'L5': a5,\n        'M6': a6,\n        'R7': a7, 'R8': a8,\n        'O9': a9, 'O10': a10, 'O11': a11\n    }\n\ndef mse_loss_and_derivative(output_vec, input_vec):           # MSE loss and error derivative given output and input\n    diff = output_vec - input_vec\n    loss = np.mean(diff**2)\n    dloss_dout = (2/3) * diff  # shape (3,1)\n    return loss, dloss_dout\n\ndef sigmoid_derivative(x):                                    # derivative of sigmoid activation\n    return x * (1 - x)\n\ndef backpropagate(activations, weights, biases, dloss_dout):  # backpropagate the error derivatives\n    I1, I2, I3 = activations['I1'], activations['I2'], activations['I3']\n    a4, a5 = activations['L4'], activations['L5']\n    a6 = activations['M6']\n    a7, a8 = activations['R7'], activations['R8']\n    O9, O10, O11 = activations['O9'], activations['O10'], activations['O11']\n\n    delta9 = dloss_dout[0, 0]                                 # error terms (delta) for output nodes = dLoss/dOutput\n    delta10 = dloss_dout[1, 0]\n    delta11 = dloss_dout[2, 0]\n\n    grad_weights = {}                                         # gradients for weights from R7, R8 to O9, O10, O11\n    grad_biases = {}\n\n    grad_weights['w79'] = delta9 * a7\n    grad_weights['w89'] = delta9 * a8\n    grad_weights['w710'] = delta10 * a7\n    grad_weights['w810'] = delta10 * a8\n    grad_weights['w711'] = delta11 * a7\n    grad_weights['w811'] = delta11 * a8\n\n    grad_biases['b9'] = delta9\n    grad_biases['b10'] = delta10\n    grad_biases['b11'] = delta11\n\n    delta_r7 = (delta9 * weights['w79'] + delta10 * weights['w710'] + delta11 * weights['w711']) * sigmoid_derivative(a7) # gradients for R7 and R8\n    delta_r8 = (delta9 * weights['w89'] + delta10 * weights['w810'] + delta11 * weights['w811']) * sigmoid_derivative(a8)\n\n    grad_weights['w67'] = delta_r7 * a6                       # gradients for weights from M6 to R7, R8\n    grad_weights['w68'] = delta_r8 * a6\n\n    grad_biases['b7'] = delta_r7\n    grad_biases['b8'] = delta_r8\n\n    delta_m6 = (delta_r7 * weights['w67'] + delta_r8 * weights['w68']) * sigmoid_derivative(a6) # backpropagate delta to M6 (sigmoid)\n\n    grad_weights['w46'] = delta_m6 * a4                       # gradients for weights from L4, L5 to M6\n    grad_weights['w56'] = delta_m6 * a5\n\n    grad_biases['b6'] = delta_m6\n\n    delta_l4 = delta_m6 * weights['w46'] * sigmoid_derivative(a4) # backpropagate delta to L4, L5 (sigmoid)\n    delta_l5 = delta_m6 * weights['w56'] * sigmoid_derivative(a5)\n\n    grad_weights['w14'] = delta_l4 * I1                       # gradients for weights from I1, I2, I3 to L4\n    grad_weights['w24'] = delta_l4 * I2\n    grad_weights['w34'] = delta_l4 * I3\n\n    grad_biases['b4'] = delta_l4\n\n    grad_weights['w15'] = delta_l5 * I1                       # gradients for weights from I1, I2, I3 to L5\n    grad_weights['w25'] = delta_l5 * I2\n    grad_weights['w35'] = delta_l5 * I3\n\n    grad_biases['b5'] = delta_l5\n    return grad_weights, grad_biases\n\ndef update_parameters(weights, biases, grad_weights, grad_biases, learning_rate): # update the weights and biased by derivatives and learning rate\n    for key in grad_weights:                                  # update weights\n        weights[key] -= learning_rate * grad_weights[key]\n    for key in grad_biases:                                   # update biases\n        biases[key] -= learning_rate * grad_biases[key]\n    return weights, biases \n```", "```py\npositions = {                                                 # node positions\n    'I1': (0, 2), 'I2': (0, 1), 'I3': (0, 0),\n    'L4': (1, 1.5), 'L5': (1, 0.5),\n    'M6': (2, 1),\n    'R7': (3, 1.5), 'R8': (3, 0.5),\n    'O9': (4, 2), 'O10': (4, 1), 'O11': (4, 0),\n}\n\nnode_colors = {                                               # node colors\n    'I1': 'white', 'I2': 'white', 'I3': 'white',\n    'L4': 'white', 'L5': 'white',\n    'M6': 'white',\n    'R7': 'white', 'R8': 'white',\n    'O9': 'white', 'O10': 'white', 'O11': 'white',\n}\n\nedges = [                                                     # edges and weight labels\n    ('I1', 'L4', 'lightcoral'), ('I2', 'L4', 'red'), ('I3', 'L4', 'darkred'),\n    ('I1', 'L5', 'dodgerblue'), ('I2', 'L5', 'blue'), ('I3', 'L5', 'darkblue'),\n    ('L4', 'M6', 'orange'), ('L5', 'M6', 'darkorange'),\n    ('M6', 'R7', 'orange'), ('M6', 'R8', 'darkorange'),\n    ('R7', 'O9', 'lightcoral'), ('R7', 'O10', 'red'), ('R7', 'O11', 'darkred'),\n    ('R8', 'O9', 'dodgerblue'), ('R8', 'O10', 'blue'), ('R8', 'O11', 'darkblue'),\n]\n\nweight_labels = { (src, dst,): f\"$\\\\lambda_{{{src[1]}{dst[1:]}}}$\" for (src, dst, color) in edges }\n\nbias_offsets = {                                              # bias vector offsets\n    'L4': (0.06, 0.12), 'L5': (0.06, 0.12),\n    'M6': (0.0, 0.15),\n    'R7': (-0.06, 0.12), 'R8': (-0.06, 0.12),\n    'O9': (0.0, 0.15), 'O10': (0.0, 0.15), 'O11': (0.0, 0.15),\n}\n\nbias_labels = { node: f\"$b_{{{node[1:]}}}$\" for node in bias_offsets.keys() }\n# Plot\nfig, ax = plt.subplots(figsize=(11, 6))\n\ncustom_weight_offsets = {                                     # custom label offsets for select overlapping weights\n    ('I2', 'L4'): (-0.20, 0.0),\n    ('I2', 'L5'): (-0.2, 0.20),\n    ('R8', 'O9'): (0.15, 0.35),\n    ('R8', 'O10'): (0.15, 0.16),\n}\n\nfor (src, dst, color) in edges:                               # plot edges and weight labels\n    x0, y0 = positions[src]\n    x1, y1 = positions[dst]\n    ax.plot([x0, x1], [y0, y1], color=color, linewidth=1, zorder=1)\n    xm, ym = (x0 + x1) / 2, (y0 + y1) / 2\n    dx, dy = custom_weight_offsets.get((src, dst), (0, 0.08))\n    ax.text(xm + dx, ym + dy, weight_labels[(src, dst)],\n            fontsize=9, ha='center', va='center', color = color, zorder=5)\n\nfor node, (x, y) in positions.items():                        # white back circles\n    ax.scatter(x, y, s=1000, color='white', zorder=2)\n\nfor node, (x, y) in positions.items():                        # node circles and labels\n    ax.scatter(x, y, s=500, color=node_colors[node], edgecolors='black', zorder=3)\n    ax.text(x, y, node, ha='center', va='center', fontsize=9, zorder=4)\n\nfor node, (dx, dy) in bias_offsets.items():                   # bias arrows and tighter label placement\n    nx, ny = positions[node]\n    bx, by = nx + dx, ny + dy\n    ax.annotate(\"\", xy=(nx, ny), xytext=(bx, by),\n                arrowprops=dict(arrowstyle=\"->\", color='black'), zorder=2)\n    ax.text(bx, by, bias_labels[node], ha='right', va='bottom', fontsize=10)\n\n# Final formatting\nax.set_xlim(-0.5, 4.5)\nax.set_ylim(-0.5, 2.7)\nax.axis('off'); plt.tight_layout(); plt.show() \n```", "```py\nnp.random.seed(seed = seed+1)                                 # set random seed\nnbatch = 12; nnodes = 3; sigma = 0.1                          # set number of data (total number of data), number of nodes (must be 3), error st.dev.\nymat = np.zeros(nbatch); x = np.arange(1,nnodes+1,1); Xmat = np.zeros([nbatch,nnodes])\ndata = []\nfor ibatch in range(0,nbatch):                                # loop over synthetic data\n    m = np.random.uniform(low = -2.0, high = 2.0)\n    Xmat[ibatch] = (x-2.0)*m + np.random.normal(loc = 0.0, scale=sigma,size=nnodes)\n    ymat[ibatch] = np.dot(x, Xmat[ibatch]) / np.dot(x, x)\n    data.append(Xmat[ibatch].reshape(3,1))\n\nrank = rankdata(Xmat[:,-1])                                   # rank data to improve (alternate) adjacent labels' locations\nplt.subplot(111)                                              # plot the synthetic data\nfor ibatch in range(0,nbatch):                                \n    plt.scatter(Xmat[ibatch],x,color=cmap(ibatch/(nbatch)),edgecolor='black',lw=1,zorder=10)\n    plt.plot(Xmat[ibatch],x,color=cmap(ibatch/(nbatch)),lw=2,zorder=1)\n    custom_positions = [1,2,3,3.2]\n    custom_labels = ['I1','I2','I3','Y']\n    if rank[ibatch] % 2 == 0:\n        plt.annotate(np.round(ymat[ibatch],2),[Xmat[ibatch][-1],3.18],size=9,color='black',ha='center')\n    else:\n        plt.annotate(np.round(ymat[ibatch],2),[Xmat[ibatch][-1],3.25],size=9,color='black',ha='center') \n    plt.annotate(ibatch+1,[Xmat[ibatch][0],0.9],size=9,color='black',ha='center')\n    plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)\n    plt.ylim([3.4,0.8]); plt.xlim([-1.5,1.5]); plt.ylabel('Input Nodes'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Data and Labels')\nplt.annotate('Data Index: ',[-1.4,0.9])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nepochs = 10000                                                # set hyperparameters\nbatch_size = nbatch\nlearning_rate = 0.1\nseed = 13\nnp.random.seed(seed=seed)\n\noutput_mat = np.zeros((batch_size,epochs,3)); loss_mat = np.zeros((epochs)); M6_mat = np.zeros((batch_size,epochs))\n\nweights, biases = initialize_parameters()                     # initialize weights and biases\n\nfor epoch in range(epochs):\n    sum_grad_w = {k: 0 for k in weights.keys()}               # initialize zero dictionary to average backpropogated gradients\n    sum_grad_b = {k: 0 for k in biases.keys()}\n    epoch_loss = 0\n    for idata,input_vec in enumerate(data):\n        activations = forward_pass(input_vec, weights, biases) # forward pass\n        M6_mat[idata,epoch] = activations['M6']\n        output_vec = np.array([[activations['O9']], [activations['O10']], [activations['O11']]])\n        output_mat[idata,epoch,:] = output_vec.reshape(3)\n        loss, dloss_dout = mse_loss_and_derivative(output_vec, input_vec) # compute loss and derivative\n        epoch_loss += loss\n        grad_w, grad_b = backpropagate(activations, weights, biases, dloss_dout) # backpropagation the derivative\n        for k in grad_w:                                      # accumulate gradients\n            sum_grad_w[k] += grad_w[k]\n        for k in grad_b:\n            sum_grad_b[k] += grad_b[k]\n    avg_grad_w = {k: v / batch_size for k, v in sum_grad_w.items()} # average gradients over batch\n    avg_grad_b = {k: v / batch_size for k, v in sum_grad_b.items()}\n    epoch_loss /= batch_size\n    loss_mat[epoch] = epoch_loss\n    weights, biases = update_parameters(weights, biases, avg_grad_w, avg_grad_b, learning_rate) # update parameters\n    # if epoch % 500 == 0:                                    # print loss every 100 training epochs\n    #     print(f\"Epoch {epoch}, Loss: {epoch_loss:.6f}\")\n\nplt.subplot(111)                                              # plot training error vs. training epoch\nplt.plot(np.arange(0,epoch+1,1),loss_mat,color='red',label=r'MSE'); plt.xlim([1,epoch]); plt.ylim([0,1])\nplt.xlabel('Epochs'); plt.ylabel(r'Mean Square Error (L2 loss)'); plt.title('Autoencoder Average Batch L2 Loss vs. Training Epoch')\nadd_grid(); plt.legend(loc='upper right'); plt.xscale('linear')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlinear_model = LinearRegression().fit(ymat.reshape(-1, 1), M6_mat[:,-1]) # fit linear model to regress latent on training data slope\n\nplt.subplot(111)                                              # plot latent vs. training data slope\nplt.plot(np.linspace(-0.4,0.4,100),linear_model.predict(np.linspace(-0.4,0.4,100).reshape(-1,1)),color='red',zorder=-1)\nfor ibatch,input_vec in enumerate(data):                      # plot and label training data\n    plt.scatter(ymat[ibatch],M6_mat[ibatch,-1],color=cmap(ibatch/(nbatch)),edgecolor='black',marker='o',s=30,zorder=10)\n    plt.annotate(ibatch+1,[ymat[ibatch]-0.01,M6_mat[ibatch,-1]+0.01],size=9,color='black',ha='center',zorder=100) \nplt.ylabel('M6 Output'); plt.xlabel(r'Sample Slope, $m_i$'); plt.title('Latent Node Output vs. Sample Slope')\nplt.ylim([0.1,0.8]); plt.xlim([-0.4,0.4]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nfor idata,input_vec in enumerate(data):                       # plot training data and reconstructions \n    plt.subplot(4,3,idata+1)\n    plt.scatter(Xmat[idata],x,color=cmap(idata/(nbatch+2)),edgecolor='black',lw=1,zorder=10)\n    plt.plot(Xmat[idata],x,lw=1,zorder=1,color=cmap(idata/(nbatch+2)),label='data')\n    custom_positions = [1,2,3,3.2]\n    custom_labels = ['I1','I2','I3','Y']\n    plt.annotate(np.round(ymat[idata],2),[Xmat[idata][-1],3.25],size=9,color='black',ha='center')  \n    plt.scatter(output_mat[idata,-1,:],x,lw=1,color=cmap(idata/(nbatch+2)))\n    plt.plot(output_mat[idata,-1,:],x,lw=1,ls='--',color=cmap(idata/(nbatch+2)),label='reconstruction')\n    plt.legend(loc='upper left')\n    plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)\n    plt.ylim([3.5,0.8]); plt.xlim([-2.5,2.5]); plt.ylabel('index'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Training Data #' + str(idata+1))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=4.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nnp.random.seed(seed = seed+7)\nnbatch_test = 12; nnodes = 3; sigma = 0.1\nymat_test = np.zeros(nbatch); x = np.arange(1,nnodes+1,1); Xmat_test = np.zeros([nbatch,nnodes])\ndata_test = []\nfor ibatch in range(0,nbatch):\n    m = np.random.uniform(low = -2.0, high = 2.0)\n    Xmat_test[ibatch] = (x-2.0)*m + np.random.normal(loc = 0.0, scale=sigma,size=nnodes)\n    ymat_test[ibatch] = np.dot(x, Xmat_test[ibatch]) / np.dot(x, x)\n    data_test.append(Xmat_test[ibatch].reshape(3,1))\n\nrank = rankdata(Xmat_test[:,-1])\nplt.subplot(111)\nfor ibatch in range(0,nbatch_test):\n    plt.scatter(Xmat_test[ibatch],x,color=cmap(ibatch/(nbatch)),edgecolor='black',lw=1,zorder=10)\n    plt.plot(Xmat_test[ibatch],x,color=cmap(ibatch/(nbatch)),lw=2,zorder=1)\n    custom_positions = [1,2,3,3.2]\n    custom_labels = ['I1','I2','I3','Y']\n    if rank[ibatch] % 2 == 0:\n        plt.annotate(np.round(ymat_test[ibatch],2),[Xmat_test[ibatch][-1],3.18],size=9,color='black',ha='center')\n    else:\n        plt.annotate(np.round(ymat_test[ibatch],2),[Xmat_test[ibatch][-1],3.25],size=9,color='black',ha='center') \n    plt.annotate(ibatch+13,[Xmat_test[ibatch][0],0.9],size=9,color='black',ha='center')\n    plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)\n    plt.ylim([3.4,0.8]); plt.xlim([-1.5,1.5]); plt.ylabel('Input Nodes'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Data and Labels')\nplt.annotate('Test Data Index: ',[-1.45,0.9])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\noutput_vec_test = np.zeros((len(data_test),3))\nfor idata_test,input_vec_test in enumerate(data_test):\n    activations = forward_pass(input_vec_test, weights, biases)                                                    # forward pass\n    output_vec_test[idata_test,:] = np.array([[activations['O9']], [activations['O10']], [activations['O11']]]).reshape(-1) \n```", "```py\nfor idata,input_vec_test in enumerate(data_test):\n    plt.subplot(4,3,idata+1)\n    plt.scatter(input_vec_test,x,color=cmap(idata/(nbatch)),edgecolor='black',lw=1,zorder=10)\n    plt.plot(input_vec_test,x,lw=1,zorder=1,color=cmap(idata/(nbatch)),label='data')\n    custom_positions = [1,2,3,3.2]\n    custom_labels = ['I1','I2','I3','Y']\n    # plt.annotate(np.round(ymat[idata],2),[Xmat[idata][-1],3.25],size=8,color='black',ha='center') \n    plt.scatter(output_vec_test[idata,:],x,lw=1,color=cmap(idata/(nbatch)))\n    plt.plot(output_vec_test[idata,:],x,lw=1,ls='--',color=cmap(idata/(nbatch)),label='reconstruction')\n    plt.legend(loc='upper left'); plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)\n    plt.ylim([3.5,0.8]); plt.xlim([-1.5,1.5]); plt.ylabel('index'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Test Image #' + str(idata+13))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=4.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nignore_warnings = True                                        # ignore warnings?\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator, AutoLocator) # control of axes ticks\nplt.rc('axes', axisbelow=True)                                # set axes and grids in the background for all plots\nfrom scipy.stats import rankdata                              # to assist with plot label placement\nfrom sklearn.linear_model import LinearRegression             # fit the relationship between latent and training data slope \nseed = 13                                                     # random number seed\ncmap = plt.cm.tab20                                           # default colormap\nplt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\nif ignore_warnings == True:                                   \n    import warnings\n    warnings.filterwarnings('ignore') \n```", "```py\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef xavier(n_in, n_out):                                      # Xavier initializer function\n    limit = np.sqrt(6 / (n_in + n_out))\n    return np.random.uniform(-limit, limit)\n\ndef sigmoid(x):                                               # sigmoid activation\n    return 1 / (1 + np.exp(-x))\n\ndef initialize_parameters():                                  # initialize all weights and biases and build dictionaries of both\n    weights = {                            \n        'w14': xavier(3, 2),\n        'w24': xavier(3, 2),\n        'w34': xavier(3, 2),\n        'w15': xavier(3, 2),\n        'w25': xavier(3, 2),\n        'w35': xavier(3, 2),\n        'w46': xavier(2, 1),\n        'w56': xavier(2, 1),\n        'w67': xavier(1, 2),\n        'w68': xavier(1, 2),\n        'w79': xavier(2, 3),\n        'w89': xavier(2, 3),\n        'w710': xavier(2, 3),\n        'w810': xavier(2, 3),\n        'w711': xavier(2, 3),\n        'w811': xavier(2, 3),\n    }\n    biases = {                                                # biases (one per neuron, excluding input)\n        'b4': 0.0,\n        'b5': 0.0,\n        'b6': 0.0,\n        'b7': 0.0,\n        'b8': 0.0,\n        'b9': 0.0,\n        'b10': 0.0,\n        'b11': 0.0\n    }\n    return weights, biases \n\ndef forward_pass(input_vec, weights, biases):                 # forward pass of the autoencoder\n    I1, I2, I3 = input_vec.flatten()                               # input nodes (I1, I2, I3)\n    z4 = weights['w14'] * I1 + weights['w24'] * I2 + weights['w34'] * I3 + biases['b4'] # encoder\n    a4 = sigmoid(z4)\n\n    z5 = weights['w15'] * I1 + weights['w25'] * I2 + weights['w35'] * I3 + biases['b5']\n    a5 = sigmoid(z5)\n\n    z6 = weights['w46'] * a4 + weights['w56'] * a5 + biases['b6'] # bottlekneck\n    a6 = sigmoid(z6)\n\n    z7 = weights['w67'] * a6 + biases['b7']                   # decoder\n    a7 = sigmoid(z7)\n\n    z8 = weights['w68'] * a6 + biases['b8']\n    a8 = sigmoid(z8)\n\n    z9 = weights['w79'] * a7 + weights['w89'] * a8 + biases['b9']\n    a9 = z9  \n\n    z10 = weights['w710'] * a7 + weights['w810'] * a8 + biases['b10']\n    a10 = z10  # linear\n\n    z11 = weights['w711'] * a7 + weights['w811'] * a8 + biases['b11']\n    a11 = z11  # linear\n\n    return {                                                  # return all activations as a dictionary\n        'I1': I1, 'I2': I2, 'I3': I3,\n        'L4': a4, 'L5': a5,\n        'M6': a6,\n        'R7': a7, 'R8': a8,\n        'O9': a9, 'O10': a10, 'O11': a11\n    }\n\ndef mse_loss_and_derivative(output_vec, input_vec):           # MSE loss and error derivative given output and input\n    diff = output_vec - input_vec\n    loss = np.mean(diff**2)\n    dloss_dout = (2/3) * diff  # shape (3,1)\n    return loss, dloss_dout\n\ndef sigmoid_derivative(x):                                    # derivative of sigmoid activation\n    return x * (1 - x)\n\ndef backpropagate(activations, weights, biases, dloss_dout):  # backpropagate the error derivatives\n    I1, I2, I3 = activations['I1'], activations['I2'], activations['I3']\n    a4, a5 = activations['L4'], activations['L5']\n    a6 = activations['M6']\n    a7, a8 = activations['R7'], activations['R8']\n    O9, O10, O11 = activations['O9'], activations['O10'], activations['O11']\n\n    delta9 = dloss_dout[0, 0]                                 # error terms (delta) for output nodes = dLoss/dOutput\n    delta10 = dloss_dout[1, 0]\n    delta11 = dloss_dout[2, 0]\n\n    grad_weights = {}                                         # gradients for weights from R7, R8 to O9, O10, O11\n    grad_biases = {}\n\n    grad_weights['w79'] = delta9 * a7\n    grad_weights['w89'] = delta9 * a8\n    grad_weights['w710'] = delta10 * a7\n    grad_weights['w810'] = delta10 * a8\n    grad_weights['w711'] = delta11 * a7\n    grad_weights['w811'] = delta11 * a8\n\n    grad_biases['b9'] = delta9\n    grad_biases['b10'] = delta10\n    grad_biases['b11'] = delta11\n\n    delta_r7 = (delta9 * weights['w79'] + delta10 * weights['w710'] + delta11 * weights['w711']) * sigmoid_derivative(a7) # gradients for R7 and R8\n    delta_r8 = (delta9 * weights['w89'] + delta10 * weights['w810'] + delta11 * weights['w811']) * sigmoid_derivative(a8)\n\n    grad_weights['w67'] = delta_r7 * a6                       # gradients for weights from M6 to R7, R8\n    grad_weights['w68'] = delta_r8 * a6\n\n    grad_biases['b7'] = delta_r7\n    grad_biases['b8'] = delta_r8\n\n    delta_m6 = (delta_r7 * weights['w67'] + delta_r8 * weights['w68']) * sigmoid_derivative(a6) # backpropagate delta to M6 (sigmoid)\n\n    grad_weights['w46'] = delta_m6 * a4                       # gradients for weights from L4, L5 to M6\n    grad_weights['w56'] = delta_m6 * a5\n\n    grad_biases['b6'] = delta_m6\n\n    delta_l4 = delta_m6 * weights['w46'] * sigmoid_derivative(a4) # backpropagate delta to L4, L5 (sigmoid)\n    delta_l5 = delta_m6 * weights['w56'] * sigmoid_derivative(a5)\n\n    grad_weights['w14'] = delta_l4 * I1                       # gradients for weights from I1, I2, I3 to L4\n    grad_weights['w24'] = delta_l4 * I2\n    grad_weights['w34'] = delta_l4 * I3\n\n    grad_biases['b4'] = delta_l4\n\n    grad_weights['w15'] = delta_l5 * I1                       # gradients for weights from I1, I2, I3 to L5\n    grad_weights['w25'] = delta_l5 * I2\n    grad_weights['w35'] = delta_l5 * I3\n\n    grad_biases['b5'] = delta_l5\n    return grad_weights, grad_biases\n\ndef update_parameters(weights, biases, grad_weights, grad_biases, learning_rate): # update the weights and biased by derivatives and learning rate\n    for key in grad_weights:                                  # update weights\n        weights[key] -= learning_rate * grad_weights[key]\n    for key in grad_biases:                                   # update biases\n        biases[key] -= learning_rate * grad_biases[key]\n    return weights, biases \n```", "```py\npositions = {                                                 # node positions\n    'I1': (0, 2), 'I2': (0, 1), 'I3': (0, 0),\n    'L4': (1, 1.5), 'L5': (1, 0.5),\n    'M6': (2, 1),\n    'R7': (3, 1.5), 'R8': (3, 0.5),\n    'O9': (4, 2), 'O10': (4, 1), 'O11': (4, 0),\n}\n\nnode_colors = {                                               # node colors\n    'I1': 'white', 'I2': 'white', 'I3': 'white',\n    'L4': 'white', 'L5': 'white',\n    'M6': 'white',\n    'R7': 'white', 'R8': 'white',\n    'O9': 'white', 'O10': 'white', 'O11': 'white',\n}\n\nedges = [                                                     # edges and weight labels\n    ('I1', 'L4', 'lightcoral'), ('I2', 'L4', 'red'), ('I3', 'L4', 'darkred'),\n    ('I1', 'L5', 'dodgerblue'), ('I2', 'L5', 'blue'), ('I3', 'L5', 'darkblue'),\n    ('L4', 'M6', 'orange'), ('L5', 'M6', 'darkorange'),\n    ('M6', 'R7', 'orange'), ('M6', 'R8', 'darkorange'),\n    ('R7', 'O9', 'lightcoral'), ('R7', 'O10', 'red'), ('R7', 'O11', 'darkred'),\n    ('R8', 'O9', 'dodgerblue'), ('R8', 'O10', 'blue'), ('R8', 'O11', 'darkblue'),\n]\n\nweight_labels = { (src, dst,): f\"$\\\\lambda_{{{src[1]}{dst[1:]}}}$\" for (src, dst, color) in edges }\n\nbias_offsets = {                                              # bias vector offsets\n    'L4': (0.06, 0.12), 'L5': (0.06, 0.12),\n    'M6': (0.0, 0.15),\n    'R7': (-0.06, 0.12), 'R8': (-0.06, 0.12),\n    'O9': (0.0, 0.15), 'O10': (0.0, 0.15), 'O11': (0.0, 0.15),\n}\n\nbias_labels = { node: f\"$b_{{{node[1:]}}}$\" for node in bias_offsets.keys() }\n# Plot\nfig, ax = plt.subplots(figsize=(11, 6))\n\ncustom_weight_offsets = {                                     # custom label offsets for select overlapping weights\n    ('I2', 'L4'): (-0.20, 0.0),\n    ('I2', 'L5'): (-0.2, 0.20),\n    ('R8', 'O9'): (0.15, 0.35),\n    ('R8', 'O10'): (0.15, 0.16),\n}\n\nfor (src, dst, color) in edges:                               # plot edges and weight labels\n    x0, y0 = positions[src]\n    x1, y1 = positions[dst]\n    ax.plot([x0, x1], [y0, y1], color=color, linewidth=1, zorder=1)\n    xm, ym = (x0 + x1) / 2, (y0 + y1) / 2\n    dx, dy = custom_weight_offsets.get((src, dst), (0, 0.08))\n    ax.text(xm + dx, ym + dy, weight_labels[(src, dst)],\n            fontsize=9, ha='center', va='center', color = color, zorder=5)\n\nfor node, (x, y) in positions.items():                        # white back circles\n    ax.scatter(x, y, s=1000, color='white', zorder=2)\n\nfor node, (x, y) in positions.items():                        # node circles and labels\n    ax.scatter(x, y, s=500, color=node_colors[node], edgecolors='black', zorder=3)\n    ax.text(x, y, node, ha='center', va='center', fontsize=9, zorder=4)\n\nfor node, (dx, dy) in bias_offsets.items():                   # bias arrows and tighter label placement\n    nx, ny = positions[node]\n    bx, by = nx + dx, ny + dy\n    ax.annotate(\"\", xy=(nx, ny), xytext=(bx, by),\n                arrowprops=dict(arrowstyle=\"->\", color='black'), zorder=2)\n    ax.text(bx, by, bias_labels[node], ha='right', va='bottom', fontsize=10)\n\n# Final formatting\nax.set_xlim(-0.5, 4.5)\nax.set_ylim(-0.5, 2.7)\nax.axis('off'); plt.tight_layout(); plt.show() \n```", "```py\nnp.random.seed(seed = seed+1)                                 # set random seed\nnbatch = 12; nnodes = 3; sigma = 0.1                          # set number of data (total number of data), number of nodes (must be 3), error st.dev.\nymat = np.zeros(nbatch); x = np.arange(1,nnodes+1,1); Xmat = np.zeros([nbatch,nnodes])\ndata = []\nfor ibatch in range(0,nbatch):                                # loop over synthetic data\n    m = np.random.uniform(low = -2.0, high = 2.0)\n    Xmat[ibatch] = (x-2.0)*m + np.random.normal(loc = 0.0, scale=sigma,size=nnodes)\n    ymat[ibatch] = np.dot(x, Xmat[ibatch]) / np.dot(x, x)\n    data.append(Xmat[ibatch].reshape(3,1))\n\nrank = rankdata(Xmat[:,-1])                                   # rank data to improve (alternate) adjacent labels' locations\nplt.subplot(111)                                              # plot the synthetic data\nfor ibatch in range(0,nbatch):                                \n    plt.scatter(Xmat[ibatch],x,color=cmap(ibatch/(nbatch)),edgecolor='black',lw=1,zorder=10)\n    plt.plot(Xmat[ibatch],x,color=cmap(ibatch/(nbatch)),lw=2,zorder=1)\n    custom_positions = [1,2,3,3.2]\n    custom_labels = ['I1','I2','I3','Y']\n    if rank[ibatch] % 2 == 0:\n        plt.annotate(np.round(ymat[ibatch],2),[Xmat[ibatch][-1],3.18],size=9,color='black',ha='center')\n    else:\n        plt.annotate(np.round(ymat[ibatch],2),[Xmat[ibatch][-1],3.25],size=9,color='black',ha='center') \n    plt.annotate(ibatch+1,[Xmat[ibatch][0],0.9],size=9,color='black',ha='center')\n    plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)\n    plt.ylim([3.4,0.8]); plt.xlim([-1.5,1.5]); plt.ylabel('Input Nodes'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Data and Labels')\nplt.annotate('Data Index: ',[-1.4,0.9])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nepochs = 10000                                                # set hyperparameters\nbatch_size = nbatch\nlearning_rate = 0.1\nseed = 13\nnp.random.seed(seed=seed)\n\noutput_mat = np.zeros((batch_size,epochs,3)); loss_mat = np.zeros((epochs)); M6_mat = np.zeros((batch_size,epochs))\n\nweights, biases = initialize_parameters()                     # initialize weights and biases\n\nfor epoch in range(epochs):\n    sum_grad_w = {k: 0 for k in weights.keys()}               # initialize zero dictionary to average backpropogated gradients\n    sum_grad_b = {k: 0 for k in biases.keys()}\n    epoch_loss = 0\n    for idata,input_vec in enumerate(data):\n        activations = forward_pass(input_vec, weights, biases) # forward pass\n        M6_mat[idata,epoch] = activations['M6']\n        output_vec = np.array([[activations['O9']], [activations['O10']], [activations['O11']]])\n        output_mat[idata,epoch,:] = output_vec.reshape(3)\n        loss, dloss_dout = mse_loss_and_derivative(output_vec, input_vec) # compute loss and derivative\n        epoch_loss += loss\n        grad_w, grad_b = backpropagate(activations, weights, biases, dloss_dout) # backpropagation the derivative\n        for k in grad_w:                                      # accumulate gradients\n            sum_grad_w[k] += grad_w[k]\n        for k in grad_b:\n            sum_grad_b[k] += grad_b[k]\n    avg_grad_w = {k: v / batch_size for k, v in sum_grad_w.items()} # average gradients over batch\n    avg_grad_b = {k: v / batch_size for k, v in sum_grad_b.items()}\n    epoch_loss /= batch_size\n    loss_mat[epoch] = epoch_loss\n    weights, biases = update_parameters(weights, biases, avg_grad_w, avg_grad_b, learning_rate) # update parameters\n    # if epoch % 500 == 0:                                    # print loss every 100 training epochs\n    #     print(f\"Epoch {epoch}, Loss: {epoch_loss:.6f}\")\n\nplt.subplot(111)                                              # plot training error vs. training epoch\nplt.plot(np.arange(0,epoch+1,1),loss_mat,color='red',label=r'MSE'); plt.xlim([1,epoch]); plt.ylim([0,1])\nplt.xlabel('Epochs'); plt.ylabel(r'Mean Square Error (L2 loss)'); plt.title('Autoencoder Average Batch L2 Loss vs. Training Epoch')\nadd_grid(); plt.legend(loc='upper right'); plt.xscale('linear')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlinear_model = LinearRegression().fit(ymat.reshape(-1, 1), M6_mat[:,-1]) # fit linear model to regress latent on training data slope\n\nplt.subplot(111)                                              # plot latent vs. training data slope\nplt.plot(np.linspace(-0.4,0.4,100),linear_model.predict(np.linspace(-0.4,0.4,100).reshape(-1,1)),color='red',zorder=-1)\nfor ibatch,input_vec in enumerate(data):                      # plot and label training data\n    plt.scatter(ymat[ibatch],M6_mat[ibatch,-1],color=cmap(ibatch/(nbatch)),edgecolor='black',marker='o',s=30,zorder=10)\n    plt.annotate(ibatch+1,[ymat[ibatch]-0.01,M6_mat[ibatch,-1]+0.01],size=9,color='black',ha='center',zorder=100) \nplt.ylabel('M6 Output'); plt.xlabel(r'Sample Slope, $m_i$'); plt.title('Latent Node Output vs. Sample Slope')\nplt.ylim([0.1,0.8]); plt.xlim([-0.4,0.4]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nfor idata,input_vec in enumerate(data):                       # plot training data and reconstructions \n    plt.subplot(4,3,idata+1)\n    plt.scatter(Xmat[idata],x,color=cmap(idata/(nbatch+2)),edgecolor='black',lw=1,zorder=10)\n    plt.plot(Xmat[idata],x,lw=1,zorder=1,color=cmap(idata/(nbatch+2)),label='data')\n    custom_positions = [1,2,3,3.2]\n    custom_labels = ['I1','I2','I3','Y']\n    plt.annotate(np.round(ymat[idata],2),[Xmat[idata][-1],3.25],size=9,color='black',ha='center')  \n    plt.scatter(output_mat[idata,-1,:],x,lw=1,color=cmap(idata/(nbatch+2)))\n    plt.plot(output_mat[idata,-1,:],x,lw=1,ls='--',color=cmap(idata/(nbatch+2)),label='reconstruction')\n    plt.legend(loc='upper left')\n    plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)\n    plt.ylim([3.5,0.8]); plt.xlim([-2.5,2.5]); plt.ylabel('index'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Training Data #' + str(idata+1))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=4.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nnp.random.seed(seed = seed+7)\nnbatch_test = 12; nnodes = 3; sigma = 0.1\nymat_test = np.zeros(nbatch); x = np.arange(1,nnodes+1,1); Xmat_test = np.zeros([nbatch,nnodes])\ndata_test = []\nfor ibatch in range(0,nbatch):\n    m = np.random.uniform(low = -2.0, high = 2.0)\n    Xmat_test[ibatch] = (x-2.0)*m + np.random.normal(loc = 0.0, scale=sigma,size=nnodes)\n    ymat_test[ibatch] = np.dot(x, Xmat_test[ibatch]) / np.dot(x, x)\n    data_test.append(Xmat_test[ibatch].reshape(3,1))\n\nrank = rankdata(Xmat_test[:,-1])\nplt.subplot(111)\nfor ibatch in range(0,nbatch_test):\n    plt.scatter(Xmat_test[ibatch],x,color=cmap(ibatch/(nbatch)),edgecolor='black',lw=1,zorder=10)\n    plt.plot(Xmat_test[ibatch],x,color=cmap(ibatch/(nbatch)),lw=2,zorder=1)\n    custom_positions = [1,2,3,3.2]\n    custom_labels = ['I1','I2','I3','Y']\n    if rank[ibatch] % 2 == 0:\n        plt.annotate(np.round(ymat_test[ibatch],2),[Xmat_test[ibatch][-1],3.18],size=9,color='black',ha='center')\n    else:\n        plt.annotate(np.round(ymat_test[ibatch],2),[Xmat_test[ibatch][-1],3.25],size=9,color='black',ha='center') \n    plt.annotate(ibatch+13,[Xmat_test[ibatch][0],0.9],size=9,color='black',ha='center')\n    plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)\n    plt.ylim([3.4,0.8]); plt.xlim([-1.5,1.5]); plt.ylabel('Input Nodes'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Data and Labels')\nplt.annotate('Test Data Index: ',[-1.45,0.9])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\noutput_vec_test = np.zeros((len(data_test),3))\nfor idata_test,input_vec_test in enumerate(data_test):\n    activations = forward_pass(input_vec_test, weights, biases)                                                    # forward pass\n    output_vec_test[idata_test,:] = np.array([[activations['O9']], [activations['O10']], [activations['O11']]]).reshape(-1) \n```", "```py\nfor idata,input_vec_test in enumerate(data_test):\n    plt.subplot(4,3,idata+1)\n    plt.scatter(input_vec_test,x,color=cmap(idata/(nbatch)),edgecolor='black',lw=1,zorder=10)\n    plt.plot(input_vec_test,x,lw=1,zorder=1,color=cmap(idata/(nbatch)),label='data')\n    custom_positions = [1,2,3,3.2]\n    custom_labels = ['I1','I2','I3','Y']\n    # plt.annotate(np.round(ymat[idata],2),[Xmat[idata][-1],3.25],size=8,color='black',ha='center') \n    plt.scatter(output_vec_test[idata,:],x,lw=1,color=cmap(idata/(nbatch)))\n    plt.plot(output_vec_test[idata,:],x,lw=1,ls='--',color=cmap(idata/(nbatch)),label='reconstruction')\n    plt.legend(loc='upper left'); plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)\n    plt.ylim([3.5,0.8]); plt.xlim([-1.5,1.5]); plt.ylabel('index'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Test Image #' + str(idata+13))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=4.1, wspace=0.2, hspace=0.2); plt.show() \n```"]