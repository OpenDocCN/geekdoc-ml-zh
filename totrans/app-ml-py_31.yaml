- en: Artificial Neural Networks Test
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œæµ‹è¯•
- en: åŸæ–‡ï¼š[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ANN.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ANN.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ANN.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ANN.html)
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·JÂ·çš®å°”å¥‡ï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Pythonä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Pythonä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
- en: 'Chapter of e-book â€œApplied Machine Learning in Python: a Hands-on Guide with
    Codeâ€.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç”µå­ä¹¦â€œPythonä¸­åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„å®æˆ˜æŒ‡å—â€çš„ç« èŠ‚ã€‚
- en: 'Cite this e-Book as:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å¼•ç”¨æ­¤ç”µå­ä¹¦å¦‚ä¸‹ï¼š
- en: 'Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with
    Code* [e-book]. Zenodo. doi:10.5281/zenodo.15169138 [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: çš®å°”å¥‡ï¼ŒM.J.ï¼Œ2024ï¼Œ*Pythonä¸­åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„å®æˆ˜æŒ‡å—* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138
    [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)
- en: 'The workflows in this book and more are available here:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠæ›´å¤šå†…å®¹åœ¨æ­¤å¤„å¯ç”¨ï¼š
- en: 'Cite the MachineLearningDemos GitHub Repository as:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¼•ç”¨ MachineLearningDemos GitHub ä»“åº“å¦‚ä¸‹ï¼š
- en: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration
    Workflows Repository* (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312\.
    GitHub repository: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'çš®å°”å¥‡ï¼ŒM.J.ï¼Œ2024ï¼Œ*MachineLearningDemos: Python æœºå™¨å­¦ä¹ æ¼”ç¤ºå·¥ä½œæµç¨‹å­˜å‚¨åº“*ï¼ˆ0.0.3ï¼‰[è½¯ä»¶]. Zenodo.
    DOI: 10.5281/zenodo.13835312\. GitHub ä»“åº“ï¼š[GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
- en: By Michael J. Pyrcz
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…ï¼šè¿ˆå…‹å°”Â·JÂ·çš®å°”å¥‡
- en: Â© Copyright 2024.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Â© ç‰ˆæƒ 2024ã€‚
- en: This chapter is a tutorial for / demonstration of **Artificial Neural Networks**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç« æ˜¯å…³äº/æ¼”ç¤º **äººå·¥ç¥ç»ç½‘ç»œ** çš„æ•™ç¨‹ã€‚
- en: '**YouTube Lecture**: check out my lectures on:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**YouTube è®²åº§**ï¼šæŸ¥çœ‹æˆ‘åœ¨ä»¥ä¸‹æ–¹é¢çš„è®²åº§ï¼š'
- en: '[Artificial Neural Networks](https://youtu.be/A9PiCMY_6nM?si=NxWSU_5RgQ4w55EL)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[äººå·¥ç¥ç»ç½‘ç»œ](https://youtu.be/A9PiCMY_6nM?si=NxWSU_5RgQ4w55EL)'
- en: '[Convolutional Neural Networks](https://youtu.be/za2my_XDoOs?si=LeHU6p2_fc9dX4Yt)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å·ç§¯ç¥ç»ç½‘ç»œ](https://youtu.be/za2my_XDoOs?si=LeHU6p2_fc9dX4Yt)'
- en: These lectures are all part of my [Machine Learning Course](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)
    on YouTube with linked well-documented Python workflows and interactive dashboards.
    My goal is to share accessible, actionable, and repeatable educational content.
    If you want to know about my motivation, check out [Michaelâ€™s Story](https://michaelpyrcz.com/my-story).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘ YouTube ä¸Šçš„ [æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)
    çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«æœ‰è‰¯å¥½æ–‡æ¡£è®°å½•çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºç†è§£ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœä½ æƒ³çŸ¥é“æˆ‘çš„åŠ¨æœºï¼Œè¯·æŸ¥çœ‹ [è¿ˆå…‹å°”çš„ç»å†](https://michaelpyrcz.com/my-story)ã€‚
- en: Motivation
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ¨æœº
- en: Artificial neural networks are very powerful, nature inspired computing based
    on an analogy of brain
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œéå¸¸å¼ºå¤§ï¼Œæ˜¯åŸºäºå¤§è„‘ç±»æ¯”çš„è‡ªç„¶å¯å‘è®¡ç®—
- en: I suggest that they are like a reptilian brain, without planning and higher
    order reasoning
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºå®ƒä»¬å°±åƒçˆ¬è¡ŒåŠ¨ç‰©çš„è„‘ï¼Œæ²¡æœ‰è®¡åˆ’å’Œé«˜çº§æ¨ç†
- en: In addition, artificial neural networks are a building block of many other deep
    learning methods, for example,
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œäººå·¥ç¥ç»ç½‘ç»œæ˜¯è®¸å¤šå…¶ä»–æ·±åº¦å­¦ä¹ æ–¹æ³•çš„åŸºçŸ³ï¼Œä¾‹å¦‚ï¼Œ
- en: convolutional neural networks
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œ
- en: recurrent neural networks
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¾ªç¯ç¥ç»ç½‘ç»œ
- en: generative adversarial networks
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ
- en: autoencoders
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨ç¼–ç å™¨
- en: Nature inspired computing is looking to nature for inspiration to develop novel
    problem-solving methods,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç„¶å¯å‘è®¡ç®—æ­£åœ¨å¯»æ‰¾è‡ªç„¶ç•Œçš„çµæ„Ÿæ¥å¼€å‘æ–°çš„é—®é¢˜è§£å†³æ–¹æ³•ï¼Œ
- en: '**artificial neural networks** are inspired by biological neural networks'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**äººå·¥ç¥ç»ç½‘ç»œ**æ˜¯å—ç”Ÿç‰©ç¥ç»ç½‘ç»œå¯å‘çš„'
- en: '**nodes** - in our model are artificial neurons, simple processors'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠ‚ç‚¹** - åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­æ˜¯äººå·¥ç¥ç»å…ƒï¼Œç®€å•çš„å¤„ç†å™¨'
- en: '**connections** between nodes are artificial synapses'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥**æ˜¯äººå·¥çªè§¦'
- en: intelligence emerges from many connected simple processors. For the remainder
    of this chapter, I will used the terms nodes and connections to describe our artificial
    neural network.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ™ºèƒ½æ˜¯ä»è®¸å¤šè¿æ¥çš„ç®€å•å¤„ç†å™¨ä¸­äº§ç”Ÿçš„ã€‚åœ¨æœ¬ç« çš„å‰©ä½™éƒ¨åˆ†ï¼Œæˆ‘å°†ä½¿ç”¨èŠ‚ç‚¹å’Œè¿æ¥æœ¯è¯­æ¥æè¿°æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œã€‚
- en: Neural Network Concepts
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ¦‚å¿µ
- en: Here are some key aspects of artificial neural networks,
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€äº›äººå·¥ç¥ç»ç½‘ç»œçš„å…³é”®æ–¹é¢ï¼Œ
- en: '**Basic Design** - *â€œâ€¦a computing system made up of a number of simple, highly
    interconnected processing elements, which process information by their dynamic
    state response to external inputs.â€* Caudill (1989).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºæœ¬è®¾è®¡** - *â€œâ€¦ä¸€ä¸ªç”±è®¸å¤šç®€å•ã€é«˜åº¦äº’è”çš„å¤„ç†å…ƒç´ ç»„æˆçš„è®¡ç®—ç³»ç»Ÿï¼Œå®ƒä»¬é€šè¿‡å¯¹å¤–éƒ¨è¾“å…¥çš„åŠ¨æ€çŠ¶æ€å“åº”æ¥å¤„ç†ä¿¡æ¯ã€‚â€* Caudill (1989)ã€‚'
- en: '**Still a Prediction Model** - while these models may be quite complicated
    with even millions of trainable model parameters, weights and biases, they are
    still a function that maps from predictor features to response features,'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»ç„¶æ˜¯ä¸€ä¸ªé¢„æµ‹æ¨¡å‹** - è™½ç„¶è¿™äº›æ¨¡å‹å¯èƒ½éå¸¸å¤æ‚ï¼Œç”šè‡³æœ‰æ•°ç™¾ä¸‡ä¸ªå¯è®­ç»ƒæ¨¡å‹å‚æ•°ã€æƒé‡å’Œåå·®ï¼Œä½†å®ƒä»¬ä»ç„¶æ˜¯ä¸€ä¸ªå°†é¢„æµ‹ç‰¹å¾æ˜ å°„åˆ°å“åº”ç‰¹å¾çš„å‡½æ•°ï¼Œ'
- en: \[ Y=f(X)+\epsilon \]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Y=f(X)+\epsilon \]
- en: '**Supervised learning** â€“ we provide training data with predictor features,
    \(X_1,\ldots,ğ‘‹_ğ‘š\) and response feature(s), \(ğ‘Œ_1,\ldots,ğ‘Œ_K\), with the expectation
    of some model prediction error, \(\epsilon\).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›‘ç£å­¦ä¹ ** â€“ æˆ‘ä»¬æä¾›å¸¦æœ‰é¢„æµ‹ç‰¹å¾ï¼Œ\(X_1,\ldots,ğ‘‹_ğ‘š\) å’Œå“åº”ç‰¹å¾ï¼ˆ\(ğ‘Œ_1,\ldots,ğ‘Œ_K\)ï¼‰çš„è®­ç»ƒæ•°æ®ï¼ŒæœŸæœ›æ¨¡å‹é¢„æµ‹è¯¯å·®ï¼Œ\(\epsilon\)ã€‚'
- en: '**Nonlinearity** - nonlinearity is imparted to the system through the application
    of nonlinear activation functions to model nonlinear relationships'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**éçº¿æ€§** - é€šè¿‡åº”ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°æ¥æ¨¡å‹éçº¿æ€§å…³ç³»ï¼Œå°†éçº¿æ€§å¼•å…¥åˆ°ç³»ç»Ÿä¸­'
- en: '**Universal Function Approximator (Universal Approximation Theorem)** - ANNs
    have the ability to learn any possible function shape of \(f\) over an interval,
    for an arbitrary wide (single hidden layer) by Cybenko (1989) and arbitrary depth
    by Lu and others (2017)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**é€šç”¨å‡½æ•°é€¼è¿‘å™¨ï¼ˆé€šç”¨é€¼è¿‘å®šç†ï¼‰** - ANNs èƒ½å¤Ÿå­¦ä¹ åŒºé—´å†… \(f\) çš„ä»»ä½•å¯èƒ½å‡½æ•°å½¢çŠ¶ï¼Œå¯¹äºä»»æ„å®½ï¼ˆå•éšè—å±‚ï¼‰ç”± Cybenko (1989)
    æå‡ºï¼Œå¯¹äºä»»æ„æ·±åº¦ç”± Lu å’Œå…¶ä»–äºº (2017) æå‡º'
- en: A Simple Network
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„ç½‘ç»œ
- en: To get started, letâ€™s build a neural net, single hidden layer, fully connected,
    feed-forward neural network,
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹ï¼Œè®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå•éšè—å±‚ï¼Œå…¨è¿æ¥ï¼Œå‰é¦ˆç¥ç»ç½‘ç»œï¼Œ
- en: '![](../Images/832ee38f2f09d395115eb31b95358103.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/832ee38f2f09d395115eb31b95358103.png)'
- en: Simple demonstration artificial neural network.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€å•æ¼”ç¤ºäººå·¥ç¥ç»ç½‘ç»œã€‚
- en: We use this example artificial neural network in the descriptions below and
    as an actual example that we will train and predict with by-hand!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ä¸‹é¢çš„æè¿°ä¸­ä½¿ç”¨è¿™ä¸ªç¤ºä¾‹äººå·¥ç¥ç»ç½‘ç»œï¼Œå¹¶å°†å…¶ä½œä¸ºæˆ‘ä»¬å°†æ‰‹åŠ¨è®­ç»ƒå’Œé¢„æµ‹çš„å®é™…ç¤ºä¾‹ï¼
- en: Now letâ€™s label the parts of our network,
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ ‡è®°æˆ‘ä»¬ç½‘ç»œçš„éƒ¨åˆ†ï¼Œ
- en: '![](../Images/0be0dcf09e698cf22dc0881c818c0dbc.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0be0dcf09e698cf22dc0881c818c0dbc.png)'
- en: Simple demonstration artificial neural network with the parts labeled, including
    3 inputs nodes, 2 hidden nodes and 1 output node fully connected.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰æ ‡è®°çš„ç®€å•æ¼”ç¤ºäººå·¥ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬ 3 ä¸ªè¾“å…¥èŠ‚ç‚¹ã€2 ä¸ªéšè—èŠ‚ç‚¹å’Œ 1 ä¸ªè¾“å‡ºèŠ‚ç‚¹å®Œå…¨è¿æ¥ã€‚
- en: Our artificial neural network has,
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œå…·æœ‰ï¼Œ
- en: 3 predictor features, \(X_1\), \(X_2\) and \(X_3\)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œ\(X_1\)ã€\(X_2\) å’Œ \(X_3\)
- en: 3 input nodes, \(I_1\), \(I_2\) and \(I_3\)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œ\(I_1\)ã€\(I_2\) å’Œ \(I_3\)
- en: 2 hidden layer nodes, \(H_4\) and \(H_5\)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2ä¸ªéšè—å±‚èŠ‚ç‚¹ï¼Œ\(H_4\) å’Œ \(H_5\)
- en: 1 output node, \(O_6\)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1ä¸ªè¾“å‡ºèŠ‚ç‚¹ï¼Œ\(O_6\)
- en: 1 response feature, \(Y_1\)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1ä¸ªå“åº”ç‰¹å¾ï¼Œ\(Y_1\)
- en: where all nodes fully connected. Note, deep learning is a neural network with
    more than 1 hidden layer, but for brevity letâ€™s continue with our non-deep learning
    artificial neural network.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ‰€æœ‰èŠ‚ç‚¹éƒ½æ˜¯å®Œå…¨è¿æ¥çš„ã€‚æ³¨æ„ï¼Œæ·±åº¦å­¦ä¹ æ˜¯ä¸€ä¸ªå…·æœ‰è¶…è¿‡ 1 ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œï¼Œä½†ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬ç»§ç»­ä½¿ç”¨æˆ‘ä»¬çš„éæ·±åº¦å­¦ä¹ äººå·¥ç¥ç»ç½‘ç»œã€‚
- en: Comments on Network Nomenclature
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç½‘ç»œå‘½åæ³•çš„è¯„è®º
- en: Just a couple more comments about my network nomenclature. My goal is to maximize
    simplicity and clarity,
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæˆ‘çš„ç½‘ç»œå‘½åæ³•çš„å‡ ç‚¹è¯„è®ºã€‚æˆ‘çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç®€æ´æ€§å’Œæ¸…æ™°åº¦ï¼Œ
- en: '**Network Nodes and Connections** - I choose to use unique numbers for all
    nodes, \(I_1\), \(I_2\), \(I_3\), \(H_4\), \(H_5\) and \(O_6\), instead of repeating
    numbers over each layer, \(I_1\), \(I_2\), \(I_3\), \(H_1\), \(H_2\), and \(O_1\)
    to simplify the notation for the weights; therefore, when I say \(\lambda_{1,4}\)
    you know exactly where this weight is applied in the network, from node \(I_1\)
    to node \(H_4\).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç½‘ç»œèŠ‚ç‚¹å’Œè¿æ¥** - æˆ‘é€‰æ‹©ä¸ºæ‰€æœ‰èŠ‚ç‚¹ä½¿ç”¨å”¯ä¸€çš„æ•°å­—ï¼Œ\(I_1\), \(I_2\), \(I_3\), \(H_4\), \(H_5\) å’Œ
    \(O_6\)ï¼Œè€Œä¸æ˜¯åœ¨æ¯ä¸ªå±‚é‡å¤ä½¿ç”¨æ•°å­—ï¼Œä¾‹å¦‚ \(I_1\), \(I_2\), \(I_3\), \(H_1\), \(H_2\) å’Œ \(O_1\)ï¼Œä»¥ç®€åŒ–æƒé‡çš„è¡¨ç¤ºï¼›å› æ­¤ï¼Œå½“æˆ‘è¯´
    \(\lambda_{1,4}\) æ—¶ï¼Œä½ çŸ¥é“è¿™ä¸ªæƒé‡åœ¨ç½‘ç»œçš„å“ªä¸ªä½ç½®åº”ç”¨ï¼Œä»èŠ‚ç‚¹ \(I_1\) åˆ°èŠ‚ç‚¹ \(H_4\)ã€‚'
- en: '**Node Outputs** - I use the node label to also describe the output from the
    node, for example \(O_6\) is the output node, \(O_6\), and also the signal or
    value output from node \(O_6\),'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠ‚ç‚¹è¾“å‡º** - æˆ‘ä½¿ç”¨èŠ‚ç‚¹æ ‡ç­¾æ¥æè¿°èŠ‚ç‚¹çš„è¾“å‡ºï¼Œä¾‹å¦‚ \(O_6\) æ˜¯è¾“å‡ºèŠ‚ç‚¹ï¼Œ\(O_6\)ï¼Œä»¥åŠä»èŠ‚ç‚¹ \(O_6\) è¾“å‡ºçš„ä¿¡å·æˆ–å€¼ï¼Œ'
- en: \[ O_6 = \sigma \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 \right)
    \]
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \sigma \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 \right)
    \]
- en: '**Pre- and Post-activation** - at our nodes \(H_4\), \(H_5\), and \(O_6\),
    we have the node input before activation and the node output after activation,
    I use the notation \(H_{4_{in}}\), \(H_{5_{in}}\), and \(O_{6_{in}}\) for the
    pre-activation input,'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é¢„æ¿€æ´»å’Œåæ¿€æ´»** - åœ¨æˆ‘ä»¬çš„èŠ‚ç‚¹ \(H_4\), \(H_5\) å’Œ \(O_6\) ä¸­ï¼Œæˆ‘ä»¬æœ‰èŠ‚ç‚¹æ¿€æ´»å‰çš„è¾“å…¥å’Œæ¿€æ´»åçš„è¾“å‡ºï¼Œæˆ‘ä½¿ç”¨ \(H_{4_{in}}\),
    \(H_{5_{in}}\) å’Œ \(O_{6_{in}}\) æ¥è¡¨ç¤ºé¢„æ¿€æ´»è¾“å…¥ï¼Œ'
- en: \[ H_{4_{in}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 \]\[ H_{5_{in}} = \lambda_{1,5} \cdot I_1 + \lambda_{2,5} \cdot I_2
    + \lambda_{2,5} \cdot I_3 \]\[ O_{6_{in}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6}
    \cdot H_5 \]
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4_{in}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 \]\[ H_{5_{in}} = \lambda_{1,5} \cdot I_1 + \lambda_{2,5} \cdot I_2
    + \lambda_{2,5} \cdot I_3 \]\[ O_{6_{in}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6}
    \cdot H_5 \]
- en: \(\quad\) and \(H_4\), \(H_5\), and \(O_6\) for the post-activation node output.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: \(\quad\) ä»¥åŠ \(H_4\), \(H_5\) å’Œ \(O_6\) ä½œä¸ºåæ¿€æ´»èŠ‚ç‚¹çš„è¾“å‡ºã€‚
- en: \[ H_{4} = \sigma \left( H_{4_{in}} \right) = \sigma \left( \lambda_{1,4} \cdot
    I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 \right) \]\[ H_{5} = \sigma
    \left( H_{5_{in}} \right) = \sigma \left( \lambda_{1,5} \cdot I_1 + \lambda_{2,5}
    \cdot I_2 + \lambda_{2,5} \cdot I_3 \right) \]\[ O_6 = \sigma \left( O_{6_{in}}
    \right) = \sigma \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 \right)
    \]
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4} = \sigma \left( H_{4_{in}} \right) = \sigma \left( \lambda_{1,4} \cdot
    I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 \right) \]\[ H_{5} = \sigma
    \left( H_{5_{in}} \right) = \sigma \left( \lambda_{1,5} \cdot I_1 + \lambda_{2,5}
    \cdot I_2 + \lambda_{2,5} \cdot I_3 \right) \]\[ O_6 = \sigma \left( O_{6_{in}}
    \right) = \sigma \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 \right)
    \]
- en: It is important to have clean, clear notation because with back propagation
    we have to step through the nodes, going from post-activation to pre-activation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ¸…æ™°ã€æ˜ç¡®çš„ç¬¦å·å¾ˆé‡è¦ï¼Œå› ä¸ºåœ¨ä½¿ç”¨åå‘ä¼ æ’­æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»é€æ­¥é€šè¿‡èŠ‚ç‚¹ï¼Œä»åæ¿€æ´»åˆ°é¢„æ¿€æ´»ã€‚
- en: often variables like \(z\) are applied for pre-activation in neural network
    literature, but I feel this is ambiguous and may cause confusion as we provide
    a nuts and bolts approach, explicitly describing every equation, to describe exactly
    how neural networks are trained and predict
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¥ç»ç½‘ç»œæ–‡çŒ®ä¸­ï¼Œé€šå¸¸ä½¿ç”¨åƒ \(z\) è¿™æ ·çš„å˜é‡è¿›è¡Œé¢„æ¿€æ´»ï¼Œä½†æˆ‘è®¤ä¸ºè¿™å¾ˆæ¨¡ç³Šï¼Œå¯èƒ½ä¼šå¼•èµ·æ··æ·†ï¼Œå› ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªèºä¸é’‰å’Œèºæ¯çš„æ–¹æ³•ï¼Œæ˜ç¡®æè¿°æ¯ä¸ªæ–¹ç¨‹ï¼Œä»¥ç²¾ç¡®æè¿°ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•è®­ç»ƒå’Œé¢„æµ‹çš„
- en: Description of the Network Approach
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç½‘ç»œæ–¹æ³•æè¿°
- en: Letâ€™s talk about the network, the parts and how information flows through the
    network.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è°ˆè°ˆç½‘ç»œï¼Œå…¶ç»„æˆéƒ¨åˆ†ä»¥åŠä¿¡æ¯æ˜¯å¦‚ä½•åœ¨ç½‘ç»œä¸­æµåŠ¨çš„ã€‚
- en: '**Feed-forward** â€“ all information flows from left to right. Each node sends
    the same signal along the connections to all the nodes in the next layer,'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‰é¦ˆ** â€“ æ‰€æœ‰ä¿¡æ¯ä»å·¦åˆ°å³æµåŠ¨ã€‚æ¯ä¸ªèŠ‚ç‚¹æ²¿ç€è¿æ¥å°†ç›¸åŒçš„ä¿¡å·å‘é€åˆ°ä¸‹ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹ï¼Œ'
- en: '![](../Images/02be26cd4d5476666cd2e1a72c3005fe.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02be26cd4d5476666cd2e1a72c3005fe.png)'
- en: Feed forward, fully connected, with each node sending the same signal to all
    the nodes in the next layer.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¦ˆï¼Œå…¨è¿æ¥ï¼Œæ¯ä¸ªèŠ‚ç‚¹å°†ç›¸åŒçš„ä¿¡å·å‘é€åˆ°ä¸‹ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹ã€‚
- en: '**Input Layer** - the input features are passed directly to the input nodes,
    in the case of continuous predictor features, there is one input node per feature
    and the features are,'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å…¥å±‚** - è¾“å…¥ç‰¹å¾ç›´æ¥ä¼ é€’åˆ°è¾“å…¥èŠ‚ç‚¹ï¼Œå¯¹äºè¿ç»­é¢„æµ‹ç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾æœ‰ä¸€ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œå¹¶ä¸”ç‰¹å¾æ˜¯ï¼Œ'
- en: min / max normalization to a range \(\left[ âˆ’1,1 \right]\) or \(\left[ 0,1 \right]\)
    to improve activation function sensitivity and to remove the influence of scale
    differences in predictor features and to improve solution stability, i.e., smooth
    reduction in the training loss while training
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æœ€å°/æœ€å¤§å½’ä¸€åŒ–åˆ°èŒƒå›´ \(\left[ âˆ’1,1 \right]\) æˆ– \(\left[ 0,1 \right]\) ä»¥æé«˜æ¿€æ´»å‡½æ•°çš„æ•æ„Ÿæ€§ï¼Œå¹¶æ¶ˆé™¤é¢„æµ‹ç‰¹å¾ä¸­å°ºåº¦å·®å¼‚çš„å½±å“ï¼Œä»¥æé«˜è§£å†³æ–¹æ¡ˆçš„ç¨³å®šæ€§ï¼Œå³ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹³æ»‘å‡å°‘è®­ç»ƒæŸå¤±
- en: '![](../Images/cb20e85861a55df55677b55ce737bbd5.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb20e85861a55df55677b55ce737bbd5.png)'
- en: Highlighting the input layer, the first layer that receives the normalized predictor
    features.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: çªå‡ºæ˜¾ç¤ºè¾“å…¥å±‚ï¼Œè¿™æ˜¯æ¥æ”¶å½’ä¸€åŒ–é¢„æµ‹ç‰¹å¾çš„ç¬¬ä¸€å±‚ã€‚
- en: In the case of categorical predictor features, we have one input node per each
    category for each predictor feature, i.e., after one-hot-encoding of the feature
    where each encoding is passed to a separate input node.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†ç±»é¢„æµ‹ç‰¹å¾ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªé¢„æµ‹ç‰¹å¾ä¸­çš„æ¯ä¸ªç±»åˆ«æœ‰ä¸€ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œå³ç‰¹å¾çš„ä¸€çƒ­ç¼–ç åï¼Œæ¯ä¸ªç¼–ç éƒ½ä¼ é€’åˆ°ä¸€ä¸ªå•ç‹¬çš„è¾“å…¥èŠ‚ç‚¹ã€‚
- en: recall one-hot-encoding, 1 if the specific category, 0 otherwise, replaces the
    categorical feature with a binary vector with length as the number of categories.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›å¿†ä¸€ä¸‹ä¸€çƒ­ç¼–ç ï¼Œå¦‚æœç‰¹å®šç±»åˆ«ä¸º1ï¼Œå¦åˆ™ä¸º0ï¼Œç”¨é•¿åº¦ç­‰äºç±»åˆ«æ•°é‡çš„äºŒè¿›åˆ¶å‘é‡æ›¿æ¢åˆ†ç±»ç‰¹å¾ã€‚
- en: '![](../Images/51d695aaaa36c3450f3997691fb0090f.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51d695aaaa36c3450f3997691fb0090f.png)'
- en: The input layer of our artificial neural network highlighted. The first layer
    that receives one-hot-encoding of a single categorical predictor feature.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œä¸­çš„è¾“å…¥å±‚è¢«çªå‡ºæ˜¾ç¤ºã€‚è¿™æ˜¯æ¥æ”¶å•ä¸ªåˆ†ç±»é¢„æµ‹ç‰¹å¾çš„ä¸€çƒ­ç¼–ç çš„ç¬¬ä¸€å±‚ã€‚
- en: we could also use a single input node per categorical predictor and assign thresholds
    to each categories, for example \(\left[ 0.0, 0.5, 1.0 \right]\) for 3 categories,
    but this assumes an ordinal categorical feature
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¸ºæ¯ä¸ªåˆ†ç±»é¢„æµ‹ç‰¹å¾ä½¿ç”¨å•ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œå¹¶ä¸ºæ¯ä¸ªç±»åˆ«åˆ†é…é˜ˆå€¼ï¼Œä¾‹å¦‚å¯¹äº3ä¸ªç±»åˆ«ï¼Œå¯ä»¥æ˜¯ \(\left[ 0.0, 0.5, 1.0 \right]\)ï¼Œä½†è¿™å‡è®¾åˆ†ç±»ç‰¹å¾æ˜¯é¡ºåºçš„ã€‚
- en: '**Hidden Layer** - the input layer values \(I_1, I_2, I_2\) are weighted with
    learnable weights,'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**éšè—å±‚** - è¾“å…¥å±‚å€¼ \(I_1, I_2, I_2\) ä¸å¯å­¦ä¹ çš„æƒé‡ç›¸ä¹˜ï¼Œ'
- en: \[ \lambda_{1,4}, \lambda_{2,4}, \lambda_{3,4}, \lambda_{1,5}, \lambda_{2,5},
    \lambda_{3,5} \]
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{1,4}, \lambda_{2,4}, \lambda_{3,4}, \lambda_{1,5}, \lambda_{2,5},
    \lambda_{3,5} \]
- en: in the hidden layer nodes, the weighted input layer values, \(\lambda_{1,4}
    \cdot I_1, \lambda_{2,4} \cdot I_2 \cdot I_2, \ldots, \lambda_{3,5} \cdot I_3\)
    are summed with the addition of a trainable bias term in each node, \(b_4\) and
    \(b_5\).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨éšè—å±‚èŠ‚ç‚¹ä¸­ï¼ŒåŠ æƒè¾“å…¥å±‚å€¼ï¼Œ\(\lambda_{1,4} \cdot I_1, \lambda_{2,4} \cdot I_2 \cdot I_2,
    \ldots, \lambda_{3,5} \cdot I_3\) ä¸æ¯ä¸ªèŠ‚ç‚¹ä¸­å¯è®­ç»ƒçš„åç½®é¡¹ \(b_4\) å’Œ \(b_5\) ç›¸åŠ ã€‚
- en: \[ H_{4_{in}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]\[ H_{5_{in}} = \lambda_{1,5} \cdot I_1 + \lambda_{2,5} \cdot
    I_2 + \lambda_{3,5} \cdot I_3 + b_5 \]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4_{in}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]\[ H_{5_{in}} = \lambda_{1,5} \cdot I_1 + \lambda_{2,5} \cdot
    I_2 + \lambda_{3,5} \cdot I_3 + b_5 \]
- en: the nonlinear activation is applied,
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: éçº¿æ€§æ¿€æ´»å‡½æ•°è¢«åº”ç”¨ï¼Œ
- en: \[ H_{4} = \sigma \bigl( \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 +
    \lambda_{3,4} \cdot I_3 + b_4 \bigr) \]\[ H_{5} = \sigma \bigl( \lambda_{1,5}
    \cdot I_1 + \lambda_{2,5} \cdot I_2 + \lambda_{3,5} \cdot I_3 + b_5 \bigr) \]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4} = \sigma \bigl( \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 +
    \lambda_{3,4} \cdot I_3 + b_4 \bigr) \]\[ H_{5} = \sigma \bigl( \lambda_{1,5}
    \cdot I_1 + \lambda_{2,5} \cdot I_2 + \lambda_{3,5} \cdot I_3 + b_5 \bigr) \]
- en: the output from the input layer nodes to all hidden layer nodes is contant (again,
    each node sends the same value to all nodes in the next layer)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥å±‚èŠ‚ç‚¹åˆ°æ‰€æœ‰éšè—å±‚èŠ‚ç‚¹çš„è¾“å‡ºæ˜¯æ’å®šçš„ï¼ˆå†æ¬¡å¼ºè°ƒï¼Œæ¯ä¸ªèŠ‚ç‚¹å°†ç›¸åŒçš„å€¼å‘é€åˆ°ä¸‹ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹ï¼‰
- en: '![](../Images/e2342c26953974528579d98e504c15c8.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2342c26953974528579d98e504c15c8.png)'
- en: The hidden layer of our artificial neural network highlighted. The input layer
    nodes' outputs are weighted and passed into the hidden layer nodes. The output
    from the hidden layer nodes to all output layer nodes is constant.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œä¸­çš„éšè—å±‚è¢«çªå‡ºæ˜¾ç¤ºã€‚è¾“å…¥å±‚èŠ‚ç‚¹çš„è¾“å‡ºè¢«åŠ æƒå¹¶ä¼ é€’åˆ°éšè—å±‚èŠ‚ç‚¹ã€‚éšè—å±‚èŠ‚ç‚¹åˆ°æ‰€æœ‰è¾“å‡ºå±‚èŠ‚ç‚¹çš„è¾“å‡ºæ˜¯æ’å®šçš„ã€‚
- en: '**Output Layer** - for continuous response features there is one output node
    per normalized response feature. Once again the weighted linear combination of
    inputs plus a node bias are calculated,'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡ºå±‚** - å¯¹äºè¿ç»­å“åº”ç‰¹å¾ï¼Œæ¯ä¸ªå½’ä¸€åŒ–å“åº”ç‰¹å¾æœ‰ä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹ã€‚å†æ¬¡è®¡ç®—è¾“å…¥çš„åŠ æƒçº¿æ€§ç»„åˆåŠ ä¸ŠèŠ‚ç‚¹åç½®ï¼Œ'
- en: \[ O_{6_{in}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \]
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{in}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \]
- en: and then activation is applied, but for a continuous response feature, typically
    identity (linear) transform is applied,
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œä½†å¯¹äºè¿ç»­å“åº”ç‰¹å¾ï¼Œé€šå¸¸åº”ç”¨æ’ç­‰ï¼ˆçº¿æ€§ï¼‰å˜æ¢ï¼Œ
- en: \[ O_6 = \alpha \bigl( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \bigr) = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 = O_{6_{in}}
    \]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \alpha \bigl( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \bigr) = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 = O_{6_{in}}
    \]
- en: backtransformation from normalized to original response feature(s) are then
    applied to recover the ultimate prediction
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»å½’ä¸€åŒ–åˆ°åŸå§‹å“åº”ç‰¹å¾çš„åè½¬æ¢ç„¶ååº”ç”¨ä»¥æ¢å¤æœ€ç»ˆé¢„æµ‹
- en: as with continuous predictor features, min / max normalization is applied to
    continuous response features to a range [âˆ’1,1] or [0,1] to improve activation
    function sensitivity
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°±åƒè¿ç»­é¢„æµ‹ç‰¹å¾ä¸€æ ·ï¼Œå¯¹è¿ç»­å“åº”ç‰¹å¾åº”ç”¨æœ€å°/æœ€å¤§å½’ä¸€åŒ–ï¼Œå°†å…¶èŒƒå›´è®¾ç½®ä¸º[âˆ’1,1]æˆ–[0,1]ï¼Œä»¥æé«˜æ¿€æ´»å‡½æ•°çš„æ•æ„Ÿæ€§
- en: '![](../Images/87d8ca10f44d6d5d97c7e6ced2d415e5.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87d8ca10f44d6d5d97c7e6ced2d415e5.png)'
- en: The output layer of our artificial neural network highlighted. The hidden layer
    nodes' outputs are weighted and passed into the output layer nodes. The output
    from the hidden layer nodes is constant, but the weights vary over the hidden
    layer node to output layer node connections.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: çªå‡ºæ˜¾ç¤ºæˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œçš„è¾“å‡ºå±‚ã€‚éšè—å±‚èŠ‚ç‚¹çš„è¾“å‡ºè¢«åŠ æƒå¹¶ä¼ é€’åˆ°è¾“å‡ºå±‚èŠ‚ç‚¹ã€‚éšè—å±‚èŠ‚ç‚¹çš„è¾“å‡ºæ˜¯æ’å®šçš„ï¼Œä½†æƒé‡åœ¨éšè—å±‚èŠ‚ç‚¹åˆ°è¾“å‡ºå±‚èŠ‚ç‚¹çš„è¿æ¥ä¸­å˜åŒ–ã€‚
- en: In the case of a categorical response feature, once again one-hot-encoding is
    applied, therefore, there is one output node per category.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†ç±»å“åº”ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œå†æ¬¡åº”ç”¨one-hotç¼–ç ï¼Œå› æ­¤ï¼Œæ¯ä¸ªç±»åˆ«éƒ½æœ‰ä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹ã€‚
- en: the prediction is the probability of each category
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢„æµ‹æ˜¯æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡
- en: '![](../Images/11c0969ef52f11e6929df9ff8e4d92c2.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11c0969ef52f11e6929df9ff8e4d92c2.png)'
- en: Highlighting the input layer, the first layer that receives one-hot-encoding
    of a single categorical predictor feature.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: çªå‡ºæ˜¾ç¤ºè¾“å…¥å±‚ï¼Œè¿™æ˜¯æ¥æ”¶å•ä¸ªåˆ†ç±»é¢„æµ‹ç‰¹å¾one-hotç¼–ç çš„ç¬¬ä¸€å±‚ã€‚
- en: Walkthrough the Network
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç½‘ç»œæ¦‚è¿°
- en: Now we are ready to walkthough the artificial neural network.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½æ¦‚è¿°äººå·¥ç¥ç»ç½‘ç»œã€‚
- en: we follow a single path to illustrate the precise calculations associated with
    making a prediction with an artificial neural network
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éµå¾ªå•ä¸€è·¯å¾„æ¥å±•ç¤ºä¸ä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹ç›¸å…³çš„ç²¾ç¡®è®¡ç®—
- en: The full forward pass is explained next.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹æ–‡å°†è§£é‡Šå®Œæ•´çš„æ­£å‘ä¼ é€’ã€‚
- en: '**Inside an Input Layer Node** - input layer nodes just pass the predictor
    features,'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨è¾“å…¥å±‚èŠ‚ç‚¹å†…éƒ¨** - è¾“å…¥å±‚èŠ‚ç‚¹ä»…ä¼ é€’é¢„æµ‹ç‰¹å¾ï¼Œ'
- en: normalized continuous predictor feature value
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½’ä¸€åŒ–çš„è¿ç»­é¢„æµ‹ç‰¹å¾å€¼
- en: a single one-hot-encoding value [0 or 1] for categorical prediction features
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•ä¸ªone-hotç¼–ç å€¼[0æˆ–1]ç”¨äºåˆ†ç±»é¢„æµ‹ç‰¹å¾
- en: into the hidden layer nodes, with general vector notation,
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¾“å…¥ä¼ é€’åˆ°éšè—å±‚èŠ‚ç‚¹ï¼Œä½¿ç”¨é€šç”¨å‘é‡è¡¨ç¤ºï¼Œ
- en: \[ I = X \]![](../Images/4e1c6258eff75f9d4ce767b39cd522fb.png)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \[ I = X \]![](../Images/4e1c6258eff75f9d4ce767b39cd522fb.png)
- en: Walkthrough of an artificial neural network, the input layer node receives one-hot-encoding
    of a single categorical predictor feature and passes it to all of the hidden layer
    nodes.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œçš„æ¦‚è¿°ï¼Œè¾“å…¥å±‚èŠ‚ç‚¹æ¥æ”¶å•ä¸ªåˆ†ç±»é¢„æµ‹ç‰¹å¾çš„one-hotç¼–ç å¹¶å°†å…¶ä¼ é€’ç»™æ‰€æœ‰éšè—å±‚èŠ‚ç‚¹ã€‚
- en: We can generalize over all input layer nodes with,
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼æ³›åŒ–æ‰€æœ‰è¾“å…¥å±‚èŠ‚ç‚¹ï¼Œ
- en: \[ H_j = H_{j_{in}} = X_j \]
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_j = H_{j_{in}} = X_j \]
- en: '**Inside an Hidden Layer Node**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨éšè—å±‚èŠ‚ç‚¹å†…éƒ¨**'
- en: The hidden layer nodes are simple processors. The take linearly weighted combinations
    of inputs, add a node bias term and then nonlinearly transform the result, this
    transform is call the activation function, \(\alpha\).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: éšè—å±‚èŠ‚ç‚¹æ˜¯ç®€å•çš„å¤„ç†å™¨ã€‚å®ƒä»¬å¯¹è¾“å…¥è¿›è¡Œçº¿æ€§åŠ æƒçš„ç»„åˆï¼Œæ·»åŠ ä¸€ä¸ªèŠ‚ç‚¹åç½®é¡¹ï¼Œç„¶åå°†ç»“æœè¿›è¡Œéçº¿æ€§è½¬æ¢ï¼Œè¿™ç§è½¬æ¢ç§°ä¸ºæ¿€æ´»å‡½æ•°ï¼Œ\(\alpha\)ã€‚
- en: indeed, a very simple processor!
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¡®å®ï¼Œä¸€ä¸ªéå¸¸ç®€å•çš„å¤„ç†å™¨ï¼
- en: through many interconnected nodes we gain a very flexible predictor, emergent
    ability to characterize complicated, nonlinear patterns.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡è®¸å¤šç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ï¼Œæˆ‘ä»¬è·å¾—ä¸€ä¸ªéå¸¸çµæ´»çš„é¢„æµ‹å™¨ï¼Œèƒ½å¤Ÿè¡¨å¾å¤æ‚ã€éçº¿æ€§çš„æ¨¡å¼ã€‚
- en: Prior to activation we have,
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¿€æ´»ä¹‹å‰ï¼Œæˆ‘ä»¬æœ‰ï¼Œ
- en: \[ H_{4_{in}} = \sum_{i=1}^{3} \left( \lambda_{i,4} \cdot I_i \right) + b_4
    \]
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4_{in}} = \sum_{i=1}^{3} \left( \lambda_{i,4} \cdot I_i \right) + b_4
    \]
- en: and after activation we have,
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¿€æ´»ä¹‹åï¼Œæˆ‘ä»¬æœ‰ï¼Œ
- en: \[ H_4 = \alpha \left(H_{4_{in}} \right) \]
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_4 = \alpha \left(H_{4_{in}} \right) \]
- en: We can express the simple processor in the node with general vector notation
    as,
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨é€šç”¨å‘é‡è¡¨ç¤ºæ³•åœ¨èŠ‚ç‚¹ä¸­è¡¨è¾¾è¿™ä¸ªç®€å•çš„å¤„ç†å™¨ï¼Œ
- en: \[ H_4 = \alpha \left(b_4 + \lambda_{j,4}^T I \right) \]![](../Images/b2f8e46ea497049f4b95c03b8812eea7.png)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_4 = \alpha \left(b_4 + \lambda_{j,4}^T I \right) \]![](../Images/b2f8e46ea497049f4b95c03b8812eea7.png)
- en: Walkthrough of an artificial neural network, the hidden layer linearly weights
    the input from each input layer node, adds a node bias term and then applies an
    activation function and passes this to all nodes in the next layer, i.e., the
    output layer for our example artificial neural network.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œçš„æ¦‚è¿°ï¼Œéšè—å±‚çº¿æ€§åŠ æƒè¾“å…¥æ¥è‡ªæ¯ä¸ªè¾“å…¥å±‚èŠ‚ç‚¹ï¼Œæ·»åŠ ä¸€ä¸ªèŠ‚ç‚¹åç½®é¡¹ï¼Œç„¶ååº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™ä¸‹ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹ï¼Œå³æˆ‘ä»¬çš„ç¤ºä¾‹äººå·¥ç¥ç»ç½‘ç»œçš„è¾“å‡ºå±‚ã€‚
- en: We can generalize over all hidden layer nodes with,
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼æ¨å¹¿æ‰€æœ‰éšè—å±‚èŠ‚ç‚¹ï¼Œ
- en: \[ H_{j_{in}} = \sum_{i=1}^{|I|} \left( \lambda_{i,j} \cdot I_i \right) + b_j
    \]
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{j_{in}} = \sum_{i=1}^{|I|} \left( \lambda_{i,j} \cdot I_i \right) + b_j
    \]
- en: and after activation, the node output is,
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»åï¼ŒèŠ‚ç‚¹è¾“å‡ºä¸ºï¼Œ
- en: \[ H_{j_{in}} = \sigma \bigl( \sum_{i=1}^{|I|} \left( \lambda_{i,j} \cdot I_i
    \right) + b_j \bigr) \]
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{j_{in}} = \sigma \bigl( \sum_{i=1}^{|I|} \left( \lambda_{i,j} \cdot I_i
    \right) + b_j \bigr) \]
- en: '**Inside an Output Layer Node**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡ºå±‚èŠ‚ç‚¹å†…éƒ¨**'
- en: The output layer nodes take linearly weighted combinations of nodesâ€™ inputs,
    adds a node bias term and then transforms the result with an activation function,
    \(\alpha\), same as the hidden layer nodes,
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå±‚èŠ‚ç‚¹é‡‡ç”¨èŠ‚ç‚¹è¾“å…¥çš„çº¿æ€§åŠ æƒç»„åˆï¼Œæ·»åŠ ä¸€ä¸ªèŠ‚ç‚¹åç½®é¡¹ï¼Œç„¶åä½¿ç”¨ä¸éšè—å±‚èŠ‚ç‚¹ç›¸åŒçš„æ¿€æ´»å‡½æ•°\(\alpha\)è¿›è¡Œè½¬æ¢ï¼Œ
- en: Prior to activation we have,
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¿€æ´»ä¹‹å‰ï¼Œ
- en: \[ O_{6_{in}} = \sum_{i=4}^{5} \left( \lambda_{i,6} \cdot H_i \right) + b_6
    \]
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{in}} = \sum_{i=4}^{5} \left( \lambda_{i,6} \cdot H_i \right) + b_6
    \]
- en: and after activation, assuming identity activation we have,
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»åï¼Œå‡è®¾æ’ç­‰æ¿€æ´»ï¼Œæˆ‘ä»¬æœ‰ï¼Œ
- en: \[ O_6 = \alpha \left(O_{6_{in}} \right) \]
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \alpha \left(O_{6_{in}} \right) \]
- en: We can express the simple processor in the node with general vector notation
    as,
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨é€šç”¨å‘é‡ç¬¦å·è¡¨ç¤ºèŠ‚ç‚¹ä¸­çš„ç®€å•å¤„ç†å™¨ï¼Œ
- en: \[ O_6 = \alpha\left(b_6 + \lambda_{j,6}^T H\right) \]![](../Images/22c5dd7e545497f42cf0b949f0b92954.png)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \alpha\left(b_6 + \lambda_{j,6}^T H\right) \]![](../Images/22c5dd7e545497f42cf0b949f0b92954.png)
- en: Walkthrough of an artificial neural network, the output layer linearly weights
    the input from each hidden layer node, adds a node bias term and then applies
    an activation function, typically linear for continuous response features and
    passes this as an output.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œçš„æ¦‚è¿°ï¼Œè¾“å‡ºå±‚çº¿æ€§åŠ æƒè¾“å…¥æ¥è‡ªæ¯ä¸ªéšè—å±‚èŠ‚ç‚¹ï¼Œæ·»åŠ ä¸€ä¸ªèŠ‚ç‚¹åç½®é¡¹ï¼Œç„¶ååº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸å¯¹äºè¿ç»­å“åº”ç‰¹å¾æ˜¯çº¿æ€§çš„ï¼Œå¹¶å°†å…¶ä½œä¸ºè¾“å‡ºä¼ é€’ã€‚
- en: and for categorical response features, softmax activation is commonly applied,
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†ç±»å“åº”ç‰¹å¾ï¼Œé€šå¸¸åº”ç”¨softmaxæ¿€æ´»ï¼Œ
- en: \[ O_j = \alpha(O_{j_{in}}) = \frac{e^{O_{j_{in}}}}{\sum_{\iota=1}^{K} e^{O_{\iota_{in}}}}
    \]![](../Images/ff80ab0dac26f531ce5cc5ce3cd9cf5d.png)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_j = \alpha(O_{j_{in}}) = \frac{e^{O_{j_{in}}}}{\sum_{\iota=1}^{K} e^{O_{\iota_{in}}}}
    \]![](../Images/ff80ab0dac26f531ce5cc5ce3cd9cf5d.png)
- en: Walkthrough of an artificial neural network, the output layer linearly weights
    the input from each hidden layer node, adds a node bias term and then applies
    an activation function, typically linear for continuous response features and
    passes this as an output.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œçš„æ¦‚è¿°ï¼Œè¾“å‡ºå±‚çº¿æ€§åŠ æƒè¾“å…¥æ¥è‡ªæ¯ä¸ªéšè—å±‚èŠ‚ç‚¹ï¼Œæ·»åŠ ä¸€ä¸ªèŠ‚ç‚¹åç½®é¡¹ï¼Œç„¶ååº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸å¯¹äºè¿ç»­å“åº”ç‰¹å¾æ˜¯çº¿æ€§çš„ï¼Œå¹¶å°†å…¶ä½œä¸ºè¾“å‡ºä¼ é€’ã€‚
- en: softmax activation ensures that the output over all the output layer nodes are
    valid probabilities including,
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: softmaxæ¿€æ´»ç¡®ä¿æ‰€æœ‰è¾“å‡ºå±‚èŠ‚ç‚¹çš„è¾“å‡ºéƒ½æ˜¯æœ‰æ•ˆçš„æ¦‚ç‡ï¼ŒåŒ…æ‹¬ï¼Œ
- en: '**nonnegativity** - through the exponentiation'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éè´Ÿæ€§** - é€šè¿‡æŒ‡æ•°è¿ç®—'
- en: '**closure** - probabilities sum to 1.0 through the denominator normalizing
    the result'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°é—­** - é€šè¿‡åˆ†æ¯å½’ä¸€åŒ–ç»“æœï¼Œæ¦‚ç‡ä¹‹å’Œä¸º1.0'
- en: Note, for all future discussions and demonstrations, I assume a standardized
    continuous responce feature.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå¯¹äºæ‰€æœ‰æœªæ¥çš„è®¨è®ºå’Œæ¼”ç¤ºï¼Œæˆ‘å‡è®¾ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¿ç»­å“åº”ç‰¹å¾ã€‚
- en: Network Forward Pass
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç½‘ç»œæ­£å‘ä¼ é€’
- en: Now that we have completed a walk-through of our network on a single path, letâ€™s
    combine all the paths through our network to demonstrate a complete forward pass
    through our artificial neural network.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†ä¸€æ¡è·¯å¾„ä¸Šç½‘ç»œçš„æ¦‚è¿°ï¼Œè®©æˆ‘ä»¬ç»“åˆé€šè¿‡æˆ‘ä»¬ç½‘ç»œçš„å…¨éƒ¨è·¯å¾„æ¥æ¼”ç¤ºäººå·¥ç¥ç»ç½‘ç»œçš„ä¸€æ¬¡å®Œæ•´æ­£å‘ä¼ é€’ã€‚
- en: this is the calculation required to make a prediction with out,
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è¿›è¡Œé¢„æµ‹æ‰€éœ€çš„è®¡ç®—ï¼Œä¸ä½¿ç”¨outï¼Œ
- en: \[ O_6 = \sigma_{O_6} \bigl( \lambda_{4,6} \cdot \sigma_{H_4} \left( \lambda_{1,4}
    I_1 + \lambda_{2,4} I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot
    \sigma_{H_5} \left( \lambda_{1,5} I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3
    + b_5 \right) + b_6 \bigr) \]
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \sigma_{O_6} \bigl( \lambda_{4,6} \cdot \sigma_{H_4} \left( \lambda_{1,4}
    I_1 + \lambda_{2,4} I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot
    \sigma_{H_5} \left( \lambda_{1,5} I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3
    + b_5 \right) + b_6 \bigr) \]
- en: where the activation functions \(\sigma_{H_4}\) = \(\sigma_{H_5}\) = \(\sigma\)
    are sigmoid, and \(\sigma_{O_6}\) is linear (identity), so we could simplify the
    forward pass to,
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ¿€æ´»å‡½æ•° \(\sigma_{H_4}\) = \(\sigma_{H_5}\) = \(\sigma\) æ˜¯Sigmoidï¼Œè€Œ \(\sigma_{O_6}\)
    æ˜¯çº¿æ€§ï¼ˆæ’ç­‰ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç®€åŒ–å‰å‘ä¼ æ’­ä¸ºï¼Œ
- en: \[ O_6 = \lambda_{4,6} \cdot \sigma \left( \lambda_{1,4} I_1 + \lambda_{2,4}
    I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot \sigma \left( \lambda_{1,5}
    I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3 + b_5 \right) + b_6 \]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \lambda_{4,6} \cdot \sigma \left( \lambda_{1,4} I_1 + \lambda_{2,4}
    I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot \sigma \left( \lambda_{1,5}
    I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3 + b_5 \right) + b_6 \]
- en: This emphasizes that our neural network is a nested set of activated linear
    systems, i.e., linearly weighted averages plus bias terms applied to activation
    functions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¼ºè°ƒäº†æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ªåµŒå¥—çš„æ¿€æ´»çº¿æ€§ç³»ç»Ÿé›†åˆï¼Œå³çº¿æ€§åŠ æƒçš„å¹³å‡å€¼åŠ ä¸Šåº”ç”¨äºæ¿€æ´»å‡½æ•°çš„åå·®é¡¹ã€‚
- en: Number of Model Parameters
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°æ•°é‡
- en: In general, there are many model parameters, \(theta\), in an artificial neural
    network. First, letâ€™s clarify these definitions to describe our artificial neural
    network,
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸æƒ…å†µä¸‹ï¼Œäººå·¥ç¥ç»ç½‘ç»œä¸­æœ‰è®¸å¤šæ¨¡å‹å‚æ•°ï¼Œ\(theta\)ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ˜ç¡®è¿™äº›å®šä¹‰æ¥æè¿°æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œï¼Œ
- en: '**neural network width** - the number of nodes in the layers of the neural
    network'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¥ç»ç½‘ç»œå®½åº¦** - ç¥ç»ç½‘ç»œå±‚çš„èŠ‚ç‚¹æ•°'
- en: '**neural network depth** - the number of layers in the neural network, typically
    the input layer is not included in this calculation'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¥ç»ç½‘ç»œæ·±åº¦** - ç¥ç»ç½‘ç»œä¸­çš„å±‚æ•°ï¼Œé€šå¸¸è¾“å…¥å±‚ä¸åŒ…æ‹¬åœ¨è¿™ä¸ªè®¡ç®—ä¸­'
- en: Now, letâ€™s assume the following compact notation for a 3 layer artificial neural
    network, input, output and 1 hidden layer, with the width of each layer as,
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å‡è®¾ä¸€ä¸ªç´§å‡‘çš„è¡¨ç¤ºæ³•ç”¨äºä¸€ä¸ª3å±‚äººå·¥ç¥ç»ç½‘ç»œï¼Œè¾“å…¥ã€è¾“å‡ºå’Œ1ä¸ªéšè—å±‚ï¼Œæ¯å±‚çš„å®½åº¦ä¸ºï¼Œ
- en: number of input nodes, \(p\)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥èŠ‚ç‚¹æ•° \(p\)
- en: number of hidden layer nodes, \(m\)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éšè—å±‚èŠ‚ç‚¹æ•° \(m\)
- en: and number of output nodes, \(k\)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å’Œè¾“å‡ºèŠ‚ç‚¹æ•° \(k\)
- en: '![](../Images/1b5b7051a7f760fb3b71c71560ca7613.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b5b7051a7f760fb3b71c71560ca7613.png)'
- en: Notation for artificial neural network width, number of input nodes, \(p\),
    number of hidden layer nodes, \(m\), and number of output nodes, \(k\).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œçš„å®½åº¦è¡¨ç¤ºæ³•ï¼Œè¾“å…¥èŠ‚ç‚¹æ•° \(p\)ï¼Œéšè—å±‚èŠ‚ç‚¹æ•° \(m\)ï¼Œå’Œè¾“å‡ºèŠ‚ç‚¹æ•° \(k\)ã€‚
- en: fully connected, so for every connection there is a weight,
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: å…¨è¿æ¥ï¼Œå› æ­¤å¯¹äºæ¯ä¸ªè¿æ¥éƒ½æœ‰ä¸€ä¸ªæƒé‡ï¼Œ
- en: \[ \lambda_{ğ¼_{1,\ldots,ğ‘},ğ»_{1,\ldots,ğ‘š} } \quad \text{and} \quad \lambda_{ğ»_{1,\ldots,ğ‘š},ğ‘‚_{1,\ldots,ğ‘˜}
    } \]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{ğ¼_{1,\ldots,ğ‘},ğ»_{1,\ldots,ğ‘š} } \quad \text{å’Œ} \quad \lambda_{ğ»_{1,\ldots,ğ‘š},ğ‘‚_{1,\ldots,ğ‘˜}
    } \]
- en: with full connectivity the number of weights is
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…¨è¿æ¥çš„æƒ…å†µä¸‹ï¼Œæƒé‡çš„æ•°é‡æ˜¯
- en: \[ ğ‘ \times ğ‘š \quad \text{and} \quad ğ‘š \times ğ‘˜ \]
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: \[ ğ‘ \times ğ‘š \quad \text{å’Œ} \quad ğ‘š \times ğ‘˜ \]
- en: and at each hidden layer node there is a bias term,
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”åœ¨æ¯ä¸ªéšè—å±‚èŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªåå·®é¡¹ï¼Œ
- en: \[ ğ‘_{H_{1,\ldots,m} } \]
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: \[ ğ‘_{H_{1,\ldots,m} } \]
- en: and at every output node there is a bias term,
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”åœ¨æ¯ä¸ªè¾“å‡ºèŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªåå·®é¡¹ï¼Œ
- en: \[ ğ‘_{O_{1,\ldots,k} } \]
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: \[ ğ‘_{O_{1,\ldots,k} } \]
- en: Therefore, the number of model parameters is,
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ¨¡å‹å‚æ•°çš„æ•°é‡æ˜¯ï¼Œ
- en: \[ |\theta| = ğ‘ \times ğ‘š + ğ‘š \times ğ‘˜ + ğ‘š + ğ‘˜ \]
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta| = ğ‘ \times ğ‘š + ğ‘š \times ğ‘˜ + ğ‘š + ğ‘˜ \]
- en: this assumes an unique bias term at each hidden layer node and output layer
    node, but in some case the same bias term may be applied over the entire layer.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å‡è®¾æ¯ä¸ªéšè—å±‚èŠ‚ç‚¹å’Œè¾“å‡ºå±‚èŠ‚ç‚¹æœ‰ä¸€ä¸ªå”¯ä¸€çš„åå·®é¡¹ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç›¸åŒçš„åå·®é¡¹å¯èƒ½åº”ç”¨äºæ•´ä¸ªå±‚ã€‚
- en: For our example, with \(p = 3\), \(m = 2\) and \(k = 1\), then the number of
    model parameters are,
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„ä¾‹å­ï¼Œ\(p = 3\)ï¼Œ\(m = 2\) å’Œ \(k = 1\)ï¼Œé‚£ä¹ˆæ¨¡å‹å‚æ•°çš„æ•°é‡æ˜¯ï¼Œ
- en: \[ |\theta| = ğ‘ \times ğ‘š + ğ‘š \times ğ‘˜ + ğ‘š + ğ‘˜ \]
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta| = ğ‘ \times ğ‘š + ğ‘š \times ğ‘˜ + ğ‘š + ğ‘˜ \]
- en: after substitution we have,
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£å…¥åæˆ‘ä»¬å¾—åˆ°ï¼Œ
- en: \[ |\theta| = 3 \times 2 + 2 \times 1 + 2 + 1 = 11 \]
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta| = 3 \times 2 + 2 \times 1 + 2 + 1 = 11 \]
- en: I select this as a manageable number of parameters, so we can train and visualize
    our model, but consider a more typical model size by increasing our artificial
    neural networkâ€™s width, with \(p = 10\), \(m = 20\) and \(k = 3\), then we have
    many more model parameters,
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘é€‰æ‹©è¿™ä¸ªä½œä¸ºå¯ç®¡ç†çš„å‚æ•°æ•°é‡ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥è®­ç»ƒå’Œå¯è§†åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä½†é€šè¿‡å¢åŠ æˆ‘ä»¬äººå·¥ç¥ç»ç½‘ç»œçš„å®½åº¦ï¼Œè€ƒè™‘ä¸€ä¸ªæ›´å…¸å‹çš„æ¨¡å‹å¤§å°ï¼Œå³ \(p = 10\)ï¼Œ\(m
    = 20\) å’Œ \(k = 3\)ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†æœ‰æ›´å¤šçš„æ¨¡å‹å‚æ•°ï¼Œ
- en: \[ |\theta| = 10 \times 20 + 20 \times 3 + 20 + 3 = 283 \]
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta| = 10 \times 20 + 20 \times 3 + 20 + 3 = 283 \]
- en: If we add hidden layers, increase our artificial neural networkâ€™s depth, the
    number of model parameters will grow very quickly.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ·»åŠ éšè—å±‚ï¼Œå¢åŠ æˆ‘ä»¬äººå·¥ç¥ç»ç½‘ç»œçš„æ·±åº¦ï¼Œæ¨¡å‹å‚æ•°çš„æ•°é‡å°†è¿…é€Ÿå¢é•¿ã€‚
- en: we can generalize this calculation for any fully connected, feed forward neural
    network, given a \(W\) vector with the number of nodes, i.e., the width of each
    layer,
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æ¨å¹¿è¿™ä¸ªè®¡ç®—åˆ°ä»»ä½•å…¨è¿æ¥çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç»™å®šä¸€ä¸ª \(W\) å‘é‡ï¼ŒåŒ…å«èŠ‚ç‚¹çš„æ•°é‡ï¼Œå³æ¯å±‚çš„å®½åº¦ï¼Œ
- en: \[ \mathbf{L} = [l_0, l_1, l_2, \dots, l_n] \]
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{L} = [l_0, l_1, l_2, \dots, l_n] \]
- en: where,
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ
- en: \(l_0\) is the number of input neurons
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(l_0\) æ˜¯è¾“å…¥ç¥ç»å…ƒçš„æ•°é‡
- en: \(l_1, \dots, l_{n-1}\) are the widths of the hidden layers
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(l_1, \dots, l_{n-1}\) æ˜¯éšè—å±‚çš„å®½åº¦
- en: \(l_n\) is the number of output neurons
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(l_n\) æ˜¯è¾“å‡ºç¥ç»å…ƒçš„æ•°é‡
- en: The total number of connection weights is,
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: è¿æ¥æƒé‡çš„æ€»æ•°æ˜¯ï¼Œ
- en: \[ |\theta_{weights}| = \sum_{i=1}^{n} l_i \cdot l_{i-1} \]
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta_{weights}| = \sum_{i=1}^{n} l_i \cdot l_{i-1} \]
- en: the total number of node biases (there are not bias parameters in the input
    layer nodes, \(l_0\)) is,
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹åç½®çš„æ€»æ•°ï¼ˆè¾“å…¥å±‚èŠ‚ç‚¹ä¸­æ²¡æœ‰åç½®å‚æ•°ï¼Œ\(l_0\)ï¼‰æ˜¯ï¼Œ
- en: \[ |\theta_{biases}| = \sum_{i=1}^{n} l_i \]
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta_{biases}| = \sum_{i=1}^{n} l_i \]
- en: the total number of trainable model parameters, connectioned weights and node
    biases, is,
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è®­ç»ƒæ¨¡å‹å‚æ•°ã€è¿æ¥æƒé‡å’ŒèŠ‚ç‚¹åç½®çš„æ€»æ•°æ˜¯ï¼Œ
- en: \[ |\theta| = \sum_{i=1}^{n} \left( l_i \cdot l_{i-1} + l_i \right) = \sum_{i=1}^{n}
    l_i \cdot (l_{i-1} + 1) \]
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta| = \sum_{i=1}^{n} \left( l_i \cdot l_{i-1} + l_i \right) = \sum_{i=1}^{n}
    l_i \cdot (l_{i-1} + 1) \]
- en: Letâ€™s take an example of artificial neural network with 4 hidden layers, with
    network width by-layer vector of,
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ä¸ªå…·æœ‰4ä¸ªéšè—å±‚çš„äººå·¥ç¥ç»ç½‘ç»œç¤ºä¾‹ï¼Œç½‘ç»œå®½åº¦æŒ‰å±‚å‘é‡è¡¨ç¤ºï¼Œ
- en: \[ \mathbf{L} = [10, 8, 6, 4, 2, 1] \]
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{L} = [10, 8, 6, 4, 2, 1] \]
- en: The total number of connection weights is,
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: è¿æ¥æƒé‡çš„æ€»æ•°æ˜¯ï¼Œ
- en: \[ |\theta_{L_{weights}}| = \sum_{i=1}^{5} l_i \cdot l_{i-1} = (8 \cdot 10)
    + (6 \cdot 8) + (4 \cdot 6) + (2 \cdot 4) + (1 \cdot 2) = 80 + 48 + 24 + 8 + 2
    = 162 \]
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta_{L_{weights}}| = \sum_{i=1}^{5} l_i \cdot l_{i-1} = (8 \cdot 10)
    + (6 \cdot 8) + (4 \cdot 6) + (2 \cdot 4) + (1 \cdot 2) = 80 + 48 + 24 + 8 + 2
    = 162 \]
- en: and the total number of node biases is,
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹åç½®çš„æ€»æ•°æ˜¯ï¼Œ
- en: \[ |\theta_{L_{biases}}| = \sum_{i=1}^{5} l_i = 8 + 6 + 4 + 2 + 1 = 21 \]
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta_{L_{biases}}| = \sum_{i=1}^{5} l_i = 8 + 6 + 4 + 2 + 1 = 21 \]
- en: and finally the total nuber of trainable parameters is,
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¯è®­ç»ƒå‚æ•°çš„æ€»æ•°æ˜¯ï¼Œ
- en: \[ |\theta_L| = \sum_{i=1}^{5} l_i \cdot (l_{i-1} + 1) = (8 \cdot 11) + (6 \cdot
    9) + (4 \cdot 7) + (2 \cdot 5) + (1 \cdot 3) = 88 + 54 + 28 + 10 + 3 = 183 \]
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta_L| = \sum_{i=1}^{5} l_i \cdot (l_{i-1} + 1) = (8 \cdot 11) + (6 \cdot
    9) + (4 \cdot 7) + (2 \cdot 5) + (1 \cdot 3) = 88 + 54 + 28 + 10 + 3 = 183 \]
- en: Activation Functions
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°
- en: The activation function is a transformation of the linear combination of the
    weighted node inputs plus the node bias term. Nonlinear activation,
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°æ˜¯åŠ æƒèŠ‚ç‚¹è¾“å…¥çš„çº¿æ€§ç»„åˆåŠ ä¸ŠèŠ‚ç‚¹åç½®é¡¹çš„è½¬æ¢ã€‚éçº¿æ€§æ¿€æ´»ï¼Œ
- en: introduces non-linear properties, and complexity to the network
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼•å…¥ç½‘ç»œçš„éçº¿æ€§å±æ€§å’Œå¤æ‚æ€§
- en: prevents the network from collapsing
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é˜²æ­¢ç½‘ç»œå´©æºƒ
- en: Without the nonlinear activation function we would have linear regression, the
    entire system collapses.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œæˆ‘ä»¬å°†å¾—åˆ°çº¿æ€§å›å½’ï¼Œæ•´ä¸ªç³»ç»Ÿä¼šå´©æºƒã€‚
- en: For more information about activation functions and a demonstration of the collapse
    without nonlinear activation to multilinear regression see the associated chapter
    in this e-book, [Neural Network Activation Functions](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_activation_functions.html).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæ¿€æ´»å‡½æ•°çš„æ›´å¤šä¿¡æ¯ä»¥åŠéçº¿æ€§æ¿€æ´»ç¼ºå¤±æ—¶å¯¼è‡´çš„å¤šçº¿æ€§å›å½’å´©æºƒçš„æ¼”ç¤ºï¼Œè¯·å‚é˜…æœ¬ç”µå­ä¹¦çš„ç›¸å…³ç« èŠ‚ï¼Œ[ç¥ç»ç½‘ç»œæ¿€æ´»å‡½æ•°](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_activation_functions.html)ã€‚
- en: Training Networks Steps
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒç½‘ç»œæ­¥éª¤
- en: Training an artificial neural network proceeds iteratively by these steps,
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒäººå·¥ç¥ç»ç½‘ç»œé€šè¿‡ä»¥ä¸‹æ­¥éª¤è¿­ä»£è¿›è¡Œï¼Œ
- en: initialized the model parameters
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–æ¨¡å‹å‚æ•°
- en: forward pass to make a prediction
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‰å‘ä¼ é€’ä»¥è¿›è¡Œé¢„æµ‹
- en: calculate the error derivative based on the prediction and truth over training
    data
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ¹æ®é¢„æµ‹å’ŒçœŸå®å€¼åœ¨è®­ç»ƒæ•°æ®ä¸Šè®¡ç®—è¯¯å·®å¯¼æ•°
- en: backpropagate the error derivative back through the artificial neural network
    to calculate the derivatives of the error over all the model weights and biases
    parameters
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œåå‘ä¼ æ’­è¯¯å·®å¯¼æ•°ï¼Œä»¥è®¡ç®—æ‰€æœ‰æ¨¡å‹æƒé‡å’Œåç½®å‚æ•°çš„è¯¯å·®å¯¼æ•°
- en: update the model parameters based on the derivatives and learning rates
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ¹æ®å¯¼æ•°å’Œå­¦ä¹ ç‡æ›´æ–°æ¨¡å‹å‚æ•°
- en: repeat until convergence.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡å¤ç›´åˆ°æ”¶æ•›ã€‚
- en: '![](../Images/c3a5bc8956f8ceda05ddf9b582cd141d.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3a5bc8956f8ceda05ddf9b582cd141d.png)'
- en: The iterative steps for training an artificial neural network.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒäººå·¥ç¥ç»ç½‘ç»œçš„è¿­ä»£æ­¥éª¤ã€‚
- en: Hereâ€™s some details on each step,
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯æ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯ï¼Œ
- en: '**Initializing the Model Parameters** - initialize all model parameters with
    typically small (near zero) random values. Hereâ€™s a couple common methods,'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åˆå§‹åŒ–æ¨¡å‹å‚æ•°** - é€šå¸¸ä½¿ç”¨æ¥è¿‘é›¶çš„å°éšæœºå€¼åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹å‚æ•°ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§æ–¹æ³•ï¼Œ'
- en: '**Xavier Weight Initialization** - random realizations from uniform distributions
    specified by \(U[\text{min}, \text{max}]\),'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Xavier æƒé‡åˆå§‹åŒ–** - ä»ç”± \(U[\text{min}, \text{max}]\) æŒ‡å®šçš„å‡åŒ€åˆ†å¸ƒä¸­éšæœºå®ç°ï¼Œ'
- en: \[ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}}, \frac{1}{\sqrt{p}} \right]
    (p^\ell) \]
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}}, \frac{1}{\sqrt{p}} \right]
    (p^\ell) \]
- en: where \(F^{-1}_U\) is the inverse of the CDF, \(p\) is the number of inputs,
    and \(p^{\ell}\) is a random cumulative probability value drawn from the uniform
    distribution, \(U[0,1]\).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ \(F^{-1}_U\) æ˜¯CDFçš„é€†ï¼Œ\(p\) æ˜¯è¾“å…¥çš„æ•°é‡ï¼Œ\(p^{\ell}\) æ˜¯ä»å‡åŒ€åˆ†å¸ƒ \(U[0,1]\) ä¸­æŠ½å–çš„éšæœºç´¯ç§¯æ¦‚ç‡å€¼ã€‚
- en: '**Normalized Xavier Weight Initialization** - random realizations from uniform
    distributions specified by \(U[\text{min}, \text{max}]\),'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å½’ä¸€åŒ–Xavieræƒé‡åˆå§‹åŒ–** - ä»ç”± \(U[\text{min}, \text{max}]\) æŒ‡å®šçš„å‡åŒ€åˆ†å¸ƒä¸­éšæœºå–å€¼ï¼Œ'
- en: \[ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k}
    \right] (p^\ell) \]
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k}
    \right] (p^\ell) \]
- en: where \(F^{-1}_U\) is the inverse of the CDF, \(p\) is the number of inputs,
    \(k\) is the number of outputs, and \(p^{\ell}\) is a random cumulative probability
    value drawn from the uniform distribution, \(U[0,1]\).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ \(F^{-1}_U\) æ˜¯CDFçš„é€†ï¼Œ\(p\) æ˜¯è¾“å…¥çš„æ•°é‡ï¼Œ\(k\) æ˜¯è¾“å‡ºçš„æ•°é‡ï¼Œ\(p^{\ell}\) æ˜¯ä»å‡åŒ€åˆ†å¸ƒ \(U[0,1]\)
    ä¸­æŠ½å–çš„éšæœºç´¯ç§¯æ¦‚ç‡å€¼ã€‚
- en: For example, if we return to our first hidden layer node,
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªéšè—å±‚èŠ‚ç‚¹ï¼Œ
- en: '![](../Images/b2f8e46ea497049f4b95c03b8812eea7.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2f8e46ea497049f4b95c03b8812eea7.png)'
- en: First hidden layer node with 3 inputs, and 1 output.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªéšè—å±‚èŠ‚ç‚¹æœ‰3ä¸ªè¾“å…¥ï¼Œ1ä¸ªè¾“å‡ºã€‚
- en: we have \(p = 3\) and \(k = 1\), and we draw from the uniform distribution,
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ \(p = 3\) å’Œ \(k = 1\)ï¼Œå¹¶ä»å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–ï¼Œ
- en: \[ U \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] = U \left[ \frac{-1}{\sqrt{3}+1},
    \frac{1}{\sqrt{3}+1} \right] \]
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] = U \left[ \frac{-1}{\sqrt{3}+1},
    \frac{1}{\sqrt{3}+1} \right] \]
- en: '**Forward Pass** - to make a prediction, \(\hat{y}\). Initial predictions will
    be random for the first iteration, but will improve over iterations. Once again
    for our model the forward pass is,'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å‰å‘ä¼ æ’­** - è¿›è¡Œé¢„æµ‹ï¼Œ\(\hat{y}\)ã€‚åˆå§‹é¢„æµ‹åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£å°†æ˜¯éšæœºçš„ï¼Œä½†ä¼šéšç€è¿­ä»£æ¬¡æ•°çš„å¢åŠ è€Œæ”¹è¿›ã€‚å¯¹äºæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå‰å‘ä¼ æ’­æ˜¯ï¼Œ'
- en: \[ O_6 = \lambda_{4,6} \cdot \sigma \left( \lambda_{1,4} I_1 + \lambda_{2,4}
    I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot \sigma \left( \lambda_{1,5}
    I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3 + b_5 \right) + b_6 \]![](../Images/08556ecbd47d143019d0163dc95761cf.png)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \lambda_{4,6} \cdot \sigma \left( \lambda_{1,4} I_1 + \lambda_{2,4}
    I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot \sigma \left( \lambda_{1,5}
    I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3 + b_5 \right) + b_6 \]![](../Images/08556ecbd47d143019d0163dc95761cf.png)
- en: Prediction with our artificial neural network initialized with random model
    parameters, weights and biases.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨éšæœºæ¨¡å‹å‚æ•°ã€æƒé‡å’Œåå·®åˆå§‹åŒ–çš„äººå·¥ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹ã€‚
- en: '**Calculate the Error Derivative** - given a loss of,'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è®¡ç®—è¯¯å·®å¯¼æ•°** - ç»™å®šä¸€ä¸ªæŸå¤±ï¼Œ'
- en: \[ L = \frac{1}{2} \left(\hat{y} - y \right)^2 \]
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L = \frac{1}{2} \left(\hat{y} - y \right)^2 \]
- en: and the error derivative, i.e., rate of change of in error given a change in
    model estimate is,
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠè¯¯å·®å¯¼æ•°ï¼Œå³ç»™å®šæ¨¡å‹ä¼°è®¡çš„å˜åŒ–ï¼Œè¯¯å·®çš„å˜åŒ–ç‡ï¼Œ
- en: \[ \frac{\partial L}{\partial \hat{y}} = \hat{Y} - Y \]
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial \hat{y}} = \hat{Y} - Y \]
- en: For now, letâ€™s only consider a single estimate, and we will address more than
    1 training data later.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬åªè€ƒè™‘å•ä¸ªä¼°è®¡ï¼Œç¨åæˆ‘ä»¬å°†è®¨è®ºè¶…è¿‡1ä¸ªè®­ç»ƒæ•°æ®ã€‚
- en: '**Backpropagate the Error Derivative** - we shift back through the artificial
    neural network to calculate the derivatives of the error over all the model weights
    and biases parameters, with the chain rule, for example the loss derivative backpropagated
    to the output of node \(H_4\),'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åå‘ä¼ æ’­è¯¯å·®å¯¼æ•°** - æˆ‘ä»¬é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œåå‘è®¡ç®—æ‰€æœ‰æ¨¡å‹æƒé‡å’Œåå·®å‚æ•°çš„è¯¯å·®å¯¼æ•°ï¼Œä¾‹å¦‚ï¼Œä¾‹å¦‚æŸå¤±å¯¼æ•°åå‘ä¼ æ’­åˆ°èŠ‚ç‚¹ \(H_4\) çš„è¾“å‡ºï¼Œ'
- en: \[ \frac{\partial L}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    L}{\partial O_6} = \lambda_{4,6} \cdot \bigl( (1 - O_6) \cdot O_6 \bigr) \cdot
    (O_6 - y) \]
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    L}{\partial O_6} = \lambda_{4,6} \cdot \bigl( (1 - O_6) \cdot O_6 \bigr) \cdot
    (O_6 - y) \]
- en: '**Update the Model Parameters** - based on the derivatives, \(\frac{\partial
    L}{\partial \lambda_{i,j}}\) and learning rates, \(\eta\), like this,'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ›´æ–°æ¨¡å‹å‚æ•°** - æ ¹æ®å¯¼æ•° \(\frac{\partial L}{\partial \lambda_{i,j}}\) å’Œå­¦ä¹ ç‡ \(\eta\)ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œ'
- en: \[ \lambda_{i,j}^{\ell} = \lambda_{i,j}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{i,j}} \]
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{i,j}^{\ell} = \lambda_{i,j}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{i,j}} \]
- en: '**Repeat Until Convergence** - return to step 1, until the error, \(L\), is
    reduced to an acceptable level, i.e., model convergence is the condition to stop
    the iterations'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é‡å¤ç›´åˆ°æ”¶æ•›** - è¿”å›æ­¥éª¤ 1ï¼Œç›´åˆ°è¯¯å·® \(L\) é™ä½åˆ°å¯æ¥å—çš„æ°´å¹³ï¼Œå³æ¨¡å‹æ”¶æ•›æ˜¯åœæ­¢è¿­ä»£çš„æ¡ä»¶'
- en: These are the steps, now letâ€™s dive into the details for each, but first letâ€™s
    start with the mathematical framework for backpropagation - the chain rule.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯æ­¥éª¤ï¼Œç°åœ¨è®©æˆ‘ä»¬æ·±å…¥åˆ°æ¯ä¸ªæ­¥éª¤çš„ç»†èŠ‚ï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬ä»åå‘ä¼ æ’­çš„æ•°å­¦æ¡†æ¶â€”â€”é“¾å¼æ³•åˆ™å¼€å§‹ã€‚
- en: The Chain Rule
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é“¾å¼æ³•åˆ™
- en: Upon reflection, it is clear that the forward pass through our artificial neural
    network involves a sequence of nested operations that progressively transform
    the input signals as they propagate from the input nodes, through each layer,
    to the output nodes.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: åæ€åï¼Œå¾ˆæ˜æ˜¾ï¼Œé€šè¿‡æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ é€’æ¶‰åŠä¸€ç³»åˆ—åµŒå¥—æ“ä½œï¼Œè¿™äº›æ“ä½œé€æ­¥å°†è¾“å…¥ä¿¡å·ä»è¾“å…¥èŠ‚ç‚¹ä¼ æ’­åˆ°æ¯ä¸€å±‚ï¼Œæœ€ç»ˆåˆ°è¾¾è¾“å‡ºèŠ‚ç‚¹ã€‚
- en: So we can represent this as a sequence of nested operations,
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤æˆ‘ä»¬å¯ä»¥å°†æ­¤è¡¨ç¤ºä¸ºä¸€ç³»åˆ—åµŒå¥—æ“ä½œï¼Œ
- en: \[ f = f(x) \quad g = g(f) \quad y = h(g) \]
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f = f(x) \quad g = g(f) \quad y = h(g) \]
- en: and now in this form to emphasize the nesting of operations,
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä»¥è¿™ç§å½¢å¼å¼ºè°ƒæ“ä½œçš„åµŒå¥—ï¼Œ
- en: \[ y = h \bigl( g(f(x)) \bigr) \]
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y = h \bigl( g(f(x)) \bigr) \]
- en: By applying the chain rule to the nested functions \(y = h \bigl( g(f(x)) \bigr)\),
    we can solve for \(\frac{\partial y}{\partial x}\) as,
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†é“¾å¼æ³•åˆ™åº”ç”¨äºåµŒå¥—å‡½æ•° \(y = h \bigl( g(f(x)) \bigr)\)ï¼Œæˆ‘ä»¬å¯ä»¥æ±‚è§£ \(\frac{\partial y}{\partial
    x}\) å¦‚ä¸‹ï¼Œ
- en: \[ \frac{\partial y}{\partial x} = \frac{\partial h}{\partial g} \cdot \frac{\partial
    g}{\partial f} \cdot \frac{\partial f}{\partial x} \]
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial y}{\partial x} = \frac{\partial h}{\partial g} \cdot \frac{\partial
    g}{\partial f} \cdot \frac{\partial f}{\partial x} \]
- en: where we chain together the partial derivatives for all the operators to solve
    derivative of the output, \(y\), given the input, \(x\).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æˆ‘ä»¬å°†æ‰€æœ‰ç®—å­çš„åå¯¼æ•°é“¾å¼ç›¸åŠ ï¼Œä»¥æ±‚è§£ç»™å®šè¾“å…¥ \(x\) çš„è¾“å‡º \(y\) çš„å¯¼æ•°ã€‚
- en: we can compute derivatives at any intermediate point in the nested functions,
    for example, stepping backwards one step,
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨åµŒå¥—å‡½æ•°çš„ä»»ä½•ä¸­é—´ç‚¹è®¡ç®—å¯¼æ•°ï¼Œä¾‹å¦‚ï¼Œå‘åèµ°ä¸€æ­¥ï¼Œ
- en: \[ \frac{\partial f}{\partial x} \]
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial f}{\partial x} \]
- en: and now two steps,
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯ä¸¤æ­¥ï¼Œ
- en: \[ \frac{\partial g}{\partial x} = \frac{\partial g}{\partial f} \cdot \frac{\partial
    f}{\partial x} \]
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial g}{\partial x} = \frac{\partial g}{\partial f} \cdot \frac{\partial
    f}{\partial x} \]
- en: and all the way with three steps,
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”ä¸€ç›´ç”¨ä¸‰ä¸ªæ­¥éª¤ï¼Œ
- en: \[ \frac{\partial y}{\partial x} = \frac{\partial h}{\partial g} \cdot \frac{\partial
    g}{\partial f} \cdot \frac{\partial f}{\partial x} \]
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial y}{\partial x} = \frac{\partial h}{\partial g} \cdot \frac{\partial
    g}{\partial f} \cdot \frac{\partial f}{\partial x} \]
- en: This is what we do with backpropagation, but this may be too abstract! Letâ€™s
    move to a very simple feed forward neural network with only these three nodes,
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬é€šè¿‡åå‘ä¼ æ’­æ‰€åšçš„äº‹æƒ…ï¼Œä½†è¿™å¯èƒ½è¿‡äºæŠ½è±¡ï¼è®©æˆ‘ä»¬è½¬å‘ä¸€ä¸ªéå¸¸ç®€å•çš„åªæœ‰è¿™ä¸‰ä¸ªèŠ‚ç‚¹çš„æ­£å‘ä¼ æ’­ç¥ç»ç½‘ç»œï¼Œ
- en: \(I_1\) - input node
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(I_1\) - è¾“å…¥èŠ‚ç‚¹
- en: \(H_2 = h(I_1)\) - hidden layer node, a function of \(I_1\)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(H_2 = h(I_1)\) - éšè—å±‚èŠ‚ç‚¹ï¼Œæ˜¯ \(I_1\) çš„å‡½æ•°
- en: \(O_3 = o(H_2)\) - output node, a function of \(H_2\)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(O_3 = o(H_2)\) - è¾“å‡ºèŠ‚ç‚¹ï¼Œæ˜¯ \(H_2\) çš„å‡½æ•°
- en: this is still intentionally abstract, i.e., without mention of weights and biases,
    to help you develop a mental framework of backpropagation with neural netowrks
    by the chain rule, we will dive into the details immediately after this discussion.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ä»ç„¶æ˜¯æœ‰æ„ä¸ºä¹‹çš„æŠ½è±¡ï¼Œå³æ²¡æœ‰æåŠæƒé‡å’Œåå·®ï¼Œä»¥å¸®åŠ©æ‚¨é€šè¿‡é“¾å¼æ³•åˆ™å‘å±•ç¥ç»ç½‘ç»œåå‘ä¼ æ’­çš„å¿ƒç†æ¡†æ¶ï¼Œæˆ‘ä»¬å°†åœ¨è¿™æ¬¡è®¨è®ºä¹‹åç«‹å³æ·±å…¥ç»†èŠ‚ã€‚
- en: 'The output \(O_3\) depends on the input \(I_1\) through these nested functions:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡º \(O_3\) é€šè¿‡è¿™äº›åµŒå¥—å‡½æ•°ä¾èµ–äºè¾“å…¥ \(I_1\)ï¼š
- en: \[ O_3 = o \bigl( h(I_1) \bigr) \]
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_3 = o \bigl( h(I_1) \bigr) \]
- en: Using the **chain rule**, the gradient of the output with respect to backpropagating
    one step,
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨**é“¾å¼æ³•åˆ™**ï¼Œåå‘ä¼ æ’­ä¸€æ­¥çš„è¾“å‡ºæ¢¯åº¦ï¼Œ
- en: \[ \frac{\partial O_3}{\partial H_2} \]
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_3}{\partial H_2} \]
- en: and with respect to backpropagating two steps,
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠåå‘ä¼ æ’­ä¸¤æ­¥ï¼Œ
- en: \[ \frac{\partial O_3}{\partial I_1} = \frac{\partial H_2}{\partial I_1} \cdot
    \frac{\partial O_3}{\partial H_2} \]
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_3}{\partial I_1} = \frac{\partial H_2}{\partial I_1} \cdot
    \frac{\partial O_3}{\partial H_2} \]
- en: This shows how the gradient **backpropagates** through the network,
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¾ç¤ºäº†æ¢¯åº¦å¦‚ä½•é€šè¿‡ç½‘ç»œ**åå‘ä¼ æ’­**ï¼Œ
- en: \(\frac{\partial O_3}{\partial I_1}\) - is the local gradient at the hidden
    node
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\frac{\partial O_3}{\partial I_1}\) - æ˜¯éšè—èŠ‚ç‚¹çš„å±€éƒ¨æ¢¯åº¦
- en: \(\frac{\partial O_3}{\partial H_2}\) - is the local gradient at the output
    node
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\frac{\partial O_3}{\partial H_2}\) - æ˜¯è¾“å‡ºèŠ‚ç‚¹çš„å±€éƒ¨æ¢¯åº¦
- en: By backpropagation we can calculate the deriviates with respect to all parts
    of the network, how the input node signal \(I_1\), or hidden nodel signal \(H_2\)
    affect the output \(O_3\), \(\frac{\partial O_3}{\partial I_1}\) and \(frac{\partial
    O_3}{\partial H_2}\) respsectively.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åå‘ä¼ æ’­ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ç›¸å¯¹äºç½‘ç»œæ‰€æœ‰éƒ¨åˆ†çš„å¯¼æ•°ï¼Œè¾“å…¥èŠ‚ç‚¹ä¿¡å· \(I_1\) æˆ–éšè—èŠ‚ç‚¹ä¿¡å· \(H_2\) å¦‚ä½•å½±å“è¾“å‡º \(O_3\)ï¼Œåˆ†åˆ«æ˜¯
    \(\frac{\partial O_3}{\partial I_1}\) å’Œ \(frac{\partial O_3}{\partial H_2}\)ã€‚
- en: and more importantly, how changes in the input \(I_1\), or \(H_2\) affect the
    change in model loss, \(\frac{\partial L}{\partial I_1}\) and \(frac{\partial
    L}{\partial H_2}\) respsectively.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´é‡è¦çš„æ˜¯ï¼Œè¾“å…¥ \(I_1\) æˆ– \(H_2\) çš„å˜åŒ–å¦‚ä½•å½±å“æ¨¡å‹æŸå¤±çš„å˜åŒ–ï¼Œåˆ†åˆ«æ˜¯ \(\frac{\partial L}{\partial I_1}\)
    å’Œ \(frac{\partial L}{\partial H_2}\)ã€‚
- en: This chain of partial derivatives, move backwards step by step through the neural
    network layers, is the fundamental mechanism behind **backpropagation**. Next
    we will derive and demonstrate each of the parts of backpropagation and then finally
    put this together to show backpropagation over our entire network.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€ç³»åˆ—çš„åå¯¼æ•°ï¼Œé€æ­¥åå‘é€šè¿‡ç¥ç»ç½‘ç»œå±‚ï¼Œæ˜¯**åå‘ä¼ æ’­**èƒŒåçš„åŸºæœ¬æœºåˆ¶ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°†æ¨å¯¼å¹¶æ¼”ç¤ºåå‘ä¼ æ’­çš„å„ä¸ªéƒ¨åˆ†ï¼Œç„¶åæœ€ç»ˆå°†å®ƒä»¬ç»„åˆèµ·æ¥ï¼Œå±•ç¤ºæ•´ä¸ªç½‘ç»œçš„åå‘ä¼ æ’­ã€‚
- en: Neural Networks Backpropagation Building Blocks
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œåå‘ä¼ æ’­æ„å»ºå—
- en: Letâ€™s cover the numerical building blocks for backpropagation. Once you understand
    these backpropagation building blocks, you will be able to backpropagate our simple
    network and even any complicated artificial neural networks by hand,
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®¨è®ºåå‘ä¼ æ’­çš„æ•°å€¼æ„å»ºå—ã€‚ä¸€æ—¦ä½ ç†è§£äº†è¿™äº›åå‘ä¼ æ’­æ„å»ºå—ï¼Œä½ å°†èƒ½å¤Ÿæ‰‹åŠ¨åå‘ä¼ æ’­æˆ‘ä»¬çš„ç®€å•ç½‘ç»œï¼Œç”šè‡³ä»»ä½•å¤æ‚çš„äººå·¥ç¥ç»ç½‘ç»œï¼Œ
- en: calculating the loss derivative
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±å¯¼æ•°
- en: backpropagation through nodes
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡èŠ‚ç‚¹è¿›è¡Œåå‘ä¼ æ’­
- en: backpropagation along connections
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ²¿ç€è¿æ¥è¿›è¡Œåå‘ä¼ æ’­
- en: accounting for multiple paths
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è€ƒè™‘å¤šæ¡è·¯å¾„
- en: loss derivatives with respect to weights and biases
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å…³äºæƒé‡å’Œåç½®çš„æŸå¤±å¯¼æ•°
- en: For now I demonstrate backpropagation of this loss derivative for a single training
    data sample, \(y\).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œæˆ‘æ¼”ç¤ºäº†å•ä¸ªè®­ç»ƒæ•°æ®æ ·æœ¬ \(y\) çš„è¿™ä¸ªæŸå¤±å¯¼æ•°çš„åå‘ä¼ æ’­ã€‚
- en: I address multiple samples later, \(y_i, i=1, \ldots, n\)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å°†åœ¨åé¢è®¨è®ºå¤šä¸ªæ ·æœ¬ï¼Œ\(y_i, i=1, \ldots, n\)
- en: Letâ€™s start with calculating the loss derivative.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»è®¡ç®—æŸå¤±å¯¼æ•°å¼€å§‹ã€‚
- en: Calculating the Loss Derivative
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±å¯¼æ•°
- en: Backpropagation is based on the concept of allocating or propagating the loss
    derivative backwards through the neural network,
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­åŸºäºå°†æŸå¤±å¯¼æ•°åˆ†é…æˆ–ä¼ æ’­å›ç¥ç»ç½‘ç»œçš„æ¦‚å¿µï¼Œ
- en: we calculate the loss derivative and then distribute it sequentially, in reverse
    direction, from network output back towards the network input
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—æŸå¤±å¯¼æ•°ï¼Œç„¶åæŒ‰é¡ºåºä»ç½‘ç»œè¾“å‡ºåå‘åˆ†å¸ƒåˆ°ç½‘ç»œè¾“å…¥
- en: it is important to know that we are working with derivatives, and that backpropagation
    is NOT distributing error, although as you will see it may look that way!
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯è¦çŸ¥é“æˆ‘ä»¬æ­£åœ¨å¤„ç†å¯¼æ•°ï¼Œå¹¶ä¸”åå‘ä¼ æ’­å¹¶ä¸æ˜¯åœ¨åˆ†é…è¯¯å·®ï¼Œå°½ç®¡æ­£å¦‚ä½ å°†çœ‹åˆ°çš„ï¼Œå®ƒå¯èƒ½çœ‹èµ·æ¥æ˜¯è¿™æ ·ï¼
- en: We start by defining the loss, given the truth, \(ğ‘¦\), and our prediction, \(\hat{y}
    = O_6\), we calculate our \(L^2\) loss as,
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå®šä¹‰æŸå¤±ï¼Œç»™å®šçœŸå®å€¼ \(ğ‘¦\) å’Œæˆ‘ä»¬çš„é¢„æµ‹ \(\hat{y} = O_6\)ï¼Œæˆ‘ä»¬è®¡ç®— \(L^2\) æŸå¤±å¦‚ä¸‹ï¼Œ
- en: \[ L = \frac{1}{2} \left( \hat{y} - y \right)^2 \]
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L = \frac{1}{2} \left( \hat{y} - y \right)^2 \]
- en: our choice of loss function allows us to use the prediction error as the loss
    derivative! We calculate the loss derivative as the partial derivative of the
    loss with respect to the estimate, \(\frac{\partial ğ¿}{\partial \hat{y}}\),
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€‰æ‹©çš„æŸå¤±å‡½æ•°å…è®¸æˆ‘ä»¬ä½¿ç”¨é¢„æµ‹è¯¯å·®ä½œä¸ºæŸå¤±å¯¼æ•°ï¼æˆ‘ä»¬è®¡ç®—æŸå¤±å¯¼æ•°ä½œä¸ºæŸå¤±ç›¸å¯¹äºä¼°è®¡çš„åå¯¼æ•°ï¼Œ\(\frac{\partial ğ¿}{\partial
    \hat{y}}\),
- en: \[ \frac{\partial \mathcal{L}}{\partial \hat{y}} = \frac{\partial \frac{1}{2}
    \left( \hat{y} - y \right)^2 }{\partial \hat{y}} = \hat{y} - y \]
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial \hat{y}} = \frac{\partial \frac{1}{2}
    \left( \hat{y} - y \right)^2 }{\partial \hat{y}} = \hat{y} - y \]
- en: You see what I mean, we are backpropagating the loss derivative, but due to
    our formulation of the \(L^2\) loss, we only have to calculate the error at our
    output node output, but once again - it is the loss derivative.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜ç™½æˆ‘çš„æ„æ€ï¼Œæˆ‘ä»¬æ­£åœ¨åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ï¼Œä½†ç”±äºæˆ‘ä»¬å¯¹ \(L^2\) æŸå¤±çš„å…¬å¼åŒ–ï¼Œæˆ‘ä»¬åªéœ€è¦è®¡ç®—è¾“å‡ºèŠ‚ç‚¹è¾“å‡ºå¤„çš„è¯¯å·®ï¼Œä½†å†æ¬¡å¼ºè°ƒâ€”â€”å®ƒæ˜¯æŸå¤±å¯¼æ•°ã€‚
- en: '![](../Images/6abf780fa4544caa6735a8f4ad075bd2.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6abf780fa4544caa6735a8f4ad075bd2.png)'
- en: Calculation of the loss derivative at the output of an output layer node, $O_6$.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—è¾“å‡ºå±‚èŠ‚ç‚¹ \(O_6\) çš„æŸå¤±å¯¼æ•°ã€‚
- en: For the example of our simple artificial neural network with the output at node,
    \(O_6\), our loss derivative is,
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„ç®€å•äººå·¥ç¥ç»ç½‘ç»œï¼Œè¾“å‡ºåœ¨èŠ‚ç‚¹ \(O_6\)ï¼Œæˆ‘ä»¬çš„æŸå¤±å¯¼æ•°æ˜¯ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial O_6} = \frac{\partial \mathcal{L}}{\hat{y}}
    = \hat{y} - y = O_6 - y \]
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial O_6} = \frac{\partial \mathcal{L}}{\hat{y}}
    = \hat{y} - y = O_6 - y \]
- en: So this is our loss derivative backpropagated to the output our output node,
    and we are now we are ready to backpropagate this loss derivative through our
    artificial neural network, letâ€™s talk about how we step through nodes and along
    connections.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™æ˜¯æˆ‘ä»¬åå‘ä¼ æ’­åˆ°è¾“å‡ºèŠ‚ç‚¹çš„æŸå¤±å¯¼æ•°ï¼Œæˆ‘ä»¬ç°åœ¨å‡†å¤‡é€šè¿‡æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œåå‘ä¼ æ’­è¿™ä¸ªæŸå¤±å¯¼æ•°ï¼Œè®©æˆ‘ä»¬è°ˆè°ˆå¦‚ä½•é€æ­¥é€šè¿‡èŠ‚ç‚¹å’Œæ²¿ç€è¿æ¥ã€‚
- en: Backpropagation through Output Node with Identity Activation
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…·æœ‰æ’ç­‰æ¿€æ´»å‡½æ•°çš„è¾“å‡ºèŠ‚ç‚¹ \(O_6\) çš„åå‘ä¼ æ’­
- en: Letâ€™s backpropagate through our output node, \(O_6\), from post-activation to
    pre-activation. To do this we need the partial derivative our activation function.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡è¾“å‡ºèŠ‚ç‚¹ \(O_6\) ä»åæ¿€æ´»åˆ°å‰æ¿€æ´»è¿›è¡Œåå‘ä¼ æ’­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ¿€æ´»å‡½æ•°çš„åå¯¼æ•°ã€‚
- en: since this is an output node with a regression artificial neural network I have
    selected the identity or linear activation function.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºè¿™æ˜¯ä¸€ä¸ªå…·æœ‰å›å½’äººå·¥ç¥ç»ç½‘ç»œçš„è¾“å‡ºèŠ‚ç‚¹ï¼Œæˆ‘é€‰æ‹©äº†æ’ç­‰æˆ–çº¿æ€§æ¿€æ´»å‡½æ•°ã€‚
- en: '![](../Images/1141c4e67c550f275e9079501fe522b2.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1141c4e67c550f275e9079501fe522b2.png)'
- en: Backpropagation of the loss derivative through the node, $O_6$, from $O_6$ post-activation
    output to $O_{6_{in}}$ pre-activation input.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡èŠ‚ç‚¹ \(O_6\) ä» \(O_6\) åæ¿€æ´»è¾“å‡ºåˆ° \(O_{6_{in}}\) å‰æ¿€æ´»è¾“å…¥çš„åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ã€‚
- en: 'The identity activation at output node \(O_6\) is defined as:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŠ‚ç‚¹ \(O_6\) å¤„çš„æ’ç­‰æ¿€æ´»å‡½æ•°å®šä¹‰ä¸ºï¼š
- en: \[ O_6 = \sigma(O_{6_{in}}) = O_6 \]
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \sigma(O_{6_{in}}) = O_6 \]
- en: The derivative of the identity activation at node \(O_6\) with respect to its
    input \(O_{6_{in}}\), i.e., crossing node \(O_6\) is,
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ \(O_6\) å¤„çš„æ’ç­‰æ¿€æ´»å‡½æ•°çš„å¯¼æ•°ç›¸å¯¹äºå…¶è¾“å…¥ \(O_{6_{in}}\)ï¼Œå³ç©¿è¿‡èŠ‚ç‚¹ \(O_6\)ï¼Œä¸ºï¼Œ
- en: \[ \frac{\partial O_6}{\partial O_{6_{in}}} = \frac{\partial \left(O_{6_{in}}
    \right)}{\partial O_{6_{in}}} = 1.0 \]
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_6}{\partial O_{6_{in}}} = \frac{\partial \left(O_{6_{in}}
    \right)}{\partial O_{6_{in}}} = 1.0 \]
- en: Note, we just need \(O_6\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative with respect to the node
    output, \(\frac{\partial \mathcal{L}}{\partial O_6}\), and to the loss derivative
    with respect to the node input, \(\frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}}\),
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦èŠ‚ç‚¹ \(O_6\) çš„è¾“å‡ºä¿¡å·ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªä¿¡å·æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»¥ä»èŠ‚ç‚¹è¾“å‡ºç›¸å¯¹äºæŸå¤±å¯¼æ•° \(\frac{\partial
    \mathcal{L}}{\partial O_6}\) åå‘ä¼ æ’­ï¼Œä»¥åŠä»èŠ‚ç‚¹è¾“å…¥ç›¸å¯¹äºæŸå¤±å¯¼æ•° \(\frac{\partial \mathcal{L}}{\partial
    O_{6_{\text{in}}}}\) åå‘ä¼ æ’­ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = \frac{\partial
    O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6}
    = 1.0 \cdot (O_6 - y) \]
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = \frac{\partial
    O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6}
    = 1.0 \cdot (O_6 - y) \]
- en: Now that we have backpropagated through an output node, letâ€™s backpropagation
    along the \(H_4\) to \(O_6\) connection from the hidden layer.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¢ç„¶æˆ‘ä»¬å·²ç»é€šè¿‡è¾“å‡ºèŠ‚ç‚¹è¿›è¡Œäº†åå‘ä¼ æ’­ï¼Œé‚£ä¹ˆè®©æˆ‘ä»¬æ²¿ç€éšè—å±‚ä¸­ä» \(H_4\) åˆ° \(O_6\) çš„è¿æ¥è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: Backpropagation along Connections
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ²¿ç€è¿æ¥çš„åå‘ä¼ æ’­
- en: Now letâ€™s backpropagate along the connection between nodes \(O_6\) and \(H_4\).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ²¿ç€èŠ‚ç‚¹ \(O_6\) å’Œ \(H_4\) ä¹‹é—´çš„è¿æ¥è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: '![](../Images/1c686c4b3a6fe96317a79c7333542353.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c686c4b3a6fe96317a79c7333542353.png)'
- en: Backpropagation of the loss derivative through node \(O_6\), from \(O_6\) post-activation
    output to $O_{6_{in}}$ pre-activation input and then along the connection to the
    output from node \(H_4\).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡èŠ‚ç‚¹ \(O_6\) ä» \(O_6\) åæ¿€æ´»è¾“å‡ºåˆ° \(O_{6_{in}}\) å‰æ¿€æ´»è¾“å…¥çš„åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ï¼Œç„¶åæ²¿ç€è¿æ¥åˆ°èŠ‚ç‚¹ \(H_4\)
    çš„è¾“å‡ºã€‚
- en: Preactivation, the input to node \(ğ‘‚_6\) is calculated as,
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰æ¿€æ´»é˜¶æ®µï¼ŒèŠ‚ç‚¹ \(ğ‘‚_6\) çš„è¾“å…¥è®¡ç®—å¦‚ä¸‹ï¼Œ
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
- en: We calculate the derivative along the connection as,
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—è¿æ¥ä¸Šçš„å¯¼æ•°å¦‚ä¸‹ï¼Œ
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \frac{\partial}{\partial
    H_4} \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right) =
    \lambda_{4,6} \]
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \frac{\partial}{\partial
    H_4} \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right) =
    \lambda_{4,6} \]
- en: by resolving the above partial derivative, we see that backpropagation along
    a connection by applying the connection weight.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è§£å†³ä¸Šè¿°åå¯¼æ•°ï¼Œæˆ‘ä»¬å‘ç°é€šè¿‡åº”ç”¨è¿æ¥æƒé‡æ¥æ²¿ç€è¿æ¥è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \lambda_{4,6} \]
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \lambda_{4,6} \]
- en: Note, we just need the current connection weight \(\lambda_{4,6}\). Now we can
    add this to our chain rule to backpropagate along the \(H_4\) to \(O_6\) connection
    from loss derivative with respect to the output layer node input \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\), to the loss derivative with respect
    to the hidden layer node output \(\frac{\partial \mathcal{L}}{\partial H_4}\).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦å½“å‰çš„è¿æ¥æƒé‡ \(\lambda_{4,6}\)ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œé€šè¿‡ \(H_4\) åˆ° \(O_6\) çš„è¿æ¥ä»æŸå¤±ç›¸å¯¹äºè¾“å‡ºå±‚èŠ‚ç‚¹è¾“å…¥çš„å¯¼æ•°
    \(\frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}}\) åå‘ä¼ æ’­åˆ°æŸå¤±ç›¸å¯¹äºéšè—å±‚èŠ‚ç‚¹è¾“å‡ºçš„å¯¼æ•°
    \(\frac{\partial \mathcal{L}}{\partial H_4}\)ã€‚
- en: \[ \frac{\partial \mathcal{L}}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \lambda_{4,6} \bigl( \cdot (1 - O_6) \cdot O_6 \bigr)
    \cdot (O_6 - y) \]
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \lambda_{4,6} \bigl( \cdot (1 - O_6) \cdot O_6 \bigr)
    \cdot (O_6 - y) \]
- en: Backpropagation through Nodes with Sigmoid Activation
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¸¦æœ‰sigmoidæ¿€æ´»çš„èŠ‚ç‚¹åå‘ä¼ æ’­
- en: Letâ€™s backpropagate through a hidden layer node, \(H_4\), from postactivation
    to preactivation. To do this we need the partial derivative our activation function.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªéšè—å±‚èŠ‚ç‚¹ \(H_4\) ä»åæ¿€æ´»åˆ°å‰æ¿€æ´»è¿›è¡Œåå‘ä¼ æ’­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ¿€æ´»å‡½æ•°çš„åå¯¼æ•°ã€‚
- en: we are assuming sigmoid activation for all hidden layer nodes
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‡è®¾æ‰€æœ‰éšè—å±‚èŠ‚ç‚¹ä½¿ç”¨sigmoidæ¿€æ´»
- en: for super clean logic, everyone resolves the activation derivative as a function
    of the output rather than as typical the input,
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¶…çº§æ¸…æ™°çš„é€»è¾‘ï¼Œæ¯ä¸ªäººéƒ½å°†æ¿€æ´»å¯¼æ•°ä½œä¸ºè¾“å‡ºè€Œä¸æ˜¯å…¸å‹è¾“å…¥çš„å‡½æ•°æ¥æ±‚è§£ï¼Œ
- en: '![](../Images/10ef488402716c36aa45b497ea54f6bb.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10ef488402716c36aa45b497ea54f6bb.png)'
- en: Backpropagation of the loss derivative through the node, $H_4$, from $H_4$ postactivation
    output to $H_{4_{in}}$ preactivation input.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡èŠ‚ç‚¹ \(H_4\) ä» \(H_4\) åæ¿€æ´»è¾“å‡ºåˆ° \(H_{4_{in}}\) å‰æ¿€æ´»è¾“å…¥çš„åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ã€‚
- en: 'The sigmoid activation at output node \(H_4\) is defined as:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŠ‚ç‚¹ \(H_4\) çš„sigmoidæ¿€æ´»å®šä¹‰ä¸ºï¼š
- en: \[ H_4 = \sigma(H_{4_{in}}) = \frac{1}{1 + e^{-H_{4_{in}}}} \]
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_4 = \sigma(H_{4_{in}}) = \frac{1}{1 + e^{-H_{4_{in}}}} \]
- en: The derivative of the sigmoid activation at node \(H_4\) with respect to its
    input \(H_{4_{in}}\), i.e., crossing node \(H_4\) is,
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ \(H_4\) çš„sigmoidæ¿€æ´»ç›¸å¯¹äºå…¶è¾“å…¥ \(H_{4_{in}}\) çš„å¯¼æ•°ï¼Œå³ç©¿è¶ŠèŠ‚ç‚¹ \(H_4\) æ˜¯ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + e^{-H_{4_{in}}}} \right) \]
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + e^{-H_{4_{in}}}} \right) \]
- en: Now, for compact notation letâ€™s set,
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†ç´§å‡‘çš„è¡¨ç¤ºï¼Œæˆ‘ä»¬è®¾ï¼Œ
- en: \[ u = e^{-H_{4_{in}}} \]
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: \[ u = e^{-H_{4_{in}}} \]
- en: and substituting we have,
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä»£å…¥æˆ‘ä»¬å¾—åˆ°ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) \]
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) \]
- en: and by the chain rule we can extend it to,
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶æ‰©å±•ä¸ºï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) = -\frac{u}{(1 + u)^2} \cdot \frac{\partial u}{\partial
    H_{4_{in}}} \]
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) = -\frac{u}{(1 + u)^2} \cdot \frac{\partial u}{\partial
    H_{4_{in}}} \]
- en: 'The derivative of \(u = e^{-H_{4_{in}}}\) with respect to \(H_{4_{in}}\) is:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: \(u = e^{-H_{4_{in}}}\) å…³äº \(H_{4_{in}}\) çš„å¯¼æ•°æ˜¯ï¼š
- en: \[ \frac{\partial u}{\partial H_{4_{in}}} = -e^{-H_{4_{in}}} = -u \]
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial u}{\partial H_{4_{in}}} = -e^{-H_{4_{in}}} = -u \]
- en: now we can substitute,
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥æ›¿æ¢ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = -\frac{1}{(1+u)^2} \cdot (-u)
    = \frac{u}{(1+u)^2} \]
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = -\frac{1}{(1+u)^2} \cdot (-u)
    = \frac{u}{(1+u)^2} \]
- en: Express in terms of node \(H_4\) output, \(H_4 = \frac{1}{1 + u}\),
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨èŠ‚ç‚¹ \(H_4\) çš„è¾“å‡ºè¡¨ç¤ºï¼Œ\(H_4 = \frac{1}{1 + u}\),
- en: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \frac{\left(1 - H_4\right)/H_4}{\left(1/H_4\right)^2}
    = \frac{1 - H_4}{H_4} \cdot H_4^2 = \left(1 - H_4\right) \cdot H_4 \]
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \frac{\left(1 - H_4\right)/H_4}{\left(1/H_4\right)^2}
    = \frac{1 - H_4}{H_4} \cdot H_4^2 = \left(1 - H_4\right) \cdot H_4 \]
- en: So we can backpropagate through our node, \(H_4\), from node post-activation
    output, \(H_4\) to node pre-activation input, \(H_{4_{in}}\), by,
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æˆ‘ä»¬çš„èŠ‚ç‚¹ \(H_4\) ä»èŠ‚ç‚¹åæ¿€æ´»è¾“å‡º \(H_4\) åˆ°èŠ‚ç‚¹å‰æ¿€æ´»è¾“å…¥ \(H_{4_{in}}\) è¿›è¡Œåå‘ä¼ æ’­ï¼Œé€šè¿‡ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \left(1 - H_4\right) \cdot
    H_4 \]
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \left(1 - H_4\right) \cdot
    H_4 \]
- en: Note, we just need \(H_4\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative to the output of node
    \(H_4\) and to the input of node \(H_4\),
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦èŠ‚ç‚¹ $H_4$ çš„è¾“å‡ºä¿¡å· \(H_4\)ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªä¿¡å·æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»¥ä»æŸå¤±å¯¼æ•°åå‘ä¼ æ’­åˆ°èŠ‚ç‚¹ \(H_4\)
    çš„è¾“å‡ºä»¥åŠèŠ‚ç‚¹ \(H_4\) çš„è¾“å…¥ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} = \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6}
    \cdot 1.0 \cdot (O_6 - y) \]
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} = \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6}
    \cdot 1.0 \cdot (O_6 - y) \]
- en: Now we can handle all cases of backpropagation through the nodes in our network.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å¤„ç†ç½‘ç»œä¸­èŠ‚ç‚¹çš„æ‰€æœ‰åå‘ä¼ æ’­æƒ…å†µã€‚
- en: Backpropagation Along Another Connection
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ²¿å¦ä¸€æ¡è¿æ¥è¿›è¡Œåå‘ä¼ æ’­
- en: For continuity and completeness, letâ€™s repeat the previously described method
    to backpropagate along the connection \(I_1\) to \(H_4\).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿ç»­æ€§å’Œå®Œæ•´æ€§ï¼Œè®©æˆ‘ä»¬é‡å¤ä¹‹å‰æè¿°çš„æ–¹æ³•ï¼Œä»¥æ²¿è¿æ¥ \(I_1\) åˆ° \(H_4\) è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: '![](../Images/76f408b8ed852b03a168e6069bb716db.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76f408b8ed852b03a168e6069bb716db.png)'
- en: Backpropagation of the loss derivative along the connection from $H_4$ to $I_1$.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¿ç€ä» $H_4$ åˆ° $I_1$ çš„è¿æ¥è¿›è¡ŒæŸå¤±å¯¼æ•°çš„åå‘ä¼ æ’­ã€‚
- en: Once again, preactivation the input to node \(H_4\) is calculated as,
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼ŒèŠ‚ç‚¹ \(H_4\) çš„è¾“å…¥çš„é¢„æ¿€æ´»è®¡ç®—å¦‚ä¸‹ï¼Œ
- en: \[ H_{4_{\text{in}}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4_{\text{in}}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]
- en: We calculate the derivative along the connection as,
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—æ²¿è¿æ¥çš„å¯¼æ•°å¦‚ä¸‹ï¼Œ
- en: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \frac{\partial \left(\lambda_{1,4}
    \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 + b_4 \right)}{\partial
    I_1} = \lambda_{1,4} \]
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \frac{\partial \left(\lambda_{1,4}
    \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 + b_4 \right)}{\partial
    I_1} = \lambda_{1,4} \]
- en: by resolving the above partial derivative, we see that backpropagation along
    a connection by applying the connection weight.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ±‚è§£ä¸Šè¿°åå¯¼æ•°ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é€šè¿‡åº”ç”¨è¿æ¥æƒé‡æ²¿è¿æ¥è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \lambda_{4,6} \]
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \lambda_{4,6} \]
- en: Note, we just need the current connection weight \(\lambda_{4,6}\). Now we can
    add this to our chain rule to backpropagate along the \(H_4\) to \(O_6\) connection
    from loss derivative with respect to the output layer node input \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\), to the loss derivative with respect
    to the hidden layer node output \(\frac{\partial \mathcal{L}}{\partial H_4}\).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦å½“å‰çš„è¿æ¥æƒé‡ \(\lambda_{4,6}\)ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæƒé‡æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»¥ä»æŸå¤±å¯¼æ•°åå‘ä¼ æ’­åˆ°è¾“å‡ºå±‚èŠ‚ç‚¹è¾“å…¥
    \(\frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}}\)ï¼Œåˆ°éšè—å±‚èŠ‚ç‚¹è¾“å‡º \(\frac{\partial
    \mathcal{L}}{\partial H_4}\) çš„ \(H_4\) åˆ° \(O_6\) è¿æ¥ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) \]
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) \]
- en: Accounting for Multiple Paths
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è€ƒè™‘å¤šæ¡è·¯å¾„
- en: Our loss derivative with respect to the node output \(I_1\), \(\frac{\partial
    \mathcal{L}}{\partial I_1}\) is not correct!
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å…³äºèŠ‚ç‚¹è¾“å‡º \(I_1\) çš„æŸå¤±å¯¼æ•° \(\frac{\partial \mathcal{L}}{\partial I_1}\) æ˜¯ä¸æ­£ç¡®çš„ï¼
- en: we accounted for the \(O_6\) to \(H_4\) to \(I_1\) path, but we did not acccount
    for the \(O_6\) to \(H_5\) to \(I_1\) path
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»è€ƒè™‘äº†ä» \(O_6\) åˆ° \(H_4\) åˆ° \(I_1\) çš„è·¯å¾„ï¼Œä½†æ²¡æœ‰è€ƒè™‘ä» \(O_6\) åˆ° \(H_5\) åˆ° \(I_1\)
    çš„è·¯å¾„
- en: '![](../Images/6a68404e4b3d2589dcfe15beb5175c60.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a68404e4b3d2589dcfe15beb5175c60.png)'
- en: Multiple paths for backpropagation to input node, $I_1$, from output node $O_6$.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¾“å‡ºèŠ‚ç‚¹ $O_6$ åˆ°è¾“å…¥èŠ‚ç‚¹ $I_1$ çš„åå‘ä¼ æ’­å­˜åœ¨å¤šæ¡è·¯å¾„ã€‚
- en: To account for multiple paths we just need to sum over all the paths.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è€ƒè™‘å¤šæ¡è·¯å¾„ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹æ‰€æœ‰è·¯å¾„è¿›è¡Œæ±‚å’Œã€‚
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
- en: we can evaluate this as,
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¯„ä¼°ä¸ºï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) + \lambda_{1,5}
    \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \cdot 1.0 \cdot (O_6
    - y) \]
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) + \lambda_{1,5}
    \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \cdot 1.0 \cdot (O_6
    - y) \]
- en: and then simplify by removing the 1.0 values and grouping terms as,
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åé€šè¿‡ç§»é™¤ 1.0 å€¼å’Œåˆ†ç»„é¡¹è¿›è¡Œç®€åŒ–ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
- en: and now we can evaluate this simplified form as,
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªç®€åŒ–çš„å½¢å¼ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1 - H_5)
    \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1 - H_5)
    \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
- en: Backpropagation through Input Nodes with Identity Activation
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡è¾“å…¥èŠ‚ç‚¹è¿›è¡Œæ’ç­‰æ¿€æ´»çš„é€†ä¼ æ’­
- en: Letâ€™s backpropagate through our input node, \(I_1\), from postactivation to
    preactivation. To do this we need the partial derivative our activation function.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡è¾“å…¥èŠ‚ç‚¹ \(I_1\) ä»åæ¿€æ´»åˆ°å‰æ¿€æ´»è¿›è¡Œé€†ä¼ æ’­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ¿€æ´»å‡½æ•°çš„åå¯¼æ•°ã€‚
- en: since this is an input node I have selected the identity or linear activation
    function.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºè¿™æ˜¯ä¸€ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œæˆ‘é€‰æ‹©äº†æ’ç­‰æˆ–çº¿æ€§æ¿€æ´»å‡½æ•°ã€‚
- en: '![](../Images/bd05208edfb4ac5d5cc0f8cfe808ae50.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd05208edfb4ac5d5cc0f8cfe808ae50.png)'
- en: Backpropagation of the loss derivative through the node, $I_1$, from $I_1$ postactivation
    output to $I_{1_{in}}$ preactivation input.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ä»èŠ‚ç‚¹ \(I_1\) çš„åæ¿€æ´»è¾“å‡ºåˆ°å‰æ¿€æ´»è¾“å…¥çš„æŸå¤±å¯¼æ•°é€†ä¼ æ’­ã€‚
- en: 'The identity activation at output node \(I_1\) is defined as:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŠ‚ç‚¹ \(I_1\) ä¸Šçš„æ’ç­‰æ¿€æ´»å®šä¹‰ä¸ºï¼š
- en: \[ I_1 = \sigma(I_{1_{in}}) = I_1 \]
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: \[ I_1 = \sigma(I_{1_{in}}) = I_1 \]
- en: The derivative of the identity activation at node \(I_1\) with respect to its
    input \(I_{1_{in}}\), i.e., passing through node \(I_1\) is,
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ \(I_1\) ä¸Šæ’ç­‰æ¿€æ´»ç›¸å¯¹äºå…¶è¾“å…¥ \(I_{1_{in}}\) çš„å¯¼æ•°ï¼Œå³é€šè¿‡èŠ‚ç‚¹ \(I_1\) ä¼ é€’ï¼Œæ˜¯ï¼Œ
- en: \[ \frac{\partial I_1}{\partial I_{1_{in}}} = \frac{\partial \left(I_{1_{in}}
    \right)}{\partial I_{1_{in}}} = 1.0 \]
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial I_1}{\partial I_{1_{in}}} = \frac{\partial \left(I_{1_{in}}
    \right)}{\partial I_{1_{in}}} = 1.0 \]
- en: Note, we just need \(I_1\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative with respect to the node
    output, \(\frac{\partial \mathcal{L}}{\partial I_1}\), and to the loss derivative
    with respect to the node input, \(\frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}}\),
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦ \(I_1\)ï¼Œå³ä»èŠ‚ç‚¹è¾“å‡ºçš„ä¿¡å·ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªå€¼æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»èŠ‚ç‚¹è¾“å‡ºç›¸å¯¹äºæŸå¤±å¯¼æ•° \(\frac{\partial
    \mathcal{L}}{\partial I_1}\) é€†ä¼ æ’­åˆ°èŠ‚ç‚¹è¾“å…¥ç›¸å¯¹äºæŸå¤±å¯¼æ•° \(\frac{\partial \mathcal{L}}{\partial
    I_{1_{\text{in}}}}\)ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \frac{\partial I_1}{\partial
    I_{1_{in}}} \cdot \frac{\partial H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} + \frac{\partial I_1}{\partial I_{1_{in}}} \cdot \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial
    O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \frac{\partial I_1}{\partial
    I_{1_{in}}} \cdot \frac{\partial H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} + \frac{\partial I_1}{\partial I_{1_{in}}} \cdot \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial
    O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
- en: we can evaluate this as,
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªå½¢å¼ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = 1.0 \cdot \lambda_{1,4}
    \cdot \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6
    - y) + 1.0 \cdot \lambda_{1,5} \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6}
    \cdot 1.0 \cdot (O_6 - y) \]
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = 1.0 \cdot \lambda_{1,4}
    \cdot \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6
    - y) + 1.0 \cdot \lambda_{1,5} \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6}
    \cdot 1.0 \cdot (O_6 - y) \]
- en: For fun I designed this notation for maximum clarity,
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¹è¶£ï¼Œæˆ‘è®¾è®¡äº†è¿™ç§è¡¨ç¤ºæ³•ä»¥å®ç°æœ€å¤§æ¸…æ™°åº¦ï¼Œ
- en: \[ \frac{\partial L}{\partial I_{1_{\text{in}}}} = \overbrace{1.0}^{\textstyle
    \frac{\partial I_{1}}{\partial I_{1_{in}}}} \left[ \overbrace{\lambda_{1,4}}^{\textstyle
    \frac{\partial H_{4_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_4) \cdot
    H_4}^{\textstyle \frac{\partial H_4}{\partial H_{4_{\text{in}}}}} \cdot \overbrace{\lambda_{4,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_4}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} + \overbrace{\lambda_{1,5}}^{\textstyle \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_5) \cdot H_5}^{\textstyle
    \frac{\partial H_5}{\partial H_{5_{\text{in}}}}} \cdot \overbrace{\lambda_{5,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_5}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} \right] \]
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial I_{1_{\text{in}}}} = \overbrace{1.0}^{\textstyle
    \frac{\partial I_{1}}{\partial I_{1_{in}}}} \left[ \overbrace{\lambda_{1,4}}^{\textstyle
    \frac{\partial H_{4_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_4) \cdot
    H_4}^{\textstyle \frac{\partial H_4}{\partial H_{4_{\text{in}}}}} \cdot \overbrace{\lambda_{4,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_4}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} + \overbrace{\lambda_{1,5}}^{\textstyle \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_5) \cdot H_5}^{\textstyle
    \frac{\partial H_5}{\partial H_{5_{\text{in}}}}} \cdot \overbrace{\lambda_{5,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_5}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} \right] \]
- en: But this can be simplified by removing the 1.0 values and grouping terms as,
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™å¯ä»¥é€šè¿‡ç§»é™¤1.0å€¼å¹¶ç»„åˆé¡¹æ¥ç®€åŒ–ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
- en: and now we can evaluate this simplified form as,
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªç®€åŒ–çš„å½¢å¼ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \lambda_{1,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \lambda_{1,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
- en: For completeness here is the backpropagation for the other input nodes, hereâ€™s
    \(\frac{\partial \mathcal{L}}{\partial I_{2_{in}}}\),
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œæ˜¯å…¶ä»–è¾“å…¥èŠ‚ç‚¹çš„åå‘ä¼ æ’­ï¼Œè¿™é‡Œæ˜¯ \(\frac{\partial \mathcal{L}}{\partial I_{2_{in}}}\)ï¼Œ
- en: '![](../Images/9b186fef88b4e0ad1fa6191394b0e0dd.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b186fef88b4e0ad1fa6191394b0e0dd.png)'
- en: Backpropagation of the loss derivative for input node 2.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥èŠ‚ç‚¹2çš„æŸå¤±å¯¼æ•°åå‘ä¼ æ’­ã€‚
- en: For brevity I have remove the 1.0s and grouped like terms,
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´ï¼Œæˆ‘å·²ç§»é™¤1.0å¹¶åˆå¹¶åŒç±»é¡¹ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_2} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_2} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_2} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_2} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
- en: and now we can evaluate this simplified form as,
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªç®€åŒ–çš„å½¢å¼ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \lambda_{2,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{2,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \lambda_{2,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{2,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
- en: and here is \(\frac{\partial \mathcal{L}}{\partial I_{3_{in}}}\),
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ \(\frac{\partial \mathcal{L}}{\partial I_{3_{in}}}\),
- en: '![](../Images/90c6a46319d78ae1b9d4f8cdc131fd2a.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90c6a46319d78ae1b9d4f8cdc131fd2a.png)'
- en: Backpropagation of the loss derivative for input node 3.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥èŠ‚ç‚¹3çš„æŸå¤±å¯¼æ•°åå‘ä¼ æ’­ã€‚
- en: For brevity I have remove the 1.0s and grouped like terms,
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´ï¼Œæˆ‘å·²ç§»é™¤1.0å¹¶åˆå¹¶åŒç±»é¡¹ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_3} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_3} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_3} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_3} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
- en: and now we can evaluate this simplified form as,
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªç®€åŒ–çš„å½¢å¼ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \lambda_{3,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{3,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \lambda_{3,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{3,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
- en: Loss Derivatives with Respect to Weights and Biases
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç›¸å¯¹äºæƒé‡å’Œåç½®çš„æŸå¤±å¯¼æ•°
- en: Now we have back propagated the loss derivative through our network.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»é€šè¿‡æˆ‘ä»¬çš„ç½‘ç»œåå‘ä¼ æ’­äº†æŸå¤±å¯¼æ•°ã€‚
- en: '![](../Images/97de6228871f7910de4d7a8fa86e0bd8.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97de6228871f7910de4d7a8fa86e0bd8.png)'
- en: Backpropagated loss derivatives with respect to all network nodes inputs and
    outputs.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ç½‘ç»œèŠ‚ç‚¹è¾“å…¥å’Œè¾“å‡ºçš„åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ã€‚
- en: and we have the loss derivative with respect to the input and output of each
    node in our network,
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†ç›¸å¯¹äºç½‘ç»œä¸­æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥å’Œè¾“å‡ºçš„æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial I_1},\quad \frac{\partial \mathcal{L}}{\partial I_{2_{\text{in}}}},\quad
    \frac{\partial \mathcal{L}}{\partial I_2},\quad \frac{\partial \mathcal{L}}{\partial
    I_{3_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial I_3},\quad \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial
    H_4},\quad \frac{\partial \mathcal{L}}{\partial H_5},\quad \frac{\partial \mathcal{L}}{\partial
    H_5},\quad \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial O_6} \]
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial I_1},\quad \frac{\partial \mathcal{L}}{\partial I_{2_{\text{in}}}},\quad
    \frac{\partial \mathcal{L}}{\partial I_2},\quad \frac{\partial \mathcal{L}}{\partial
    I_{3_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial I_3},\quad \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial
    H_4},\quad \frac{\partial \mathcal{L}}{\partial H_5},\quad \frac{\partial \mathcal{L}}{\partial
    H_5},\quad \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial O_6} \]
- en: But what we actually need is the loss derivative with respect to each connection
    weights,
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬å®é™…ä¸Šéœ€è¦çš„æ˜¯ç›¸å¯¹äºæ¯ä¸ªè¿æ¥æƒé‡çš„æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}},\quad \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,4}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{1,5}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{2,5}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,5}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{5,6}} \]
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}},\quad \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,4}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{1,5}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{2,5}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,5}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{5,6}} \]
- en: and node biases,
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠèŠ‚ç‚¹åå·®ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial b_4},\quad \frac{\partial \mathcal{L}}{\partial
    b_5},\quad \frac{\partial \mathcal{L}}{\partial b_6} \]
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial b_4},\quad \frac{\partial \mathcal{L}}{\partial
    b_5},\quad \frac{\partial \mathcal{L}}{\partial b_6} \]
- en: How do we backpropagate the loss derivative to a connection weight? Letâ€™s start
    with the \(H_4\) to \(O_6\) connection.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•å°†æŸå¤±å¯¼æ•°åå‘ä¼ æ’­åˆ°è¿æ¥æƒé‡ï¼Ÿè®©æˆ‘ä»¬ä» \(H_4\) åˆ° \(O_6\) çš„è¿æ¥å¼€å§‹ã€‚
- en: '![](../Images/bf1e650cd4b0613fa09aa5d5c0e72e26.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf1e650cd4b0613fa09aa5d5c0e72e26.png)'
- en: Backpropagated loss derivatives with respect to a connection weight.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­çš„è¿æ¥æƒé‡ç›¸å…³çš„æŸå¤±å¯¼æ•°ã€‚
- en: Preactivation, input to node \(O_6\) we have,
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æ¿€æ´»ï¼ŒèŠ‚ç‚¹ \(O_6\) çš„è¾“å…¥ä¸ºï¼Œ
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
- en: We calculate the derivative with respect to the connection weight as,
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—ä¸è¿æ¥æƒé‡ç›¸å…³çš„å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial \lambda_{4,6}} = \frac{\partial
    \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial
    \lambda_{4,6}} = H_4 \]
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_{6_{\text{in}}}}{\partial \lambda_{4,6}} = \frac{\partial
    \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial
    \lambda_{4,6}} = H_4 \]
- en: We need the output of the node in the previous layer passed along the connection
    to backpropagate to the loss derivative with respect to the connection weight
    from the input to the next node,
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦å°†å‰ä¸€å±‚èŠ‚ç‚¹çš„è¾“å‡ºé€šè¿‡è¿æ¥ä¼ é€’ï¼Œä»¥ä¾¿åå‘ä¼ æ’­åˆ°ä¸è¿æ¥æƒé‡ç›¸å…³çš„æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot 1.0 \cdot (O_6 - y) \]
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot 1.0 \cdot (O_6 - y) \]
- en: Now, for completeness, here are the equations for all of our networkâ€™s connection
    weights.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œåˆ—å‡ºäº†æˆ‘ä»¬ç½‘ç»œæ‰€æœ‰è¿æ¥æƒé‡çš„æ–¹ç¨‹ã€‚
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_5 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_5 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
- en: See the pattern, the loss derivatives with respect to connection weights are,
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹çœ‹è¿™ä¸ªæ¨¡å¼ï¼Œå…³äºè¿æ¥æƒé‡çš„æŸå¤±å¯¼æ•°æ˜¯ï¼Œ
- en: \[ \text{Connection Signal} \times \text{Loss Derivative of Next Node Input}
    \]
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{è¿æ¥ä¿¡å·} \times \text{ä¸‹ä¸€èŠ‚ç‚¹è¾“å…¥çš„æŸå¤±å¯¼æ•°} \]
- en: Now how do we backpropagate the loss derivative to a node bias? Letâ€™s start
    with the \(O_6\) node.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¦‚ä½•å°†æŸå¤±å¯¼æ•°åå‘ä¼ æ’­åˆ°èŠ‚ç‚¹åç½®ï¼Ÿè®©æˆ‘ä»¬ä» \(O_6\) èŠ‚ç‚¹å¼€å§‹ã€‚
- en: '![](../Images/646997a228f8c1e8a3b2743ac13e388a.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/646997a228f8c1e8a3b2743ac13e388a.png)'
- en: Backpropagated loss derivatives with respect to a node bias.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºèŠ‚ç‚¹åç½®çš„åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ã€‚
- en: Once again, the preactivation, input to node \(O_6\) is,
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ¬¡åˆä¸€æ¬¡ï¼ŒèŠ‚ç‚¹ \(O_6\) çš„é¢„æ¿€æ´»è¾“å…¥æ˜¯ï¼Œ
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
- en: We calculate the derivative of a connection weight as,
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—è¿æ¥æƒé‡çš„å¯¼æ•°å¦‚ä¸‹ï¼Œ
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial b_6} = \frac{\partial \left( \lambda_{4,6}
    \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial b_6} = 1.0 \]
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_{6_{\text{in}}}}{\partial b_6} = \frac{\partial \left( \lambda_{4,6}
    \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial b_6} = 1.0 \]
- en: so our bias loss derivative is equal to the node input loss derivative,
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬çš„åç½®æŸå¤±å¯¼æ•°ç­‰äºèŠ‚ç‚¹è¾“å…¥æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial O_{6_{\text{in}}}}{\partial
    b_6} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = 1.0 \cdot
    \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial O_{6_{\text{in}}}}{\partial
    b_6} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = 1.0 \cdot
    \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
- en: For completeness here are all the loss derivatives with respect to node biases,
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œåˆ—å‡ºäº†ç›¸å¯¹äºèŠ‚ç‚¹åå·®çš„æ‰€æœ‰æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial \mathcal{L}}{\partial
    O_{6_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial b_4} = \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial
    b_5} = \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial \mathcal{L}}{\partial
    O_{6_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial b_4} = \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial
    b_5} = \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]
- en: See the pattern, the loss derivatives with respect to node biases are,
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹çœ‹è¿™ä¸ªæ¨¡å¼ï¼Œç›¸å¯¹äºèŠ‚ç‚¹åå·®çš„æŸå¤±å¯¼æ•°æ˜¯ï¼Œ
- en: \[ \text{Loss Derivative of the Node Input} \]
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{èŠ‚ç‚¹è¾“å…¥çš„æŸå¤±å¯¼æ•°} \]
- en: Backpropagation Example
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­ç¤ºä¾‹
- en: Letâ€™s take the backpropagation method explained above and apply them to my interactive
    neural network.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°†ä¸Šé¢è§£é‡Šçš„åå‘ä¼ æ’­æ–¹æ³•åº”ç”¨åˆ°æˆ‘çš„äº¤äº’å¼ç¥ç»ç½‘ç»œä¸­ã€‚
- en: Hereâ€™s the result for our first training epoch with only 1 sample,
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€æ¬¡è®­ç»ƒå‘¨æœŸä»…æœ‰ä¸€ä¸ªæ ·æœ¬çš„ç»“æœï¼Œ
- en: '![](../Images/b8b7d56b334d9728490cb2bc1408535e.png)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8b7d56b334d9728490cb2bc1408535e.png)'
- en: Backpropagation result for the first iteration.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ¬¡è¿­ä»£çš„åå‘ä¼ æ’­ç»“æœã€‚
- en: My interactive dashboard provides all the loss derivatives with respect to the
    input for each node and the output signals from each node, so for example we can
    calculate \(\frac{\partial L}{\partial \lambda_{4,6}}\) as,
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„äº¤äº’å¼ä»ªè¡¨æ¿æä¾›äº†æ¯ä¸ªèŠ‚ç‚¹ç›¸å¯¹äºè¾“å…¥çš„æ‰€æœ‰æŸå¤±å¯¼æ•°ä»¥åŠæ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºä¿¡å·ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®— \(\frac{\partial L}{\partial
    \lambda_{4,6}}\) å¦‚ä¸‹ï¼Œ
- en: \[ \frac{\partial L}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial L}{\partial O_{6_{\text{in}}}} = H_4 \cdot
    \frac{\partial L}{\partial O_{6_{\text{in}}}} = 0.42 \cdot 1.00 = 0.42 \]
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial L}{\partial O_{6_{\text{in}}}} = H_4 \cdot
    \frac{\partial L}{\partial O_{6_{\text{in}}}} = 0.42 \cdot 1.00 = 0.42 \]
- en: Hereâ€™s the loss derivatives with respect to connection weights for the other
    hidden layer to output node connection,
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å…¶ä»–éšè—å±‚åˆ°è¾“å‡ºèŠ‚ç‚¹è¿æ¥ç›¸å¯¹äºè¿æ¥æƒé‡çš„æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial L}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial L}{\partial O_{6_{\text{in}}}} = H_5 \cdot
    \frac{\partial L}{\partial O_{6_{\text{in}}}} = 0.60 \cdot 1.00 = 0.60 \]
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial L}{\partial O_{6_{\text{in}}}} = H_5 \cdot
    \frac{\partial L}{\partial O_{6_{\text{in}}}} = 0.60 \cdot 1.00 = 0.60 \]
- en: and now letâ€™s get all the input to hidden layer connections,
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è·å–æ‰€æœ‰è¾“å…¥åˆ°éšè—å±‚è¿æ¥çš„æ•°æ®ï¼Œ
- en: \[ \frac{\partial L}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_1 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.50 \cdot (-0.13) = -0.07 \]\[
    \frac{\partial L}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_1 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.50 \cdot (-0.10) = -0.05 \]\[
    \frac{\partial L}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_2 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.20 \cdot (-0.13) = -0.03 \]\[
    \frac{\partial L}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = I_2 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.20 \cdot (-0.10) = -0.02 \]\[
    \frac{\partial L}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_3 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.70 \cdot (-0.13) = -0.09 \]\[
    \frac{\partial L}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = I_3 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.70 \cdot (-0.10) = -0.07 \]
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_1 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.50 \cdot (-0.13) = -0.07 \]\[
    \frac{\partial L}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_1 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.50 \cdot (-0.10) = -0.05 \]\[
    \frac{\partial L}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_2 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.20 \cdot (-0.13) = -0.03 \]\[
    \frac{\partial L}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = I_2 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.20 \cdot (-0.10) = -0.02 \]\[
    \frac{\partial L}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_3 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.70 \cdot (-0.13) = -0.09 \]\[
    \frac{\partial L}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = I_3 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.70 \cdot (-0.10) = -0.07 \]
- en: This takes care of all of the connection weight error derivatives, now lets
    take care of the node bias error derivatives.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¤„ç†äº†æ‰€æœ‰è¿æ¥æƒé‡è¯¯å·®å¯¼æ•°ï¼Œç°åœ¨æˆ‘ä»¬æ¥å¤„ç†èŠ‚ç‚¹åå·®è¯¯å·®å¯¼æ•°ã€‚
- en: the node bias error derivatives are the same as the node peractivation error
    derivatives. Now letâ€™s calculate the bias terms in the hidden layer,
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹åå·®è¯¯å·®å¯¼æ•°ä¸èŠ‚ç‚¹æ¿€æ´»è¯¯å·®å¯¼æ•°ç›¸åŒã€‚ç°åœ¨æˆ‘ä»¬æ¥è®¡ç®—éšè—å±‚ä¸­çš„åå·®é¡¹ï¼Œ
- en: \[ \frac{\partial L}{\partial b_4} = \frac{\partial H_{4_{\text{in}}}}{\partial
    b_4} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = 1.0 \cdot (-0.13) =
    -0.13 \]\[ \frac{\partial L}{\partial b_5} = \frac{\partial H_{5_{\text{in}}}}{\partial
    b_5} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = 1.0 \cdot (-0.1) =
    -0.10 \]
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial b_4} = \frac{\partial H_{4_{\text{in}}}}{\partial
    b_4} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = 1.0 \cdot (-0.13) =
    -0.13 \]\[ \frac{\partial L}{\partial b_5} = \frac{\partial H_{5_{\text{in}}}}{\partial
    b_5} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = 1.0 \cdot (-0.1) =
    -0.10 \]
- en: Updating Model Parameters
  id: totrans-455
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ›´æ–°æ¨¡å‹å‚æ•°
- en: The loss derivatives with respect to each of the model parameters are the gradients,
    so we are ready to use gradient descent optimization with the addition of,
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæ¯ä¸ªæ¨¡å‹å‚æ•°çš„æŸå¤±å¯¼æ•°æ˜¯æ¢¯åº¦ï¼Œå› æ­¤æˆ‘ä»¬å‡†å¤‡å¥½ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼Œå¹¶æ·»åŠ ï¼Œ
- en: '**learning rate** - to scale the rate of change of the model updates we assign
    a learning rate, \(\eta\). For our model parameter examples from above,'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡** - ä¸ºäº†ç¼©æ”¾æ¨¡å‹æ›´æ–°çš„å˜åŒ–ç‡ï¼Œæˆ‘ä»¬åˆ†é…ä¸€ä¸ªå­¦ä¹ ç‡ï¼Œ\(\eta\)ã€‚å¯¹äºä¸Šé¢æˆ‘ä»¬çš„æ¨¡å‹å‚æ•°ç¤ºä¾‹ï¼Œ'
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{4,6}} \]\[ \lambda_{1,4}^{\ell} = \lambda_{1,4}^{\ell - 1}
    + \eta \cdot \frac{\partial L}{\partial \lambda_{1,4}} \]\[ b_j^{\ell} = b_j^{\ell
    - 1} + \eta \cdot \frac{\partial L}{\partial b_j} \]
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{4,6}} \]\[ \lambda_{1,4}^{\ell} = \lambda_{1,4}^{\ell - 1}
    + \eta \cdot \frac{\partial L}{\partial \lambda_{1,4}} \]\[ b_j^{\ell} = b_j^{\ell
    - 1} + \eta \cdot \frac{\partial L}{\partial b_j} \]
- en: recall, this process of gradient calculation and model parameters, weights and
    biases, updating is iterated and is known as gradient descent optimization.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œè¿™ä¸ªæ¢¯åº¦è®¡ç®—å’Œæ¨¡å‹å‚æ•°ã€æƒé‡å’Œåå·®æ›´æ–°çš„è¿‡ç¨‹æ˜¯è¿­ä»£çš„ï¼Œè¢«ç§°ä¸ºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚
- en: the goal is to explore the loss hypersurface, avoiding and escaping local minimums
    and ultimately finding the global minimum.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯æ¢ç´¢æŸå¤±è¶…æ›²é¢ï¼Œé¿å…å’Œé€ƒç¦»å±€éƒ¨æœ€å°å€¼ï¼Œæœ€ç»ˆæ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚
- en: learning rate, also known as step size is commonly set between 0.0 and 1.0,
    note 0.01 is the default in Keras module of TensorFlow
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡ï¼Œä¹Ÿç§°ä¸ºæ­¥é•¿ï¼Œé€šå¸¸è®¾ç½®åœ¨ 0.0 å’Œ 1.0 ä¹‹é—´ï¼Œæ³¨æ„åœ¨ TensorFlow çš„ Keras æ¨¡å—ä¸­é»˜è®¤å€¼ä¸º 0.01
- en: '**Low Learning Rate** â€“ more stable, but a slower solution, may get stuck in
    a local minimum'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½å­¦ä¹ ç‡** â€“ æ›´ç¨³å®šï¼Œä½†è§£çš„é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€å°å€¼'
- en: '**High Learning Rate** â€“ may be unstable, but perhaps a faster solution, may
    diverge out of the global minimum'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é«˜å­¦ä¹ ç‡** â€“ å¯èƒ½ä¸ç¨³å®šï¼Œä½†å¯èƒ½æ˜¯ä¸€ä¸ªæ›´å¿«çš„è§£ï¼Œå¯èƒ½ä¼šåç¦»å…¨å±€æœ€å°å€¼'
- en: One strategy is to start with a high learning rate and then to decrease the
    learning rate over the iterations
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§ç­–ç•¥æ˜¯å¼€å§‹ä½¿ç”¨é«˜å­¦ä¹ ç‡ï¼Œç„¶ååœ¨è¿­ä»£è¿‡ç¨‹ä¸­é™ä½å­¦ä¹ ç‡
- en: '**Learning Rate Decay** - set as > 0 to avoid mitigate oscillations,'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡è¡°å‡** - è®¾ç½®ä¸º > 0 ä»¥é¿å…å‡è½»æŒ¯è¡ï¼Œ'
- en: \[ \eta^{\ell} = \eta^{\ell - 1} \cdot \left( \frac{1}{1 + \text{decay} \cdot
    \ell} \right) \]
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \eta^{\ell} = \eta^{\ell - 1} \cdot \left( \frac{1}{1 + \text{decay} \cdot
    \ell} \right) \]
- en: where \(\ell\) is the model training epoch
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ \(\ell\) æ˜¯æ¨¡å‹è®­ç»ƒè½®æ¬¡
- en: Notice that the model parameter updates are for a single training data case?
    Consider this single model parameter,
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæ¨¡å‹å‚æ•°æ›´æ–°æ˜¯é’ˆå¯¹å•ä¸ªè®­ç»ƒæ•°æ®æ¡ˆä¾‹çš„ï¼Ÿè€ƒè™‘è¿™ä¸ªå•ä¸ªæ¨¡å‹å‚æ•°ï¼Œ
- en: we calculate the update over all samples in the batch and apply the average
    of the updates.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—æ‰¹ä¸­æ‰€æœ‰æ ·æœ¬çš„æ›´æ–°ï¼Œå¹¶åº”ç”¨æ›´æ–°çš„å¹³å‡å€¼ã€‚
- en: \[ \frac{\partial L}{\partial \lambda_{4,6}} = H_4 \cdot \frac{\partial L}{\partial
    O_{6_{\text{in}}}} = 0.42 \cdot 1.00 = 0.42 \]
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial \lambda_{4,6}} = H_4 \cdot \frac{\partial L}{\partial
    O_{6_{\text{in}}}} = 0.42 \cdot 1.00 = 0.42 \]
- en: is applied to update the \(\lambda_{4,6}\) parameter as,
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨äºæ›´æ–° \(\lambda_{4,6}\) å‚æ•°ï¼Œ
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{4,6}} \]
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{4,6}} \]
- en: is dependent on \(H_4\) node output, and \(L\) error that are for a single sample,
    \(ğ‘¥_1,\ldots,ğ‘¥_ğ‘š\) and \(ğ‘¦\); therefore, we cannot calculate a single parameter
    update over all our \(1,\ldots,n\) training data samples.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾èµ–äº \(H_4\) èŠ‚ç‚¹è¾“å‡ºå’Œ \(L\) é”™è¯¯ï¼Œè¿™äº›æ˜¯é’ˆå¯¹å•ä¸ªæ ·æœ¬çš„ï¼Œ\(ğ‘¥_1,\ldots,ğ‘¥_ğ‘š\) å’Œ \(ğ‘¦\)ï¼›å› æ­¤ï¼Œæˆ‘ä»¬ä¸èƒ½è®¡ç®—æ‰€æœ‰
    \(1,\ldots,n\) è®­ç»ƒæ•°æ®æ ·æœ¬çš„å•ä¸ªå‚æ•°æ›´æ–°ã€‚
- en: instead we can calculate \(1,\ldots,n\) updates and then apply the average of
    all the updates to our model parameters,
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è€Œæˆ‘ä»¬å¯ä»¥è®¡ç®— \(1,\ldots,n\) æ¬¡æ›´æ–°ï¼Œç„¶åå°†æ‰€æœ‰æ›´æ–°çš„å¹³å‡å€¼åº”ç”¨åˆ°æˆ‘ä»¬çš„æ¨¡å‹å‚æ•°ä¸Šï¼Œ
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \frac{1}{n_{batch}} \sum_{i=1}^{n_{batch}}
    \eta \cdot \frac{\partial L}{\partial \lambda_{4,6}} \]
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \frac{1}{n_{batch}} \sum_{i=1}^{n_{batch}}
    \eta \cdot \frac{\partial L}{\partial \lambda_{4,6}} \]
- en: since the learning rate is a constant, we can move it out of the sum and now
    we are averaging the gradients,
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºå­¦ä¹ ç‡æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ç§»å‡ºæ±‚å’Œç¬¦å·ä¹‹å¤–ï¼Œç°åœ¨æˆ‘ä»¬æ­£åœ¨å¹³å‡æ¢¯åº¦ï¼Œ
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \frac{1}{n_{batch}}
    \sum_{i=1}^{n_{batch}} \frac{\partial L}{\partial \lambda_{4,6}} \]
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \frac{1}{n_{batch}}
    \sum_{i=1}^{n_{batch}} \frac{\partial L}{\partial \lambda_{4,6}} \]
- en: Training Epochs
  id: totrans-478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒè½®æ¬¡
- en: This is a good time to talk about stochastic gradient descent optimization,
    first letâ€™s define some common terms,
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯è®¨è®ºéšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–çš„å¥½æ—¶æœºï¼Œé¦–å…ˆè®©æˆ‘ä»¬å®šä¹‰ä¸€äº›å¸¸è§æœ¯è¯­ï¼Œ
- en: '**Batch Gradient Descent** - updates the model parameters after passing through
    all of the data'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰¹é‡æ¢¯åº¦ä¸‹é™** - åœ¨é€šè¿‡æ‰€æœ‰æ•°æ®åæ›´æ–°æ¨¡å‹å‚æ•°'
- en: '**Stochastic Gradient Descent** - updates the model parameters over each sample
    data'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éšæœºæ¢¯åº¦ä¸‹é™** - åœ¨æ¯ä¸ªæ ·æœ¬æ•°æ®ä¸Šæ›´æ–°æ¨¡å‹å‚æ•°'
- en: '**Mini-batch Gradient Descent** - updates the model parameter after passing
    through a single batch'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°æ‰¹é‡æ¢¯åº¦ä¸‹é™** - åœ¨é€šè¿‡å•ä¸ªæ‰¹æ¬¡åæ›´æ–°æ¨¡å‹å‚æ•°'
- en: With mini-batch gradient descent stochasticity is introduced through the use
    of subsets of the data, known as batches,
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ•°æ®å­é›†ï¼Œç§°ä¸ºæ‰¹æ¬¡ï¼Œé€šè¿‡å¼•å…¥éšæœºæ€§æ¥ä½¿ç”¨å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼Œ
- en: for example, if we divide our 100 samples into 4 batches, then we iterate over
    each batch separately
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æŠŠæˆ‘ä»¬çš„ 100 ä¸ªæ ·æœ¬åˆ†æˆ 4 æ‰¹æ¬¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å•ç‹¬éå†æ¯ä¸€æ‰¹
- en: we speed up the individual updates, fewer data are faster to calculate, but
    we introduce more error
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åŠ å¿«äº†å•ä¸ªæ›´æ–°ï¼Œæ›´å°‘çš„æ•°æ®æ›´å¿«åœ°è®¡ç®—ï¼Œä½†å¼•å…¥äº†æ›´å¤šçš„é”™è¯¯
- en: this often helps the training explore for the global minimum and avoid getting
    stuck in local minimums and along ridges in the loss hypersurface
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™é€šå¸¸æœ‰åŠ©äºè®­ç»ƒæ¢ç´¢å…¨å±€æœ€å°å€¼ï¼Œå¹¶é¿å…é™·å…¥å±€éƒ¨æœ€å°å€¼å’ŒæŸå¤±è¶…æ›²é¢ä¸Šçš„è„Š
- en: Finally our last definition here,
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå®šä¹‰çš„æœ€åä¸€ä¸ªå®šä¹‰ï¼Œ
- en: '**epoch** - is one pass over all of the data, so that would be 4 iterations
    of updating the model parameters if we have 4 mini-batches'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è½®æ¬¡** - æ˜¯å¯¹å…¨éƒ¨æ•°æ®çš„å•æ¬¡éå†ï¼Œæ‰€ä»¥å¦‚æœæœ‰ 4 ä¸ªå°æ‰¹é‡ï¼Œé‚£ä¹ˆæ›´æ–°æ¨¡å‹å‚æ•°çš„è¿­ä»£æ¬¡æ•°å°†æ˜¯ 4 æ¬¡'
- en: There are many other considerations that I will add later including,
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜ä¼šæ·»åŠ è®¸å¤šå…¶ä»–è€ƒè™‘å› ç´ ï¼ŒåŒ…æ‹¬ï¼Œ
- en: momentum
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ¨é‡
- en: adaptive optimization
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”ä¼˜åŒ–
- en: Now letâ€™s build the above artificial neural network by-hand and visualize the
    solution!
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ‰‹åŠ¨æ„å»ºä¸Šè¿°äººå·¥ç¥ç»ç½‘ç»œå¹¶å¯è§†åŒ–è§£å†³æ–¹æ¡ˆï¼
- en: this is by-hand so that you can see every calculation. I intentionally avoided
    using TensorFlow or PyTorch.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”±æ‰‹ç»˜çš„ï¼Œè¿™æ ·ä½ å¯ä»¥çœ‹åˆ°æ¯ä¸€ä¸ªè®¡ç®—ã€‚æˆ‘æ•…æ„é¿å…ä½¿ç”¨TensorFlowæˆ–PyTorchã€‚
- en: Interactive Dashboard
  id: totrans-494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº¤äº’å¼ä»ªè¡¨æ¿
- en: I built out an interactive Python dashboard with the code below for training
    an artificial neural network. You can step through the training iteration and
    observe over the training epochs,
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨ä¸‹é¢çš„ä»£ç æ„å»ºäº†ä¸€ä¸ªäº¤äº’å¼Pythonä»ªè¡¨æ¿ï¼Œç”¨äºè®­ç»ƒäººå·¥ç¥ç»ç½‘ç»œã€‚ä½ å¯ä»¥é€æ­¥é€šè¿‡è®­ç»ƒè¿­ä»£ï¼Œå¹¶åœ¨è®­ç»ƒæ—¶é—´æ­¥é•¿ä¸­è§‚å¯Ÿï¼Œ
- en: model parameters
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°
- en: forward pass predictions
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‰å‘ä¼ é€’é¢„æµ‹
- en: backpropagation of error derivatives
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é”™è¯¯å¯¼æ•°çš„åå‘ä¼ æ’­
- en: If you would like to see artificial neural networks in action, check out my
    [ANN interactive Python dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_ANN.ipynb),
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³è¦çœ‹åˆ°äººå·¥ç¥ç»ç½‘ç»œçš„å®é™…åº”ç”¨ï¼Œè¯·æŸ¥çœ‹æˆ‘çš„[ANNäº¤äº’å¼Pythonä»ªè¡¨æ¿](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_ANN.ipynb)ï¼Œ
- en: '![](../Images/ab7e3b58fcfdbf8f7a2c19b5d78c7736.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/ab7e3b58fcfdbf8f7a2c19b5d78c7736.png)'
- en: Interactive artificial neural network training Python dashboard.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤äº’å¼äººå·¥ç¥ç»ç½‘ç»œè®­ç»ƒPythonä»ªè¡¨æ¿ã€‚
- en: Import Required Packages
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¼å…¥æ‰€éœ€çš„åŒ…
- en: We will also need some standard packages. These should have been installed with
    Anaconda 3.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸Anaconda 3ä¸€èµ·å®‰è£…ã€‚
- en: '[PRE0]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing â€˜python -m pip install [package-name]â€™. More assistance is available
    with the respective package docs.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½éœ€è¦é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨Windowsä¸Šæ‰“å¼€å‘½ä»¤çª—å£ï¼Œç„¶åè¾“å…¥â€˜python -m pip install
    [package-name]â€™æ¥å®Œæˆã€‚æ›´å¤šå¸®åŠ©å¯ä»¥åœ¨ç›¸åº”åŒ…çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ã€‚
- en: Declare Functions
  id: totrans-506
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å£°æ˜å‡½æ•°
- en: Hereâ€™s the functions to make, train and visualize our artificial neural network.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯åˆ›å»ºã€è®­ç»ƒå’Œå¯è§†åŒ–æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œçš„å‡½æ•°ã€‚
- en: '[PRE1]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Simple ANN
  id: totrans-509
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç®€å•ANN
- en: 'I wrote this code to specify a simple ANN:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç¼–å†™äº†è¿™æ®µä»£ç æ¥æŒ‡å®šä¸€ä¸ªç®€å•çš„ANNï¼š
- en: three input nodes, 2 hidden nodes and 1 output node
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‰ä¸ªè¾“å…¥èŠ‚ç‚¹ã€2ä¸ªéšè—èŠ‚ç‚¹å’Œ1ä¸ªè¾“å‡ºèŠ‚ç‚¹
- en: 'and to train the ANN by iteratively performing the forward calculation and
    backpropagation. I calculate:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿­ä»£æ‰§è¡Œå‰å‘è®¡ç®—å’Œåå‘ä¼ æ’­æ¥è®­ç»ƒANNã€‚æˆ‘è®¡ç®—ï¼š
- en: the error and then propagate it to each node
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é”™è¯¯ç„¶åä¼ æ’­åˆ°æ¯ä¸ªèŠ‚ç‚¹
- en: solve for the partial derivatives of the error with respect to each weight and
    bias
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ±‚è§£è¯¯å·®å¯¹æ¯ä¸ªæƒé‡å’Œåå·®çš„åå¯¼æ•°
- en: all weights, biases and partial derivatives for all epoch are recorded in vectors
    for plotting
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æƒé‡ã€åå·®å’Œæ‰€æœ‰æ—¶é—´æ­¥é•¿çš„åå¯¼æ•°éƒ½è®°å½•åœ¨å‘é‡ä¸­ä»¥ä¾¿ç»˜å›¾
- en: '[PRE2]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now Visualize the Network for a Specific Epoch
  id: totrans-517
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç°åœ¨å¯è§†åŒ–ç‰¹å®šæ—¶é—´æ­¥é•¿çš„ç½‘ç»œ
- en: I wrote a custom network visualization below, select iepoch and visualize the
    artificial neural network for a specific epoch.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸‹é¢å†™äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„ç½‘ç»œå¯è§†åŒ–ï¼Œé€‰æ‹©iepochå¹¶å¯è§†åŒ–ç‰¹å®šæ—¶é—´æ­¥é•¿çš„äººå·¥ç¥ç»ç½‘ç»œã€‚
- en: '[PRE3]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![_images/f1c2dc9fc05ec8a3d07b5642c59c42dd91a2b2ed35c547faa409371a1e776270.png](../Images/6c1b34f06ebf3ebfafbbd206fc1033ca.png)'
  id: totrans-520
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/6c1b34f06ebf3ebfafbbd206fc1033ca.png)'
- en: Check the ANN Convergence
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ANNæ”¶æ•›æ€§
- en: Now we plot the weights, biases and prediction over the epochs to check the
    training convergence.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç»˜åˆ¶æƒé‡ã€åå·®å’Œé¢„æµ‹å€¼éšæ—¶é—´æ­¥é•¿çš„å˜åŒ–ï¼Œä»¥æ£€æŸ¥è®­ç»ƒçš„æ”¶æ•›æ€§ã€‚
- en: '[PRE4]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![_images/96a2f9a7007123700054af340b223a0c9017cbf262afe43f97c8f1b445ec44a6.png](../Images/41e5da630bdb6e70068ef584a2fc3d47.png)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/41e5da630bdb6e70068ef584a2fc3d47.png)'
- en: Comments
  id: totrans-525
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ³¨é‡Š
- en: This was a basic treatment of artificial neural networks. Much more could be
    done and discussed, I have many more resources. Check out my [shared resource
    inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture links
    at the start of this chapter with resource links in the videosâ€™ descriptions.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¯¹äººå·¥ç¥ç»ç½‘ç»œçš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šå…¶ä»–èµ„æºã€‚è¯·æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„YouTubeè®²åº§é“¾æ¥ã€‚
- en: I hope this is helpful,
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ
- en: '*Michael*'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”*'
- en: About the Author
  id: totrans-529
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºä½œè€…
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡40è‹±äº©æ ¡å›­å†…ï¼Œè¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆçš„åŠå…¬å®¤ã€‚
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·çš®å°”å¥‡æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[Cockrellå·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œåœ¨é‚£é‡Œä»–ä»äº‹å’Œæ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜ï¼Œ
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ˜¯[èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œå¹¶åœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤æ‹…ä»»æ ¸å¿ƒæ•™å‘˜
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚'
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”å·²ç»æ’°å†™äº†è¶…è¿‡70ç¯‡[åŒè¡Œè¯„å®¡çš„å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[PythonåŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[åœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[Pythonåº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPyå®è·µæŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[Pythonåº”ç”¨æœºå™¨å­¦ä¹ ï¼šä»£ç å®è·µæŒ‡å—](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚
- en: All of Michaelâ€™s university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michaelâ€™s work and shared educational resources visit his
    Website.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTubeé¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å«100å¤šä¸ªPythonäº¤äº’å¼ä»ªè¡¨æ¿å’Œ40å¤šä¸ªGitHubä»“åº“ä¸­çš„è¯¦ç»†æ–‡æ¡£å·¥ä½œæµç¨‹ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚
- en: Want to Work Together?
  id: totrans-537
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒ³è¦ä¸€èµ·å·¥ä½œå—ï¼Ÿ
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™ä¸ªå†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚åŠ ã€‚
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? Iâ€™d be happy to drop by and work with you!
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„Ÿå…´è¶£åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å¯ä»¥é€šè¿‡[mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu)è”ç³»ã€‚
- en: Iâ€™m always happy to discuss,
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ
- en: '*Michael*'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”*'
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: Michael Pyrczï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡Cockrellå·¥ç¨‹å­¦é™¢å’ŒJacksonåœ°çƒç§‘å­¦å­¦é™¢
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥è·å–ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Pythonä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Pythonä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
- en: Motivation
  id: totrans-546
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ¨æœº
- en: Artificial neural networks are very powerful, nature inspired computing based
    on an analogy of brain
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œéå¸¸å¼ºå¤§ï¼Œæ˜¯åŸºäºå¤§è„‘ç±»æ¯”çš„è‡ªç„¶å¯å‘è®¡ç®—
- en: I suggest that they are like a reptilian brain, without planning and higher
    order reasoning
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºå®ƒä»¬å°±åƒçˆ¬è¡ŒåŠ¨ç‰©çš„å¤§è„‘ï¼Œæ²¡æœ‰è®¡åˆ’å’Œé«˜çº§æ¨ç†
- en: In addition, artificial neural networks are a building block of many other deep
    learning methods, for example,
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œäººå·¥ç¥ç»ç½‘ç»œæ˜¯è®¸å¤šå…¶ä»–æ·±åº¦å­¦ä¹ æ–¹æ³•çš„åŸºçŸ³ï¼Œä¾‹å¦‚ï¼Œ
- en: convolutional neural networks
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œ
- en: recurrent neural networks
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¾ªç¯ç¥ç»ç½‘ç»œ
- en: generative adversarial networks
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ
- en: autoencoders
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨ç¼–ç å™¨
- en: Nature inspired computing is looking to nature for inspiration to develop novel
    problem-solving methods,
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç„¶å¯å‘è®¡ç®—æ­£åœ¨å¯»æ‰¾è‡ªç„¶ç•Œä¸­çš„çµæ„Ÿæ¥å¼€å‘æ–°çš„é—®é¢˜è§£å†³æ–¹æ³•ï¼Œ
- en: '**artificial neural networks** are inspired by biological neural networks'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**äººå·¥ç¥ç»ç½‘ç»œ**æ˜¯å—ç”Ÿç‰©ç¥ç»ç½‘ç»œå¯å‘çš„'
- en: '**nodes** - in our model are artificial neurons, simple processors'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠ‚ç‚¹** - åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­æ˜¯äººå·¥ç¥ç»å…ƒï¼Œç®€å•çš„å¤„ç†å™¨'
- en: '**connections** between nodes are artificial synapses'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥æ˜¯äººå·¥çªè§¦**'
- en: intelligence emerges from many connected simple processors. For the remainder
    of this chapter, I will used the terms nodes and connections to describe our artificial
    neural network.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: æ™ºèƒ½æ˜¯ä»è®¸å¤šè¿æ¥çš„ç®€å•å¤„ç†å™¨ä¸­äº§ç”Ÿçš„ã€‚åœ¨æœ¬ç« çš„å‰©ä½™éƒ¨åˆ†ï¼Œæˆ‘å°†ä½¿ç”¨èŠ‚ç‚¹å’Œè¿æ¥è¿™ä¸¤ä¸ªæœ¯è¯­æ¥æè¿°æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œã€‚
- en: Neural Network Concepts
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ¦‚å¿µ
- en: Here are some key aspects of artificial neural networks,
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯äººå·¥ç¥ç»ç½‘ç»œçš„ä¸€äº›å…³é”®æ–¹é¢ï¼Œ
- en: '**Basic Design** - *â€œâ€¦a computing system made up of a number of simple, highly
    interconnected processing elements, which process information by their dynamic
    state response to external inputs.â€* Caudill (1989).'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºæœ¬è®¾è®¡** - *â€œâ€¦ä¸€ä¸ªç”±è®¸å¤šç®€å•ã€é«˜åº¦äº’è”çš„å¤„ç†å…ƒç´ ç»„æˆçš„è®¡ç®—ç³»ç»Ÿï¼Œå®ƒä»¬é€šè¿‡å¯¹å¤–éƒ¨è¾“å…¥çš„åŠ¨æ€çŠ¶æ€å“åº”æ¥å¤„ç†ä¿¡æ¯ã€‚â€* Caudill (1989)ã€‚'
- en: '**Still a Prediction Model** - while these models may be quite complicated
    with even millions of trainable model parameters, weights and biases, they are
    still a function that maps from predictor features to response features,'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»ç„¶æ˜¯ä¸€ä¸ªé¢„æµ‹æ¨¡å‹** - è™½ç„¶è¿™äº›æ¨¡å‹å¯èƒ½éå¸¸å¤æ‚ï¼Œç”šè‡³æœ‰æ•°ç™¾ä¸‡ä¸ªå¯è®­ç»ƒçš„æ¨¡å‹å‚æ•°ã€æƒé‡å’Œåå·®ï¼Œä½†å®ƒä»¬ä»ç„¶æ˜¯ä¸€ä¸ªä»é¢„æµ‹ç‰¹å¾æ˜ å°„åˆ°å“åº”ç‰¹å¾çš„å‡½æ•°ï¼Œ'
- en: \[ Y=f(X)+\epsilon \]
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Y=f(X)+\epsilon \]
- en: '**Supervised learning** â€“ we provide training data with predictor features,
    \(X_1,\ldots,ğ‘‹_ğ‘š\) and response feature(s), \(ğ‘Œ_1,\ldots,ğ‘Œ_K\), with the expectation
    of some model prediction error, \(\epsilon\).'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›‘ç£å­¦ä¹ ** â€“ æˆ‘ä»¬æä¾›å¸¦æœ‰é¢„æµ‹ç‰¹å¾çš„æ•°æ®é›†ï¼Œ\(X_1,\ldots,ğ‘‹_ğ‘š\) å’Œå“åº”ç‰¹å¾ï¼ˆsï¼‰ï¼Œ\(ğ‘Œ_1,\ldots,ğ‘Œ_K\)ï¼ŒæœŸæœ›æŸç§æ¨¡å‹é¢„æµ‹è¯¯å·®ï¼Œ\(\epsilon\)ã€‚'
- en: '**Nonlinearity** - nonlinearity is imparted to the system through the application
    of nonlinear activation functions to model nonlinear relationships'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '**éçº¿æ€§** - é€šè¿‡åº”ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°æ¥æ¨¡å‹éçº¿æ€§å…³ç³»ï¼Œå°†éçº¿æ€§å¼•å…¥ç³»ç»Ÿ'
- en: '**Universal Function Approximator (Universal Approximation Theorem)** - ANNs
    have the ability to learn any possible function shape of \(f\) over an interval,
    for an arbitrary wide (single hidden layer) by Cybenko (1989) and arbitrary depth
    by Lu and others (2017)'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '**é€šç”¨å‡½æ•°é€¼è¿‘å™¨ï¼ˆé€šç”¨é€¼è¿‘å®šç†ï¼‰** - ANNsèƒ½å¤Ÿå­¦ä¹ åœ¨ä»»æ„å®½ï¼ˆå•éšè—å±‚ï¼‰å’Œä»»æ„æ·±åº¦ï¼ˆLuç­‰äººï¼Œ2017ï¼‰çš„åŒºé—´å†…\(f\)çš„ä»»ä½•å¯èƒ½å‡½æ•°å½¢çŠ¶ï¼Œç”±Cybenko
    (1989) å’Œ Lu åŠå…¶åŒäº‹æå‡º'
- en: A Simple Network
  id: totrans-567
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„ç½‘ç»œ
- en: To get started, letâ€™s build a neural net, single hidden layer, fully connected,
    feed-forward neural network,
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹ï¼Œè®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå•éšè—å±‚ï¼Œå…¨è¿æ¥ï¼Œå‰é¦ˆç¥ç»ç½‘ç»œï¼Œ
- en: '![](../Images/832ee38f2f09d395115eb31b95358103.png)'
  id: totrans-569
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/832ee38f2f09d395115eb31b95358103.png)'
- en: Simple demonstration artificial neural network.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€å•æ¼”ç¤ºäººå·¥ç¥ç»ç½‘ç»œã€‚
- en: We use this example artificial neural network in the descriptions below and
    as an actual example that we will train and predict with by-hand!
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ä¸‹é¢çš„æè¿°ä¸­ä½¿ç”¨è¿™ä¸ªç¤ºä¾‹äººå·¥ç¥ç»ç½‘ç»œï¼Œå¹¶å°†å…¶ä½œä¸ºä¸€ä¸ªæˆ‘ä»¬å°†é€šè¿‡æ‰‹å·¥è®­ç»ƒå’Œé¢„æµ‹çš„å®é™…ç¤ºä¾‹ï¼
- en: Now letâ€™s label the parts of our network,
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ ‡æ³¨æˆ‘ä»¬ç½‘ç»œçš„éƒ¨åˆ†ï¼Œ
- en: '![](../Images/0be0dcf09e698cf22dc0881c818c0dbc.png)'
  id: totrans-573
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0be0dcf09e698cf22dc0881c818c0dbc.png)'
- en: Simple demonstration artificial neural network with the parts labeled, including
    3 inputs nodes, 2 hidden nodes and 1 output node fully connected.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡æ³¨äº†éƒ¨åˆ†çš„ç®€å•æ¼”ç¤ºäººå·¥ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬3ä¸ªè¾“å…¥èŠ‚ç‚¹ã€2ä¸ªéšè—èŠ‚ç‚¹å’Œ1ä¸ªå®Œå…¨è¿æ¥çš„è¾“å‡ºèŠ‚ç‚¹ã€‚
- en: Our artificial neural network has,
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œå…·æœ‰ï¼Œ
- en: 3 predictor features, \(X_1\), \(X_2\) and \(X_3\)
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œ\(X_1\)ã€\(X_2\) å’Œ \(X_3\)
- en: 3 input nodes, \(I_1\), \(I_2\) and \(I_3\)
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œ\(I_1\)ã€\(I_2\) å’Œ \(I_3\)
- en: 2 hidden layer nodes, \(H_4\) and \(H_5\)
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2ä¸ªéšè—å±‚èŠ‚ç‚¹ï¼Œ\(H_4\) å’Œ \(H_5\)
- en: 1 output node, \(O_6\)
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1ä¸ªè¾“å‡ºèŠ‚ç‚¹ï¼Œ\(O_6\)
- en: 1 response feature, \(Y_1\)
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1ä¸ªå“åº”ç‰¹å¾ï¼Œ\(Y_1\)
- en: where all nodes fully connected. Note, deep learning is a neural network with
    more than 1 hidden layer, but for brevity letâ€™s continue with our non-deep learning
    artificial neural network.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ‰€æœ‰èŠ‚ç‚¹éƒ½æ˜¯å®Œå…¨è¿æ¥çš„ã€‚æ³¨æ„ï¼Œæ·±åº¦å­¦ä¹ æ˜¯ä¸€ä¸ªå…·æœ‰è¶…è¿‡1ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œï¼Œä½†ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬ç»§ç»­ä½¿ç”¨æˆ‘ä»¬çš„éæ·±åº¦å­¦ä¹ äººå·¥ç¥ç»ç½‘ç»œã€‚
- en: Comments on Network Nomenclature
  id: totrans-582
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºç½‘ç»œå‘½åæ³•çš„æ³¨é‡Š
- en: Just a couple more comments about my network nomenclature. My goal is to maximize
    simplicity and clarity,
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæˆ‘çš„ç½‘ç»œå‘½åæ³•è¿˜æœ‰ä¸€äº›é¢å¤–çš„æ³¨é‡Šã€‚æˆ‘çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç®€æ´æ€§å’Œæ¸…æ™°åº¦ï¼Œ
- en: '**Network Nodes and Connections** - I choose to use unique numbers for all
    nodes, \(I_1\), \(I_2\), \(I_3\), \(H_4\), \(H_5\) and \(O_6\), instead of repeating
    numbers over each layer, \(I_1\), \(I_2\), \(I_3\), \(H_1\), \(H_2\), and \(O_1\)
    to simplify the notation for the weights; therefore, when I say \(\lambda_{1,4}\)
    you know exactly where this weight is applied in the network, from node \(I_1\)
    to node \(H_4\).'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç½‘ç»œèŠ‚ç‚¹å’Œè¿æ¥** - æˆ‘é€‰æ‹©ä¸ºæ‰€æœ‰èŠ‚ç‚¹ä½¿ç”¨å”¯ä¸€çš„æ•°å­—ï¼Œ\(I_1\)ã€\(I_2\)ã€\(I_3\)ã€\(H_4\)ã€\(H_5\) å’Œ \(O_6\)ï¼Œè€Œä¸æ˜¯åœ¨æ¯ä¸ªå±‚ä¸­é‡å¤æ•°å­—ï¼Œ\(I_1\)ã€\(I_2\)ã€\(I_3\)ã€\(H_1\)ã€\(H_2\)
    å’Œ \(O_1\)ï¼Œä»¥ç®€åŒ–æƒé‡çš„è¡¨ç¤ºï¼›å› æ­¤ï¼Œå½“æˆ‘è¯´ \(\lambda_{1,4}\) æ—¶ï¼Œä½ çŸ¥é“è¿™ä¸ªæƒé‡åœ¨ç½‘ç»œçš„å“ªä¸ªä½ç½®åº”ç”¨ï¼Œä»èŠ‚ç‚¹ \(I_1\) åˆ°èŠ‚ç‚¹
    \(H_4\)ã€‚'
- en: '**Node Outputs** - I use the node label to also describe the output from the
    node, for example \(O_6\) is the output node, \(O_6\), and also the signal or
    value output from node \(O_6\),'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠ‚ç‚¹è¾“å‡º** - æˆ‘ä½¿ç”¨èŠ‚ç‚¹æ ‡ç­¾æ¥æè¿°èŠ‚ç‚¹çš„è¾“å‡ºï¼Œä¾‹å¦‚ \(O_6\) æ˜¯è¾“å‡ºèŠ‚ç‚¹ï¼Œ\(O_6\)ï¼Œä»¥åŠä»èŠ‚ç‚¹ \(O_6\) è¾“å‡ºçš„ä¿¡å·æˆ–å€¼ï¼Œ'
- en: \[ O_6 = \sigma \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 \right)
    \]
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \sigma \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 \right)
    \]
- en: '**Pre- and Post-activation** - at our nodes \(H_4\), \(H_5\), and \(O_6\),
    we have the node input before activation and the node output after activation,
    I use the notation \(H_{4_{in}}\), \(H_{5_{in}}\), and \(O_{6_{in}}\) for the
    pre-activation input,'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‰æ¿€æ´»å’Œåæ¿€æ´»** - åœ¨æˆ‘ä»¬çš„èŠ‚ç‚¹ \(H_4\)ã€\(H_5\) å’Œ \(O_6\) ä¸Šï¼Œæˆ‘ä»¬æœ‰èŠ‚ç‚¹æ¿€æ´»å‰çš„è¾“å…¥å’Œæ¿€æ´»åçš„è¾“å‡ºï¼Œæˆ‘ä½¿ç”¨ \(H_{4_{in}}\)ã€\(H_{5_{in}}\)
    å’Œ \(O_{6_{in}}\) è¡¨ç¤ºå‰æ¿€æ´»è¾“å…¥ï¼Œ'
- en: \[ H_{4_{in}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 \]\[ H_{5_{in}} = \lambda_{1,5} \cdot I_1 + \lambda_{2,5} \cdot I_2
    + \lambda_{2,5} \cdot I_3 \]\[ O_{6_{in}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6}
    \cdot H_5 \]
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4_{in}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 \]\[ H_{5_{in}} = \lambda_{1,5} \cdot I_1 + \lambda_{2,5} \cdot I_2
    + \lambda_{2,5} \cdot I_3 \]\[ O_{6_{in}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6}
    \cdot H_5 \]
- en: \(\quad\) and \(H_4\), \(H_5\), and \(O_6\) for the post-activation node output.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: \(\quad\) ä»¥åŠ \(H_4\)ã€\(H_5\) å’Œ \(O_6\) ä½œä¸ºåæ¿€æ´»èŠ‚ç‚¹çš„è¾“å‡ºã€‚
- en: \[ H_{4} = \sigma \left( H_{4_{in}} \right) = \sigma \left( \lambda_{1,4} \cdot
    I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 \right) \]\[ H_{5} = \sigma
    \left( H_{5_{in}} \right) = \sigma \left( \lambda_{1,5} \cdot I_1 + \lambda_{2,5}
    \cdot I_2 + \lambda_{2,5} \cdot I_3 \right) \]\[ O_6 = \sigma \left( O_{6_{in}}
    \right) = \sigma \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 \right)
    \]
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4} = \sigma \left( H_{4_{in}} \right) = \sigma \left( \lambda_{1,4} \cdot
    I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 \right) \]\[ H_{5} = \sigma
    \left( H_{5_{in}} \right) = \sigma \left( \lambda_{1,5} \cdot I_1 + \lambda_{2,5}
    \cdot I_2 + \lambda_{2,5} \cdot I_3 \right) \]\[ O_6 = \sigma \left( O_{6_{in}}
    \right) = \sigma \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 \right)
    \]
- en: It is important to have clean, clear notation because with back propagation
    we have to step through the nodes, going from post-activation to pre-activation.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: æ¸…æ™°ã€æ¸…æ™°çš„ç¬¦å·å¾ˆé‡è¦ï¼Œå› ä¸ºåœ¨ä½¿ç”¨åå‘ä¼ æ’­æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»éå†èŠ‚ç‚¹ï¼Œä»åæ¿€æ´»åˆ°å‰æ¿€æ´»ã€‚
- en: often variables like \(z\) are applied for pre-activation in neural network
    literature, but I feel this is ambiguous and may cause confusion as we provide
    a nuts and bolts approach, explicitly describing every equation, to describe exactly
    how neural networks are trained and predict
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¥ç»ç½‘ç»œæ–‡çŒ®ä¸­ï¼Œç»å¸¸ä½¿ç”¨åƒ \(z\) è¿™æ ·çš„å˜é‡è¿›è¡Œé¢„æ¿€æ´»ï¼Œä½†æˆ‘æ„Ÿè§‰è¿™å¾ˆæ¨¡ç³Šï¼Œå¯èƒ½ä¼šå¼•èµ·æ··æ·†ï¼Œå› ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªèºä¸é’‰å’Œèºæ¯çš„æ–¹æ³•ï¼Œæ˜ç¡®æè¿°æ¯ä¸ªæ–¹ç¨‹ï¼Œä»¥ç²¾ç¡®æè¿°ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•è®­ç»ƒå’Œé¢„æµ‹çš„
- en: Description of the Network Approach
  id: totrans-593
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç½‘ç»œæ–¹æ³•æè¿°
- en: Letâ€™s talk about the network, the parts and how information flows through the
    network.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è°ˆè°ˆç½‘ç»œï¼Œå…¶ç»„æˆéƒ¨åˆ†ä»¥åŠä¿¡æ¯å¦‚ä½•é€šè¿‡ç½‘ç»œæµåŠ¨ã€‚
- en: '**Feed-forward** â€“ all information flows from left to right. Each node sends
    the same signal along the connections to all the nodes in the next layer,'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‰é¦ˆ** â€“ æ‰€æœ‰ä¿¡æ¯ä»å·¦åˆ°å³æµåŠ¨ã€‚æ¯ä¸ªèŠ‚ç‚¹é€šè¿‡è¿æ¥å°†ç›¸åŒçš„ä¿¡å·å‘é€åˆ°ä¸‹ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹ï¼Œ'
- en: '![](../Images/02be26cd4d5476666cd2e1a72c3005fe.png)'
  id: totrans-596
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02be26cd4d5476666cd2e1a72c3005fe.png)'
- en: Feed forward, fully connected, with each node sending the same signal to all
    the nodes in the next layer.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¦ˆï¼Œå…¨è¿æ¥ï¼Œæ¯ä¸ªèŠ‚ç‚¹å°†ç›¸åŒçš„ä¿¡å·å‘é€åˆ°ä¸‹ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹ã€‚
- en: '**Input Layer** - the input features are passed directly to the input nodes,
    in the case of continuous predictor features, there is one input node per feature
    and the features are,'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å…¥å±‚** - è¾“å…¥ç‰¹å¾ç›´æ¥ä¼ é€’åˆ°è¾“å…¥èŠ‚ç‚¹ï¼Œå¯¹äºè¿ç»­é¢„æµ‹ç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾æœ‰ä¸€ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œç‰¹å¾æ˜¯ï¼Œ'
- en: min / max normalization to a range \(\left[ âˆ’1,1 \right]\) or \(\left[ 0,1 \right]\)
    to improve activation function sensitivity and to remove the influence of scale
    differences in predictor features and to improve solution stability, i.e., smooth
    reduction in the training loss while training
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æœ€å°/æœ€å¤§å½’ä¸€åŒ–åˆ°èŒƒå›´ \(\left[ âˆ’1,1 \right]\) æˆ– \(\left[ 0,1 \right]\) ä»¥æé«˜æ¿€æ´»å‡½æ•°çš„æ•æ„Ÿæ€§ï¼Œå¹¶æ¶ˆé™¤é¢„æµ‹ç‰¹å¾ä¸­å°ºåº¦å·®å¼‚çš„å½±å“ï¼Œä»¥æé«˜è§£å†³æ–¹æ¡ˆçš„ç¨³å®šæ€§ï¼Œå³ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹³æ»‘å‡å°‘è®­ç»ƒæŸå¤±
- en: '![](../Images/cb20e85861a55df55677b55ce737bbd5.png)'
  id: totrans-600
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb20e85861a55df55677b55ce737bbd5.png)'
- en: Highlighting the input layer, the first layer that receives the normalized predictor
    features.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: çªå‡ºæ˜¾ç¤ºè¾“å…¥å±‚ï¼Œæ¥æ”¶å½’ä¸€åŒ–é¢„æµ‹ç‰¹å¾çš„ç¬¬ä¸€å±‚ã€‚
- en: In the case of categorical predictor features, we have one input node per each
    category for each predictor feature, i.e., after one-hot-encoding of the feature
    where each encoding is passed to a separate input node.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†ç±»é¢„æµ‹ç‰¹å¾ï¼Œæ¯ä¸ªé¢„æµ‹ç‰¹å¾æ¯ä¸ªç±»åˆ«éƒ½æœ‰ä¸€ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œå³ç‰¹å¾ç»è¿‡ç‹¬çƒ­ç¼–ç åï¼Œæ¯ä¸ªç¼–ç ä¼ é€’åˆ°ä¸€ä¸ªå•ç‹¬çš„è¾“å…¥èŠ‚ç‚¹ã€‚
- en: recall one-hot-encoding, 1 if the specific category, 0 otherwise, replaces the
    categorical feature with a binary vector with length as the number of categories.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›å¿†ä¸€ä¸‹ç‹¬çƒ­ç¼–ç ï¼Œå¦‚æœå±äºç‰¹å®šç±»åˆ«ï¼Œåˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0ï¼Œç”¨é•¿åº¦ç­‰äºç±»åˆ«æ•°é‡çš„äºŒè¿›åˆ¶å‘é‡æ›¿æ¢åˆ†ç±»ç‰¹å¾ã€‚
- en: '![](../Images/51d695aaaa36c3450f3997691fb0090f.png)'
  id: totrans-604
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51d695aaaa36c3450f3997691fb0090f.png)'
- en: The input layer of our artificial neural network highlighted. The first layer
    that receives one-hot-encoding of a single categorical predictor feature.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œè¾“å…¥å±‚è¢«çªå‡ºæ˜¾ç¤ºã€‚æ¥æ”¶å•ä¸ªåˆ†ç±»é¢„æµ‹ç‰¹å¾ç‹¬çƒ­ç¼–ç çš„ç¬¬ä¸€å±‚ã€‚
- en: we could also use a single input node per categorical predictor and assign thresholds
    to each categories, for example \(\left[ 0.0, 0.5, 1.0 \right]\) for 3 categories,
    but this assumes an ordinal categorical feature
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¸ºæ¯ä¸ªåˆ†ç±»é¢„æµ‹ç‰¹å¾ä½¿ç”¨å•ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œå¹¶ä¸ºæ¯ä¸ªç±»åˆ«åˆ†é…é˜ˆå€¼ï¼Œä¾‹å¦‚å¯¹äº3ä¸ªç±»åˆ«ï¼Œ\(\left[ 0.0, 0.5, 1.0 \right]\)ï¼Œä½†è¿™å‡è®¾åˆ†ç±»ç‰¹å¾æ˜¯æœ‰åºçš„
- en: '**Hidden Layer** - the input layer values \(I_1, I_2, I_2\) are weighted with
    learnable weights,'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '**éšè—å±‚** - è¾“å…¥å±‚å€¼ \(I_1, I_2, I_2\) ä¸å¯å­¦ä¹ çš„æƒé‡ç›¸ä¹˜ï¼Œ'
- en: \[ \lambda_{1,4}, \lambda_{2,4}, \lambda_{3,4}, \lambda_{1,5}, \lambda_{2,5},
    \lambda_{3,5} \]
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{1,4}, \lambda_{2,4}, \lambda_{3,4}, \lambda_{1,5}, \lambda_{2,5},
    \lambda_{3,5} \]
- en: in the hidden layer nodes, the weighted input layer values, \(\lambda_{1,4}
    \cdot I_1, \lambda_{2,4} \cdot I_2 \cdot I_2, \ldots, \lambda_{3,5} \cdot I_3\)
    are summed with the addition of a trainable bias term in each node, \(b_4\) and
    \(b_5\).
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨éšè—å±‚èŠ‚ç‚¹ä¸­ï¼ŒåŠ æƒè¾“å…¥å±‚å€¼ \(\lambda_{1,4} \cdot I_1, \lambda_{2,4} \cdot I_2 \cdot I_2,
    \ldots, \lambda_{3,5} \cdot I_3\) ä¸æ¯ä¸ªèŠ‚ç‚¹ä¸­å¯è®­ç»ƒåç½®é¡¹çš„åŠ å’Œç›¸åŠ ï¼Œ\(b_4\) å’Œ \(b_5\)ã€‚
- en: \[ H_{4_{in}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]\[ H_{5_{in}} = \lambda_{1,5} \cdot I_1 + \lambda_{2,5} \cdot
    I_2 + \lambda_{3,5} \cdot I_3 + b_5 \]
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4_{in}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]\[ H_{5_{in}} = \lambda_{1,5} \cdot I_1 + \lambda_{2,5} \cdot
    I_2 + \lambda_{3,5} \cdot I_3 + b_5 \]
- en: the nonlinear activation is applied,
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨éçº¿æ€§æ¿€æ´»ï¼Œ
- en: \[ H_{4} = \sigma \bigl( \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 +
    \lambda_{3,4} \cdot I_3 + b_4 \bigr) \]\[ H_{5} = \sigma \bigl( \lambda_{1,5}
    \cdot I_1 + \lambda_{2,5} \cdot I_2 + \lambda_{3,5} \cdot I_3 + b_5 \bigr) \]
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4} = \sigma \bigl( \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 +
    \lambda_{3,4} \cdot I_3 + b_4 \bigr) \]\[ H_{5} = \sigma \bigl( \lambda_{1,5}
    \cdot I_1 + \lambda_{2,5} \cdot I_2 + \lambda_{3,5} \cdot I_3 + b_5 \bigr) \]
- en: the output from the input layer nodes to all hidden layer nodes is contant (again,
    each node sends the same value to all nodes in the next layer)
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥å±‚èŠ‚ç‚¹åˆ°æ‰€æœ‰éšè—å±‚èŠ‚ç‚¹çš„è¾“å‡ºæ˜¯æ’å®šçš„ï¼ˆå†æ¬¡å¼ºè°ƒï¼Œæ¯ä¸ªèŠ‚ç‚¹å°†ç›¸åŒçš„å€¼å‘é€åˆ°ä¸‹ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹ï¼‰
- en: '![](../Images/e2342c26953974528579d98e504c15c8.png)'
  id: totrans-614
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2342c26953974528579d98e504c15c8.png)'
- en: The hidden layer of our artificial neural network highlighted. The input layer
    nodes' outputs are weighted and passed into the hidden layer nodes. The output
    from the hidden layer nodes to all output layer nodes is constant.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œéšè—å±‚è¢«çªå‡ºæ˜¾ç¤ºã€‚è¾“å…¥å±‚èŠ‚ç‚¹çš„è¾“å‡ºè¢«åŠ æƒå¹¶ä¼ é€’åˆ°éšè—å±‚èŠ‚ç‚¹ã€‚éšè—å±‚èŠ‚ç‚¹åˆ°æ‰€æœ‰è¾“å‡ºå±‚èŠ‚ç‚¹çš„è¾“å‡ºæ˜¯æ’å®šçš„ã€‚
- en: '**Output Layer** - for continuous response features there is one output node
    per normalized response feature. Once again the weighted linear combination of
    inputs plus a node bias are calculated,'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡ºå±‚** - å¯¹äºè¿ç»­å“åº”ç‰¹å¾ï¼Œæ¯ä¸ªå½’ä¸€åŒ–å“åº”ç‰¹å¾æœ‰ä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹ã€‚å†æ¬¡è®¡ç®—è¾“å…¥çš„åŠ æƒçº¿æ€§ç»„åˆåŠ ä¸ŠèŠ‚ç‚¹åç½®ï¼Œ'
- en: \[ O_{6_{in}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \]
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{in}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \]
- en: and then activation is applied, but for a continuous response feature, typically
    identity (linear) transform is applied,
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œä½†å¯¹äºè¿ç»­å“åº”ç‰¹å¾ï¼Œé€šå¸¸åº”ç”¨æ’ç­‰ï¼ˆçº¿æ€§ï¼‰å˜æ¢ï¼Œ
- en: \[ O_6 = \alpha \bigl( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \bigr) = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 = O_{6_{in}}
    \]
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \alpha \bigl( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \bigr) = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 = O_{6_{in}}
    \]
- en: backtransformation from normalized to original response feature(s) are then
    applied to recover the ultimate prediction
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶åä»å½’ä¸€åŒ–åˆ°åŸå§‹å“åº”ç‰¹å¾çš„åè½¬æ¢è¢«åº”ç”¨ï¼Œä»¥æ¢å¤æœ€ç»ˆçš„é¢„æµ‹
- en: as with continuous predictor features, min / max normalization is applied to
    continuous response features to a range [âˆ’1,1] or [0,1] to improve activation
    function sensitivity
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸è¿ç»­é¢„æµ‹ç‰¹å¾ä¸€æ ·ï¼Œå¯¹è¿ç»­å“åº”ç‰¹å¾åº”ç”¨æœ€å°/æœ€å¤§å½’ä¸€åŒ–ï¼Œå°†å…¶èŒƒå›´è°ƒæ•´ä¸º[âˆ’1,1]æˆ–[0,1]ï¼Œä»¥æé«˜æ¿€æ´»å‡½æ•°çš„æ•æ„Ÿæ€§
- en: '![](../Images/87d8ca10f44d6d5d97c7e6ced2d415e5.png)'
  id: totrans-622
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87d8ca10f44d6d5d97c7e6ced2d415e5.png)'
- en: The output layer of our artificial neural network highlighted. The hidden layer
    nodes' outputs are weighted and passed into the output layer nodes. The output
    from the hidden layer nodes is constant, but the weights vary over the hidden
    layer node to output layer node connections.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œè¾“å‡ºå±‚è¢«çªå‡ºæ˜¾ç¤ºã€‚éšè—å±‚èŠ‚ç‚¹çš„è¾“å‡ºè¢«åŠ æƒå¹¶ä¼ é€’åˆ°è¾“å‡ºå±‚èŠ‚ç‚¹ã€‚éšè—å±‚èŠ‚ç‚¹çš„è¾“å‡ºæ˜¯æ’å®šçš„ï¼Œä½†æƒé‡åœ¨éšè—å±‚èŠ‚ç‚¹åˆ°è¾“å‡ºå±‚èŠ‚ç‚¹çš„è¿æ¥ä¸­å˜åŒ–ã€‚
- en: In the case of a categorical response feature, once again one-hot-encoding is
    applied, therefore, there is one output node per category.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†ç±»å“åº”ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œå†æ¬¡åº”ç”¨one-hotç¼–ç ï¼Œå› æ­¤ï¼Œæ¯ä¸ªç±»åˆ«æœ‰ä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹ã€‚
- en: the prediction is the probability of each category
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢„æµ‹æ˜¯æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡
- en: '![](../Images/11c0969ef52f11e6929df9ff8e4d92c2.png)'
  id: totrans-626
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11c0969ef52f11e6929df9ff8e4d92c2.png)'
- en: Highlighting the input layer, the first layer that receives one-hot-encoding
    of a single categorical predictor feature.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: çªå‡ºæ˜¾ç¤ºè¾“å…¥å±‚ï¼Œè¿™æ˜¯æ¥æ”¶å•ä¸ªåˆ†ç±»é¢„æµ‹ç‰¹å¾one-hotç¼–ç çš„ç¬¬ä¸€å±‚ã€‚
- en: Walkthrough the Network
  id: totrans-628
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éå†ç½‘ç»œ
- en: Now we are ready to walkthough the artificial neural network.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½éå†äººå·¥ç¥ç»ç½‘ç»œã€‚
- en: we follow a single path to illustrate the precise calculations associated with
    making a prediction with an artificial neural network
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éµå¾ªå•ä¸€è·¯å¾„æ¥å±•ç¤ºä¸ä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹ç›¸å…³çš„ç²¾ç¡®è®¡ç®—
- en: The full forward pass is explained next.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªå°†è§£é‡Šå®Œæ•´çš„æ­£å‘ä¼ æ’­ã€‚
- en: '**Inside an Input Layer Node** - input layer nodes just pass the predictor
    features,'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å…¥å±‚èŠ‚ç‚¹å†…éƒ¨** - è¾“å…¥å±‚èŠ‚ç‚¹åªæ˜¯ä¼ é€’é¢„æµ‹ç‰¹å¾ï¼Œ'
- en: normalized continuous predictor feature value
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ‡å‡†åŒ–çš„è¿ç»­é¢„æµ‹ç‰¹å¾å€¼
- en: a single one-hot-encoding value [0 or 1] for categorical prediction features
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†ç±»é¢„æµ‹ç‰¹å¾ï¼Œæœ‰ä¸€ä¸ªå•ç‹¬çš„one-hotç¼–ç å€¼[0æˆ–1]
- en: into the hidden layer nodes, with general vector notation,
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: è¿›å…¥éšè—å±‚èŠ‚ç‚¹ï¼Œä½¿ç”¨ä¸€èˆ¬çš„å‘é‡è¡¨ç¤ºï¼Œ
- en: \[ I = X \]![](../Images/4e1c6258eff75f9d4ce767b39cd522fb.png)
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: \[ I = X \]![](../Images/4e1c6258eff75f9d4ce767b39cd522fb.png)
- en: Walkthrough of an artificial neural network, the input layer node receives one-hot-encoding
    of a single categorical predictor feature and passes it to all of the hidden layer
    nodes.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ï¼Œè¾“å…¥å±‚èŠ‚ç‚¹æ¥æ”¶å•ä¸ªåˆ†ç±»é¢„æµ‹ç‰¹å¾çš„one-hotç¼–ç ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™æ‰€æœ‰éšè—å±‚èŠ‚ç‚¹ã€‚
- en: We can generalize over all input layer nodes with,
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼å¯¹æ‰€æœ‰è¾“å…¥å±‚èŠ‚ç‚¹è¿›è¡Œæ³›åŒ–ï¼Œ
- en: \[ H_j = H_{j_{in}} = X_j \]
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_j = H_{j_{in}} = X_j \]
- en: '**Inside an Hidden Layer Node**'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '**éšè—å±‚èŠ‚ç‚¹å†…éƒ¨**'
- en: The hidden layer nodes are simple processors. The take linearly weighted combinations
    of inputs, add a node bias term and then nonlinearly transform the result, this
    transform is call the activation function, \(\alpha\).
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: éšè—å±‚èŠ‚ç‚¹æ˜¯ç®€å•çš„å¤„ç†å™¨ã€‚å®ƒä»¬å¯¹è¾“å…¥è¿›è¡Œçº¿æ€§åŠ æƒç»„åˆï¼Œæ·»åŠ èŠ‚ç‚¹åå·®é¡¹ï¼Œç„¶åéçº¿æ€§åœ°è½¬æ¢ç»“æœï¼Œè¿™ç§è½¬æ¢ç§°ä¸ºæ¿€æ´»å‡½æ•°ï¼Œ\(\alpha\)ã€‚
- en: indeed, a very simple processor!
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¡®å®ï¼Œä¸€ä¸ªéå¸¸ç®€å•çš„å¤„ç†å™¨ï¼
- en: through many interconnected nodes we gain a very flexible predictor, emergent
    ability to characterize complicated, nonlinear patterns.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡è®¸å¤šç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ï¼Œæˆ‘ä»¬è·å¾—ä¸€ä¸ªéå¸¸çµæ´»çš„é¢„æµ‹å™¨ï¼Œæ¶Œç°å‡ºè¡¨å¾å¤æ‚ã€éçº¿æ€§æ¨¡å¼çš„èƒ½åŠ›ã€‚
- en: Prior to activation we have,
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¿€æ´»ä¹‹å‰ï¼Œæˆ‘ä»¬æœ‰ï¼Œ
- en: \[ H_{4_{in}} = \sum_{i=1}^{3} \left( \lambda_{i,4} \cdot I_i \right) + b_4
    \]
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4_{in}} = \sum_{i=1}^{3} \left( \lambda_{i,4} \cdot I_i \right) + b_4
    \]
- en: and after activation we have,
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»åï¼Œæˆ‘ä»¬æœ‰ï¼Œ
- en: \[ H_4 = \alpha \left(H_{4_{in}} \right) \]
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_4 = \alpha \left(H_{4_{in}} \right) \]
- en: We can express the simple processor in the node with general vector notation
    as,
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨é€šç”¨å‘é‡ç¬¦å·åœ¨èŠ‚ç‚¹ä¸­è¡¨ç¤ºç®€å•çš„å¤„ç†å™¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œ
- en: \[ H_4 = \alpha \left(b_4 + \lambda_{j,4}^T I \right) \]![](../Images/b2f8e46ea497049f4b95c03b8812eea7.png)
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_4 = \alpha \left(b_4 + \lambda_{j,4}^T I \right) \]![](../Images/b2f8e46ea497049f4b95c03b8812eea7.png)
- en: Walkthrough of an artificial neural network, the hidden layer linearly weights
    the input from each input layer node, adds a node bias term and then applies an
    activation function and passes this to all nodes in the next layer, i.e., the
    output layer for our example artificial neural network.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œçš„æ¦‚è¿°ï¼Œéšè—å±‚çº¿æ€§åœ°åŠ æƒæ¯ä¸ªè¾“å…¥å±‚èŠ‚ç‚¹çš„è¾“å…¥ï¼Œæ·»åŠ èŠ‚ç‚¹åå·®é¡¹ï¼Œç„¶ååº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œå¹¶å°†æ­¤ä¼ é€’ç»™ä¸‹ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹ï¼Œå³æˆ‘ä»¬çš„ç¤ºä¾‹äººå·¥ç¥ç»ç½‘ç»œçš„è¾“å‡ºå±‚ã€‚
- en: We can generalize over all hidden layer nodes with,
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼å¯¹æ‰€æœ‰éšè—å±‚èŠ‚ç‚¹è¿›è¡Œæ³›åŒ–ï¼Œ
- en: \[ H_{j_{in}} = \sum_{i=1}^{|I|} \left( \lambda_{i,j} \cdot I_i \right) + b_j
    \]
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{j_{in}} = \sum_{i=1}^{|I|} \left( \lambda_{i,j} \cdot I_i \right) + b_j
    \]
- en: and after activation, the node output is,
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»åï¼ŒèŠ‚ç‚¹è¾“å‡ºæ˜¯ï¼Œ
- en: \[ H_{j_{in}} = \sigma \bigl( \sum_{i=1}^{|I|} \left( \lambda_{i,j} \cdot I_i
    \right) + b_j \bigr) \]
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{j_{in}} = \sigma \bigl( \sum_{i=1}^{|I|} \left( \lambda_{i,j} \cdot I_i
    \right) + b_j \bigr) \]
- en: '**Inside an Output Layer Node**'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡ºå±‚èŠ‚ç‚¹å†…éƒ¨**'
- en: The output layer nodes take linearly weighted combinations of nodesâ€™ inputs,
    adds a node bias term and then transforms the result with an activation function,
    \(\alpha\), same as the hidden layer nodes,
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå±‚èŠ‚ç‚¹å¯¹èŠ‚ç‚¹çš„è¾“å…¥è¿›è¡Œçº¿æ€§åŠ æƒç»„åˆï¼Œæ·»åŠ èŠ‚ç‚¹åå·®é¡¹ï¼Œç„¶åä½¿ç”¨ä¸éšè—å±‚èŠ‚ç‚¹ç›¸åŒçš„æ¿€æ´»å‡½æ•°ï¼Œ\(\alpha\)ï¼Œè¿›è¡Œè½¬æ¢ï¼Œ
- en: Prior to activation we have,
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¿€æ´»ä¹‹å‰ï¼Œæˆ‘ä»¬æœ‰ï¼Œ
- en: \[ O_{6_{in}} = \sum_{i=4}^{5} \left( \lambda_{i,6} \cdot H_i \right) + b_6
    \]
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{in}} = \sum_{i=4}^{5} \left( \lambda_{i,6} \cdot H_i \right) + b_6
    \]
- en: and after activation, assuming identity activation we have,
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»åï¼Œå‡è®¾ä½¿ç”¨æ’ç­‰æ¿€æ´»ï¼Œæˆ‘ä»¬æœ‰ï¼Œ
- en: \[ O_6 = \alpha \left(O_{6_{in}} \right) \]
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \alpha \left(O_{6_{in}} \right) \]
- en: We can express the simple processor in the node with general vector notation
    as,
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨é€šç”¨å‘é‡ç¬¦å·åœ¨èŠ‚ç‚¹ä¸­è¡¨ç¤ºç®€å•çš„å¤„ç†å™¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œ
- en: \[ O_6 = \alpha\left(b_6 + \lambda_{j,6}^T H\right) \]![](../Images/22c5dd7e545497f42cf0b949f0b92954.png)
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \alpha\left(b_6 + \lambda_{j,6}^T H\right) \]![](../Images/22c5dd7e545497f42cf0b949f0b92954.png)
- en: Walkthrough of an artificial neural network, the output layer linearly weights
    the input from each hidden layer node, adds a node bias term and then applies
    an activation function, typically linear for continuous response features and
    passes this as an output.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œçš„æ¦‚è¿°ï¼Œè¾“å‡ºå±‚çº¿æ€§åœ°åŠ æƒæ¯ä¸ªéšè—å±‚èŠ‚ç‚¹çš„è¾“å…¥ï¼Œæ·»åŠ èŠ‚ç‚¹åå·®é¡¹ï¼Œç„¶ååº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸å¯¹äºè¿ç»­å“åº”ç‰¹å¾æ˜¯çº¿æ€§çš„ï¼Œå¹¶å°†æ­¤ä½œä¸ºè¾“å‡ºä¼ é€’ã€‚
- en: and for categorical response features, softmax activation is commonly applied,
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†ç±»å“åº”ç‰¹å¾ï¼Œé€šå¸¸åº”ç”¨softmaxæ¿€æ´»ï¼Œ
- en: \[ O_j = \alpha(O_{j_{in}}) = \frac{e^{O_{j_{in}}}}{\sum_{\iota=1}^{K} e^{O_{\iota_{in}}}}
    \]![](../Images/ff80ab0dac26f531ce5cc5ce3cd9cf5d.png)
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_j = \alpha(O_{j_{in}}) = \frac{e^{O_{j_{in}}}}{\sum_{\iota=1}^{K} e^{O_{\iota_{in}}}}
    \]![](../Images/ff80ab0dac26f531ce5cc5ce3cd9cf5d.png)
- en: Walkthrough of an artificial neural network, the output layer linearly weights
    the input from each hidden layer node, adds a node bias term and then applies
    an activation function, typically linear for continuous response features and
    passes this as an output.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œçš„æ¦‚è¿°ï¼Œè¾“å‡ºå±‚çº¿æ€§åœ°åŠ æƒæ¯ä¸ªéšè—å±‚èŠ‚ç‚¹çš„è¾“å…¥ï¼Œæ·»åŠ èŠ‚ç‚¹åå·®é¡¹ï¼Œç„¶ååº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸å¯¹äºè¿ç»­å“åº”ç‰¹å¾æ˜¯çº¿æ€§çš„ï¼Œå¹¶å°†æ­¤ä½œä¸ºè¾“å‡ºä¼ é€’ã€‚
- en: softmax activation ensures that the output over all the output layer nodes are
    valid probabilities including,
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: softmaxæ¿€æ´»ç¡®ä¿æ‰€æœ‰è¾“å‡ºå±‚èŠ‚ç‚¹çš„è¾“å‡ºéƒ½æ˜¯æœ‰æ•ˆçš„æ¦‚ç‡ï¼ŒåŒ…æ‹¬ï¼Œ
- en: '**nonnegativity** - through the exponentiation'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éè´Ÿæ€§** - é€šè¿‡æŒ‡æ•°è¿ç®—'
- en: '**closure** - probabilities sum to 1.0 through the denominator normalizing
    the result'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é—­åˆ** - æ¦‚ç‡é€šè¿‡åˆ†æ¯å½’ä¸€åŒ–åæ±‚å’Œä¸º1.0'
- en: Note, for all future discussions and demonstrations, I assume a standardized
    continuous responce feature.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œåœ¨æ‰€æœ‰æœªæ¥çš„è®¨è®ºå’Œæ¼”ç¤ºä¸­ï¼Œæˆ‘å‡è®¾æœ‰ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¿ç»­å“åº”ç‰¹å¾ã€‚
- en: Network Forward Pass
  id: totrans-671
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç½‘ç»œå‰å‘ä¼ é€’
- en: Now that we have completed a walk-through of our network on a single path, letâ€™s
    combine all the paths through our network to demonstrate a complete forward pass
    through our artificial neural network.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†ä¸€æ¡è·¯å¾„ä¸Šå¯¹ç½‘ç»œçš„éå†ï¼Œè®©æˆ‘ä»¬ç»“åˆç½‘ç»œä¸­çš„æ‰€æœ‰è·¯å¾„æ¥å±•ç¤ºé€šè¿‡äººå·¥ç¥ç»ç½‘ç»œçš„ä¸€æ¬¡å®Œæ•´å‰å‘ä¼ é€’ã€‚
- en: this is the calculation required to make a prediction with out,
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è¿›è¡Œé¢„æµ‹æ‰€éœ€çš„è®¡ç®—ï¼Œ
- en: \[ O_6 = \sigma_{O_6} \bigl( \lambda_{4,6} \cdot \sigma_{H_4} \left( \lambda_{1,4}
    I_1 + \lambda_{2,4} I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot
    \sigma_{H_5} \left( \lambda_{1,5} I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3
    + b_5 \right) + b_6 \bigr) \]
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \sigma_{O_6} \bigl( \lambda_{4,6} \cdot \sigma_{H_4} \left( \lambda_{1,4}
    I_1 + \lambda_{2,4} I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot
    \sigma_{H_5} \left( \lambda_{1,5} I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3
    + b_5 \right) + b_6 \bigr) \]
- en: where the activation functions \(\sigma_{H_4}\) = \(\sigma_{H_5}\) = \(\sigma\)
    are sigmoid, and \(\sigma_{O_6}\) is linear (identity), so we could simplify the
    forward pass to,
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ¿€æ´»å‡½æ•°\(\sigma_{H_4}\) = \(\sigma_{H_5}\) = \(\sigma\)æ˜¯Sigmoidå‡½æ•°ï¼Œè€Œ\(\sigma_{O_6}\)æ˜¯çº¿æ€§ï¼ˆæ’ç­‰ï¼‰å‡½æ•°ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç®€åŒ–å‰å‘ä¼ é€’ä¸ºï¼Œ
- en: \[ O_6 = \lambda_{4,6} \cdot \sigma \left( \lambda_{1,4} I_1 + \lambda_{2,4}
    I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot \sigma \left( \lambda_{1,5}
    I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3 + b_5 \right) + b_6 \]
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \lambda_{4,6} \cdot \sigma \left( \lambda_{1,4} I_1 + \lambda_{2,4}
    I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot \sigma \left( \lambda_{1,5}
    I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3 + b_5 \right) + b_6 \]
- en: This emphasizes that our neural network is a nested set of activated linear
    systems, i.e., linearly weighted averages plus bias terms applied to activation
    functions.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¼ºè°ƒäº†æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ªåµŒå¥—çš„æ¿€æ´»çº¿æ€§ç³»ç»Ÿï¼Œå³çº¿æ€§åŠ æƒçš„å¹³å‡å€¼åŠ ä¸Šåº”ç”¨äºæ¿€æ´»å‡½æ•°çš„åç½®é¡¹ã€‚
- en: Number of Model Parameters
  id: totrans-678
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°æ•°é‡
- en: In general, there are many model parameters, \(theta\), in an artificial neural
    network. First, letâ€™s clarify these definitions to describe our artificial neural
    network,
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œäººå·¥ç¥ç»ç½‘ç»œä¸­æœ‰è®¸å¤šæ¨¡å‹å‚æ•°ï¼Œ\(theta\)ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ˜ç¡®è¿™äº›å®šä¹‰æ¥æè¿°æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œï¼Œ
- en: '**neural network width** - the number of nodes in the layers of the neural
    network'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¥ç»ç½‘ç»œå®½åº¦** - ç¥ç»ç½‘ç»œå±‚ä¸­çš„èŠ‚ç‚¹æ•°'
- en: '**neural network depth** - the number of layers in the neural network, typically
    the input layer is not included in this calculation'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¥ç»ç½‘ç»œæ·±åº¦** - ç¥ç»ç½‘ç»œä¸­çš„å±‚æ•°ï¼Œé€šå¸¸è¾“å…¥å±‚ä¸åŒ…æ‹¬åœ¨è¿™ä¸ªè®¡ç®—ä¸­'
- en: Now, letâ€™s assume the following compact notation for a 3 layer artificial neural
    network, input, output and 1 hidden layer, with the width of each layer as,
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å‡è®¾ä¸€ä¸ª3å±‚äººå·¥ç¥ç»ç½‘ç»œçš„ç´§å‡‘è¡¨ç¤ºæ³•ï¼ŒåŒ…æ‹¬è¾“å…¥ã€è¾“å‡ºå’Œ1ä¸ªéšè—å±‚ï¼Œæ¯å±‚çš„å®½åº¦å¦‚ä¸‹ï¼Œ
- en: number of input nodes, \(p\)
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥èŠ‚ç‚¹æ•°ï¼Œ\(p\)
- en: number of hidden layer nodes, \(m\)
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éšè—å±‚èŠ‚ç‚¹æ•°ï¼Œ\(m\)
- en: and number of output nodes, \(k\)
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŠ‚ç‚¹æ•°ï¼Œ\(k\)
- en: '![](../Images/1b5b7051a7f760fb3b71c71560ca7613.png)'
  id: totrans-686
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b5b7051a7f760fb3b71c71560ca7613.png)'
- en: Notation for artificial neural network width, number of input nodes, \(p\),
    number of hidden layer nodes, \(m\), and number of output nodes, \(k\).
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œå®½åº¦ã€è¾“å…¥èŠ‚ç‚¹æ•°ã€\(p\)ã€éšè—å±‚èŠ‚ç‚¹æ•°ã€\(m\)å’Œè¾“å‡ºèŠ‚ç‚¹æ•°ã€\(k\)çš„è¡¨ç¤ºæ³•ã€‚
- en: fully connected, so for every connection there is a weight,
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œå…¨è¿æ¥ï¼Œå› æ­¤å¯¹äºæ¯ä¸ªè¿æ¥éƒ½æœ‰ä¸€ä¸ªæƒé‡ï¼Œ
- en: \[ \lambda_{ğ¼_{1,\ldots,ğ‘},ğ»_{1,\ldots,ğ‘š} } \quad \text{and} \quad \lambda_{ğ»_{1,\ldots,ğ‘š},ğ‘‚_{1,\ldots,ğ‘˜}
    } \]
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{ğ¼_{1,\ldots,ğ‘},ğ»_{1,\ldots,ğ‘š} } \quad \text{å’Œ} \quad \lambda_{ğ»_{1,\ldots,ğ‘š},ğ‘‚_{1,\ldots,ğ‘˜}
    } \]
- en: with full connectivity the number of weights is
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…¨è¿æ¥çš„æƒ…å†µä¸‹ï¼Œæƒé‡çš„æ•°é‡æ˜¯
- en: \[ ğ‘ \times ğ‘š \quad \text{and} \quad ğ‘š \times ğ‘˜ \]
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: \[ ğ‘ \times ğ‘š \quad \text{å’Œ} \quad ğ‘š \times ğ‘˜ \]
- en: and at each hidden layer node there is a bias term,
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªéšè—å±‚èŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªåç½®é¡¹ï¼Œ
- en: \[ ğ‘_{H_{1,\ldots,m} } \]
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: \[ ğ‘_{H_{1,\ldots,m} } \]
- en: and at every output node there is a bias term,
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªè¾“å‡ºèŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªåç½®é¡¹ï¼Œ
- en: \[ ğ‘_{O_{1,\ldots,k} } \]
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: \[ ğ‘_{O_{1,\ldots,k} } \]
- en: Therefore, the number of model parameters is,
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ¨¡å‹å‚æ•°çš„æ•°é‡æ˜¯ï¼Œ
- en: \[ |\theta| = ğ‘ \times ğ‘š + ğ‘š \times ğ‘˜ + ğ‘š + ğ‘˜ \]
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta| = ğ‘ \times ğ‘š + ğ‘š \times ğ‘˜ + ğ‘š + ğ‘˜ \]
- en: this assumes an unique bias term at each hidden layer node and output layer
    node, but in some case the same bias term may be applied over the entire layer.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å‡è®¾æ¯ä¸ªéšè—å±‚èŠ‚ç‚¹å’Œè¾“å‡ºå±‚èŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªç‹¬ç‰¹çš„åç½®é¡¹ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç›¸åŒçš„åç½®é¡¹å¯èƒ½åº”ç”¨äºæ•´ä¸ªå±‚ã€‚
- en: For our example, with \(p = 3\), \(m = 2\) and \(k = 1\), then the number of
    model parameters are,
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„ä¾‹å­ï¼Œå½“\(p = 3\)ï¼Œ\(m = 2\)å’Œ\(k = 1\)æ—¶ï¼Œæ¨¡å‹å‚æ•°çš„æ•°é‡æ˜¯ï¼Œ
- en: \[ |\theta| = ğ‘ \times ğ‘š + ğ‘š \times ğ‘˜ + ğ‘š + ğ‘˜ \]
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta| = ğ‘ \times ğ‘š + ğ‘š \times ğ‘˜ + ğ‘š + ğ‘˜ \]
- en: after substitution we have,
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£å…¥åæˆ‘ä»¬å¾—åˆ°ï¼Œ
- en: \[ |\theta| = 3 \times 2 + 2 \times 1 + 2 + 1 = 11 \]
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta| = 3 \times 2 + 2 \times 1 + 2 + 1 = 11 \]
- en: I select this as a manageable number of parameters, so we can train and visualize
    our model, but consider a more typical model size by increasing our artificial
    neural networkâ€™s width, with \(p = 10\), \(m = 20\) and \(k = 3\), then we have
    many more model parameters,
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘é€‰æ‹©è¿™ä¸ªå¯ç®¡ç†çš„å‚æ•°æ•°é‡ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è®­ç»ƒå’Œå¯è§†åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä½†é€šè¿‡å¢åŠ äººå·¥ç¥ç»ç½‘ç»œçš„å®½åº¦ï¼Œä»¥ \(p = 10\)ï¼Œ\(m = 20\) å’Œ \(k
    = 3\)ï¼Œæˆ‘ä»¬ä¼šæœ‰æ›´å¤šçš„æ¨¡å‹å‚æ•°ï¼Œ
- en: \[ |\theta| = 10 \times 20 + 20 \times 3 + 20 + 3 = 283 \]
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta| = 10 \times 20 + 20 \times 3 + 20 + 3 = 283 \]
- en: If we add hidden layers, increase our artificial neural networkâ€™s depth, the
    number of model parameters will grow very quickly.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å¢åŠ éšè—å±‚ï¼Œå¢åŠ äººå·¥ç¥ç»ç½‘ç»œçš„æ·±åº¦ï¼Œæ¨¡å‹å‚æ•°çš„æ•°é‡å°†è¿…é€Ÿå¢é•¿ã€‚
- en: we can generalize this calculation for any fully connected, feed forward neural
    network, given a \(W\) vector with the number of nodes, i.e., the width of each
    layer,
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªè®¡ç®—æ¨å¹¿åˆ°ä»»ä½•å…¨è¿æ¥ã€å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç»™å®šä¸€ä¸ª \(W\) å‘é‡ï¼ŒåŒ…å«èŠ‚ç‚¹æ•°ï¼Œå³æ¯å±‚çš„å®½åº¦ï¼Œ
- en: \[ \mathbf{L} = [l_0, l_1, l_2, \dots, l_n] \]
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{L} = [l_0, l_1, l_2, \dots, l_n] \]
- en: where,
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ
- en: \(l_0\) is the number of input neurons
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(l_0\) æ˜¯è¾“å…¥ç¥ç»å…ƒçš„æ•°é‡
- en: \(l_1, \dots, l_{n-1}\) are the widths of the hidden layers
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(l_1, \dots, l_{n-1}\) æ˜¯éšè—å±‚çš„å®½åº¦
- en: \(l_n\) is the number of output neurons
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(l_n\) æ˜¯è¾“å‡ºç¥ç»å…ƒçš„æ•°é‡
- en: The total number of connection weights is,
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: è¿æ¥æƒé‡çš„æ€»æ•°æ˜¯ï¼Œ
- en: \[ |\theta_{weights}| = \sum_{i=1}^{n} l_i \cdot l_{i-1} \]
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta_{weights}| = \sum_{i=1}^{n} l_i \cdot l_{i-1} \]
- en: the total number of node biases (there are not bias parameters in the input
    layer nodes, \(l_0\)) is,
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹åå·®çš„æ€»æ•°ï¼ˆè¾“å…¥å±‚èŠ‚ç‚¹ä¸­æ²¡æœ‰åå·®å‚æ•°ï¼Œ\(l_0\)ï¼‰æ˜¯ï¼Œ
- en: \[ |\theta_{biases}| = \sum_{i=1}^{n} l_i \]
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta_{biases}| = \sum_{i=1}^{n} l_i \]
- en: the total number of trainable model parameters, connectioned weights and node
    biases, is,
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è®­ç»ƒæ¨¡å‹å‚æ•°ã€è¿æ¥æƒé‡å’ŒèŠ‚ç‚¹åå·®çš„æ€»æ•°æ˜¯ï¼Œ
- en: \[ |\theta| = \sum_{i=1}^{n} \left( l_i \cdot l_{i-1} + l_i \right) = \sum_{i=1}^{n}
    l_i \cdot (l_{i-1} + 1) \]
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta| = \sum_{i=1}^{n} \left( l_i \cdot l_{i-1} + l_i \right) = \sum_{i=1}^{n}
    l_i \cdot (l_{i-1} + 1) \]
- en: Letâ€™s take an example of artificial neural network with 4 hidden layers, with
    network width by-layer vector of,
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»¥ä¸€ä¸ªæœ‰4ä¸ªéšè—å±‚çš„äººå·¥ç¥ç»ç½‘ç»œä¸ºä¾‹ï¼Œç½‘ç»œå®½åº¦æŒ‰å±‚å‘é‡ï¼Œ
- en: \[ \mathbf{L} = [10, 8, 6, 4, 2, 1] \]
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{L} = [10, 8, 6, 4, 2, 1] \]
- en: The total number of connection weights is,
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: è¿æ¥æƒé‡çš„æ€»æ•°æ˜¯ï¼Œ
- en: \[ |\theta_{L_{weights}}| = \sum_{i=1}^{5} l_i \cdot l_{i-1} = (8 \cdot 10)
    + (6 \cdot 8) + (4 \cdot 6) + (2 \cdot 4) + (1 \cdot 2) = 80 + 48 + 24 + 8 + 2
    = 162 \]
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta_{L_{weights}}| = \sum_{i=1}^{5} l_i \cdot l_{i-1} = (8 \cdot 10)
    + (6 \cdot 8) + (4 \cdot 6) + (2 \cdot 4) + (1 \cdot 2) = 80 + 48 + 24 + 8 + 2
    = 162 \]
- en: and the total number of node biases is,
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹åå·®çš„æ€»æ•°æ˜¯ï¼Œ
- en: \[ |\theta_{L_{biases}}| = \sum_{i=1}^{5} l_i = 8 + 6 + 4 + 2 + 1 = 21 \]
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta_{L_{biases}}| = \sum_{i=1}^{5} l_i = 8 + 6 + 4 + 2 + 1 = 21 \]
- en: and finally the total nuber of trainable parameters is,
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¯è®­ç»ƒå‚æ•°çš„æ€»æ•°æ˜¯ï¼Œ
- en: \[ |\theta_L| = \sum_{i=1}^{5} l_i \cdot (l_{i-1} + 1) = (8 \cdot 11) + (6 \cdot
    9) + (4 \cdot 7) + (2 \cdot 5) + (1 \cdot 3) = 88 + 54 + 28 + 10 + 3 = 183 \]
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\theta_L| = \sum_{i=1}^{5} l_i \cdot (l_{i-1} + 1) = (8 \cdot 11) + (6 \cdot
    9) + (4 \cdot 7) + (2 \cdot 5) + (1 \cdot 3) = 88 + 54 + 28 + 10 + 3 = 183 \]
- en: Activation Functions
  id: totrans-726
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°
- en: The activation function is a transformation of the linear combination of the
    weighted node inputs plus the node bias term. Nonlinear activation,
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°æ˜¯åŠ æƒèŠ‚ç‚¹è¾“å…¥çš„çº¿æ€§ç»„åˆåŠ ä¸ŠèŠ‚ç‚¹åå·®é¡¹çš„è½¬æ¢ã€‚éçº¿æ€§æ¿€æ´»ï¼Œ
- en: introduces non-linear properties, and complexity to the network
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼•å…¥äº†ç½‘ç»œçš„éçº¿æ€§å±æ€§å’Œå¤æ‚æ€§
- en: prevents the network from collapsing
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é˜²æ­¢ç½‘ç»œå´©æºƒ
- en: Without the nonlinear activation function we would have linear regression, the
    entire system collapses.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œæˆ‘ä»¬ä¼šæœ‰çº¿æ€§å›å½’ï¼Œæ•´ä¸ªç³»ç»Ÿä¼šå´©æºƒã€‚
- en: For more information about activation functions and a demonstration of the collapse
    without nonlinear activation to multilinear regression see the associated chapter
    in this e-book, [Neural Network Activation Functions](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_activation_functions.html).
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šå…³äºæ¿€æ´»å‡½æ•°å’Œæ²¡æœ‰éçº¿æ€§æ¿€æ´»åˆ°å¤šå…ƒçº¿æ€§å›å½’çš„æ¼”ç¤ºä¿¡æ¯ï¼Œè¯·å‚é˜…æœ¬ç”µå­ä¹¦çš„ç›¸å…³ç« èŠ‚ï¼Œ[ç¥ç»ç½‘ç»œæ¿€æ´»å‡½æ•°](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_activation_functions.html)ã€‚
- en: Training Networks Steps
  id: totrans-732
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒç½‘ç»œæ­¥éª¤
- en: Training an artificial neural network proceeds iteratively by these steps,
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™äº›æ­¥éª¤è¿­ä»£åœ°è®­ç»ƒäººå·¥ç¥ç»ç½‘ç»œï¼Œ
- en: initialized the model parameters
  id: totrans-734
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–æ¨¡å‹å‚æ•°
- en: forward pass to make a prediction
  id: totrans-735
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‰å‘ä¼ é€’ä»¥è¿›è¡Œé¢„æµ‹
- en: calculate the error derivative based on the prediction and truth over training
    data
  id: totrans-736
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ¹æ®è®­ç»ƒæ•°æ®ä¸­çš„é¢„æµ‹å’ŒçœŸå®å€¼è®¡ç®—è¯¯å·®å¯¼æ•°
- en: backpropagate the error derivative back through the artificial neural network
    to calculate the derivatives of the error over all the model weights and biases
    parameters
  id: totrans-737
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œåå‘ä¼ æ’­è¯¯å·®å¯¼æ•°ï¼Œä»¥è®¡ç®—æ‰€æœ‰æ¨¡å‹æƒé‡å’Œåç½®å‚æ•°çš„è¯¯å·®å¯¼æ•°
- en: update the model parameters based on the derivatives and learning rates
  id: totrans-738
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ¹æ®å¯¼æ•°å’Œå­¦ä¹ ç‡æ›´æ–°æ¨¡å‹å‚æ•°
- en: repeat until convergence.
  id: totrans-739
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡å¤ç›´åˆ°æ”¶æ•›ã€‚
- en: '![](../Images/c3a5bc8956f8ceda05ddf9b582cd141d.png)'
  id: totrans-740
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3a5bc8956f8ceda05ddf9b582cd141d.png)'
- en: The iterative steps for training an artificial neural network.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒäººå·¥ç¥ç»ç½‘ç»œçš„è¿­ä»£æ­¥éª¤ã€‚
- en: Hereâ€™s some details on each step,
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æ¯ä¸ªæ­¥éª¤çš„ä¸€äº›ç»†èŠ‚ï¼Œ
- en: '**Initializing the Model Parameters** - initialize all model parameters with
    typically small (near zero) random values. Hereâ€™s a couple common methods,'
  id: totrans-743
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åˆå§‹åŒ–æ¨¡å‹å‚æ•°** - é€šå¸¸ä½¿ç”¨æ¥è¿‘é›¶çš„å°éšæœºå€¼åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹å‚æ•°ã€‚è¿™é‡Œæœ‰ä¸€äº›å¸¸è§çš„æ–¹æ³•ï¼Œ'
- en: '**Xavier Weight Initialization** - random realizations from uniform distributions
    specified by \(U[\text{min}, \text{max}]\),'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Xavier æƒé‡åˆå§‹åŒ–** - ä»ç”± \(U[\text{min}, \text{max}]\) æŒ‡å®šçš„å‡åŒ€åˆ†å¸ƒä¸­éšæœºå®ç°ï¼Œ'
- en: \[ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}}, \frac{1}{\sqrt{p}} \right]
    (p^\ell) \]
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}}, \frac{1}{\sqrt{p}} \right]
    (p^\ell) \]
- en: where \(F^{-1}_U\) is the inverse of the CDF, \(p\) is the number of inputs,
    and \(p^{\ell}\) is a random cumulative probability value drawn from the uniform
    distribution, \(U[0,1]\).
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ \(F^{-1}_U\) æ˜¯ CDF çš„é€†ï¼Œ\(p\) æ˜¯è¾“å…¥çš„æ•°é‡ï¼Œ\(p^{\ell}\) æ˜¯ä»å‡åŒ€åˆ†å¸ƒ \(U[0,1]\) ä¸­æŠ½å–çš„éšæœºç´¯ç§¯æ¦‚ç‡å€¼ã€‚
- en: '**Normalized Xavier Weight Initialization** - random realizations from uniform
    distributions specified by \(U[\text{min}, \text{max}]\),'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å½’ä¸€åŒ– Xavier æƒé‡åˆå§‹åŒ–** - ä»ç”± \(U[\text{min}, \text{max}]\) æŒ‡å®šçš„å‡åŒ€åˆ†å¸ƒä¸­éšæœºå®ç°ï¼Œ'
- en: \[ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k}
    \right] (p^\ell) \]
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k}
    \right] (p^\ell) \]
- en: where \(F^{-1}_U\) is the inverse of the CDF, \(p\) is the number of inputs,
    \(k\) is the number of outputs, and \(p^{\ell}\) is a random cumulative probability
    value drawn from the uniform distribution, \(U[0,1]\).
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ \(F^{-1}_U\) æ˜¯ CDF çš„é€†ï¼Œ\(p\) æ˜¯è¾“å…¥çš„æ•°é‡ï¼Œ\(k\) æ˜¯è¾“å‡ºçš„æ•°é‡ï¼Œ\(p^{\ell}\) æ˜¯ä»å‡åŒ€åˆ†å¸ƒ \(U[0,1]\)
    ä¸­æŠ½å–çš„éšæœºç´¯ç§¯æ¦‚ç‡å€¼ã€‚
- en: For example, if we return to our first hidden layer node,
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªéšè—å±‚èŠ‚ç‚¹ï¼Œ
- en: '![](../Images/b2f8e46ea497049f4b95c03b8812eea7.png)'
  id: totrans-751
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2f8e46ea497049f4b95c03b8812eea7.png)'
- en: First hidden layer node with 3 inputs, and 1 output.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªéšè—å±‚èŠ‚ç‚¹æœ‰ 3 ä¸ªè¾“å…¥ï¼Œ1 ä¸ªè¾“å‡ºã€‚
- en: we have \(p = 3\) and \(k = 1\), and we draw from the uniform distribution,
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ \(p = 3\) å’Œ \(k = 1\)ï¼Œå¹¶ä»å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–ï¼Œ
- en: \[ U \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] = U \left[ \frac{-1}{\sqrt{3}+1},
    \frac{1}{\sqrt{3}+1} \right] \]
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] = U \left[ \frac{-1}{\sqrt{3}+1},
    \frac{1}{\sqrt{3}+1} \right] \]
- en: '**Forward Pass** - to make a prediction, \(\hat{y}\). Initial predictions will
    be random for the first iteration, but will improve over iterations. Once again
    for our model the forward pass is,'
  id: totrans-755
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ­£å‘ä¼ æ’­** - ä¸ºäº†åšå‡ºé¢„æµ‹ï¼Œ\(\hat{y}\)ã€‚åˆå§‹é¢„æµ‹åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£å°†æ˜¯éšæœºçš„ï¼Œä½†ä¼šéšç€è¿­ä»£è€Œæ”¹è¿›ã€‚å†æ¬¡ï¼Œå¯¹äºæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæ­£å‘ä¼ æ’­æ˜¯ï¼Œ'
- en: \[ O_6 = \lambda_{4,6} \cdot \sigma \left( \lambda_{1,4} I_1 + \lambda_{2,4}
    I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot \sigma \left( \lambda_{1,5}
    I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3 + b_5 \right) + b_6 \]![](../Images/08556ecbd47d143019d0163dc95761cf.png)
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \lambda_{4,6} \cdot \sigma \left( \lambda_{1,4} I_1 + \lambda_{2,4}
    I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot \sigma \left( \lambda_{1,5}
    I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3 + b_5 \right) + b_6 \]![](../Images/08556ecbd47d143019d0163dc95761cf.png)
- en: Prediction with our artificial neural network initialized with random model
    parameters, weights and biases.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹å‚æ•°ã€æƒé‡å’Œåç½®çš„äººå·¥ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹ã€‚
- en: '**Calculate the Error Derivative** - given a loss of,'
  id: totrans-758
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è®¡ç®—è¯¯å·®å¯¼æ•°** - ç»™å®šä¸€ä¸ªæŸå¤±ï¼Œ'
- en: \[ L = \frac{1}{2} \left(\hat{y} - y \right)^2 \]
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L = \frac{1}{2} \left(\hat{y} - y \right)^2 \]
- en: and the error derivative, i.e., rate of change of in error given a change in
    model estimate is,
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠè¯¯å·®å¯¼æ•°ï¼Œå³è¯¯å·®éšæ¨¡å‹ä¼°è®¡å˜åŒ–çš„å˜åŒ–ç‡ï¼Œ
- en: \[ \frac{\partial L}{\partial \hat{y}} = \hat{Y} - Y \]
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial \hat{y}} = \hat{Y} - Y \]
- en: For now, letâ€™s only consider a single estimate, and we will address more than
    1 training data later.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬åªè€ƒè™‘ä¸€ä¸ªä¼°è®¡ï¼Œç¨åæˆ‘ä»¬å°†è§£å†³è¶…è¿‡ 1 ä¸ªè®­ç»ƒæ•°æ®ã€‚
- en: '**Backpropagate the Error Derivative** - we shift back through the artificial
    neural network to calculate the derivatives of the error over all the model weights
    and biases parameters, with the chain rule, for example the loss derivative backpropagated
    to the output of node \(H_4\),'
  id: totrans-763
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åå‘ä¼ æ’­è¯¯å·®å¯¼æ•°** - æˆ‘ä»¬é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œå›é€€ä»¥è®¡ç®—æ‰€æœ‰æ¨¡å‹æƒé‡å’Œåå·®å‚æ•°çš„è¯¯å·®å¯¼æ•°ï¼Œä¾‹å¦‚ï¼ŒæŸå¤±å¯¼æ•°åå‘ä¼ æ’­åˆ°èŠ‚ç‚¹ \(H_4\) çš„è¾“å‡ºï¼Œ'
- en: \[ \frac{\partial L}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    L}{\partial O_6} = \lambda_{4,6} \cdot \bigl( (1 - O_6) \cdot O_6 \bigr) \cdot
    (O_6 - y) \]
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    L}{\partial O_6} = \lambda_{4,6} \cdot \bigl( (1 - O_6) \cdot O_6 \bigr) \cdot
    (O_6 - y) \]
- en: '**Update the Model Parameters** - based on the derivatives, \(\frac{\partial
    L}{\partial \lambda_{i,j}}\) and learning rates, \(\eta\), like this,'
  id: totrans-765
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ›´æ–°æ¨¡å‹å‚æ•°** - åŸºäºå¯¼æ•° \(\frac{\partial L}{\partial \lambda_{i,j}}\) å’Œå­¦ä¹ ç‡ \(\eta\)ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œ'
- en: \[ \lambda_{i,j}^{\ell} = \lambda_{i,j}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{i,j}} \]
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{i,j}^{\ell} = \lambda_{i,j}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{i,j}} \]
- en: '**Repeat Until Convergence** - return to step 1, until the error, \(L\), is
    reduced to an acceptable level, i.e., model convergence is the condition to stop
    the iterations'
  id: totrans-767
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é‡å¤ç›´åˆ°æ”¶æ•›** - è¿”å›åˆ°æ­¥éª¤ 1ï¼Œç›´åˆ°è¯¯å·® \(L\) é™ä½åˆ°å¯æ¥å—çš„æ°´å¹³ï¼Œå³æ¨¡å‹æ”¶æ•›æ˜¯åœæ­¢è¿­ä»£çš„æ¡ä»¶'
- en: These are the steps, now letâ€™s dive into the details for each, but first letâ€™s
    start with the mathematical framework for backpropagation - the chain rule.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯æ­¥éª¤ï¼Œç°åœ¨è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨æ¯ä¸ªæ­¥éª¤çš„ç»†èŠ‚ï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬ä»åå‘ä¼ æ’­çš„æ•°å­¦æ¡†æ¶â€”â€”é“¾å¼æ³•åˆ™å¼€å§‹ã€‚
- en: The Chain Rule
  id: totrans-769
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é“¾å¼æ³•åˆ™
- en: Upon reflection, it is clear that the forward pass through our artificial neural
    network involves a sequence of nested operations that progressively transform
    the input signals as they propagate from the input nodes, through each layer,
    to the output nodes.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: åæ€åï¼Œå¾ˆæ˜æ˜¾ï¼Œæˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­æ¶‰åŠä¸€ç³»åˆ—åµŒå¥—æ“ä½œï¼Œè¿™äº›æ“ä½œé€æ­¥å°†è¾“å…¥ä¿¡å·ä»è¾“å…¥èŠ‚ç‚¹ä¼ æ’­åˆ°æ¯ä¸€å±‚ï¼Œæœ€ç»ˆåˆ°è¾¾è¾“å‡ºèŠ‚ç‚¹ã€‚
- en: So we can represent this as a sequence of nested operations,
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ­¤è¡¨ç¤ºä¸ºä¸€ç³»åˆ—åµŒå¥—æ“ä½œï¼Œ
- en: \[ f = f(x) \quad g = g(f) \quad y = h(g) \]
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f = f(x) \quad g = g(f) \quad y = h(g) \]
- en: and now in this form to emphasize the nesting of operations,
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä»¥è¿™ç§å½¢å¼å¼ºè°ƒæ“ä½œçš„åµŒå¥—ï¼Œ
- en: \[ y = h \bigl( g(f(x)) \bigr) \]
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y = h \bigl( g(f(x)) \bigr) \]
- en: By applying the chain rule to the nested functions \(y = h \bigl( g(f(x)) \bigr)\),
    we can solve for \(\frac{\partial y}{\partial x}\) as,
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†é“¾å¼æ³•åˆ™åº”ç”¨äºåµŒå¥—å‡½æ•° \(y = h \bigl( g(f(x)) \bigr)\)ï¼Œæˆ‘ä»¬å¯ä»¥æ±‚è§£ \(\frac{\partial y}{\partial
    x}\) å¦‚ä¸‹ï¼Œ
- en: \[ \frac{\partial y}{\partial x} = \frac{\partial h}{\partial g} \cdot \frac{\partial
    g}{\partial f} \cdot \frac{\partial f}{\partial x} \]
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial y}{\partial x} = \frac{\partial h}{\partial g} \cdot \frac{\partial
    g}{\partial f} \cdot \frac{\partial f}{\partial x} \]
- en: where we chain together the partial derivatives for all the operators to solve
    derivative of the output, \(y\), given the input, \(x\).
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰ç®—å­çš„åå¯¼æ•°é“¾å¼ç›¸åŠ ï¼Œä»¥æ±‚è§£ç»™å®šè¾“å…¥ \(x\) çš„è¾“å‡º \(y\) çš„å¯¼æ•°ã€‚
- en: we can compute derivatives at any intermediate point in the nested functions,
    for example, stepping backwards one step,
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨åµŒå¥—å‡½æ•°çš„ä»»ä½•ä¸­é—´ç‚¹è®¡ç®—å¯¼æ•°ï¼Œä¾‹å¦‚ï¼Œå‘åå›é€€ä¸€æ­¥ï¼Œ
- en: \[ \frac{\partial f}{\partial x} \]
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial f}{\partial x} \]
- en: and now two steps,
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å›é€€ä¸¤æ­¥ï¼Œ
- en: \[ \frac{\partial g}{\partial x} = \frac{\partial g}{\partial f} \cdot \frac{\partial
    f}{\partial x} \]
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial g}{\partial x} = \frac{\partial g}{\partial f} \cdot \frac{\partial
    f}{\partial x} \]
- en: and all the way with three steps,
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åé€æ­¥å›é€€ä¸‰æ­¥ï¼Œ
- en: \[ \frac{\partial y}{\partial x} = \frac{\partial h}{\partial g} \cdot \frac{\partial
    g}{\partial f} \cdot \frac{\partial f}{\partial x} \]
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial y}{\partial x} = \frac{\partial h}{\partial g} \cdot \frac{\partial
    g}{\partial f} \cdot \frac{\partial f}{\partial x} \]
- en: This is what we do with backpropagation, but this may be too abstract! Letâ€™s
    move to a very simple feed forward neural network with only these three nodes,
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬é€šè¿‡åå‘ä¼ æ’­æ‰€åšçš„äº‹æƒ…ï¼Œä½†è¿™å¯èƒ½è¿‡äºæŠ½è±¡ï¼è®©æˆ‘ä»¬è½¬å‘ä¸€ä¸ªéå¸¸ç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå®ƒåªæœ‰è¿™ä¸‰ä¸ªèŠ‚ç‚¹ï¼Œ
- en: \(I_1\) - input node
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(I_1\) - è¾“å…¥èŠ‚ç‚¹
- en: \(H_2 = h(I_1)\) - hidden layer node, a function of \(I_1\)
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(H_2 = h(I_1)\) - éšè—å±‚èŠ‚ç‚¹ï¼Œæ˜¯ \(I_1\) çš„å‡½æ•°
- en: \(O_3 = o(H_2)\) - output node, a function of \(H_2\)
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(O_3 = o(H_2)\) - è¾“å‡ºèŠ‚ç‚¹ï¼Œæ˜¯ \(H_2\) çš„å‡½æ•°
- en: this is still intentionally abstract, i.e., without mention of weights and biases,
    to help you develop a mental framework of backpropagation with neural netowrks
    by the chain rule, we will dive into the details immediately after this discussion.
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ä»ç„¶æ˜¯æœ‰æ„ä¸ºä¹‹çš„æŠ½è±¡ï¼Œå³æ²¡æœ‰æåŠæƒé‡å’Œåå·®ï¼Œä»¥å¸®åŠ©æ‚¨é€šè¿‡é“¾å¼æ³•åˆ™å‘å±•ç¥ç»ç½‘ç»œåå‘ä¼ æ’­çš„å¿ƒæ™ºæ¡†æ¶ï¼Œæˆ‘ä»¬å°†åœ¨è¿™æ¬¡è®¨è®ºä¹‹åç«‹å³æ·±å…¥ç»†èŠ‚ã€‚
- en: 'The output \(O_3\) depends on the input \(I_1\) through these nested functions:'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡º \(O_3\) é€šè¿‡è¿™äº›åµŒå¥—å‡½æ•°ä¾èµ–äºè¾“å…¥ \(I_1\)ï¼š
- en: \[ O_3 = o \bigl( h(I_1) \bigr) \]
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_3 = o \bigl( h(I_1) \bigr) \]
- en: Using the **chain rule**, the gradient of the output with respect to backpropagating
    one step,
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ **é“¾å¼æ³•åˆ™**ï¼Œè¾“å‡ºç›¸å¯¹äºåå‘ä¼ æ’­ä¸€æ­¥çš„æ¢¯åº¦ï¼Œ
- en: \[ \frac{\partial O_3}{\partial H_2} \]
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_3}{\partial H_2} \]
- en: and with respect to backpropagating two steps,
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠå…³äºåå‘ä¼ æ’­ä¸¤æ­¥çš„ï¼Œ
- en: \[ \frac{\partial O_3}{\partial I_1} = \frac{\partial H_2}{\partial I_1} \cdot
    \frac{\partial O_3}{\partial H_2} \]
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_3}{\partial I_1} = \frac{\partial H_2}{\partial I_1} \cdot
    \frac{\partial O_3}{\partial H_2} \]
- en: This shows how the gradient **backpropagates** through the network,
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¾ç¤ºäº†æ¢¯åº¦æ˜¯å¦‚ä½• **åå‘ä¼ æ’­** é€šè¿‡ç½‘ç»œçš„ï¼Œ
- en: \(\frac{\partial O_3}{\partial I_1}\) - is the local gradient at the hidden
    node
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\frac{\partial O_3}{\partial I_1}\) - æ˜¯éšè—èŠ‚ç‚¹çš„å±€éƒ¨æ¢¯åº¦
- en: \(\frac{\partial O_3}{\partial H_2}\) - is the local gradient at the output
    node
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\frac{\partial O_3}{\partial H_2}\) - æ˜¯è¾“å‡ºèŠ‚ç‚¹çš„å±€éƒ¨æ¢¯åº¦
- en: By backpropagation we can calculate the deriviates with respect to all parts
    of the network, how the input node signal \(I_1\), or hidden nodel signal \(H_2\)
    affect the output \(O_3\), \(\frac{\partial O_3}{\partial I_1}\) and \(frac{\partial
    O_3}{\partial H_2}\) respsectively.
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åå‘ä¼ æ’­ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ç›¸å¯¹äºç½‘ç»œæ‰€æœ‰éƒ¨åˆ†çš„å¯¼æ•°ï¼Œå¦‚ä½•è¾“å…¥èŠ‚ç‚¹ä¿¡å· \(I_1\) æˆ–éšè—èŠ‚ç‚¹ä¿¡å· \(H_2\) å½±å“è¾“å‡º \(O_3\)ï¼Œåˆ†åˆ«æ˜¯
    \(\frac{\partial O_3}{\partial I_1}\) å’Œ \(frac{\partial O_3}{\partial H_2}\)ã€‚
- en: and more importantly, how changes in the input \(I_1\), or \(H_2\) affect the
    change in model loss, \(\frac{\partial L}{\partial I_1}\) and \(frac{\partial
    L}{\partial H_2}\) respsectively.
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´é‡è¦çš„æ˜¯ï¼Œè¾“å…¥ \(I_1\) æˆ– \(H_2\) çš„å˜åŒ–å¦‚ä½•å½±å“æ¨¡å‹æŸå¤±çš„å˜åŒ–ï¼Œåˆ†åˆ«æ˜¯ \(\frac{\partial L}{\partial I_1}\)
    å’Œ \(frac{\partial L}{\partial H_2}\)ã€‚
- en: This chain of partial derivatives, move backwards step by step through the neural
    network layers, is the fundamental mechanism behind **backpropagation**. Next
    we will derive and demonstrate each of the parts of backpropagation and then finally
    put this together to show backpropagation over our entire network.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€ç³»åˆ—çš„åå¯¼æ•°ï¼Œé€šè¿‡é€æ­¥åå‘é€šè¿‡ç¥ç»ç½‘ç»œå±‚ï¼Œæ˜¯åå‘ä¼ æ’­èƒŒåçš„åŸºæœ¬æœºåˆ¶ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°†æ¨å¯¼å¹¶æ¼”ç¤ºåå‘ä¼ æ’­çš„å„ä¸ªéƒ¨åˆ†ï¼Œç„¶åæœ€ç»ˆå°†å®ƒä»¬ç»„åˆèµ·æ¥ï¼Œä»¥å±•ç¤ºæ•´ä¸ªç½‘ç»œçš„åå‘ä¼ æ’­ã€‚
- en: Neural Networks Backpropagation Building Blocks
  id: totrans-801
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œåå‘ä¼ æ’­æ„å»ºå—
- en: Letâ€™s cover the numerical building blocks for backpropagation. Once you understand
    these backpropagation building blocks, you will be able to backpropagate our simple
    network and even any complicated artificial neural networks by hand,
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»‹ç»åå‘ä¼ æ’­çš„æ•°å€¼æ„å»ºå—ã€‚ä¸€æ—¦ä½ ç†è§£äº†è¿™äº›åå‘ä¼ æ’­æ„å»ºå—ï¼Œä½ å°†èƒ½å¤Ÿæ‰‹åŠ¨åå‘ä¼ æ’­æˆ‘ä»¬çš„ç®€å•ç½‘ç»œï¼Œç”šè‡³ä»»ä½•å¤æ‚çš„äººå·¥ç¥ç»ç½‘ç»œï¼Œ
- en: calculating the loss derivative
  id: totrans-803
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±å¯¼æ•°
- en: backpropagation through nodes
  id: totrans-804
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡èŠ‚ç‚¹çš„åå‘ä¼ æ’­
- en: backpropagation along connections
  id: totrans-805
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ²¿ç€è¿æ¥çš„åå‘ä¼ æ’­
- en: accounting for multiple paths
  id: totrans-806
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°å¤šæ¡è·¯å¾„
- en: loss derivatives with respect to weights and biases
  id: totrans-807
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å…³äºæƒé‡å’Œåç½®çš„æŸå¤±å¯¼æ•°
- en: For now I demonstrate backpropagation of this loss derivative for a single training
    data sample, \(y\).
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰æˆ‘æ¼”ç¤ºäº†å•ä¸ªè®­ç»ƒæ•°æ®æ ·æœ¬ \(y\) çš„è¿™ä¸ªæŸå¤±å¯¼æ•°çš„åå‘ä¼ æ’­ã€‚
- en: I address multiple samples later, \(y_i, i=1, \ldots, n\)
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å°†åœ¨åé¢å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œ\(y_i, i=1, \ldots, n\)
- en: Letâ€™s start with calculating the loss derivative.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»è®¡ç®—æŸå¤±å¯¼æ•°å¼€å§‹ã€‚
- en: Calculating the Loss Derivative
  id: totrans-811
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±å¯¼æ•°
- en: Backpropagation is based on the concept of allocating or propagating the loss
    derivative backwards through the neural network,
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­åŸºäºå°†æŸå¤±å¯¼æ•°åˆ†é…æˆ–ä¼ æ’­å›ç¥ç»ç½‘ç»œçš„æ¦‚å¿µï¼Œ
- en: we calculate the loss derivative and then distribute it sequentially, in reverse
    direction, from network output back towards the network input
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—æŸå¤±å¯¼æ•°ï¼Œç„¶åæŒ‰é¡ºåºä»ç½‘ç»œè¾“å‡ºåå‘åˆ†å¸ƒåˆ°ç½‘ç»œè¾“å…¥
- en: it is important to know that we are working with derivatives, and that backpropagation
    is NOT distributing error, although as you will see it may look that way!
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯è¦çŸ¥é“æˆ‘ä»¬æ­£åœ¨å¤„ç†å¯¼æ•°ï¼Œå¹¶ä¸”åå‘ä¼ æ’­ä¸æ˜¯åˆ†é…é”™è¯¯ï¼Œå°½ç®¡æ­£å¦‚ä½ å°†çœ‹åˆ°çš„ï¼Œå®ƒå¯èƒ½çœ‹èµ·æ¥æ˜¯è¿™æ ·ï¼
- en: We start by defining the loss, given the truth, \(ğ‘¦\), and our prediction, \(\hat{y}
    = O_6\), we calculate our \(L^2\) loss as,
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå®šä¹‰æŸå¤±ï¼Œç»™å®šçœŸå®å€¼ \(ğ‘¦\) å’Œæˆ‘ä»¬çš„é¢„æµ‹ \(\hat{y} = O_6\)ï¼Œæˆ‘ä»¬è®¡ç®—æˆ‘ä»¬çš„ \(L^2\) æŸå¤±ä¸ºï¼Œ
- en: \[ L = \frac{1}{2} \left( \hat{y} - y \right)^2 \]
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L = \frac{1}{2} \left( \hat{y} - y \right)^2 \]
- en: our choice of loss function allows us to use the prediction error as the loss
    derivative! We calculate the loss derivative as the partial derivative of the
    loss with respect to the estimate, \(\frac{\partial ğ¿}{\partial \hat{y}}\),
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€‰æ‹©æŸå¤±å‡½æ•°å…è®¸æˆ‘ä»¬ä½¿ç”¨é¢„æµ‹è¯¯å·®ä½œä¸ºæŸå¤±å¯¼æ•°ï¼æˆ‘ä»¬è®¡ç®—æŸå¤±å¯¼æ•°ä¸ºæŸå¤±ç›¸å¯¹äºä¼°è®¡çš„åå¯¼æ•°ï¼Œ\(\frac{\partial ğ¿}{\partial \hat{y}}\),
- en: \[ \frac{\partial \mathcal{L}}{\partial \hat{y}} = \frac{\partial \frac{1}{2}
    \left( \hat{y} - y \right)^2 }{\partial \hat{y}} = \hat{y} - y \]
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial \hat{y}} = \frac{\partial \frac{1}{2}
    \left( \hat{y} - y \right)^2 }{\partial \hat{y}} = \hat{y} - y \]
- en: You see what I mean, we are backpropagating the loss derivative, but due to
    our formulation of the \(L^2\) loss, we only have to calculate the error at our
    output node output, but once again - it is the loss derivative.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜ç™½æˆ‘çš„æ„æ€ï¼Œæˆ‘ä»¬æ­£åœ¨åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ï¼Œä½†ç”±äºæˆ‘ä»¬å¯¹ \(L^2\) æŸå¤±çš„å…¬å¼åŒ–ï¼Œæˆ‘ä»¬åªéœ€è¦è®¡ç®—è¾“å‡ºèŠ‚ç‚¹çš„è¯¯å·®ï¼Œä½†å†æ¬¡å¼ºè°ƒâ€”â€”å®ƒæ˜¯æŸå¤±å¯¼æ•°ã€‚
- en: '![](../Images/6abf780fa4544caa6735a8f4ad075bd2.png)'
  id: totrans-820
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6abf780fa4544caa6735a8f4ad075bd2.png)'
- en: Calculation of the loss derivative at the output of an output layer node, $O_6$.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—è¾“å‡ºå±‚èŠ‚ç‚¹ \(O_6\) çš„æŸå¤±å¯¼æ•°ã€‚
- en: For the example of our simple artificial neural network with the output at node,
    \(O_6\), our loss derivative is,
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬ç®€å•çš„äººå·¥ç¥ç»ç½‘ç»œç¤ºä¾‹ï¼Œè¾“å‡ºåœ¨èŠ‚ç‚¹ \(O_6\)ï¼Œæˆ‘ä»¬çš„æŸå¤±å¯¼æ•°æ˜¯ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial O_6} = \frac{\partial \mathcal{L}}{\hat{y}}
    = \hat{y} - y = O_6 - y \]
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial O_6} = \frac{\partial \mathcal{L}}{\hat{y}}
    = \hat{y} - y = O_6 - y \]
- en: So this is our loss derivative backpropagated to the output our output node,
    and we are now we are ready to backpropagate this loss derivative through our
    artificial neural network, letâ€™s talk about how we step through nodes and along
    connections.
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™æ˜¯æˆ‘ä»¬çš„æŸå¤±å¯¼æ•°åå‘ä¼ æ’­åˆ°è¾“å‡ºèŠ‚ç‚¹ï¼Œæˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œåå‘ä¼ æ’­è¿™ä¸ªæŸå¤±å¯¼æ•°ï¼Œè®©æˆ‘ä»¬è°ˆè°ˆå¦‚ä½•é€æ­¥é€šè¿‡èŠ‚ç‚¹å’Œæ²¿ç€è¿æ¥ã€‚
- en: Backpropagation through Output Node with Identity Activation
  id: totrans-825
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¸¦æœ‰èº«ä»½æ¿€æ´»çš„è¾“å‡ºèŠ‚ç‚¹åå‘ä¼ æ’­
- en: Letâ€™s backpropagate through our output node, \(O_6\), from post-activation to
    pre-activation. To do this we need the partial derivative our activation function.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡è¾“å‡ºèŠ‚ç‚¹ \(O_6\) ä»åæ¿€æ´»åˆ°å‰æ¿€æ´»åå‘ä¼ æ’­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ¿€æ´»å‡½æ•°çš„åå¯¼æ•°ã€‚
- en: since this is an output node with a regression artificial neural network I have
    selected the identity or linear activation function.
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºè¿™æ˜¯ä¸€ä¸ªå›å½’äººå·¥ç¥ç»ç½‘ç»œçš„è¾“å‡ºèŠ‚ç‚¹ï¼Œæˆ‘é€‰æ‹©äº†èº«ä»½æˆ–çº¿æ€§æ¿€æ´»å‡½æ•°ã€‚
- en: '![](../Images/1141c4e67c550f275e9079501fe522b2.png)'
  id: totrans-828
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1141c4e67c550f275e9079501fe522b2.png)'
- en: Backpropagation of the loss derivative through the node, $O_6$, from $O_6$ post-activation
    output to $O_{6_{in}}$ pre-activation input.
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡èŠ‚ç‚¹ \(O_6\) åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ï¼Œä» \(O_6\) åæ¿€æ´»è¾“å‡ºåˆ° \(O_{6_{in}}\) å‰æ¿€æ´»è¾“å…¥ã€‚
- en: 'The identity activation at output node \(O_6\) is defined as:'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŠ‚ç‚¹ \(O_6\) çš„èº«ä»½æ¿€æ´»å®šä¹‰ä¸ºï¼š
- en: \[ O_6 = \sigma(O_{6_{in}}) = O_6 \]
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \sigma(O_{6_{in}}) = O_6 \]
- en: The derivative of the identity activation at node \(O_6\) with respect to its
    input \(O_{6_{in}}\), i.e., crossing node \(O_6\) is,
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ \(O_6\) çš„èº«ä»½æ¿€æ´»ç›¸å¯¹äºå…¶è¾“å…¥ \(O_{6_{in}}\) çš„å¯¼æ•°ï¼Œå³ç©¿è¿‡èŠ‚ç‚¹ \(O_6\) æ˜¯ï¼Œ
- en: \[ \frac{\partial O_6}{\partial O_{6_{in}}} = \frac{\partial \left(O_{6_{in}}
    \right)}{\partial O_{6_{in}}} = 1.0 \]
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_6}{\partial O_{6_{in}}} = \frac{\partial \left(O_{6_{in}}
    \right)}{\partial O_{6_{in}}} = 1.0 \]
- en: Note, we just need \(O_6\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative with respect to the node
    output, \(\frac{\partial \mathcal{L}}{\partial O_6}\), and to the loss derivative
    with respect to the node input, \(\frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}}\),
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦èŠ‚ç‚¹ \(O_6\) çš„è¾“å‡ºä¿¡å·ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªä¿¡å·æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»èŠ‚ç‚¹è¾“å‡ºç›¸å¯¹äºæŸå¤±å¯¼æ•° \(\frac{\partial
    \mathcal{L}}{\partial O_6}\) åå‘ä¼ æ’­ï¼Œåˆ°èŠ‚ç‚¹è¾“å…¥ç›¸å¯¹äºæŸå¤±å¯¼æ•° \(\frac{\partial \mathcal{L}}{\partial
    O_{6_{\text{in}}}}\)ã€‚
- en: \[ \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = \frac{\partial
    O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6}
    = 1.0 \cdot (O_6 - y) \]
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = \frac{\partial
    O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6}
    = 1.0 \cdot (O_6 - y) \]
- en: Now that we have backpropagated through an output node, letâ€™s backpropagation
    along the \(H_4\) to \(O_6\) connection from the hidden layer.
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»é€šè¿‡è¾“å‡ºèŠ‚ç‚¹åå‘ä¼ æ’­ï¼Œè®©æˆ‘ä»¬ä»éšè—å±‚ \(H_4\) åˆ° \(O_6\) è¿æ¥åå‘ä¼ æ’­ã€‚
- en: Backpropagation along Connections
  id: totrans-837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ²¿ç€è¿æ¥åå‘ä¼ æ’­
- en: Now letâ€™s backpropagate along the connection between nodes \(O_6\) and \(H_4\).
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬åå‘ä¼ æ’­èŠ‚ç‚¹ \(O_6\) å’Œ \(H_4\) ä¹‹é—´çš„è¿æ¥ã€‚
- en: '![](../Images/1c686c4b3a6fe96317a79c7333542353.png)'
  id: totrans-839
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c686c4b3a6fe96317a79c7333542353.png)'
- en: Backpropagation of the loss derivative through node \(O_6\), from \(O_6\) post-activation
    output to $O_{6_{in}}$ pre-activation input and then along the connection to the
    output from node \(H_4\).
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡èŠ‚ç‚¹ \(O_6\) åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ï¼Œä» \(O_6\) åæ¿€æ´»è¾“å‡ºåˆ° \(O_{6_{in}}\) å‰æ¿€æ´»è¾“å…¥ï¼Œç„¶åæ²¿ç€è¿æ¥åˆ°èŠ‚ç‚¹ \(H_4\)
    çš„è¾“å‡ºã€‚
- en: Preactivation, the input to node \(ğ‘‚_6\) is calculated as,
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: å‰æ¿€æ´»ï¼ŒèŠ‚ç‚¹ \(O_6\) çš„è¾“å…¥è®¡ç®—å¦‚ä¸‹ï¼Œ
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
- en: We calculate the derivative along the connection as,
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—è¿æ¥ä¸Šçš„å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \frac{\partial}{\partial
    H_4} \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right) =
    \lambda_{4,6} \]
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \frac{\partial}{\partial
    H_4} \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right) =
    \lambda_{4,6} \]
- en: by resolving the above partial derivative, we see that backpropagation along
    a connection by applying the connection weight.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è§£ä¸Šè¿°åå¯¼æ•°ï¼Œæˆ‘ä»¬çœ‹åˆ°é€šè¿‡åº”ç”¨è¿æ¥æƒé‡è¿›è¡Œè¿æ¥çš„åå‘ä¼ æ’­ã€‚
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \lambda_{4,6} \]
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \lambda_{4,6} \]
- en: Note, we just need the current connection weight \(\lambda_{4,6}\). Now we can
    add this to our chain rule to backpropagate along the \(H_4\) to \(O_6\) connection
    from loss derivative with respect to the output layer node input \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\), to the loss derivative with respect
    to the hidden layer node output \(\frac{\partial \mathcal{L}}{\partial H_4}\).
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦å½“å‰çš„è¿æ¥æƒé‡ \(\lambda_{4,6}\)ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œæ²¿ç€ \(H_4\) åˆ° \(O_6\) çš„è¿æ¥ä»è¾“å‡ºå±‚èŠ‚ç‚¹è¾“å…¥çš„æŸå¤±å¯¼æ•°
    \(\frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}}\)ï¼Œåˆ°éšè—å±‚èŠ‚ç‚¹è¾“å‡ºçš„æŸå¤±å¯¼æ•° \(\frac{\partial
    \mathcal{L}}{\partial H_4}\) è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: \[ \frac{\partial \mathcal{L}}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \lambda_{4,6} \bigl( \cdot (1 - O_6) \cdot O_6 \bigr)
    \cdot (O_6 - y) \]
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \lambda_{4,6} \bigl( \cdot (1 - O_6) \cdot O_6 \bigr)
    \cdot (O_6 - y) \]
- en: Backpropagation through Nodes with Sigmoid Activation
  id: totrans-849
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡å…·æœ‰Sigmoidæ¿€æ´»çš„èŠ‚ç‚¹è¿›è¡Œåå‘ä¼ æ’­
- en: Letâ€™s backpropagate through a hidden layer node, \(H_4\), from postactivation
    to preactivation. To do this we need the partial derivative our activation function.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡éšè—å±‚èŠ‚ç‚¹ \(H_4\) ä»æ¿€æ´»ååˆ°æ¿€æ´»å‰è¿›è¡Œåå‘ä¼ æ’­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ¿€æ´»å‡½æ•°çš„åå¯¼æ•°ã€‚
- en: we are assuming sigmoid activation for all hidden layer nodes
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‡è®¾æ‰€æœ‰éšè—å±‚èŠ‚ç‚¹éƒ½ä½¿ç”¨sigmoidæ¿€æ´»
- en: for super clean logic, everyone resolves the activation derivative as a function
    of the output rather than as typical the input,
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†é€»è¾‘çš„æ¸…æ™°ï¼Œæ¯ä¸ªäººéƒ½å°†æ¿€æ´»å¯¼æ•°ä½œä¸ºè¾“å‡ºè€Œä¸æ˜¯å…¸å‹è¾“å…¥çš„å‡½æ•°æ¥è§£æï¼Œ
- en: '![](../Images/10ef488402716c36aa45b497ea54f6bb.png)'
  id: totrans-853
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/10ef488402716c36aa45b497ea54f6bb.png)'
- en: Backpropagation of the loss derivative through the node, $H_4$, from $H_4$ postactivation
    output to $H_{4_{in}}$ preactivation input.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ¿€æ´»åçš„è¾“å‡º \(H_4\) åˆ°æ¿€æ´»å‰çš„è¾“å…¥ \(H_{4_{in}}\) çš„èŠ‚ç‚¹ \(H_4\) çš„æŸå¤±å¯¼æ•°è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: 'The sigmoid activation at output node \(H_4\) is defined as:'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŠ‚ç‚¹ \(H_4\) çš„sigmoidæ¿€æ´»å®šä¹‰ä¸ºï¼š
- en: \[ H_4 = \sigma(H_{4_{in}}) = \frac{1}{1 + e^{-H_{4_{in}}}} \]
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_4 = \sigma(H_{4_{in}}) = \frac{1}{1 + e^{-H_{4_{in}}}} \]
- en: The derivative of the sigmoid activation at node \(H_4\) with respect to its
    input \(H_{4_{in}}\), i.e., crossing node \(H_4\) is,
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ \(H_4\) çš„sigmoidæ¿€æ´»ç›¸å¯¹äºå…¶è¾“å…¥ \(H_{4_{in}}\) çš„å¯¼æ•°ï¼Œå³ç©¿è¶ŠèŠ‚ç‚¹ \(H_4\) æ˜¯ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + e^{-H_{4_{in}}}} \right) \]
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + e^{-H_{4_{in}}}} \right) \]
- en: Now, for compact notation letâ€™s set,
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†ç®€æ´çš„è¡¨ç¤ºï¼Œæˆ‘ä»¬è®¾ï¼Œ
- en: \[ u = e^{-H_{4_{in}}} \]
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: \[ u = e^{-H_{4_{in}}} \]
- en: and substituting we have,
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: æ›¿æ¢åæˆ‘ä»¬å¾—åˆ°ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) \]
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) \]
- en: and by the chain rule we can extend it to,
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥æ‰©å±•åˆ°ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) = -\frac{u}{(1 + u)^2} \cdot \frac{\partial u}{\partial
    H_{4_{in}}} \]
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) = -\frac{u}{(1 + u)^2} \cdot \frac{\partial u}{\partial
    H_{4_{in}}} \]
- en: 'The derivative of \(u = e^{-H_{4_{in}}}\) with respect to \(H_{4_{in}}\) is:'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: \(u = e^{-H_{4_{in}}}\) å…³äº \(H_{4_{in}}\) çš„å¯¼æ•°æ˜¯ï¼š
- en: \[ \frac{\partial u}{\partial H_{4_{in}}} = -e^{-H_{4_{in}}} = -u \]
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial u}{\partial H_{4_{in}}} = -e^{-H_{4_{in}}} = -u \]
- en: now we can substitute,
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥æ›¿æ¢ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = -\frac{1}{(1+u)^2} \cdot (-u)
    = \frac{u}{(1+u)^2} \]
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = -\frac{1}{(1+u)^2} \cdot (-u)
    = \frac{u}{(1+u)^2} \]
- en: Express in terms of node \(H_4\) output, \(H_4 = \frac{1}{1 + u}\),
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨èŠ‚ç‚¹ \(H_4\) çš„è¾“å‡ºè¡¨ç¤ºï¼Œ\(H_4 = \frac{1}{1 + u}\)ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \frac{\left(1 - H_4\right)/H_4}{\left(1/H_4\right)^2}
    = \frac{1 - H_4}{H_4} \cdot H_4^2 = \left(1 - H_4\right) \cdot H_4 \]
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \frac{\left(1 - H_4\right)/H_4}{\left(1/H_4\right)^2}
    = \frac{1 - H_4}{H_4} \cdot H_4^2 = \left(1 - H_4\right) \cdot H_4 \]
- en: So we can backpropagate through our node, \(H_4\), from node post-activation
    output, \(H_4\) to node pre-activation input, \(H_{4_{in}}\), by,
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼é€šè¿‡èŠ‚ç‚¹ \(H_4\) ä»èŠ‚ç‚¹åæ¿€æ´»è¾“å‡º \(H_4\) åå‘ä¼ æ’­åˆ°èŠ‚ç‚¹å‰æ¿€æ´»è¾“å…¥ \(H_{4_{\text{in}}}\)ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \left(1 - H_4\right) \cdot
    H_4 \]
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \left(1 - H_4\right) \cdot
    H_4 \]
- en: Note, we just need \(H_4\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative to the output of node
    \(H_4\) and to the input of node \(H_4\),
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦èŠ‚ç‚¹ \(H_4\) çš„è¾“å‡ºä¿¡å· \(H_4\)ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªä¿¡å·æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»æŸå¤±å¯¼æ•°åå‘ä¼ æ’­åˆ°èŠ‚ç‚¹ \(H_4\)
    çš„è¾“å‡ºä»¥åŠèŠ‚ç‚¹ \(H_4\) çš„è¾“å…¥ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} = \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6}
    \cdot 1.0 \cdot (O_6 - y) \]
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} = \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6}
    \cdot 1.0 \cdot (O_6 - y) \]
- en: Now we can handle all cases of backpropagation through the nodes in our network.
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å¤„ç†ç½‘ç»œä¸­èŠ‚ç‚¹çš„æ‰€æœ‰åå‘ä¼ æ’­æƒ…å†µã€‚
- en: Backpropagation Along Another Connection
  id: totrans-876
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ²¿ç€å¦ä¸€ä¸ªè¿æ¥åå‘ä¼ æ’­
- en: For continuity and completeness, letâ€™s repeat the previously described method
    to backpropagate along the connection \(I_1\) to \(H_4\).
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿ç»­æ€§å’Œå®Œæ•´æ€§ï¼Œè®©æˆ‘ä»¬é‡å¤ä¹‹å‰æè¿°çš„æ–¹æ³•ï¼Œæ²¿ç€è¿æ¥ \(I_1\) åˆ° \(H_4\) åå‘ä¼ æ’­ã€‚
- en: '![](../Images/76f408b8ed852b03a168e6069bb716db.png)'
  id: totrans-878
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76f408b8ed852b03a168e6069bb716db.png)'
- en: Backpropagation of the loss derivative along the connection from $H_4$ to $I_1$.
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¿ç€ä» \(H_4\) åˆ° \(I_1\) çš„è¿æ¥åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ã€‚
- en: Once again, preactivation the input to node \(H_4\) is calculated as,
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼ŒèŠ‚ç‚¹ \(H_4\) çš„å‰æ¿€æ´»è¾“å…¥è®¡ç®—å¦‚ä¸‹ï¼Œ
- en: \[ H_{4_{\text{in}}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4_{\text{in}}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]
- en: We calculate the derivative along the connection as,
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ²¿ç€è¿æ¥è®¡ç®—å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \frac{\partial \left(\lambda_{1,4}
    \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 + b_4 \right)}{\partial
    I_1} = \lambda_{1,4} \]
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \frac{\partial \left(\lambda_{1,4}
    \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 + b_4 \right)}{\partial
    I_1} = \lambda_{1,4} \]
- en: by resolving the above partial derivative, we see that backpropagation along
    a connection by applying the connection weight.
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è§£å†³ä¸Šè¿°åå¯¼æ•°ï¼Œæˆ‘ä»¬çœ‹åˆ°é€šè¿‡åº”ç”¨è¿æ¥æƒé‡è¿›è¡Œè¿æ¥çš„åå‘ä¼ æ’­ï¼Œ
- en: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \lambda_{4,6} \]
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \lambda_{4,6} \]
- en: Note, we just need the current connection weight \(\lambda_{4,6}\). Now we can
    add this to our chain rule to backpropagate along the \(H_4\) to \(O_6\) connection
    from loss derivative with respect to the output layer node input \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\), to the loss derivative with respect
    to the hidden layer node output \(\frac{\partial \mathcal{L}}{\partial H_4}\).
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦å½“å‰è¿æ¥æƒé‡ \(\lambda_{4,6}\)ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæƒé‡æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»è¾“å‡ºå±‚èŠ‚ç‚¹è¾“å…¥çš„æŸå¤±å¯¼æ•° \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\) åå‘ä¼ æ’­åˆ°éšè—å±‚èŠ‚ç‚¹è¾“å‡ºçš„æŸå¤±å¯¼æ•° \(\frac{\partial
    \mathcal{L}}{\partial H_4}\)ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) \]
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) \]
- en: Accounting for Multiple Paths
  id: totrans-888
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è€ƒè™‘å¤šæ¡è·¯å¾„
- en: Our loss derivative with respect to the node output \(I_1\), \(\frac{\partial
    \mathcal{L}}{\partial I_1}\) is not correct!
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å…³äºèŠ‚ç‚¹è¾“å‡º \(I_1\) çš„æŸå¤±å¯¼æ•° \(\frac{\partial \mathcal{L}}{\partial I_1}\) æ˜¯ä¸æ­£ç¡®çš„ï¼
- en: we accounted for the \(O_6\) to \(H_4\) to \(I_1\) path, but we did not acccount
    for the \(O_6\) to \(H_5\) to \(I_1\) path
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è€ƒè™‘äº†ä» \(O_6\) åˆ° \(H_4\) åˆ° \(I_1\) çš„è·¯å¾„ï¼Œä½†æ²¡æœ‰è€ƒè™‘ä» \(O_6\) åˆ° \(H_5\) åˆ° \(I_1\) çš„è·¯å¾„
- en: '![](../Images/6a68404e4b3d2589dcfe15beb5175c60.png)'
  id: totrans-891
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a68404e4b3d2589dcfe15beb5175c60.png)'
- en: Multiple paths for backpropagation to input node, $I_1$, from output node $O_6$.
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¾“å‡ºèŠ‚ç‚¹ $O_6$ åˆ°è¾“å…¥èŠ‚ç‚¹ $I_1$ çš„åå‘ä¼ æ’­è·¯å¾„ã€‚
- en: To account for multiple paths we just need to sum over all the paths.
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è€ƒè™‘å¤šæ¡è·¯å¾„ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹æ‰€æœ‰è·¯å¾„è¿›è¡Œæ±‚å’Œã€‚
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
- en: we can evaluate this as,
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†å…¶è¯„ä¼°ä¸ºï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) + \lambda_{1,5}
    \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \cdot 1.0 \cdot (O_6
    - y) \]
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) + \lambda_{1,5}
    \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \cdot 1.0 \cdot (O_6
    - y) \]
- en: and then simplify by removing the 1.0 values and grouping terms as,
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åé€šè¿‡ç§»é™¤ 1.0 å€¼å’Œåˆå¹¶é¡¹æ¥ç®€åŒ–ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
- en: and now we can evaluate this simplified form as,
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªç®€åŒ–çš„å½¢å¼ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1 - H_5)
    \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1 - H_5)
    \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
- en: Backpropagation through Input Nodes with Identity Activation
  id: totrans-901
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡å…·æœ‰æ’ç­‰æ¿€æ´»çš„è¾“å…¥èŠ‚ç‚¹è¿›è¡Œåå‘ä¼ æ’­
- en: Letâ€™s backpropagate through our input node, \(I_1\), from postactivation to
    preactivation. To do this we need the partial derivative our activation function.
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡è¾“å…¥èŠ‚ç‚¹ \(I_1\) ä»åæ¿€æ´»åˆ°å‰æ¿€æ´»è¿›è¡Œåå‘ä¼ æ’­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ¿€æ´»å‡½æ•°çš„åå¯¼æ•°ã€‚
- en: since this is an input node I have selected the identity or linear activation
    function.
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºè¿™æ˜¯ä¸€ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œæˆ‘é€‰æ‹©äº†æ’ç­‰æˆ–çº¿æ€§æ¿€æ´»å‡½æ•°ã€‚
- en: '![](../Images/bd05208edfb4ac5d5cc0f8cfe808ae50.png)'
  id: totrans-904
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd05208edfb4ac5d5cc0f8cfe808ae50.png)'
- en: Backpropagation of the loss derivative through the node, $I_1$, from $I_1$ postactivation
    output to $I_{1_{in}}$ preactivation input.
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡èŠ‚ç‚¹ $I_1$ çš„æŸå¤±å¯¼æ•°åå‘ä¼ æ’­ï¼Œä» $I_1$ çš„åæ¿€æ´»è¾“å‡ºåˆ° $I_{1_{in}}$ çš„å‰æ¿€æ´»è¾“å…¥ã€‚
- en: 'The identity activation at output node \(I_1\) is defined as:'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŠ‚ç‚¹ \(I_1\) çš„æ’ç­‰æ¿€æ´»å®šä¹‰ä¸ºï¼š
- en: \[ I_1 = \sigma(I_{1_{in}}) = I_1 \]
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: \[ I_1 = \sigma(I_{1_{in}}) = I_1 \]
- en: The derivative of the identity activation at node \(I_1\) with respect to its
    input \(I_{1_{in}}\), i.e., passing through node \(I_1\) is,
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ \(I_1\) çš„æ’ç­‰æ¿€æ´»ç›¸å¯¹äºå…¶è¾“å…¥ \(I_{1_{in}}\) çš„å¯¼æ•°ï¼Œå³é€šè¿‡èŠ‚ç‚¹ \(I_1\) ä¼ é€’ï¼Œæ˜¯ï¼š
- en: \[ \frac{\partial I_1}{\partial I_{1_{in}}} = \frac{\partial \left(I_{1_{in}}
    \right)}{\partial I_{1_{in}}} = 1.0 \]
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial I_1}{\partial I_{1_{in}}} = \frac{\partial \left(I_{1_{in}}
    \right)}{\partial I_{1_{in}}} = 1.0 \]
- en: Note, we just need \(I_1\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative with respect to the node
    output, \(\frac{\partial \mathcal{L}}{\partial I_1}\), and to the loss derivative
    with respect to the node input, \(\frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}}\),
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦ä»èŠ‚ç‚¹è¾“å‡ºçš„ä¿¡å·\(I_1\)ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»èŠ‚ç‚¹è¾“å‡ºçš„æŸå¤±å¯¼æ•°\(\frac{\partial \mathcal{L}}{\partial
    I_1}\)åå‘ä¼ æ’­åˆ°èŠ‚ç‚¹è¾“å…¥çš„æŸå¤±å¯¼æ•°\(\frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}}\)ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \frac{\partial I_1}{\partial
    I_{1_{in}}} \cdot \frac{\partial H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} + \frac{\partial I_1}{\partial I_{1_{in}}} \cdot \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial
    O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \frac{\partial I_1}{\partial
    I_{1_{in}}} \cdot \frac{\partial H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} + \frac{\partial I_1}{\partial I_{1_{in}}} \cdot \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial
    O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
- en: we can evaluate this as,
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¯„ä¼°ä¸ºï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = 1.0 \cdot \lambda_{1,4}
    \cdot \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6
    - y) + 1.0 \cdot \lambda_{1,5} \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6}
    \cdot 1.0 \cdot (O_6 - y) \]
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = 1.0 \cdot \lambda_{1,4}
    \cdot \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6
    - y) + 1.0 \cdot \lambda_{1,5} \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6}
    \cdot 1.0 \cdot (O_6 - y) \]
- en: For fun I designed this notation for maximum clarity,
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¶£å‘³æ€§ï¼Œæˆ‘è®¾è®¡äº†è¿™ç§è¡¨ç¤ºæ³•ä»¥å®ç°æœ€å¤§æ¸…æ™°åº¦ï¼Œ
- en: \[ \frac{\partial L}{\partial I_{1_{\text{in}}}} = \overbrace{1.0}^{\textstyle
    \frac{\partial I_{1}}{\partial I_{1_{in}}}} \left[ \overbrace{\lambda_{1,4}}^{\textstyle
    \frac{\partial H_{4_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_4) \cdot
    H_4}^{\textstyle \frac{\partial H_4}{\partial H_{4_{\text{in}}}}} \cdot \overbrace{\lambda_{4,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_4}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} + \overbrace{\lambda_{1,5}}^{\textstyle \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_5) \cdot H_5}^{\textstyle
    \frac{\partial H_5}{\partial H_{5_{\text{in}}}}} \cdot \overbrace{\lambda_{5,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_5}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} \right] \]
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial I_{1_{\text{in}}}} = \overbrace{1.0}^{\textstyle
    \frac{\partial I_{1}}{\partial I_{1_{in}}}} \left[ \overbrace{\lambda_{1,4}}^{\textstyle
    \frac{\partial H_{4_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_4) \cdot
    H_4}^{\textstyle \frac{\partial H_4}{\partial H_{4_{\text{in}}}}} \cdot \overbrace{\lambda_{4,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_4}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} + \overbrace{\lambda_{1,5}}^{\textstyle \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_5) \cdot H_5}^{\textstyle
    \frac{\partial H_5}{\partial H_{5_{\text{in}}}}} \cdot \overbrace{\lambda_{5,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_5}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} \right] \]
- en: But this can be simplified by removing the 1.0 values and grouping terms as,
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™å¯ä»¥é€šè¿‡ç§»é™¤1.0å€¼å¹¶ç»„åˆé¡¹æ¥ç®€åŒ–ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
- en: and now we can evaluate this simplified form as,
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªç®€åŒ–çš„å½¢å¼ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \lambda_{1,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \lambda_{1,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
- en: For completeness here is the backpropagation for the other input nodes, hereâ€™s
    \(\frac{\partial \mathcal{L}}{\partial I_{2_{in}}}\),
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œæä¾›äº†å…¶ä»–è¾“å…¥èŠ‚ç‚¹çš„åå‘ä¼ æ’­ï¼Œè¿™é‡Œæ˜¯ \(\frac{\partial \mathcal{L}}{\partial I_{2_{in}}}\)ï¼Œ
- en: '![](../Images/9b186fef88b4e0ad1fa6191394b0e0dd.png)'
  id: totrans-921
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b186fef88b4e0ad1fa6191394b0e0dd.png)'
- en: Backpropagation of the loss derivative for input node 2.
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥èŠ‚ç‚¹2çš„æŸå¤±å¯¼æ•°çš„åå‘ä¼ æ’­ã€‚
- en: For brevity I have remove the 1.0s and grouped like terms,
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´ï¼Œæˆ‘å·²ç§»é™¤1.0så¹¶å°†åŒç±»é¡¹åˆ†ç»„ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_2} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_2} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_2} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_2} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
- en: and now we can evaluate this simplified form as,
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªç®€åŒ–çš„å½¢å¼ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \lambda_{2,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{2,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \lambda_{2,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{2,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
- en: and here is \(\frac{\partial \mathcal{L}}{\partial I_{3_{in}}}\),
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ \(\frac{\partial \mathcal{L}}{\partial I_{3_{in}}}\)ï¼Œ
- en: '![](../Images/90c6a46319d78ae1b9d4f8cdc131fd2a.png)'
  id: totrans-928
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90c6a46319d78ae1b9d4f8cdc131fd2a.png)'
- en: Backpropagation of the loss derivative for input node 3.
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥èŠ‚ç‚¹3çš„æŸå¤±å¯¼æ•°çš„åå‘ä¼ æ’­ã€‚
- en: For brevity I have remove the 1.0s and grouped like terms,
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´ï¼Œæˆ‘å·²ç§»é™¤1.0så¹¶å°†åŒç±»é¡¹åˆ†ç»„ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_3} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_3} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_3} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_3} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
- en: and now we can evaluate this simplified form as,
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªç®€åŒ–çš„å½¢å¼ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \lambda_{3,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{3,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \lambda_{3,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{3,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
- en: Loss Derivatives with Respect to Weights and Biases
  id: totrans-934
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…³äºæƒé‡å’Œåç½®çš„æŸå¤±å¯¼æ•°
- en: Now we have back propagated the loss derivative through our network.
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»é€šè¿‡æˆ‘ä»¬çš„ç½‘ç»œåå‘ä¼ æ’­äº†æŸå¤±å¯¼æ•°ã€‚
- en: '![](../Images/97de6228871f7910de4d7a8fa86e0bd8.png)'
  id: totrans-936
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97de6228871f7910de4d7a8fa86e0bd8.png)'
- en: Backpropagated loss derivatives with respect to all network nodes inputs and
    outputs.
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæ‰€æœ‰ç½‘ç»œèŠ‚ç‚¹è¾“å…¥å’Œè¾“å‡ºçš„æŸå¤±å¯¼æ•°çš„åå‘ä¼ æ’­ã€‚
- en: and we have the loss derivative with respect to the input and output of each
    node in our network,
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°äº†ç½‘ç»œä¸­æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥å’Œè¾“å‡ºçš„æŸå¤±å¯¼æ•°ã€‚
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial I_1},\quad \frac{\partial \mathcal{L}}{\partial I_{2_{\text{in}}}},\quad
    \frac{\partial \mathcal{L}}{\partial I_2},\quad \frac{\partial \mathcal{L}}{\partial
    I_{3_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial I_3},\quad \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial
    H_4},\quad \frac{\partial \mathcal{L}}{\partial H_5},\quad \frac{\partial \mathcal{L}}{\partial
    H_5},\quad \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial O_6} \]
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial I_1},\quad \frac{\partial \mathcal{L}}{\partial I_{2_{\text{in}}}},\quad
    \frac{\partial \mathcal{L}}{\partial I_2},\quad \frac{\partial \mathcal{L}}{\partial
    I_{3_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial I_3},\quad \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial
    H_4},\quad \frac{\partial \mathcal{L}}{\partial H_5},\quad \frac{\partial \mathcal{L}}{\partial
    H_5},\quad \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial O_6} \]
- en: But what we actually need is the loss derivative with respect to each connection
    weights,
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬å®é™…ä¸Šéœ€è¦çš„æ˜¯æ¯ä¸ªè¿æ¥æƒé‡çš„æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}},\quad \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,4}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{1,5}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{2,5}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,5}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{5,6}} \]
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}},\quad \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,4}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{1,5}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{2,5}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,5}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{5,6}} \]
- en: and node biases,
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠèŠ‚ç‚¹åå·®ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial b_4},\quad \frac{\partial \mathcal{L}}{\partial
    b_5},\quad \frac{\partial \mathcal{L}}{\partial b_6} \]
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial b_4},\quad \frac{\partial \mathcal{L}}{\partial
    b_5},\quad \frac{\partial \mathcal{L}}{\partial b_6} \]
- en: How do we backpropagate the loss derivative to a connection weight? Letâ€™s start
    with the \(H_4\) to \(O_6\) connection.
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•å°†æŸå¤±å¯¼æ•°åå‘ä¼ æ’­åˆ°è¿æ¥æƒé‡ï¼Ÿè®©æˆ‘ä»¬ä» \(H_4\) åˆ° \(O_6\) çš„è¿æ¥å¼€å§‹ã€‚
- en: '![](../Images/bf1e650cd4b0613fa09aa5d5c0e72e26.png)'
  id: totrans-945
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf1e650cd4b0613fa09aa5d5c0e72e26.png)'
- en: Backpropagated loss derivatives with respect to a connection weight.
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿æ¥æƒé‡ç›¸å…³çš„åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ã€‚
- en: Preactivation, input to node \(O_6\) we have,
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æ¿€æ´»ï¼ŒèŠ‚ç‚¹ \(O_6\) çš„è¾“å…¥ä¸ºï¼Œ
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
- en: We calculate the derivative with respect to the connection weight as,
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—è¿æ¥æƒé‡çš„å¯¼æ•°å¦‚ä¸‹ï¼Œ
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial \lambda_{4,6}} = \frac{\partial
    \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial
    \lambda_{4,6}} = H_4 \]
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_{6_{\text{in}}}}{\partial \lambda_{4,6}} = \frac{\partial
    \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial
    \lambda_{4,6}} = H_4 \]
- en: We need the output of the node in the previous layer passed along the connection
    to backpropagate to the loss derivative with respect to the connection weight
    from the input to the next node,
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦å‰ä¸€å±‚èŠ‚ç‚¹é€šè¿‡è¿æ¥ä¼ é€’çš„è¾“å‡ºï¼Œä»¥åå‘ä¼ æ’­åˆ°è¿æ¥æƒé‡ç›¸å¯¹äºè¾“å…¥åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot 1.0 \cdot (O_6 - y) \]
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot 1.0 \cdot (O_6 - y) \]
- en: Now, for completeness, here are the equations for all of our networkâ€™s connection
    weights.
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œæ˜¯æˆ‘ä»¬ç½‘ç»œæ‰€æœ‰è¿æ¥æƒé‡çš„æ–¹ç¨‹ã€‚
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_5 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_5 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
- en: See the pattern, the loss derivatives with respect to connection weights are,
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹çœ‹è¿™ä¸ªæ¨¡å¼ï¼Œå…³äºè¿æ¥æƒé‡çš„æŸå¤±å¯¼æ•°æ˜¯ï¼Œ
- en: \[ \text{Connection Signal} \times \text{Loss Derivative of Next Node Input}
    \]
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{è¿æ¥ä¿¡å·} \times \text{ä¸‹ä¸€ä¸ªèŠ‚ç‚¹è¾“å…¥çš„æŸå¤±å¯¼æ•°} \]
- en: Now how do we backpropagate the loss derivative to a node bias? Letâ€™s start
    with the \(O_6\) node.
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¦‚ä½•å°†æŸå¤±å¯¼æ•°é€†å‘ä¼ æ’­åˆ°èŠ‚ç‚¹åå·®ï¼Ÿè®©æˆ‘ä»¬ä» \(O_6\) èŠ‚ç‚¹å¼€å§‹ã€‚
- en: '![](../Images/646997a228f8c1e8a3b2743ac13e388a.png)'
  id: totrans-958
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/646997a228f8c1e8a3b2743ac13e388a.png)'
- en: Backpropagated loss derivatives with respect to a node bias.
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºèŠ‚ç‚¹åå·®çš„é€†å‘ä¼ æ’­æŸå¤±å¯¼æ•°ã€‚
- en: Once again, the preactivation, input to node \(O_6\) is,
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼ŒèŠ‚ç‚¹ \(O_6\) çš„å‰æ¿€æ´»è¾“å…¥æ˜¯ï¼Œ
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
- en: We calculate the derivative of a connection weight as,
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—è¿æ¥æƒé‡çš„å¯¼æ•°å¦‚ä¸‹ï¼Œ
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial b_6} = \frac{\partial \left( \lambda_{4,6}
    \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial b_6} = 1.0 \]
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_{6_{\text{in}}}}{\partial b_6} = \frac{\partial \left( \lambda_{4,6}
    \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial b_6} = 1.0 \]
- en: so our bias loss derivative is equal to the node input loss derivative,
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬çš„åå·®æŸå¤±å¯¼æ•°ç­‰äºèŠ‚ç‚¹è¾“å…¥æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial O_{6_{\text{in}}}}{\partial
    b_6} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = 1.0 \cdot
    \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial O_{6_{\text{in}}}}{\partial
    b_6} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = 1.0 \cdot
    \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
- en: For completeness here are all the loss derivatives with respect to node biases,
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œåˆ—å‡ºäº†æ‰€æœ‰å…³äºèŠ‚ç‚¹åç½®çš„æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial \mathcal{L}}{\partial
    O_{6_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial b_4} = \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial
    b_5} = \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial \mathcal{L}}{\partial
    O_{6_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial b_4} = \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial
    b_5} = \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]
- en: See the pattern, the loss derivatives with respect to node biases are,
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹çœ‹è¿™ä¸ªæ¨¡å¼ï¼Œå…³äºèŠ‚ç‚¹åç½®çš„æŸå¤±å¯¼æ•°æ˜¯ï¼Œ
- en: \[ \text{Loss Derivative of the Node Input} \]
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{èŠ‚ç‚¹è¾“å…¥çš„æŸå¤±å¯¼æ•°} \]
- en: Calculating the Loss Derivative
  id: totrans-970
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±å¯¼æ•°
- en: Backpropagation is based on the concept of allocating or propagating the loss
    derivative backwards through the neural network,
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­åŸºäºå°†æŸå¤±å¯¼æ•°å‘ååˆ†é…æˆ–ä¼ æ’­é€šè¿‡ç¥ç»ç½‘ç»œçš„æ¦‚å¿µï¼Œ
- en: we calculate the loss derivative and then distribute it sequentially, in reverse
    direction, from network output back towards the network input
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—æŸå¤±å¯¼æ•°ï¼Œç„¶åæŒ‰é¡ºåºä»ç½‘ç»œè¾“å‡ºåå‘åˆ†å¸ƒåˆ°ç½‘ç»œè¾“å…¥
- en: it is important to know that we are working with derivatives, and that backpropagation
    is NOT distributing error, although as you will see it may look that way!
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯è¦çŸ¥é“æˆ‘ä»¬æ­£åœ¨å¤„ç†å¯¼æ•°ï¼Œå¹¶ä¸”åå‘ä¼ æ’­ä¸æ˜¯åˆ†é…é”™è¯¯ï¼Œå°½ç®¡å¦‚æ‚¨æ‰€è§ï¼Œå®ƒå¯èƒ½çœ‹èµ·æ¥æ˜¯è¿™æ ·ï¼
- en: We start by defining the loss, given the truth, \(ğ‘¦\), and our prediction, \(\hat{y}
    = O_6\), we calculate our \(L^2\) loss as,
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå®šä¹‰æŸå¤±ï¼Œç»™å®šçœŸå®å€¼ \(ğ‘¦\) å’Œæˆ‘ä»¬çš„é¢„æµ‹ \(\hat{y} = O_6\)ï¼Œæˆ‘ä»¬è®¡ç®—æˆ‘ä»¬çš„ \(L^2\) æŸå¤±å¦‚ä¸‹ï¼Œ
- en: \[ L = \frac{1}{2} \left( \hat{y} - y \right)^2 \]
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L = \frac{1}{2} \left( \hat{y} - y \right)^2 \]
- en: our choice of loss function allows us to use the prediction error as the loss
    derivative! We calculate the loss derivative as the partial derivative of the
    loss with respect to the estimate, \(\frac{\partial ğ¿}{\partial \hat{y}}\),
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€‰æ‹©çš„æŸå¤±å‡½æ•°å…è®¸æˆ‘ä»¬ä½¿ç”¨é¢„æµ‹è¯¯å·®ä½œä¸ºæŸå¤±å¯¼æ•°ï¼æˆ‘ä»¬è®¡ç®—æŸå¤±å¯¼æ•°ä½œä¸ºæŸå¤±ç›¸å¯¹äºä¼°è®¡çš„åå¯¼æ•°ï¼Œ\(\frac{\partial ğ¿}{\partial
    \hat{y}}\)ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial \hat{y}} = \frac{\partial \frac{1}{2}
    \left( \hat{y} - y \right)^2 }{\partial \hat{y}} = \hat{y} - y \]
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial \hat{y}} = \frac{\partial \frac{1}{2}
    \left( \hat{y} - y \right)^2 }{\partial \hat{y}} = \hat{y} - y \]
- en: You see what I mean, we are backpropagating the loss derivative, but due to
    our formulation of the \(L^2\) loss, we only have to calculate the error at our
    output node output, but once again - it is the loss derivative.
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥çœ‹åˆ°æˆ‘çš„æ„æ€ï¼Œæˆ‘ä»¬æ­£åœ¨åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ï¼Œä½†ç”±äºæˆ‘ä»¬ \(L^2\) æŸå¤±çš„å…¬å¼ï¼Œæˆ‘ä»¬åªéœ€è¦è®¡ç®—è¾“å‡ºèŠ‚ç‚¹çš„é”™è¯¯ï¼Œä½†å†æ¬¡å¼ºè°ƒâ€”â€”å®ƒæ˜¯æŸå¤±å¯¼æ•°ã€‚
- en: '![](../Images/6abf780fa4544caa6735a8f4ad075bd2.png)'
  id: totrans-979
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6abf780fa4544caa6735a8f4ad075bd2.png)'
- en: Calculation of the loss derivative at the output of an output layer node, $O_6$.
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºå±‚èŠ‚ç‚¹ \(O_6\) å¤„è®¡ç®—æŸå¤±å¯¼æ•°ã€‚
- en: For the example of our simple artificial neural network with the output at node,
    \(O_6\), our loss derivative is,
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„ç®€å•äººå·¥ç¥ç»ç½‘ç»œç¤ºä¾‹ï¼Œè¾“å‡ºåœ¨èŠ‚ç‚¹ \(O_6\)ï¼Œæˆ‘ä»¬çš„æŸå¤±å¯¼æ•°æ˜¯ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial O_6} = \frac{\partial \mathcal{L}}{\hat{y}}
    = \hat{y} - y = O_6 - y \]
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial O_6} = \frac{\partial \mathcal{L}}{\hat{y}}
    = \hat{y} - y = O_6 - y \]
- en: So this is our loss derivative backpropagated to the output our output node,
    and we are now we are ready to backpropagate this loss derivative through our
    artificial neural network, letâ€™s talk about how we step through nodes and along
    connections.
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™æ˜¯æˆ‘ä»¬åå‘ä¼ æ’­åˆ°è¾“å‡ºèŠ‚ç‚¹ï¼Œå³è¾“å‡ºèŠ‚ç‚¹ï¼Œæˆ‘ä»¬ç°åœ¨å‡†å¤‡é€šè¿‡æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œåå‘ä¼ æ’­è¿™ä¸ªæŸå¤±å¯¼æ•°ï¼Œè®©æˆ‘ä»¬è°ˆè°ˆæˆ‘ä»¬å¦‚ä½•é€æ­¥é€šè¿‡èŠ‚ç‚¹å’Œè¿æ¥ã€‚
- en: Backpropagation through Output Node with Identity Activation
  id: totrans-984
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡å…·æœ‰æ’ç­‰æ¿€æ´»å‡½æ•°çš„è¾“å‡ºèŠ‚ç‚¹è¿›è¡Œåå‘ä¼ æ’­
- en: Letâ€™s backpropagate through our output node, \(O_6\), from post-activation to
    pre-activation. To do this we need the partial derivative our activation function.
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡è¾“å‡ºèŠ‚ç‚¹ \(O_6\) ä»åæ¿€æ´»åˆ°å‰æ¿€æ´»è¿›è¡Œåå‘ä¼ æ’­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ¿€æ´»å‡½æ•°çš„åå¯¼æ•°ã€‚
- en: since this is an output node with a regression artificial neural network I have
    selected the identity or linear activation function.
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºè¿™æ˜¯ä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹ï¼Œå¹¶ä¸”æˆ‘é€‰æ‹©çš„æ˜¯å›å½’äººå·¥ç¥ç»ç½‘ç»œä¸­çš„æ’ç­‰æˆ–çº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œ
- en: '![](../Images/1141c4e67c550f275e9079501fe522b2.png)'
  id: totrans-987
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1141c4e67c550f275e9079501fe522b2.png)'
- en: Backpropagation of the loss derivative through the node, $O_6$, from $O_6$ post-activation
    output to $O_{6_{in}}$ pre-activation input.
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡èŠ‚ç‚¹ \(O_6\) ä»åæ¿€æ´»è¾“å‡ºåˆ°é¢„æ¿€æ´»è¾“å…¥çš„åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ã€‚
- en: 'The identity activation at output node \(O_6\) is defined as:'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŠ‚ç‚¹ \(O_6\) çš„æ’ç­‰æ¿€æ´»å®šä¹‰ä¸ºï¼š
- en: \[ O_6 = \sigma(O_{6_{in}}) = O_6 \]
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_6 = \sigma(O_{6_{in}}) = O_6 \]
- en: The derivative of the identity activation at node \(O_6\) with respect to its
    input \(O_{6_{in}}\), i.e., crossing node \(O_6\) is,
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ \(O_6\) çš„æ’ç­‰æ¿€æ´»ç›¸å¯¹äºå…¶è¾“å…¥ \(O_{6_{in}}\) çš„å¯¼æ•°ï¼Œå³é€šè¿‡èŠ‚ç‚¹ \(O_6\) çš„äº¤å‰æ˜¯ï¼Œ
- en: \[ \frac{\partial O_6}{\partial O_{6_{in}}} = \frac{\partial \left(O_{6_{in}}
    \right)}{\partial O_{6_{in}}} = 1.0 \]
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_6}{\partial O_{6_{in}}} = \frac{\partial \left(O_{6_{in}}
    \right)}{\partial O_{6_{in}}} = 1.0 \]
- en: Note, we just need \(O_6\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative with respect to the node
    output, \(\frac{\partial \mathcal{L}}{\partial O_6}\), and to the loss derivative
    with respect to the node input, \(\frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}}\),
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦èŠ‚ç‚¹ \(O_6\) çš„è¾“å‡ºä¿¡å·ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªä¿¡å·æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»¥ä¾¿ä»èŠ‚ç‚¹è¾“å‡ºçš„æŸå¤±å¯¼æ•° \(\frac{\partial
    \mathcal{L}}{\partial O_6}\) å’ŒèŠ‚ç‚¹è¾“å…¥çš„æŸå¤±å¯¼æ•° \(\frac{\partial \mathcal{L}}{\partial
    O_{6_{\text{in}}}}\) è¿›è¡Œåå‘ä¼ æ’­ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = \frac{\partial
    O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6}
    = 1.0 \cdot (O_6 - y) \]
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = \frac{\partial
    O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6}
    = 1.0 \cdot (O_6 - y) \]
- en: Now that we have backpropagated through an output node, letâ€™s backpropagation
    along the \(H_4\) to \(O_6\) connection from the hidden layer.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»é€šè¿‡ä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹è¿›è¡Œäº†åå‘ä¼ æ’­ï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬æ²¿ç€ä»éšè—å±‚åˆ° \(H_4\) åˆ° \(O_6\) çš„è¿æ¥è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: Backpropagation along Connections
  id: totrans-996
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ²¿ç€è¿æ¥è¿›è¡Œåå‘ä¼ æ’­
- en: Now letâ€™s backpropagate along the connection between nodes \(O_6\) and \(H_4\).
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ²¿ç€èŠ‚ç‚¹ \(O_6\) å’Œ \(H_4\) ä¹‹é—´çš„è¿æ¥è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: '![](../Images/1c686c4b3a6fe96317a79c7333542353.png)'
  id: totrans-998
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c686c4b3a6fe96317a79c7333542353.png)'
- en: Backpropagation of the loss derivative through node \(O_6\), from \(O_6\) post-activation
    output to $O_{6_{in}}$ pre-activation input and then along the connection to the
    output from node \(H_4\).
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡èŠ‚ç‚¹ \(O_6\) ä»åæ¿€æ´»è¾“å‡ºåˆ° \(O_{6_{in}}\) é¢„æ¿€æ´»è¾“å…¥çš„åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ï¼Œç„¶åæ²¿ç€è¿æ¥åˆ°èŠ‚ç‚¹ \(H_4\) çš„è¾“å‡ºã€‚
- en: Preactivation, the input to node \(ğ‘‚_6\) is calculated as,
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æ¿€æ´»ï¼ŒèŠ‚ç‚¹ \(ğ‘‚_6\) çš„è¾“å…¥è®¡ç®—å¦‚ä¸‹ï¼Œ
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
- en: We calculate the derivative along the connection as,
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—è¿æ¥çš„å¯¼æ•°å¦‚ä¸‹ï¼Œ
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \frac{\partial}{\partial
    H_4} \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right) =
    \lambda_{4,6} \]
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \frac{\partial}{\partial
    H_4} \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right) =
    \lambda_{4,6} \]
- en: by resolving the above partial derivative, we see that backpropagation along
    a connection by applying the connection weight.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è§£ä¸Šè¿°åå¯¼æ•°ï¼Œæˆ‘ä»¬çœ‹åˆ°é€šè¿‡åº”ç”¨è¿æ¥æƒé‡è¿›è¡Œè¿æ¥çš„åå‘ä¼ æ’­ã€‚
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \lambda_{4,6} \]
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \lambda_{4,6} \]
- en: Note, we just need the current connection weight \(\lambda_{4,6}\). Now we can
    add this to our chain rule to backpropagate along the \(H_4\) to \(O_6\) connection
    from loss derivative with respect to the output layer node input \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\), to the loss derivative with respect
    to the hidden layer node output \(\frac{\partial \mathcal{L}}{\partial H_4}\).
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦å½“å‰çš„è¿æ¥æƒé‡ \(\lambda_{4,6}\)ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæƒé‡æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»¥ä¾¿æ²¿ç€ä»æŸå¤±å¯¹è¾“å‡ºå±‚èŠ‚ç‚¹è¾“å…¥çš„å¯¼æ•°
    \(\frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}}\) åˆ°æŸå¤±å¯¹éšè—å±‚èŠ‚ç‚¹è¾“å‡ºçš„å¯¼æ•° \(\frac{\partial
    \mathcal{L}}{\partial H_4}\) çš„ \(H_4\) åˆ° \(O_6\) è¿æ¥è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: \[ \frac{\partial \mathcal{L}}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \lambda_{4,6} \bigl( \cdot (1 - O_6) \cdot O_6 \bigr)
    \cdot (O_6 - y) \]
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \lambda_{4,6} \bigl( \cdot (1 - O_6) \cdot O_6 \bigr)
    \cdot (O_6 - y) \]
- en: Backpropagation through Nodes with Sigmoid Activation
  id: totrans-1008
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡å…·æœ‰ Sigmoid æ¿€æ´»çš„èŠ‚ç‚¹è¿›è¡Œåå‘ä¼ æ’­
- en: Letâ€™s backpropagate through a hidden layer node, \(H_4\), from postactivation
    to preactivation. To do this we need the partial derivative our activation function.
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»ä¸€ä¸ªéšè—å±‚èŠ‚ç‚¹ \(H_4\) ä»åæ¿€æ´»åˆ°å‰æ¿€æ´»è¿›è¡Œåå‘ä¼ æ’­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ¿€æ´»å‡½æ•°çš„åå¯¼æ•°ã€‚
- en: we are assuming sigmoid activation for all hidden layer nodes
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‡è®¾æ‰€æœ‰éšè—å±‚èŠ‚ç‚¹ä½¿ç”¨sigmoidæ¿€æ´»
- en: for super clean logic, everyone resolves the activation derivative as a function
    of the output rather than as typical the input,
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†é€»è¾‘ä¸Šçš„æ¸…æ™°ï¼Œæ¯ä¸ªäººéƒ½å°†æ¿€æ´»å¯¼æ•°ä½œä¸ºè¾“å‡ºè€Œä¸æ˜¯å…¸å‹è¾“å…¥çš„å‡½æ•°æ¥è§£æï¼Œ
- en: '![](../Images/10ef488402716c36aa45b497ea54f6bb.png)'
  id: totrans-1012
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10ef488402716c36aa45b497ea54f6bb.png)'
- en: Backpropagation of the loss derivative through the node, $H_4$, from $H_4$ postactivation
    output to $H_{4_{in}}$ preactivation input.
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡èŠ‚ç‚¹ \(H_4\) ä» \(H_4\) åæ¿€æ´»è¾“å‡ºåˆ° \(H_{4_{in}}\) å‰æ¿€æ´»è¾“å…¥çš„åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ã€‚
- en: 'The sigmoid activation at output node \(H_4\) is defined as:'
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŠ‚ç‚¹ \(H_4\) çš„sigmoidæ¿€æ´»å®šä¹‰ä¸ºï¼š
- en: \[ H_4 = \sigma(H_{4_{in}}) = \frac{1}{1 + e^{-H_{4_{in}}}} \]
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_4 = \sigma(H_{4_{in}}) = \frac{1}{1 + e^{-H_{4_{in}}}} \]
- en: The derivative of the sigmoid activation at node \(H_4\) with respect to its
    input \(H_{4_{in}}\), i.e., crossing node \(H_4\) is,
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ \(H_4\) çš„sigmoidæ¿€æ´»ç›¸å¯¹äºå…¶è¾“å…¥ \(H_{4_{in}}\) çš„å¯¼æ•°ï¼Œå³ç©¿è¿‡èŠ‚ç‚¹ \(H_4\) æ˜¯ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + e^{-H_{4_{in}}}} \right) \]
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + e^{-H_{4_{in}}}} \right) \]
- en: Now, for compact notation letâ€™s set,
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†ç®€æ´çš„è¡¨ç¤ºï¼Œæˆ‘ä»¬è®¾ï¼Œ
- en: \[ u = e^{-H_{4_{in}}} \]
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: \[ u = e^{-H_{4_{in}}} \]
- en: and substituting we have,
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä»£å…¥æˆ‘ä»¬å¾—åˆ°ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) \]
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) \]
- en: and by the chain rule we can extend it to,
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶æ‰©å±•ä¸ºï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) = -\frac{u}{(1 + u)^2} \cdot \frac{\partial u}{\partial
    H_{4_{in}}} \]
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) = -\frac{u}{(1 + u)^2} \cdot \frac{\partial u}{\partial
    H_{4_{in}}} \]
- en: 'The derivative of \(u = e^{-H_{4_{in}}}\) with respect to \(H_{4_{in}}\) is:'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: \(u = e^{-H_{4_{in}}}\) å¯¹ \(H_{4_{in}}\) çš„å¯¼æ•°æ˜¯ï¼š
- en: \[ \frac{\partial u}{\partial H_{4_{in}}} = -e^{-H_{4_{in}}} = -u \]
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial u}{\partial H_{4_{in}}} = -e^{-H_{4_{in}}} = -u \]
- en: now we can substitute,
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥ä»£å…¥ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = -\frac{1}{(1+u)^2} \cdot (-u)
    = \frac{u}{(1+u)^2} \]
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = -\frac{1}{(1+u)^2} \cdot (-u)
    = \frac{u}{(1+u)^2} \]
- en: Express in terms of node \(H_4\) output, \(H_4 = \frac{1}{1 + u}\),
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨èŠ‚ç‚¹ \(H_4\) çš„è¾“å‡ºè¡¨ç¤ºï¼Œ\(H_4 = \frac{1}{1 + u}\)ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \frac{\left(1 - H_4\right)/H_4}{\left(1/H_4\right)^2}
    = \frac{1 - H_4}{H_4} \cdot H_4^2 = \left(1 - H_4\right) \cdot H_4 \]
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \frac{\left(1 - H_4\right)/H_4}{\left(1/H_4\right)^2}
    = \frac{1 - H_4}{H_4} \cdot H_4^2 = \left(1 - H_4\right) \cdot H_4 \]
- en: So we can backpropagate through our node, \(H_4\), from node post-activation
    output, \(H_4\) to node pre-activation input, \(H_{4_{in}}\), by,
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æˆ‘ä»¬çš„èŠ‚ç‚¹ \(H_4\) ä»èŠ‚ç‚¹åæ¿€æ´»è¾“å‡º \(H_4\) åˆ°èŠ‚ç‚¹å‰æ¿€æ´»è¾“å…¥ \(H_{4_{in}}\) è¿›è¡Œåå‘ä¼ æ’­ï¼Œ
- en: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \left(1 - H_4\right) \cdot
    H_4 \]
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \left(1 - H_4\right) \cdot
    H_4 \]
- en: Note, we just need \(H_4\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative to the output of node
    \(H_4\) and to the input of node \(H_4\),
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦èŠ‚ç‚¹ \(H_4\) çš„è¾“å‡ºä¿¡å·ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»æŸå¤±å¯¼æ•°åå‘ä¼ æ’­åˆ°èŠ‚ç‚¹ \(H_4\) çš„è¾“å‡ºå’ŒèŠ‚ç‚¹ \(H_4\)
    çš„è¾“å…¥ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} = \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6}
    \cdot 1.0 \cdot (O_6 - y) \]
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} = \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6}
    \cdot 1.0 \cdot (O_6 - y) \]
- en: Now we can handle all cases of backpropagation through the nodes in our network.
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å¤„ç†æˆ‘ä»¬ç½‘ç»œä¸­èŠ‚ç‚¹çš„æ‰€æœ‰åå‘ä¼ æ’­æƒ…å†µã€‚
- en: Backpropagation Along Another Connection
  id: totrans-1035
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ²¿å¦ä¸€ä¸ªè¿æ¥è¿›è¡Œåå‘ä¼ æ’­
- en: For continuity and completeness, letâ€™s repeat the previously described method
    to backpropagate along the connection \(I_1\) to \(H_4\).
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿ç»­æ€§å’Œå®Œæ•´æ€§ï¼Œè®©æˆ‘ä»¬é‡å¤ä¹‹å‰æè¿°çš„æ–¹æ³•ï¼Œé€šè¿‡è¿æ¥ \(I_1\) åˆ° \(H_4\) è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: '![](../Images/76f408b8ed852b03a168e6069bb716db.png)'
  id: totrans-1037
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76f408b8ed852b03a168e6069bb716db.png)'
- en: Backpropagation of the loss derivative along the connection from $H_4$ to $I_1$.
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¿ç€ä» \(H_4\) åˆ° \(I_1\) çš„è¿æ¥è¿›è¡ŒæŸå¤±å¯¼æ•°çš„åå‘ä¼ æ’­ã€‚
- en: Once again, preactivation the input to node \(H_4\) is calculated as,
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼ŒèŠ‚ç‚¹ \(H_4\) çš„å‰æ¿€æ´»è¾“å…¥è®¡ç®—å¦‚ä¸‹ï¼Œ
- en: \[ H_{4_{\text{in}}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_{4_{\text{in}}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]
- en: We calculate the derivative along the connection as,
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—æ²¿è¿æ¥çš„å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \frac{\partial \left(\lambda_{1,4}
    \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 + b_4 \right)}{\partial
    I_1} = \lambda_{1,4} \]
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \frac{\partial \left(\lambda_{1,4}
    \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 + b_4 \right)}{\partial
    I_1} = \lambda_{1,4} \]
- en: by resolving the above partial derivative, we see that backpropagation along
    a connection by applying the connection weight.
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è§£å†³ä¸Šè¿°åå¯¼æ•°ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é€šè¿‡åº”ç”¨è¿æ¥æƒé‡æ²¿è¿æ¥è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \lambda_{4,6} \]
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \lambda_{4,6} \]
- en: Note, we just need the current connection weight \(\lambda_{4,6}\). Now we can
    add this to our chain rule to backpropagate along the \(H_4\) to \(O_6\) connection
    from loss derivative with respect to the output layer node input \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\), to the loss derivative with respect
    to the hidden layer node output \(\frac{\partial \mathcal{L}}{\partial H_4}\).
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦å½“å‰çš„è¿æ¥æƒé‡ \(\lambda_{4,6}\)ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»è¾“å‡ºå±‚èŠ‚ç‚¹è¾“å…¥çš„æŸå¤±å¯¼æ•° \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\)ï¼Œåˆ°éšè—å±‚èŠ‚ç‚¹è¾“å‡ºçš„æŸå¤±å¯¼æ•° \(\frac{\partial \mathcal{L}}{\partial
    H_4}\) æ²¿ \(H_4\) åˆ° \(O_6\) è¿æ¥è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) \]
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) \]
- en: Accounting for Multiple Paths
  id: totrans-1047
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è€ƒè™‘å¤šæ¡è·¯å¾„
- en: Our loss derivative with respect to the node output \(I_1\), \(\frac{\partial
    \mathcal{L}}{\partial I_1}\) is not correct!
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å…³äºèŠ‚ç‚¹è¾“å‡º \(I_1\) çš„æŸå¤±å¯¼æ•° \(\frac{\partial \mathcal{L}}{\partial I_1}\) æ˜¯ä¸æ­£ç¡®çš„ï¼
- en: we accounted for the \(O_6\) to \(H_4\) to \(I_1\) path, but we did not acccount
    for the \(O_6\) to \(H_5\) to \(I_1\) path
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»è€ƒè™‘äº†ä» \(O_6\) åˆ° \(H_4\) åˆ° \(I_1\) çš„è·¯å¾„ï¼Œä½†æ²¡æœ‰è€ƒè™‘ä» \(O_6\) åˆ° \(H_5\) åˆ° \(I_1\)
    çš„è·¯å¾„
- en: '![](../Images/6a68404e4b3d2589dcfe15beb5175c60.png)'
  id: totrans-1050
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a68404e4b3d2589dcfe15beb5175c60.png)'
- en: Multiple paths for backpropagation to input node, $I_1$, from output node $O_6$.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¾“å‡ºèŠ‚ç‚¹ \(O_6\) åˆ°è¾“å…¥èŠ‚ç‚¹ \(I_1\) çš„åå‘ä¼ æ’­æœ‰å¤šæ¡è·¯å¾„ã€‚
- en: To account for multiple paths we just need to sum over all the paths.
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è€ƒè™‘å¤šæ¡è·¯å¾„ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹æ‰€æœ‰è·¯å¾„è¿›è¡Œæ±‚å’Œã€‚
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
- en: we can evaluate this as,
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†å…¶è¯„ä¼°ä¸ºï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) + \lambda_{1,5}
    \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \cdot 1.0 \cdot (O_6
    - y) \]
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) + \lambda_{1,5}
    \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \cdot 1.0 \cdot (O_6
    - y) \]
- en: and then simplify by removing the 1.0 values and grouping terms as,
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åé€šè¿‡ç§»é™¤ 1.0 å€¼å’Œåˆ†ç»„é¡¹è¿›è¡Œç®€åŒ–ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
- en: and now we can evaluate this simplified form as,
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªç®€åŒ–çš„å½¢å¼ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1 - H_5)
    \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1 - H_5)
    \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
- en: Backpropagation through Input Nodes with Identity Activation
  id: totrans-1060
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡å…·æœ‰æ’ç­‰æ¿€æ´»çš„è¾“å…¥èŠ‚ç‚¹è¿›è¡Œåå‘ä¼ æ’­
- en: Letâ€™s backpropagate through our input node, \(I_1\), from postactivation to
    preactivation. To do this we need the partial derivative our activation function.
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡æˆ‘ä»¬çš„è¾“å…¥èŠ‚ç‚¹ \(I_1\) ä»åæ¿€æ´»åˆ°å‰æ¿€æ´»è¿›è¡Œåå‘ä¼ æ’­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ¿€æ´»å‡½æ•°çš„åå¯¼æ•°ã€‚
- en: since this is an input node I have selected the identity or linear activation
    function.
  id: totrans-1062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºè¿™æ˜¯ä¸€ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œæˆ‘é€‰æ‹©äº†æ’ç­‰æˆ–çº¿æ€§æ¿€æ´»å‡½æ•°ã€‚
- en: '![](../Images/bd05208edfb4ac5d5cc0f8cfe808ae50.png)'
  id: totrans-1063
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd05208edfb4ac5d5cc0f8cfe808ae50.png)'
- en: Backpropagation of the loss derivative through the node, $I_1$, from $I_1$ postactivation
    output to $I_{1_{in}}$ preactivation input.
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡èŠ‚ç‚¹ $I_1$ çš„æŸå¤±å¯¼æ•°åå‘ä¼ æ’­ï¼Œä» $I_1$ çš„åæ¿€æ´»è¾“å‡ºåˆ° $I_{1_{in}}$ çš„å‰æ¿€æ´»è¾“å…¥ã€‚
- en: 'The identity activation at output node \(I_1\) is defined as:'
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŠ‚ç‚¹ \(I_1\) çš„æ’ç­‰æ¿€æ´»å®šä¹‰ä¸ºï¼š
- en: \[ I_1 = \sigma(I_{1_{in}}) = I_1 \]
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: \[ I_1 = \sigma(I_{1_{in}}) = I_1 \]
- en: The derivative of the identity activation at node \(I_1\) with respect to its
    input \(I_{1_{in}}\), i.e., passing through node \(I_1\) is,
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ \(I_1\) çš„æ’ç­‰æ¿€æ´»ç›¸å¯¹äºå…¶è¾“å…¥ \(I_{1_{in}}\) çš„å¯¼æ•°ï¼Œå³é€šè¿‡èŠ‚ç‚¹ \(I_1\) çš„ä¼ é€’ï¼Œæ˜¯ï¼Œ
- en: \[ \frac{\partial I_1}{\partial I_{1_{in}}} = \frac{\partial \left(I_{1_{in}}
    \right)}{\partial I_{1_{in}}} = 1.0 \]
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial I_1}{\partial I_{1_{in}}} = \frac{\partial \left(I_{1_{in}}
    \right)}{\partial I_{1_{in}}} = 1.0 \]
- en: Note, we just need \(I_1\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative with respect to the node
    output, \(\frac{\partial \mathcal{L}}{\partial I_1}\), and to the loss derivative
    with respect to the node input, \(\frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}}\),
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€è¦ \(I_1\)ï¼Œå³ä»èŠ‚ç‚¹è¾“å‡ºçš„ä¿¡å·ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæ·»åŠ åˆ°æˆ‘ä»¬çš„é“¾å¼æ³•åˆ™ä¸­ï¼Œä»èŠ‚ç‚¹è¾“å‡ºç›¸å¯¹äºæŸå¤±å¯¼æ•° \(\frac{\partial
    \mathcal{L}}{\partial I_1}\) åå‘ä¼ æ’­ï¼Œåˆ°èŠ‚ç‚¹è¾“å…¥ç›¸å¯¹äºæŸå¤±å¯¼æ•° \(\frac{\partial \mathcal{L}}{\partial
    I_{1_{\text{in}}}}\)ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \frac{\partial I_1}{\partial
    I_{1_{in}}} \cdot \frac{\partial H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} + \frac{\partial I_1}{\partial I_{1_{in}}} \cdot \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial
    O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \frac{\partial I_1}{\partial
    I_{1_{in}}} \cdot \frac{\partial H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} + \frac{\partial I_1}{\partial I_{1_{in}}} \cdot \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial
    O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
- en: we can evaluate this as,
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¿™æ ·è¯„ä¼°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = 1.0 \cdot \lambda_{1,4}
    \cdot \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6
    - y) + 1.0 \cdot \lambda_{1,5} \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6}
    \cdot 1.0 \cdot (O_6 - y) \]
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = 1.0 \cdot \lambda_{1,4}
    \cdot \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6
    - y) + 1.0 \cdot \lambda_{1,5} \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6}
    \cdot 1.0 \cdot (O_6 - y) \]
- en: For fun I designed this notation for maximum clarity,
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¶£å‘³æ€§ï¼Œæˆ‘è®¾è®¡äº†è¿™ç§ç¬¦å·ä»¥å®ç°æœ€å¤§æ¸…æ™°åº¦ï¼Œ
- en: \[ \frac{\partial L}{\partial I_{1_{\text{in}}}} = \overbrace{1.0}^{\textstyle
    \frac{\partial I_{1}}{\partial I_{1_{in}}}} \left[ \overbrace{\lambda_{1,4}}^{\textstyle
    \frac{\partial H_{4_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_4) \cdot
    H_4}^{\textstyle \frac{\partial H_4}{\partial H_{4_{\text{in}}}}} \cdot \overbrace{\lambda_{4,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_4}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} + \overbrace{\lambda_{1,5}}^{\textstyle \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_5) \cdot H_5}^{\textstyle
    \frac{\partial H_5}{\partial H_{5_{\text{in}}}}} \cdot \overbrace{\lambda_{5,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_5}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} \right] \]
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial I_{1_{\text{in}}}} = \overbrace{1.0}^{\textstyle
    \frac{\partial I_{1}}{\partial I_{1_{in}}}} \left[ \overbrace{\lambda_{1,4}}^{\textstyle
    \frac{\partial H_{4_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_4) \cdot
    H_4}^{\textstyle \frac{\partial H_4}{\partial H_{4_{\text{in}}}}} \cdot \overbrace{\lambda_{4,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_4}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} + \overbrace{\lambda_{1,5}}^{\textstyle \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_5) \cdot H_5}^{\textstyle
    \frac{\partial H_5}{\partial H_{5_{\text{in}}}}} \cdot \overbrace{\lambda_{5,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_5}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} \right] \]
- en: But this can be simplified by removing the 1.0 values and grouping terms as,
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¯ä»¥é€šè¿‡ç§»é™¤1.0å€¼å¹¶åˆå¹¶åŒç±»é¡¹æ¥ç®€åŒ–ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
- en: and now we can evaluate this simplified form as,
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªç®€åŒ–çš„å½¢å¼äº†ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \lambda_{1,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \lambda_{1,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
- en: For completeness here is the backpropagation for the other input nodes, hereâ€™s
    \(\frac{\partial \mathcal{L}}{\partial I_{2_{in}}}\),
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œæä¾›äº†å…¶ä»–è¾“å…¥èŠ‚ç‚¹çš„åå‘ä¼ æ’­ï¼Œè¿™é‡Œæ˜¯ \(\frac{\partial \mathcal{L}}{\partial I_{2_{in}}}\)ï¼Œ
- en: '![](../Images/9b186fef88b4e0ad1fa6191394b0e0dd.png)'
  id: totrans-1080
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b186fef88b4e0ad1fa6191394b0e0dd.png)'
- en: Backpropagation of the loss derivative for input node 2.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥èŠ‚ç‚¹2çš„æŸå¤±å¯¼æ•°åå‘ä¼ æ’­ã€‚
- en: For brevity I have remove the 1.0s and grouped like terms,
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘å·²ç»ç§»é™¤äº†1.0å¹¶åˆå¹¶äº†åŒç±»é¡¹ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_2} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_2} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_2} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_2} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
- en: and now we can evaluate this simplified form as,
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªç®€åŒ–çš„å½¢å¼äº†ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \lambda_{2,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{2,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \lambda_{2,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{2,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
- en: and here is \(\frac{\partial \mathcal{L}}{\partial I_{3_{in}}}\),
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ \(\frac{\partial \mathcal{L}}{\partial I_{3_{in}}}\)ï¼Œ
- en: '![](../Images/90c6a46319d78ae1b9d4f8cdc131fd2a.png)'
  id: totrans-1087
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90c6a46319d78ae1b9d4f8cdc131fd2a.png)'
- en: Backpropagation of the loss derivative for input node 3.
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­è¾“å…¥èŠ‚ç‚¹3çš„æŸå¤±å¯¼æ•°ã€‚
- en: For brevity I have remove the 1.0s and grouped like terms,
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘å·²ç§»é™¤1.0å¹¶åˆå¹¶äº†åŒç±»é¡¹ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_3} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_3} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_3} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_3} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
- en: and now we can evaluate this simplified form as,
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°è¿™ä¸ªç®€åŒ–çš„å½¢å¼ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \lambda_{3,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{3,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \lambda_{3,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{3,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
- en: Loss Derivatives with Respect to Weights and Biases
  id: totrans-1093
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…³äºæƒé‡å’Œåå·®çš„æŸå¤±å¯¼æ•°
- en: Now we have back propagated the loss derivative through our network.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»é€šè¿‡æˆ‘ä»¬çš„ç½‘ç»œåå‘ä¼ æ’­äº†æŸå¤±å¯¼æ•°ã€‚
- en: '![](../Images/97de6228871f7910de4d7a8fa86e0bd8.png)'
  id: totrans-1095
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97de6228871f7910de4d7a8fa86e0bd8.png)'
- en: Backpropagated loss derivatives with respect to all network nodes inputs and
    outputs.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­æ‰€æœ‰ç½‘ç»œèŠ‚ç‚¹è¾“å…¥å’Œè¾“å‡ºçš„æŸå¤±å¯¼æ•°ã€‚
- en: and we have the loss derivative with respect to the input and output of each
    node in our network,
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°äº†ç½‘ç»œä¸­æ¯ä¸ªèŠ‚ç‚¹è¾“å…¥å’Œè¾“å‡ºçš„æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial I_1},\quad \frac{\partial \mathcal{L}}{\partial I_{2_{\text{in}}}},\quad
    \frac{\partial \mathcal{L}}{\partial I_2},\quad \frac{\partial \mathcal{L}}{\partial
    I_{3_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial I_3},\quad \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial
    H_4},\quad \frac{\partial \mathcal{L}}{\partial H_5},\quad \frac{\partial \mathcal{L}}{\partial
    H_5},\quad \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial O_6} \]
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial I_1},\quad \frac{\partial \mathcal{L}}{\partial I_{2_{\text{in}}}},\quad
    \frac{\partial \mathcal{L}}{\partial I_2},\quad \frac{\partial \mathcal{L}}{\partial
    I_{3_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial I_3},\quad \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial
    H_4},\quad \frac{\partial \mathcal{L}}{\partial H_5},\quad \frac{\partial \mathcal{L}}{\partial
    H_5},\quad \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial O_6} \]
- en: But what we actually need is the loss derivative with respect to each connection
    weights,
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬å®é™…ä¸Šéœ€è¦çš„æ˜¯æ¯ä¸ªè¿æ¥æƒé‡çš„æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}},\quad \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,4}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{1,5}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{2,5}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,5}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{5,6}} \]
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}},\quad \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,4}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{1,5}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{2,5}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,5}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{5,6}} \]
- en: and node biases,
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠèŠ‚ç‚¹åå·®ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial b_4},\quad \frac{\partial \mathcal{L}}{\partial
    b_5},\quad \frac{\partial \mathcal{L}}{\partial b_6} \]
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial b_4},\quad \frac{\partial \mathcal{L}}{\partial
    b_5},\quad \frac{\partial \mathcal{L}}{\partial b_6} \]
- en: How do we backpropagate the loss derivative to a connection weight? Letâ€™s start
    with the \(H_4\) to \(O_6\) connection.
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•å°†æŸå¤±å¯¼æ•°åå‘ä¼ æ’­åˆ°è¿æ¥æƒé‡ï¼Ÿè®©æˆ‘ä»¬ä»\(H_4\)åˆ°\(O_6\)çš„è¿æ¥å¼€å§‹ã€‚
- en: '![](../Images/bf1e650cd4b0613fa09aa5d5c0e72e26.png)'
  id: totrans-1104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf1e650cd4b0613fa09aa5d5c0e72e26.png)'
- en: Backpropagated loss derivatives with respect to a connection weight.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºè¿æ¥æƒé‡çš„åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ã€‚
- en: Preactivation, input to node \(O_6\) we have,
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æ¿€æ´»ï¼ŒèŠ‚ç‚¹\(O_6\)çš„è¾“å…¥ï¼Œ
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
- en: We calculate the derivative with respect to the connection weight as,
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—å…³äºè¿æ¥æƒé‡çš„å¯¼æ•°å¦‚ä¸‹ï¼Œ
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial \lambda_{4,6}} = \frac{\partial
    \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial
    \lambda_{4,6}} = H_4 \]
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_{6_{\text{in}}}}{\partial \lambda_{4,6}} = \frac{\partial
    \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial
    \lambda_{4,6}} = H_4 \]
- en: We need the output of the node in the previous layer passed along the connection
    to backpropagate to the loss derivative with respect to the connection weight
    from the input to the next node,
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦å°†å‰ä¸€å±‚èŠ‚ç‚¹çš„è¾“å‡ºé€šè¿‡è¿æ¥ä¼ é€’ï¼Œä»¥åå‘ä¼ æ’­åˆ°è¿æ¥æƒé‡ç›¸å¯¹äºä¸‹ä¸€ä¸ªèŠ‚ç‚¹è¾“å…¥çš„æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot 1.0 \cdot (O_6 - y) \]
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot 1.0 \cdot (O_6 - y) \]
- en: Now, for completeness, here are the equations for all of our networkâ€™s connection
    weights.
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œæ˜¯æˆ‘ä»¬ç½‘ç»œæ‰€æœ‰è¿æ¥æƒé‡çš„æ–¹ç¨‹ã€‚
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_5 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_5 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
- en: See the pattern, the loss derivatives with respect to connection weights are,
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹åˆ°æ¨¡å¼äº†ï¼Œå…³äºè¿æ¥æƒé‡çš„æŸå¤±å¯¼æ•°æ˜¯ï¼Œ
- en: \[ \text{Connection Signal} \times \text{Loss Derivative of Next Node Input}
    \]
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{è¿æ¥ä¿¡å·} \times \text{ä¸‹ä¸€ä¸ªèŠ‚ç‚¹è¾“å…¥çš„æŸå¤±å¯¼æ•°} \]
- en: Now how do we backpropagate the loss derivative to a node bias? Letâ€™s start
    with the \(O_6\) node.
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å¦‚ä½•å°†æŸå¤±å¯¼æ•°åå‘ä¼ æ’­åˆ°èŠ‚ç‚¹åç½®ï¼Ÿè®©æˆ‘ä»¬ä» \(O_6\) èŠ‚ç‚¹å¼€å§‹ã€‚
- en: '![](../Images/646997a228f8c1e8a3b2743ac13e388a.png)'
  id: totrans-1117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/646997a228f8c1e8a3b2743ac13e388a.png)'
- en: Backpropagated loss derivatives with respect to a node bias.
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºèŠ‚ç‚¹åç½®çš„åå‘ä¼ æ’­æŸå¤±å¯¼æ•°ã€‚
- en: Once again, the preactivation, input to node \(O_6\) is,
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡å¼ºè°ƒï¼Œé¢„æ¿€æ´»ï¼ŒèŠ‚ç‚¹ \(O_6\) çš„è¾“å…¥æ˜¯ï¼Œ
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
- en: We calculate the derivative of a connection weight as,
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—è¿æ¥æƒé‡çš„å¯¼æ•°å¦‚ä¸‹ï¼Œ
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial b_6} = \frac{\partial \left( \lambda_{4,6}
    \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial b_6} = 1.0 \]
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial O_{6_{\text{in}}}}{\partial b_6} = \frac{\partial \left( \lambda_{4,6}
    \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial b_6} = 1.0 \]
- en: so our bias loss derivative is equal to the node input loss derivative,
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬çš„åç½®æŸå¤±å¯¼æ•°ç­‰äºèŠ‚ç‚¹è¾“å…¥æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial O_{6_{\text{in}}}}{\partial
    b_6} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = 1.0 \cdot
    \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial O_{6_{\text{in}}}}{\partial
    b_6} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = 1.0 \cdot
    \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
- en: For completeness here are all the loss derivatives with respect to node biases,
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œåˆ—å‡ºäº†æ‰€æœ‰å…³äºèŠ‚ç‚¹åç½®çš„æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial \mathcal{L}}{\partial
    O_{6_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial b_4} = \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial
    b_5} = \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial \mathcal{L}}{\partial
    O_{6_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial b_4} = \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial
    b_5} = \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]
- en: See the pattern, the loss derivatives with respect to node biases are,
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹çœ‹è¿™ä¸ªæ¨¡å¼ï¼Œå…³äºèŠ‚ç‚¹åç½®çš„æŸå¤±å¯¼æ•°æ˜¯ï¼Œ
- en: \[ \text{Loss Derivative of the Node Input} \]
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{èŠ‚ç‚¹è¾“å…¥çš„æŸå¤±å¯¼æ•°} \]
- en: Backpropagation Example
  id: totrans-1129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­ç¤ºä¾‹
- en: Letâ€™s take the backpropagation method explained above and apply them to my interactive
    neural network.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°†ä¸Šé¢è§£é‡Šçš„åå‘ä¼ æ’­æ–¹æ³•åº”ç”¨åˆ°æˆ‘çš„äº¤äº’å¼ç¥ç»ç½‘ç»œä¸­ã€‚
- en: Hereâ€™s the result for our first training epoch with only 1 sample,
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯åªæœ‰1ä¸ªæ ·æœ¬çš„ç¬¬ä¸€è½®è®­ç»ƒçš„ç»“æœï¼Œ
- en: '![](../Images/b8b7d56b334d9728490cb2bc1408535e.png)'
  id: totrans-1132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8b7d56b334d9728490cb2bc1408535e.png)'
- en: Backpropagation result for the first iteration.
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€è½®è¿­ä»£çš„åå‘ä¼ æ’­ç»“æœã€‚
- en: My interactive dashboard provides all the loss derivatives with respect to the
    input for each node and the output signals from each node, so for example we can
    calculate \(\frac{\partial L}{\partial \lambda_{4,6}}\) as,
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„äº¤äº’å¼ä»ªè¡¨æ¿æä¾›äº†æ¯ä¸ªèŠ‚ç‚¹ç›¸å¯¹äºè¾“å…¥çš„æ‰€æœ‰æŸå¤±å¯¼æ•°ä»¥åŠæ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºä¿¡å·ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®— \(\frac{\partial L}{\partial
    \lambda_{4,6}}\) å¦‚ä¸‹ï¼Œ
- en: \[ \frac{\partial L}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial L}{\partial O_{6_{\text{in}}}} = H_4 \cdot
    \frac{\partial L}{\partial O_{6_{\text{in}}}} = 0.42 \cdot 1.00 = 0.42 \]
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial L}{\partial O_{6_{\text{in}}}} = H_4 \cdot
    \frac{\partial L}{\partial O_{6_{\text{in}}}} = 0.42 \cdot 1.00 = 0.42 \]
- en: Hereâ€™s the loss derivatives with respect to connection weights for the other
    hidden layer to output node connection,
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯å…³äºå…¶ä»–éšè—å±‚åˆ°è¾“å‡ºèŠ‚ç‚¹è¿æ¥çš„æŸå¤±å¯¼æ•°ï¼Œ
- en: \[ \frac{\partial L}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial L}{\partial O_{6_{\text{in}}}} = H_5 \cdot
    \frac{\partial L}{\partial O_{6_{\text{in}}}} = 0.60 \cdot 1.00 = 0.60 \]
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial L}{\partial O_{6_{\text{in}}}} = H_5 \cdot
    \frac{\partial L}{\partial O_{6_{\text{in}}}} = 0.60 \cdot 1.00 = 0.60 \]
- en: and now letâ€™s get all the input to hidden layer connections,
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è·å–æ‰€æœ‰è¾“å…¥åˆ°éšè—å±‚è¿æ¥ï¼Œ
- en: \[ \frac{\partial L}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_1 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.50 \cdot (-0.13) = -0.07 \]\[
    \frac{\partial L}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_1 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.50 \cdot (-0.10) = -0.05 \]\[
    \frac{\partial L}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_2 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.20 \cdot (-0.13) = -0.03 \]\[
    \frac{\partial L}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = I_2 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.20 \cdot (-0.10) = -0.02 \]\[
    \frac{\partial L}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_3 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.70 \cdot (-0.13) = -0.09 \]\[
    \frac{\partial L}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = I_3 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.70 \cdot (-0.10) = -0.07 \]
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_1 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.50 \cdot (-0.13) = -0.07 \]\[
    \frac{\partial L}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_1 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.50 \cdot (-0.10) = -0.05 \]\[
    \frac{\partial L}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_2 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.20 \cdot (-0.13) = -0.03 \]\[
    \frac{\partial L}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = I_2 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.20 \cdot (-0.10) = -0.02 \]\[
    \frac{\partial L}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_3 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.70 \cdot (-0.13) = -0.09 \]\[
    \frac{\partial L}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = I_3 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.70 \cdot (-0.10) = -0.07 \]
- en: This takes care of all of the connection weight error derivatives, now lets
    take care of the node bias error derivatives.
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¤„ç†äº†æ‰€æœ‰è¿æ¥æƒé‡è¯¯å·®çš„å¯¼æ•°ï¼Œç°åœ¨è®©æˆ‘ä»¬å¤„ç†èŠ‚ç‚¹åç½®è¯¯å·®çš„å¯¼æ•°ã€‚
- en: the node bias error derivatives are the same as the node peractivation error
    derivatives. Now letâ€™s calculate the bias terms in the hidden layer,
  id: totrans-1141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹åç½®è¯¯å·®çš„å¯¼æ•°ä¸èŠ‚ç‚¹æ¯ä¸ªæ¿€æ´»è¯¯å·®çš„å¯¼æ•°ç›¸åŒã€‚ç°åœ¨è®©æˆ‘ä»¬è®¡ç®—éšè—å±‚ä¸­çš„åç½®é¡¹ï¼Œ
- en: \[ \frac{\partial L}{\partial b_4} = \frac{\partial H_{4_{\text{in}}}}{\partial
    b_4} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = 1.0 \cdot (-0.13) =
    -0.13 \]\[ \frac{\partial L}{\partial b_5} = \frac{\partial H_{5_{\text{in}}}}{\partial
    b_5} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = 1.0 \cdot (-0.1) =
    -0.10 \]
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial b_4} = \frac{\partial H_{4_{\text{in}}}}{\partial
    b_4} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = 1.0 \cdot (-0.13) =
    -0.13 \]\[ \frac{\partial L}{\partial b_5} = \frac{\partial H_{5_{\text{in}}}}{\partial
    b_5} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = 1.0 \cdot (-0.1) =
    -0.10 \]
- en: Updating Model Parameters
  id: totrans-1143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ›´æ–°æ¨¡å‹å‚æ•°
- en: The loss derivatives with respect to each of the model parameters are the gradients,
    so we are ready to use gradient descent optimization with the addition of,
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°ç›¸å¯¹äºæ¯ä¸ªå‚æ•°çš„æŸå¤±å¯¼æ•°æ˜¯æ¢¯åº¦ï¼Œå› æ­¤æˆ‘ä»¬å‡†å¤‡ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼Œå¹¶æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼Œ
- en: '**learning rate** - to scale the rate of change of the model updates we assign
    a learning rate, \(\eta\). For our model parameter examples from above,'
  id: totrans-1145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡** - ä¸ºäº†ç¼©æ”¾æ¨¡å‹æ›´æ–°çš„å˜åŒ–ç‡ï¼Œæˆ‘ä»¬åˆ†é…ä¸€ä¸ªå­¦ä¹ ç‡ï¼Œ\(\eta\)ã€‚å¯¹äºä¸Šé¢æåˆ°çš„æ¨¡å‹å‚æ•°ç¤ºä¾‹ï¼Œ'
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{4,6}} \]\[ \lambda_{1,4}^{\ell} = \lambda_{1,4}^{\ell - 1}
    + \eta \cdot \frac{\partial L}{\partial \lambda_{1,4}} \]\[ b_j^{\ell} = b_j^{\ell
    - 1} + \eta \cdot \frac{\partial L}{\partial b_j} \]
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{4,6}} \]\[ \lambda_{1,4}^{\ell} = \lambda_{1,4}^{\ell - 1}
    + \eta \cdot \frac{\partial L}{\partial \lambda_{1,4}} \]\[ b_j^{\ell} = b_j^{\ell
    - 1} + \eta \cdot \frac{\partial L}{\partial b_j} \]
- en: recall, this process of gradient calculation and model parameters, weights and
    biases, updating is iterated and is known as gradient descent optimization.
  id: totrans-1147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›æƒ³ä¸€ä¸‹ï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯æ¢¯åº¦è®¡ç®—å’Œæ¨¡å‹å‚æ•°ã€æƒé‡å’Œåç½®çš„æ›´æ–°ï¼Œæ˜¯è¿­ä»£è¿›è¡Œçš„ï¼Œè¢«ç§°ä¸ºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚
- en: the goal is to explore the loss hypersurface, avoiding and escaping local minimums
    and ultimately finding the global minimum.
  id: totrans-1148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯æ¢ç´¢æŸå¤±è¶…æ›²é¢ï¼Œé¿å…å’Œé€ƒç¦»å±€éƒ¨æœ€å°å€¼ï¼Œæœ€ç»ˆæ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚
- en: learning rate, also known as step size is commonly set between 0.0 and 1.0,
    note 0.01 is the default in Keras module of TensorFlow
  id: totrans-1149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡ï¼Œä¹Ÿç§°ä¸ºæ­¥é•¿ï¼Œé€šå¸¸è®¾ç½®åœ¨ 0.0 å’Œ 1.0 ä¹‹é—´ï¼Œæ³¨æ„åœ¨ TensorFlow çš„ Keras æ¨¡å—ä¸­é»˜è®¤ä¸º 0.01
- en: '**Low Learning Rate** â€“ more stable, but a slower solution, may get stuck in
    a local minimum'
  id: totrans-1150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½å­¦ä¹ ç‡** â€“ æ›´ç¨³å®šï¼Œä½†è§£çš„é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€å°å€¼'
- en: '**High Learning Rate** â€“ may be unstable, but perhaps a faster solution, may
    diverge out of the global minimum'
  id: totrans-1151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é«˜å­¦ä¹ ç‡** â€“ å¯èƒ½ä¸ç¨³å®šï¼Œä½†å¯èƒ½æ˜¯ä¸€ä¸ªæ›´å¿«çš„è§£ï¼Œå¯èƒ½ä¼šå‘æ•£å‡ºå…¨å±€æœ€å°å€¼'
- en: One strategy is to start with a high learning rate and then to decrease the
    learning rate over the iterations
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§ç­–ç•¥æ˜¯å¼€å§‹ä½¿ç”¨é«˜å­¦ä¹ ç‡ï¼Œç„¶ååœ¨è¿­ä»£è¿‡ç¨‹ä¸­é™ä½å­¦ä¹ ç‡
- en: '**Learning Rate Decay** - set as > 0 to avoid mitigate oscillations,'
  id: totrans-1153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡è¡°å‡** - è®¾ç½®ä¸º > 0 ä»¥é¿å…å‡è½»æŒ¯è¡ï¼Œ'
- en: \[ \eta^{\ell} = \eta^{\ell - 1} \cdot \left( \frac{1}{1 + \text{decay} \cdot
    \ell} \right) \]
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \eta^{\ell} = \eta^{\ell - 1} \cdot \left( \frac{1}{1 + \text{decay} \cdot
    \ell} \right) \]
- en: where \(\ell\) is the model training epoch
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ \(\ell\) æ˜¯æ¨¡å‹è®­ç»ƒå‘¨æœŸ
- en: Notice that the model parameter updates are for a single training data case?
    Consider this single model parameter,
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæ¨¡å‹å‚æ•°æ›´æ–°æ˜¯é’ˆå¯¹å•ä¸ªè®­ç»ƒæ•°æ®æ¡ˆä¾‹çš„ï¼Ÿè€ƒè™‘è¿™ä¸ªå•ä¸ªæ¨¡å‹å‚æ•°ï¼Œ
- en: we calculate the update over all samples in the batch and apply the average
    of the updates.
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—æ‰¹ä¸­æ‰€æœ‰æ ·æœ¬çš„æ›´æ–°ï¼Œå¹¶åº”ç”¨æ›´æ–°çš„å¹³å‡å€¼ã€‚
- en: \[ \frac{\partial L}{\partial \lambda_{4,6}} = H_4 \cdot \frac{\partial L}{\partial
    O_{6_{\text{in}}}} = 0.42 \cdot 1.00 = 0.42 \]
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial L}{\partial \lambda_{4,6}} = H_4 \cdot \frac{\partial L}{\partial
    O_{6_{\text{in}}}} = 0.42 \cdot 1.00 = 0.42 \]
- en: is applied to update the \(\lambda_{4,6}\) parameter as,
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨ä»¥æ›´æ–° \(\lambda_{4,6}\) å‚æ•°ï¼Œ
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{4,6}} \]
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{4,6}} \]
- en: is dependent on \(H_4\) node output, and \(L\) error that are for a single sample,
    \(ğ‘¥_1,\ldots,ğ‘¥_ğ‘š\) and \(ğ‘¦\); therefore, we cannot calculate a single parameter
    update over all our \(1,\ldots,n\) training data samples.
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¾èµ–äº \(H_4\) èŠ‚ç‚¹è¾“å‡ºå’Œ \(L\) é”™è¯¯ï¼Œè¿™äº›æ˜¯é’ˆå¯¹å•ä¸ªæ ·æœ¬ \(ğ‘¥_1,\ldots,ğ‘¥_ğ‘š\) å’Œ \(ğ‘¦\) çš„ï¼›å› æ­¤ï¼Œæˆ‘ä»¬ä¸èƒ½åœ¨æ‰€æœ‰
    \(1,\ldots,n\) ä¸ªè®­ç»ƒæ•°æ®æ ·æœ¬ä¸Šè®¡ç®—å•ä¸ªå‚æ•°æ›´æ–°ã€‚
- en: instead we can calculate \(1,\ldots,n\) updates and then apply the average of
    all the updates to our model parameters,
  id: totrans-1162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è®¡ç®— \(1,\ldots,n\) æ¬¡æ›´æ–°ï¼Œç„¶åå°†æ‰€æœ‰æ›´æ–°çš„å¹³å‡å€¼åº”ç”¨äºæˆ‘ä»¬çš„æ¨¡å‹å‚æ•°ï¼Œ
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \frac{1}{n_{batch}} \sum_{i=1}^{n_{batch}}
    \eta \cdot \frac{\partial L}{\partial \lambda_{4,6}} \]
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \frac{1}{n_{batch}} \sum_{i=1}^{n_{batch}}
    \eta \cdot \frac{\partial L}{\partial \lambda_{4,6}} \]
- en: since the learning rate is a constant, we can move it out of the sum and now
    we are averaging the gradients,
  id: totrans-1164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºå­¦ä¹ ç‡æ˜¯å¸¸æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ç§»å‡ºæ±‚å’Œç¬¦å·ï¼Œç°åœ¨æˆ‘ä»¬æ­£åœ¨å¹³å‡æ¢¯åº¦ï¼Œ
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \frac{1}{n_{batch}}
    \sum_{i=1}^{n_{batch}} \frac{\partial L}{\partial \lambda_{4,6}} \]
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \frac{1}{n_{batch}}
    \sum_{i=1}^{n_{batch}} \frac{\partial L}{\partial \lambda_{4,6}} \]
- en: Training Epochs
  id: totrans-1166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒå‘¨æœŸ
- en: This is a good time to talk about stochastic gradient descent optimization,
    first letâ€™s define some common terms,
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯è®¨è®ºéšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–çš„å¥½æ—¶æœºï¼Œé¦–å…ˆè®©æˆ‘ä»¬å®šä¹‰ä¸€äº›å¸¸è§æœ¯è¯­ï¼Œ
- en: '**Batch Gradient Descent** - updates the model parameters after passing through
    all of the data'
  id: totrans-1168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰¹é‡æ¢¯åº¦ä¸‹é™** - åœ¨é€šè¿‡æ‰€æœ‰æ•°æ®åæ›´æ–°æ¨¡å‹å‚æ•°'
- en: '**Stochastic Gradient Descent** - updates the model parameters over each sample
    data'
  id: totrans-1169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éšæœºæ¢¯åº¦ä¸‹é™** - åœ¨æ¯ä¸ªæ ·æœ¬æ•°æ®ä¸Šæ›´æ–°æ¨¡å‹å‚æ•°'
- en: '**Mini-batch Gradient Descent** - updates the model parameter after passing
    through a single batch'
  id: totrans-1170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°æ‰¹é‡æ¢¯åº¦ä¸‹é™** - åœ¨é€šè¿‡å•ä¸ªæ‰¹åæ›´æ–°æ¨¡å‹å‚æ•°'
- en: With mini-batch gradient descent stochasticity is introduced through the use
    of subsets of the data, known as batches,
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å°æ‰¹é‡æ¢¯åº¦ä¸‹é™æ—¶ï¼Œé€šè¿‡ä½¿ç”¨æ•°æ®å­é›†ï¼ˆç§°ä¸ºæ‰¹æ¬¡ï¼‰å¼•å…¥éšæœºæ€§ï¼Œ
- en: for example, if we divide our 100 samples into 4 batches, then we iterate over
    each batch separately
  id: totrans-1172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æŠŠæˆ‘ä»¬çš„ 100 ä¸ªæ ·æœ¬åˆ†æˆ 4 ä¸ªæ‰¹æ¬¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬åˆ†åˆ«è¿­ä»£æ¯ä¸ªæ‰¹æ¬¡
- en: we speed up the individual updates, fewer data are faster to calculate, but
    we introduce more error
  id: totrans-1173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åŠ å¿«äº†å•ä¸ªæ›´æ–°çš„é€Ÿåº¦ï¼Œæ›´å°‘çš„æ•°æ®æ›´å¿«åœ°è®¡ç®—ï¼Œä½†å¼•å…¥äº†æ›´å¤šçš„é”™è¯¯
- en: this often helps the training explore for the global minimum and avoid getting
    stuck in local minimums and along ridges in the loss hypersurface
  id: totrans-1174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™é€šå¸¸æœ‰åŠ©äºè®­ç»ƒæ¢ç´¢å…¨å±€æœ€å°å€¼ï¼Œå¹¶é¿å…é™·å…¥æŸå¤±è¶…æ›²é¢ä¸Šçš„å±€éƒ¨æœ€å°å€¼å’Œè„Šä¸­
- en: Finally our last definition here,
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå®šä¹‰æœ€åä¸€ä¸ªæœ¯è¯­ï¼Œ
- en: '**epoch** - is one pass over all of the data, so that would be 4 iterations
    of updating the model parameters if we have 4 mini-batches'
  id: totrans-1176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‘¨æœŸ** - æ˜¯å¯¹å…¨éƒ¨æ•°æ®çš„å•æ¬¡éå†ï¼Œå› æ­¤å¦‚æœæœ‰ 4 ä¸ªå°æ‰¹é‡ï¼Œé‚£ä¹ˆå°†è¿›è¡Œ 4 æ¬¡æ›´æ–°æ¨¡å‹å‚æ•°çš„è¿­ä»£'
- en: There are many other considerations that I will add later including,
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜ä¼šæ·»åŠ è®¸å¤šå…¶ä»–è€ƒè™‘å› ç´ ï¼ŒåŒ…æ‹¬ï¼Œ
- en: momentum
  id: totrans-1178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ¨é‡
- en: adaptive optimization
  id: totrans-1179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”ä¼˜åŒ–
- en: Now letâ€™s build the above artificial neural network by-hand and visualize the
    solution!
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ‰‹åŠ¨æ„å»ºä¸Šè¿°äººå·¥ç¥ç»ç½‘ç»œå¹¶å¯è§†åŒ–è§£å†³æ–¹æ¡ˆï¼
- en: this is by-hand so that you can see every calculation. I intentionally avoided
    using TensorFlow or PyTorch.
  id: totrans-1181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ‰‹åŠ¨ç¼–å†™çš„ï¼Œè¿™æ ·ä½ å¯ä»¥çœ‹åˆ°æ¯ä¸ªè®¡ç®—ã€‚æˆ‘æ•…æ„é¿å…ä½¿ç”¨TensorFlowæˆ–PyTorchã€‚
- en: Interactive Dashboard
  id: totrans-1182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº¤äº’å¼ä»ªè¡¨æ¿
- en: I built out an interactive Python dashboard with the code below for training
    an artificial neural network. You can step through the training iteration and
    observe over the training epochs,
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨ä»¥ä¸‹ä»£ç æ„å»ºäº†ä¸€ä¸ªäº¤äº’å¼Pythonä»ªè¡¨æ¿ï¼Œç”¨äºè®­ç»ƒäººå·¥ç¥ç»ç½‘ç»œã€‚ä½ å¯ä»¥é€æ­¥é€šè¿‡è®­ç»ƒè¿­ä»£ï¼Œå¹¶åœ¨è®­ç»ƒæ—¶é—´æ­¥ä¸­è§‚å¯Ÿï¼Œ
- en: model parameters
  id: totrans-1184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°
- en: forward pass predictions
  id: totrans-1185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‰å‘ä¼ é€’é¢„æµ‹
- en: backpropagation of error derivatives
  id: totrans-1186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é”™è¯¯å¯¼æ•°çš„åå‘ä¼ æ’­
- en: If you would like to see artificial neural networks in action, check out my
    [ANN interactive Python dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_ANN.ipynb),
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³çœ‹åˆ°äººå·¥ç¥ç»ç½‘ç»œçš„å®é™…åº”ç”¨ï¼Œè¯·æŸ¥çœ‹æˆ‘çš„[ANNäº¤äº’å¼Pythonä»ªè¡¨æ¿](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_ANN.ipynb)ï¼Œ
- en: '![](../Images/ab7e3b58fcfdbf8f7a2c19b5d78c7736.png)'
  id: totrans-1188
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/ab7e3b58fcfdbf8f7a2c19b5d78c7736.png)'
- en: Interactive artificial neural network training Python dashboard.
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤äº’å¼äººå·¥ç¥ç»ç½‘ç»œè®­ç»ƒPythonä»ªè¡¨æ¿ã€‚
- en: Import Required Packages
  id: totrans-1190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¼å…¥æ‰€éœ€åŒ…
- en: We will also need some standard packages. These should have been installed with
    Anaconda 3.
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸Anaconda 3ä¸€èµ·å®‰è£…ã€‚
- en: '[PRE5]'
  id: totrans-1192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing â€˜python -m pip install [package-name]â€™. More assistance is available
    with the respective package docs.
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½é¦–å…ˆéœ€è¦å®‰è£…å…¶ä¸­çš„ä¸€äº›åŒ…ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨Windowsä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥â€˜python -m pip install
    [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£ä¸­æä¾›äº†æ›´å¤šå¸®åŠ©ã€‚
- en: Declare Functions
  id: totrans-1194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å£°æ˜å‡½æ•°
- en: Hereâ€™s the functions to make, train and visualize our artificial neural network.
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯åˆ›å»ºã€è®­ç»ƒå’Œå¯è§†åŒ–æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œçš„å‡½æ•°ã€‚
- en: '[PRE6]'
  id: totrans-1196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The Simple ANN
  id: totrans-1197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç®€å•ANN
- en: 'I wrote this code to specify a simple ANN:'
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç¼–å†™äº†ä»¥ä¸‹ä»£ç æ¥æŒ‡å®šä¸€ä¸ªç®€å•çš„ANNï¼š
- en: three input nodes, 2 hidden nodes and 1 output node
  id: totrans-1199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‰ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œ2ä¸ªéšè—èŠ‚ç‚¹å’Œ1ä¸ªè¾“å‡ºèŠ‚ç‚¹
- en: 'and to train the ANN by iteratively performing the forward calculation and
    backpropagation. I calculate:'
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”é€šè¿‡è¿­ä»£æ‰§è¡Œå‰å‘è®¡ç®—å’Œåå‘ä¼ æ’­æ¥è®­ç»ƒANNã€‚æˆ‘è®¡ç®—ï¼š
- en: the error and then propagate it to each node
  id: totrans-1201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é”™è¯¯ä»¥åŠå°†å…¶ä¼ æ’­åˆ°æ¯ä¸ªèŠ‚ç‚¹
- en: solve for the partial derivatives of the error with respect to each weight and
    bias
  id: totrans-1202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ±‚è§£è¯¯å·®ç›¸å¯¹äºæ¯ä¸ªæƒé‡å’Œåå·®çš„åå¯¼æ•°
- en: all weights, biases and partial derivatives for all epoch are recorded in vectors
    for plotting
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æƒé‡ã€åå·®å’Œæ‰€æœ‰æ—¶é—´æ­¥çš„åå¯¼æ•°éƒ½è®°å½•åœ¨å‘é‡ä¸­ä»¥ä¾¿ç»˜å›¾
- en: '[PRE7]'
  id: totrans-1204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now Visualize the Network for a Specific Epoch
  id: totrans-1205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç°åœ¨å¯è§†åŒ–ç‰¹å®šæ—¶é—´æ­¥çš„ç½‘ç»œ
- en: I wrote a custom network visualization below, select iepoch and visualize the
    artificial neural network for a specific epoch.
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ä¸‹é¢ç¼–å†™äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„ç½‘ç»œå¯è§†åŒ–ï¼Œé€‰æ‹©iepochå¹¶å¯è§†åŒ–ç‰¹å®šæ—¶é—´æ­¥çš„äººå·¥ç¥ç»ç½‘ç»œã€‚
- en: '[PRE8]'
  id: totrans-1207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![_images/f1c2dc9fc05ec8a3d07b5642c59c42dd91a2b2ed35c547faa409371a1e776270.png](../Images/6c1b34f06ebf3ebfafbbd206fc1033ca.png)'
  id: totrans-1208
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/6c1b34f06ebf3ebfafbbd206fc1033ca.png)'
- en: Check the ANN Convergence
  id: totrans-1209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ANNæ”¶æ•›æ€§
- en: Now we plot the weights, biases and prediction over the epochs to check the
    training convergence.
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç»˜åˆ¶æƒé‡ã€åå·®å’Œé¢„æµ‹éšæ—¶é—´çš„å˜åŒ–ï¼Œä»¥æ£€æŸ¥è®­ç»ƒæ”¶æ•›æ€§ã€‚
- en: '[PRE9]'
  id: totrans-1211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![_images/96a2f9a7007123700054af340b223a0c9017cbf262afe43f97c8f1b445ec44a6.png](../Images/41e5da630bdb6e70068ef584a2fc3d47.png)'
  id: totrans-1212
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/41e5da630bdb6e70068ef584a2fc3d47.png)'
- en: Comments
  id: totrans-1213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„è®º
- en: This was a basic treatment of artificial neural networks. Much more could be
    done and discussed, I have many more resources. Check out my [shared resource
    inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture links
    at the start of this chapter with resource links in the videosâ€™ descriptions.
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¯¹äººå·¥ç¥ç»ç½‘ç»œçš„åŸºæœ¬ä»‹ç»ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„è§†é¢‘è®²åº§YouTubeé“¾æ¥ã€‚
- en: I hope this is helpful,
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ
- en: '*Michael*'
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”*'
- en: About the Author
  id: totrans-1217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºä½œè€…
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  id: totrans-1218
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡40è‹±äº©æ ¡å›­å†…ï¼Œè¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆçš„åŠå…¬å®¤ã€‚
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·çš®å°”å¥‡æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[Cockrellå·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œåœ¨é‚£é‡Œä»–ä»äº‹å’Œæ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜ï¼Œ
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  id: totrans-1221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ˜¯[èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œå¹¶åœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤æ‹…ä»»æ ¸å¿ƒæ•™å‘˜
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  id: totrans-1222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿˜æ˜¯[è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”å·²ç»æ’°å†™äº†70å¤šç¯‡[åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[PythonåŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[åœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[Pythonåº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPyåŠ¨æ‰‹æŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[Pythonåº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„åŠ¨æ‰‹æŒ‡å—](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚
- en: All of Michaelâ€™s university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michaelâ€™s work and shared educational resources visit his
    Website.
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTubeé¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œé™„æœ‰100å¤šä¸ªPythonäº¤äº’å¼ä»ªè¡¨æ¿å’Œ40å¤šä¸ªGitHubä»“åº“ä¸­çš„è¯¦ç»†æ–‡æ¡£å·¥ä½œæµç¨‹é“¾æ¥ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ã€‚æƒ³äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚
- en: Want to Work Together?
  id: totrans-1225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚åŠ ã€‚
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? Iâ€™d be happy to drop by and work with you!
  id: totrans-1227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  id: totrans-1228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒ³è¦åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  id: totrans-1229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡[mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu)è”ç³»æˆ‘ã€‚
- en: Iâ€™m always happy to discuss,
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ
- en: '*Michael*'
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”*'
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·çš®å°”å¥‡å…¹ï¼Œåšå£«ï¼Œæ³¨å†Œå·¥ç¨‹å¸ˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢æ•™æˆ
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥è·å–ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Pythonä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Pythonä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
