- en: Artificial Neural Networks Test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ANN.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ANN.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with
    Code‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite this e-Book as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with
    Code* [e-book]. Zenodo. doi:10.5281/zenodo.15169138 [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflows in this book and more are available here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite the MachineLearningDemos GitHub Repository as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration
    Workflows Repository* (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312\.
    GitHub repository: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
  prefs: []
  type: TYPE_NORMAL
- en: By Michael J. Pyrcz
  prefs: []
  type: TYPE_NORMAL
- en: ¬© Copyright 2024.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is a tutorial for / demonstration of **Artificial Neural Networks**.
  prefs: []
  type: TYPE_NORMAL
- en: '**YouTube Lecture**: check out my lectures on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Artificial Neural Networks](https://youtu.be/A9PiCMY_6nM?si=NxWSU_5RgQ4w55EL)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Convolutional Neural Networks](https://youtu.be/za2my_XDoOs?si=LeHU6p2_fc9dX4Yt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These lectures are all part of my [Machine Learning Course](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)
    on YouTube with linked well-documented Python workflows and interactive dashboards.
    My goal is to share accessible, actionable, and repeatable educational content.
    If you want to know about my motivation, check out [Michael‚Äôs Story](https://michaelpyrcz.com/my-story).
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Artificial neural networks are very powerful, nature inspired computing based
    on an analogy of brain
  prefs: []
  type: TYPE_NORMAL
- en: I suggest that they are like a reptilian brain, without planning and higher
    order reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, artificial neural networks are a building block of many other deep
    learning methods, for example,
  prefs: []
  type: TYPE_NORMAL
- en: convolutional neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: recurrent neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: generative adversarial networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nature inspired computing is looking to nature for inspiration to develop novel
    problem-solving methods,
  prefs: []
  type: TYPE_NORMAL
- en: '**artificial neural networks** are inspired by biological neural networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nodes** - in our model are artificial neurons, simple processors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**connections** between nodes are artificial synapses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: intelligence emerges from many connected simple processors. For the remainder
    of this chapter, I will used the terms nodes and connections to describe our artificial
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network Concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here are some key aspects of artificial neural networks,
  prefs: []
  type: TYPE_NORMAL
- en: '**Basic Design** - *‚Äú‚Ä¶a computing system made up of a number of simple, highly
    interconnected processing elements, which process information by their dynamic
    state response to external inputs.‚Äù* Caudill (1989).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Still a Prediction Model** - while these models may be quite complicated
    with even millions of trainable model parameters, weights and biases, they are
    still a function that maps from predictor features to response features,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ Y=f(X)+\epsilon \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning** ‚Äì we provide training data with predictor features,
    \(X_1,\ldots,ùëã_ùëö\) and response feature(s), \(ùëå_1,\ldots,ùëå_K\), with the expectation
    of some model prediction error, \(\epsilon\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nonlinearity** - nonlinearity is imparted to the system through the application
    of nonlinear activation functions to model nonlinear relationships'
  prefs: []
  type: TYPE_NORMAL
- en: '**Universal Function Approximator (Universal Approximation Theorem)** - ANNs
    have the ability to learn any possible function shape of \(f\) over an interval,
    for an arbitrary wide (single hidden layer) by Cybenko (1989) and arbitrary depth
    by Lu and others (2017)'
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get started, let‚Äôs build a neural net, single hidden layer, fully connected,
    feed-forward neural network,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/832ee38f2f09d395115eb31b95358103.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple demonstration artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: We use this example artificial neural network in the descriptions below and
    as an actual example that we will train and predict with by-hand!
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs label the parts of our network,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0be0dcf09e698cf22dc0881c818c0dbc.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple demonstration artificial neural network with the parts labeled, including
    3 inputs nodes, 2 hidden nodes and 1 output node fully connected.
  prefs: []
  type: TYPE_NORMAL
- en: Our artificial neural network has,
  prefs: []
  type: TYPE_NORMAL
- en: 3 predictor features, \(X_1\), \(X_2\) and \(X_3\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3 input nodes, \(I_1\), \(I_2\) and \(I_3\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2 hidden layer nodes, \(H_4\) and \(H_5\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 output node, \(O_6\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 response feature, \(Y_1\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: where all nodes fully connected. Note, deep learning is a neural network with
    more than 1 hidden layer, but for brevity let‚Äôs continue with our non-deep learning
    artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Comments on Network Nomenclature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just a couple more comments about my network nomenclature. My goal is to maximize
    simplicity and clarity,
  prefs: []
  type: TYPE_NORMAL
- en: '**Network Nodes and Connections** - I choose to use unique numbers for all
    nodes, \(I_1\), \(I_2\), \(I_3\), \(H_4\), \(H_5\) and \(O_6\), instead of repeating
    numbers over each layer, \(I_1\), \(I_2\), \(I_3\), \(H_1\), \(H_2\), and \(O_1\)
    to simplify the notation for the weights; therefore, when I say \(\lambda_{1,4}\)
    you know exactly where this weight is applied in the network, from node \(I_1\)
    to node \(H_4\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node Outputs** - I use the node label to also describe the output from the
    node, for example \(O_6\) is the output node, \(O_6\), and also the signal or
    value output from node \(O_6\),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ O_6 = \sigma \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 \right)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre- and Post-activation** - at our nodes \(H_4\), \(H_5\), and \(O_6\),
    we have the node input before activation and the node output after activation,
    I use the notation \(H_{4_{in}}\), \(H_{5_{in}}\), and \(O_{6_{in}}\) for the
    pre-activation input,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ H_{4_{in}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 \]\[ H_{5_{in}} = \lambda_{1,5} \cdot I_1 + \lambda_{2,5} \cdot I_2
    + \lambda_{2,5} \cdot I_3 \]\[ O_{6_{in}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6}
    \cdot H_5 \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\quad\) and \(H_4\), \(H_5\), and \(O_6\) for the post-activation node output.
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{4} = \sigma \left( H_{4_{in}} \right) = \sigma \left( \lambda_{1,4} \cdot
    I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 \right) \]\[ H_{5} = \sigma
    \left( H_{5_{in}} \right) = \sigma \left( \lambda_{1,5} \cdot I_1 + \lambda_{2,5}
    \cdot I_2 + \lambda_{2,5} \cdot I_3 \right) \]\[ O_6 = \sigma \left( O_{6_{in}}
    \right) = \sigma \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 \right)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: It is important to have clean, clear notation because with back propagation
    we have to step through the nodes, going from post-activation to pre-activation.
  prefs: []
  type: TYPE_NORMAL
- en: often variables like \(z\) are applied for pre-activation in neural network
    literature, but I feel this is ambiguous and may cause confusion as we provide
    a nuts and bolts approach, explicitly describing every equation, to describe exactly
    how neural networks are trained and predict
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description of the Network Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs talk about the network, the parts and how information flows through the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feed-forward** ‚Äì all information flows from left to right. Each node sends
    the same signal along the connections to all the nodes in the next layer,'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02be26cd4d5476666cd2e1a72c3005fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Feed forward, fully connected, with each node sending the same signal to all
    the nodes in the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Layer** - the input features are passed directly to the input nodes,
    in the case of continuous predictor features, there is one input node per feature
    and the features are,'
  prefs: []
  type: TYPE_NORMAL
- en: min / max normalization to a range \(\left[ ‚àí1,1 \right]\) or \(\left[ 0,1 \right]\)
    to improve activation function sensitivity and to remove the influence of scale
    differences in predictor features and to improve solution stability, i.e., smooth
    reduction in the training loss while training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/cb20e85861a55df55677b55ce737bbd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Highlighting the input layer, the first layer that receives the normalized predictor
    features.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of categorical predictor features, we have one input node per each
    category for each predictor feature, i.e., after one-hot-encoding of the feature
    where each encoding is passed to a separate input node.
  prefs: []
  type: TYPE_NORMAL
- en: recall one-hot-encoding, 1 if the specific category, 0 otherwise, replaces the
    categorical feature with a binary vector with length as the number of categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/51d695aaaa36c3450f3997691fb0090f.png)'
  prefs: []
  type: TYPE_IMG
- en: The input layer of our artificial neural network highlighted. The first layer
    that receives one-hot-encoding of a single categorical predictor feature.
  prefs: []
  type: TYPE_NORMAL
- en: we could also use a single input node per categorical predictor and assign thresholds
    to each categories, for example \(\left[ 0.0, 0.5, 1.0 \right]\) for 3 categories,
    but this assumes an ordinal categorical feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden Layer** - the input layer values \(I_1, I_2, I_2\) are weighted with
    learnable weights,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{1,4}, \lambda_{2,4}, \lambda_{3,4}, \lambda_{1,5}, \lambda_{2,5},
    \lambda_{3,5} \]
  prefs: []
  type: TYPE_NORMAL
- en: in the hidden layer nodes, the weighted input layer values, \(\lambda_{1,4}
    \cdot I_1, \lambda_{2,4} \cdot I_2 \cdot I_2, \ldots, \lambda_{3,5} \cdot I_3\)
    are summed with the addition of a trainable bias term in each node, \(b_4\) and
    \(b_5\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ H_{4_{in}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]\[ H_{5_{in}} = \lambda_{1,5} \cdot I_1 + \lambda_{2,5} \cdot
    I_2 + \lambda_{3,5} \cdot I_3 + b_5 \]
  prefs: []
  type: TYPE_NORMAL
- en: the nonlinear activation is applied,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{4} = \sigma \bigl( \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 +
    \lambda_{3,4} \cdot I_3 + b_4 \bigr) \]\[ H_{5} = \sigma \bigl( \lambda_{1,5}
    \cdot I_1 + \lambda_{2,5} \cdot I_2 + \lambda_{3,5} \cdot I_3 + b_5 \bigr) \]
  prefs: []
  type: TYPE_NORMAL
- en: the output from the input layer nodes to all hidden layer nodes is contant (again,
    each node sends the same value to all nodes in the next layer)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e2342c26953974528579d98e504c15c8.png)'
  prefs: []
  type: TYPE_IMG
- en: The hidden layer of our artificial neural network highlighted. The input layer
    nodes' outputs are weighted and passed into the hidden layer nodes. The output
    from the hidden layer nodes to all output layer nodes is constant.
  prefs: []
  type: TYPE_NORMAL
- en: '**Output Layer** - for continuous response features there is one output node
    per normalized response feature. Once again the weighted linear combination of
    inputs plus a node bias are calculated,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{in}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \]
  prefs: []
  type: TYPE_NORMAL
- en: and then activation is applied, but for a continuous response feature, typically
    identity (linear) transform is applied,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_6 = \alpha \bigl( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \bigr) = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 = O_{6_{in}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: backtransformation from normalized to original response feature(s) are then
    applied to recover the ultimate prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as with continuous predictor features, min / max normalization is applied to
    continuous response features to a range [‚àí1,1] or [0,1] to improve activation
    function sensitivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/87d8ca10f44d6d5d97c7e6ced2d415e5.png)'
  prefs: []
  type: TYPE_IMG
- en: The output layer of our artificial neural network highlighted. The hidden layer
    nodes' outputs are weighted and passed into the output layer nodes. The output
    from the hidden layer nodes is constant, but the weights vary over the hidden
    layer node to output layer node connections.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a categorical response feature, once again one-hot-encoding is
    applied, therefore, there is one output node per category.
  prefs: []
  type: TYPE_NORMAL
- en: the prediction is the probability of each category
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/11c0969ef52f11e6929df9ff8e4d92c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Highlighting the input layer, the first layer that receives one-hot-encoding
    of a single categorical predictor feature.
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough the Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we are ready to walkthough the artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: we follow a single path to illustrate the precise calculations associated with
    making a prediction with an artificial neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full forward pass is explained next.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inside an Input Layer Node** - input layer nodes just pass the predictor
    features,'
  prefs: []
  type: TYPE_NORMAL
- en: normalized continuous predictor feature value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a single one-hot-encoding value [0 or 1] for categorical prediction features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: into the hidden layer nodes, with general vector notation,
  prefs: []
  type: TYPE_NORMAL
- en: \[ I = X \]![](../Images/4e1c6258eff75f9d4ce767b39cd522fb.png)
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough of an artificial neural network, the input layer node receives one-hot-encoding
    of a single categorical predictor feature and passes it to all of the hidden layer
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: We can generalize over all input layer nodes with,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_j = H_{j_{in}} = X_j \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Inside an Hidden Layer Node**'
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layer nodes are simple processors. The take linearly weighted combinations
    of inputs, add a node bias term and then nonlinearly transform the result, this
    transform is call the activation function, \(\alpha\).
  prefs: []
  type: TYPE_NORMAL
- en: indeed, a very simple processor!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: through many interconnected nodes we gain a very flexible predictor, emergent
    ability to characterize complicated, nonlinear patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prior to activation we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{4_{in}} = \sum_{i=1}^{3} \left( \lambda_{i,4} \cdot I_i \right) + b_4
    \]
  prefs: []
  type: TYPE_NORMAL
- en: and after activation we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_4 = \alpha \left(H_{4_{in}} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: We can express the simple processor in the node with general vector notation
    as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_4 = \alpha \left(b_4 + \lambda_{j,4}^T I \right) \]![](../Images/b2f8e46ea497049f4b95c03b8812eea7.png)
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough of an artificial neural network, the hidden layer linearly weights
    the input from each input layer node, adds a node bias term and then applies an
    activation function and passes this to all nodes in the next layer, i.e., the
    output layer for our example artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: We can generalize over all hidden layer nodes with,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{j_{in}} = \sum_{i=1}^{|I|} \left( \lambda_{i,j} \cdot I_i \right) + b_j
    \]
  prefs: []
  type: TYPE_NORMAL
- en: and after activation, the node output is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{j_{in}} = \sigma \bigl( \sum_{i=1}^{|I|} \left( \lambda_{i,j} \cdot I_i
    \right) + b_j \bigr) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Inside an Output Layer Node**'
  prefs: []
  type: TYPE_NORMAL
- en: The output layer nodes take linearly weighted combinations of nodes‚Äô inputs,
    adds a node bias term and then transforms the result with an activation function,
    \(\alpha\), same as the hidden layer nodes,
  prefs: []
  type: TYPE_NORMAL
- en: Prior to activation we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{in}} = \sum_{i=4}^{5} \left( \lambda_{i,6} \cdot H_i \right) + b_6
    \]
  prefs: []
  type: TYPE_NORMAL
- en: and after activation, assuming identity activation we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_6 = \alpha \left(O_{6_{in}} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: We can express the simple processor in the node with general vector notation
    as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_6 = \alpha\left(b_6 + \lambda_{j,6}^T H\right) \]![](../Images/22c5dd7e545497f42cf0b949f0b92954.png)
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough of an artificial neural network, the output layer linearly weights
    the input from each hidden layer node, adds a node bias term and then applies
    an activation function, typically linear for continuous response features and
    passes this as an output.
  prefs: []
  type: TYPE_NORMAL
- en: and for categorical response features, softmax activation is commonly applied,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_j = \alpha(O_{j_{in}}) = \frac{e^{O_{j_{in}}}}{\sum_{\iota=1}^{K} e^{O_{\iota_{in}}}}
    \]![](../Images/ff80ab0dac26f531ce5cc5ce3cd9cf5d.png)
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough of an artificial neural network, the output layer linearly weights
    the input from each hidden layer node, adds a node bias term and then applies
    an activation function, typically linear for continuous response features and
    passes this as an output.
  prefs: []
  type: TYPE_NORMAL
- en: softmax activation ensures that the output over all the output layer nodes are
    valid probabilities including,
  prefs: []
  type: TYPE_NORMAL
- en: '**nonnegativity** - through the exponentiation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**closure** - probabilities sum to 1.0 through the denominator normalizing
    the result'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, for all future discussions and demonstrations, I assume a standardized
    continuous responce feature.
  prefs: []
  type: TYPE_NORMAL
- en: Network Forward Pass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have completed a walk-through of our network on a single path, let‚Äôs
    combine all the paths through our network to demonstrate a complete forward pass
    through our artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: this is the calculation required to make a prediction with out,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ O_6 = \sigma_{O_6} \bigl( \lambda_{4,6} \cdot \sigma_{H_4} \left( \lambda_{1,4}
    I_1 + \lambda_{2,4} I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot
    \sigma_{H_5} \left( \lambda_{1,5} I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3
    + b_5 \right) + b_6 \bigr) \]
  prefs: []
  type: TYPE_NORMAL
- en: where the activation functions \(\sigma_{H_4}\) = \(\sigma_{H_5}\) = \(\sigma\)
    are sigmoid, and \(\sigma_{O_6}\) is linear (identity), so we could simplify the
    forward pass to,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_6 = \lambda_{4,6} \cdot \sigma \left( \lambda_{1,4} I_1 + \lambda_{2,4}
    I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot \sigma \left( \lambda_{1,5}
    I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3 + b_5 \right) + b_6 \]
  prefs: []
  type: TYPE_NORMAL
- en: This emphasizes that our neural network is a nested set of activated linear
    systems, i.e., linearly weighted averages plus bias terms applied to activation
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Number of Model Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, there are many model parameters, \(theta\), in an artificial neural
    network. First, let‚Äôs clarify these definitions to describe our artificial neural
    network,
  prefs: []
  type: TYPE_NORMAL
- en: '**neural network width** - the number of nodes in the layers of the neural
    network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**neural network depth** - the number of layers in the neural network, typically
    the input layer is not included in this calculation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let‚Äôs assume the following compact notation for a 3 layer artificial neural
    network, input, output and 1 hidden layer, with the width of each layer as,
  prefs: []
  type: TYPE_NORMAL
- en: number of input nodes, \(p\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: number of hidden layer nodes, \(m\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and number of output nodes, \(k\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/1b5b7051a7f760fb3b71c71560ca7613.png)'
  prefs: []
  type: TYPE_IMG
- en: Notation for artificial neural network width, number of input nodes, \(p\),
    number of hidden layer nodes, \(m\), and number of output nodes, \(k\).
  prefs: []
  type: TYPE_NORMAL
- en: fully connected, so for every connection there is a weight,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{ùêº_{1,\ldots,ùëù},ùêª_{1,\ldots,ùëö} } \quad \text{and} \quad \lambda_{ùêª_{1,\ldots,ùëö},ùëÇ_{1,\ldots,ùëò}
    } \]
  prefs: []
  type: TYPE_NORMAL
- en: with full connectivity the number of weights is
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëù \times ùëö \quad \text{and} \quad ùëö \times ùëò \]
  prefs: []
  type: TYPE_NORMAL
- en: and at each hidden layer node there is a bias term,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëè_{H_{1,\ldots,m} } \]
  prefs: []
  type: TYPE_NORMAL
- en: and at every output node there is a bias term,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëè_{O_{1,\ldots,k} } \]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the number of model parameters is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta| = ùëù \times ùëö + ùëö \times ùëò + ùëö + ùëò \]
  prefs: []
  type: TYPE_NORMAL
- en: this assumes an unique bias term at each hidden layer node and output layer
    node, but in some case the same bias term may be applied over the entire layer.
  prefs: []
  type: TYPE_NORMAL
- en: For our example, with \(p = 3\), \(m = 2\) and \(k = 1\), then the number of
    model parameters are,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta| = ùëù \times ùëö + ùëö \times ùëò + ùëö + ùëò \]
  prefs: []
  type: TYPE_NORMAL
- en: after substitution we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta| = 3 \times 2 + 2 \times 1 + 2 + 1 = 11 \]
  prefs: []
  type: TYPE_NORMAL
- en: I select this as a manageable number of parameters, so we can train and visualize
    our model, but consider a more typical model size by increasing our artificial
    neural network‚Äôs width, with \(p = 10\), \(m = 20\) and \(k = 3\), then we have
    many more model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta| = 10 \times 20 + 20 \times 3 + 20 + 3 = 283 \]
  prefs: []
  type: TYPE_NORMAL
- en: If we add hidden layers, increase our artificial neural network‚Äôs depth, the
    number of model parameters will grow very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: we can generalize this calculation for any fully connected, feed forward neural
    network, given a \(W\) vector with the number of nodes, i.e., the width of each
    layer,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \mathbf{L} = [l_0, l_1, l_2, \dots, l_n] \]
  prefs: []
  type: TYPE_NORMAL
- en: where,
  prefs: []
  type: TYPE_NORMAL
- en: \(l_0\) is the number of input neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(l_1, \dots, l_{n-1}\) are the widths of the hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(l_n\) is the number of output neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total number of connection weights is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta_{weights}| = \sum_{i=1}^{n} l_i \cdot l_{i-1} \]
  prefs: []
  type: TYPE_NORMAL
- en: the total number of node biases (there are not bias parameters in the input
    layer nodes, \(l_0\)) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta_{biases}| = \sum_{i=1}^{n} l_i \]
  prefs: []
  type: TYPE_NORMAL
- en: the total number of trainable model parameters, connectioned weights and node
    biases, is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta| = \sum_{i=1}^{n} \left( l_i \cdot l_{i-1} + l_i \right) = \sum_{i=1}^{n}
    l_i \cdot (l_{i-1} + 1) \]
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs take an example of artificial neural network with 4 hidden layers, with
    network width by-layer vector of,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{L} = [10, 8, 6, 4, 2, 1] \]
  prefs: []
  type: TYPE_NORMAL
- en: The total number of connection weights is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta_{L_{weights}}| = \sum_{i=1}^{5} l_i \cdot l_{i-1} = (8 \cdot 10)
    + (6 \cdot 8) + (4 \cdot 6) + (2 \cdot 4) + (1 \cdot 2) = 80 + 48 + 24 + 8 + 2
    = 162 \]
  prefs: []
  type: TYPE_NORMAL
- en: and the total number of node biases is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta_{L_{biases}}| = \sum_{i=1}^{5} l_i = 8 + 6 + 4 + 2 + 1 = 21 \]
  prefs: []
  type: TYPE_NORMAL
- en: and finally the total nuber of trainable parameters is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta_L| = \sum_{i=1}^{5} l_i \cdot (l_{i-1} + 1) = (8 \cdot 11) + (6 \cdot
    9) + (4 \cdot 7) + (2 \cdot 5) + (1 \cdot 3) = 88 + 54 + 28 + 10 + 3 = 183 \]
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The activation function is a transformation of the linear combination of the
    weighted node inputs plus the node bias term. Nonlinear activation,
  prefs: []
  type: TYPE_NORMAL
- en: introduces non-linear properties, and complexity to the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prevents the network from collapsing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without the nonlinear activation function we would have linear regression, the
    entire system collapses.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about activation functions and a demonstration of the collapse
    without nonlinear activation to multilinear regression see the associated chapter
    in this e-book, [Neural Network Activation Functions](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_activation_functions.html).
  prefs: []
  type: TYPE_NORMAL
- en: Training Networks Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training an artificial neural network proceeds iteratively by these steps,
  prefs: []
  type: TYPE_NORMAL
- en: initialized the model parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: forward pass to make a prediction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: calculate the error derivative based on the prediction and truth over training
    data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: backpropagate the error derivative back through the artificial neural network
    to calculate the derivatives of the error over all the model weights and biases
    parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: update the model parameters based on the derivatives and learning rates
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: repeat until convergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/c3a5bc8956f8ceda05ddf9b582cd141d.png)'
  prefs: []
  type: TYPE_IMG
- en: The iterative steps for training an artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs some details on each step,
  prefs: []
  type: TYPE_NORMAL
- en: '**Initializing the Model Parameters** - initialize all model parameters with
    typically small (near zero) random values. Here‚Äôs a couple common methods,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Xavier Weight Initialization** - random realizations from uniform distributions
    specified by \(U[\text{min}, \text{max}]\),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}}, \frac{1}{\sqrt{p}} \right]
    (p^\ell) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(F^{-1}_U\) is the inverse of the CDF, \(p\) is the number of inputs,
    and \(p^{\ell}\) is a random cumulative probability value drawn from the uniform
    distribution, \(U[0,1]\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalized Xavier Weight Initialization** - random realizations from uniform
    distributions specified by \(U[\text{min}, \text{max}]\),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k}
    \right] (p^\ell) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(F^{-1}_U\) is the inverse of the CDF, \(p\) is the number of inputs,
    \(k\) is the number of outputs, and \(p^{\ell}\) is a random cumulative probability
    value drawn from the uniform distribution, \(U[0,1]\).
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we return to our first hidden layer node,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2f8e46ea497049f4b95c03b8812eea7.png)'
  prefs: []
  type: TYPE_IMG
- en: First hidden layer node with 3 inputs, and 1 output.
  prefs: []
  type: TYPE_NORMAL
- en: we have \(p = 3\) and \(k = 1\), and we draw from the uniform distribution,
  prefs: []
  type: TYPE_NORMAL
- en: \[ U \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] = U \left[ \frac{-1}{\sqrt{3}+1},
    \frac{1}{\sqrt{3}+1} \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward Pass** - to make a prediction, \(\hat{y}\). Initial predictions will
    be random for the first iteration, but will improve over iterations. Once again
    for our model the forward pass is,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ O_6 = \lambda_{4,6} \cdot \sigma \left( \lambda_{1,4} I_1 + \lambda_{2,4}
    I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot \sigma \left( \lambda_{1,5}
    I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3 + b_5 \right) + b_6 \]![](../Images/08556ecbd47d143019d0163dc95761cf.png)
  prefs: []
  type: TYPE_NORMAL
- en: Prediction with our artificial neural network initialized with random model
    parameters, weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculate the Error Derivative** - given a loss of,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ L = \frac{1}{2} \left(\hat{y} - y \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and the error derivative, i.e., rate of change of in error given a change in
    model estimate is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial \hat{y}} = \hat{Y} - Y \]
  prefs: []
  type: TYPE_NORMAL
- en: For now, let‚Äôs only consider a single estimate, and we will address more than
    1 training data later.
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagate the Error Derivative** - we shift back through the artificial
    neural network to calculate the derivatives of the error over all the model weights
    and biases parameters, with the chain rule, for example the loss derivative backpropagated
    to the output of node \(H_4\),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    L}{\partial O_6} = \lambda_{4,6} \cdot \bigl( (1 - O_6) \cdot O_6 \bigr) \cdot
    (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Update the Model Parameters** - based on the derivatives, \(\frac{\partial
    L}{\partial \lambda_{i,j}}\) and learning rates, \(\eta\), like this,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \lambda_{i,j}^{\ell} = \lambda_{i,j}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{i,j}} \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Repeat Until Convergence** - return to step 1, until the error, \(L\), is
    reduced to an acceptable level, i.e., model convergence is the condition to stop
    the iterations'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are the steps, now let‚Äôs dive into the details for each, but first let‚Äôs
    start with the mathematical framework for backpropagation - the chain rule.
  prefs: []
  type: TYPE_NORMAL
- en: The Chain Rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Upon reflection, it is clear that the forward pass through our artificial neural
    network involves a sequence of nested operations that progressively transform
    the input signals as they propagate from the input nodes, through each layer,
    to the output nodes.
  prefs: []
  type: TYPE_NORMAL
- en: So we can represent this as a sequence of nested operations,
  prefs: []
  type: TYPE_NORMAL
- en: \[ f = f(x) \quad g = g(f) \quad y = h(g) \]
  prefs: []
  type: TYPE_NORMAL
- en: and now in this form to emphasize the nesting of operations,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = h \bigl( g(f(x)) \bigr) \]
  prefs: []
  type: TYPE_NORMAL
- en: By applying the chain rule to the nested functions \(y = h \bigl( g(f(x)) \bigr)\),
    we can solve for \(\frac{\partial y}{\partial x}\) as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial y}{\partial x} = \frac{\partial h}{\partial g} \cdot \frac{\partial
    g}{\partial f} \cdot \frac{\partial f}{\partial x} \]
  prefs: []
  type: TYPE_NORMAL
- en: where we chain together the partial derivatives for all the operators to solve
    derivative of the output, \(y\), given the input, \(x\).
  prefs: []
  type: TYPE_NORMAL
- en: we can compute derivatives at any intermediate point in the nested functions,
    for example, stepping backwards one step,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \frac{\partial f}{\partial x} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now two steps,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial g}{\partial x} = \frac{\partial g}{\partial f} \cdot \frac{\partial
    f}{\partial x} \]
  prefs: []
  type: TYPE_NORMAL
- en: and all the way with three steps,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial y}{\partial x} = \frac{\partial h}{\partial g} \cdot \frac{\partial
    g}{\partial f} \cdot \frac{\partial f}{\partial x} \]
  prefs: []
  type: TYPE_NORMAL
- en: This is what we do with backpropagation, but this may be too abstract! Let‚Äôs
    move to a very simple feed forward neural network with only these three nodes,
  prefs: []
  type: TYPE_NORMAL
- en: \(I_1\) - input node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(H_2 = h(I_1)\) - hidden layer node, a function of \(I_1\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(O_3 = o(H_2)\) - output node, a function of \(H_2\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this is still intentionally abstract, i.e., without mention of weights and biases,
    to help you develop a mental framework of backpropagation with neural netowrks
    by the chain rule, we will dive into the details immediately after this discussion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output \(O_3\) depends on the input \(I_1\) through these nested functions:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_3 = o \bigl( h(I_1) \bigr) \]
  prefs: []
  type: TYPE_NORMAL
- en: Using the **chain rule**, the gradient of the output with respect to backpropagating
    one step,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_3}{\partial H_2} \]
  prefs: []
  type: TYPE_NORMAL
- en: and with respect to backpropagating two steps,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_3}{\partial I_1} = \frac{\partial H_2}{\partial I_1} \cdot
    \frac{\partial O_3}{\partial H_2} \]
  prefs: []
  type: TYPE_NORMAL
- en: This shows how the gradient **backpropagates** through the network,
  prefs: []
  type: TYPE_NORMAL
- en: \(\frac{\partial O_3}{\partial I_1}\) - is the local gradient at the hidden
    node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\frac{\partial O_3}{\partial H_2}\) - is the local gradient at the output
    node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By backpropagation we can calculate the deriviates with respect to all parts
    of the network, how the input node signal \(I_1\), or hidden nodel signal \(H_2\)
    affect the output \(O_3\), \(\frac{\partial O_3}{\partial I_1}\) and \(frac{\partial
    O_3}{\partial H_2}\) respsectively.
  prefs: []
  type: TYPE_NORMAL
- en: and more importantly, how changes in the input \(I_1\), or \(H_2\) affect the
    change in model loss, \(\frac{\partial L}{\partial I_1}\) and \(frac{\partial
    L}{\partial H_2}\) respsectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chain of partial derivatives, move backwards step by step through the neural
    network layers, is the fundamental mechanism behind **backpropagation**. Next
    we will derive and demonstrate each of the parts of backpropagation and then finally
    put this together to show backpropagation over our entire network.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks Backpropagation Building Blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs cover the numerical building blocks for backpropagation. Once you understand
    these backpropagation building blocks, you will be able to backpropagate our simple
    network and even any complicated artificial neural networks by hand,
  prefs: []
  type: TYPE_NORMAL
- en: calculating the loss derivative
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: backpropagation through nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: backpropagation along connections
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: accounting for multiple paths
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: loss derivatives with respect to weights and biases
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For now I demonstrate backpropagation of this loss derivative for a single training
    data sample, \(y\).
  prefs: []
  type: TYPE_NORMAL
- en: I address multiple samples later, \(y_i, i=1, \ldots, n\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs start with calculating the loss derivative.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the Loss Derivative
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Backpropagation is based on the concept of allocating or propagating the loss
    derivative backwards through the neural network,
  prefs: []
  type: TYPE_NORMAL
- en: we calculate the loss derivative and then distribute it sequentially, in reverse
    direction, from network output back towards the network input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is important to know that we are working with derivatives, and that backpropagation
    is NOT distributing error, although as you will see it may look that way!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We start by defining the loss, given the truth, \(ùë¶\), and our prediction, \(\hat{y}
    = O_6\), we calculate our \(L^2\) loss as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ L = \frac{1}{2} \left( \hat{y} - y \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: our choice of loss function allows us to use the prediction error as the loss
    derivative! We calculate the loss derivative as the partial derivative of the
    loss with respect to the estimate, \(\frac{\partial ùêø}{\partial \hat{y}}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial \hat{y}} = \frac{\partial \frac{1}{2}
    \left( \hat{y} - y \right)^2 }{\partial \hat{y}} = \hat{y} - y \]
  prefs: []
  type: TYPE_NORMAL
- en: You see what I mean, we are backpropagating the loss derivative, but due to
    our formulation of the \(L^2\) loss, we only have to calculate the error at our
    output node output, but once again - it is the loss derivative.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6abf780fa4544caa6735a8f4ad075bd2.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculation of the loss derivative at the output of an output layer node, $O_6$.
  prefs: []
  type: TYPE_NORMAL
- en: For the example of our simple artificial neural network with the output at node,
    \(O_6\), our loss derivative is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial O_6} = \frac{\partial \mathcal{L}}{\hat{y}}
    = \hat{y} - y = O_6 - y \]
  prefs: []
  type: TYPE_NORMAL
- en: So this is our loss derivative backpropagated to the output our output node,
    and we are now we are ready to backpropagate this loss derivative through our
    artificial neural network, let‚Äôs talk about how we step through nodes and along
    connections.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through Output Node with Identity Activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs backpropagate through our output node, \(O_6\), from post-activation to
    pre-activation. To do this we need the partial derivative our activation function.
  prefs: []
  type: TYPE_NORMAL
- en: since this is an output node with a regression artificial neural network I have
    selected the identity or linear activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/1141c4e67c550f275e9079501fe522b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative through the node, $O_6$, from $O_6$ post-activation
    output to $O_{6_{in}}$ pre-activation input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The identity activation at output node \(O_6\) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_6 = \sigma(O_{6_{in}}) = O_6 \]
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of the identity activation at node \(O_6\) with respect to its
    input \(O_{6_{in}}\), i.e., crossing node \(O_6\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_6}{\partial O_{6_{in}}} = \frac{\partial \left(O_{6_{in}}
    \right)}{\partial O_{6_{in}}} = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need \(O_6\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative with respect to the node
    output, \(\frac{\partial \mathcal{L}}{\partial O_6}\), and to the loss derivative
    with respect to the node input, \(\frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = \frac{\partial
    O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6}
    = 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have backpropagated through an output node, let‚Äôs backpropagation
    along the \(H_4\) to \(O_6\) connection from the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation along Connections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let‚Äôs backpropagate along the connection between nodes \(O_6\) and \(H_4\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c686c4b3a6fe96317a79c7333542353.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative through node \(O_6\), from \(O_6\) post-activation
    output to $O_{6_{in}}$ pre-activation input and then along the connection to the
    output from node \(H_4\).
  prefs: []
  type: TYPE_NORMAL
- en: Preactivation, the input to node \(ùëÇ_6\) is calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the derivative along the connection as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \frac{\partial}{\partial
    H_4} \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right) =
    \lambda_{4,6} \]
  prefs: []
  type: TYPE_NORMAL
- en: by resolving the above partial derivative, we see that backpropagation along
    a connection by applying the connection weight.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \lambda_{4,6} \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need the current connection weight \(\lambda_{4,6}\). Now we can
    add this to our chain rule to backpropagate along the \(H_4\) to \(O_6\) connection
    from loss derivative with respect to the output layer node input \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\), to the loss derivative with respect
    to the hidden layer node output \(\frac{\partial \mathcal{L}}{\partial H_4}\).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \lambda_{4,6} \bigl( \cdot (1 - O_6) \cdot O_6 \bigr)
    \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through Nodes with Sigmoid Activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs backpropagate through a hidden layer node, \(H_4\), from postactivation
    to preactivation. To do this we need the partial derivative our activation function.
  prefs: []
  type: TYPE_NORMAL
- en: we are assuming sigmoid activation for all hidden layer nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for super clean logic, everyone resolves the activation derivative as a function
    of the output rather than as typical the input,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/10ef488402716c36aa45b497ea54f6bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative through the node, $H_4$, from $H_4$ postactivation
    output to $H_{4_{in}}$ preactivation input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sigmoid activation at output node \(H_4\) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_4 = \sigma(H_{4_{in}}) = \frac{1}{1 + e^{-H_{4_{in}}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of the sigmoid activation at node \(H_4\) with respect to its
    input \(H_{4_{in}}\), i.e., crossing node \(H_4\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + e^{-H_{4_{in}}}} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now, for compact notation let‚Äôs set,
  prefs: []
  type: TYPE_NORMAL
- en: \[ u = e^{-H_{4_{in}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: and substituting we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: and by the chain rule we can extend it to,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) = -\frac{u}{(1 + u)^2} \cdot \frac{\partial u}{\partial
    H_{4_{in}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The derivative of \(u = e^{-H_{4_{in}}}\) with respect to \(H_{4_{in}}\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial u}{\partial H_{4_{in}}} = -e^{-H_{4_{in}}} = -u \]
  prefs: []
  type: TYPE_NORMAL
- en: now we can substitute,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = -\frac{1}{(1+u)^2} \cdot (-u)
    = \frac{u}{(1+u)^2} \]
  prefs: []
  type: TYPE_NORMAL
- en: Express in terms of node \(H_4\) output, \(H_4 = \frac{1}{1 + u}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \frac{\left(1 - H_4\right)/H_4}{\left(1/H_4\right)^2}
    = \frac{1 - H_4}{H_4} \cdot H_4^2 = \left(1 - H_4\right) \cdot H_4 \]
  prefs: []
  type: TYPE_NORMAL
- en: So we can backpropagate through our node, \(H_4\), from node post-activation
    output, \(H_4\) to node pre-activation input, \(H_{4_{in}}\), by,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \left(1 - H_4\right) \cdot
    H_4 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need \(H_4\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative to the output of node
    \(H_4\) and to the input of node \(H_4\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} = \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6}
    \cdot 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now we can handle all cases of backpropagation through the nodes in our network.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation Along Another Connection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For continuity and completeness, let‚Äôs repeat the previously described method
    to backpropagate along the connection \(I_1\) to \(H_4\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76f408b8ed852b03a168e6069bb716db.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative along the connection from $H_4$ to $I_1$.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, preactivation the input to node \(H_4\) is calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{4_{\text{in}}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the derivative along the connection as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \frac{\partial \left(\lambda_{1,4}
    \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 + b_4 \right)}{\partial
    I_1} = \lambda_{1,4} \]
  prefs: []
  type: TYPE_NORMAL
- en: by resolving the above partial derivative, we see that backpropagation along
    a connection by applying the connection weight.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \lambda_{4,6} \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need the current connection weight \(\lambda_{4,6}\). Now we can
    add this to our chain rule to backpropagate along the \(H_4\) to \(O_6\) connection
    from loss derivative with respect to the output layer node input \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\), to the loss derivative with respect
    to the hidden layer node output \(\frac{\partial \mathcal{L}}{\partial H_4}\).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Accounting for Multiple Paths
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our loss derivative with respect to the node output \(I_1\), \(\frac{\partial
    \mathcal{L}}{\partial I_1}\) is not correct!
  prefs: []
  type: TYPE_NORMAL
- en: we accounted for the \(O_6\) to \(H_4\) to \(I_1\) path, but we did not acccount
    for the \(O_6\) to \(H_5\) to \(I_1\) path
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6a68404e4b3d2589dcfe15beb5175c60.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiple paths for backpropagation to input node, $I_1$, from output node $O_6$.
  prefs: []
  type: TYPE_NORMAL
- en: To account for multiple paths we just need to sum over all the paths.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: we can evaluate this as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) + \lambda_{1,5}
    \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \cdot 1.0 \cdot (O_6
    - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: and then simplify by removing the 1.0 values and grouping terms as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now we can evaluate this simplified form as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1 - H_5)
    \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through Input Nodes with Identity Activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs backpropagate through our input node, \(I_1\), from postactivation to
    preactivation. To do this we need the partial derivative our activation function.
  prefs: []
  type: TYPE_NORMAL
- en: since this is an input node I have selected the identity or linear activation
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/bd05208edfb4ac5d5cc0f8cfe808ae50.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative through the node, $I_1$, from $I_1$ postactivation
    output to $I_{1_{in}}$ preactivation input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The identity activation at output node \(I_1\) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ I_1 = \sigma(I_{1_{in}}) = I_1 \]
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of the identity activation at node \(I_1\) with respect to its
    input \(I_{1_{in}}\), i.e., passing through node \(I_1\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial I_1}{\partial I_{1_{in}}} = \frac{\partial \left(I_{1_{in}}
    \right)}{\partial I_{1_{in}}} = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need \(I_1\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative with respect to the node
    output, \(\frac{\partial \mathcal{L}}{\partial I_1}\), and to the loss derivative
    with respect to the node input, \(\frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \frac{\partial I_1}{\partial
    I_{1_{in}}} \cdot \frac{\partial H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} + \frac{\partial I_1}{\partial I_{1_{in}}} \cdot \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial
    O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: we can evaluate this as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = 1.0 \cdot \lambda_{1,4}
    \cdot \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6
    - y) + 1.0 \cdot \lambda_{1,5} \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6}
    \cdot 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: For fun I designed this notation for maximum clarity,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial I_{1_{\text{in}}}} = \overbrace{1.0}^{\textstyle
    \frac{\partial I_{1}}{\partial I_{1_{in}}}} \left[ \overbrace{\lambda_{1,4}}^{\textstyle
    \frac{\partial H_{4_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_4) \cdot
    H_4}^{\textstyle \frac{\partial H_4}{\partial H_{4_{\text{in}}}}} \cdot \overbrace{\lambda_{4,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_4}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} + \overbrace{\lambda_{1,5}}^{\textstyle \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_5) \cdot H_5}^{\textstyle
    \frac{\partial H_5}{\partial H_{5_{\text{in}}}}} \cdot \overbrace{\lambda_{5,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_5}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: But this can be simplified by removing the 1.0 values and grouping terms as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now we can evaluate this simplified form as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \lambda_{1,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: For completeness here is the backpropagation for the other input nodes, here‚Äôs
    \(\frac{\partial \mathcal{L}}{\partial I_{2_{in}}}\),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b186fef88b4e0ad1fa6191394b0e0dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative for input node 2.
  prefs: []
  type: TYPE_NORMAL
- en: For brevity I have remove the 1.0s and grouped like terms,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_2} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_2} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now we can evaluate this simplified form as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \lambda_{2,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{2,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: and here is \(\frac{\partial \mathcal{L}}{\partial I_{3_{in}}}\),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90c6a46319d78ae1b9d4f8cdc131fd2a.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative for input node 3.
  prefs: []
  type: TYPE_NORMAL
- en: For brevity I have remove the 1.0s and grouped like terms,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_3} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_3} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now we can evaluate this simplified form as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \lambda_{3,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{3,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Loss Derivatives with Respect to Weights and Biases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we have back propagated the loss derivative through our network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97de6228871f7910de4d7a8fa86e0bd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagated loss derivatives with respect to all network nodes inputs and
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: and we have the loss derivative with respect to the input and output of each
    node in our network,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial I_1},\quad \frac{\partial \mathcal{L}}{\partial I_{2_{\text{in}}}},\quad
    \frac{\partial \mathcal{L}}{\partial I_2},\quad \frac{\partial \mathcal{L}}{\partial
    I_{3_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial I_3},\quad \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial
    H_4},\quad \frac{\partial \mathcal{L}}{\partial H_5},\quad \frac{\partial \mathcal{L}}{\partial
    H_5},\quad \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: But what we actually need is the loss derivative with respect to each connection
    weights,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}},\quad \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,4}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{1,5}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{2,5}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,5}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{5,6}} \]
  prefs: []
  type: TYPE_NORMAL
- en: and node biases,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial b_4},\quad \frac{\partial \mathcal{L}}{\partial
    b_5},\quad \frac{\partial \mathcal{L}}{\partial b_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: How do we backpropagate the loss derivative to a connection weight? Let‚Äôs start
    with the \(H_4\) to \(O_6\) connection.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf1e650cd4b0613fa09aa5d5c0e72e26.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagated loss derivatives with respect to a connection weight.
  prefs: []
  type: TYPE_NORMAL
- en: Preactivation, input to node \(O_6\) we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the derivative with respect to the connection weight as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial \lambda_{4,6}} = \frac{\partial
    \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial
    \lambda_{4,6}} = H_4 \]
  prefs: []
  type: TYPE_NORMAL
- en: We need the output of the node in the previous layer passed along the connection
    to backpropagate to the loss derivative with respect to the connection weight
    from the input to the next node,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now, for completeness, here are the equations for all of our network‚Äôs connection
    weights.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_5 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: See the pattern, the loss derivatives with respect to connection weights are,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Connection Signal} \times \text{Loss Derivative of Next Node Input}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Now how do we backpropagate the loss derivative to a node bias? Let‚Äôs start
    with the \(O_6\) node.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/646997a228f8c1e8a3b2743ac13e388a.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagated loss derivatives with respect to a node bias.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the preactivation, input to node \(O_6\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the derivative of a connection weight as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial b_6} = \frac{\partial \left( \lambda_{4,6}
    \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial b_6} = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: so our bias loss derivative is equal to the node input loss derivative,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial O_{6_{\text{in}}}}{\partial
    b_6} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = 1.0 \cdot
    \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: For completeness here are all the loss derivatives with respect to node biases,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial \mathcal{L}}{\partial
    O_{6_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial b_4} = \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial
    b_5} = \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: See the pattern, the loss derivatives with respect to node biases are,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Loss Derivative of the Node Input} \]
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs take the backpropagation method explained above and apply them to my interactive
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs the result for our first training epoch with only 1 sample,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b8b7d56b334d9728490cb2bc1408535e.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation result for the first iteration.
  prefs: []
  type: TYPE_NORMAL
- en: My interactive dashboard provides all the loss derivatives with respect to the
    input for each node and the output signals from each node, so for example we can
    calculate \(\frac{\partial L}{\partial \lambda_{4,6}}\) as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial L}{\partial O_{6_{\text{in}}}} = H_4 \cdot
    \frac{\partial L}{\partial O_{6_{\text{in}}}} = 0.42 \cdot 1.00 = 0.42 \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs the loss derivatives with respect to connection weights for the other
    hidden layer to output node connection,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial L}{\partial O_{6_{\text{in}}}} = H_5 \cdot
    \frac{\partial L}{\partial O_{6_{\text{in}}}} = 0.60 \cdot 1.00 = 0.60 \]
  prefs: []
  type: TYPE_NORMAL
- en: and now let‚Äôs get all the input to hidden layer connections,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_1 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.50 \cdot (-0.13) = -0.07 \]\[
    \frac{\partial L}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_1 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.50 \cdot (-0.10) = -0.05 \]\[
    \frac{\partial L}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_2 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.20 \cdot (-0.13) = -0.03 \]\[
    \frac{\partial L}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = I_2 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.20 \cdot (-0.10) = -0.02 \]\[
    \frac{\partial L}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_3 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.70 \cdot (-0.13) = -0.09 \]\[
    \frac{\partial L}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = I_3 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.70 \cdot (-0.10) = -0.07 \]
  prefs: []
  type: TYPE_NORMAL
- en: This takes care of all of the connection weight error derivatives, now lets
    take care of the node bias error derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: the node bias error derivatives are the same as the node peractivation error
    derivatives. Now let‚Äôs calculate the bias terms in the hidden layer,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial b_4} = \frac{\partial H_{4_{\text{in}}}}{\partial
    b_4} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = 1.0 \cdot (-0.13) =
    -0.13 \]\[ \frac{\partial L}{\partial b_5} = \frac{\partial H_{5_{\text{in}}}}{\partial
    b_5} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = 1.0 \cdot (-0.1) =
    -0.10 \]
  prefs: []
  type: TYPE_NORMAL
- en: Updating Model Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The loss derivatives with respect to each of the model parameters are the gradients,
    so we are ready to use gradient descent optimization with the addition of,
  prefs: []
  type: TYPE_NORMAL
- en: '**learning rate** - to scale the rate of change of the model updates we assign
    a learning rate, \(\eta\). For our model parameter examples from above,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{4,6}} \]\[ \lambda_{1,4}^{\ell} = \lambda_{1,4}^{\ell - 1}
    + \eta \cdot \frac{\partial L}{\partial \lambda_{1,4}} \]\[ b_j^{\ell} = b_j^{\ell
    - 1} + \eta \cdot \frac{\partial L}{\partial b_j} \]
  prefs: []
  type: TYPE_NORMAL
- en: recall, this process of gradient calculation and model parameters, weights and
    biases, updating is iterated and is known as gradient descent optimization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the goal is to explore the loss hypersurface, avoiding and escaping local minimums
    and ultimately finding the global minimum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: learning rate, also known as step size is commonly set between 0.0 and 1.0,
    note 0.01 is the default in Keras module of TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low Learning Rate** ‚Äì more stable, but a slower solution, may get stuck in
    a local minimum'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Learning Rate** ‚Äì may be unstable, but perhaps a faster solution, may
    diverge out of the global minimum'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One strategy is to start with a high learning rate and then to decrease the
    learning rate over the iterations
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning Rate Decay** - set as > 0 to avoid mitigate oscillations,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \eta^{\ell} = \eta^{\ell - 1} \cdot \left( \frac{1}{1 + \text{decay} \cdot
    \ell} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\ell\) is the model training epoch
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the model parameter updates are for a single training data case?
    Consider this single model parameter,
  prefs: []
  type: TYPE_NORMAL
- en: we calculate the update over all samples in the batch and apply the average
    of the updates.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial \lambda_{4,6}} = H_4 \cdot \frac{\partial L}{\partial
    O_{6_{\text{in}}}} = 0.42 \cdot 1.00 = 0.42 \]
  prefs: []
  type: TYPE_NORMAL
- en: is applied to update the \(\lambda_{4,6}\) parameter as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{4,6}} \]
  prefs: []
  type: TYPE_NORMAL
- en: is dependent on \(H_4\) node output, and \(L\) error that are for a single sample,
    \(ùë•_1,\ldots,ùë•_ùëö\) and \(ùë¶\); therefore, we cannot calculate a single parameter
    update over all our \(1,\ldots,n\) training data samples.
  prefs: []
  type: TYPE_NORMAL
- en: instead we can calculate \(1,\ldots,n\) updates and then apply the average of
    all the updates to our model parameters,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \frac{1}{n_{batch}} \sum_{i=1}^{n_{batch}}
    \eta \cdot \frac{\partial L}{\partial \lambda_{4,6}} \]
  prefs: []
  type: TYPE_NORMAL
- en: since the learning rate is a constant, we can move it out of the sum and now
    we are averaging the gradients,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \frac{1}{n_{batch}}
    \sum_{i=1}^{n_{batch}} \frac{\partial L}{\partial \lambda_{4,6}} \]
  prefs: []
  type: TYPE_NORMAL
- en: Training Epochs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a good time to talk about stochastic gradient descent optimization,
    first let‚Äôs define some common terms,
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch Gradient Descent** - updates the model parameters after passing through
    all of the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic Gradient Descent** - updates the model parameters over each sample
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mini-batch Gradient Descent** - updates the model parameter after passing
    through a single batch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With mini-batch gradient descent stochasticity is introduced through the use
    of subsets of the data, known as batches,
  prefs: []
  type: TYPE_NORMAL
- en: for example, if we divide our 100 samples into 4 batches, then we iterate over
    each batch separately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we speed up the individual updates, fewer data are faster to calculate, but
    we introduce more error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this often helps the training explore for the global minimum and avoid getting
    stuck in local minimums and along ridges in the loss hypersurface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally our last definition here,
  prefs: []
  type: TYPE_NORMAL
- en: '**epoch** - is one pass over all of the data, so that would be 4 iterations
    of updating the model parameters if we have 4 mini-batches'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many other considerations that I will add later including,
  prefs: []
  type: TYPE_NORMAL
- en: momentum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: adaptive optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let‚Äôs build the above artificial neural network by-hand and visualize the
    solution!
  prefs: []
  type: TYPE_NORMAL
- en: this is by-hand so that you can see every calculation. I intentionally avoided
    using TensorFlow or PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interactive Dashboard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I built out an interactive Python dashboard with the code below for training
    an artificial neural network. You can step through the training iteration and
    observe over the training epochs,
  prefs: []
  type: TYPE_NORMAL
- en: model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: forward pass predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: backpropagation of error derivatives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you would like to see artificial neural networks in action, check out my
    [ANN interactive Python dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_ANN.ipynb),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab7e3b58fcfdbf8f7a2c19b5d78c7736.png)'
  prefs: []
  type: TYPE_IMG
- en: Interactive artificial neural network training Python dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Import Required Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will also need some standard packages. These should have been installed with
    Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available
    with the respective package docs.
  prefs: []
  type: TYPE_NORMAL
- en: Declare Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs the functions to make, train and visualize our artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Simple ANN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I wrote this code to specify a simple ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: three input nodes, 2 hidden nodes and 1 output node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'and to train the ANN by iteratively performing the forward calculation and
    backpropagation. I calculate:'
  prefs: []
  type: TYPE_NORMAL
- en: the error and then propagate it to each node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: solve for the partial derivatives of the error with respect to each weight and
    bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: all weights, biases and partial derivatives for all epoch are recorded in vectors
    for plotting
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now Visualize the Network for a Specific Epoch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I wrote a custom network visualization below, select iepoch and visualize the
    artificial neural network for a specific epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/f1c2dc9fc05ec8a3d07b5642c59c42dd91a2b2ed35c547faa409371a1e776270.png](../Images/6c1b34f06ebf3ebfafbbd206fc1033ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Check the ANN Convergence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we plot the weights, biases and prediction over the epochs to check the
    training convergence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/96a2f9a7007123700054af340b223a0c9017cbf262afe43f97c8f1b445ec44a6.png](../Images/41e5da630bdb6e70068ef584a2fc3d47.png)'
  prefs: []
  type: TYPE_IMG
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of artificial neural networks. Much more could be
    done and discussed, I have many more resources. Check out my [shared resource
    inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture links
    at the start of this chapter with resource links in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Artificial neural networks are very powerful, nature inspired computing based
    on an analogy of brain
  prefs: []
  type: TYPE_NORMAL
- en: I suggest that they are like a reptilian brain, without planning and higher
    order reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, artificial neural networks are a building block of many other deep
    learning methods, for example,
  prefs: []
  type: TYPE_NORMAL
- en: convolutional neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: recurrent neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: generative adversarial networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nature inspired computing is looking to nature for inspiration to develop novel
    problem-solving methods,
  prefs: []
  type: TYPE_NORMAL
- en: '**artificial neural networks** are inspired by biological neural networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nodes** - in our model are artificial neurons, simple processors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**connections** between nodes are artificial synapses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: intelligence emerges from many connected simple processors. For the remainder
    of this chapter, I will used the terms nodes and connections to describe our artificial
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network Concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here are some key aspects of artificial neural networks,
  prefs: []
  type: TYPE_NORMAL
- en: '**Basic Design** - *‚Äú‚Ä¶a computing system made up of a number of simple, highly
    interconnected processing elements, which process information by their dynamic
    state response to external inputs.‚Äù* Caudill (1989).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Still a Prediction Model** - while these models may be quite complicated
    with even millions of trainable model parameters, weights and biases, they are
    still a function that maps from predictor features to response features,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ Y=f(X)+\epsilon \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning** ‚Äì we provide training data with predictor features,
    \(X_1,\ldots,ùëã_ùëö\) and response feature(s), \(ùëå_1,\ldots,ùëå_K\), with the expectation
    of some model prediction error, \(\epsilon\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nonlinearity** - nonlinearity is imparted to the system through the application
    of nonlinear activation functions to model nonlinear relationships'
  prefs: []
  type: TYPE_NORMAL
- en: '**Universal Function Approximator (Universal Approximation Theorem)** - ANNs
    have the ability to learn any possible function shape of \(f\) over an interval,
    for an arbitrary wide (single hidden layer) by Cybenko (1989) and arbitrary depth
    by Lu and others (2017)'
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get started, let‚Äôs build a neural net, single hidden layer, fully connected,
    feed-forward neural network,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/832ee38f2f09d395115eb31b95358103.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple demonstration artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: We use this example artificial neural network in the descriptions below and
    as an actual example that we will train and predict with by-hand!
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs label the parts of our network,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0be0dcf09e698cf22dc0881c818c0dbc.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple demonstration artificial neural network with the parts labeled, including
    3 inputs nodes, 2 hidden nodes and 1 output node fully connected.
  prefs: []
  type: TYPE_NORMAL
- en: Our artificial neural network has,
  prefs: []
  type: TYPE_NORMAL
- en: 3 predictor features, \(X_1\), \(X_2\) and \(X_3\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3 input nodes, \(I_1\), \(I_2\) and \(I_3\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2 hidden layer nodes, \(H_4\) and \(H_5\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 output node, \(O_6\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 response feature, \(Y_1\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: where all nodes fully connected. Note, deep learning is a neural network with
    more than 1 hidden layer, but for brevity let‚Äôs continue with our non-deep learning
    artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Comments on Network Nomenclature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just a couple more comments about my network nomenclature. My goal is to maximize
    simplicity and clarity,
  prefs: []
  type: TYPE_NORMAL
- en: '**Network Nodes and Connections** - I choose to use unique numbers for all
    nodes, \(I_1\), \(I_2\), \(I_3\), \(H_4\), \(H_5\) and \(O_6\), instead of repeating
    numbers over each layer, \(I_1\), \(I_2\), \(I_3\), \(H_1\), \(H_2\), and \(O_1\)
    to simplify the notation for the weights; therefore, when I say \(\lambda_{1,4}\)
    you know exactly where this weight is applied in the network, from node \(I_1\)
    to node \(H_4\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node Outputs** - I use the node label to also describe the output from the
    node, for example \(O_6\) is the output node, \(O_6\), and also the signal or
    value output from node \(O_6\),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ O_6 = \sigma \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 \right)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre- and Post-activation** - at our nodes \(H_4\), \(H_5\), and \(O_6\),
    we have the node input before activation and the node output after activation,
    I use the notation \(H_{4_{in}}\), \(H_{5_{in}}\), and \(O_{6_{in}}\) for the
    pre-activation input,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ H_{4_{in}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 \]\[ H_{5_{in}} = \lambda_{1,5} \cdot I_1 + \lambda_{2,5} \cdot I_2
    + \lambda_{2,5} \cdot I_3 \]\[ O_{6_{in}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6}
    \cdot H_5 \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\quad\) and \(H_4\), \(H_5\), and \(O_6\) for the post-activation node output.
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{4} = \sigma \left( H_{4_{in}} \right) = \sigma \left( \lambda_{1,4} \cdot
    I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 \right) \]\[ H_{5} = \sigma
    \left( H_{5_{in}} \right) = \sigma \left( \lambda_{1,5} \cdot I_1 + \lambda_{2,5}
    \cdot I_2 + \lambda_{2,5} \cdot I_3 \right) \]\[ O_6 = \sigma \left( O_{6_{in}}
    \right) = \sigma \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 \right)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: It is important to have clean, clear notation because with back propagation
    we have to step through the nodes, going from post-activation to pre-activation.
  prefs: []
  type: TYPE_NORMAL
- en: often variables like \(z\) are applied for pre-activation in neural network
    literature, but I feel this is ambiguous and may cause confusion as we provide
    a nuts and bolts approach, explicitly describing every equation, to describe exactly
    how neural networks are trained and predict
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description of the Network Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs talk about the network, the parts and how information flows through the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feed-forward** ‚Äì all information flows from left to right. Each node sends
    the same signal along the connections to all the nodes in the next layer,'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02be26cd4d5476666cd2e1a72c3005fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Feed forward, fully connected, with each node sending the same signal to all
    the nodes in the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Layer** - the input features are passed directly to the input nodes,
    in the case of continuous predictor features, there is one input node per feature
    and the features are,'
  prefs: []
  type: TYPE_NORMAL
- en: min / max normalization to a range \(\left[ ‚àí1,1 \right]\) or \(\left[ 0,1 \right]\)
    to improve activation function sensitivity and to remove the influence of scale
    differences in predictor features and to improve solution stability, i.e., smooth
    reduction in the training loss while training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/cb20e85861a55df55677b55ce737bbd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Highlighting the input layer, the first layer that receives the normalized predictor
    features.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of categorical predictor features, we have one input node per each
    category for each predictor feature, i.e., after one-hot-encoding of the feature
    where each encoding is passed to a separate input node.
  prefs: []
  type: TYPE_NORMAL
- en: recall one-hot-encoding, 1 if the specific category, 0 otherwise, replaces the
    categorical feature with a binary vector with length as the number of categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/51d695aaaa36c3450f3997691fb0090f.png)'
  prefs: []
  type: TYPE_IMG
- en: The input layer of our artificial neural network highlighted. The first layer
    that receives one-hot-encoding of a single categorical predictor feature.
  prefs: []
  type: TYPE_NORMAL
- en: we could also use a single input node per categorical predictor and assign thresholds
    to each categories, for example \(\left[ 0.0, 0.5, 1.0 \right]\) for 3 categories,
    but this assumes an ordinal categorical feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden Layer** - the input layer values \(I_1, I_2, I_2\) are weighted with
    learnable weights,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{1,4}, \lambda_{2,4}, \lambda_{3,4}, \lambda_{1,5}, \lambda_{2,5},
    \lambda_{3,5} \]
  prefs: []
  type: TYPE_NORMAL
- en: in the hidden layer nodes, the weighted input layer values, \(\lambda_{1,4}
    \cdot I_1, \lambda_{2,4} \cdot I_2 \cdot I_2, \ldots, \lambda_{3,5} \cdot I_3\)
    are summed with the addition of a trainable bias term in each node, \(b_4\) and
    \(b_5\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ H_{4_{in}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]\[ H_{5_{in}} = \lambda_{1,5} \cdot I_1 + \lambda_{2,5} \cdot
    I_2 + \lambda_{3,5} \cdot I_3 + b_5 \]
  prefs: []
  type: TYPE_NORMAL
- en: the nonlinear activation is applied,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{4} = \sigma \bigl( \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 +
    \lambda_{3,4} \cdot I_3 + b_4 \bigr) \]\[ H_{5} = \sigma \bigl( \lambda_{1,5}
    \cdot I_1 + \lambda_{2,5} \cdot I_2 + \lambda_{3,5} \cdot I_3 + b_5 \bigr) \]
  prefs: []
  type: TYPE_NORMAL
- en: the output from the input layer nodes to all hidden layer nodes is contant (again,
    each node sends the same value to all nodes in the next layer)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e2342c26953974528579d98e504c15c8.png)'
  prefs: []
  type: TYPE_IMG
- en: The hidden layer of our artificial neural network highlighted. The input layer
    nodes' outputs are weighted and passed into the hidden layer nodes. The output
    from the hidden layer nodes to all output layer nodes is constant.
  prefs: []
  type: TYPE_NORMAL
- en: '**Output Layer** - for continuous response features there is one output node
    per normalized response feature. Once again the weighted linear combination of
    inputs plus a node bias are calculated,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{in}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \]
  prefs: []
  type: TYPE_NORMAL
- en: and then activation is applied, but for a continuous response feature, typically
    identity (linear) transform is applied,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_6 = \alpha \bigl( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \bigr) = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 = O_{6_{in}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: backtransformation from normalized to original response feature(s) are then
    applied to recover the ultimate prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as with continuous predictor features, min / max normalization is applied to
    continuous response features to a range [‚àí1,1] or [0,1] to improve activation
    function sensitivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/87d8ca10f44d6d5d97c7e6ced2d415e5.png)'
  prefs: []
  type: TYPE_IMG
- en: The output layer of our artificial neural network highlighted. The hidden layer
    nodes' outputs are weighted and passed into the output layer nodes. The output
    from the hidden layer nodes is constant, but the weights vary over the hidden
    layer node to output layer node connections.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a categorical response feature, once again one-hot-encoding is
    applied, therefore, there is one output node per category.
  prefs: []
  type: TYPE_NORMAL
- en: the prediction is the probability of each category
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/11c0969ef52f11e6929df9ff8e4d92c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Highlighting the input layer, the first layer that receives one-hot-encoding
    of a single categorical predictor feature.
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough the Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we are ready to walkthough the artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: we follow a single path to illustrate the precise calculations associated with
    making a prediction with an artificial neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full forward pass is explained next.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inside an Input Layer Node** - input layer nodes just pass the predictor
    features,'
  prefs: []
  type: TYPE_NORMAL
- en: normalized continuous predictor feature value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a single one-hot-encoding value [0 or 1] for categorical prediction features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: into the hidden layer nodes, with general vector notation,
  prefs: []
  type: TYPE_NORMAL
- en: \[ I = X \]![](../Images/4e1c6258eff75f9d4ce767b39cd522fb.png)
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough of an artificial neural network, the input layer node receives one-hot-encoding
    of a single categorical predictor feature and passes it to all of the hidden layer
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: We can generalize over all input layer nodes with,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_j = H_{j_{in}} = X_j \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Inside an Hidden Layer Node**'
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layer nodes are simple processors. The take linearly weighted combinations
    of inputs, add a node bias term and then nonlinearly transform the result, this
    transform is call the activation function, \(\alpha\).
  prefs: []
  type: TYPE_NORMAL
- en: indeed, a very simple processor!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: through many interconnected nodes we gain a very flexible predictor, emergent
    ability to characterize complicated, nonlinear patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prior to activation we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{4_{in}} = \sum_{i=1}^{3} \left( \lambda_{i,4} \cdot I_i \right) + b_4
    \]
  prefs: []
  type: TYPE_NORMAL
- en: and after activation we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_4 = \alpha \left(H_{4_{in}} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: We can express the simple processor in the node with general vector notation
    as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_4 = \alpha \left(b_4 + \lambda_{j,4}^T I \right) \]![](../Images/b2f8e46ea497049f4b95c03b8812eea7.png)
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough of an artificial neural network, the hidden layer linearly weights
    the input from each input layer node, adds a node bias term and then applies an
    activation function and passes this to all nodes in the next layer, i.e., the
    output layer for our example artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: We can generalize over all hidden layer nodes with,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{j_{in}} = \sum_{i=1}^{|I|} \left( \lambda_{i,j} \cdot I_i \right) + b_j
    \]
  prefs: []
  type: TYPE_NORMAL
- en: and after activation, the node output is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{j_{in}} = \sigma \bigl( \sum_{i=1}^{|I|} \left( \lambda_{i,j} \cdot I_i
    \right) + b_j \bigr) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Inside an Output Layer Node**'
  prefs: []
  type: TYPE_NORMAL
- en: The output layer nodes take linearly weighted combinations of nodes‚Äô inputs,
    adds a node bias term and then transforms the result with an activation function,
    \(\alpha\), same as the hidden layer nodes,
  prefs: []
  type: TYPE_NORMAL
- en: Prior to activation we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{in}} = \sum_{i=4}^{5} \left( \lambda_{i,6} \cdot H_i \right) + b_6
    \]
  prefs: []
  type: TYPE_NORMAL
- en: and after activation, assuming identity activation we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_6 = \alpha \left(O_{6_{in}} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: We can express the simple processor in the node with general vector notation
    as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_6 = \alpha\left(b_6 + \lambda_{j,6}^T H\right) \]![](../Images/22c5dd7e545497f42cf0b949f0b92954.png)
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough of an artificial neural network, the output layer linearly weights
    the input from each hidden layer node, adds a node bias term and then applies
    an activation function, typically linear for continuous response features and
    passes this as an output.
  prefs: []
  type: TYPE_NORMAL
- en: and for categorical response features, softmax activation is commonly applied,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_j = \alpha(O_{j_{in}}) = \frac{e^{O_{j_{in}}}}{\sum_{\iota=1}^{K} e^{O_{\iota_{in}}}}
    \]![](../Images/ff80ab0dac26f531ce5cc5ce3cd9cf5d.png)
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough of an artificial neural network, the output layer linearly weights
    the input from each hidden layer node, adds a node bias term and then applies
    an activation function, typically linear for continuous response features and
    passes this as an output.
  prefs: []
  type: TYPE_NORMAL
- en: softmax activation ensures that the output over all the output layer nodes are
    valid probabilities including,
  prefs: []
  type: TYPE_NORMAL
- en: '**nonnegativity** - through the exponentiation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**closure** - probabilities sum to 1.0 through the denominator normalizing
    the result'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, for all future discussions and demonstrations, I assume a standardized
    continuous responce feature.
  prefs: []
  type: TYPE_NORMAL
- en: Network Forward Pass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have completed a walk-through of our network on a single path, let‚Äôs
    combine all the paths through our network to demonstrate a complete forward pass
    through our artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: this is the calculation required to make a prediction with out,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ O_6 = \sigma_{O_6} \bigl( \lambda_{4,6} \cdot \sigma_{H_4} \left( \lambda_{1,4}
    I_1 + \lambda_{2,4} I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot
    \sigma_{H_5} \left( \lambda_{1,5} I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3
    + b_5 \right) + b_6 \bigr) \]
  prefs: []
  type: TYPE_NORMAL
- en: where the activation functions \(\sigma_{H_4}\) = \(\sigma_{H_5}\) = \(\sigma\)
    are sigmoid, and \(\sigma_{O_6}\) is linear (identity), so we could simplify the
    forward pass to,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_6 = \lambda_{4,6} \cdot \sigma \left( \lambda_{1,4} I_1 + \lambda_{2,4}
    I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot \sigma \left( \lambda_{1,5}
    I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3 + b_5 \right) + b_6 \]
  prefs: []
  type: TYPE_NORMAL
- en: This emphasizes that our neural network is a nested set of activated linear
    systems, i.e., linearly weighted averages plus bias terms applied to activation
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Number of Model Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, there are many model parameters, \(theta\), in an artificial neural
    network. First, let‚Äôs clarify these definitions to describe our artificial neural
    network,
  prefs: []
  type: TYPE_NORMAL
- en: '**neural network width** - the number of nodes in the layers of the neural
    network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**neural network depth** - the number of layers in the neural network, typically
    the input layer is not included in this calculation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let‚Äôs assume the following compact notation for a 3 layer artificial neural
    network, input, output and 1 hidden layer, with the width of each layer as,
  prefs: []
  type: TYPE_NORMAL
- en: number of input nodes, \(p\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: number of hidden layer nodes, \(m\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and number of output nodes, \(k\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/1b5b7051a7f760fb3b71c71560ca7613.png)'
  prefs: []
  type: TYPE_IMG
- en: Notation for artificial neural network width, number of input nodes, \(p\),
    number of hidden layer nodes, \(m\), and number of output nodes, \(k\).
  prefs: []
  type: TYPE_NORMAL
- en: fully connected, so for every connection there is a weight,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{ùêº_{1,\ldots,ùëù},ùêª_{1,\ldots,ùëö} } \quad \text{and} \quad \lambda_{ùêª_{1,\ldots,ùëö},ùëÇ_{1,\ldots,ùëò}
    } \]
  prefs: []
  type: TYPE_NORMAL
- en: with full connectivity the number of weights is
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëù \times ùëö \quad \text{and} \quad ùëö \times ùëò \]
  prefs: []
  type: TYPE_NORMAL
- en: and at each hidden layer node there is a bias term,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëè_{H_{1,\ldots,m} } \]
  prefs: []
  type: TYPE_NORMAL
- en: and at every output node there is a bias term,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëè_{O_{1,\ldots,k} } \]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the number of model parameters is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta| = ùëù \times ùëö + ùëö \times ùëò + ùëö + ùëò \]
  prefs: []
  type: TYPE_NORMAL
- en: this assumes an unique bias term at each hidden layer node and output layer
    node, but in some case the same bias term may be applied over the entire layer.
  prefs: []
  type: TYPE_NORMAL
- en: For our example, with \(p = 3\), \(m = 2\) and \(k = 1\), then the number of
    model parameters are,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta| = ùëù \times ùëö + ùëö \times ùëò + ùëö + ùëò \]
  prefs: []
  type: TYPE_NORMAL
- en: after substitution we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta| = 3 \times 2 + 2 \times 1 + 2 + 1 = 11 \]
  prefs: []
  type: TYPE_NORMAL
- en: I select this as a manageable number of parameters, so we can train and visualize
    our model, but consider a more typical model size by increasing our artificial
    neural network‚Äôs width, with \(p = 10\), \(m = 20\) and \(k = 3\), then we have
    many more model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta| = 10 \times 20 + 20 \times 3 + 20 + 3 = 283 \]
  prefs: []
  type: TYPE_NORMAL
- en: If we add hidden layers, increase our artificial neural network‚Äôs depth, the
    number of model parameters will grow very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: we can generalize this calculation for any fully connected, feed forward neural
    network, given a \(W\) vector with the number of nodes, i.e., the width of each
    layer,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \mathbf{L} = [l_0, l_1, l_2, \dots, l_n] \]
  prefs: []
  type: TYPE_NORMAL
- en: where,
  prefs: []
  type: TYPE_NORMAL
- en: \(l_0\) is the number of input neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(l_1, \dots, l_{n-1}\) are the widths of the hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(l_n\) is the number of output neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total number of connection weights is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta_{weights}| = \sum_{i=1}^{n} l_i \cdot l_{i-1} \]
  prefs: []
  type: TYPE_NORMAL
- en: the total number of node biases (there are not bias parameters in the input
    layer nodes, \(l_0\)) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta_{biases}| = \sum_{i=1}^{n} l_i \]
  prefs: []
  type: TYPE_NORMAL
- en: the total number of trainable model parameters, connectioned weights and node
    biases, is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta| = \sum_{i=1}^{n} \left( l_i \cdot l_{i-1} + l_i \right) = \sum_{i=1}^{n}
    l_i \cdot (l_{i-1} + 1) \]
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs take an example of artificial neural network with 4 hidden layers, with
    network width by-layer vector of,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{L} = [10, 8, 6, 4, 2, 1] \]
  prefs: []
  type: TYPE_NORMAL
- en: The total number of connection weights is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta_{L_{weights}}| = \sum_{i=1}^{5} l_i \cdot l_{i-1} = (8 \cdot 10)
    + (6 \cdot 8) + (4 \cdot 6) + (2 \cdot 4) + (1 \cdot 2) = 80 + 48 + 24 + 8 + 2
    = 162 \]
  prefs: []
  type: TYPE_NORMAL
- en: and the total number of node biases is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta_{L_{biases}}| = \sum_{i=1}^{5} l_i = 8 + 6 + 4 + 2 + 1 = 21 \]
  prefs: []
  type: TYPE_NORMAL
- en: and finally the total nuber of trainable parameters is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\theta_L| = \sum_{i=1}^{5} l_i \cdot (l_{i-1} + 1) = (8 \cdot 11) + (6 \cdot
    9) + (4 \cdot 7) + (2 \cdot 5) + (1 \cdot 3) = 88 + 54 + 28 + 10 + 3 = 183 \]
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The activation function is a transformation of the linear combination of the
    weighted node inputs plus the node bias term. Nonlinear activation,
  prefs: []
  type: TYPE_NORMAL
- en: introduces non-linear properties, and complexity to the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prevents the network from collapsing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without the nonlinear activation function we would have linear regression, the
    entire system collapses.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about activation functions and a demonstration of the collapse
    without nonlinear activation to multilinear regression see the associated chapter
    in this e-book, [Neural Network Activation Functions](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_activation_functions.html).
  prefs: []
  type: TYPE_NORMAL
- en: Training Networks Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training an artificial neural network proceeds iteratively by these steps,
  prefs: []
  type: TYPE_NORMAL
- en: initialized the model parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: forward pass to make a prediction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: calculate the error derivative based on the prediction and truth over training
    data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: backpropagate the error derivative back through the artificial neural network
    to calculate the derivatives of the error over all the model weights and biases
    parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: update the model parameters based on the derivatives and learning rates
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: repeat until convergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/c3a5bc8956f8ceda05ddf9b582cd141d.png)'
  prefs: []
  type: TYPE_IMG
- en: The iterative steps for training an artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs some details on each step,
  prefs: []
  type: TYPE_NORMAL
- en: '**Initializing the Model Parameters** - initialize all model parameters with
    typically small (near zero) random values. Here‚Äôs a couple common methods,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Xavier Weight Initialization** - random realizations from uniform distributions
    specified by \(U[\text{min}, \text{max}]\),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}}, \frac{1}{\sqrt{p}} \right]
    (p^\ell) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(F^{-1}_U\) is the inverse of the CDF, \(p\) is the number of inputs,
    and \(p^{\ell}\) is a random cumulative probability value drawn from the uniform
    distribution, \(U[0,1]\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalized Xavier Weight Initialization** - random realizations from uniform
    distributions specified by \(U[\text{min}, \text{max}]\),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k}
    \right] (p^\ell) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(F^{-1}_U\) is the inverse of the CDF, \(p\) is the number of inputs,
    \(k\) is the number of outputs, and \(p^{\ell}\) is a random cumulative probability
    value drawn from the uniform distribution, \(U[0,1]\).
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we return to our first hidden layer node,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2f8e46ea497049f4b95c03b8812eea7.png)'
  prefs: []
  type: TYPE_IMG
- en: First hidden layer node with 3 inputs, and 1 output.
  prefs: []
  type: TYPE_NORMAL
- en: we have \(p = 3\) and \(k = 1\), and we draw from the uniform distribution,
  prefs: []
  type: TYPE_NORMAL
- en: \[ U \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] = U \left[ \frac{-1}{\sqrt{3}+1},
    \frac{1}{\sqrt{3}+1} \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward Pass** - to make a prediction, \(\hat{y}\). Initial predictions will
    be random for the first iteration, but will improve over iterations. Once again
    for our model the forward pass is,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ O_6 = \lambda_{4,6} \cdot \sigma \left( \lambda_{1,4} I_1 + \lambda_{2,4}
    I_2 + \lambda_{3,4} I_3 + b_4 \right) + \lambda_{5,6} \cdot \sigma \left( \lambda_{1,5}
    I_1 + \lambda_{2,5} I_2 + \lambda_{3,5} I_3 + b_5 \right) + b_6 \]![](../Images/08556ecbd47d143019d0163dc95761cf.png)
  prefs: []
  type: TYPE_NORMAL
- en: Prediction with our artificial neural network initialized with random model
    parameters, weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculate the Error Derivative** - given a loss of,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ L = \frac{1}{2} \left(\hat{y} - y \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and the error derivative, i.e., rate of change of in error given a change in
    model estimate is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial \hat{y}} = \hat{Y} - Y \]
  prefs: []
  type: TYPE_NORMAL
- en: For now, let‚Äôs only consider a single estimate, and we will address more than
    1 training data later.
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagate the Error Derivative** - we shift back through the artificial
    neural network to calculate the derivatives of the error over all the model weights
    and biases parameters, with the chain rule, for example the loss derivative backpropagated
    to the output of node \(H_4\),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    L}{\partial O_6} = \lambda_{4,6} \cdot \bigl( (1 - O_6) \cdot O_6 \bigr) \cdot
    (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Update the Model Parameters** - based on the derivatives, \(\frac{\partial
    L}{\partial \lambda_{i,j}}\) and learning rates, \(\eta\), like this,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \lambda_{i,j}^{\ell} = \lambda_{i,j}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{i,j}} \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Repeat Until Convergence** - return to step 1, until the error, \(L\), is
    reduced to an acceptable level, i.e., model convergence is the condition to stop
    the iterations'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are the steps, now let‚Äôs dive into the details for each, but first let‚Äôs
    start with the mathematical framework for backpropagation - the chain rule.
  prefs: []
  type: TYPE_NORMAL
- en: The Chain Rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Upon reflection, it is clear that the forward pass through our artificial neural
    network involves a sequence of nested operations that progressively transform
    the input signals as they propagate from the input nodes, through each layer,
    to the output nodes.
  prefs: []
  type: TYPE_NORMAL
- en: So we can represent this as a sequence of nested operations,
  prefs: []
  type: TYPE_NORMAL
- en: \[ f = f(x) \quad g = g(f) \quad y = h(g) \]
  prefs: []
  type: TYPE_NORMAL
- en: and now in this form to emphasize the nesting of operations,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = h \bigl( g(f(x)) \bigr) \]
  prefs: []
  type: TYPE_NORMAL
- en: By applying the chain rule to the nested functions \(y = h \bigl( g(f(x)) \bigr)\),
    we can solve for \(\frac{\partial y}{\partial x}\) as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial y}{\partial x} = \frac{\partial h}{\partial g} \cdot \frac{\partial
    g}{\partial f} \cdot \frac{\partial f}{\partial x} \]
  prefs: []
  type: TYPE_NORMAL
- en: where we chain together the partial derivatives for all the operators to solve
    derivative of the output, \(y\), given the input, \(x\).
  prefs: []
  type: TYPE_NORMAL
- en: we can compute derivatives at any intermediate point in the nested functions,
    for example, stepping backwards one step,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \frac{\partial f}{\partial x} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now two steps,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial g}{\partial x} = \frac{\partial g}{\partial f} \cdot \frac{\partial
    f}{\partial x} \]
  prefs: []
  type: TYPE_NORMAL
- en: and all the way with three steps,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial y}{\partial x} = \frac{\partial h}{\partial g} \cdot \frac{\partial
    g}{\partial f} \cdot \frac{\partial f}{\partial x} \]
  prefs: []
  type: TYPE_NORMAL
- en: This is what we do with backpropagation, but this may be too abstract! Let‚Äôs
    move to a very simple feed forward neural network with only these three nodes,
  prefs: []
  type: TYPE_NORMAL
- en: \(I_1\) - input node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(H_2 = h(I_1)\) - hidden layer node, a function of \(I_1\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(O_3 = o(H_2)\) - output node, a function of \(H_2\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this is still intentionally abstract, i.e., without mention of weights and biases,
    to help you develop a mental framework of backpropagation with neural netowrks
    by the chain rule, we will dive into the details immediately after this discussion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output \(O_3\) depends on the input \(I_1\) through these nested functions:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_3 = o \bigl( h(I_1) \bigr) \]
  prefs: []
  type: TYPE_NORMAL
- en: Using the **chain rule**, the gradient of the output with respect to backpropagating
    one step,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_3}{\partial H_2} \]
  prefs: []
  type: TYPE_NORMAL
- en: and with respect to backpropagating two steps,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_3}{\partial I_1} = \frac{\partial H_2}{\partial I_1} \cdot
    \frac{\partial O_3}{\partial H_2} \]
  prefs: []
  type: TYPE_NORMAL
- en: This shows how the gradient **backpropagates** through the network,
  prefs: []
  type: TYPE_NORMAL
- en: \(\frac{\partial O_3}{\partial I_1}\) - is the local gradient at the hidden
    node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\frac{\partial O_3}{\partial H_2}\) - is the local gradient at the output
    node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By backpropagation we can calculate the deriviates with respect to all parts
    of the network, how the input node signal \(I_1\), or hidden nodel signal \(H_2\)
    affect the output \(O_3\), \(\frac{\partial O_3}{\partial I_1}\) and \(frac{\partial
    O_3}{\partial H_2}\) respsectively.
  prefs: []
  type: TYPE_NORMAL
- en: and more importantly, how changes in the input \(I_1\), or \(H_2\) affect the
    change in model loss, \(\frac{\partial L}{\partial I_1}\) and \(frac{\partial
    L}{\partial H_2}\) respsectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chain of partial derivatives, move backwards step by step through the neural
    network layers, is the fundamental mechanism behind **backpropagation**. Next
    we will derive and demonstrate each of the parts of backpropagation and then finally
    put this together to show backpropagation over our entire network.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks Backpropagation Building Blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs cover the numerical building blocks for backpropagation. Once you understand
    these backpropagation building blocks, you will be able to backpropagate our simple
    network and even any complicated artificial neural networks by hand,
  prefs: []
  type: TYPE_NORMAL
- en: calculating the loss derivative
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: backpropagation through nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: backpropagation along connections
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: accounting for multiple paths
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: loss derivatives with respect to weights and biases
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For now I demonstrate backpropagation of this loss derivative for a single training
    data sample, \(y\).
  prefs: []
  type: TYPE_NORMAL
- en: I address multiple samples later, \(y_i, i=1, \ldots, n\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs start with calculating the loss derivative.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the Loss Derivative
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Backpropagation is based on the concept of allocating or propagating the loss
    derivative backwards through the neural network,
  prefs: []
  type: TYPE_NORMAL
- en: we calculate the loss derivative and then distribute it sequentially, in reverse
    direction, from network output back towards the network input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is important to know that we are working with derivatives, and that backpropagation
    is NOT distributing error, although as you will see it may look that way!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We start by defining the loss, given the truth, \(ùë¶\), and our prediction, \(\hat{y}
    = O_6\), we calculate our \(L^2\) loss as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ L = \frac{1}{2} \left( \hat{y} - y \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: our choice of loss function allows us to use the prediction error as the loss
    derivative! We calculate the loss derivative as the partial derivative of the
    loss with respect to the estimate, \(\frac{\partial ùêø}{\partial \hat{y}}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial \hat{y}} = \frac{\partial \frac{1}{2}
    \left( \hat{y} - y \right)^2 }{\partial \hat{y}} = \hat{y} - y \]
  prefs: []
  type: TYPE_NORMAL
- en: You see what I mean, we are backpropagating the loss derivative, but due to
    our formulation of the \(L^2\) loss, we only have to calculate the error at our
    output node output, but once again - it is the loss derivative.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6abf780fa4544caa6735a8f4ad075bd2.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculation of the loss derivative at the output of an output layer node, $O_6$.
  prefs: []
  type: TYPE_NORMAL
- en: For the example of our simple artificial neural network with the output at node,
    \(O_6\), our loss derivative is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial O_6} = \frac{\partial \mathcal{L}}{\hat{y}}
    = \hat{y} - y = O_6 - y \]
  prefs: []
  type: TYPE_NORMAL
- en: So this is our loss derivative backpropagated to the output our output node,
    and we are now we are ready to backpropagate this loss derivative through our
    artificial neural network, let‚Äôs talk about how we step through nodes and along
    connections.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through Output Node with Identity Activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs backpropagate through our output node, \(O_6\), from post-activation to
    pre-activation. To do this we need the partial derivative our activation function.
  prefs: []
  type: TYPE_NORMAL
- en: since this is an output node with a regression artificial neural network I have
    selected the identity or linear activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/1141c4e67c550f275e9079501fe522b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative through the node, $O_6$, from $O_6$ post-activation
    output to $O_{6_{in}}$ pre-activation input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The identity activation at output node \(O_6\) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_6 = \sigma(O_{6_{in}}) = O_6 \]
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of the identity activation at node \(O_6\) with respect to its
    input \(O_{6_{in}}\), i.e., crossing node \(O_6\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_6}{\partial O_{6_{in}}} = \frac{\partial \left(O_{6_{in}}
    \right)}{\partial O_{6_{in}}} = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need \(O_6\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative with respect to the node
    output, \(\frac{\partial \mathcal{L}}{\partial O_6}\), and to the loss derivative
    with respect to the node input, \(\frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = \frac{\partial
    O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6}
    = 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have backpropagated through an output node, let‚Äôs backpropagation
    along the \(H_4\) to \(O_6\) connection from the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation along Connections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let‚Äôs backpropagate along the connection between nodes \(O_6\) and \(H_4\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c686c4b3a6fe96317a79c7333542353.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative through node \(O_6\), from \(O_6\) post-activation
    output to $O_{6_{in}}$ pre-activation input and then along the connection to the
    output from node \(H_4\).
  prefs: []
  type: TYPE_NORMAL
- en: Preactivation, the input to node \(ùëÇ_6\) is calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the derivative along the connection as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \frac{\partial}{\partial
    H_4} \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right) =
    \lambda_{4,6} \]
  prefs: []
  type: TYPE_NORMAL
- en: by resolving the above partial derivative, we see that backpropagation along
    a connection by applying the connection weight.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \lambda_{4,6} \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need the current connection weight \(\lambda_{4,6}\). Now we can
    add this to our chain rule to backpropagate along the \(H_4\) to \(O_6\) connection
    from loss derivative with respect to the output layer node input \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\), to the loss derivative with respect
    to the hidden layer node output \(\frac{\partial \mathcal{L}}{\partial H_4}\).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \lambda_{4,6} \bigl( \cdot (1 - O_6) \cdot O_6 \bigr)
    \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through Nodes with Sigmoid Activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs backpropagate through a hidden layer node, \(H_4\), from postactivation
    to preactivation. To do this we need the partial derivative our activation function.
  prefs: []
  type: TYPE_NORMAL
- en: we are assuming sigmoid activation for all hidden layer nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for super clean logic, everyone resolves the activation derivative as a function
    of the output rather than as typical the input,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/10ef488402716c36aa45b497ea54f6bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative through the node, $H_4$, from $H_4$ postactivation
    output to $H_{4_{in}}$ preactivation input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sigmoid activation at output node \(H_4\) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_4 = \sigma(H_{4_{in}}) = \frac{1}{1 + e^{-H_{4_{in}}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of the sigmoid activation at node \(H_4\) with respect to its
    input \(H_{4_{in}}\), i.e., crossing node \(H_4\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + e^{-H_{4_{in}}}} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now, for compact notation let‚Äôs set,
  prefs: []
  type: TYPE_NORMAL
- en: \[ u = e^{-H_{4_{in}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: and substituting we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: and by the chain rule we can extend it to,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) = -\frac{u}{(1 + u)^2} \cdot \frac{\partial u}{\partial
    H_{4_{in}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The derivative of \(u = e^{-H_{4_{in}}}\) with respect to \(H_{4_{in}}\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial u}{\partial H_{4_{in}}} = -e^{-H_{4_{in}}} = -u \]
  prefs: []
  type: TYPE_NORMAL
- en: now we can substitute,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = -\frac{1}{(1+u)^2} \cdot (-u)
    = \frac{u}{(1+u)^2} \]
  prefs: []
  type: TYPE_NORMAL
- en: Express in terms of node \(H_4\) output, \(H_4 = \frac{1}{1 + u}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \frac{\left(1 - H_4\right)/H_4}{\left(1/H_4\right)^2}
    = \frac{1 - H_4}{H_4} \cdot H_4^2 = \left(1 - H_4\right) \cdot H_4 \]
  prefs: []
  type: TYPE_NORMAL
- en: So we can backpropagate through our node, \(H_4\), from node post-activation
    output, \(H_4\) to node pre-activation input, \(H_{4_{in}}\), by,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \left(1 - H_4\right) \cdot
    H_4 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need \(H_4\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative to the output of node
    \(H_4\) and to the input of node \(H_4\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} = \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6}
    \cdot 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now we can handle all cases of backpropagation through the nodes in our network.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation Along Another Connection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For continuity and completeness, let‚Äôs repeat the previously described method
    to backpropagate along the connection \(I_1\) to \(H_4\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76f408b8ed852b03a168e6069bb716db.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative along the connection from $H_4$ to $I_1$.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, preactivation the input to node \(H_4\) is calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{4_{\text{in}}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the derivative along the connection as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \frac{\partial \left(\lambda_{1,4}
    \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 + b_4 \right)}{\partial
    I_1} = \lambda_{1,4} \]
  prefs: []
  type: TYPE_NORMAL
- en: by resolving the above partial derivative, we see that backpropagation along
    a connection by applying the connection weight.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \lambda_{4,6} \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need the current connection weight \(\lambda_{4,6}\). Now we can
    add this to our chain rule to backpropagate along the \(H_4\) to \(O_6\) connection
    from loss derivative with respect to the output layer node input \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\), to the loss derivative with respect
    to the hidden layer node output \(\frac{\partial \mathcal{L}}{\partial H_4}\).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Accounting for Multiple Paths
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our loss derivative with respect to the node output \(I_1\), \(\frac{\partial
    \mathcal{L}}{\partial I_1}\) is not correct!
  prefs: []
  type: TYPE_NORMAL
- en: we accounted for the \(O_6\) to \(H_4\) to \(I_1\) path, but we did not acccount
    for the \(O_6\) to \(H_5\) to \(I_1\) path
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6a68404e4b3d2589dcfe15beb5175c60.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiple paths for backpropagation to input node, $I_1$, from output node $O_6$.
  prefs: []
  type: TYPE_NORMAL
- en: To account for multiple paths we just need to sum over all the paths.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: we can evaluate this as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) + \lambda_{1,5}
    \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \cdot 1.0 \cdot (O_6
    - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: and then simplify by removing the 1.0 values and grouping terms as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now we can evaluate this simplified form as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1 - H_5)
    \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through Input Nodes with Identity Activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs backpropagate through our input node, \(I_1\), from postactivation to
    preactivation. To do this we need the partial derivative our activation function.
  prefs: []
  type: TYPE_NORMAL
- en: since this is an input node I have selected the identity or linear activation
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/bd05208edfb4ac5d5cc0f8cfe808ae50.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative through the node, $I_1$, from $I_1$ postactivation
    output to $I_{1_{in}}$ preactivation input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The identity activation at output node \(I_1\) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ I_1 = \sigma(I_{1_{in}}) = I_1 \]
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of the identity activation at node \(I_1\) with respect to its
    input \(I_{1_{in}}\), i.e., passing through node \(I_1\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial I_1}{\partial I_{1_{in}}} = \frac{\partial \left(I_{1_{in}}
    \right)}{\partial I_{1_{in}}} = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need \(I_1\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative with respect to the node
    output, \(\frac{\partial \mathcal{L}}{\partial I_1}\), and to the loss derivative
    with respect to the node input, \(\frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \frac{\partial I_1}{\partial
    I_{1_{in}}} \cdot \frac{\partial H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} + \frac{\partial I_1}{\partial I_{1_{in}}} \cdot \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial
    O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: we can evaluate this as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = 1.0 \cdot \lambda_{1,4}
    \cdot \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6
    - y) + 1.0 \cdot \lambda_{1,5} \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6}
    \cdot 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: For fun I designed this notation for maximum clarity,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial I_{1_{\text{in}}}} = \overbrace{1.0}^{\textstyle
    \frac{\partial I_{1}}{\partial I_{1_{in}}}} \left[ \overbrace{\lambda_{1,4}}^{\textstyle
    \frac{\partial H_{4_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_4) \cdot
    H_4}^{\textstyle \frac{\partial H_4}{\partial H_{4_{\text{in}}}}} \cdot \overbrace{\lambda_{4,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_4}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} + \overbrace{\lambda_{1,5}}^{\textstyle \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_5) \cdot H_5}^{\textstyle
    \frac{\partial H_5}{\partial H_{5_{\text{in}}}}} \cdot \overbrace{\lambda_{5,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_5}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: But this can be simplified by removing the 1.0 values and grouping terms as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now we can evaluate this simplified form as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \lambda_{1,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: For completeness here is the backpropagation for the other input nodes, here‚Äôs
    \(\frac{\partial \mathcal{L}}{\partial I_{2_{in}}}\),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b186fef88b4e0ad1fa6191394b0e0dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative for input node 2.
  prefs: []
  type: TYPE_NORMAL
- en: For brevity I have remove the 1.0s and grouped like terms,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_2} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_2} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now we can evaluate this simplified form as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \lambda_{2,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{2,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: and here is \(\frac{\partial \mathcal{L}}{\partial I_{3_{in}}}\),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90c6a46319d78ae1b9d4f8cdc131fd2a.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative for input node 3.
  prefs: []
  type: TYPE_NORMAL
- en: For brevity I have remove the 1.0s and grouped like terms,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_3} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_3} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now we can evaluate this simplified form as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \lambda_{3,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{3,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Loss Derivatives with Respect to Weights and Biases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we have back propagated the loss derivative through our network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97de6228871f7910de4d7a8fa86e0bd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagated loss derivatives with respect to all network nodes inputs and
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: and we have the loss derivative with respect to the input and output of each
    node in our network,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial I_1},\quad \frac{\partial \mathcal{L}}{\partial I_{2_{\text{in}}}},\quad
    \frac{\partial \mathcal{L}}{\partial I_2},\quad \frac{\partial \mathcal{L}}{\partial
    I_{3_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial I_3},\quad \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial
    H_4},\quad \frac{\partial \mathcal{L}}{\partial H_5},\quad \frac{\partial \mathcal{L}}{\partial
    H_5},\quad \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: But what we actually need is the loss derivative with respect to each connection
    weights,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}},\quad \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,4}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{1,5}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{2,5}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,5}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{5,6}} \]
  prefs: []
  type: TYPE_NORMAL
- en: and node biases,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial b_4},\quad \frac{\partial \mathcal{L}}{\partial
    b_5},\quad \frac{\partial \mathcal{L}}{\partial b_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: How do we backpropagate the loss derivative to a connection weight? Let‚Äôs start
    with the \(H_4\) to \(O_6\) connection.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf1e650cd4b0613fa09aa5d5c0e72e26.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagated loss derivatives with respect to a connection weight.
  prefs: []
  type: TYPE_NORMAL
- en: Preactivation, input to node \(O_6\) we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the derivative with respect to the connection weight as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial \lambda_{4,6}} = \frac{\partial
    \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial
    \lambda_{4,6}} = H_4 \]
  prefs: []
  type: TYPE_NORMAL
- en: We need the output of the node in the previous layer passed along the connection
    to backpropagate to the loss derivative with respect to the connection weight
    from the input to the next node,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now, for completeness, here are the equations for all of our network‚Äôs connection
    weights.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_5 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: See the pattern, the loss derivatives with respect to connection weights are,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Connection Signal} \times \text{Loss Derivative of Next Node Input}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Now how do we backpropagate the loss derivative to a node bias? Let‚Äôs start
    with the \(O_6\) node.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/646997a228f8c1e8a3b2743ac13e388a.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagated loss derivatives with respect to a node bias.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the preactivation, input to node \(O_6\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the derivative of a connection weight as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial b_6} = \frac{\partial \left( \lambda_{4,6}
    \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial b_6} = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: so our bias loss derivative is equal to the node input loss derivative,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial O_{6_{\text{in}}}}{\partial
    b_6} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = 1.0 \cdot
    \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: For completeness here are all the loss derivatives with respect to node biases,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial \mathcal{L}}{\partial
    O_{6_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial b_4} = \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial
    b_5} = \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: See the pattern, the loss derivatives with respect to node biases are,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Loss Derivative of the Node Input} \]
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the Loss Derivative
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Backpropagation is based on the concept of allocating or propagating the loss
    derivative backwards through the neural network,
  prefs: []
  type: TYPE_NORMAL
- en: we calculate the loss derivative and then distribute it sequentially, in reverse
    direction, from network output back towards the network input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is important to know that we are working with derivatives, and that backpropagation
    is NOT distributing error, although as you will see it may look that way!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We start by defining the loss, given the truth, \(ùë¶\), and our prediction, \(\hat{y}
    = O_6\), we calculate our \(L^2\) loss as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ L = \frac{1}{2} \left( \hat{y} - y \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: our choice of loss function allows us to use the prediction error as the loss
    derivative! We calculate the loss derivative as the partial derivative of the
    loss with respect to the estimate, \(\frac{\partial ùêø}{\partial \hat{y}}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial \hat{y}} = \frac{\partial \frac{1}{2}
    \left( \hat{y} - y \right)^2 }{\partial \hat{y}} = \hat{y} - y \]
  prefs: []
  type: TYPE_NORMAL
- en: You see what I mean, we are backpropagating the loss derivative, but due to
    our formulation of the \(L^2\) loss, we only have to calculate the error at our
    output node output, but once again - it is the loss derivative.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6abf780fa4544caa6735a8f4ad075bd2.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculation of the loss derivative at the output of an output layer node, $O_6$.
  prefs: []
  type: TYPE_NORMAL
- en: For the example of our simple artificial neural network with the output at node,
    \(O_6\), our loss derivative is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial O_6} = \frac{\partial \mathcal{L}}{\hat{y}}
    = \hat{y} - y = O_6 - y \]
  prefs: []
  type: TYPE_NORMAL
- en: So this is our loss derivative backpropagated to the output our output node,
    and we are now we are ready to backpropagate this loss derivative through our
    artificial neural network, let‚Äôs talk about how we step through nodes and along
    connections.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through Output Node with Identity Activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs backpropagate through our output node, \(O_6\), from post-activation to
    pre-activation. To do this we need the partial derivative our activation function.
  prefs: []
  type: TYPE_NORMAL
- en: since this is an output node with a regression artificial neural network I have
    selected the identity or linear activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/1141c4e67c550f275e9079501fe522b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative through the node, $O_6$, from $O_6$ post-activation
    output to $O_{6_{in}}$ pre-activation input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The identity activation at output node \(O_6\) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_6 = \sigma(O_{6_{in}}) = O_6 \]
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of the identity activation at node \(O_6\) with respect to its
    input \(O_{6_{in}}\), i.e., crossing node \(O_6\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_6}{\partial O_{6_{in}}} = \frac{\partial \left(O_{6_{in}}
    \right)}{\partial O_{6_{in}}} = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need \(O_6\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative with respect to the node
    output, \(\frac{\partial \mathcal{L}}{\partial O_6}\), and to the loss derivative
    with respect to the node input, \(\frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = \frac{\partial
    O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6}
    = 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have backpropagated through an output node, let‚Äôs backpropagation
    along the \(H_4\) to \(O_6\) connection from the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation along Connections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let‚Äôs backpropagate along the connection between nodes \(O_6\) and \(H_4\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c686c4b3a6fe96317a79c7333542353.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative through node \(O_6\), from \(O_6\) post-activation
    output to $O_{6_{in}}$ pre-activation input and then along the connection to the
    output from node \(H_4\).
  prefs: []
  type: TYPE_NORMAL
- en: Preactivation, the input to node \(ùëÇ_6\) is calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the derivative along the connection as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \frac{\partial}{\partial
    H_4} \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right) =
    \lambda_{4,6} \]
  prefs: []
  type: TYPE_NORMAL
- en: by resolving the above partial derivative, we see that backpropagation along
    a connection by applying the connection weight.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial H_4} = \lambda_{4,6} \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need the current connection weight \(\lambda_{4,6}\). Now we can
    add this to our chain rule to backpropagate along the \(H_4\) to \(O_6\) connection
    from loss derivative with respect to the output layer node input \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\), to the loss derivative with respect
    to the hidden layer node output \(\frac{\partial \mathcal{L}}{\partial H_4}\).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial H_4} = \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \lambda_{4,6} \bigl( \cdot (1 - O_6) \cdot O_6 \bigr)
    \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through Nodes with Sigmoid Activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs backpropagate through a hidden layer node, \(H_4\), from postactivation
    to preactivation. To do this we need the partial derivative our activation function.
  prefs: []
  type: TYPE_NORMAL
- en: we are assuming sigmoid activation for all hidden layer nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for super clean logic, everyone resolves the activation derivative as a function
    of the output rather than as typical the input,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/10ef488402716c36aa45b497ea54f6bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative through the node, $H_4$, from $H_4$ postactivation
    output to $H_{4_{in}}$ preactivation input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sigmoid activation at output node \(H_4\) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_4 = \sigma(H_{4_{in}}) = \frac{1}{1 + e^{-H_{4_{in}}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of the sigmoid activation at node \(H_4\) with respect to its
    input \(H_{4_{in}}\), i.e., crossing node \(H_4\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + e^{-H_{4_{in}}}} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now, for compact notation let‚Äôs set,
  prefs: []
  type: TYPE_NORMAL
- en: \[ u = e^{-H_{4_{in}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: and substituting we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: and by the chain rule we can extend it to,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = \frac{\partial}{\partial H_{4_{in}}}
    \left( \frac{1}{1 + u} \right) = -\frac{u}{(1 + u)^2} \cdot \frac{\partial u}{\partial
    H_{4_{in}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The derivative of \(u = e^{-H_{4_{in}}}\) with respect to \(H_{4_{in}}\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial u}{\partial H_{4_{in}}} = -e^{-H_{4_{in}}} = -u \]
  prefs: []
  type: TYPE_NORMAL
- en: now we can substitute,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{in}}} = -\frac{1}{(1+u)^2} \cdot (-u)
    = \frac{u}{(1+u)^2} \]
  prefs: []
  type: TYPE_NORMAL
- en: Express in terms of node \(H_4\) output, \(H_4 = \frac{1}{1 + u}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \frac{\left(1 - H_4\right)/H_4}{\left(1/H_4\right)^2}
    = \frac{1 - H_4}{H_4} \cdot H_4^2 = \left(1 - H_4\right) \cdot H_4 \]
  prefs: []
  type: TYPE_NORMAL
- en: So we can backpropagate through our node, \(H_4\), from node post-activation
    output, \(H_4\) to node pre-activation input, \(H_{4_{in}}\), by,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_4}{\partial H_{4_{\text{in}}}} = \left(1 - H_4\right) \cdot
    H_4 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need \(H_4\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative to the output of node
    \(H_4\) and to the input of node \(H_4\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} = \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} = \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6}
    \cdot 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now we can handle all cases of backpropagation through the nodes in our network.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation Along Another Connection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For continuity and completeness, let‚Äôs repeat the previously described method
    to backpropagate along the connection \(I_1\) to \(H_4\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76f408b8ed852b03a168e6069bb716db.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative along the connection from $H_4$ to $I_1$.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, preactivation the input to node \(H_4\) is calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ H_{4_{\text{in}}} = \lambda_{1,4} \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4}
    \cdot I_3 + b_4 \]
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the derivative along the connection as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \frac{\partial \left(\lambda_{1,4}
    \cdot I_1 + \lambda_{2,4} \cdot I_2 + \lambda_{3,4} \cdot I_3 + b_4 \right)}{\partial
    I_1} = \lambda_{1,4} \]
  prefs: []
  type: TYPE_NORMAL
- en: by resolving the above partial derivative, we see that backpropagation along
    a connection by applying the connection weight.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial H_{4_{\text{in}}}}{\partial I_1} = \lambda_{4,6} \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need the current connection weight \(\lambda_{4,6}\). Now we can
    add this to our chain rule to backpropagate along the \(H_4\) to \(O_6\) connection
    from loss derivative with respect to the output layer node input \(\frac{\partial
    \mathcal{L}}{\partial O_{6_{\text{in}}}}\), to the loss derivative with respect
    to the hidden layer node output \(\frac{\partial \mathcal{L}}{\partial H_4}\).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Accounting for Multiple Paths
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our loss derivative with respect to the node output \(I_1\), \(\frac{\partial
    \mathcal{L}}{\partial I_1}\) is not correct!
  prefs: []
  type: TYPE_NORMAL
- en: we accounted for the \(O_6\) to \(H_4\) to \(I_1\) path, but we did not acccount
    for the \(O_6\) to \(H_5\) to \(I_1\) path
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6a68404e4b3d2589dcfe15beb5175c60.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiple paths for backpropagation to input node, $I_1$, from output node $O_6$.
  prefs: []
  type: TYPE_NORMAL
- en: To account for multiple paths we just need to sum over all the paths.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}}
    \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: we can evaluate this as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6 - y) + \lambda_{1,5}
    \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \cdot 1.0 \cdot (O_6
    - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: and then simplify by removing the 1.0 values and grouping terms as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \frac{\partial H_{4_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now we can evaluate this simplified form as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_1} = \left[ \lambda_{1,4} \cdot \bigl((1
    - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1 - H_5)
    \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through Input Nodes with Identity Activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs backpropagate through our input node, \(I_1\), from postactivation to
    preactivation. To do this we need the partial derivative our activation function.
  prefs: []
  type: TYPE_NORMAL
- en: since this is an input node I have selected the identity or linear activation
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/bd05208edfb4ac5d5cc0f8cfe808ae50.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative through the node, $I_1$, from $I_1$ postactivation
    output to $I_{1_{in}}$ preactivation input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The identity activation at output node \(I_1\) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ I_1 = \sigma(I_{1_{in}}) = I_1 \]
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of the identity activation at node \(I_1\) with respect to its
    input \(I_{1_{in}}\), i.e., passing through node \(I_1\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial I_1}{\partial I_{1_{in}}} = \frac{\partial \left(I_{1_{in}}
    \right)}{\partial I_{1_{in}}} = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, we just need \(I_1\) the output signal from the node. Now we can add this
    to our chain rule to backpropagate from loss derivative with respect to the node
    output, \(\frac{\partial \mathcal{L}}{\partial I_1}\), and to the loss derivative
    with respect to the node input, \(\frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \frac{\partial I_1}{\partial
    I_{1_{in}}} \cdot \frac{\partial H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial
    H_4}{\partial H_{4_{\text{in}}}} \cdot \frac{\partial O_{6_{\text{in}}}}{\partial
    H_4} \cdot \frac{\partial O_6}{\partial O_{6_{\text{in}}}} \cdot \frac{\partial
    \mathcal{L}}{\partial O_6} + \frac{\partial I_1}{\partial I_{1_{in}}} \cdot \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_5} \cdot \frac{\partial O_6}{\partial
    O_{6_{\text{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: we can evaluate this as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = 1.0 \cdot \lambda_{1,4}
    \cdot \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} \cdot 1.0 \cdot (O_6
    - y) + 1.0 \cdot \lambda_{1,5} \cdot \bigl((1 - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6}
    \cdot 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: For fun I designed this notation for maximum clarity,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial I_{1_{\text{in}}}} = \overbrace{1.0}^{\textstyle
    \frac{\partial I_{1}}{\partial I_{1_{in}}}} \left[ \overbrace{\lambda_{1,4}}^{\textstyle
    \frac{\partial H_{4_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_4) \cdot
    H_4}^{\textstyle \frac{\partial H_4}{\partial H_{4_{\text{in}}}}} \cdot \overbrace{\lambda_{4,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_4}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} + \overbrace{\lambda_{1,5}}^{\textstyle \frac{\partial
    H_{5_{\text{in}}}}{\partial I_1}} \cdot \overbrace{(1 - H_5) \cdot H_5}^{\textstyle
    \frac{\partial H_5}{\partial H_{5_{\text{in}}}}} \cdot \overbrace{\lambda_{5,6}}^{\textstyle
    \frac{\partial O_{6_{\text{in}}}}{\partial H_5}} \cdot \overbrace{1.0}^{\textstyle
    \frac{\partial O_{6}}{\partial O_{6_{in}}}} \cdot \overbrace{(O_6 - Y_1^N)}^{\textstyle
    \frac{\partial L}{\partial O_{6}}} \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: But this can be simplified by removing the 1.0 values and grouping terms as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_1} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_1} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now we can evaluate this simplified form as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{in}}} = \left[ \lambda_{1,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{1,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: For completeness here is the backpropagation for the other input nodes, here‚Äôs
    \(\frac{\partial \mathcal{L}}{\partial I_{2_{in}}}\),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b186fef88b4e0ad1fa6191394b0e0dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative for input node 2.
  prefs: []
  type: TYPE_NORMAL
- en: For brevity I have remove the 1.0s and grouped like terms,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_2} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_2} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now we can evaluate this simplified form as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{2_{in}}} = \left[ \lambda_{2,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{2,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: and here is \(\frac{\partial \mathcal{L}}{\partial I_{3_{in}}}\),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90c6a46319d78ae1b9d4f8cdc131fd2a.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of the loss derivative for input node 3.
  prefs: []
  type: TYPE_NORMAL
- en: For brevity I have remove the 1.0s and grouped like terms,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \frac{\partial
    H_{4_{\text{in}}}}{\partial I_3} \cdot \frac{\partial H_4}{\partial H_{4_{\text{in}}}}
    \cdot \frac{\partial O_{6_{\text{in}}}}{\partial H_4} + \frac{\partial H_{5_{\text{in}}}}{\partial
    I_3} \cdot \frac{\partial H_5}{\partial H_{5_{\text{in}}}} \cdot \frac{\partial
    O_{6_{\text{in}}}}{\partial H_5} \right] \cdot \frac{\partial \mathcal{L}}{\partial
    O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: and now we can evaluate this simplified form as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{3_{in}}} = \left[ \lambda_{3,4} \cdot
    \bigl((1 - H_4) \cdot H_4 \bigr) \cdot \lambda_{4,6} + \lambda_{3,5} \cdot \bigl((1
    - H_5) \cdot H_5 \bigr) \cdot \lambda_{5,6} \right] \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Loss Derivatives with Respect to Weights and Biases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we have back propagated the loss derivative through our network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97de6228871f7910de4d7a8fa86e0bd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagated loss derivatives with respect to all network nodes inputs and
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: and we have the loss derivative with respect to the input and output of each
    node in our network,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial I_{1_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial I_1},\quad \frac{\partial \mathcal{L}}{\partial I_{2_{\text{in}}}},\quad
    \frac{\partial \mathcal{L}}{\partial I_2},\quad \frac{\partial \mathcal{L}}{\partial
    I_{3_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial I_3},\quad \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}},\quad \frac{\partial \mathcal{L}}{\partial
    H_4},\quad \frac{\partial \mathcal{L}}{\partial H_5},\quad \frac{\partial \mathcal{L}}{\partial
    H_5},\quad \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}},\quad \frac{\partial
    \mathcal{L}}{\partial O_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: But what we actually need is the loss derivative with respect to each connection
    weights,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}},\quad \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,4}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{1,5}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{2,5}},\quad \frac{\partial \mathcal{L}}{\partial
    \lambda_{3,5}},\quad \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}},\quad
    \frac{\partial \mathcal{L}}{\partial \lambda_{5,6}} \]
  prefs: []
  type: TYPE_NORMAL
- en: and node biases,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial b_4},\quad \frac{\partial \mathcal{L}}{\partial
    b_5},\quad \frac{\partial \mathcal{L}}{\partial b_6} \]
  prefs: []
  type: TYPE_NORMAL
- en: How do we backpropagate the loss derivative to a connection weight? Let‚Äôs start
    with the \(H_4\) to \(O_6\) connection.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf1e650cd4b0613fa09aa5d5c0e72e26.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagated loss derivatives with respect to a connection weight.
  prefs: []
  type: TYPE_NORMAL
- en: Preactivation, input to node \(O_6\) we have,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the derivative with respect to the connection weight as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial \lambda_{4,6}} = \frac{\partial
    \left( \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial
    \lambda_{4,6}} = H_4 \]
  prefs: []
  type: TYPE_NORMAL
- en: We need the output of the node in the previous layer passed along the connection
    to backpropagate to the loss derivative with respect to the connection weight
    from the input to the next node,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot 1.0 \cdot (O_6 - y) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now, for completeness, here are the equations for all of our network‚Äôs connection
    weights.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_1 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_2 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} =
    I_3 \cdot \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_4 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]\[ \frac{\partial
    \mathcal{L}}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} =
    H_5 \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: See the pattern, the loss derivatives with respect to connection weights are,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Connection Signal} \times \text{Loss Derivative of Next Node Input}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Now how do we backpropagate the loss derivative to a node bias? Let‚Äôs start
    with the \(O_6\) node.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/646997a228f8c1e8a3b2743ac13e388a.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagated loss derivatives with respect to a node bias.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the preactivation, input to node \(O_6\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ O_{6_{\text{in}}} = \lambda_{4,6} \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the derivative of a connection weight as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial O_{6_{\text{in}}}}{\partial b_6} = \frac{\partial \left( \lambda_{4,6}
    \cdot H_4 + \lambda_{5,6} \cdot H_5 + b_6 \right)}{\partial b_6} = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: so our bias loss derivative is equal to the node input loss derivative,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial O_{6_{\text{in}}}}{\partial
    b_6} \cdot \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} = 1.0 \cdot
    \frac{\partial \mathcal{L}}{\partial O_{6_{\text{in}}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: For completeness here are all the loss derivatives with respect to node biases,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial \mathcal{L}}{\partial
    O_{6_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial b_4} = \frac{\partial
    \mathcal{L}}{\partial H_{4_{\text{in}}}} \]\[ \frac{\partial \mathcal{L}}{\partial
    b_5} = \frac{\partial \mathcal{L}}{\partial H_{5_{\text{in}}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: See the pattern, the loss derivatives with respect to node biases are,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Loss Derivative of the Node Input} \]
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs take the backpropagation method explained above and apply them to my interactive
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs the result for our first training epoch with only 1 sample,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b8b7d56b334d9728490cb2bc1408535e.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation result for the first iteration.
  prefs: []
  type: TYPE_NORMAL
- en: My interactive dashboard provides all the loss derivatives with respect to the
    input for each node and the output signals from each node, so for example we can
    calculate \(\frac{\partial L}{\partial \lambda_{4,6}}\) as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial \lambda_{4,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{4,6}} \cdot \frac{\partial L}{\partial O_{6_{\text{in}}}} = H_4 \cdot
    \frac{\partial L}{\partial O_{6_{\text{in}}}} = 0.42 \cdot 1.00 = 0.42 \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs the loss derivatives with respect to connection weights for the other
    hidden layer to output node connection,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial \lambda_{5,6}} = \frac{\partial O_{6_{\text{in}}}}{\partial
    \lambda_{5,6}} \cdot \frac{\partial L}{\partial O_{6_{\text{in}}}} = H_5 \cdot
    \frac{\partial L}{\partial O_{6_{\text{in}}}} = 0.60 \cdot 1.00 = 0.60 \]
  prefs: []
  type: TYPE_NORMAL
- en: and now let‚Äôs get all the input to hidden layer connections,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial \lambda_{1,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{1,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_1 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.50 \cdot (-0.13) = -0.07 \]\[
    \frac{\partial L}{\partial \lambda_{1,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{1,5}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_1 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.50 \cdot (-0.10) = -0.05 \]\[
    \frac{\partial L}{\partial \lambda_{2,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{2,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_2 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.20 \cdot (-0.13) = -0.03 \]\[
    \frac{\partial L}{\partial \lambda_{2,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{2,5}} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = I_2 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.20 \cdot (-0.10) = -0.02 \]\[
    \frac{\partial L}{\partial \lambda_{3,4}} = \frac{\partial H_{4_{\text{in}}}}{\partial
    \lambda_{3,4}} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = I_3 \cdot
    \frac{\partial L}{\partial H_{4_{\text{in}}}} = 0.70 \cdot (-0.13) = -0.09 \]\[
    \frac{\partial L}{\partial \lambda_{3,5}} = \frac{\partial H_{5_{\text{in}}}}{\partial
    \lambda_{3,5}} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = I_3 \cdot
    \frac{\partial L}{\partial H_{5_{\text{in}}}} = 0.70 \cdot (-0.10) = -0.07 \]
  prefs: []
  type: TYPE_NORMAL
- en: This takes care of all of the connection weight error derivatives, now lets
    take care of the node bias error derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: the node bias error derivatives are the same as the node peractivation error
    derivatives. Now let‚Äôs calculate the bias terms in the hidden layer,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial b_4} = \frac{\partial H_{4_{\text{in}}}}{\partial
    b_4} \cdot \frac{\partial L}{\partial H_{4_{\text{in}}}} = 1.0 \cdot (-0.13) =
    -0.13 \]\[ \frac{\partial L}{\partial b_5} = \frac{\partial H_{5_{\text{in}}}}{\partial
    b_5} \cdot \frac{\partial L}{\partial H_{5_{\text{in}}}} = 1.0 \cdot (-0.1) =
    -0.10 \]
  prefs: []
  type: TYPE_NORMAL
- en: Updating Model Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The loss derivatives with respect to each of the model parameters are the gradients,
    so we are ready to use gradient descent optimization with the addition of,
  prefs: []
  type: TYPE_NORMAL
- en: '**learning rate** - to scale the rate of change of the model updates we assign
    a learning rate, \(\eta\). For our model parameter examples from above,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{4,6}} \]\[ \lambda_{1,4}^{\ell} = \lambda_{1,4}^{\ell - 1}
    + \eta \cdot \frac{\partial L}{\partial \lambda_{1,4}} \]\[ b_j^{\ell} = b_j^{\ell
    - 1} + \eta \cdot \frac{\partial L}{\partial b_j} \]
  prefs: []
  type: TYPE_NORMAL
- en: recall, this process of gradient calculation and model parameters, weights and
    biases, updating is iterated and is known as gradient descent optimization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the goal is to explore the loss hypersurface, avoiding and escaping local minimums
    and ultimately finding the global minimum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: learning rate, also known as step size is commonly set between 0.0 and 1.0,
    note 0.01 is the default in Keras module of TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low Learning Rate** ‚Äì more stable, but a slower solution, may get stuck in
    a local minimum'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Learning Rate** ‚Äì may be unstable, but perhaps a faster solution, may
    diverge out of the global minimum'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One strategy is to start with a high learning rate and then to decrease the
    learning rate over the iterations
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning Rate Decay** - set as > 0 to avoid mitigate oscillations,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \eta^{\ell} = \eta^{\ell - 1} \cdot \left( \frac{1}{1 + \text{decay} \cdot
    \ell} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\ell\) is the model training epoch
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the model parameter updates are for a single training data case?
    Consider this single model parameter,
  prefs: []
  type: TYPE_NORMAL
- en: we calculate the update over all samples in the batch and apply the average
    of the updates.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial L}{\partial \lambda_{4,6}} = H_4 \cdot \frac{\partial L}{\partial
    O_{6_{\text{in}}}} = 0.42 \cdot 1.00 = 0.42 \]
  prefs: []
  type: TYPE_NORMAL
- en: is applied to update the \(\lambda_{4,6}\) parameter as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \cdot \frac{\partial
    L}{\partial \lambda_{4,6}} \]
  prefs: []
  type: TYPE_NORMAL
- en: is dependent on \(H_4\) node output, and \(L\) error that are for a single sample,
    \(ùë•_1,\ldots,ùë•_ùëö\) and \(ùë¶\); therefore, we cannot calculate a single parameter
    update over all our \(1,\ldots,n\) training data samples.
  prefs: []
  type: TYPE_NORMAL
- en: instead we can calculate \(1,\ldots,n\) updates and then apply the average of
    all the updates to our model parameters,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \frac{1}{n_{batch}} \sum_{i=1}^{n_{batch}}
    \eta \cdot \frac{\partial L}{\partial \lambda_{4,6}} \]
  prefs: []
  type: TYPE_NORMAL
- en: since the learning rate is a constant, we can move it out of the sum and now
    we are averaging the gradients,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda_{4,6}^{\ell} = \lambda_{4,6}^{\ell - 1} + \eta \frac{1}{n_{batch}}
    \sum_{i=1}^{n_{batch}} \frac{\partial L}{\partial \lambda_{4,6}} \]
  prefs: []
  type: TYPE_NORMAL
- en: Training Epochs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a good time to talk about stochastic gradient descent optimization,
    first let‚Äôs define some common terms,
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch Gradient Descent** - updates the model parameters after passing through
    all of the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic Gradient Descent** - updates the model parameters over each sample
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mini-batch Gradient Descent** - updates the model parameter after passing
    through a single batch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With mini-batch gradient descent stochasticity is introduced through the use
    of subsets of the data, known as batches,
  prefs: []
  type: TYPE_NORMAL
- en: for example, if we divide our 100 samples into 4 batches, then we iterate over
    each batch separately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we speed up the individual updates, fewer data are faster to calculate, but
    we introduce more error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this often helps the training explore for the global minimum and avoid getting
    stuck in local minimums and along ridges in the loss hypersurface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally our last definition here,
  prefs: []
  type: TYPE_NORMAL
- en: '**epoch** - is one pass over all of the data, so that would be 4 iterations
    of updating the model parameters if we have 4 mini-batches'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many other considerations that I will add later including,
  prefs: []
  type: TYPE_NORMAL
- en: momentum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: adaptive optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let‚Äôs build the above artificial neural network by-hand and visualize the
    solution!
  prefs: []
  type: TYPE_NORMAL
- en: this is by-hand so that you can see every calculation. I intentionally avoided
    using TensorFlow or PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interactive Dashboard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I built out an interactive Python dashboard with the code below for training
    an artificial neural network. You can step through the training iteration and
    observe over the training epochs,
  prefs: []
  type: TYPE_NORMAL
- en: model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: forward pass predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: backpropagation of error derivatives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you would like to see artificial neural networks in action, check out my
    [ANN interactive Python dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_ANN.ipynb),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab7e3b58fcfdbf8f7a2c19b5d78c7736.png)'
  prefs: []
  type: TYPE_IMG
- en: Interactive artificial neural network training Python dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Import Required Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will also need some standard packages. These should have been installed with
    Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available
    with the respective package docs.
  prefs: []
  type: TYPE_NORMAL
- en: Declare Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs the functions to make, train and visualize our artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The Simple ANN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I wrote this code to specify a simple ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: three input nodes, 2 hidden nodes and 1 output node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'and to train the ANN by iteratively performing the forward calculation and
    backpropagation. I calculate:'
  prefs: []
  type: TYPE_NORMAL
- en: the error and then propagate it to each node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: solve for the partial derivatives of the error with respect to each weight and
    bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: all weights, biases and partial derivatives for all epoch are recorded in vectors
    for plotting
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now Visualize the Network for a Specific Epoch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I wrote a custom network visualization below, select iepoch and visualize the
    artificial neural network for a specific epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/f1c2dc9fc05ec8a3d07b5642c59c42dd91a2b2ed35c547faa409371a1e776270.png](../Images/6c1b34f06ebf3ebfafbbd206fc1033ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Check the ANN Convergence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we plot the weights, biases and prediction over the epochs to check the
    training convergence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/96a2f9a7007123700054af340b223a0c9017cbf262afe43f97c8f1b445ec44a6.png](../Images/41e5da630bdb6e70068ef584a2fc3d47.png)'
  prefs: []
  type: TYPE_IMG
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of artificial neural networks. Much more could be
    done and discussed, I have many more resources. Check out my [shared resource
    inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture links
    at the start of this chapter with resource links in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
