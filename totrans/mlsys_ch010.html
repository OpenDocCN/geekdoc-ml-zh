<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch010.xhtml</title>
  <style>
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    /* Figure formatting */
    .quarto-layout-panel>figure>figcaption,
    .quarto-layout-panel>.panel-caption {
      margin-top: 10pt;
    }

    .quarto-layout-row {
      display: flex;
      align-items: flex-start;
    }

    .quarto-layout-valign-top {
      align-items: flex-start;
    }

    .quarto-layout-valign-bottom {
      align-items: flex-end;
    }

    .quarto-layout-valign-center {
      align-items: center;
    }

    .quarto-layout-cell {
      position: relative;
      margin-right: 20px;
    }

    .quarto-layout-cell:last-child {
      margin-right: 0;
    }

    .quarto-layout-cell figure,
    .quarto-layout-cell>p {
      margin: 0.2em;
    }

    .quarto-layout-cell .html-widget {
      width: 100% !important;
    }

    .quarto-layout-cell div figure p {
      margin: 0;
    }

    .quarto-layout-cell figure {
      display: inline-block;
      margin-inline-start: 0;
      margin-inline-end: 0;
    }

    .quarto-layout-cell table {
      display: inline-table;
    }

    .quarto-layout-cell-subref figcaption {
      font-style: italic;
      text-align: center;
    }

    .quarto-figure>figure {
      width: 100%;
    }

    .quarto-figure-left>figure>p {
      text-align: left;
    }

    .quarto-figure-center>figure>p {
      text-align: center;
    }

    .quarto-figure-right>figure>p {
      text-align: right;
    }

    figure>p:empty {
      display: none;
    }

    figure>p:first-child {
      margin-top: 0;
      margin-bottom: 0;
    }

    figure>figcaption {
      margin-top: 0.5em;
    }

    figcaption {
      font-size: 0.8em;
    }

    details {
      margin-bottom: 1em;
    }

    details[show] {
      margin-bottom: 0;
    }

    .quarto-unresolved-ref {
      font-weight: 600;
    }

    .quarto-cover-image {
      float: right;
      margin-left: 30px;
    }

    .cell-output-display {
      overflow-x: scroll;
    }

    .hidden {
      display: none;
    }
  </style>
</head>
<body epub:type="bodymatter">
<section id="sec-dnn-architectures" class="level1">
<h1>DNN Architectures</h1>
<div class="{layout-narrow}">
<div class="column-margin">
<p><em>DALL¬∑E 3 Prompt: A visually striking rectangular image illustrating the interplay between deep learning algorithms like CNNs, RNNs, and Attention Networks, interconnected with machine learning systems. The composition features neural network diagrams blending seamlessly with representations of computational systems such as processors, graphs, and data streams. Bright neon tones contrast against a dark futuristic background, symbolizing cutting-edge technology and intricate system complexity.</em></p>
</div>
<p> <img src="../media/file53.png" alt="" /></p>
</div>
<section id="purpose-3" class="level2 unnumbered">
<h2 class="unnumbered">Purpose</h2>
<p><em>Why do architectural choices in neural networks affect system design decisions that determine computational feasibility, hardware requirements, and deployment constraints?</em></p>
<p>Neural network architectures represent engineering decisions that directly determine system performance and deployment viability. Each architectural choice creates cascading effects throughout the system stack: memory bandwidth demands, computational complexity patterns, parallelization opportunities, and hardware acceleration compatibility. Understanding these architectural implications enables engineers to make informed trade-offs between model capability and system constraints, predict computational bottlenecks before they occur, and select appropriate hardware platforms. Architectural decisions determine whether machine learning systems meet performance requirements within available computational resources. This understanding proves essential for building scalable AI systems that can be deployed effectively across diverse environments.</p>
<div title="Learning Objectives">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Learning Objectives</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p>Distinguish the computational characteristics and inductive biases of the four main neural network architectural families (MLPs, CNNs, RNNs, Transformers)</p></li>
<li><p>Analyze how architectural design choices determine computational complexity, memory requirements, and parallelization opportunities</p></li>
<li><p>Evaluate the system-level implications of architectural patterns on hardware utilization, memory bandwidth, and deployment constraints</p></li>
<li><p>Apply the architecture selection framework to match data characteristics with appropriate neural network designs for specific applications</p></li>
<li><p>Assess computational and memory trade-offs between different architectural approaches using complexity analysis</p></li>
<li><p>Examine how fundamental computational primitives (matrix multiplication, convolution, attention) map to hardware acceleration opportunities</p></li>
<li><p>Critique common architectural selection fallacies and their impact on system performance and deployment success</p></li>
<li><p>Synthesize the unified inductive bias framework explaining architecture-data compatibility patterns</p></li>
</ul>
</div>
</div>
</div>
</div>
</section>
<section id="sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="level2">
<h2>Architectural Principles and Engineering Trade-offs</h2>
<p>The systematic organization of neural computations into effective architectures represents one of the most consequential developments in contemporary machine learning systems. Building on the mathematical foundations of neural computation established in <a href="ch009.xhtml#sec-dl-primer" class="quarto-xref">Chapter¬†3</a>, this chapter investigates the architectural principles that govern how operations (matrix multiplications, nonlinear activations, and gradient-based optimization) are structured to address complex computational problems. This architectural perspective bridges the gap between mathematical theory and practical systems implementation, examining how design choices at the network level determine system-wide performance characteristics.</p>
<p>This chapter centers on an engineering trade-off that permeates machine learning systems design. While mathematical theory, particularly universal approximation results, establishes that neural networks possess remarkable representational flexibility, practical deployment necessitates computational efficiency achievable only through judicious architectural specialization. This tension manifests across multiple dimensions: theoretical universality versus computational tractability, representational completeness versus memory efficiency, and mathematical generality versus domain-specific optimization. The resolution of these tensions through architectural innovation constitutes a primary driver of progress in machine learning systems.</p>
<p>Contemporary neural architectures emerge from systematic responses to specific computational challenges encountered when deploying general mathematical frameworks on structured data. Each architectural paradigm embodies distinct inductive biases (implicit assumptions about data structure and relationships) that enable efficient learning while constraining the hypothesis space in domain-appropriate ways. These architectural innovations represent engineering solutions to the challenge of organizing computational primitives into patterns that achieve optimal balance between representational capacity and computational efficiency.</p>
<p>This chapter examines four architectural families that collectively define the conceptual landscape of modern neural computation. Multi-Layer Perceptrons serve as the canonical implementation of universal approximation theory, demonstrating how dense connectivity enables general pattern recognition while illustrating the computational costs of architectural generality. Convolutional Neural Networks introduce the paradigm of spatial architectural specialization, exploiting translational invariance and local connectivity to achieve significant efficiency gains while preserving representational power for spatial data. Recurrent Neural Networks extend architectural specialization to temporal domains, incorporating explicit memory mechanisms that enable sequential processing capabilities absent from feedforward architectures. Attention mechanisms and Transformer architectures represent the current evolutionary frontier, replacing fixed structural assumptions with dynamic, content-dependent computation that achieves remarkable capability while maintaining computational efficiency through parallelizable operations.</p>
<p>The systems engineering significance of these architectural patterns extends beyond mere algorithmic considerations. Each architectural choice creates distinct computational signatures that propagate through every level of the implementation stack, determining memory access patterns, parallelization strategies, hardware utilization characteristics, and ultimately system feasibility within resource constraints. Understanding these architectural implications proves essential for engineers responsible for system design, resource allocation, and performance optimization in production environments.</p>
<p>This chapter adopts a systems-oriented analytical framework that illuminates the relationships between architectural abstractions and concrete implementation requirements. For each architectural family, we systematically examine the computational primitives that determine hardware resource demands, the organizational principles that enable efficient algorithmic implementation, the memory hierarchy implications that affect system scalability, and the trade-offs between architectural sophistication and computational overhead.</p>
<p>The analytical approach builds systematically upon the neural network foundations established in <a href="ch009.xhtml#sec-dl-primer" class="quarto-xref">Chapter¬†3</a>, extending core concepts of forward propagation, backpropagation, and gradient-based optimization by examining how architectural specialization organizes these operations to exploit problem-specific structure. Understanding the evolutionary relationships connecting these architectural paradigms and their distinct computational characteristics, practitioners develop the conceptual tools necessary for principled decision-making regarding architectural selection, resource planning, and system optimization in complex deployment scenarios.</p>
</section>
<section id="sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="level2">
<h2>Multi-Layer Perceptrons: Dense Pattern Processing</h2>
<p>Multi-Layer Perceptrons (MLPs) represent the fully-connected architectures introduced in <a href="ch009.xhtml#sec-dl-primer" class="quarto-xref">Chapter¬†3</a>, now examined through the lens of architectural choice and systems trade-offs. MLPs embody an inductive bias: <strong>they assume no prior structure in the data, allowing any input to relate to any output</strong>. This architectural choice enables maximum flexibility by treating all input relationships as equally plausible, making MLPs versatile but computationally intensive compared to specialized alternatives. Their computational power was established theoretically by the Universal Approximation Theorem (UAT)<a href="#fn1" class="footnote-ref" id="fnref1" epub:type="noteref" role="doc-noteref">1</a> <span class="citation" data-cites="cybenko1989approximation hornik1989multilayer">(<a href="ch058.xhtml#ref-cybenko1989approximation">Cybenko 1989</a>; <a href="ch058.xhtml#ref-hornik1989multilayer">Hornik, Stinchcombe, and White 1989</a>)</span>, which we encountered as a footnote in <a href="ch009.xhtml#sec-dl-primer" class="quarto-xref">Chapter¬†3</a>. This theorem states that a sufficiently large MLP with non-linear activation functions can approximate any continuous function on a compact domain, given suitable weights and biases.</p>
<div class="callout-definition" title="Multi-Layer Perceptrons">
<p><strong><em>Multi-Layer Perceptrons (MLPs)</em></strong> are <em>fully-connected neural networks</em> where every neuron connects to all neurons in adjacent layers, providing <em>maximum flexibility</em> through <em>universal approximation</em> at the cost of <em>high parameter counts</em> and <em>computational intensity</em>.</p>
</div>
<p>In practice, the UAT explains why MLPs succeed across diverse tasks while revealing the gap between theoretical capability and practical implementation. The theorem guarantees that <em>some</em> MLP can approximate any function, yet provides no guidance on requisite network size or weight determination. This gap becomes critical in real-world applications: while MLPs can theoretically solve any pattern recognition problem, achieving this capability may require impractically large networks or extensive computation. This theoretical power drives the selection of MLPs for tabular data, recommendation systems, and problems where input relationships are unknown, while these practical limitations motivated the development of specialized architectures that exploit data structure for computational efficiency, as detailed in <a href="ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section¬†4.1</a>.</p>
<p>When applied to the MNIST handwritten digit recognition challenge<a href="#fn2" class="footnote-ref" id="fnref2" epub:type="noteref" role="doc-noteref">2</a>, an MLP demonstrates its computational approach by transforming a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>28</mn><mo>√ó</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics></math> pixel image into digit classification.</p>
<section id="sec-dnn-architectures-pattern-processing-needs-c45a" class="level3">
<h3>Pattern Processing Needs</h3>
<p>Deep learning models frequently encounter problems where any input feature may influence any output, absent inherent constraints on these relationships. Financial market analysis exemplifies this challenge: any economic indicator may affect any market outcome. Similarly, in natural language processing, the meaning of a word may depend on any other word in the sentence. These scenarios demand an architectural pattern capable of learning arbitrary relationships across all input features.</p>
<p>Dense pattern processing addresses these challenges through several key capabilities. First, it enables unrestricted feature interactions where each output can depend on any combination of inputs. Second, it supports learned feature importance, enabling the system to determine which connections matter rather than relying on prescribed relationships. Finally, it provides adaptive representation, enabling the network to reshape its internal representations based on the data.</p>
<p>The MNIST digit recognition task illustrates this uncertainty: while humans might focus on specific parts of digits (loops in ‚Äò6‚Äô or crossings in ‚Äò8‚Äô), the pixel combinations critical for classification remain indeterminate. A ‚Äò7‚Äô written with a serif may share pixel patterns with a ‚Äò2‚Äô, while variations in handwriting mean discriminative features may appear anywhere in the image. This uncertainty about feature relationships necessitates a dense processing approach where every pixel can potentially influence the classification decision.</p>
<p>This requirement for unrestricted connectivity leads directly to the mathematical foundation of MLPs.</p>
</section>
<section id="sec-dnn-architectures-algorithmic-structure-c012" class="level3">
<h3>Algorithmic Structure</h3>
<p>MLPs enable unrestricted feature interactions through a direct algorithmic solution: complete connectivity between all nodes. This connectivity requirement manifests through a series of fully-connected layers, where each neuron connects to every neuron in adjacent layers, the ‚Äúdense‚Äù connectivity pattern introduced in <a href="ch009.xhtml#sec-dl-primer" class="quarto-xref">Chapter¬†3</a>.</p>
<p>This architectural principle translates the dense connectivity pattern into matrix multiplication operations<a href="#fn3" class="footnote-ref" id="fnref3" epub:type="noteref" role="doc-noteref">3</a>, establishing the mathematical foundation that makes MLPs computationally tractable. As illustrated in <a href="ch010.xhtml#fig-mlp" class="quarto-xref">Figure¬†4.1</a>, each layer transforms its input through the fundamental operation introduced in <a href="ch009.xhtml#sec-dl-primer" class="quarto-xref">Chapter¬†3</a>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ùê°</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><msup><mi>ùê°</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>ùêñ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>ùêõ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">
\mathbf{h}^{(l)} = f\big(\mathbf{h}^{(l-1)}\mathbf{W}^{(l)} + \mathbf{b}^{(l)}\big)
</annotation></semantics></math></p>
<p>Recall that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ùê°</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{h}^{(l)}</annotation></semantics></math> represents the layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math> output (activation vector), <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ùê°</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{h}^{(l-1)}</annotation></semantics></math> represents the input from the previous layer, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ùêñ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(l)}</annotation></semantics></math> denotes the weight matrix for layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ùêõ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{b}^{(l)}</annotation></semantics></math> denotes the bias vector, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚ãÖ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation></semantics></math> denotes the activation function (such as ReLU, as detailed in <a href="ch009.xhtml#sec-dl-primer" class="quarto-xref">Chapter¬†3</a>). This layer-wise transformation, while conceptually simple, creates computational patterns whose efficiency depends critically on how we organize these operations for different problem structures.</p>
<div id="fig-mlp" class="quarto-float quarto-figure quarto-figure-center" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../media/file54.svg" alt="" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure¬†4.1: <strong>Layered Transformations</strong>: Multi-Layer Perceptrons (MLPs) implement dense connectivity through sequential matrix multiplications and non-linear activations, supporting complex feature interactions and hierarchical representations of input data. Each layer transforms the input vector from the previous layer, producing a new vector that serves as input to the subsequent layer, as defined by the equation in the text. Source: <span class="citation" data-cites="reagen2017deep">(<a href="ch058.xhtml#ref-reagen2017deep">Reagen et al. 2017</a>)</span>.
</figcaption>
</figure>
</div>
<p>The dimensions of these operations reveal the computational scale of dense pattern processing:</p>
<ul>
<li>Input vector: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ùê°</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>‚àà</mo><msup><mi>‚Ñù</mi><msub><mi>d</mi><mtext mathvariant="normal">in</mtext></msub></msup></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(0)} \in \mathbb{R}^{d_{\text{in}}}</annotation></semantics></math> (treated as a row vector in this formulation) represents all potential input features</li>
<li>Weight matrices: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ùêñ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><msub><mi>d</mi><mtext mathvariant="normal">in</mtext></msub><mo>√ó</mo><msub><mi>d</mi><mtext mathvariant="normal">out</mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}^{(l)} \in \mathbb{R}^{d_{\text{in}} \times d_{\text{out}}}</annotation></semantics></math> capture all possible input-output relationships</li>
<li>Output vector: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ùê°</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>‚àà</mo><msup><mi>‚Ñù</mi><msub><mi>d</mi><mtext mathvariant="normal">out</mtext></msub></msup></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(l)} \in \mathbb{R}^{d_{\text{out}}}</annotation></semantics></math> produces transformed representations</li>
</ul>
<div class="callout-example" title="Concrete Computation Example">
<p>Consider a simplified 4-pixel image processed by a 3-neuron hidden layer:</p>
<p><strong>Input</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ùê°</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0.8</mn><mo>,</mo><mn>0.2</mn><mo>,</mo><mn>0.9</mn><mo>,</mo><mn>0.1</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(0)} = [0.8, 0.2, 0.9, 0.1]</annotation></semantics></math> (4 pixel intensities)</p>
<p><strong>Weight matrix</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ùêñ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.5</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>‚àí</mi><mn>0.2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>‚àí</mi><mn>0.3</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.8</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.4</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.2</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>‚àí</mi><mn>0.4</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.6</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.7</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.3</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>‚àí</mi><mn>0.1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{W}^{(1)} = \begin{bmatrix} 0.5 &amp; 0.1 &amp; -0.2 \\ -0.3 &amp; 0.8 &amp; 0.4 \\ 0.2 &amp; -0.4 &amp; 0.6 \\ 0.7 &amp; 0.3 &amp; -0.1 \end{bmatrix}</annotation></semantics></math> (4√ó3 matrix)</p>
<p><strong>Computation</strong>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="center" style="text-align: center"><msup><mi>ùê≥</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>ùê°</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>ùêñ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.5</mn><mo>√ó</mo><mn>0.8</mn><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚àí</mi><mn>0.3</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>√ó</mo><mn>0.2</mn><mo>+</mo><mn>0.2</mn><mo>√ó</mo><mn>0.9</mn><mo>+</mo><mn>0.7</mn><mo>√ó</mo><mn>0.1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.1</mn><mo>√ó</mo><mn>0.8</mn><mo>+</mo><mn>0.8</mn><mo>√ó</mo><mn>0.2</mn><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚àí</mi><mn>0.4</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>√ó</mo><mn>0.9</mn><mo>+</mo><mn>0.3</mn><mo>√ó</mo><mn>0.1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚àí</mi><mn>0.2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>√ó</mo><mn>0.8</mn><mo>+</mo><mn>0.4</mn><mo>√ó</mo><mn>0.2</mn><mo>+</mo><mn>0.6</mn><mo>√ó</mo><mn>0.9</mn><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚àí</mi><mn>0.1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>√ó</mo><mn>0.1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.65</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>‚àí</mi><mn>0.17</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.47</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*}
\mathbf{z}^{(1)} = \mathbf{h}^{(0)}\mathbf{W}^{(1)} = \begin{bmatrix} 0.5√ó0.8 + (-0.3)√ó0.2 + 0.2√ó0.9 + 0.7√ó0.1 \\ 0.1√ó0.8 + 0.8√ó0.2 + (-0.4)√ó0.9 + 0.3√ó0.1 \\ (-0.2)√ó0.8 + 0.4√ó0.2 + 0.6√ó0.9 + (-0.1)√ó0.1 \end{bmatrix}
\\
= \begin{bmatrix} 0.65 \\ -0.17 \\ 0.47 \end{bmatrix}
\end{gather*}</annotation></semantics></math> <strong>After ReLU</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ùê°</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0.65</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0.47</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(1)} = [0.65, 0, 0.47]</annotation></semantics></math> (negative values zeroed)</p>
<p>Each hidden neuron combines ALL input pixels with different weights, demonstrating unrestricted feature interaction.</p>
</div>
<p>The MNIST example demonstrates the practical scale of these operations:</p>
<ul>
<li>Each 784-dimensional input (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>28</mn><mo>√ó</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics></math> pixels) connects to every neuron in the first hidden layer</li>
<li>A hidden layer with 100 neurons requires a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>784</mn><mo>√ó</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">784\times 100</annotation></semantics></math> weight matrix</li>
<li>Each weight in this matrix represents a learnable relationship between an input pixel and a hidden feature</li>
</ul>
<p>This algorithmic structure addresses the need for arbitrary feature relationships while creating specific computational patterns that computer systems must accommodate.</p>
<section id="sec-dnn-architectures-architectural-characteristics-47b4" class="level4">
<h4>Architectural Characteristics</h4>
<p>This dense connectivity approach creates both advantages and trade-offs. Dense connectivity provides the universal approximation capability established earlier but introduces computational redundancy. While this theoretical power enables MLPs to model any continuous function given sufficient width, this flexibility necessitates numerous parameters to learn relatively simple patterns. The dense connections ensure that every input feature influences every output, yielding maximum expressiveness at the cost of maximum computational expense.</p>
<p>These trade-offs motivate sophisticated optimization techniques that reduce computational demands while preserving model capability. Structured pruning can eliminate 80-90% of connections with minimal accuracy loss, while quantization reduces precision requirements from 32-bit to 8-bit or lower. While <a href="ch016.xhtml#sec-model-optimizations" class="quarto-xref">Chapter¬†10</a> details these compression strategies, the architectural foundations established here determine which optimization approaches prove most effective for dense connectivity patterns, with <a href="ch017.xhtml#sec-ai-acceleration" class="quarto-xref">Chapter¬†11</a> exploring hardware-specific implementations that exploit regular matrix operation structure.</p>
</section>
</section>
<section id="sec-dnn-architectures-computational-mapping-fe7e" class="level3">
<h3>Computational Mapping</h3>
<p>The mathematical representation of dense matrix multiplication maps to specific computational patterns that systems must handle. This mapping progresses from mathematical abstraction to computational reality, as demonstrated in the first implementation shown in <a href="ch010.xhtml#lst-mlp_layer_matrix" class="quarto-xref">Listing¬†4.1</a>.</p>
<p>The function mlp_layer_matrix directly mirrors the mathematical equation, employing high-level matrix operations (<code>matmul</code>) to express the computation in a single line while abstracting the underlying complexity. This implementation style characterizes deep learning frameworks, where optimized libraries manage the actual computation.</p>
<div id="lst-mlp_layer_matrix" class="listing quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-lst">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-mlp_layer_matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing¬†4.1: This implementation shows neural networks performing weighted sum and activation functions across layers using matrix operations. The code emphasizes the core computational pattern in multi-layer perceptrons.
</figcaption>
<div aria-describedby="lst-mlp_layer_matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">def</span> mlp_layer_matrix(X, W, b):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span class="co"># X: input matrix (batch_size √ó num_inputs)</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>    <span class="co"># W: weight matrix (num_inputs √ó num_outputs)</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>    <span class="co"># b: bias vector (num_outputs)</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>    H <span class="op">=</span> activation(matmul(X, W) <span class="op">+</span> b)</span>
<span id="cb1-6"><a href="#cb1-6"></a>    <span class="co"># One clean line of math</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>    <span class="cf">return</span> H</span></code></pre></div>
</div>
</figure>
</div>
<p>To understand the system implications of this architecture, we must look ‚Äúunder the hood‚Äù of the high-level framework call. The elegant one-line matrix multiplication <code>output = matmul(X, W)</code> is, from the hardware‚Äôs perspective, a series of nested loops that expose the true computational demands on the system. This translation from logical model to physical execution reveals critical patterns that determine memory access, parallelization strategies, and hardware utilization.</p>
<p>The second implementation, <code>mlp_layer_compute</code> (shown in <a href="ch010.xhtml#lst-mlp_layer_compute" class="quarto-xref">Listing¬†4.2</a>), exposes the actual computational pattern through nested loops. This version reveals what really happens when we compute a layer‚Äôs output: we process each sample in the batch, computing each output neuron by accumulating weighted contributions from all inputs.</p>
<div id="lst-mlp_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-lst">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-mlp_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing¬†4.2: This implementation computes each output neuron by accumulating weighted contributions from all inputs across the batch. The detailed step-by-step process exposes how a single layer in a neural network processes data, emphasizing the role of biases and weighted sums in producing outputs.
</figcaption>
<div aria-describedby="lst-mlp_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> mlp_layer_compute(X, W, b):</span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="co"># Process each sample in the batch</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>    <span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb2-4"><a href="#cb2-4"></a>        <span class="co"># Compute each output neuron</span></span>
<span id="cb2-5"><a href="#cb2-5"></a>        <span class="cf">for</span> out <span class="kw">in</span> <span class="bu">range</span>(num_outputs):</span>
<span id="cb2-6"><a href="#cb2-6"></a>            <span class="co"># Initialize with bias</span></span>
<span id="cb2-7"><a href="#cb2-7"></a>            Z[batch, out] <span class="op">=</span> b[out]</span>
<span id="cb2-8"><a href="#cb2-8"></a>            <span class="co"># Accumulate weighted inputs</span></span>
<span id="cb2-9"><a href="#cb2-9"></a>            <span class="cf">for</span> in_ <span class="kw">in</span> <span class="bu">range</span>(num_inputs):</span>
<span id="cb2-10"><a href="#cb2-10"></a>                Z[batch, out] <span class="op">+=</span> X[batch, in_] <span class="op">*</span> W[in_, out]</span>
<span id="cb2-11"><a href="#cb2-11"></a></span>
<span id="cb2-12"><a href="#cb2-12"></a>    H <span class="op">=</span> activation(Z)</span>
<span id="cb2-13"><a href="#cb2-13"></a>    <span class="cf">return</span> H</span></code></pre></div>
</div>
</figure>
</div>
<p>This translation from mathematical abstraction to concrete computation exposes how dense matrix multiplication decomposes into nested loops of simpler operations. The outer loop processes each sample in the batch, while the middle loop computes values for each output neuron. Within the innermost loop, the system performs repeated multiply-accumulate operations<a href="#fn4" class="footnote-ref" id="fnref4" epub:type="noteref" role="doc-noteref">4</a>, combining each input with its corresponding weight.</p>
<p>In the MNIST example, each output neuron requires 784 multiply-accumulate operations and at least 1,568 memory accesses (784 for inputs, 784 for weights). While actual implementations use optimizations through libraries like BLAS<a href="#fn5" class="footnote-ref" id="fnref5" epub:type="noteref" role="doc-noteref">5</a> or cuBLAS, these patterns drive key system design decisions. The hardware architectures that accelerate these matrix operations, including GPU tensor cores<a href="#fn6" class="footnote-ref" id="fnref6" epub:type="noteref" role="doc-noteref">6</a> and specialized AI accelerators, are covered in <a href="ch017.xhtml#sec-ai-acceleration" class="quarto-xref">Chapter¬†11</a>.</p>
</section>
<section id="sec-dnn-architectures-system-implications-7a8f" class="level3">
<h3>System Implications</h3>
<p>Neural network architectures exhibit distinct system-level characteristics that exhibit three core dimensions for systematic analysis: memory requirements, computation needs, and data movement. This framework enables consistent analysis of how algorithmic patterns influence system design decisions, revealing both commonalities and architecture-specific optimizations. We apply this framework throughout our analysis of each architecture family. These system-level considerations build directly on the foundational concepts of neural network computation patterns, memory systems, and system scaling discussed in <a href="ch009.xhtml#sec-dl-primer" class="quarto-xref">Chapter¬†3</a>.</p>
<section id="sec-dnn-architectures-memory-requirements-4900" class="level4">
<h4>Memory Requirements</h4>
<p>For dense pattern processing, the memory requirements stem from storing and accessing weights, inputs, and intermediate results. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. Each forward pass must access all these weights, along with input data and intermediate results. The all-to-all connectivity pattern means there‚Äôs no inherent locality in these accesses; every output needs every input and its corresponding weights.</p>
<p>These memory access patterns enable optimization through careful data organization and reuse. Modern processors handle these dense access patterns through specialized approaches: CPUs leverage their cache hierarchy for data reuse, while GPUs employ memory architectures designed for high-bandwidth access to large parameter matrices. Frameworks abstract these optimizations through high-performance matrix operations (as detailed in our earlier analysis).</p>
</section>
<section id="sec-dnn-architectures-computation-needs-9cb4" class="level4">
<h4>Computation Needs</h4>
<p>The core computation revolves around multiply-accumulate operations arranged in nested loops. Each output value requires as many multiply-accumulates as there are inputs. For MNIST, this requires 784 multiply-accumulates per output neuron. With 100 neurons in the hidden layer, 78,400 multiply-accumulates are performed for a single input image. While these operations are simple, their volume and arrangement create specific demands on processing resources.</p>
<p>This computational structure enables specific optimization strategies in modern hardware. The dense matrix multiplication pattern parallelizes across multiple processing units, with each handling different subsets of neurons. Modern hardware accelerators take advantage of this through specialized matrix multiplication units, while software frameworks automatically convert these operations into optimized BLAS (Basic Linear Algebra Subprograms) calls. CPUs and GPUs can both exploit cache locality by carefully tiling the computation to maximize data reuse, though their specific approaches differ based on their architectural strengths.</p>
</section>
<section id="sec-dnn-architectures-data-movement-fc16" class="level4">
<h4>Data Movement</h4>
<p>The all-to-all connectivity pattern in MLPs creates significant data movement requirements. Each multiply-accumulate operation needs three pieces of data: an input value, a weight value, and the running sum. For our MNIST example layer, computing a single output value requires moving 784 inputs and 784 weights to wherever the computation occurs. This movement pattern repeats for each of the 100 output neurons, creating large data transfer demands between memory and compute units.</p>
<p>The predictable data movement patterns enable strategic data staging and transfer optimizations. Different architectures address this challenge through various mechanisms; CPUs use prefetching and multi-level caches, while GPUs employ high-bandwidth memory systems and latency hiding through massive threading. Software frameworks orchestrate these data movements through memory management systems that reduce redundant transfers and increase data reuse.</p>
<p>This analysis of MLP computational demands reveals a crucial insight: while dense connectivity provides universal approximation capabilities, it creates significant inefficiencies when data exhibits inherent structure. This mismatch between architectural assumptions and data characteristics motivated the development of specialized approaches that could exploit structural patterns for computational gain.</p>
</section>
</section>
</section>
<section id="sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff" class="level2">
<h2>CNNs: Spatial Pattern Processing</h2>
<p>The computational intensity and parameter requirements of MLPs reveal a mismatch when applied to structured data. Building on the computational complexity considerations outlined in <a href="ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section¬†4.1</a>, this inefficiency motivated the development of architectural patterns that exploit inherent data structure.</p>
<p>Convolutional Neural Networks emerged as the solution to this challenge <span class="citation" data-cites="lecun1998gradient krizhevsky2012imagenet">(<a href="ch058.xhtml#ref-lecun1998gradient">Lecun et al. 1998</a>; <a href="ch058.xhtml#ref-krizhevsky2012imagenet">Krizhevsky, Sutskever, and Hinton 2017a</a>)</span>, embodying a specific inductive bias: they assume spatial locality and translation invariance, where nearby pixels are related and patterns can appear anywhere. This architectural assumption enables two key innovations that enhance efficiency for spatially structured data. Parameter sharing allows the same feature detector to be applied across different spatial positions, reducing parameters from millions to thousands while improving generalization. Local connectivity restricts connections to spatially adjacent regions, reflecting the insight that spatial proximity correlates with feature relevance.</p>
<div class="callout-definition" title="Convolutional Neural Networks">
<p><strong><em>Convolutional Neural Networks (CNNs)</em></strong> are neural architectures that exploit <em>spatial structure</em> through <em>local connectivity</em> and <em>parameter sharing</em>, using <em>learnable filters</em> to build <em>hierarchical representations</em> with substantially fewer parameters than fully-connected networks.</p>
</div>
<p>These architectural innovations represent a trade-off in deep learning design: sacrificing the theoretical generality of MLPs for practical efficiency gains when data exhibits known structure. While MLPs treat each input element independently, CNNs exploit spatial relationships to achieve computational savings and improved performance on vision tasks.</p>
<section id="sec-dnn-architectures-pattern-processing-needs-a4ce" class="level3">
<h3>Pattern Processing Needs</h3>
<p>Spatial pattern processing addresses scenarios where the relationship between data points depends on their relative positions or proximity. Consider processing a natural image: a pixel‚Äôs relationship with its neighbors is important for detecting edges, textures, and shapes. These local patterns then combine hierarchically to form more complex features: edges form shapes, shapes form objects, and objects form scenes.</p>
<p>This hierarchical spatial pattern processing appears across many domains. In computer vision, local pixel patterns form edges and textures that combine into recognizable objects. Speech processing relies on patterns across nearby time segments to identify phonemes and words. Sensor networks analyze correlations between physically proximate sensors to understand environmental patterns. Medical imaging depends on recognizing tissue patterns that indicate biological structures.</p>
<p>Focusing on image processing to illustrate these principles, if we want to detect a cat in an image, certain spatial patterns must be recognized: the triangular shape of ears, the round contours of the face, the texture of fur. Importantly, these patterns maintain their meaning regardless of where they appear in the image. A cat is still a cat whether it appears in the top-left or bottom-right corner. This indicates two key requirements for spatial pattern processing: the ability to detect local patterns and the ability to recognize these patterns regardless of their position<a href="#fn7" class="footnote-ref" id="fnref7" epub:type="noteref" role="doc-noteref">7</a>. <a href="ch010.xhtml#fig-cnn-spatial-processing" class="quarto-xref">Figure¬†4.2</a> shows convolutional neural networks achieving this through hierarchical feature extraction, where simple patterns compose into increasingly complex representations at successive layers.</p>
<div id="fig-cnn-spatial-processing" class="quarto-float quarto-figure quarto-figure-center" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-cnn-spatial-processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../media/file55.svg" alt="" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-spatial-processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure¬†4.2: <strong>Spatial Feature Extraction</strong>: Convolutional neural networks identify patterns independent of their location in an image by applying learnable filters across the input, enabling robust object recognition. These filters detect local features, and their repeated application across the image creates translation invariance, the ability to recognize a pattern regardless of its position.
</figcaption>
</figure>
</div>
<p>This leads us to the convolutional neural network architecture (CNN), pioneered by Yann LeCun<a href="#fn8" class="footnote-ref" id="fnref8" epub:type="noteref" role="doc-noteref">8</a> and <span class="citation" data-cites="lecun1989backpropagation">Y. LeCun et al. (<a href="ch058.xhtml#ref-lecun1989backpropagation">1989</a>)</span>. CNNs achieve this through several key innovations: parameter sharing<a href="#fn9" class="footnote-ref" id="fnref9" epub:type="noteref" role="doc-noteref">9</a>, local connectivity, and translation invariance<a href="#fn10" class="footnote-ref" id="fnref10" epub:type="noteref" role="doc-noteref">10</a>.</p>
</section>
<section id="sec-dnn-architectures-algorithmic-structure-a10c" class="level3">
<h3>Algorithmic Structure</h3>
<p>The core operation in a CNN can be expressed mathematically as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>ùêá</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><munder><mo>‚àë</mo><mrow><mi>d</mi><mi>i</mi></mrow></munder><munder><mo>‚àë</mo><mrow><mi>d</mi><mi>j</mi></mrow></munder><munder><mo>‚àë</mo><mi>c</mi></munder><msubsup><mi>ùêñ</mi><mrow><mi>d</mi><mi>i</mi><mo>,</mo><mi>d</mi><mi>j</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><msubsup><mi>ùêá</mi><mrow><mi>i</mi><mo>+</mo><mi>d</mi><mi>i</mi><mo>,</mo><mi>j</mi><mo>+</mo><mi>d</mi><mi>j</mi><mo>,</mo><mi>c</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>+</mo><msubsup><mi>ùêõ</mi><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\mathbf{H}^{(l)}_{i,j,k} = f\left(\sum_{di}\sum_{dj}\sum_{c} \mathbf{W}^{(l)}_{di,dj,c,k}\mathbf{H}^{(l-1)}_{i+di,j+dj,c} + \mathbf{b}^{(l)}_k\right)
</annotation></semantics></math></p>
<p>This equation describes how CNNs process spatial data. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>ùêá</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\mathbf{H}^{(l)}_{i,j,k}</annotation></semantics></math> is the output at spatial position <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math> in channel <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> of layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>. The triple sum iterates over the filter dimensions: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>i</mi><mo>,</mo><mi>d</mi><mi>j</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(di,dj)</annotation></semantics></math> scans the spatial filter size, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math> covers input channels. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>ùêñ</mi><mrow><mi>d</mi><mi>i</mi><mo>,</mo><mi>d</mi><mi>j</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\mathbf{W}^{(l)}_{di,dj,c,k}</annotation></semantics></math> represents the filter weights, capturing local spatial patterns. Unlike MLPs that connect all inputs to outputs, CNNs only connect local spatial neighborhoods.</p>
<p>Breaking down the notation further, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math> corresponds to spatial positions, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> indexes output channels, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math> indexes input channels, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>i</mi><mo>,</mo><mi>d</mi><mi>j</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(di,dj)</annotation></semantics></math> spans the local receptive field<a href="#fn11" class="footnote-ref" id="fnref11" epub:type="noteref" role="doc-noteref">11</a>. Unlike the dense matrix multiplication of MLPs, this operation:</p>
<ul>
<li>Processes local neighborhoods (typically <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>√ó</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">5\times 5</annotation></semantics></math>)</li>
<li>Reuses the same weights at each spatial position</li>
<li>Maintains spatial structure in its output</li>
</ul>
<p>To illustrate this process concretely, consider the MNIST digit classification task with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>28</mn><mo>√ó</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics></math> grayscale images. Each convolutional layer applies a set of filters (e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math>) that slide across the image, computing local weighted sums. If we use 32 filters with padding to preserve dimensions, the layer produces a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>28</mn><mo>√ó</mo><mn>28</mn><mo>√ó</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">28\times 28\times 32</annotation></semantics></math> output, where each spatial position contains 32 different feature measurements of its local neighborhood. This contrasts sharply with the Multi-Layer Perceptron (MLP) approach, where the entire image is flattened into a 784-dimensional vector before processing.</p>
<p>This algorithmic structure directly implements the requirements for spatial pattern processing, creating distinct computational patterns that influence system design. Unlike MLPs, convolutional networks preserve spatial locality, leveraging the hierarchical feature extraction principles established above. These properties drive architectural optimizations in AI accelerators, where operations such as data reuse, tiling, and parallel filter computation are important for performance.</p>
<div title="Mathematical Background">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Mathematical Background</strong></p>
</div>
<div class="callout-content">
<p>Group theory provides the mathematical framework for understanding symmetries and transformations in data. Translation equivariance means that shifting an input produces a correspondingly shifted output‚Äîa key property that enables CNNs to recognize patterns regardless of position.</p>
</div>
</div>
</div>
</div>
<p>Group theory provides the framework for understanding CNN effectiveness<a href="#fn12" class="footnote-ref" id="fnref12" epub:type="noteref" role="doc-noteref">12</a>, which provides a mathematical framework for understanding symmetries in data. Translation invariance emerges because convolution is equivariant with respect to the translation group‚Äîif we shift the input image, the output feature maps shift by the same amount. Mathematically, if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>T</mi><mi>v</mi></msub><annotation encoding="application/x-tex">T_v</annotation></semantics></math> represents translation by vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>, then a convolutional layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> satisfies: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>T</mi><mi>v</mi></msub><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>T</mi><mi>v</mi></msub><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(T_v x) = T_v f(x)</annotation></semantics></math>. This equivariance property allows CNNs to learn features that generalize across spatial locations.</p>
<p>The choice of convolution reflects deeper principles about inductive bias<a href="#fn13" class="footnote-ref" id="fnref13" epub:type="noteref" role="doc-noteref">13</a> in neural architecture design. By restricting connectivity to local neighborhoods and sharing parameters across spatial positions, CNNs encode prior knowledge about the structure of visual data: that important features are local and translation-invariant. This architectural constraint reduces the hypothesis space<a href="#fn14" class="footnote-ref" id="fnref14" epub:type="noteref" role="doc-noteref">14</a> that the network must search, enabling more efficient learning from limited data compared to fully connected networks.</p>
<p>CNNs naturally implement hierarchical representation learning through their layered structure. Early layers detect low-level features like edges and textures with small receptive fields, while deeper layers combine these into increasingly complex patterns with larger receptive fields. This hierarchical organization mirrors the structure of the visual cortex and enables CNNs to build compositional representations: complex objects are represented as compositions of simpler parts. The mathematical foundation for this emerges from the fact that stacking convolutional layers creates a tree-like dependency structure, where each deep neuron depends on an exponentially large set of input pixels, enabling efficient representation of hierarchical patterns.</p>
<section id="sec-dnn-architectures-architectural-characteristics-e309" class="level4">
<h4>Architectural Characteristics</h4>
<p>Parameter sharing dramatically reduces complexity compared to MLPs by reusing the same filters across spatial locations. This sharing embodies the assumption that useful features (such as edges or textures) can appear anywhere in an image, making the same feature detector valuable across all spatial positions.</p>
<p>The architectural efficiency of CNNs enables further optimization through specialized techniques. Depthwise separable convolutions decompose standard convolutions into depthwise and pointwise operations, reducing computation by 8-9√ó for typical mobile deployments. Channel pruning eliminates entire feature maps based on importance metrics, achieving 40-50% FLOPs reduction with &lt;1% accuracy loss. These optimization strategies build on spatial locality principles, with <a href="ch016.xhtml#sec-model-optimizations" class="quarto-xref">Chapter¬†10</a> exploring hardware-specific implementations and <a href="ch017.xhtml#sec-ai-acceleration" class="quarto-xref">Chapter¬†11</a> detailing how modern processors exploit convolution‚Äôs inherent data reuse patterns.</p>
<p>As illustrated in <a href="ch010.xhtml#fig-cnn" class="quarto-xref">Figure¬†4.3</a>, convolution operations involve sliding a small filter over the input image to generate a feature map<a href="#fn15" class="footnote-ref" id="fnref15" epub:type="noteref" role="doc-noteref">15</a>. This process captures local structures while maintaining translation invariance. For an interactive visual exploration of convolutional networks, the <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a> project provides an insightful demonstration of how these networks are constructed.</p>
<div id="fig-cnn" class="quarto-float quarto-figure quarto-figure-center" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../media/file56.svg" alt="" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure¬†4.3: The convolution operation processes input data through localized feature extraction using filters that slide across the image to identify patterns regardless of their position.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-dnn-architectures-computational-mapping-fea5" class="level3">
<h3>Computational Mapping</h3>
<p>Convolution operations create computational patterns different from MLP dense matrix multiplication. This translation from mathematical operations to implementation details reveals distinct computational characteristics.</p>
<p>The first implementation, <code>conv_layer_spatial</code> (shown in <a href="ch010.xhtml#lst-conv_layer_spatial" class="quarto-xref">Listing¬†4.3</a>), uses high-level convolution operations to express the computation concisely. This is typical in deep learning frameworks, where optimized libraries handle the underlying complexity.</p>
<div id="lst-conv_layer_spatial" class="listing quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-lst">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-conv_layer_spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing¬†4.3: This hierarchical approach processes input data through feature extraction using a convolution operation that combines a kernel and bias before applying an activation function.
</figcaption>
<div aria-describedby="lst-conv_layer_spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">def</span> conv_layer_spatial(<span class="bu">input</span>, kernel, bias):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    output <span class="op">=</span> convolution(<span class="bu">input</span>, kernel) <span class="op">+</span> bias</span>
<span id="cb3-3"><a href="#cb3-3"></a>    <span class="cf">return</span> activation(output)</span></code></pre></div>
</div>
</figure>
</div>
<p>The bridge between the logical model and physical execution becomes critical for understanding CNN system requirements. While the high-level convolution operation appears as a simple sliding window computation, the hardware must orchestrate complex data movement patterns and exploit spatial locality for efficiency.</p>
<p>The second implementation, conv_layer_compute (see <a href="ch010.xhtml#lst-conv_layer_compute" class="quarto-xref">Listing¬†4.4</a>), reveals the actual computational pattern: nested loops that process each spatial position, applying the same filter weights to local regions of the input. These seven nested loops expose the true nature of convolution‚Äôs computational structure and the optimization opportunities it creates.</p>
<div id="lst-conv_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-lst">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-conv_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing¬†4.4: <strong>Nested Loops</strong>: Convolutional layers process input through multiple nested loops that handle batched images, spatial dimensions, output channels, kernel windows, and input features, revealing the detailed computational structure of convolution operations.
</figcaption>
<div aria-describedby="lst-conv_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">def</span> conv_layer_compute(<span class="bu">input</span>, kernel, bias):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="co"># Loop 1: Process each image in batch</span></span>
<span id="cb4-3"><a href="#cb4-3"></a>    <span class="cf">for</span> image <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a>        <span class="co"># Loop 2&amp;3: Move across image spatially</span></span>
<span id="cb4-6"><a href="#cb4-6"></a>        <span class="cf">for</span> y <span class="kw">in</span> <span class="bu">range</span>(height):</span>
<span id="cb4-7"><a href="#cb4-7"></a>            <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(width):</span>
<span id="cb4-8"><a href="#cb4-8"></a></span>
<span id="cb4-9"><a href="#cb4-9"></a>                <span class="co"># Loop 4: Compute each output feature</span></span>
<span id="cb4-10"><a href="#cb4-10"></a>                <span class="cf">for</span> out_channel <span class="kw">in</span> <span class="bu">range</span>(num_output_channels):</span>
<span id="cb4-11"><a href="#cb4-11"></a>                    result <span class="op">=</span> bias[out_channel]</span>
<span id="cb4-12"><a href="#cb4-12"></a></span>
<span id="cb4-13"><a href="#cb4-13"></a>                    <span class="co"># Loop 5&amp;6: Move across kernel window</span></span>
<span id="cb4-14"><a href="#cb4-14"></a>                    <span class="cf">for</span> ky <span class="kw">in</span> <span class="bu">range</span>(kernel_height):</span>
<span id="cb4-15"><a href="#cb4-15"></a>                        <span class="cf">for</span> kx <span class="kw">in</span> <span class="bu">range</span>(kernel_width):</span>
<span id="cb4-16"><a href="#cb4-16"></a></span>
<span id="cb4-17"><a href="#cb4-17"></a>                            <span class="co"># Loop 7: Process each input feature</span></span>
<span id="cb4-18"><a href="#cb4-18"></a>                            <span class="cf">for</span> in_channel <span class="kw">in</span> <span class="bu">range</span>(</span>
<span id="cb4-19"><a href="#cb4-19"></a>                                num_input_channels</span>
<span id="cb4-20"><a href="#cb4-20"></a>                            ):</span>
<span id="cb4-21"><a href="#cb4-21"></a>                                <span class="co"># Get input value from correct window position</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>                                in_y <span class="op">=</span> y <span class="op">+</span> ky</span>
<span id="cb4-23"><a href="#cb4-23"></a>                                in_x <span class="op">=</span> x <span class="op">+</span> kx</span>
<span id="cb4-24"><a href="#cb4-24"></a>                                <span class="co"># Perform multiply-accumulate operation</span></span>
<span id="cb4-25"><a href="#cb4-25"></a>                                result <span class="op">+=</span> (</span>
<span id="cb4-26"><a href="#cb4-26"></a>                                    <span class="bu">input</span>[</span>
<span id="cb4-27"><a href="#cb4-27"></a>                                        image, in_y, in_x, in_channel</span>
<span id="cb4-28"><a href="#cb4-28"></a>                                    ]</span>
<span id="cb4-29"><a href="#cb4-29"></a>                                    <span class="op">*</span> kernel[</span>
<span id="cb4-30"><a href="#cb4-30"></a>                                        ky,</span>
<span id="cb4-31"><a href="#cb4-31"></a>                                        kx,</span>
<span id="cb4-32"><a href="#cb4-32"></a>                                        in_channel,</span>
<span id="cb4-33"><a href="#cb4-33"></a>                                        out_channel,</span>
<span id="cb4-34"><a href="#cb4-34"></a>                                    ]</span>
<span id="cb4-35"><a href="#cb4-35"></a>                                )</span>
<span id="cb4-36"><a href="#cb4-36"></a></span>
<span id="cb4-37"><a href="#cb4-37"></a>                    <span class="co"># Store result for this output position</span></span>
<span id="cb4-38"><a href="#cb4-38"></a>                    output[image, y, x, out_channel] <span class="op">=</span> result</span></code></pre></div>
</div>
</figure>
</div>
<p>The seven nested loops reveal different aspects of the computation:</p>
<ul>
<li>Outer loops (1-3) manage position: which image and where in the image</li>
<li>Middle loop (4) handles output features: computing different learned patterns</li>
<li>Inner loops (5-7) perform the actual convolution: sliding the kernel window</li>
</ul>
<p>Examining this process in detail, the outer two loops (<code>for y</code> and <code>for x</code>) traverse each spatial position in the output feature map (for the MNIST example, this traverses all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>28</mn><mo>√ó</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics></math> positions). At each position, values are computed for each output channel (<code>for k</code> loop), representing different learned features or patterns‚Äîthe 32 different feature detectors.</p>
<p>The inner three loops implement the actual convolution operation at each position. For each output value, we process a local <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> region of the input (the <code>dy</code> and <code>dx</code> loops) across all input channels (<code>for c</code> loop). This creates a sliding window effect, where the same <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> filter moves across the image, performing multiply-accumulates between the filter weights and the local input values. Unlike the MLP‚Äôs global connectivity, this local processing pattern means each output value depends only on a small neighborhood of the input.</p>
<p>For our MNIST example with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> filters and 32 output channels, each output position requires only 9 multiply-accumulate operations per input channel, compared to the 784 operations needed in our MLP layer. This operation must be repeated for every spatial position <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>28</mn><mo>√ó</mo><mn>28</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(28\times 28)</annotation></semantics></math> and every output channel (32).</p>
<p>While using fewer operations per output, the spatial structure creates different patterns of memory access and computation that systems must handle. These patterns influence system design, creating both challenges and opportunities for optimization.</p>
</section>
<section id="sec-dnn-architectures-system-implications-f25d" class="level3">
<h3>System Implications</h3>
<p>CNNs exhibit distinctive system-level patterns that differ significantly from MLP dense connectivity across all three analysis dimensions.</p>
<section id="sec-dnn-architectures-memory-requirements-7e2b" class="level4">
<h4>Memory Requirements</h4>
<p>For convolutional layers, memory requirements center around two key components: filter weights and feature maps. Unlike MLPs that require storing full connection matrices, CNNs use small, reusable filters. For a typical CNN processing 224√ó224 ImageNet images, a convolutional layer with 64 filters of size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> requires storing only 576 weight parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>√ó</mo><mn>3</mn><mo>√ó</mo><mn>64</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(3\times 3\times 64)</annotation></semantics></math>, dramatically less than the millions of weights needed for equivalent fully-connected processing. The system must store feature maps for all spatial positions, creating a different memory demand. A 224√ó224 input with 64 output channels requires storing 3.2 million activation values (224√ó224√ó64).</p>
<p>These memory access patterns suggest opportunities for optimization through weight reuse and careful feature map management. Processors optimize these spatial patterns by caching filter weights for reuse across positions while streaming feature map data. Frameworks implement spatial optimizations through specialized memory layouts that enable filter reuse and spatial locality in feature map access. CPUs and GPUs approach this differently. CPUs use their cache hierarchy to keep frequently used filters resident, while GPUs employ specialized memory architectures designed for the spatial access patterns of image processing. The detailed architecture design principles for these specialized processors are covered in <a href="ch017.xhtml#sec-ai-acceleration" class="quarto-xref">Chapter¬†11</a>.</p>
</section>
<section id="sec-dnn-architectures-computation-needs-22a5" class="level4">
<h4>Computation Needs</h4>
<p>The core computation in CNNs involves repeatedly applying small filters across spatial positions. Each output value requires a local multiply-accumulate operation over the filter region. For ImageNet processing with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> filters and 64 output channels, computing one spatial position involves 576 multiply-accumulates <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>√ó</mo><mn>3</mn><mo>√ó</mo><mn>64</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(3\times 3\times 64)</annotation></semantics></math>, and this must be repeated for all 50,176 spatial positions (224√ó224). While each individual computation involves fewer operations than an MLP layer, the total computational load remains large due to spatial repetition.</p>
<p>This computational pattern presents different optimization opportunities than MLPs. The regular, repeated nature of convolution operations enables efficient hardware utilization through structured parallelism. Modern processors exploit this pattern in various ways. CPUs leverage SIMD instructions<a href="#fn16" class="footnote-ref" id="fnref16" epub:type="noteref" role="doc-noteref">16</a> to process multiple filter positions simultaneously, while GPUs parallelize computation across spatial positions and channels. The model optimization techniques that further reduce these computational demands, including specialized convolution optimizations and sparsity patterns, are detailed in <a href="ch016.xhtml#sec-model-optimizations" class="quarto-xref">Chapter¬†10</a>.</p>
</section>
<section id="sec-dnn-architectures-data-movement-9a0e" class="level4">
<h4>Data Movement</h4>
<p>The sliding window pattern of convolutions creates a distinctive data movement profile. Unlike MLPs where each weight is used once per forward pass, CNN filter weights are reused many times as the filter slides across spatial positions. For ImageNet processing, each <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> filter weight is reused 50,176 times (once for each position in the 224√ó224 feature map). This creates a different challenge: the system must stream input features through the computation unit while keeping filter weights stable.</p>
<p>The predictable spatial access pattern enables strategic data movement optimizations. Different architectures handle this movement pattern through specialized mechanisms. CPUs maintain frequently used filter weights in cache while streaming through input features. GPUs employ memory architectures optimized for spatial locality and provide hardware support for efficient sliding window operations. Deep learning frameworks orchestrate these movements by organizing computations to maximize filter weight reuse and minimize redundant feature map accesses.</p>
</section>
</section>
</section>
<section id="sec-dnn-architectures-rnns-sequential-pattern-processing-ea14" class="level2">
<h2>RNNs: Sequential Pattern Processing</h2>
<p>Convolutional Neural Networks achieved efficiency gains by exploiting spatial locality, yet their architectural assumptions fail when patterns depend on temporal order rather than spatial proximity. While CNNs excel at recognizing "what" is present in data through shared feature detectors, they cannot capture "when" events occur or how they relate across time. This limitation manifests in domains such as natural language processing, where word meaning depends on sentential context, and time-series analysis, where future values depend on historical patterns.</p>
<p>Sequential data presents a challenge distinct from spatial processing: patterns can span arbitrary temporal distances, rendering fixed-size kernels ineffective. While spatial convolution leverages the principle that nearby pixels are typically related, temporal relationships operate differently. Important connections may span hundreds or thousands of time steps with no correlation to proximity. Traditional feedforward architectures, including CNNs, process each input independently and cannot maintain the temporal context necessary for these long-range dependencies.</p>
<p>Recurrent Neural Networks address this architectural limitation <span class="citation" data-cites="elman1990finding hochreiter1997long">(<a href="ch058.xhtml#ref-elman1990finding">Elman 1990</a>; <a href="ch058.xhtml#ref-hochreiter1997long">Hochreiter and Schmidhuber 1997</a>)</span> by embodying a temporal inductive bias: they assume sequential dependence, where the order of information matters and the past influences the present. This architectural assumption guides the introduction of memory as a component of the computational model. Rather than processing inputs in isolation, RNNs maintain an internal state that propagates information from previous time steps, enabling the network to condition its current output on historical context. This architecture embodies another trade-off: while CNNs sacrifice theoretical generality for spatial efficiency, RNNs introduce computational dependencies that challenge parallel execution in exchange for temporal processing capabilities.</p>
<div class="callout-definition" title="Recurrent Neural Networks">
<p><strong><em>Recurrent Neural Networks (RNNs)</em></strong> are sequential neural architectures that maintain <em>internal memory state</em> across time steps through <em>recurrent connections</em>, enabling <em>variable-length sequence processing</em> at the cost of <em>sequential computation</em> that prevents parallelization.</p>
</div>
<div title="Coverage Note">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Coverage Note</strong></p>
</div>
<div class="callout-content">
<p>This section covers of RNNs, emphasizing their core contributions to sequential processing and the architectural principles that influenced modern attention mechanisms. While RNNs introduced critical concepts‚Äîmemory states, temporal dependencies, and sequential computation‚Äîcontemporary practice increasingly favors attention-based architectures for sequence modeling. We focus on foundational principles rather than extensive implementation variants, dedicating significant depth to the attention mechanisms and Transformers (<a href="ch010.xhtml#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="quarto-xref">Section¬†4.5</a>) that have largely superseded RNNs in production systems while building directly on the insights gained from recurrent architectures.</p>
</div>
</div>
</div>
</div>
<section id="sec-dnn-architectures-pattern-processing-needs-c18e" class="level3">
<h3>Pattern Processing Needs</h3>
<p>Sequential pattern processing addresses scenarios where current input interpretation depends on preceding information. In natural language processing, word meaning often depends heavily on previous words in the sentence. Context determines interpretation, as evidenced by the varying meanings of words based on surrounding terms. Similarly, in speech recognition, phoneme interpretation depends on surrounding sounds, while financial forecasting requires understanding historical data patterns.</p>
<p>The challenge in sequential processing lies in maintaining and updating relevant context over time. Human text comprehension does not restart with each word; rather, a running understanding evolves as new information is processed. Similarly, time-series data processing encounters patterns spanning different timescales, from immediate dependencies to long-term trends. This necessitates an architecture capable of both maintaining state over time and updating it based on new inputs.</p>
<p>These requirements translate into specific architectural demands: the system must maintain internal state to capture temporal context, update this state based on new inputs, and learn which historical information is relevant for current predictions. Unlike MLPs and CNNs, which process fixed-size inputs, sequential processing must accommodate variable-length sequences while maintaining computational efficiency. These requirements culminate in the recurrent neural network (RNN) architecture.</p>
</section>
<section id="sec-dnn-architectures-algorithmic-structure-279b" class="level3">
<h3>Algorithmic Structure</h3>
<p>RNNs address sequential processing through recurrent connections, distinguishing them from MLPs and CNNs. Rather than merely mapping inputs to outputs, RNNs maintain an internal state updated at each time step, creating a memory mechanism that propagates information forward in time. This temporal dependency modeling capability was first explored by <span class="citation" data-cites="elman1990finding">Elman (<a href="ch058.xhtml#ref-elman1990finding">1990</a>)</span>, who demonstrated RNN capacity to identify structure in time-dependent data. Basic RNNs suffer from the vanishing gradient problem<a href="#fn17" class="footnote-ref" id="fnref17" epub:type="noteref" role="doc-noteref">17</a>, constraining their ability to learn long-term dependencies.</p>
<p>The core operation in a basic RNN can be expressed mathematically as: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ùê°</mi><mi>t</mi></msub><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ùêñ</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><msub><mi>ùê°</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>ùêñ</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><msub><mi>ùê±</mi><mi>t</mi></msub><mo>+</mo><msub><mi>ùêõ</mi><mi>h</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ùê°</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{h}_t</annotation></semantics></math> denotes the hidden state at time <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ùê±</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{x}_t</annotation></semantics></math> denotes the input at time <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ùêñ</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{W}_{hh}</annotation></semantics></math> contains the recurrent weights, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ùêñ</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{W}_{xh}</annotation></semantics></math> contains the input weights, as illustrated in the unfolded network structure in <a href="ch010.xhtml#fig-rnn" class="quarto-xref">Figure¬†4.4</a>.</p>
<p>In word sequence processing, each word may be represented as a 100-dimensional vector (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ùê±</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{x}_t</annotation></semantics></math>), with a hidden state of 128 dimensions (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ùê°</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{h}_t</annotation></semantics></math>). At each time step, the network combines the current input with its previous state to update its sequential understanding, establishing a memory mechanism capable of capturing patterns across time steps.</p>
<p>This recurrent structure fulfills sequential processing requirements through connections that maintain internal state and propagate information forward in time. Rather than processing all inputs independently, RNNs process sequential data by iteratively updating a hidden state based on the current input and the previous hidden state, as depicted in <a href="ch010.xhtml#fig-rnn" class="quarto-xref">Figure¬†4.4</a>. This architecture suits tasks including language modeling, speech recognition, and time-series forecasting.</p>
<p>RNNs implement a recursive algorithm where each time step‚Äôs function call depends on the result of the previous call. Analogous to recursive functions that maintain state through the call stack, RNNs maintain state through their hidden vectors. The mathematical formula <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ùê°</mi><mi>t</mi></msub><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ùê°</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>ùê±</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{h}_t = f(\mathbf{h}_{t-1}, \mathbf{x}_t)</annotation></semantics></math> directly parallels recursive function definitions where <code>f(n) = g(f(n-1), input(n))</code>. This correspondence explains RNN capacity to handle variable-length sequences: just as recursive algorithms process lists of arbitrary length by applying the same function recursively, RNNs process sequences of any length by applying the same recurrent computation.</p>
<section id="sec-dnn-architectures-efficiency-optimization-ce92" class="level4">
<h4>Efficiency and Optimization</h4>
<p>Sequential processing creates computational bottlenecks but enables unique efficiency characteristics for memory usage. RNNs achieve constant memory overhead for hidden state storage regardless of sequence length, making them extremely memory-efficient for long sequences. While Transformers require O(n¬≤) memory for sequence length n, RNNs maintain fixed memory usage, enabling processing of sequences thousands of steps long on modest hardware.</p>
<p>Structured pruning of hidden-to-hidden connections can achieve 10x speedup while maintaining sequence modeling capability. The recurrent weight matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><annotation encoding="application/x-tex">W_{hh}</annotation></semantics></math> typically dominates parameter count for large hidden states, but magnitude-based pruning reveals that 70-80% of these connections contribute minimally to temporal dependencies. Block-structured pruning maintains computational efficiency while enabling significant model compression.</p>
<p>Sequential operations accumulate quantization errors, requiring careful quantization point placement and gradient scaling for stable low-precision training. Unlike feedforward networks where quantization errors remain localized, RNN errors propagate through time, making INT8 quantization more challenging. Per-timestep quantization schemes and careful handling of hidden state precision are required for maintaining accuracy in quantized RNN deployments.</p>
<div id="fig-rnn" class="quarto-float quarto-figure quarto-figure-center" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-rnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../media/file57.svg" alt="" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure¬†4.4: <strong>Recurrent Neural Network Unfolding</strong>: Rnns process sequential data by maintaining a hidden state that incorporates information from previous time steps through this diagram. the unfolded structure explicitly represents the temporal dependencies modeled by the recurrent weights, enabling the network to learn patterns across variable-length sequences.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-dnn-architectures-computational-mapping-0096" class="level3">
<h3>Computational Mapping</h3>
<p>RNN sequential processing creates computational patterns different from both MLPs and CNNs, extending the architectural diversity discussed in <a href="ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section¬†4.1</a>. This implementation approach shows temporal dependencies translating into specific computational requirements.</p>
<p>As shown in <a href="ch010.xhtml#lst-rnn_layer_step" class="quarto-xref">Listing¬†4.5</a>, the <code>rnn_layer_step</code> function shows the operation using high-level matrix operations found in deep learning frameworks. It handles a single time step, taking the current input <code>x_t</code> and previous hidden state <code>h_prev</code>, along with two weight matrices: <code>W_hh</code> for hidden-to-hidden connections and <code>W_xh</code> for input-to-hidden connections. Through matrix multiplication operations (<code>matmul</code>), it merges the previous state and current input to generate the next hidden state.</p>
<div id="lst-rnn_layer_step" class="listing quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-lst">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-rnn_layer_step-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing¬†4.5: <strong>RNN Layer Step</strong>: Neural networks process sequential data through transformations that integrate current inputs and past states.
</figcaption>
<div aria-describedby="lst-rnn_layer_step-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">def</span> rnn_layer_step(x_t, h_prev, W_hh, W_xh, b):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="co"># x_t: input at time t (batch_size √ó input_dim)</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>    <span class="co"># h_prev: previous hidden state (batch_size √ó hidden_dim)</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>    <span class="co"># W_hh: recurrent weights (hidden_dim √ó hidden_dim)</span></span>
<span id="cb5-5"><a href="#cb5-5"></a>    <span class="co"># W_xh: input weights (input_dim √ó hidden_dim)</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>    h_t <span class="op">=</span> activation(matmul(h_prev, W_hh) <span class="op">+</span> matmul(x_t, W_xh) <span class="op">+</span> b)</span>
<span id="cb5-7"><a href="#cb5-7"></a>    <span class="cf">return</span> h_t</span></code></pre></div>
</div>
</figure>
</div>
<p>Understanding RNN system implications requires examining how the elegant mathematical abstraction translates into hardware execution patterns. The simple recurrence relation <code>h_t = tanh(W_hh h_{t-1} + W_xh x_t + b)</code> conceals a computational structure that creates unique challenges: sequential dependencies that prevent parallelization, memory access patterns that differ from feedforward networks, and state management requirements that affect system design.</p>
<p>The detailed implementation (<a href="ch010.xhtml#lst-rnn_layer_compute" class="quarto-xref">Listing¬†4.6</a>) reveals the computational reality beneath the mathematical abstraction. The nested loop structure exposes how sequential processing creates both limitations and opportunities in system optimization.</p>
<div id="lst-rnn_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-lst">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-rnn_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing¬†4.6: <strong>Recurrent Layer Computation</strong>: Computes the hidden state at each time step through sequential transformations involving previous states and current inputs.
</figcaption>
<div aria-describedby="lst-rnn_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">def</span> rnn_layer_compute(x_t, h_prev, W_hh, W_xh, b):</span>
<span id="cb6-2"><a href="#cb6-2"></a>    <span class="co"># Initialize next hidden state</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>    h_t <span class="op">=</span> np.zeros_like(h_prev)</span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a>    <span class="co"># Loop 1: Process each sequence in the batch</span></span>
<span id="cb6-6"><a href="#cb6-6"></a>    <span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb6-7"><a href="#cb6-7"></a>        <span class="co"># Loop 2: Compute recurrent contribution</span></span>
<span id="cb6-8"><a href="#cb6-8"></a>        <span class="co"># (h_prev √ó W_hh)</span></span>
<span id="cb6-9"><a href="#cb6-9"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-10"><a href="#cb6-10"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-11"><a href="#cb6-11"></a>                h_t[batch, i] <span class="op">+=</span> h_prev[batch, j] <span class="op">*</span> W_hh[j, i]</span>
<span id="cb6-12"><a href="#cb6-12"></a></span>
<span id="cb6-13"><a href="#cb6-13"></a>        <span class="co"># Loop 3: Compute input contribution (x_t √ó W_xh)</span></span>
<span id="cb6-14"><a href="#cb6-14"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-15"><a href="#cb6-15"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(input_dim):</span>
<span id="cb6-16"><a href="#cb6-16"></a>                h_t[batch, i] <span class="op">+=</span> x_t[batch, j] <span class="op">*</span> W_xh[j, i]</span>
<span id="cb6-17"><a href="#cb6-17"></a></span>
<span id="cb6-18"><a href="#cb6-18"></a>        <span class="co"># Loop 4: Add bias and apply activation</span></span>
<span id="cb6-19"><a href="#cb6-19"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-20"><a href="#cb6-20"></a>            h_t[batch, i] <span class="op">=</span> activation(h_t[batch, i] <span class="op">+</span> b[i])</span>
<span id="cb6-21"><a href="#cb6-21"></a></span>
<span id="cb6-22"><a href="#cb6-22"></a>    <span class="cf">return</span> h_t</span></code></pre></div>
</div>
</figure>
</div>
<p>The nested loops in <code>rnn_layer_compute</code> expose the core computational pattern of RNNs (see <a href="ch010.xhtml#lst-rnn_layer_compute" class="quarto-xref">Listing¬†4.6</a>). Loop 1 processes each sequence in the batch independently, allowing for batch-level parallelism. Within each batch item, Loop 2 computes how the previous hidden state influences the next state through the recurrent weights <code>W_hh</code>. Loop 3 then incorporates new information from the current input through the input weights <code>W_xh</code>. Finally, Loop 4 adds biases and applies the activation function to produce the new hidden state.</p>
<p>For a sequence processing task with input dimension 100 and hidden state dimension 128, each time step requires two matrix multiplications: one <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn><mo>√ó</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">128\times 128</annotation></semantics></math> for the recurrent connection and one <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>100</mn><mo>√ó</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">100\times 128</annotation></semantics></math> for the input projection. While individual time steps can process in parallel across batch elements, the time steps themselves must process sequentially. This creates a unique computational pattern that systems must handle.</p>
</section>
<section id="sec-dnn-architectures-system-implications-ecf5" class="level3">
<h3>System Implications</h3>
<p>Following the analytical framework established for MLPs, RNNs exhibit distinctive patterns in memory requirements, computation needs, and data movement that differ significantly from both dense and spatial processing architectures.</p>
<section id="sec-dnn-architectures-memory-requirements-eb37" class="level4">
<h4>Memory Requirements</h4>
<p>RNNs require storing two sets of weights (input-to-hidden and hidden-to-hidden) along with the hidden state. For the example with input dimension 100 and hidden state dimension 128, this requires storing 12,800 weights for input projection <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>100</mn><mo>√ó</mo><mn>128</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(100\times 128)</annotation></semantics></math> and 16,384 weights for recurrent connections <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>128</mn><mo>√ó</mo><mn>128</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(128\times 128)</annotation></semantics></math>. Unlike CNNs where weights are reused across spatial positions, RNN weights are reused across time steps. The system must maintain the hidden state, which constitutes a key factor in memory usage and access patterns.</p>
<p>These memory access patterns create a different profile from MLPs and CNNs. Processors optimize sequential patterns by maintaining weight matrices in cache while streaming through temporal elements. Frameworks optimize temporal processing by batching sequences and managing hidden state storage between time steps. CPUs and GPUs approach this through different strategies; CPUs leverage their cache hierarchy for weight reuse; meanwhile, GPUs use specialized memory architectures designed for maintaining state across sequential operations. The specialized hardware optimizations for sequential processing, including memory banking and pipeline architectures, are detailed in <a href="ch017.xhtml#sec-ai-acceleration" class="quarto-xref">Chapter¬†11</a>.</p>
</section>
<section id="sec-dnn-architectures-computation-needs-29be" class="level4">
<h4>Computation Needs</h4>
<p>The core computation in RNNs involves repeatedly applying weight matrices across time steps. For each time step, we perform two matrix multiplications: one with the input weights and one with the recurrent weights. In our example, processing a single time step requires 12,800 multiply-accumulates for the input projection <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>100</mn><mo>√ó</mo><mn>128</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(100\times 128)</annotation></semantics></math> and 16,384 multiply-accumulates for the recurrent connection <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>128</mn><mo>√ó</mo><mn>128</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(128\times 128)</annotation></semantics></math>.</p>
<p>This computational pattern differs from both MLPs and CNNs in a key way: while we can parallelize across batch elements, we cannot parallelize across time steps due to the sequential dependency. Each time step must wait for the previous step‚Äôs hidden state before it can begin computation. This creates a tension between the inherent sequential nature of the algorithm and the desire for parallel execution in modern hardware.</p>
<p>Processors address sequential constraints through specialized approaches. CPUs pipeline operations within time steps while maintaining temporal ordering. GPUs batch multiple sequences together to maintain high throughput despite sequential dependencies. Software frameworks optimize this further by techniques like sequence packing and unrolling computations across multiple time steps when possible, enabling more efficient utilization of parallel processing resources while respecting the sequential constraints inherent in recurrent architectures.</p>
</section>
<section id="sec-dnn-architectures-data-movement-8591" class="level4">
<h4>Data Movement</h4>
<p>The sequential processing in RNNs creates a distinctive data movement pattern that differs from both MLPs and CNNs. While MLPs need each weight only once per forward pass and CNNs reuse weights across spatial positions, RNNs reuse their weights across time steps while requiring careful management of the hidden state data flow.</p>
<p>For our example with a 128-dimensional hidden state, each time step must: load the previous hidden state (128 values), access both weight matrices (29,184 total weights from both input and recurrent connections), and store the new hidden state (128 values). This pattern repeats for every element in the sequence. Unlike CNNs where we can predict and prefetch data based on spatial patterns, RNN data movement is driven by temporal dependencies.</p>
<p>Different architectures handle this sequential data movement through specialized mechanisms. CPUs maintain weight matrices in cache while streaming through sequence elements and managing hidden state updates. GPUs employ memory architectures optimized for maintaining state information across sequential operations while processing multiple sequences in parallel. Deep learning frameworks orchestrate these movements by managing data transfers between time steps and optimizing batch operations.</p>
<p>While RNNs established concepts for sequential processing, their architectural constraints create bottlenecks: sequential dependencies prevent parallelization across time steps, fixed-capacity hidden states create information bottlenecks for long sequences, and temporal proximity assumptions break down when important relationships span distant positions. These limitations motivated the development of attention mechanisms, which eliminate sequential processing constraints through dynamic, content-dependent connectivity. The following section examines how attention mechanisms address each of these RNN limitations while introducing new computational challenges. This extensive treatment reflects attention mechanisms‚Äô dominance in modern ML systems and their fundamental reimagining of sequential pattern processing.</p>
</section>
</section>
</section>
<section id="sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="level2">
<h2>Attention Mechanisms: Dynamic Pattern Processing</h2>
<p>Recurrent Neural Networks successfully introduced memory to handle sequential dependencies, but their fixed sequential processing creates limitations. RNNs process information in temporal order, making it difficult to capture relationships between distant elements and impossible to parallelize computation across sequence positions. More critically, RNNs assume that temporal proximity correlates with importance‚Äîthat nearby words or time steps are more relevant than distant ones. This assumption breaks down in many real-world scenarios.</p>
<p>Consider the sentence "The cat, which was sitting by the window overlooking the garden, was sleeping." Here, "cat" and "sleeping" are separated by multiple intervening words, yet they form the core subject-predicate relationship. RNN architectures would process all the intervening elements sequentially, potentially losing this crucial connection in their fixed-capacity hidden state. This limitation revealed the need for architectures that could identify and weight relationships based on content rather than position.</p>
<p>Attention mechanisms emerged as the solution to this architectural constraint <span class="citation" data-cites="bahdanau2014neural">(<a href="ch058.xhtml#ref-bahdanau2014neural">Bahdanau, Cho, and Bengio 2014</a>)</span> by introducing dynamic connectivity patterns that adapt based on input content. Rather than processing elements in predetermined order with fixed relationships, attention mechanisms compute the relevance between all pairs of elements and weight their interactions accordingly. This represents a shift from structural constraints to learned, data-dependent processing patterns.</p>
<div class="callout-definition" title="Attention Mechanisms">
<p><strong><em>Attention Mechanisms</em></strong> are neural components that compute <em>content-dependent relationships</em> between sequence elements through <em>query-key-value operations</em>, enabling <em>selective focus</em> on relevant information and <em>long-range dependencies</em> without positional constraints.</p>
</div>
<p>While attention mechanisms were initially used as components within recurrent architectures, the Transformer architecture <span class="citation" data-cites="vaswani2017attention">(<a href="ch058.xhtml#ref-vaswani2017attention">Vaswani et al. 2017</a>)</span> demonstrated that attention alone could entirely replace sequential processing, creating a new architectural paradigm.</p>
<div class="callout-definition" title="Transformers">
<p><strong><em>Transformers</em></strong> are neural architectures based entirely on <em>attention mechanisms</em>, using <em>multi-head self-attention</em> and <em>position encodings</em> to process sequences in <em>parallel</em> rather than sequentially, enabling efficient training and inference at scale.</p>
</div>
<section id="sec-dnn-architectures-pattern-processing-needs-b5e0" class="level3">
<h3>Pattern Processing Needs</h3>
<p>Dynamic pattern processing addresses scenarios where relationships between elements are not fixed by architecture but instead emerge from content. Language translation exemplifies this challenge: when translating ‚Äúthe bank by the river,‚Äù understanding ‚Äúbank‚Äù requires attending to ‚Äúriver,‚Äù but in ‚Äúthe bank approved the loan,‚Äù the important relationship is with ‚Äúapproved‚Äù and ‚Äúloan.‚Äù Unlike RNNs that process information sequentially or CNNs that use fixed spatial patterns, an architecture is required that can dynamically determine which relationships matter.</p>
<p>Expanding beyond language, this requirement for dynamic processing appears across many domains. In protein structure prediction, interactions between amino acids depend on their chemical properties and spatial arrangements. In graph analysis, node relationships vary based on graph structure and node features. In document analysis, connections between different sections depend on semantic content rather than just proximity.</p>
<p>Synthesizing these requirements, dynamic processing demands specific capabilities from our processing architecture. The system must compute relationships between all pairs of elements, weigh these relationships based on content, and use these weights to selectively combine information. Unlike previous architectures with fixed connectivity patterns, dynamic processing requires the flexibility to modify its computation graph based on the input itself. These capabilities naturally lead us to the attention mechanism, which serves as the foundation for the Transformer architecture examined in detail in the following sections. <a href="ch010.xhtml#fig-transformer-attention-visualized" class="quarto-xref">Figure¬†4.5</a> shows attention enabling this dynamic information flow.</p>
<div id="fig-transformer-attention-visualized" class="quarto-float quarto-figure quarto-figure-center" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-transformer-attention-visualized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../media/file58.svg" alt="" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-attention-visualized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure¬†4.5: <strong>Attention Weights</strong>: Transformer attention mechanisms dynamically assess relationships between subwords, assigning higher weights to more relevant connections within a sequence and enabling the model to focus on key information. These learned weights, visualized as connection strengths, reveal how the model attends to different parts of the input when processing language.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-basic-attention-mechanism-9500" class="level3">
<h3>Basic Attention Mechanism</h3>
<p>Attention mechanisms represent a shift from fixed architectural connections to dynamic, content-based interactions between sequence elements. This section explores the mathematical foundations of attention, examining how query-key-value operations enable flexible pattern processing. We analyze the computational requirements, memory access patterns, and system implications that make attention both powerful and computationally demanding.</p>
<section id="sec-dnn-architectures-algorithmic-structure-1af4" class="level4">
<h4>Algorithmic Structure</h4>
<p>Attention mechanisms form the foundation of dynamic pattern processing by computing weighted connections between elements based on their content <span class="citation" data-cites="bahdanau2014neural">(<a href="ch058.xhtml#ref-bahdanau2014neural">Bahdanau, Cho, and Bengio 2014</a>)</span>. This approach processes relationships that are not fixed by architecture but instead emerge from the data itself. At the core of an attention mechanism lies an operation that can be expressed mathematically as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Attention</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùêê</mi><mo>,</mo><mi>ùêä</mi><mo>,</mo><mi>ùêï</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">softmax</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>ùêê</mi><msup><mi>ùêä</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>ùêï</mi></mrow><annotation encoding="application/x-tex">
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}
\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
</annotation></semantics></math></p>
<p>This equation shows scaled dot-product attention. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêê</mi><annotation encoding="application/x-tex">\mathbf{Q}</annotation></semantics></math> (queries) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêä</mi><annotation encoding="application/x-tex">\mathbf{K}</annotation></semantics></math> (keys) are matrix-multiplied to compute similarity scores, divided by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math> (key dimension) for numerical stability, then normalized with softmax<a href="#fn18" class="footnote-ref" id="fnref18" epub:type="noteref" role="doc-noteref">18</a> to get attention weights. These weights are applied to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêï</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics></math> (values) to produce the output. The result is a weighted combination where each position receives information from all relevant positions based on content similarity.</p>
<p>In this equation, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêê</mi><annotation encoding="application/x-tex">\mathbf{Q}</annotation></semantics></math> (queries), <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêä</mi><annotation encoding="application/x-tex">\mathbf{K}</annotation></semantics></math> (keys), and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêï</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics></math> (values)<a href="#fn19" class="footnote-ref" id="fnref19" epub:type="noteref" role="doc-noteref">19</a> represent learned projections of the input. For a sequence of length <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> with dimension <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>, this operation creates an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>√ó</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N\times N</annotation></semantics></math> attention matrix, determining how each position should attend to all others.</p>
<p>The attention operation involves several key steps. First, it computes query, key, and value projections for each position in the sequence. Next, it generates an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>√ó</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N\times N</annotation></semantics></math> attention matrix through query-key interactions. These steps are illustrated in <a href="ch010.xhtml#fig-attention" class="quarto-xref">Figure¬†4.6</a>. Finally, it uses these attention weights to combine value vectors, producing the output.</p>
<div id="fig-attention" class="quarto-float quarto-figure quarto-figure-center" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../media/file59.svg" alt="" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure¬†4.6: <strong>Query-Key-Value Interaction</strong>: Transformer attention mechanisms dynamically weigh input sequence elements by computing relationships between queries, keys, and values, enabling the model to focus on relevant information. these projections facilitate the creation of an attention matrix that determines the contribution of each value vector to the final output, effectively capturing contextual dependencies within the sequence. Source: <a href="HTTPS://poloclub.GitHub.io/transformer-explainer/">transformer explainer</a>.
</figcaption>
</figure>
</div>
<p>The key is that, unlike the fixed weight matrices found in previous architectures, as shown in <a href="ch010.xhtml#fig-attention-weightcalc" class="quarto-xref">Figure¬†4.7</a>, these attention weights are computed dynamically for each input. This allows the model to adapt its processing based on the dynamic content at hand.</p>
<div id="fig-attention-weightcalc" class="quarto-float quarto-figure quarto-figure-center" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-attention-weightcalc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../media/file60.svg" alt="" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-weightcalc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure¬†4.7: <strong>Dynamic Attention Weights</strong>: Transformer models calculate attention weights dynamically based on the relationships between query, key, and value vectors, allowing the model to focus on relevant parts of the input sequence for each processing step. this contrasts with fixed-weight architectures and enables adaptive pattern processing for handling variable-length inputs and complex dependencies. Source: <a href="HTTPS://poloclub.GitHub.io/transformer-explainer/">transformer explainer</a>.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-computational-mapping-6c7e" class="level4">
<h4>Computational Mapping</h4>
<p>Attention mechanisms create computational patterns that differ significantly from previous architectures. The implementation approach shown in <a href="ch010.xhtml#lst-attention_layer_compute" class="quarto-xref">Listing¬†4.7</a> shows dynamic connectivity translating into specific computational requirements.</p>
<div id="lst-attention_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-lst">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-attention_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing¬†4.7: <strong>Attention Mechanism</strong>: Transformer models compute attention through query-key-value interactions, enabling dynamic focus across input sequences for improved language understanding.
</figcaption>
<div aria-describedby="lst-attention_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">def</span> attention_layer_matrix(Q, K, V):</span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="co"># Q, K, V: (batch_size √ó seq_len √ó d_model)</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>    scores <span class="op">=</span> matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> sqrt(</span>
<span id="cb7-4"><a href="#cb7-4"></a>        d_k</span>
<span id="cb7-5"><a href="#cb7-5"></a>    )  <span class="co"># Compute attention scores</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>    weights <span class="op">=</span> softmax(scores)  <span class="co"># Normalize scores</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>    output <span class="op">=</span> matmul(weights, V)  <span class="co"># Combine values</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>    <span class="cf">return</span> output</span>
<span id="cb7-9"><a href="#cb7-9"></a></span>
<span id="cb7-10"><a href="#cb7-10"></a></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="co"># Core computational pattern</span></span>
<span id="cb7-12"><a href="#cb7-12"></a><span class="kw">def</span> attention_layer_compute(Q, K, V):</span>
<span id="cb7-13"><a href="#cb7-13"></a>    <span class="co"># Initialize outputs</span></span>
<span id="cb7-14"><a href="#cb7-14"></a>    scores <span class="op">=</span> np.zeros((batch_size, seq_len, seq_len))</span>
<span id="cb7-15"><a href="#cb7-15"></a>    outputs <span class="op">=</span> np.zeros_like(V)</span>
<span id="cb7-16"><a href="#cb7-16"></a></span>
<span id="cb7-17"><a href="#cb7-17"></a>    <span class="co"># Loop 1: Process each sequence in batch</span></span>
<span id="cb7-18"><a href="#cb7-18"></a>    <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb7-19"><a href="#cb7-19"></a>        <span class="co"># Loop 2: Compute attention for each query position</span></span>
<span id="cb7-20"><a href="#cb7-20"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-21"><a href="#cb7-21"></a>            <span class="co"># Loop 3: Compare with each key position</span></span>
<span id="cb7-22"><a href="#cb7-22"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-23"><a href="#cb7-23"></a>                <span class="co"># Compute attention score</span></span>
<span id="cb7-24"><a href="#cb7-24"></a>                <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(d_model):</span>
<span id="cb7-25"><a href="#cb7-25"></a>                    scores[b, i, j] <span class="op">+=</span> Q[b, i, d] <span class="op">*</span> K[b, j, d]</span>
<span id="cb7-26"><a href="#cb7-26"></a>                scores[b, i, j] <span class="op">/=</span> sqrt(d_k)</span>
<span id="cb7-27"><a href="#cb7-27"></a></span>
<span id="cb7-28"><a href="#cb7-28"></a>        <span class="co"># Apply softmax to scores</span></span>
<span id="cb7-29"><a href="#cb7-29"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-30"><a href="#cb7-30"></a>            scores[b, i] <span class="op">=</span> softmax(scores[b, i])</span>
<span id="cb7-31"><a href="#cb7-31"></a></span>
<span id="cb7-32"><a href="#cb7-32"></a>        <span class="co"># Loop 4: Combine values using attention weights</span></span>
<span id="cb7-33"><a href="#cb7-33"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-34"><a href="#cb7-34"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-35"><a href="#cb7-35"></a>                <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(d_model):</span>
<span id="cb7-36"><a href="#cb7-36"></a>                    outputs[b, i, d] <span class="op">+=</span> scores[b, i, j] <span class="op">*</span> V[b, j, d]</span>
<span id="cb7-37"><a href="#cb7-37"></a></span>
<span id="cb7-38"><a href="#cb7-38"></a>    <span class="cf">return</span> outputs</span></code></pre></div>
</div>
</figure>
</div>
<p>The translation from attention‚Äôs mathematical elegance to hardware execution reveals the computational price of dynamic connectivity. While the attention equation <code>Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V</code> appears as a straightforward matrix operation, the physical implementation requires orchestrating quadratic numbers of pairwise computations that create different system demands than previous architectures.</p>
<p>The nested loops in <code>attention_layer_compute</code> expose attention‚Äôs true computational signature (see <a href="ch010.xhtml#lst-attention_layer_compute" class="quarto-xref">Listing¬†4.7</a>). The first loop processes each sequence in the batch independently. The second and third loops compute attention scores between all pairs of positions, creating the quadratic computation pattern that makes attention both powerful and computationally demanding. The fourth loop uses these attention weights to combine values from all positions, completing the dynamic connectivity pattern that defines attention mechanisms.</p>
</section>
<section id="sec-dnn-architectures-system-implications-b5aa" class="level4">
<h4>System Implications</h4>
<p>Attention mechanisms exhibit distinctive system-level patterns that differ from previous architectures through their dynamic connectivity requirements.</p>
<section id="sec-dnn-architectures-memory-requirements-3dc1" class="level5">
<h5>Memory Requirements</h5>
<p>In terms of memory requirements, attention mechanisms necessitate storage for attention weights, key-query-value projections, and intermediate feature representations. For a sequence length <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> and dimension d, each attention layer must store an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>√ó</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N\times N</annotation></semantics></math> attention weight matrix for each sequence in the batch, three sets of projection matrices for queries, keys, and values (each sized <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>√ó</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">d\times d</annotation></semantics></math>), and input and output feature maps of size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>√ó</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">N\times d</annotation></semantics></math>. The dynamic generation of attention weights for every input creates a memory access pattern where intermediate attention weights become a significant factor in memory usage.</p>
</section>
<section id="sec-dnn-architectures-computation-needs-a41e" class="level5">
<h5>Computation Needs</h5>
<p>Computation needs in attention mechanisms center around two main phases: generating attention weights and applying them to values. For each attention layer, the system performs many multiply-accumulate operations across multiple computational stages. The query-key interactions alone require <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>√ó</mo><mi>N</mi><mo>√ó</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">N\times N\times d</annotation></semantics></math> multiply-accumulates, with an equal number needed for applying attention weights to values. Additional computations are required for the projection matrices and softmax operations. This computational pattern differs from previous architectures due to its quadratic scaling with sequence length and the need to perform fresh computations for each input.</p>
</section>
<section id="sec-dnn-architectures-data-movement-12b1" class="level5">
<h5>Data Movement</h5>
<p>Data movement in attention mechanisms presents unique challenges. Each attention operation involves projecting and moving query, key, and value vectors for each position, storing and accessing the full attention weight matrix, and coordinating the movement of value vectors during the weighted combination phase. This creates a data movement pattern where intermediate attention weights become a major factor in system bandwidth requirements. Unlike the more predictable access patterns of CNNs or the sequential access of RNNs, attention operations require frequent movement of dynamically computed weights across the memory hierarchy.</p>
<p>These distinctive characteristics of attention mechanisms in terms of memory, computation, and data movement have significant implications for system design and optimization, setting the stage for the development of more advanced architectures like Transformers.</p>
</section>
</section>
</section>
<section id="sec-dnn-architectures-transformers-attentiononly-architecture-c4f0" class="level3">
<h3>Transformers: Attention-Only Architecture</h3>
<p>While attention mechanisms introduced the concept of dynamic pattern processing, they were initially applied as additions to existing architectures, particularly RNNs for sequence-to-sequence tasks. This hybrid approach still suffered from the fundamental limitations of recurrent architectures: sequential processing constraints that prevented efficient parallelization and difficulties with very long sequences. The breakthrough insight was recognizing that attention mechanisms alone could replace both convolutional and recurrent processing entirely.</p>
<p>Transformers, introduced in the landmark "Attention is All You Need" paper<a href="#fn20" class="footnote-ref" id="fnref20" epub:type="noteref" role="doc-noteref">20</a> by <span class="citation" data-cites="vaswani2017attention">Vaswani et al. (<a href="ch058.xhtml#ref-vaswani2017attention">2017</a>)</span>, embody a revolutionary inductive bias: <strong>they assume no prior structure but allow the model to learn all pairwise relationships dynamically based on content</strong>. This architectural assumption represents the culmination of the architectural evolution detailed in <a href="ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section¬†4.1</a> by eliminating all structural constraints in favor of pure content-dependent processing. Rather than adding attention to RNNs, Transformers built the entire architecture around attention mechanisms, introducing self-attention as the primary computational pattern. This architectural decision traded the parameter efficiency of CNNs and the sequential coherence of RNNs for maximum flexibility and parallelizability.</p>
<p>This represents the final step in our architectural journey: from MLPs that connected everything to everything, to CNNs that connected locally, to RNNs that connected sequentially, to Transformers that connect dynamically based on learned content relationships. Each evolution sacrificed constraints for capabilities, with Transformers achieving maximum expressivity at the computational cost established in <a href="ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section¬†4.1</a>.</p>
<section id="sec-dnn-architectures-algorithmic-structure-9d4b" class="level4">
<h4>Algorithmic Structure</h4>
<p>The key innovation in Transformers lies in their use of self-attention layers. In the self-attention mechanism used by Transformers, the Query, Key, and Value vectors are all derived from the same input sequence. This is the key distinction from earlier attention mechanisms where the query might come from a decoder while the keys and values came from an encoder. By making all components self-referential, self-attention allows the model to weigh the importance of different positions within the same sequence when encoding each position. For instance, in processing the sentence ‚ÄúThe animal didn‚Äôt cross the street because it was too wide,‚Äù self-attention allows the model to link ‚Äúit‚Äù with ‚Äústreet,‚Äù capturing long-range dependencies that are challenging for traditional sequential models.</p>
<p>The self-attention mechanism can be expressed mathematically in a form similar to the basic attention mechanism: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">SelfAttention</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùêó</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">softmax</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mrow><mi>ùêó</mi><msub><mi>ùêñ</mi><mi>ùêê</mi></msub></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mi>ùêó</mi><msub><mi>ùêñ</mi><mi>ùêä</mi></msub></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>ùêó</mi><msub><mi>ùêñ</mi><mi>ùêï</mi></msub></mrow></mrow><annotation encoding="application/x-tex">
\text{SelfAttention}(\mathbf{X}) = \text{softmax}
\left(\frac{\mathbf{XW_Q}(\mathbf{XW_K})^T}{\sqrt{d_k}}\right)\mathbf{XW_V}
</annotation></semantics></math></p>
<p>Here, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêó</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> is the input sequence, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ùêñ</mi><mi>ùêê</mi></msub><annotation encoding="application/x-tex">\mathbf{W_Q}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ùêñ</mi><mi>ùêä</mi></msub><annotation encoding="application/x-tex">\mathbf{W_K}</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ùêñ</mi><mi>ùêï</mi></msub><annotation encoding="application/x-tex">\mathbf{W_V}</annotation></semantics></math> are learned weight matrices for queries, keys, and values respectively. This formulation highlights how self-attention derives all its components from the same input, creating a dynamic, content-dependent processing pattern.</p>
<p>Building on this foundation, Transformers employ multi-head attention, which extends the self-attention mechanism by running multiple attention functions in parallel. Each ‚Äúhead‚Äù involves a separate set of query/key/value projections that can focus on different aspects of the input, allowing the model to jointly attend to information from different representation subspaces. This multi-head structure provides the model with a richer representational capability, enabling it to capture various types of relationships within the data simultaneously.</p>
<p>The mathematical formulation for multi-head attention is: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">MultiHead</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùêê</mi><mo>,</mo><mi>ùêä</mi><mo>,</mo><mi>ùêï</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Concat</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="normal">head</mtext><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mtext mathvariant="normal">head</mtext><mi>h</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>ùêñ</mi><mi>O</mi></msup></mrow><annotation encoding="application/x-tex">
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}^O
</annotation></semantics></math> where each attention head is computed as: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">head</mtext><mi>i</mi></msub><mo>=</mo><mtext mathvariant="normal">Attention</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùêê</mi><msubsup><mi>ùêñ</mi><mi>i</mi><mi>Q</mi></msubsup><mo>,</mo><mi>ùêä</mi><msubsup><mi>ùêñ</mi><mi>i</mi><mi>K</mi></msubsup><mo>,</mo><mi>ùêï</mi><msubsup><mi>ùêñ</mi><mi>i</mi><mi>V</mi></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)
</annotation></semantics></math></p>
<p>A critical component in both self-attention and multi-head attention is the scaling factor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math>, which serves an important mathematical purpose. This factor prevents the dot products from growing too large, which would push the softmax function into regions with extremely small gradients. For queries and keys of dimension <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding="application/x-tex">d_k</annotation></semantics></math>, their dot product has variance <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding="application/x-tex">d_k</annotation></semantics></math>, so dividing by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math> normalizes the variance to 1, maintaining stable gradients and enabling effective learning.<a href="#fn21" class="footnote-ref" id="fnref21" epub:type="noteref" role="doc-noteref">21</a></p>
<p>Beyond the mathematical mechanics, attention mechanisms can be understood conceptually as implementing a form of content-addressable memory system. Like hash tables that retrieve values based on key matching, attention computes similarity between a query and all available keys, then retrieves a weighted combination of corresponding values. The dot product similarity <code>Q¬∑K</code> functions like a hash function that measures how well each key matches the query. The softmax normalization ensures the weights sum to 1, implementing a probabilistic retrieval mechanism. This connection explains why attention proves effective for tasks requiring flexible information retrieval‚Äîit provides a differentiable approximation to database lookup operations.</p>
<p>From an information-theoretic perspective, attention mechanisms implement optimal information aggregation under uncertainty. The attention weights represent uncertainty about which parts of the input contain relevant information for the current processing step. The softmax operation implements a maximum entropy principle: among all possible ways to distribute attention across input positions, softmax selects the distribution with maximum entropy subject to the constraint that similarity scores determine relative importance <span class="citation" data-cites="cover2006elements">(<a href="ch058.xhtml#ref-cover2006elements">Cover and Thomas 2001</a>)</span>.</p>
</section>
<section id="sec-dnn-architectures-efficiency-optimization-94d7" class="level4">
<h4>Efficiency and Optimization</h4>
<p>Attention mechanisms are highly redundant, with many heads learning similar patterns. Head pruning and low-rank attention factorization can reduce computation by 50-80% with careful implementation. Analysis of large Transformer models reveals that most attention heads fall into a few common patterns (positional, syntactic, semantic), suggesting that explicit architectural specialization could replace learned redundancy.</p>
<p>Attention operations are particularly sensitive to quantization due to the softmax operation and the quadratic number of attention scores. Separate quantization schemes for Q, K, V projections and careful handling of softmax operations are required for stable quantization. Post-training INT8 quantization typically achieves 2-3% accuracy loss, while INT4 quantization requires more sophisticated quantization-aware training approaches.</p>
<p>The quadratic scaling with sequence length creates efficiency limitations. Sparse attention patterns (such as local windows, strided patterns, or learned sparsity) can reduce complexity from O(n¬≤) to O(n log n) or O(n) while maintaining most modeling capability. Linear attention approximations trade some expressive power for linear scaling, enabling processing of much longer sequences on limited hardware.</p>
<p>This information-theoretic interpretation reveals why attention is so effective for selective processing. The mechanism automatically balances two competing objectives: focusing on the most relevant information (minimizing entropy) while maintaining sufficient breadth to avoid missing important details (maximizing entropy). The attention pattern emerges as the optimal trade-off between these objectives, explaining why transformers can effectively handle long sequences and complex dependencies.</p>
<p>Self-attention learns dynamic activation patterns across the input sequence. Unlike CNNs which apply fixed filters or RNNs which use fixed recurrence patterns, attention learns which elements should activate together based on their content. This creates a form of adaptive connectivity where the effective network topology changes for each input. Recent research has shown that attention heads in trained models often specialize in detecting specific linguistic or semantic patterns <span class="citation" data-cites="clark2019what">(<a href="ch058.xhtml#ref-clark2019what">Clark et al. 2019</a>)</span>, suggesting that the mechanism naturally discovers interpretable structural regularities in data.</p>
<p>The Transformer architecture leverages this self-attention mechanism within a broader structure that typically includes feed-forward layers, layer normalization, and residual connections (see <a href="ch010.xhtml#fig-transformer" class="quarto-xref">Figure¬†4.8</a>). This combination allows Transformers to process input sequences in parallel, capturing complex dependencies without the need for sequential computation. As a result, Transformers have demonstrated significant effectiveness across a wide range of tasks, from natural language processing to computer vision, transforming deep learning architectures across domains.</p>
<div id="fig-transformer" class="quarto-float quarto-figure quarto-figure-center" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../media/file61.svg" alt="" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure¬†4.8: <strong>Attention Head</strong>: Neural networks compute attention through query-key-value interactions, enabling dynamic focus across subwords for improved sentence understanding. Source: Attention Is All You Need.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-computational-mapping-7fe9" class="level4">
<h4>Computational Mapping</h4>
<p>While Transformer self-attention builds upon the basic attention mechanism, it introduces distinct computational patterns that set it apart. To understand these patterns, we must examine the typical implementation of self-attention in Transformers (see <a href="ch010.xhtml#lst-self_attention_layer" class="quarto-xref">Listing¬†4.8</a>):</p>
<div id="lst-self_attention_layer" class="listing quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-lst">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-self_attention_layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing¬†4.8: <strong>Self-Attention Mechanism</strong>: Transformer models compute attention through query-key-value interactions, enabling dynamic focus across input sequences for improved language understanding.
</figcaption>
<div aria-describedby="lst-self_attention_layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">def</span> self_attention_layer(X, W_Q, W_K, W_V, d_k):</span>
<span id="cb8-2"><a href="#cb8-2"></a>    <span class="co"># X: input tensor (batch_size √ó seq_len √ó d_model)</span></span>
<span id="cb8-3"><a href="#cb8-3"></a>    <span class="co"># W_Q, W_K, W_V: weight matrices (d_model √ó d_k)</span></span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a>    Q <span class="op">=</span> matmul(X, W_Q)</span>
<span id="cb8-6"><a href="#cb8-6"></a>    K <span class="op">=</span> matmul(X, W_K)</span>
<span id="cb8-7"><a href="#cb8-7"></a>    V <span class="op">=</span> matmul(X, W_V)</span>
<span id="cb8-8"><a href="#cb8-8"></a></span>
<span id="cb8-9"><a href="#cb8-9"></a>    scores <span class="op">=</span> matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> sqrt(d_k)</span>
<span id="cb8-10"><a href="#cb8-10"></a>    attention_weights <span class="op">=</span> softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-11"><a href="#cb8-11"></a>    output <span class="op">=</span> matmul(attention_weights, V)</span>
<span id="cb8-12"><a href="#cb8-12"></a></span>
<span id="cb8-13"><a href="#cb8-13"></a>    <span class="cf">return</span> output</span>
<span id="cb8-14"><a href="#cb8-14"></a></span>
<span id="cb8-15"><a href="#cb8-15"></a></span>
<span id="cb8-16"><a href="#cb8-16"></a><span class="kw">def</span> multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads, d_k):</span>
<span id="cb8-17"><a href="#cb8-17"></a>    outputs <span class="op">=</span> []</span>
<span id="cb8-18"><a href="#cb8-18"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_heads):</span>
<span id="cb8-19"><a href="#cb8-19"></a>        head_output <span class="op">=</span> self_attention_layer(</span>
<span id="cb8-20"><a href="#cb8-20"></a>            X, W_Q[i], W_K[i], W_V[i], d_k</span>
<span id="cb8-21"><a href="#cb8-21"></a>        )</span>
<span id="cb8-22"><a href="#cb8-22"></a>        outputs.append(head_output)</span>
<span id="cb8-23"><a href="#cb8-23"></a></span>
<span id="cb8-24"><a href="#cb8-24"></a>    concat_output <span class="op">=</span> torch.cat(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-25"><a href="#cb8-25"></a>    final_output <span class="op">=</span> matmul(concat_output, W_O)</span>
<span id="cb8-26"><a href="#cb8-26"></a></span>
<span id="cb8-27"><a href="#cb8-27"></a>    <span class="cf">return</span> final_output</span></code></pre></div>
</div>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-system-implications-76dd" class="level4">
<h4>System Implications</h4>
<p>This implementation reveals key computational characteristics that apply to basic attention mechanisms, with Transformer self-attention representing a specific case. First, self-attention enables parallel processing across all positions in the sequence. This is evident in the matrix multiplications that compute <code>Q</code>, <code>K</code>, and <code>V</code> simultaneously for all positions. Unlike recurrent architectures that process inputs sequentially, this parallel nature allows for more efficient computation, especially on modern hardware designed for parallel operations.</p>
<p>Second, the attention score computation results in a matrix of size <code>(seq_len √ó seq_len)</code>, leading to quadratic complexity with respect to sequence length. This quadratic relationship becomes a significant computational bottleneck when processing long sequences, a challenge that has spurred research into more efficient attention mechanisms.</p>
<p>Third, the multi-head attention mechanism effectively runs multiple self-attention operations in parallel, each with its own set of learned projections. While this increases the computational load linearly with the number of heads, it allows the model to capture different types of relationships within the same input, enhancing the model‚Äôs representational power.</p>
<p>Fourth, the core computations in self-attention are dominated by large matrix multiplications. For a sequence of length <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> and embedding dimension <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>, the main operations involve matrices of sizes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>√ó</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(N\times d)</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mo>√ó</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(d\times d)</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>√ó</mo><mi>N</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(N\times N)</annotation></semantics></math>. These intensive matrix operations are well-suited for acceleration on specialized hardware like GPUs, but they also contribute significantly to the overall computational cost of the model.</p>
<p>Finally, self-attention generates memory-intensive intermediate results. The attention weights matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>√ó</mo><mi>N</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(N\times N)</annotation></semantics></math> and the intermediate results for each attention head create large memory requirements, especially for long sequences. This can pose challenges for deployment on memory-constrained devices and necessitates careful memory management in implementations.</p>
<p>These computational patterns create a unique profile for Transformer self-attention, distinct from previous architectures. The parallel nature of the computations makes Transformers well-suited for modern parallel processing hardware, but the quadratic complexity with sequence length poses challenges for processing long sequences. As a result, much research has focused on developing optimization techniques, such as sparse attention patterns or low-rank approximations, to address these challenges. Each of these optimizations presents its own trade-offs between computational efficiency and model expressiveness, a balance that must be carefully considered in practical applications.</p>
<p>This examination of four distinct architectural families reveals both their individual characteristics and their collective evolution. Rather than viewing these architectures in isolation, a deeper understanding emerges when we consider how they relate to each other and build upon shared foundations.</p>
</section>
</section>
</section>
<section id="sec-dnn-architectures-architectural-building-blocks-a575" class="level2">
<h2>Architectural Building Blocks</h2>
<p>Having examined four major architectural families‚ÄîMLPs, CNNs, RNNs, and Transformers‚Äîeach with distinct computational characteristics and system implications, a unifying perspective emerges. Deep learning architectures, while presented as distinct approaches in previous sections, are better understood as compositions of building blocks that evolved over time. Like complex LEGO structures built from basic bricks, modern neural networks combine and iterate on core computational patterns that emerged through decades of research <span class="citation" data-cites="lecun2015deep">(<a href="ch058.xhtml#ref-lecun2015deep">Yann LeCun, Bengio, and Hinton 2015</a>)</span>. Each architectural innovation introduced new building blocks while discovering novel applications of existing ones.</p>
<p>These building blocks and their evolution illuminate modern architectural design. The simple perceptron <span class="citation" data-cites="rosenblatt1958perceptron">(<a href="ch058.xhtml#ref-rosenblatt1958perceptron">Rosenblatt 1958</a>)</span> evolved into multi-layer networks <span class="citation" data-cites="rumelhart1986learning">(<a href="ch058.xhtml#ref-rumelhart1986learning">Rumelhart, Hinton, and Williams 1986</a>)</span>, which subsequently spawned specialized patterns for spatial and sequential processing. Each advancement preserved useful elements from predecessors while introducing new computational primitives. Contemporary architectures, such as Transformers, represent carefully engineered combinations of these building blocks.</p>
<p>This progression reveals both the evolution of neural networks and the discovery and refinement of core computational patterns that remain relevant. Building on the architectural progression outlined in <a href="ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section¬†4.1</a>, each new architecture introduces distinct computational demands and system-level challenges.</p>
<p><a href="ch010.xhtml#tbl-dl-evolution" class="quarto-xref">Table¬†4.1</a> summarizes this evolution, highlighting the key primitives and system focus for each era of deep learning development. This table captures the major shifts in deep learning architecture design and corresponding changes in system-level considerations. The progression spans from early dense matrix operations optimized for CPUs, through convolutions leveraging GPU acceleration and sequential operations necessitating sophisticated memory hierarchies, to the current era of attention mechanisms requiring flexible accelerators and high-bandwidth memory.</p>
<div id="tbl-dl-evolution" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dl-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table¬†4.1: <strong>Deep Learning Evolution</strong>: Neural network architectures have progressed from simple, fully connected layers to complex models leveraging specialized hardware and addressing sequential data dependencies. This table maps architectural eras to key computational primitives and corresponding system-level optimizations, revealing a historical trend toward increased parallelism and memory bandwidth requirements.
</figcaption>
<div aria-describedby="tbl-dl-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top" style="width:98%;">
<colgroup>
<col style="width: 24%" />
<col style="width: 28%" />
<col style="width: 21%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Era</strong></th>
<th style="text-align: left;"><strong>Dominant Architecture</strong></th>
<th style="text-align: left;"><strong>Key Primitives</strong></th>
<th style="text-align: left;"><strong>System Focus</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Early NN</strong></td>
<td style="text-align: left;">MLP</td>
<td style="text-align: left;">Dense Matrix Ops</td>
<td style="text-align: left;">CPU optimization</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>CNN Revolution</strong></td>
<td style="text-align: left;">CNN</td>
<td style="text-align: left;">Convolutions</td>
<td style="text-align: left;">GPU acceleration</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Sequence Modeling</strong></td>
<td style="text-align: left;">RNN</td>
<td style="text-align: left;">Sequential Ops</td>
<td style="text-align: left;">Memory hierarchies</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Attention Era</strong></td>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">Attention, Dynamic Compute</td>
<td style="text-align: left;">Flexible accelerators, High-bandwidth memory</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Examination of these building blocks shows primitives evolving and combining to create increasingly powerful neural network architectures.</p>
<section id="sec-dnn-architectures-evolution-perceptron-multilayer-networks-0f3f" class="level3">
<h3>Evolution from Perceptron to Multi-Layer Networks</h3>
<p>While we examined MLPs in <a href="ch010.xhtml#sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="quarto-xref">Section¬†4.2</a> as a mechanism for dense pattern processing, here we focus on how they established building blocks that appear throughout deep learning. The evolution from perceptron to MLP introduced several key concepts: the power of layer stacking, the importance of non-linear transformations, and the basic feedforward computation pattern.</p>
<p>The introduction of hidden layers between input and output created a template for feature transformation that appears in virtually every modern architecture. Even in sophisticated networks like Transformers, we find MLP-style feedforward layers performing feature processing. The concept of transforming data through successive non-linear layers has become a paradigm that transcends specific architecture types.</p>
<p>Most significantly, the development of MLPs established the backpropagation algorithm<a href="#fn22" class="footnote-ref" id="fnref22" epub:type="noteref" role="doc-noteref">22</a>, which to this day remains the cornerstone of neural network optimization. This key contribution has enabled the development of deep architectures and influenced how later architectures would be designed to maintain gradient flow.</p>
<p>These building blocks, layered feature transformation, non-linear activation, and gradient-based learning, set the foundation for more specialized architectures. Subsequent innovations often focused on structuring these basic components in new ways rather than replacing them entirely.</p>
</section>
<section id="sec-dnn-architectures-evolution-dense-spatial-processing-1d3b" class="level3">
<h3>Evolution from Dense to Spatial Processing</h3>
<p>The development of CNNs marked an architectural innovation, specifically the realization that we could specialize the dense connectivity of MLPs for spatial patterns. While retaining the core concept of layer-wise processing, CNNs introduced several building blocks that would influence all future architectures.</p>
<p>The first key innovation was the concept of parameter sharing. Unlike MLPs where each connection had its own weight, CNNs showed how the same parameters could be reused across different parts of the input. This not only made the networks more efficient but introduced the powerful idea that architectural structure could encode useful priors about the data <span class="citation" data-cites="lecun1998gradient">(<a href="ch058.xhtml#ref-lecun1998gradient">Lecun et al. 1998</a>)</span>.</p>
<p>Perhaps even more influential was the introduction of skip connections through ResNets<a href="#fn23" class="footnote-ref" id="fnref23" epub:type="noteref" role="doc-noteref">23</a> <span class="citation" data-cites="he2016deep">(<a href="ch058.xhtml#ref-he2016deep">K. He et al. 2015</a>)</span>. Originally they were designed to help train very deep CNNs, skip connections have become a building block that appears in virtually every modern architecture. They showed how direct paths through the network could help gradient flow and information propagation, a concept now central to Transformer designs.</p>
<p>CNNs also introduced batch normalization, a technique for stabilizing neural network optimization by normalizing intermediate features <span class="citation" data-cites="ioffe2015batch">(<a href="ch058.xhtml#ref-ioffe2015batch">Ioffe and Szegedy 2015a</a>)</span>. This concept of feature normalization, while originating in CNNs, evolved into layer normalization and is now a key component in modern architectures.</p>
<p>These innovations, such as parameter sharing, skip connections, and normalization, transcended their origins in spatial processing to become essential building blocks in the deep learning toolkit.</p>
</section>
<section id="sec-dnn-architectures-evolution-sequence-processing-8a78" class="level3">
<h3>Evolution of Sequence Processing</h3>
<p>While CNNs specialized MLPs for spatial patterns, sequence models adapted neural networks for temporal dependencies. RNNs introduced the concept of maintaining and updating state, a building block that influenced how networks could process sequential information, <span class="citation" data-cites="elman1990finding">(<a href="ch058.xhtml#ref-elman1990finding">Elman 1990</a>)</span>.</p>
<p>The development of LSTMs<a href="#fn24" class="footnote-ref" id="fnref24" epub:type="noteref" role="doc-noteref">24</a> and GRUs<a href="#fn25" class="footnote-ref" id="fnref25" epub:type="noteref" role="doc-noteref">25</a> brought sophisticated gating mechanisms to neural networks <span class="citation" data-cites="hochreiter1997long cho2014properties">(<a href="ch058.xhtml#ref-hochreiter1997long">Hochreiter and Schmidhuber 1997</a>; <a href="ch058.xhtml#ref-cho2014properties">Cho et al. 2014</a>)</span>. These gates, themselves small MLPs, showed how simple feedforward computations could be composed to control information flow. This concept of using neural networks to modulate other neural networks became a recurring pattern in architecture design.</p>
<p>Perhaps most significantly, sequence models demonstrated the power of adaptive computation paths. Unlike the fixed patterns of MLPs and CNNs, RNNs showed how networks could process variable-length inputs by reusing weights over time. This insight, that architectural patterns could adapt to input structure, laid groundwork for more flexible architectures.</p>
<p>Sequence models also popularized the concept of attention through encoder-decoder architectures <span class="citation" data-cites="bahdanau2014neural">(<a href="ch058.xhtml#ref-bahdanau2014neural">Bahdanau, Cho, and Bengio 2014</a>)</span>. Initially introduced as an improvement to machine translation, attention mechanisms showed how networks could learn to dynamically focus on relevant information. This building block would later become the foundation of Transformer architectures.</p>
</section>
<section id="sec-dnn-architectures-modern-architectures-synthesis-unification-b5ef" class="level3">
<h3>Modern Architectures: Synthesis and Unification</h3>
<p>Modern architectures, particularly Transformers, represent a sophisticated synthesis of these fundamental building blocks. Rather than introducing entirely new patterns, they innovate through strategic combination and refinement of existing components. The Transformer architecture exemplifies this approach: at its core, MLP-style feedforward networks process features between attention layers. The attention mechanism itself builds on sequence model concepts while eliminating recurrent connections, instead employing position embeddings inspired by CNN intuitions. The architecture extensively utilizes skip connections (see <a href="ch010.xhtml#fig-example-skip-connection" class="quarto-xref">Figure¬†4.9</a>), inherited from ResNets, while layer normalization, evolved from CNN batch normalization, stabilizes optimization <span class="citation" data-cites="ba2016layer">(<a href="ch058.xhtml#ref-ba2016layer">Ba, Kiros, and Hinton 2016</a>)</span>.</p>
<div id="fig-example-skip-connection" class="quarto-float quarto-figure quarto-figure-center" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-example-skip-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../media/file62.svg" alt="" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-example-skip-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure¬†4.9: <strong>Residual Connection</strong>: Skip connections add the input of a layer to its output, enabling gradients to flow directly through the network and mitigating the vanishing gradient problem in deep architectures. This allows training of significantly deeper networks, as seen in resnets and adopted in modern transformer architectures to improve optimization and performance.
</figcaption>
</figure>
</div>
<p>This composition of building blocks creates emergent capabilities exceeding the sum of individual components. The self-attention mechanism, while building on previous attention concepts, enables novel forms of dynamic pattern processing. The arrangement of these components‚Äîattention followed by feedforward layers, with skip connections and normalization‚Äîhas proven sufficiently effective to become a template for new architectures.</p>
<p>Recent innovations in vision and language models follow this pattern of recombining building blocks. Vision Transformers<a href="#fn26" class="footnote-ref" id="fnref26" epub:type="noteref" role="doc-noteref">26</a> adapt the Transformer architecture to images while maintaining its essential components <span class="citation" data-cites="dosovitskiy2021image">(<a href="ch058.xhtml#ref-dosovitskiy2021image">Dosovitskiy et al. 2021</a>)</span>. Large language models scale up these patterns while introducing refinements like grouped-query attention or sliding window attention, yet still rely on the core building blocks established through this architectural evolution <span class="citation" data-cites="brown2020language">(<a href="ch058.xhtml#ref-brown2020language">T. Brown et al. 2020</a>)</span>. These modern architectural innovations demonstrate the principles of efficient scaling covered in <a href="ch015.xhtml#sec-efficient-ai" class="quarto-xref">Chapter¬†9</a>, while their practical implementation challenges and optimizations are explored in <a href="ch016.xhtml#sec-model-optimizations" class="quarto-xref">Chapter¬†10</a>.</p>
<p>The following comparison of primitive utilization across different neural network architectures shows modern architectures synthesizing and innovating upon previous approaches:</p>
<div id="tbl-primitive-comparison" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-primitive-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table¬†4.2: <strong>Primitive Utilization</strong>: Neural network architectures differ in their core computational and memory access patterns, impacting hardware requirements and efficiency. Transformers uniquely combine matrix multiplication with attention mechanisms, resulting in random memory access and data movement patterns distinct from sequential rnns or strided cnns.
</figcaption>
<div aria-describedby="tbl-primitive-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top" style="width:97%;">
<colgroup>
<col style="width: 20%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 21%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Primitive Type</strong></th>
<th style="text-align: left;"><strong>MLP</strong></th>
<th style="text-align: left;"><strong>CNN</strong></th>
<th style="text-align: left;"><strong>RNN</strong></th>
<th style="text-align: left;"><strong>Transformer</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Computational</strong></td>
<td style="text-align: left;">Matrix Multiplication</td>
<td style="text-align: left;">Convolution (Matrix Mult.)</td>
<td style="text-align: left;">Matrix Mult. + State Update</td>
<td style="text-align: left;">Matrix Mult. + Attention</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Memory Access</strong></td>
<td style="text-align: left;">Sequential</td>
<td style="text-align: left;">Strided</td>
<td style="text-align: left;">Sequential + Random</td>
<td style="text-align: left;">Random (Attention)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Data Movement</strong></td>
<td style="text-align: left;">Broadcast</td>
<td style="text-align: left;">Sliding Window</td>
<td style="text-align: left;">Sequential</td>
<td style="text-align: left;">Broadcast + Gather</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>As shown in <a href="ch010.xhtml#tbl-primitive-comparison" class="quarto-xref">Table¬†4.2</a>, Transformers combine elements from previous architectures while introducing new patterns. They retain the core matrix multiplication operations common to all architectures but introduce a more complex memory access pattern with their attention mechanism. Their data movement patterns blend the broadcast operations of MLPs with the gather operations reminiscent of more dynamic architectures.</p>
<p>This synthesis of primitives in Transformers shows modern architectures innovating by recombining and refining existing building blocks from the architectural progression established in <a href="ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section¬†4.1</a>, rather than inventing entirely new computational paradigms. This evolutionary process guides the development of future architectures and helps design of efficient systems to support them.</p>
</section>
</section>
<section id="sec-dnn-architectures-systemlevel-building-blocks-72f6" class="level2">
<h2>System-Level Building Blocks</h2>
<p>Examination of different deep learning architectures enables distillation of their system requirements into primitives that underpin both hardware and software implementations. These primitives represent operations that cannot be decomposed further while maintaining their essential characteristics. Just as complex molecules are built from basic atoms, sophisticated neural networks are constructed from these operations.</p>
<section id="sec-dnn-architectures-core-computational-primitives-bd67" class="level3">
<h3>Core Computational Primitives</h3>
<p>Three operations serve as the building blocks for all deep learning computations: matrix multiplication, sliding window operations, and dynamic computation. These operations are primitive because they cannot be further decomposed without losing their essential computational properties and efficiency characteristics.</p>
<p>Matrix multiplication represents the basic form of transforming sets of features. When we multiply a matrix of inputs by a matrix of weights, we‚Äôre computing weighted combinations, which is the core operation of neural networks. For example, in our MNIST network, each 784-dimensional input vector multiplies with a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>784</mn><mo>√ó</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">784\times 100</annotation></semantics></math> weight matrix. This pattern appears everywhere: MLPs use it directly for layer computations, CNNs reshape convolutions into matrix multiplications (turning a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> convolution into a matrix operation, as illustrated in <a href="ch010.xhtml#fig-im2col-diagram" class="quarto-xref">Figure¬†4.10</a>), and Transformers use it extensively in their attention mechanisms.</p>
<section id="sec-dnn-architectures-computational-building-blocks-c3c0" class="level4">
<h4>Computational Building Blocks</h4>
<p>Modern neural networks operate through three computational patterns that appear across all architectures. These patterns explain how different architectures achieve their computational goals and why certain hardware optimizations are effective.</p>
<p>The detailed analysis of sparse computation patterns, including structured and unstructured sparsity, hardware-aware optimization strategies, and algorithm-hardware co-design principles, is addressed in <a href="ch016.xhtml#sec-model-optimizations" class="quarto-xref">Chapter¬†10</a> and <a href="ch017.xhtml#sec-ai-acceleration" class="quarto-xref">Chapter¬†11</a>.</p>
<div id="fig-im2col-diagram" class="quarto-float quarto-figure quarto-figure-center" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-im2col-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../media/file63.svg" alt="" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-im2col-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure¬†4.10: <strong>Convolution as Matrix Multiplication</strong>: Reshaping convolutional layers into matrix multiplications using the <code>im2col</code> technique, enables efficient computation using optimized BLAS libraries and allows for parallel processing on standard hardware. This transformation is crucial for accelerating cnns and forms the basis for implementing convolutions on diverse platforms.
</figcaption>
</figure>
</div>
<p>The im2col<a href="#fn27" class="footnote-ref" id="fnref27" epub:type="noteref" role="doc-noteref">27</a> (image to column) technique, developed by Intel in the 1990s, accomplishes matrix reshaping by unfolding overlapping image patches into columns of a matrix, as illustrated in <a href="ch010.xhtml#fig-im2col-diagram" class="quarto-xref">Figure¬†4.10</a>. Each sliding window position in the convolution becomes a column in the transformed matrix, while the filter kernels are arranged as rows. This allows the convolution operation to be expressed as a standard GEMM (General Matrix Multiply) operation. The transformation trades memory consumption‚Äîduplicating data where windows overlap‚Äîfor computational efficiency, enabling CNNs to leverage decades of BLAS optimizations and achieving 5-10x speedups on CPUs. In modern systems, these matrix multiplications map to specific hardware and software implementations. Hardware accelerators provide specialized tensor cores that can perform thousands of multiply-accumulates in parallel; NVIDIA‚Äôs A100 tensor cores can achieve up to 312 TFLOPS for mixed-precision (TF32) workloads, or 156 TFLOPS for FP32 through massive parallelization of these operations. Software frameworks like PyTorch and TensorFlow automatically map these high-level operations to optimized matrix libraries (NVIDIA <a href="https://developer.nvidia.com/cublas">cuBLAS</a>, Intel <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html#gs.kxb9ve">MKL</a>) that exploit these hardware capabilities.</p>
<p>Sliding window operations compute local relationships by applying the same operation to chunks of data. In CNNs processing MNIST images, a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> convolution filter slides across the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>28</mn><mo>√ó</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics></math> input, requiring <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>26</mn><mo>√ó</mo><mn>26</mn></mrow><annotation encoding="application/x-tex">26\times 26</annotation></semantics></math> windows of computation, assuming a stride size of 1. Modern hardware accelerators implement this through specialized memory access patterns and data buffering schemes that optimize data reuse. For example, Google‚Äôs TPU uses a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn><mo>√ó</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">128\times 128</annotation></semantics></math> systolic array<a href="#fn28" class="footnote-ref" id="fnref28" epub:type="noteref" role="doc-noteref">28</a> where data flows systematically through processing elements, allowing each input value to be reused across multiple computations without accessing memory.</p>
<p>Dynamic computation, where the operation itself depends on the input data, emerged prominently with attention mechanisms but represents a capability needed for adaptive processing. In Transformer attention, each query dynamically determines its interaction weights with all keys; for a sequence of length 512, 512 different weight patterns must be computed on the fly. Unlike fixed patterns where the computation graph is known in advance, dynamic computation requires runtime decisions. This creates specific implementation challenges: hardware must provide flexible data routing (modern GPUs employ dynamic scheduling) and support variable computation patterns, while software frameworks require efficient mechanisms for handling data-dependent execution paths (PyTorch‚Äôs dynamic computation graphs, TensorFlow‚Äôs dynamic control flow).</p>
<p>These primitives combine in sophisticated ways in modern architectures. A Transformer layer processing a sequence of 512 tokens demonstrates this clearly: it uses matrix multiplications for feature projections (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn><mo>√ó</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">512\times 512</annotation></semantics></math> operations implemented through tensor cores), may employ sliding windows for efficient attention over long sequences (using specialized memory access patterns for local regions), and requires dynamic computation for attention weights (computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn><mo>√ó</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">512\times 512</annotation></semantics></math> attention patterns at runtime). The way these primitives interact creates specific demands on system design, ranging from memory hierarchy organization to computation scheduling.</p>
<p>The building blocks we‚Äôve discussed help explain why certain hardware features exist (like tensor cores for matrix multiplication) and why software frameworks organize computations in particular ways (like batching similar operations together). As we move from computational primitives to consider memory access and data movement patterns, recognizing how these operations shape the demands placed on memory systems becomes essential and data transfer mechanisms. The way computational primitives are implemented and combined has direct implications for how data needs to be stored, accessed, and moved within the system.</p>
</section>
</section>
<section id="sec-dnn-architectures-memory-access-primitives-4e2e" class="level3">
<h3>Memory Access Primitives</h3>
<p>The efficiency of deep learning models depends heavily on memory access and management. Memory access often constitutes the primary bottleneck in modern ML systems; even though a matrix multiplication unit may be capable of performing thousands of operations per cycle, it will remain idle if data is not available at the requisite time. For example, accessing data from DRAM typically requires hundreds of cycles, while on-chip computation requires only a few cycles.</p>
<p>Three memory access patterns dominate in deep learning architectures: sequential access, strided access, and random access. Each pattern creates different demands on the memory system and offers different opportunities for optimization.</p>
<p>Sequential access is the simplest and most efficient pattern. Consider an MLP performing matrix multiplication with a batch of MNIST images: it needs to access both the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>784</mn><mo>√ó</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">784\times 100</annotation></semantics></math> weight matrix and the input vectors sequentially. This pattern maps well to modern memory systems; DRAM can operate in burst mode for sequential reads (achieving up to 400 GB/s in modern GPUs), and hardware prefetchers can effectively predict and fetch upcoming data. Software frameworks optimize for this by ensuring data is laid out contiguously in memory and aligning data to cache line boundaries.</p>
<p>Strided access appears prominently in CNNs, where each output position needs to access a window of input values at regular intervals. For a CNN processing MNIST images with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> filters, each output position requires accessing 9 input values with a stride matching the input width. While less efficient than sequential access, hardware supports this through pattern-aware caching strategies and specialized memory controllers. Software frameworks often transform these strided patterns into sequential access through data layout reorganization, where the im2col transformation in deep learning frameworks converts convolution‚Äôs strided access into efficient matrix multiplications.</p>
<p>Random access poses the greatest challenge for system efficiency. In a Transformer processing a sequence of 512 tokens, each attention operation potentially needs to access any position in the sequence, creating unpredictable memory access patterns. Random access can severely impact performance through cache misses (potentially causing 100+ cycle stalls per access) and unpredictable memory latencies. Systems address this through large cache hierarchies (modern GPUs have several MB of L2 cache) and sophisticated prefetching strategies, while software frameworks employ techniques like attention pattern pruning to reduce random access requirements.</p>
<p>These different memory access patterns contribute to the overall memory requirements of each architecture. To illustrate this, <a href="ch010.xhtml#tbl-arch-complexity" class="quarto-xref">Table¬†4.3</a> compares the memory complexity of MLPs, CNNs, RNNs, and Transformers.</p>
<div id="tbl-arch-complexity" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-arch-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table¬†4.3: <strong>Memory Access Complexity</strong>: Different neural network architectures exhibit varying memory access patterns and storage requirements, impacting system performance and scalability. Parameter storage scales with input dependency and model size, while activation storage represents a significant runtime cost, particularly for sequence-based models where rnns offer a parameter efficiency advantage when sequence length exceeds hidden state size (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mi>h</mi></mrow><annotation encoding="application/x-tex">n &gt; h</annotation></semantics></math>).
</figcaption>
<div aria-describedby="tbl-arch-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top" style="width:98%;">
<colgroup>
<col style="width: 15%" />
<col style="width: 19%" />
<col style="width: 19%" />
<col style="width: 23%" />
<col style="width: 19%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Architecture</strong></th>
<th style="text-align: left;"><strong>Input Dependency</strong></th>
<th style="text-align: left;"><strong>Parameter Storage</strong></th>
<th style="text-align: left;"><strong>Activation Storage</strong></th>
<th style="text-align: left;"><strong>Scaling Behavior</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>MLP</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>√ó</mo><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(N \times W)</annotation></semantics></math></td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo>√ó</mo><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(B \times W)</annotation></semantics></math></td>
<td style="text-align: left;">Predictable</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>CNN</strong></td>
<td style="text-align: left;">Constant</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>K</mi><mo>√ó</mo><mi>C</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(K \times C)</annotation></semantics></math></td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>B</mi><mo>√ó</mo><msub><mi>H</mi><mtext mathvariant="normal">img</mtext></msub></mrow><annotation encoding="application/x-tex">O(B\times H_{\text{img}}</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>√ó</mi><msub><mi>W</mi><mtext mathvariant="normal">img</mtext></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\times W_{\text{img}})</annotation></semantics></math></td>
<td style="text-align: left;">Efficient</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>RNN</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>h</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(h^2)</annotation></semantics></math></td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo>√ó</mo><mi>T</mi><mo>√ó</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(B \times T \times h)</annotation></semantics></math></td>
<td style="text-align: left;">Challenging</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Transformer</strong></td>
<td style="text-align: left;">Quadratic</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>√ó</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(N \times d)</annotation></semantics></math></td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo>√ó</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(B \times N^2)</annotation></semantics></math></td>
<td style="text-align: left;">Problematic</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Where:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>: Input or sequence size</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math>: Layer width</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>: Batch size</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>: Kernel size</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>: Number of channels</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mtext mathvariant="normal">img</mtext></msub><annotation encoding="application/x-tex">H_{\text{img}}</annotation></semantics></math>: Height of input feature map (CNN)</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>W</mi><mtext mathvariant="normal">img</mtext></msub><annotation encoding="application/x-tex">W_{\text{img}}</annotation></semantics></math>: Width of input feature map (CNN)</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>: Hidden state size (RNN)</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>: Sequence length</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>: Model dimensionality</li>
</ul>
<p><a href="ch010.xhtml#tbl-arch-complexity" class="quarto-xref">Table¬†4.3</a> reveals how memory requirements scale with different architectural choices. The quadratic scaling of activation storage in Transformers, for instance, highlights the need for large memory capacities and efficient memory management in systems designed for Transformer-based workloads. In contrast, CNNs exhibit more favorable memory scaling due to their parameter sharing and localized processing. These memory access patterns complement the computational scaling behaviors examined later in <a href="ch010.xhtml#tbl-computational-complexity" class="quarto-xref">Table¬†4.6</a>, completing the picture of each architecture‚Äôs resource requirements. These memory complexity considerations inform system-level design decisions, such as choosing memory hierarchy configurations and developing memory optimization strategies.</p>
<p>The impact of these patterns becomes clear when we consider data reuse opportunities. In CNNs, each input pixel participates in multiple convolution windows (typically 9 times for a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> filter), making effective data reuse necessary for performance. Modern GPUs provide multi-level cache hierarchies (L1, L2, shared memory) to capture this reuse, while software techniques like loop tiling ensure data remains in cache once loaded.</p>
<p>Working set size, the amount of data needed simultaneously for computation, varies dramatically across architectures. An MLP layer processing MNIST images might need only a few hundred KB (weights plus activations), while a Transformer processing long sequences can require several MB just for storing attention patterns. These differences directly influence hardware design choices, like the balance between compute units and on-chip memory, and software optimizations like activation checkpointing or attention approximation techniques.</p>
<p>Understanding these memory access patterns is essential as architectures evolve. The shift from CNNs to Transformers, for instance, has driven the development of hardware with larger on-chip memories and more sophisticated caching strategies to handle increased working sets and more dynamic access patterns. Future architectures will likely continue to be shaped by their memory access characteristics as much as their computational requirements.</p>
</section>
<section id="sec-dnn-architectures-data-movement-primitives-101a" class="level3">
<h3>Data Movement Primitives</h3>
<p>While computational and memory access patterns define what operations occur where, data movement primitives characterize how information flows through the system. These patterns are key because data movement often consumes more time and energy than computation itself, as moving data from off-chip memory typically requires 100-1000<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>√ó</mi><annotation encoding="application/x-tex">\times</annotation></semantics></math> more energy than performing a floating-point operation.</p>
<p>Four data movement patterns are prevalent in deep learning architectures: broadcast, scatter, gather, and reduction. <a href="ch010.xhtml#fig-collective-comm" class="quarto-xref">Figure¬†4.11</a> illustrates these patterns and their relationships. Broadcast operations send the same data to multiple destinations simultaneously. In matrix multiplication with batch size 32, each weight must be broadcast to process different inputs in parallel. Modern hardware supports this through specialized interconnects, NVIDIA GPUs provide hardware multicast capabilities, achieving up to 600 GB/s broadcast bandwidth, while TPUs use dedicated broadcast buses. Software frameworks optimize broadcasts by restructuring computations (like matrix tiling) to maximize data reuse.</p>
<div id="fig-collective-comm" class="quarto-float quarto-figure quarto-figure-center" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-collective-comm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../media/file64.svg" alt="" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-collective-comm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure¬†4.11: <strong>Collective Communication Patterns</strong>: Deep learning training and inference frequently require data exchange between processing units; this figure outlines four core patterns (broadcast, scatter, gather, and reduction) that define how data moves within a distributed system and impact overall performance. Understanding these patterns enables optimization of data movement, critical because communication costs often dominate computation in modern machine learning workloads.
</figcaption>
</figure>
</div>
<p>Scatter operations distribute different elements to different destinations. When parallelizing a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn><mo>√ó</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">512\times 512</annotation></semantics></math> matrix multiplication across GPU cores, each core receives a subset of the computation. This parallelization is important for performance but challenging, as memory conflicts and load imbalance, can reduce efficiency by 50% or more. Hardware provides flexible interconnects (like NVIDIA‚Äôs NVLink offering 600 GB/s bi-directional bandwidth), while software frameworks employ sophisticated work distribution algorithms to maintain high utilization.</p>
<p>Gather operations collect data from multiple sources. In Transformer attention with sequence length 512, each query must gather information from 512 different key-value pairs. These irregular access patterns are challenging, random gathering can be <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mo>√ó</mo></mrow><annotation encoding="application/x-tex">10\times</annotation></semantics></math> slower than sequential access. Hardware supports this through high-bandwidth interconnects and large caches, while software frameworks employ techniques like attention pattern pruning to reduce gathering overhead.</p>
<p>Reduction operations combine multiple values into a single result through operations like summation. When computing attention scores in Transformers or layer outputs in MLPs, efficient reduction is essential. Hardware implements tree-structured reduction networks (reducing latency from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>log</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(\log n)</annotation></semantics></math>), while software frameworks use optimized parallel reduction algorithms that can achieve near-theoretical peak performance.</p>
<p>These patterns combine in sophisticated ways. A Transformer attention operation with sequence length 512 and batch size 32 involves:</p>
<ul>
<li>Broadcasting query vectors (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn><mo>√ó</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">512\times 64</annotation></semantics></math> elements)</li>
<li>Gathering relevant keys and values (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn><mo>√ó</mo><mn>512</mn><mo>√ó</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">512\times 512\times 64</annotation></semantics></math> elements)</li>
<li>Reducing attention scores (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn><mo>√ó</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">512\times 512</annotation></semantics></math> elements per sequence)</li>
</ul>
<p>The evolution from CNNs to Transformers has increased reliance on gather and reduction operations, driving hardware innovations like more flexible interconnects and larger on-chip memories. As models grow (some now exceeding 100 billion parameters<a href="#fn29" class="footnote-ref" id="fnref29" epub:type="noteref" role="doc-noteref">29</a>), efficient data movement becomes increasingly critical, leading to innovations like near-memory processing and sophisticated data flow optimizations.</p>
</section>
<section id="sec-dnn-architectures-system-design-impact-cd41" class="level3">
<h3>System Design Impact</h3>
<p>The computational, memory access, and data movement primitives we‚Äôve explored form the foundational requirements that shape the design of systems for deep learning. The way these primitives influence hardware design, create common bottlenecks, and drive trade-offs is important for developing efficient and effective ML systems.</p>
<p>One of the most significant impacts of these primitives on system design is the push towards specialized hardware. The prevalence of matrix multiplications and convolutions in deep learning has led to the development of tensor processing units (TPUs)<a href="#fn30" class="footnote-ref" id="fnref30" epub:type="noteref" role="doc-noteref">30</a> and tensor cores in GPUs, which are specifically designed to perform these operations efficiently.</p>
<p>Memory systems have also been profoundly influenced by the demands of deep learning primitives. The need to support both sequential and random access patterns efficiently has driven the development of sophisticated memory hierarchies. High-bandwidth memory (HBM)<a href="#fn31" class="footnote-ref" id="fnref31" epub:type="noteref" role="doc-noteref">31</a> has become common in AI accelerators to support the massive data movement requirements, especially for operations like attention mechanisms in Transformers. On-chip memory hierarchies have grown in complexity, with multiple levels of caching and scratchpad memories<a href="#fn32" class="footnote-ref" id="fnref32" epub:type="noteref" role="doc-noteref">32</a> to support the diverse working set sizes of different neural network layers.</p>
<p>The data movement primitives have particularly influenced the design of interconnects and on-chip networks. The need to support efficient broadcasts, gathers, and reductions has led to the development of more flexible and higher-bandwidth interconnects. Some AI chips now feature specialized networks-on-chip designed to accelerate common data movement patterns in neural networks.</p>
<p><a href="ch010.xhtml#tbl-sys-design-implications" class="quarto-xref">Table¬†4.4</a> summarizes the system implications of these primitives:</p>
<div id="tbl-sys-design-implications" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sys-design-implications-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table¬†4.4: <strong>Primitive-Hardware Co-Design</strong>: Efficient machine learning systems require tight integration between algorithmic primitives and underlying hardware; this table maps common primitives to specific hardware accelerations and software optimizations, highlighting key challenges in their implementation. Specialized hardware, such as tensor cores and datapaths, address the computational demands of primitives like matrix multiplication and sliding windows, while software techniques like batching and dynamic graph execution further enhance performance.
</figcaption>
<div aria-describedby="tbl-sys-design-implications-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top" style="width:98%;">
<colgroup>
<col style="width: 24%" />
<col style="width: 24%" />
<col style="width: 24%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Primitive</strong></th>
<th style="text-align: left;"><strong>Hardware Impact</strong></th>
<th style="text-align: left;"><strong>Software Optimization</strong></th>
<th style="text-align: left;"><strong>Key Challenges</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Matrix Multiplication</strong></td>
<td style="text-align: left;">Tensor Cores</td>
<td style="text-align: left;">Batching, GEMM libraries</td>
<td style="text-align: left;">Parallelization, precision</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Sliding Window</strong></td>
<td style="text-align: left;">Specialized datapaths</td>
<td style="text-align: left;">Data layout optimization</td>
<td style="text-align: left;">Stride handling</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Dynamic Computation</strong></td>
<td style="text-align: left;">Flexible routing</td>
<td style="text-align: left;">Dynamic graph execution</td>
<td style="text-align: left;">Load balancing</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Sequential Access</strong></td>
<td style="text-align: left;">Burst mode DRAM</td>
<td style="text-align: left;">Contiguous allocation</td>
<td style="text-align: left;">Access latency</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Random Access</strong></td>
<td style="text-align: left;">Large caches</td>
<td style="text-align: left;">Memory-aware scheduling</td>
<td style="text-align: left;">Cache misses</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Broadcast</strong></td>
<td style="text-align: left;">Specialized interconnects</td>
<td style="text-align: left;">Operation fusion</td>
<td style="text-align: left;">Bandwidth</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Gather/Scatter</strong></td>
<td style="text-align: left;">High-bandwidth memory</td>
<td style="text-align: left;">Work distribution</td>
<td style="text-align: left;">Load balancing</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Despite these advancements, several bottlenecks persist in deep learning models. Memory bandwidth often remains a key limitation, particularly for models with large working sets or those that require frequent random access. The energy cost of data movement, especially between off-chip memory and processing units, continues to be a significant concern. For large-scale models, the communication overhead in distributed training can become a bottleneck, limiting scaling efficiency.</p>
<section id="sec-dnn-architectures-energy-consumption-analysis-across-architectures-b681" class="level4">
<h4>Energy Consumption Analysis Across Architectures</h4>
<p>Energy consumption patterns vary dramatically across neural network architectures, with implications for both datacenter deployment and edge computing scenarios. Each architectural pattern exhibits distinct energy characteristics that inform deployment decisions and optimization strategies.</p>
<p>Dense matrix operations in MLPs achieve excellent arithmetic intensity<a href="#fn33" class="footnote-ref" id="fnref33" epub:type="noteref" role="doc-noteref">33</a> (computation per data movement) but consume significant absolute energy. Each multiply-accumulate operation consumes approximately 4.6pJ, while data movement from DRAM costs 640pJ per 32-bit value. For typical MLP inference, 70-80% of energy goes to data movement rather than computation, making memory bandwidth optimization critical for energy efficiency.</p>
<p>Convolutional operations reduce energy consumption through data reuse but exhibit variable efficiency depending on implementation. Im2col-based convolution implementations trade memory for simplicity, often doubling memory requirements and energy consumption. Direct convolution implementations achieve 3-5x better energy efficiency by eliminating redundant data movement, particularly for larger kernel sizes.</p>
<p>Sequential processing in RNNs creates energy efficiency opportunities through temporal data reuse. The constant memory footprint of RNN hidden states enables aggressive caching strategies, reducing DRAM access energy by 80-90% for long sequences. The sequential dependencies limit parallelization opportunities, often resulting in suboptimal hardware utilization and higher energy per operation.</p>
<p>Attention mechanisms in Transformers exhibit the highest energy consumption per operation due to quadratic scaling and complex data movement patterns. Self-attention operations consume 2-3x more energy per FLOP than standard matrix multiplication due to irregular memory access patterns and the need to store attention matrices. This energy cost scales quadratically with sequence length, making long-sequence processing energy-prohibitive without architectural modifications.</p>
<p>System designers must navigate trade-offs in supporting different primitives, each with unique characteristics that influence system design and performance. For example, optimizing for the dense matrix operations common in MLPs and CNNs might come at the cost of flexibility needed for the more dynamic computations in attention mechanisms. Supporting large working sets for Transformers might require sacrificing energy efficiency.</p>
<p>Balancing these trade-offs requires consideration of the target workloads and deployment scenarios. Understanding the nature of each primitive guides the development of both hardware and software optimizations in ML systems, allowing designers to make informed decisions about system architecture and resource allocation.</p>
<p>The analysis of architectural patterns, computational primitives, and system implications establishes the foundation for addressing a practical challenge: how do engineers systematically choose the right architecture for their specific problem? The diversity of neural network architectures, each optimized for different data patterns and computational constraints, requires a structured approach to architecture selection. This selection process must consider not only algorithmic performance but also deployment constraints covered in <a href="ch008.xhtml#sec-ml-systems" class="quarto-xref">Chapter¬†2</a> and operational efficiency requirements detailed in <a href="ch019.xhtml#sec-ml-operations" class="quarto-xref">Chapter¬†13</a>.</p>
</section>
</section>
</section>
<section id="sec-dnn-architectures-architecture-selection-framework-7a37" class="level2">
<h2>Architecture Selection Framework</h2>
<p>The exploration of neural network architectures, from dense MLPs to dynamic Transformers, demonstrates how each design embodies specific assumptions about data structure and computational patterns. MLPs assume arbitrary feature relationships, CNNs exploit spatial locality, RNNs capture temporal dependencies, and Transformers model complex relational patterns. For practitioners facing real-world problems, a question emerges: how to systematically select the appropriate architecture for a specific use case?</p>
<p>The diversity of available architectures overwhelms practitioners, when each claims superiority for different scenarios. Successful architecture selection requires understanding principles rather than following trends: matching data characteristics to architectural strengths, evaluating computational constraints against system capabilities, and balancing accuracy requirements with deployment realities.</p>
<p>This systematic approach to architecture selection draws upon the computational patterns and system implications explored in the preceding analysis. By understanding how different architectures process information and their corresponding resource requirements, engineers can make informed decisions that align with both problem requirements and practical constraints. The framework integrates principles from efficient AI design <a href="ch015.xhtml#sec-efficient-ai" class="quarto-xref">Chapter¬†9</a> with practical deployment considerations as discussed in ML operations <a href="ch019.xhtml#sec-ml-operations" class="quarto-xref">Chapter¬†13</a>.</p>
<section id="sec-dnn-architectures-datatoarchitecture-mapping-0b9c" class="level3">
<h3>Data-to-Architecture Mapping</h3>
<p>The first step in systematic architecture selection involves understanding how different data types align with architectural strengths. Each neural network architecture evolved to address specific patterns in data: MLPs handle arbitrary relationships in tabular data, CNNs exploit spatial locality in images, RNNs capture temporal dependencies in sequences, and Transformers model complex relational patterns where any element might influence any other.</p>
<p>This alignment is not coincidental. It reflects computational trade-offs. Architectures that match data characteristics can leverage natural structure for efficiency, while mismatched architectures must work against their design assumptions, leading to poor performance or excessive resource consumption.</p>
<p><a href="ch010.xhtml#tbl-architecture-selection" class="quarto-xref">Table¬†4.5</a> provides a systematic framework for matching data characteristics to appropriate architectures:</p>
<div id="tbl-architecture-selection" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-architecture-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table¬†4.5: <strong>Architecture Selection Framework</strong>: Systematic matching of data characteristics to neural network architectures based on computational requirements and pattern types.
</figcaption>
<div aria-describedby="tbl-architecture-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top" style="width:98%;">
<colgroup>
<col style="width: 16%" />
<col style="width: 18%" />
<col style="width: 33%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Architecture</strong></th>
<th style="text-align: left;"><strong>Data Type</strong></th>
<th style="text-align: left;"><strong>Key Characteristics</strong></th>
<th style="text-align: left;"><strong>Example Applications</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>MLPs</strong></td>
<td style="text-align: left;">Tabular/Structured</td>
<td style="text-align: left;">‚Ä¢ No spatial/temporal ‚Ä¢¬†Arbitrary¬†relationships ‚Ä¢¬†Dense¬†connectivity</td>
<td style="text-align: left;">‚Ä¢ Financial modeling ‚Ä¢¬†Medical¬†measurements ‚Ä¢¬†Recommendation systems</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>CNNs</strong></td>
<td style="text-align: left;">Spatial/Grid-like</td>
<td style="text-align: left;">‚Ä¢ Local patterns ‚Ä¢¬†Translation¬†equivariance ‚Ä¢¬†Parameter¬†sharing</td>
<td style="text-align: left;">‚Ä¢ Image recognition ‚Ä¢¬†2D¬†sensor¬†data ‚Ä¢¬†Signal¬†processing</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>RNNs</strong></td>
<td style="text-align: left;">Sequential/Temporal</td>
<td style="text-align: left;">‚Ä¢ Temporal dependencies ‚Ä¢¬†Variable¬†length ‚Ä¢¬†Memory¬†across time</td>
<td style="text-align: left;">‚Ä¢ Time series forecasting ‚Ä¢¬†Simple language tasks ‚Ä¢¬†Speech recognition</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Transformers</strong></td>
<td style="text-align: left;">Complex Relational</td>
<td style="text-align: left;">‚Ä¢ Long-range dependencies ‚Ä¢¬†Attention¬†mechanisms ‚Ä¢¬†Dynamic relationships</td>
<td style="text-align: left;">‚Ä¢ Language understanding ‚Ä¢¬†Machine translation ‚Ä¢¬†Complex reasoning tasks</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>While data characteristics guide initial architecture selection, computational constraints often determine final feasibility. Understanding the scaling behavior of each architecture enables realistic resource planning and deployment decisions.</p>
</section>
<section id="sec-dnn-architectures-computational-complexity-considerations-93fb" class="level3">
<h3>Computational Complexity Considerations</h3>
<p>Architecture selection must account for computational and memory trade-offs that determine deployment feasibility. Each architecture exhibits distinct scaling behaviors that create different bottlenecks as problem size increases. Understanding these patterns enables realistic resource planning and prevents costly architectural mismatches during deployment.</p>
<p>The computational profile of each architecture reflects its underlying design philosophy. Dense architectures like MLPs prioritize representational capacity through full connectivity, while structured architectures like CNNs achieve efficiency through parameter sharing and locality assumptions. Sequential architectures like RNNs trade parallelization for memory efficiency, while attention-based architectures like Transformers exchange memory for computational flexibility. For completeness, we examine these same architectures from both computational scaling and memory access perspectives (see <a href="ch010.xhtml#tbl-arch-complexity" class="quarto-xref">Table¬†4.3</a>), as each viewpoint reveals different optimization opportunities and system design considerations.</p>
<p><a href="ch010.xhtml#tbl-computational-complexity" class="quarto-xref">Table¬†4.6</a> summarizes the key computational characteristics of each architecture:</p>
<div id="tbl-computational-complexity" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-computational-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table¬†4.6: <strong>Computational Complexity Comparison</strong>: Scaling behaviors and resource requirements for major neural network architectures. Variables: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> = dimension, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math> = hidden size, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> = kernel size, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>¬†=¬†channels, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>,</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">H,W</annotation></semantics></math> = spatial dimensions, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> = time steps, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> = sequence length, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math> = batch size.
</figcaption>
<div aria-describedby="tbl-computational-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top" style="width:98%;">
<colgroup>
<col style="width: 15%" />
<col style="width: 21%" />
<col style="width: 21%" />
<col style="width: 20%" />
<col style="width: 19%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Architecture</strong></th>
<th style="text-align: left;"><strong>Parameters</strong></th>
<th style="text-align: left;"><strong>Forward Pass</strong></th>
<th style="text-align: left;"><strong>Memory</strong></th>
<th style="text-align: left;"><strong>Parallelization</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>MLPs</strong></td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>d</mi><mtext mathvariant="normal">in</mtext></msub><mo>√ó</mo></mrow><annotation encoding="application/x-tex">O(d_{\text{in}}\times</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext mathvariant="normal">out</mtext></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">d_{\text{out}})</annotation></semantics></math> per¬†layer</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>d</mi><mtext mathvariant="normal">in</mtext></msub><mo>√ó</mo></mrow><annotation encoding="application/x-tex">O(d_{\text{in}}\times</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext mathvariant="normal">out</mtext></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">d_{\text{out}})</annotation></semantics></math> per¬†layer</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d^2)</annotation></semantics></math> weights <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mo>√ó</mo><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d\times b)</annotation></semantics></math> activations</td>
<td style="text-align: left;">Excellent Matrix ops parallel</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>CNNs</strong></td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>k</mi><mn>2</mn></msup><mo>√ó</mo></mrow><annotation encoding="application/x-tex">O(k^2\times</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mtext mathvariant="normal">in</mtext></msub><mo>√ó</mo></mrow><annotation encoding="application/x-tex">c_{\text{in}}\times</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mtext mathvariant="normal">out</mtext></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">c_{\text{out}})</annotation></semantics></math> per layer</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>H</mi><mo>√ó</mo><mi>W</mi><mo>√ó</mo></mrow><annotation encoding="application/x-tex">O(H\times W\times</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>k</mi><mn>2</mn></msup><mo>√ó</mo></mrow><annotation encoding="application/x-tex">k^2\times</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mtext mathvariant="normal">in</mtext></msub><mo>√ó</mo></mrow><annotation encoding="application/x-tex">c_{\text{in}}\times</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mtext mathvariant="normal">out</mtext></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">c_{\text{out}})</annotation></semantics></math></td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>H</mi><mo>√ó</mo><mi>W</mi><mo>√ó</mo><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(H\times W\times c)</annotation></semantics></math> features <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>k</mi><mn>2</mn></msup><mo>√ó</mo><msup><mi>c</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(k^2\times c^2)</annotation></semantics></math> weights</td>
<td style="text-align: left;">Good Spatial independence</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>RNNs</strong></td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>h</mi><mn>2</mn></msup><mo>+</mo><mi>h</mi><mo>√ó</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(h^2+h\times d)</annotation></semantics></math> total</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>T</mi><mo>√ó</mo><msup><mi>h</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(T\times h^2)</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> time¬†steps</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(h)</annotation></semantics></math> hidden state (constant)</td>
<td style="text-align: left;">Poor Sequential deps</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Transformers</strong></td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d^2)</annotation></semantics></math> projections <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo>√ó</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d^2\times h)</annotation></semantics></math> multi-head</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>√ó</mo><mi>d</mi><mo>+</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">O(n^2\times d+n</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>√ó</mi><mi>d</mi><mi>¬≤</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\times d¬≤)</annotation></semantics></math> per layer</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math> attention <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>√ó</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n\times d)</annotation></semantics></math> sequences</td>
<td style="text-align: left;">Excellent (positions) Limited by memory</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="sec-dnn-architectures-scalability-production-considerations-dcb0" class="level4">
<h4>Scalability and Production Considerations</h4>
<p>Production deployment introduces constraints beyond algorithmic performance, including latency requirements, memory limitations, energy budgets, and fault tolerance needs. Each architecture exhibits distinct production characteristics that determine real-world feasibility.</p>
<p>MLPs and CNNs scale well across multiple devices through data parallelism, achieving near-linear speedups with proper batch size scaling. RNNs face parallelization challenges due to sequential dependencies, requiring pipeline parallelism or other specialized techniques. Transformers achieve excellent parallelization across sequence positions but suffer from quadratic memory scaling that limits batch sizes and effective utilization.</p>
<p>MLPs provide predictable latency proportional to layer size, making them suitable for real-time applications with strict SLA requirements. CNNs exhibit variable latency depending on implementation strategy and hardware capabilities, with optimized implementations achieving sub-millisecond inference. RNNs create latency dependencies on sequence length, making them challenging for interactive applications. Transformers provide excellent throughput for batch processing but struggle with single-inference latency due to attention overhead.</p>
<p>Memory requirements vary significantly across architectures in production environments. MLPs require fixed memory proportional to model size, enabling straightforward capacity planning. CNNs need variable memory for feature maps that scales with input resolution, requiring dynamic memory management for variable-size inputs. RNNs maintain constant memory for hidden states but may require unbounded memory for very long sequences. Transformers face quadratic memory growth that creates hard limits on sequence length in production.</p>
<p>Fault tolerance and recovery characteristics differ significantly between architectures. MLPs and CNNs exhibit stateless computation that enables straightforward checkpointing and recovery. RNNs maintain temporal state that complicates distributed training and failure recovery procedures. Transformers combine stateless computation with massive memory requirements, making checkpoint sizes a practical concern for large models.</p>
<p>Hardware mapping efficiency varies considerably across architectural patterns. Modern MLPs achieve 80-90% of peak hardware performance on specialized tensor units. CNNs reach 60-75% efficiency depending on layer configuration and memory hierarchy design. RNNs typically achieve 30-50% of peak performance due to sequential constraints and irregular memory access patterns. Transformers achieve 70-85% efficiency for large batch sizes but drop significantly for small batches due to attention overhead.</p>
</section>
<section id="sec-dnn-architectures-hardware-mapping-optimization-strategies-5a66" class="level4">
<h4>Hardware Mapping and Optimization Strategies</h4>
<p>Different architectural patterns require distinct optimization strategies for efficient hardware mapping. Understanding these patterns enables systematic performance tuning and hardware selection decisions.</p>
<p>Dense matrix operations in MLPs map naturally to tensor processing units and GPU tensor cores. These operations benefit from several key optimizations: matrix tiling to fit cache hierarchies, mixed-precision computation to double throughput, and operation fusion to reduce memory traffic. Optimal tile sizes depend on cache hierarchy, typically 64x64 for L1 cache and 256x256 for L2, while tensor cores achieve peak efficiency with specific dimension multiples such as 16x16 blocks for Volta architecture.</p>
<p>CNNs benefit from specialized convolution algorithms and data layout optimizations that differ significantly from dense matrix operations. Im2col transformations convert convolutions to matrix multiplication but double memory usage. Winograd algorithms reduce arithmetic complexity by 2.25x for 3x3 convolutions at the cost of numerical stability. Direct convolution with custom kernels achieves optimal memory efficiency but requires architecture-specific tuning.</p>
<p>RNNs require different optimization approaches due to their temporal dependencies. Loop unrolling reduces control overhead but increases memory usage. State vectorization enables SIMD operations across multiple sequences. Wavefront parallelization exploits independence across timesteps for bidirectional processing.</p>
<p>Transformers demand specialized attention optimizations due to their quadratic complexity. FlashAttention algorithms reduce memory usage from O(n¬≤) to O(n) through online softmax computation and gradient recomputation. Sparse attention patterns including local, strided, and random approaches maintain modeling capability while reducing complexity. Multi-query attention shares key and value projections across heads, reducing memory bandwidth by 30-50%.</p>
<p>Multi-Layer Perceptrons represent the most straightforward computational pattern, with costs dominated by matrix multiplications. The dense connectivity that enables MLPs to model arbitrary relationships comes at the price of quadratic parameter growth with layer width. Each neuron connects to every neuron in the previous layer, creating large parameter counts that grow quadratically with network width. The computation is dominated by matrix-vector products, which are highly optimized on modern hardware. Matrix operations are inherently parallel and map efficiently to GPU architectures, with each output neuron computed independently. The optimization techniques for reducing these parameter counts, including pruning and low-rank approximations specifically targeting dense layers, are covered in <a href="ch016.xhtml#sec-model-optimizations" class="quarto-xref">Chapter¬†10</a>.</p>
<p>Convolutional Neural Networks achieve computational efficiency through parameter sharing and spatial locality, but their costs scale with both spatial dimensions and channel depth. The convolution operation‚Äôs computational intensity depends heavily on kernel size and feature map resolution. Parameter sharing across spatial locations dramatically reduces memory compared to equivalent MLPs, while computational cost grows linearly with image resolution and quadratically with kernel size. Feature map memory dominates usage and becomes prohibitive for high-resolution inputs. Spatial independence enables parallel processing across different spatial locations and channels, though memory bandwidth often becomes the limiting factor.</p>
<p>Recurrent Neural Networks optimize for memory efficiency at the cost of parallelization. Their sequential nature creates computational bottlenecks but enables processing of variable-length sequences with constant memory overhead. The hidden-to-hidden connections (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>h</mi><mn>2</mn></msup><annotation encoding="application/x-tex">h^2</annotation></semantics></math> term) dominate parameter count for large hidden states. Sequential dependencies prevent parallel processing across time, making RNNs inherently slower than feedforward alternatives. Their constant memory usage for hidden state storage makes RNNs memory-efficient for long sequences, with this efficiency coming at the cost of computational speed.</p>
<p>Transformers achieve maximum flexibility through attention mechanisms but pay a steep price in memory usage. Their quadratic scaling with sequence length creates limits on the sequences they can process. Parameter count scales with model dimension but remains independent of sequence length. The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>n</mi><mn>2</mn></msup><annotation encoding="application/x-tex">n^2</annotation></semantics></math> term from attention computation dominates for long sequences, while the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>√ó</mo><msup><mi>d</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n \times d^2</annotation></semantics></math> term from feed-forward layers dominates for short sequences. Attention matrices create the primary memory bottleneck, as each attention head must store pairwise similarities between all sequence positions, leading to prohibitive memory usage for long sequences. While parallelization is excellent across sequence positions and attention heads, the quadratic memory requirement often forces smaller batch sizes, limiting effective parallelization.</p>
<p>These complexity patterns define optimal domains for each architecture. MLPs excel when parameter efficiency is not critical, CNNs dominate for moderate-resolution spatial data, RNNs remain viable for very long sequences where memory is constrained, and Transformers excel for complex relational tasks where their computational cost justifies their computational cost through superior performance.</p>
</section>
</section>
<section id="sec-dnn-architectures-architectural-comparison-summary-f918" class="level3">
<h3>Architectural Comparison Summary</h3>
<p>The systematic analysis of each architectural family reveals distinct computational signatures that determine their suitability for different deployment scenarios. <a href="ch010.xhtml#tbl-architecture-comparison" class="quarto-xref">Table¬†4.7</a> provides a quantitative comparison across key systems metrics, enabling engineers to make informed trade-offs between model capability and computational constraints.</p>
<div id="tbl-architecture-comparison" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-architecture-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table¬†4.7: <strong>Quantitative Architecture Comparison</strong>: Computational complexity analysis across four major neural network architectures. Parameters scale with network dimensions (N=neurons, M=inputs, K=kernel size, C=channels, D=depth, H=hidden size, T=time steps, d=model dimension). Memory requirements reflect peak activation storage during training. Parallelism indicates amenability to parallel computation. Key bottlenecks represent primary performance limiting factors in typical deployments.
</figcaption>
<div aria-describedby="tbl-architecture-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top" style="width:97%;">
<colgroup>
<col style="width: 24%" />
<col style="width: 13%" />
<col style="width: 16%" />
<col style="width: 21%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Metric</strong></th>
<th style="text-align: left;"><strong>MLP</strong></th>
<th style="text-align: left;"><strong>CNN</strong></th>
<th style="text-align: left;"><strong>RNN</strong></th>
<th style="text-align: right;"><strong>Transformer</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Parameters</strong></td>
<td style="text-align: left;">O(N√óM)</td>
<td style="text-align: left;">O(K¬≤√óC√óD)</td>
<td style="text-align: left;">O(H¬≤)</td>
<td style="text-align: right;">O(N√ód¬≤)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>FLOPs/Sample</strong></td>
<td style="text-align: left;">O(N√óM)</td>
<td style="text-align: left;">O(K¬≤√óH√óW√óC)</td>
<td style="text-align: left;">O(T√óH¬≤)</td>
<td style="text-align: right;">O(N¬≤√ód)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Memory</strong></td>
<td style="text-align: left;">O(B√óM)</td>
<td style="text-align: left;">O(B√óH√óW√óC)</td>
<td style="text-align: left;">O(B√óT√óH)</td>
<td style="text-align: right;">O(B√óN¬≤)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>(Activations)</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Parallelism</strong></td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Low (Sequential)</td>
<td style="text-align: right;">High</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Key Bottleneck</strong></td>
<td style="text-align: left;">Memory BW</td>
<td style="text-align: left;">Memory BW</td>
<td style="text-align: left;">Sequential Dep.</td>
<td style="text-align: right;">Memory (N¬≤)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>This quantitative framework enables systematic architecture selection by explicitly revealing the scaling behaviors that determine computational feasibility. MLPs and CNNs achieve high parallelism but face memory bandwidth constraints as model size grows. RNNs maintain constant memory usage but sacrifice parallelism for sequential processing. Transformers achieve maximum expressivity but face quadratic memory scaling that limits sequence length. Understanding these trade-offs proves essential for matching architectural choices to deployment constraints.</p>
</section>
<section id="sec-dnn-architectures-decision-framework-dbe8" class="level3">
<h3>Decision Framework</h3>
<p>Effective architecture selection requires balancing multiple competing factors: data characteristics, computational resources, performance requirements, and deployment constraints. While data patterns provide initial guidance and complexity analysis establishes feasibility bounds, final architectural choices often involve nuanced trade-offs demanding systematic evaluation.</p>
<div id="fig-dnn-fm-framework" class="quarto-float quarto-figure quarto-figure-center" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-dnn-fm-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../media/file65.svg" alt="" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dnn-fm-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure¬†4.12: <strong>Architecture Selection Decision Framework</strong>: A systematic flowchart for choosing neural network architectures based on data characteristics and deployment constraints. The process begins with data type identification (text/sequences/images/tabular) to select initial architecture candidates (Transformers/RNNs/CNNs/MLPs), then iteratively evaluates memory budget, computational cost, inference speed, accuracy targets, and hardware compatibility.
</figcaption>
</figure>
</div>
<p><a href="ch010.xhtml#fig-dnn-fm-framework" class="quarto-xref">Figure¬†4.12</a> provides a structured approach to architecture selection decisions, ensuring consideration of all relevant factors while avoiding common pitfalls such as selection based on novelty or perceived sophistication. The decision flowchart guides systematic architecture selection by first matching data characteristics to architectural strengths, then validating against practical constraints. The process is inherently iterative‚Äîresource limitations or performance gaps often necessitate reconsidering earlier choices.</p>
<p>This framework applies through four key steps. First, data analysis: pattern types in data provide the strongest initial signal. Spatial data naturally aligns with CNNs, sequential data with RNNs. Second, progressive constraint validation: each constraint check (memory, computational budget, inference speed) acts as a filter. Failing any constraint necessitates either scaling down the current architecture or considering a fundamentally different approach.</p>
<p>Third, iterative trade-off handling when accuracy targets remain unmet. Additional model capacity may be required, necessitating a return to constraint checking. If deployment hardware cannot support the chosen architecture, reconsidering the entire architectural approach may be necessary. Fourth, anticipate multiple iterations, as real projects typically cycle through this framework several times before achieving optimal balance between data fit, computational feasibility, and deployment requirements.</p>
<p>This systematic approach prevents architecture selection based solely on novelty or perceived sophistication, ensuring alignment of choices with both problem requirements and system capabilities.</p>
</section>
</section>
<section id="sec-dnn-architectures-unified-framework-inductive-biases-099d" class="level2">
<h2>Unified Framework: Inductive Biases</h2>
<p>The architectural diversity explored‚Äîfrom MLPs to Transformers‚Äîshare a unified theoretical framework: each architecture embodies specific inductive biases that constrain the hypothesis space and guide learning toward solutions appropriate for different data types and problem structures.</p>
<p>Different architectures form a hierarchy of decreasing inductive bias. CNNs exhibit the strongest inductive biases through local connectivity, parameter sharing, and translation equivariance. These constraints dramatically reduce the parameter space while limiting flexibility to spatial data with local structure. RNNs demonstrate moderate inductive bias through sequential processing and shared temporal weights. The hidden state mechanism assumes that past information influences current processing, rendering them appropriate for temporal sequences.</p>
<p>MLPs maintain minimal architectural bias beyond layer-wise processing. Dense connectivity allows modeling arbitrary relationships but requires more data to learn structure that other architectures encode explicitly. Transformers represent adaptive inductive bias through learned attention patterns. The architecture can dynamically adjust its inductive bias based on the data, combining flexibility with the ability to discover relevant structural regularities.</p>
<p>All successful architectures implement forms of hierarchical representation learning, but through different mechanisms. CNNs build spatial hierarchies through progressive receptive field expansion, applying the spatial pattern processing framework detailed in <a href="ch010.xhtml#sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff" class="quarto-xref">Section¬†4.3</a>. RNNs build temporal hierarchies through hidden state evolution, extending the sequential processing approach from <a href="ch010.xhtml#sec-dnn-architectures-rnns-sequential-pattern-processing-ea14" class="quarto-xref">Section¬†4.4</a>. Transformers build content-dependent hierarchies through multi-head attention, applying the dynamic pattern processing mechanisms described in <a href="ch010.xhtml#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="quarto-xref">Section¬†4.5</a>.</p>
<p>This hierarchical organization reflects a principle: complex patterns can be efficiently represented through composition of simpler components. The success of deep learning stems from the discovery that gradient-based optimization can effectively learn these compositional structures when provided with appropriate architectural inductive biases.</p>
<p>The theoretical insights about representation learning have direct implications for systems engineering. Hierarchical representations require computational patterns that can efficiently compose lower-level features into higher-level abstractions. This drives system design decisions:</p>
<ul>
<li>Memory hierarchies must align with representational hierarchies to minimize data movement costs</li>
<li>Parallelization strategies must respect the dependency structure of hierarchical computation</li>
<li>Hardware accelerators must efficiently support the matrix operations that implement feature composition</li>
<li>Software frameworks must provide abstractions that enable efficient hierarchical computation across diverse architectures</li>
</ul>
<p>Understanding architectures as embodying different inductive biases helps explain both their strengths and their systems requirements, providing a principled foundation for architecture selection and system optimization decisions.</p>
</section>
<section id="sec-dnn-architectures-fallacies-pitfalls-3e82" class="level2">
<h2>Fallacies and Pitfalls</h2>
<p>Neural network architectures represent specialized computational structures designed for different data types and problem domains, which creates common misconceptions about their selection and deployment. The rich variety of architectural patterns‚Äîfrom dense networks to transformers‚Äîoften leads engineers to make choices based on novelty or perceived sophistication rather than task-specific requirements and computational constraints.</p>
<p><strong>Fallacy:</strong> <em>More complex architectures always perform better than simpler ones.</em></p>
<p>This misconception prompts teams to immediately adopt transformer-based models or elaborate architectures without understanding their requirements. While sophisticated architectures such as transformers excel at complex tasks requiring long-range dependencies, they require significantly more computational resources and memory. For numerous problems, particularly those with limited data or clear structural patterns, simpler architectures such as MLPs or CNNs achieve comparable accuracy with significantly less computational overhead. Architecture selection should correspond to problem complexity rather than defaulting to the most advanced option.</p>
<p><strong>Pitfall:</strong> <em>Ignoring the computational implications of architectural choices during model selection.</em></p>
<p>Many practitioners select architectures based solely on accuracy metrics from academic papers without considering computational requirements. A CNN‚Äôs spatial locality assumptions might deliver excellent accuracy for image tasks but require specialized memory access patterns. Similarly, RNNs‚Äô sequential dependencies create serialization bottlenecks that limit parallelization opportunities. This oversight leads to deployment failures when models cannot meet latency requirements or exceed memory constraints in production environments.</p>
<p><strong>Fallacy:</strong> <em>Architecture performance is independent of hardware characteristics.</em></p>
<p>This belief assumes that all architectures perform equally well across different hardware platforms. In reality, different architectures exploit different hardware features: CNNs benefit from specialized tensor cores, MLPs leverage high-bandwidth memory, and RNNs require efficient sequential processing capabilities. A model that achieves optimal performance on GPUs might perform poorly on mobile devices or embedded processors. Understanding hardware-architecture alignment is crucial for effective deployment strategies.</p>
<p><strong>Pitfall:</strong> <em>Mixing architectural patterns without understanding their interaction effects.</em></p>
<p>Combining different architectural components (such as adding attention layers to CNNs or using skip connections in RNNs) can create unexpected computational bottlenecks. Each architectural pattern exhibits distinct memory access patterns and computational characteristics. Naive combinations may eliminate the performance benefits of individual components or create memory bandwidth conflicts. Successful hybrid architectures require careful analysis of how different patterns interact at the system level.</p>
<p><strong>Pitfall:</strong> <em>Designing architectures without considering the full hardware-software co-design implications across the deployment pipeline.</em></p>
<p>Many architecture decisions optimize for high-end GPU performance without considering the complete system lifecycle from development through deployment. An architecture designed for large-scale compute clusters may be poorly suited for edge deployment due to memory constraints, lack of specialized compute units, or limited parallelization capabilities. Similarly, architectures optimized for inference latency might sacrifice development efficiency, leading to longer development cycles and higher computational costs. Effective architecture selection requires analyzing the entire system stack including compute infrastructure, model compilation and optimization tools, target deployment hardware, and operational constraints. The choice between CNN depth and width, transformer head configurations, or activation functions has cascading effects on memory bandwidth utilization, cache efficiency, and numerical precision requirements that must be considered holistically rather than in isolation.</p>
</section>
<section id="sec-dnn-architectures-summary-c495" class="level2">
<h2>Summary</h2>
<p>Neural network architectures form specialized computational structures tailored to process different types of data and solve distinct classes of problems. Multi-Layer Perceptrons handle tabular data through dense connections, convolutional networks exploit spatial locality in images, and recurrent networks process sequential information. Each architecture embodies specific assumptions about data structure and computational patterns. Modern transformer architectures unify many of these concepts through attention mechanisms that dynamically route information based on relevance rather than fixed connectivity patterns.</p>
<p>Despite their apparent diversity, these architectures share fundamental computational primitives that recur across different designs. Matrix multiplication operations form the computational core, whether in dense layers, convolutions, or attention mechanisms. Memory access patterns vary significantly between architectures, with some requiring sliding window operations for local processing while others demand global information aggregation. Dynamic computation patterns in attention mechanisms create data-dependent execution flows that challenge traditional optimization approaches.</p>
<div title="Key Takeaways">
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Key Takeaways</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Different architectures embody specific assumptions about data structure: MLPs for tabular data, CNNs for spatial relationships, RNNs for sequences, Transformers for flexible attention</li>
<li>Shared computational primitives including matrix operations, sliding windows, and dynamic routing form the foundation across diverse architectures</li>
<li>Memory access patterns and data movement requirements vary significantly between architectures, directly impacting system performance and optimization strategies</li>
<li>Understanding the mapping between algorithmic intent and system implementation enables effective performance optimization and hardware selection</li>
</ul>
</div>
</div>
</div>
</div>
<p>The architectural foundations established in this chapter‚Äîcomputational patterns, memory access characteristics, and data movement primitives‚Äîdirectly inform the design of specialized hardware and optimization strategies explored in subsequent chapters. Understanding that CNNs exhibit spatial locality enables the development of systolic arrays optimized for convolution operations (<a href="ch017.xhtml#sec-ai-acceleration" class="quarto-xref">Chapter¬†11</a>). Recognizing that Transformers demand quadratic memory scaling motivates attention-specific optimizations such as FlashAttention and sparse attention patterns (<a href="ch016.xhtml#sec-model-optimizations" class="quarto-xref">Chapter¬†10</a>). The progression from architectural understanding to hardware design to algorithmic optimization represents a systematic approach to ML systems engineering.</p>
<p>As architectures become more dynamic and sophisticated, the relationship between algorithmic innovation and systems optimization becomes increasingly critical for achieving practical performance gains in real-world deployments. The operational challenges of deploying and maintaining these sophisticated architectures in production environments are addressed in <a href="ch019.xhtml#sec-ml-operations" class="quarto-xref">Chapter¬†13</a>, while the broader implications for sustainable AI development, including energy efficiency considerations stemming from architectural choices, are explored in <a href="ch024.xhtml#sec-sustainable-ai" class="quarto-xref">Chapter¬†18</a>.</p>
<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
<div class="quiz-end">

</div>
<div>

</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" epub:type="footnotes">
<hr />
<aside epub:type="footnote" role="doc-footnote" id="fn1">
<p><a href="#fnref1" class="footnote-back" role="doc-backlink">1</a>. <strong>Universal Approximation Theorem</strong>: Proven independently by Cybenko (1989) and Hornik (1989), this result showed that neural networks could theoretically learn any function, a discovery that reinvigorated interest in neural networks after the ‚ÄúAI Winter‚Äù of the 1980s and established mathematical foundations for modern deep learning.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn2">
<p><a href="#fnref2" class="footnote-back" role="doc-backlink">2</a>. <strong>MNIST Dataset</strong>: Created by Yann LeCun, Corinna Cortes, and Chris Burges in 1998 from NIST‚Äôs database of handwritten digits, MNIST‚Äôs 60,000 training images became the ‚Äúfruit fly‚Äù of machine learning research. Despite human-level accuracy of 99.77% being achieved by various models, MNIST remains valuable for education because its simplicity allows students to focus on architectural concepts without data complexity distractions.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn3">
<p><a href="#fnref3" class="footnote-back" role="doc-backlink">3</a>. <strong>General Matrix Multiplication (GEMM)</strong>: The fundamental operation C = Œ±AB + Œ≤C that underlies most neural network computations. GEMM accounts for 90-95% of computation time in training deep networks and is the target of most AI hardware optimization. Optimized GEMM libraries like cuBLAS (NVIDIA), oneDNN (Intel), and CLBlast achieve 80-95% of theoretical peak performance through techniques like register blocking, vectorization, and hierarchical tiling. Modern AI accelerators are essentially specialized GEMM engines with additional support for activation functions and data movement.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn4">
<p><a href="#fnref4" class="footnote-back" role="doc-backlink">4</a>. <strong>Multiply-Accumulate (MAC)</strong>: The atomic operation in neural networks: multiply two values and add to running sum (result += a √ó b). Modern accelerators measure performance in MACs/second: NVIDIA A100 achieves 312 trillion MACs/second, while mobile chips achieve 1-10 trillion. Energy cost: ~4.6 picojoules per MAC, plus 640pJ for data movement.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn5">
<p><a href="#fnref5" class="footnote-back" role="doc-backlink">5</a>. <strong>Basic Linear Algebra Subprograms (BLAS)</strong>: Developed in the 1970s as a standard for basic vector and matrix operations, BLAS became the foundation for virtually all scientific computing. Modern implementations like Intel MKL and OpenBLAS can achieve 80-95% of theoretical peak performance on well-optimized workloads, making them necessary for neural network efficiency.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn6">
<p><a href="#fnref6" class="footnote-back" role="doc-backlink">6</a>. <strong>Tensor Cores</strong>: Specialized matrix multiplication units in modern GPUs that perform mixed-precision operations on 4√ó4 matrices per clock cycle. NVIDIA V100 tensor cores deliver 125 TFLOPS vs 15 TFLOPS from standard cores‚Äîa 8√ó improvement that revolutionized deep learning performance and made large model training feasible.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn7">
<p><a href="#fnref7" class="footnote-back" role="doc-backlink">7</a>. <strong>ImageNet Revolution</strong>: AlexNet‚Äôs dramatic victory in the 2012 ImageNet challenge <span class="citation" data-cites="krizhevsky2012imagenet">(<a href="ch058.xhtml#ref-krizhevsky2012imagenet">Krizhevsky, Sutskever, and Hinton 2017a</a>)</span> (reducing top-5 error from 25.8% to 15.3%) sparked the deep learning renaissance. ImageNet‚Äôs 14 million labeled images across 20,000 categories provided the scale needed to train deep CNNs, proving that ‚Äúbig data + big compute + big models‚Äù could achieve superhuman performance.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn8">
<p><a href="#fnref8" class="footnote-back" role="doc-backlink">8</a>. <strong>Yann LeCun and CNNs</strong>: LeCun‚Äôs 1989 LeNet architecture was inspired by Hubel and Wiesel‚Äôs discovery of simple and complex cells in cat visual cortex <span class="citation" data-cites="hubel1962receptive">(<a href="ch058.xhtml#ref-hubel1962receptive">Hubel and Wiesel 1962</a>)</span>. LeNet-5 achieved 99.2% accuracy on MNIST in 1998 (though this was the error rate on a subset, not full MNIST as we know it today) and was deployed by banks to read millions of checks daily, among the first large-scale commercial applications of neural networks.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn9">
<p><a href="#fnref9" class="footnote-back" role="doc-backlink">9</a>. <strong>Parameter Sharing</strong>: CNNs reuse the same filter weights across spatial positions, reducing parameters substantially. A CNN processing 224√ó224 images might use 3√ó3 filters with only 9 parameters per channel, versus an equivalent MLP requiring 50,176 parameters per neuron, a ~5,575x reduction per neuron enabling practical computer vision.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn10">
<p><a href="#fnref10" class="footnote-back" role="doc-backlink">10</a>. <strong>Translation Invariance</strong>: CNNs detect features regardless of spatial position. A cat‚Äôs ear is recognized whether in the top-left or bottom-right corner. This property emerges from convolution‚Äôs sliding window design and is important for computer vision, where objects appear at arbitrary locations in images.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn11">
<p><a href="#fnref11" class="footnote-back" role="doc-backlink">11</a>. <strong>Receptive Field</strong>: The region of the input that influences a particular output neuron. In CNNs, receptive fields grow with depth. A neuron in layer 3 might ‚Äúsee‚Äù a 7√ó7 region even with 3√ó3 filters, due to stacking. Understanding receptive field size is important for ensuring networks can capture features at the right scale for the task.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn12">
<p><a href="#fnref12" class="footnote-back" role="doc-backlink">12</a>. <strong>Group Theory in Neural Networks</strong>: Mathematical framework describing how CNNs preserve spatial relationships. Translation equivariance means shifting an input image shifts the output feature maps by the same amount‚Äîa property enabling CNNs to recognize objects regardless of position, foundational to computer vision success.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn13">
<p><a href="#fnref13" class="footnote-back" role="doc-backlink">13</a>. <strong>Inductive Bias</strong>: Prior assumptions built into model architecture about the structure of data. CNNs assume spatial locality and translation invariance, drastically reducing the space of functions they can learn compared to MLPs. This constraint enables better generalization with fewer parameters‚Äîa key principle in machine learning architecture design.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn14">
<p><a href="#fnref14" class="footnote-back" role="doc-backlink">14</a>. <strong>Hypothesis Space</strong>: The set of all possible functions a model can represent given its architecture and parameters. MLPs have a larger hypothesis space than CNNs for images, but CNNs‚Äô constrained space contains better solutions for visual tasks, demonstrating that architectural constraints often improve rather than limit performance. Recent work has extended these principles to other symmetry groups, developing Group-Equivariant CNNs that handle rotations and reflections <span class="citation" data-cites="cohen2016group">(<a href="ch058.xhtml#ref-cohen2016group">T. Cohen and Welling 2016</a>)</span>.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn15">
<p><a href="#fnref15" class="footnote-back" role="doc-backlink">15</a>. <strong>Feature Map</strong>: The output of applying a convolutional filter to an input, representing detected features at different spatial locations. A 64-filter layer produces 64 feature maps, each highlighting different patterns like edges, textures, or shapes. Feature maps become more abstract (detecting objects, faces) in deeper layers compared to early layers (detecting edges, colors).</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn16">
<p><a href="#fnref16" class="footnote-back" role="doc-backlink">16</a>. <strong>SIMD (Single Instruction, Multiple Data)</strong>: CPU instructions that perform the same operation on multiple data elements simultaneously. Modern x86 processors support AVX-512, enabling 16 single-precision operations per instruction, a 16x speedup over scalar code. SIMD is important for efficient neural network inference on CPUs, especially for edge deployment. Deep learning frameworks further optimize this through specialized convolution algorithms that transform the computation to better match hardware capabilities.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn17">
<p><a href="#fnref17" class="footnote-back" role="doc-backlink">17</a>. <strong>Vanishing Gradient Problem</strong>: During backpropagation through time, gradients shrink exponentially as they propagate backward through RNN layers. When recurrent weights have magnitude &lt; 1, gradients multiply by values &lt; 1 at each time step, vanishing after 5-10 steps and preventing learning of long-term dependencies‚Äîa key limitation solved by LSTMs and attention mechanisms.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn18">
<p><a href="#fnref18" class="footnote-back" role="doc-backlink">18</a>. <strong>Softmax Function</strong>: Converts a vector of real numbers into a probability distribution where all values sum to 1. Defined as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">softmax</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><msub><mo>‚àë</mo><mi>j</mi></msub><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}</annotation></semantics></math>, softmax amplifies differences between inputs (larger values get disproportionately higher probabilities) while ensuring valid attention weights for combining information sources.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn19">
<p><a href="#fnref19" class="footnote-back" role="doc-backlink">19</a>. <strong>Query-Key-Value Attention</strong>: Inspired by information retrieval systems where queries search through keys to retrieve values. In neural attention, queries and keys compute similarity scores (like a search engine matching queries to documents), while values contain the actual information to retrieve‚Äîa design that enables flexible, content-based information access.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn20">
<p><a href="#fnref20" class="footnote-back" role="doc-backlink">20</a>. <strong>‚ÄúAttention is All You Need‚Äù</strong>: This 2017 paper by Google researchers eliminated recurrence entirely, showing that attention mechanisms alone could achieve state-of-the-art results. The title itself became a rallying cry, and within 5 years, transformer-based models achieved breakthrough performance in language (GPT, BERT), vision (ViT), and beyond <span class="citation" data-cites="radford2018improving devlin2018bert dosovitskiy2021image">(<a href="ch058.xhtml#ref-radford2018improving">Radford et al. 2018</a>; <a href="ch058.xhtml#ref-devlin2018bert">Devlin et al. 2018b</a>; <a href="ch058.xhtml#ref-dosovitskiy2021image">Dosovitskiy et al. 2021</a>)</span>. This paper marked a historical turning point in deep learning, demonstrating that the sequential processing that defined RNNs and LSTMs was no longer necessary; attention mechanisms could capture both short and long-range dependencies through parallel computation. While the basic attention mechanism allows for content-based weighting of information from a source sequence, Transformers extend this idea by applying attention within a single sequence, enabling each element to attend to all other elements including itself.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn21">
<p><a href="#fnref21" class="footnote-back" role="doc-backlink">21</a>. <strong>Attention Scaling</strong>: Without the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math> scaling factor, large dot products would cause the softmax to saturate, producing gradients close to zero and hindering learning. This mathematical insight enables stable optimization of large Transformer models.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn22">
<p><a href="#fnref22" class="footnote-back" role="doc-backlink">22</a>. <strong>Backpropagation Algorithm</strong>: While the chain rule was known since the 1600s, Rumelhart, Hinton, and Williams (1986) showed how to efficiently apply it to train multi-layer networks. This ‚Äúlearning by error propagation‚Äù algorithm made deep networks practical and remains virtually unchanged in modern systems‚Äîa testament to its importance.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn23">
<p><a href="#fnref23" class="footnote-back" role="doc-backlink">23</a>. <strong>ResNet Revolution</strong>: ResNet (2016) solved the ‚Äúdegradation problem‚Äù where deeper networks performed worse than shallow ones. The key insight: adding identity shortcuts (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>‚Ñ±</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùê±</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>ùê±</mi></mrow><annotation encoding="application/x-tex">\mathcal{F}(\mathbf{x}) + \mathbf{x}</annotation></semantics></math>) let networks learn residual mappings instead of full transformations, enabling training of 1000+ layer networks and winning ImageNet 2015.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn24">
<p><a href="#fnref24" class="footnote-back" role="doc-backlink">24</a>. <strong>LSTM Origins</strong>: Sepp Hochreiter and J√ºrgen Schmidhuber invented LSTMs in 1997 to solve the ‚Äúvanishing gradient problem‚Äù that plagued RNNs. Their gating mechanism was inspired by biological neurons‚Äô ability to selectively retain information‚Äîa breakthrough that enabled sequence modeling and facilitated modern language models.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn25">
<p><a href="#fnref25" class="footnote-back" role="doc-backlink">25</a>. <strong>Gated Recurrent Unit (GRU)</strong>: Simplified version of LSTM introduced by Cho et al.¬†(2014) with only 2 gates instead of 3, reducing parameters by ~25% while maintaining similar performance. GRUs became popular for their computational efficiency and easier training, proving that architectural simplification can sometimes improve rather than hurt performance.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn26">
<p><a href="#fnref26" class="footnote-back" role="doc-backlink">26</a>. <strong>Vision Transformers (ViTs)</strong>: Google‚Äôs 2021 breakthrough showed that pure transformers could match CNN performance on ImageNet by treating image patches as ‚Äúwords.‚Äù ViTs split a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>224</mn><mo>√ó</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">224\times 224</annotation></semantics></math> image into <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>√ó</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">16\times 16</annotation></semantics></math> patches (196 ‚Äútokens‚Äù), proving that attention mechanisms could replace convolutional inductive biases with sufficient data.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn27">
<p><a href="#fnref27" class="footnote-back" role="doc-backlink">27</a>. <strong>Im2col (Image-to-Column)</strong>: A preprocessing technique that converts convolution operations into matrix multiplications by unfolding image patches into column vectors. A 3√ó3 convolution on a 224√ó224 image creates a matrix with ~50,000 columns, enabling efficient GEMM execution but increasing memory usage 9√ó due to overlapping patches. This transformation explains why convolutions are actually matrix operations in modern ML accelerators.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn28">
<p><a href="#fnref28" class="footnote-back" role="doc-backlink">28</a>. <strong>Systolic Array Architecture</strong>: Developed at Carnegie Mellon in 1978, systolic arrays excel at matrix operations by streaming data through a grid of processing elements. Google‚Äôs TPU v4 achieves 275 TFLOPS (bfloat16) with ~200W typical power consumption‚Äîachieving approximately 1.38 TFLOPS/W efficiency, roughly 2-3x more energy-efficient than comparable GPUs for ML workloads.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn29">
<p><a href="#fnref29" class="footnote-back" role="doc-backlink">29</a>. <strong>Parameter Scaling</strong>: The leap from AlexNet‚Äôs 62 million parameters (2012) to GPT-3‚Äôs 175 billion parameters (2020) represents a 3,000x increase in just 8 years. Modern models like GPT-4 may exceed 1 trillion parameters, requiring specialized distributed computing infrastructure and consuming megawatts of power during training.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn30">
<p><a href="#fnref30" class="footnote-back" role="doc-backlink">30</a>. <strong>Tensor Processing Units</strong>: Google‚Äôs TPUs emerged from their need to run neural networks on billions of searches daily. First deployed secretly in 2015, TPUs achieve 15-30x better performance per watt than GPUs for inference. The TPU‚Äôs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn><mo>√ó</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">128\times 128</annotation></semantics></math> systolic array performs 65,536 multiply-accumulate operations per clock cycle, revolutionizing AI hardware design. These specialized units can perform many multiply-accumulate operations in parallel, dramatically accelerating the core computations of neural networks.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn31">
<p><a href="#fnref31" class="footnote-back" role="doc-backlink">31</a>. <strong>High Bandwidth Memory (HBM)</strong>: Stacked DRAM technology providing 1+ TB/s bandwidth compared to 500 GB/s for traditional GDDR6, developed by AMD and Hynix. HBM enables the massive data movement required by modern AI workloads‚ÄîGPT-3 training requires moving 1.75 TB of parameters through memory during each forward pass.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn32">
<p><a href="#fnref32" class="footnote-back" role="doc-backlink">32</a>. <strong>Scratchpad Memory</strong>: Programmer-controlled on-chip memory providing predictable, fast access without cache management overhead. Unlike caches, scratchpads require explicit data movement but enable precise control over memory allocation‚Äîcritical for neural network accelerators where memory access patterns are known and performance must be deterministic.</p>
</aside>
<aside epub:type="footnote" role="doc-footnote" id="fn33">
<p><a href="#fnref33" class="footnote-back" role="doc-backlink">33</a>. <strong>Arithmetic Intensity</strong>: The ratio of floating-point operations to memory accesses, measured in FLOPS per byte. High arithmetic intensity (&gt;10 FLOPS/byte) enables efficient hardware utilization, while low intensity (&lt;1 FLOPS/byte) makes workloads memory-bound. Attention mechanisms typically have low arithmetic intensity, explaining their energy inefficiency.</p>
</aside>
</section>
</body>
</html>
