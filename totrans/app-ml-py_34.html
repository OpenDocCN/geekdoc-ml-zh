<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Generative Adversarial Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Generative Adversarial Networks</h1>
<blockquote>原文：<a href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_GAN.html">https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_GAN.html</a></blockquote>

<p>Michael J. Pyrcz, Professor, The University of Texas at Austin</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
<p>Chapter of e-book “Applied Machine Learning in Python: a Hands-on Guide with Code”.</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite this e-Book as:</p>
<p>Pyrcz, M.J., 2024, <em>Applied Machine Learning in Python: A Hands-on Guide with Code</em> [e-book]. Zenodo. doi:10.5281/zenodo.15169138 <a class="reference external" href="https://doi.org/10.5281/zenodo.15169138"><img alt="DOI" src="../Images/7e4ea662f44af1eae87e87ecbb962ff4.png" data-original-src="https://zenodo.org/badge/863274676.svg"/></a></p>
</div>
<p>The workflows in this book and more are available here:</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite the MachineLearningDemos GitHub Repository as:</p>
<p>Pyrcz, M.J., 2024, <em>MachineLearningDemos: Python Machine Learning Demonstration Workflows Repository</em> (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312. GitHub repository: <a class="github reference external" href="https://github.com/GeostatsGuy/MachineLearningDemos">GeostatsGuy/MachineLearningDemos</a> <a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.13835312"><img alt="DOI" src="../Images/4e3a59c17d684b06a170c4af84e0f631.png" data-original-src="https://zenodo.org/badge/862519860.svg"/></a></p>
</div>
<p>By Michael J. Pyrcz <br/>
© Copyright 2024.</p>
<p>This chapter is a tutorial for / demonstration of <strong>Generative Adversarial Networks</strong>.</p>
<p><strong>YouTube Lecture</strong>: check out my lectures on:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://youtu.be/A9PiCMY_6nM?si=NxWSU_5RgQ4w55EL">Artificial Neural Networks</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/za2my_XDoOs?si=LeHU6p2_fc9dX4Yt">Convolutional Neural Networks</a></p></li>
<li><p>Generative Adversarial Networks (TBA)</p></li>
</ul>
<p>These lectures are all part of my <a class="reference external" href="https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&amp;si=XonjO2wHdXffMpeI">Machine Learning Course</a> on YouTube with linked well-documented Python workflows and interactive dashboards. My goal is to share accessible, actionable, and repeatable educational content. If you want to know about my motivation, check out <a class="reference external" href="https://michaelpyrcz.com/my-story">Michael’s Story</a>.</p>
<section id="motivation">
<h2>Motivation</h2>
<p>What if we put together machines? Working in a competitive, adversarial manner?</p>
<ul class="simple">
<li><p>Could we make a more powerful machine learning model that learns its own loss function!</p></li>
<li><p>Could we make images that don’t collapse to exact reproduction of the images in the training set?</p></li>
</ul>
<p>Generative neural networks are very powerful, nature inspired computing deep learning method to make fake, but realistic, images by application of convolutional neural networks, an analogy of visual cortex that extend the ability of our artificial neural networks to better work with images.</p>
<p>Nature inspired computing is looking to nature for inspiration to develop novel problem-solving methods,</p>
<ul class="simple">
<li><p><strong>artificial neural networks</strong> are inspired by biological neural networks</p></li>
<li><p><strong>nodes</strong> in our model are artificial neurons, simple processors</p></li>
<li><p><strong>connections</strong> between nodes are artificial synapses</p></li>
<li><p><strong>perceptive fields</strong> regularization to improve generalization and efficiency</p></li>
</ul>
<p>intelligence emerges from many connected simple processors. For the remainder of this chapter, I will used the terms nodes and connections to describe our convolutional neural network.</p>
</section>
<section id="artificial-and-convolutional-neural-networks">
<h2>Artificial and Convolutional Neural Networks</h2>
<p>If you have not, take this opportunity to review my previous chapters in the e-book on,</p>
<p><a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ANN.html">Artificial Neural Networks</a></p>
<p>The <strong>main takeaways</strong> from my artificial neural network chapter are as follows,</p>
<ul class="simple">
<li><p><strong>architecture of a neural network</strong>, including its fundamental components, nodes (neurons) and the weighted connections between them.</p></li>
<li><p><strong>forward pass</strong> computation through the network, where each node computes a weighted sum of its inputs (including a bias term), followed by the application of a nonlinear activation function.</p></li>
<li><p><strong>computation of the error derivative</strong>, which is then backpropagated through the network via the chain rule to determine the gradients of the loss function with respect to each weight and bias.</p></li>
<li><p><strong>aggregation of these gradients</strong> across all samples in a training batch, typically by averaging, to update the model parameters.</p></li>
<li><p><strong>iterative training process</strong>, where the model is trained over multiple batches and epochs (passes over all the data) to continually refine the weights and biases until the model achieves an acceptable error rate on the test data.</p></li>
</ul>
<p><a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_CNN.html">Convolutional Neural Networks</a></p>
<p>The main takeaways from my convolutional neural network chapter are as follows,</p>
<ul class="simple">
<li><p><strong>regularization</strong> of image data with receptive fields to preserve spatial information and to avoid overfit.</p></li>
<li><p><strong>convolutional kernels</strong> with learnable weights to extraction information from images.</p></li>
</ul>
<p>For both of these chapters, I have included links to my recorded lectures and to neural networks built from scratch with NumPy only!</p>
</section>
<section id="id1">
<h2>Generative Adversarial Networks</h2>
<p>If we start with a convolutional neural network and we flip it, i.e., reverse the order of the operations,</p>
<ul class="simple">
<li><p>we have a machine that maps from a 1D vector of values, to an image, i.e., we can generate fake images by randomly assigning latent values</p></li>
<li><p>to accomplish this instead of convolution operations with activation, we have transpose convolution operations with activation to move to the next feature map</p></li>
<li><p>recall we also perform non-linear activation at each feature map to prevent network collapse</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/edf859adfdeba17f094dbc0415d2bb06.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/GAN/flipcnn.png"/>
  <figcaption style="text-align: center;"> A convolutional neural network flipped and convolution replaced with transpose convolution to go from a 1D random latent vector to a random image.
</figcaption>
</figure>
<p>But how do we train this flipped convolutional neural network to make good images?</p>
<ul class="simple">
<li><p>we could take training images and score the difference between our generated fake images, for example, with a pixel-wise squared error (L2 norm)</p></li>
<li><p>but if we did this, our machine learning model would only learn how to make this image or a limited set of training images and that would not be useful</p></li>
</ul>
<p>We want to make a diverse set of image realizations, that look and behave correctly. This is the simulation paradigm at the heart of geostatistics,</p>
<ul class="simple">
<li><p>to learn more about the simulation paradigm from geostatistics, see my <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/GeostatsPy_simulation.html">Simulation Chapter</a> from my free, online e-book, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book">Applied Geostatistics in Python</a>.</p></li>
</ul>
<p>Instead of a typically loss function, we apply a classification convolutional neural network to map from the image to a probability of a real image, i.e., our loss function is effectively a network that learns to score the loss during training.</p>
<p>We have 2 neural networks in our GAN,</p>
<ul class="simple">
<li><p><strong>generator</strong> - flipped convolutional neural network that makes random fake images</p></li>
<li><p><strong>discriminator</strong> - classification convolutional neural netwrok that calculates the probability that an image is real</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/464dd59ea97be3602e14c9ece74a40a6.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/GAN/ganschematic.png"/>
  <figcaption style="text-align: center;"> A convolutional neural network flipped and convolution replaced with transpose convolution to go from a 1D random latent vector to a random image.
</figcaption>
</figure>
</section>
<section id="indirect-adversarial-learning">
<h2>Indirect, Adversarial Learning</h2>
<p>How do we train these two coupled networks? We call each network an agent and we train them competively, e.g., they compete while learning!</p>
<ul class="simple">
<li><p>agent 1, Generator, is not trained to minimize an loss function with respect to training data (training image), no MSE!</p></li>
<li><p>instead the agent 1, Generator, is trained to fool agent 2, Discriminator</p></li>
<li><p>agent 2, Discriminator, is learning at the same time to tell the difference between the real training images and the fakes from agent 1, generator</p></li>
</ul>
<p>Each agent has their own competitive goals,</p>
<ul class="simple">
<li><p>Generator – make fakes that Discriminator classifies as real</p></li>
<li><p>Discriminator – correctly classify fake and real images</p></li>
</ul>
<p>Note, the generator never sees the real images, but by learning to fool the discriminator learns to make images like the real training images.</p>
<p>The GAN loss function is stated as,</p>
<div class="math notranslate nohighlight">
\[
\min_{\theta_G} \, \max_{\theta_D} \;
\mathbb{E}_{\mathbf{y} \sim p_{\text{data}}} \left[ \log D_{\theta_D}(\mathbf{y}) \right]
+
\mathbb{E}_{\mathbf{x} \sim p_{\mathbf{x}}} \left[ \log \left( 1 - D_{\theta_D}(G_{\theta_G}(\mathbf{x})) \right) \right]
\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta_D\)</span> - parameters (weights, biases) of the <strong>discriminator</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_G\)</span> - parameters of the <strong>generator</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(D_{\theta_D}(\cdot)\)</span> - discriminator output, given the discriminator parameters <span class="math notranslate nohighlight">\(\theta_D\)</span> (probability input is real)</p></li>
<li><p><span class="math notranslate nohighlight">\(G_{\theta_G}(\mathbf{x})\)</span> - output of the  Generator output given latent input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y} \sim p_{\text{data}}\)</span> - training images from the <strong>real image set</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x} \sim p_{\mathbf{x}}\)</span> - latent input sampled from known prior (e.g. uniform or normal)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[\cdot]\)</span> - expectation over data (i.e., average over all samples)</p></li>
<li><p><span class="math notranslate nohighlight">\(\log D(\cdot)\)</span> - log-likelihood that the discriminator assigns input as real</p></li>
<li><p><span class="math notranslate nohighlight">\(\log(1 - D(G(\cdot)))\)</span> - log-likelihood that discriminator assigns fake to generator’s output</p></li>
</ul>
<p>The discriminator wants to <strong>maximize</strong>,</p>
<div class="math notranslate nohighlight">
\[
\log D(\mathbf{y}) + \log(1 - D(G(\mathbf{x})))
\]</div>
<ul class="simple">
<li><p>tries to <strong>correctly predicts real</strong> training images as real, <span class="math notranslate nohighlight">\(\log D(\mathbf{y}) \rightarrow 0.0\)</span></p></li>
<li><p>and to <strong>correctly predicts generated</strong> fake training images as not real, <span class="math notranslate nohighlight">\(\log(1 - D(G(\mathbf{x}))) \rightarrow 0.0\)</span></p></li>
</ul>
<p>The generator want to <strong>minimize</strong>,</p>
<div class="math notranslate nohighlight">
\[
\log(1 - D(G(\mathbf{x})))
\]</div>
<ul class="simple">
<li><p>tries to <strong>fool the discriminator</strong>, discriminator classifies fake training images as  real, <span class="math notranslate nohighlight">\(\log(1 - D(G(\mathbf{x}))) \rightarrow -\infty\)</span></p></li>
</ul>
<p>To assist with understanding the GAN loss function and the system of competing agents, consider these end members,</p>
<ol class="arabic simple">
<li><p><strong>Perfect Discriminator</strong> - if the discriminator is perfect,</p></li>
</ol>
<ul class="simple">
<li><p>all real training images are classified as real, <span class="math notranslate nohighlight">\(D(\mathbf{y}) = 1\)</span></p></li>
<li><p>all fake images from the generator are classified as real, <span class="math notranslate nohighlight">\(D(G(\mathbf{x})) = 0\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\quad\)</span> then the discriminator loss is,</p>
<div class="math notranslate nohighlight">
\[
\log(1) + \log(1 - 0) = 0 + \log(1) = 0
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> this sounds like good news, i.e., the generator will then improve to catch up with the discriminator, but what actually happens is,</p>
<ul class="simple">
<li><p>generator receives <strong>no loss gradients</strong>, because the generators gradients,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial \log(1 - D(G(z)))}{\partial \theta_G}
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> if <span class="math notranslate nohighlight">\(D(G(z)) \to 0\)</span>, this derivative becomes <strong>zero</strong>, so training stalls and <strong>the generator doesn’t learn</strong>,</p>
<ul class="simple">
<li><p>this is practically solved by substituting <strong>non-saturating generator loss</strong> for the generator,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L_G = -\mathbb{E}_{z \sim p_z}[\log D(G(z))]
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> if <span class="math notranslate nohighlight">\(D(G(z)) \to 0\)</span>, then <span class="math notranslate nohighlight">\(\log D(G(z)) \to -\infty\)</span>, so the gradient becomes <strong>large</strong>, giving the generator a strong learning signal.</p>
<ol class="arabic simple" start="2">
<li><p><strong>Perfect Generator</strong> - if the generator is perfect,</p></li>
</ol>
<ul class="simple">
<li><p>all fake images have the same distribution as the real training images, <span class="math notranslate nohighlight">\(G(\mathbf{x}) \sim p_{\text{data}}\)</span></p></li>
<li><p>the best the discriminator can do is to assign a anive classification, <span class="math notranslate nohighlight">\(D(\cdot) = 0.5\)</span>, for all fake and real training images</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\quad\)</span> then the loss is,</p>
<div class="math notranslate nohighlight">
\[
\log(0.5) + \log(1 - 0.5) = \log(0.5) + \log(0.5) = -\log 4
\]</div>
<ul class="simple">
<li><p>discriminator is <strong>maximally confused</strong></p></li>
<li><p>this is a <strong>Nash equilibrium</strong> for the GAN, because no player can improve their outcome by unilaterally changing their strategy, assuming the other player’s strategy stays the same, generator is already making perfect images and discriminator can only guess naively, 50/50 real and fake.</p></li>
</ul>
</section>
<section id="import-required-packages">
<h2>Import Required Packages</h2>
<p>We will also need some standard packages. These should have been installed with Anaconda 3.</p>
<ul class="simple">
<li><p>recall our goal is to build a convolutional neural network by-hand with only basic math and array operations, so we only need NumPy along with matplotlib for plotting.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="o">%</span><span class="k">matplotlib</span> inline                                         
<span class="n">suppress_warnings</span> <span class="o">=</span> <span class="kc">True</span>                                      <span class="c1"># toggle to supress warnings</span>
<span class="kn">import</span> <span class="nn">os</span>                                                     <span class="c1"># set working directory</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                                            <span class="c1"># arrays and matrix math</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>                               <span class="c1"># for plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">patches</span>                          <span class="c1"># fancy box around agents</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span> <span class="n">AutoMinorLocator</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="n">FuncFormatter</span>
<span class="kn">import</span> <span class="nn">copy</span>                                                   <span class="c1"># deep copy dictionaries</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>                                         <span class="c1"># default color bar, no bias and friendly for color vision defeciency</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># grid behind plotting elements</span>
<span class="k">if</span> <span class="n">suppress_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>  
    <span class="kn">import</span> <span class="nn">warnings</span>                                           <span class="c1"># supress any warnings for this demonstration</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span> 
<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>                                                     <span class="c1"># random number seed for workflow repeatability</span>
</pre></div>
</div>
</div>
</div>
<p>If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing ‘python -m pip install [package-name]’. More assistance is available with the respective package docs.</p>
</section>
<section id="declare-functions">
<h2>Declare Functions</h2>
<p>Here’s the functions to make, train and visualize our generative adversarial network, including the steps,</p>
<ul class="simple">
<li><p>make a simple set of synthetic data</p></li>
<li><p>initialize the weights in our generator and discriminator</p></li>
<li><p>apply our generator and discrimintor</p></li>
<li><p>calculate the error derivative and update the generator and discriminator weights and biases</p></li>
</ul>
<p>Here’s a list of the functions,</p>
<ol class="arabic simple">
<li><p><strong>generate_real_data</strong> -  synthetic data generator</p></li>
<li><p><strong>initialize_generator_weights</strong> - assign small random weights and bias for generator</p></li>
<li><p><strong>initialize_discriminator_weights</strong> - assign small random weights and bias for discriminator</p></li>
<li><p><strong>generator_forward</strong> - calculate a set of fake data with the generator given a set of latent values and the current weights and biases</p></li>
<li><p><strong>discriminator_forward</strong> - calculate the probability of a real image over a set of images and return a 1D ndarray of probabilities</p></li>
<li><p><strong>sigmoid</strong> - activation function to apply in the generator and discriminator</p></li>
<li><p><strong>generator_gradients</strong> - compute generator gradients averaged over the batch</p></li>
<li><p><strong>discriminator_gradients</strong> - compute generator gradients averaged over the batch</p></li>
</ol>
<p>Here are the functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>                                               <span class="c1"># sigmoid activation function</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">generate_real_data</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">slope_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">),</span> <span class="n">residual_std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">):</span> <span class="c1"># make a synthetic training image set</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Generate real 3-node images with decreasing linear trend plus noise.</span>
<span class="sd">    Standardize each to have mean 0.5.</span>
<span class="sd">    Returns shape (batch_size, 3)</span>
<span class="sd">    """</span>
    <span class="n">slopes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">slope_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">slope_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">base</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># node positions for linear trend</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">trend</span> <span class="o">=</span> <span class="n">slopes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">base</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">residual_std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">trend</span> <span class="o">+</span> <span class="n">residual</span>
        <span class="n">sample</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>                       <span class="c1"># standardize to mean 0.5</span>
        <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="k">def</span> <span class="nf">initialize_generator_weights</span><span class="p">():</span>                           <span class="c1"># initialize the generator weights and return as a dictionary</span>
    <span class="c1"># Small random weights and bias for generator</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">'lambda_12'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">'lambda_13'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">'lambda_14'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">'b'</span><span class="p">:</span> <span class="mf">0.0</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">generator_forward</span><span class="p">(</span><span class="n">L1_latent</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span><span class="n">return_pre_activation</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> <span class="c1"># given latent vector and generator weights return a set of fake images</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    L1_latent: ndarray shape (batch_size,)</span>
<span class="sd">    weights: dict with keys 'lambda_1_2', 'lambda_1_3', 'lambda_1_4', 'b'</span>
<span class="sd">    Returns output ndarray shape (batch_size, 3)</span>
<span class="sd">    """</span>
    <span class="n">O2in</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'lambda_12'</span><span class="p">]</span> <span class="o">*</span> <span class="n">L1_latent</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'b'</span><span class="p">]</span>
    <span class="n">O3in</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'lambda_13'</span><span class="p">]</span> <span class="o">*</span> <span class="n">L1_latent</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'b'</span><span class="p">]</span>
    <span class="n">O4in</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'lambda_14'</span><span class="p">]</span> <span class="o">*</span> <span class="n">L1_latent</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'b'</span><span class="p">]</span>

    <span class="n">O2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">O2in</span><span class="p">)</span>
    <span class="n">O3</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">O3in</span><span class="p">)</span>
    <span class="n">O4</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">O4in</span><span class="p">)</span>
    
    <span class="n">Oin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">O2in</span><span class="p">,</span> <span class="n">O3in</span><span class="p">,</span> <span class="n">O4in</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">O2</span><span class="p">,</span> <span class="n">O3</span><span class="p">,</span> <span class="n">O4</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">;</span> 

    <span class="k">if</span> <span class="n">return_pre_activation</span><span class="p">:</span> 
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">O2in</span><span class="p">,</span> <span class="n">O3in</span><span class="p">,</span> <span class="n">O4in</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">O2</span><span class="p">,</span> <span class="n">O3</span><span class="p">,</span> <span class="n">O4</span><span class="p">])</span><span class="o">.</span><span class="n">T</span> <span class="c1"># shape (batch_size, 3)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">O2</span><span class="p">,</span> <span class="n">O3</span><span class="p">,</span> <span class="n">O4</span><span class="p">])</span><span class="o">.</span><span class="n">T</span> <span class="c1"># shape (batch_size, 3)</span>

<span class="k">def</span> <span class="nf">initialize_discriminator_weights</span><span class="p">():</span>                       <span class="c1"># initialize the discriminator weights and return as a dictionary               </span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">'lambda_58'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">'lambda_68'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">'lambda_78'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">'c'</span><span class="p">:</span> <span class="mf">0.0</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">discriminator_forward</span><span class="p">(</span><span class="n">I5</span><span class="p">,</span> <span class="n">I6</span><span class="p">,</span> <span class="n">I7</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>               <span class="c1"># given a set of images return the discriminator probability of real image</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Inputs: I5, I6, I7 shape (batch_size,)</span>
<span class="sd">    weights: dict with keys 'lambda_58', 'lambda_68', 'lambda_78', 'c'</span>
<span class="sd">    Returns probability ndarray shape (batch_size,)</span>
<span class="sd">    """</span>
    <span class="n">dO8in</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="s1">'lambda_58'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I5</span> <span class="o">+</span> 
         <span class="n">weights</span><span class="p">[</span><span class="s1">'lambda_68'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I6</span> <span class="o">+</span> 
         <span class="n">weights</span><span class="p">[</span><span class="s1">'lambda_78'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I7</span> <span class="o">+</span> 
         <span class="n">weights</span><span class="p">[</span><span class="s1">'c'</span><span class="p">])</span>
    <span class="n">dO8</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">dO8in</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dO8</span><span class="p">,</span> <span class="n">dO8in</span>

<span class="k">def</span> <span class="nf">discriminator_gradients</span><span class="p">(</span><span class="n">I5</span><span class="p">,</span> <span class="n">I6</span><span class="p">,</span> <span class="n">I7</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>     <span class="c1"># given set of images, true labels, and discriminator weights return the gradients</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Compute discriminator gradients averaged over batch.</span>
<span class="sd">    y_true: labels (1 for real, 0 for fake), shape (batch_size,)</span>
<span class="sd">    """</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y_pred</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">discriminator_forward</span><span class="p">(</span><span class="n">I5</span><span class="p">,</span> <span class="n">I6</span><span class="p">,</span> <span class="n">I7</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
    <span class="n">dO8in</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span>                                  <span class="c1"># shape (batch_size,), note this solution integrates the sigmoid activation at O8</span>
    
    <span class="n">grad_lambda_58</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO8in</span> <span class="o">*</span> <span class="n">I5</span><span class="p">)</span>
    <span class="n">grad_lambda_68</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO8in</span> <span class="o">*</span> <span class="n">I6</span><span class="p">)</span>
    <span class="n">grad_lambda_78</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO8in</span> <span class="o">*</span> <span class="n">I7</span><span class="p">)</span>
    <span class="n">grad_c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO8in</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">'lambda_58'</span><span class="p">:</span> <span class="n">grad_lambda_58</span><span class="p">,</span>
        <span class="s1">'lambda_68'</span><span class="p">:</span> <span class="n">grad_lambda_68</span><span class="p">,</span>
        <span class="s1">'lambda_78'</span><span class="p">:</span> <span class="n">grad_lambda_78</span><span class="p">,</span>
        <span class="s1">'c'</span><span class="p">:</span> <span class="n">grad_c</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">generator_gradients</span><span class="p">(</span><span class="n">L1_latent</span><span class="p">,</span> <span class="n">weights_g</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">):</span>     <span class="c1"># given latent vector, generator and discriminator weights return the gradients</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Compute gradients of generator weights using discriminator feedback.</span>
<span class="sd">    L1_latent: shape (batch_size,)</span>
<span class="sd">    weights_g: generator weights dict</span>
<span class="sd">    weights_d: discriminator weights dict</span>
<span class="sd">    """</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">L1_latent</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1">#O = generator_forward(L1_latent, weights_g)               # generator outputs, shape (batch_size, 3)</span>
    <span class="n">O_in</span><span class="p">,</span> <span class="n">O</span> <span class="o">=</span> <span class="n">generator_forward</span><span class="p">(</span><span class="n">L1_latent</span><span class="p">,</span> <span class="n">weights_g</span><span class="p">,</span> <span class="n">return_pre_activation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">O2_in</span><span class="p">,</span> <span class="n">O3_in</span><span class="p">,</span> <span class="n">O4_in</span> <span class="o">=</span> <span class="n">O_in</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">O_in</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">O_in</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span>
    <span class="n">O2</span><span class="p">,</span> <span class="n">O3</span><span class="p">,</span> <span class="n">O4</span> <span class="o">=</span> <span class="n">O</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">O</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">O</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">I5</span><span class="p">,</span> <span class="n">I6</span><span class="p">,</span> <span class="n">I7</span> <span class="o">=</span> <span class="n">O</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">O</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">O</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
    
    <span class="n">y_pred</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">discriminator_forward</span><span class="p">(</span><span class="n">I5</span><span class="p">,</span> <span class="n">I6</span><span class="p">,</span> <span class="n">I7</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">)</span>  <span class="c1"># discriminator forward pass</span>
    
    <span class="n">dO8in</span> <span class="o">=</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="mi">1</span>                                           <span class="c1"># gradient of loss w.r.t discriminator logit for generator loss, shape (batch_size,)</span>
    
    <span class="n">dO2</span> <span class="o">=</span> <span class="n">dO8in</span> <span class="o">*</span> <span class="n">weights_d</span><span class="p">[</span><span class="s1">'lambda_58'</span><span class="p">]</span>                         <span class="c1"># gradients w.r.t generator outputs</span>
    <span class="n">dO3</span> <span class="o">=</span> <span class="n">dO8in</span> <span class="o">*</span> <span class="n">weights_d</span><span class="p">[</span><span class="s1">'lambda_68'</span><span class="p">]</span>
    <span class="n">dO4</span> <span class="o">=</span> <span class="n">dO8in</span> <span class="o">*</span> <span class="n">weights_d</span><span class="p">[</span><span class="s1">'lambda_78'</span><span class="p">]</span>

    <span class="n">dO2in</span> <span class="o">=</span> <span class="n">dO2</span> <span class="o">*</span> <span class="n">O2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">O2</span><span class="p">)</span>                                  <span class="c1"># Backprop through generator sigmoid activation</span>
    <span class="n">dO3in</span> <span class="o">=</span> <span class="n">dO3</span> <span class="o">*</span> <span class="n">O3</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">O3</span><span class="p">)</span>
    <span class="n">dO4in</span> <span class="o">=</span> <span class="n">dO4</span> <span class="o">*</span> <span class="n">O4</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">O4</span><span class="p">)</span>
    
    <span class="n">grad_lambda_12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO2in</span> <span class="o">*</span> <span class="n">L1_latent</span><span class="p">)</span>                 <span class="c1"># gradients w.r.t generator weights and bias</span>
    <span class="n">grad_lambda_13</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO3in</span> <span class="o">*</span> <span class="n">L1_latent</span><span class="p">)</span>
    <span class="n">grad_lambda_14</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO4in</span> <span class="o">*</span> <span class="n">L1_latent</span><span class="p">)</span>
    <span class="n">grad_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO2in</span> <span class="o">+</span> <span class="n">dO3in</span> <span class="o">+</span> <span class="n">dO4in</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">'lambda_12'</span><span class="p">:</span> <span class="n">grad_lambda_12</span><span class="p">,</span>
        <span class="s1">'lambda_13'</span><span class="p">:</span> <span class="n">grad_lambda_13</span><span class="p">,</span>
        <span class="s1">'lambda_14'</span><span class="p">:</span> <span class="n">grad_lambda_14</span><span class="p">,</span>
        <span class="s1">'b'</span><span class="p">:</span> <span class="n">grad_b</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">fancybox</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">xy</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">text_color</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span> <span class="c1"># a dashed fancy box for the GAN plot</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Draws a dashed, rounded rectangle on a given axes.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - ax: The matplotlib axes to draw on</span>
<span class="sd">    - xy: (x, y) tuple for bottom-left corner of the box</span>
<span class="sd">    - width: Width of the box</span>
<span class="sd">    - height: Height of the box</span>
<span class="sd">    - label: Optional label text to display centered above the box</span>
<span class="sd">    - edgecolor: Border color of the box</span>
<span class="sd">    - text_color: Color of the label text (defaults to edgecolor)</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="n">text_color</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">text_color</span> <span class="o">=</span> <span class="n">edgecolor</span>

    <span class="n">box</span> <span class="o">=</span> <span class="n">patches</span><span class="o">.</span><span class="n">FancyBboxPatch</span><span class="p">(</span>                             <span class="c1"># draw box</span>
        <span class="n">xy</span><span class="p">,</span>
        <span class="n">width</span><span class="p">,</span>
        <span class="n">height</span><span class="p">,</span>
        <span class="n">boxstyle</span><span class="o">=</span><span class="s2">"round,pad=0.02"</span><span class="p">,</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">edgecolor</span><span class="o">=</span><span class="n">edgecolor</span><span class="p">,</span>
        <span class="n">facecolor</span><span class="o">=</span><span class="s2">"none"</span><span class="p">,</span>
        <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">box</span><span class="p">)</span>

    <span class="n">x_center</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">width</span> <span class="o">/</span> <span class="mi">2</span>                              <span class="c1"># add label text above the box</span>
    <span class="n">y_top</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">height</span> <span class="o">+</span> <span class="mf">0.02</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x_center</span><span class="p">,</span> <span class="n">y_top</span> <span class="o">+</span> <span class="mf">0.02</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'bottom'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">text_color</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add_grid2</span><span class="p">():</span>                                              <span class="c1"># add grid lines</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks </span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="set-the-working-directory">
<h2>Set the Working Directory</h2>
<p>I always like to do this so I don’t lose files and to simplify subsequent read and writes (avoid including the full address each time).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1">#os.chdir("c:/PGE383")                                        # set the working directory</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualize-the-generative-adversarial-network">
<h2>Visualize the Generative Adversarial Network</h2>
<p>We are implementing a minimal Generative Adversarial Network (GAN) with the 2 agents,</p>
<ul class="simple">
<li><p><strong>Generator</strong> - that produces 3-node outputs (like tiny 1D images) from a single latent input</p></li>
<li><p><strong>Discriminator</strong> - that evaluates these outputs to distinguish between <strong>real</strong> and <strong>fake</strong> samples.</p></li>
</ul>
<p>Now let’s define the parts of the <strong>Generator</strong>,</p>
<ul class="simple">
<li><p><strong>Latent Node</strong> - <span class="math notranslate nohighlight">\(L_1\)</span>, a single random value with uniform distribution, <span class="math notranslate nohighlight">\(U[0.4,1.0]\)</span>. Note we set the minimum as 0.4 to stay away from 0.0 or negative values as these would remove the slope or flip the slope of the fakes.</p></li>
<li><p><strong>Generator Weights</strong> - <span class="math notranslate nohighlight">\(\lambda_{1,2}\)</span>, <span class="math notranslate nohighlight">\(\lambda_{1,3}\)</span> and <span class="math notranslate nohighlight">\(\lambda_{1,4}\)</span> for the connections from latent to each of the output nodes. This is the simplest possible tranpose convolution, with a kernel size is 3, output nodes is 3 and latent node is 1, so the kernel does not translate. I did this to greatly simplify the book keeping, but the concepts could be extended to a more realistic convolution / tranpose convolution architectures for more realistic images sizes problem.</p></li>
<li><p><strong>Generator Bias</strong> - <span class="math notranslate nohighlight">\(b\)</span>, a single, constant bias over the output layer (output image), the nodes, <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, and <span class="math notranslate nohighlight">\(O_4\)</span></p></li>
<li><p><strong>Generator Output Nodes</strong> - <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, and <span class="math notranslate nohighlight">\(O_4\)</span>, the single and last feature map in our very simple generator; therefore, the output a 1D image with 3 nodes or pixels that are passed to the <strong>Discriminator</strong> input nodes, <span class="math notranslate nohighlight">\(I_5\)</span>, <span class="math notranslate nohighlight">\(I_6\)</span>, and <span class="math notranslate nohighlight">\(I_7\)</span></p></li>
<li><p><strong>Discriminator Input Nodes</strong> - <span class="math notranslate nohighlight">\(I_5\)</span>, <span class="math notranslate nohighlight">\(I_6\)</span>, and <span class="math notranslate nohighlight">\(I_7\)</span>, that receive the real images or the fake images from the generator output nodes, <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, <span class="math notranslate nohighlight">\(O_4\)</span></p></li>
<li><p><strong>Discriminator Weights</strong> - <span class="math notranslate nohighlight">\(\lambda_{5,8}\)</span>, <span class="math notranslate nohighlight">\(\lambda_{6,8}\)</span>, and <span class="math notranslate nohighlight">\(\lambda_{7,8}\)</span> for the connections from input nodes (input image) to the output (descision) node, <span class="math notranslate nohighlight">\(D_8\)</span></p></li>
<li><p><strong>Discriminator Bias</strong> - <span class="math notranslate nohighlight">\(c\)</span>, bias applied at the output (descision) node, <span class="math notranslate nohighlight">\(D_8\)</span></p></li>
</ul>
<p>Now let’s visualize this very simple generative adversarial network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'darksalmon'</span><span class="p">,</span><span class="s1">'deepskyblue'</span><span class="p">,</span><span class="s1">'gold'</span><span class="p">]</span>                  <span class="c1"># line colors for latent to fake to probability flow</span>
<span class="n">colors_real</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'orangered'</span><span class="p">,</span><span class="s1">'dodgerblue'</span><span class="p">,</span><span class="s1">'goldenrod'</span><span class="p">]</span>          <span class="c1"># line colors for real image to probability flow</span>

<span class="k">def</span> <span class="nf">draw_gan_architecture_full</span><span class="p">():</span>                             <span class="c1"># function to draw the GAN demonstrated in this workflow</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$L_1$"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="c1"># generator latent node</span>
            <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s2">"white"</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s2">"black"</span><span class="p">))</span>

    <span class="n">gen_outputs</span> <span class="o">=</span> <span class="p">[</span>                                           <span class="c1"># generator output nodes locations and labels</span>
        <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$O_2$"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$\lambda_{1,2}$"</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$O_3$"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$\lambda_{1,3}$"</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$O_4$"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$\lambda_{1,4}$"</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gen_outputs</span><span class="p">):</span>   <span class="c1"># generator output node labels</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
                <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s2">"white"</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s2">"black"</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>   <span class="c1"># generator output node connections</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-&gt;"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">((</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.11</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'bottom'</span><span class="p">)</span>

    <span class="n">disc_inputs</span> <span class="o">=</span> <span class="p">[</span>                                           <span class="c1"># discriminator input nodes locations and labels</span>
        <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$I_5$"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$\lambda_{5,8}$"</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$I_6$"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$\lambda_{6,8}$"</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$I_7$"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$\lambda_{7,8}$"</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">disc_inputs</span><span class="p">):</span>        <span class="c1"># discriminator input node labels</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
                <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s2">"white"</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s2">"black"</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">gen_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span> <span class="c1"># discriminator input node connections</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-&gt;"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$D_8$"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="c1"># discriminator decision node label</span>
            <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s2">"white"</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s2">"black"</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">disc_inputs</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span><span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>  <span class="c1"># discriminator output node connections</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-&gt;"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">((</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.7</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'bottom'</span><span class="p">)</span>

    <span class="n">real_inputs</span> <span class="o">=</span> <span class="p">[</span>                                           <span class="c1"># real data path below generator</span>
        <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$I_5^</span><span class="si">{real}</span><span class="s2">$"</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$I_6^</span><span class="si">{real}</span><span class="s2">$"</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$I_7^</span><span class="si">{real}</span><span class="s2">$"</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">real_inputs</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="c1"># label real data node labels</span>
                <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s2">"white"</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s2">"black"</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span>                                       <span class="c1"># arrow to discriminator inputs</span>
                    <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">disc_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">disc_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span> 
                    <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-&gt;"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors_real</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">"--"</span><span class="p">))</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.57</span><span class="p">,</span> <span class="s2">"Latent"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>    <span class="c1"># GAN part labels</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.57</span><span class="p">,</span> <span class="s2">"Classification"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.26</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.18</span><span class="p">,</span> <span class="s2">"Real Images"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.26</span><span class="p">,</span> <span class="mf">0.38</span><span class="p">,</span> <span class="s2">"Fake Images"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">)</span>

    <span class="n">fancybox</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.07</span><span class="p">,</span> <span class="mf">0.24</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Generator"</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">"grey"</span><span class="p">)</span> <span class="c1"># fancy boxes around generator and discriminator</span>
    <span class="n">fancybox</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.48</span><span class="p">,</span> <span class="mf">0.24</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.29</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Discriminator"</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">"grey"</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.68</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="s1">'c'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>           <span class="c1"># draw the biases in O2, O3, O4 and D8</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.695</span><span class="p">,</span> <span class="mf">0.47</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.685</span><span class="p">,</span> <span class="mf">0.42</span><span class="p">),</span><span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.27</span><span class="p">,</span><span class="mf">0.63</span><span class="p">,</span><span class="s1">'b'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.28</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.29</span><span class="p">,</span> <span class="mf">0.68</span><span class="p">),</span><span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.27</span><span class="p">,</span><span class="mf">0.43</span><span class="p">,</span><span class="s1">'b'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.28</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.29</span><span class="p">,</span> <span class="mf">0.48</span><span class="p">),</span><span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.27</span><span class="p">,</span><span class="mf">0.23</span><span class="p">,</span><span class="s1">'b'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.28</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.29</span><span class="p">,</span> <span class="mf">0.28</span><span class="p">),</span><span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">draw_gan_architecture_full</span><span class="p">()</span>                                  <span class="c1"># draw the GAN         </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/565edb7221aa6743a530bb98f80b562615b08280d5d3d4a52aee7ff15cefcaf2.png" src="../Images/cdcce92e4f0534109850166492b3b2ef.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/565edb7221aa6743a530bb98f80b562615b08280d5d3d4a52aee7ff15cefcaf2.png"/>
</div>
</div>
<p>Just a couple more comments about my network nomenclature. My goal is to maximize simplicity and clarity,</p>
</section>
<section id="comments-on-network-nomenclature">
<h2>Comments on Network Nomenclature</h2>
<ul class="simple">
<li><p><strong>Network Nodes and Connections</strong> - I choose to use unique numbers for all nodes, <span class="math notranslate nohighlight">\(L_1\)</span>, <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, <span class="math notranslate nohighlight">\(\ldots\)</span> instead of <span class="math notranslate nohighlight">\(L_1\)</span>, <span class="math notranslate nohighlight">\(O_1\)</span>, <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(\ldots\)</span> to simplify the notation for the weights; therefore, when I say <span class="math notranslate nohighlight">\(\lambda_{5,8}\)</span> you know exactly where this weight is applied in the network.</p></li>
<li><p><strong>Node Outputs</strong> - I use the node label to also describe the output from the node, for example <span class="math notranslate nohighlight">\(O_2\)</span> is a node in the generator’s output layer and also the signal or value output from that node.</p></li>
<li><p><strong>Pre- and Post-activation</strong> - at our nodes <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, <span class="math notranslate nohighlight">\(O_4\)</span>, and <span class="math notranslate nohighlight">\(D_8\)</span> we have the node input before activation and the node output after activation, I use the notation <span class="math notranslate nohighlight">\(O_{2_{in}}\)</span>, <span class="math notranslate nohighlight">\(O_{3_{in}}\)</span>, <span class="math notranslate nohighlight">\(O_{4_{in}}\)</span> and <span class="math notranslate nohighlight">\(D_{8_{in}}\)</span> for the pre-activation input and <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, <span class="math notranslate nohighlight">\(O_4\)</span>, and <span class="math notranslate nohighlight">\(D_8\)</span> for the post-activation node output. This is important because with back propagation we have to step through the nodes, going from post-activation to pre-activation.</p></li>
<li><p><strong>Latent</strong> - while publications often use <span class="math notranslate nohighlight">\(z\)</span> notation for the latent values, to be consistent with my notion above, I use <span class="math notranslate nohighlight">\(L_1\)</span> for the latent value, i.e., the output from my latent node, <span class="math notranslate nohighlight">\(L_1\)</span>.</p></li>
</ul>
<p>Now let’s walk-through all the parts of our example GAN and show all the math.</p>
</section>
<section id="sigmoid-activation">
<h2>Sigmoid Activation</h2>
<p>For reference, let’s visualize the sigmoid activation function,</p>
<ul class="simple">
<li><p><strong>activation</strong> - the non-linear transformation, this is the sigmoid activation</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
x_{out} = \sigma(x_{in}) = \dfrac{1}{1 + e^{-x_{in}}}
\]</div>
<ul class="simple">
<li><p><strong>activation derrivative</strong> - essential for back propogation</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sigma'(x_{in}) = \sigma(x_{out})(1 - \sigma(x_{out}))
\]</div>
<p>note, for convenience the derrivative of the sigmoid activation function with respect to the input is posed for the output.</p>
<ul class="simple">
<li><p>as we back-propogate backwards over the activation function we can us the output to step back through the activated network node</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">amin</span> <span class="o">=</span> <span class="o">-</span><span class="mf">5.0</span><span class="p">;</span> <span class="n">amax</span> <span class="o">=</span> <span class="mf">5.0</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">amin</span><span class="p">,</span><span class="n">amax</span><span class="p">,</span><span class="mi">100</span><span class="p">);</span> <span class="n">output</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">amin</span><span class="p">,</span><span class="n">amax</span><span class="p">,</span><span class="mi">100</span><span class="p">)));</span> <span class="n">derriv</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">output</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'forestgreen'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Activation'</span><span class="p">);</span> <span class="n">add_grid2</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Input'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Output'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">derriv</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'deepskyblue'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Derrivative'</span><span class="p">);</span> <span class="n">add_grid2</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Input'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Output'</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Sigmoid Activation'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">amin</span><span class="p">,</span><span class="n">amax</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f7323a3bf2b5671f3b7f629a5e2dd8966903903e490d8b510d1b437b922d20ad.png" src="../Images/6aef20df3b024801181a361544b98363.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/f7323a3bf2b5671f3b7f629a5e2dd8966903903e490d8b510d1b437b922d20ad.png"/>
</div>
</div>
<p>Let’s make some observations about the sigmoid activation and its derrivative,</p>
<ul class="simple">
<li><p><strong>sigmoid outputs</strong> - are bounded (0,1) approaching both limits asymptotically</p></li>
<li><p><strong>vanishing gradients</strong> - as the</p></li>
</ul>
</section>
<section id="generator-forward-pass">
<h2>Generator Forward Pass</h2>
<p>First, let’s walk through the generator to go from a latent value to a fake image. The generator takes a latent input,</p>
<div class="math notranslate nohighlight">
\[
L_1 \sim \mathcal{U}(0.4, 1)
\]</div>
<ul class="simple">
<li><p>recall, our simple generator has only one layer <span class="math notranslate nohighlight">\(L1\)</span>, with only 3 outputs, <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, and <span class="math notranslate nohighlight">\(O_4\)</span>, representing the fake image.</p></li>
</ul>
<p>Then latent value, <span class="math notranslate nohighlight">\(L_1\)</span>, is passed through the transpose convolution kernel to the output,</p>
<ul class="simple">
<li><p>our transpose convolution kernel has a size of 3, the same size as our output, so we don’t see it translate it, resulting in greatly simplified book keeping!</p></li>
<li><p>the tranpose convolution kernel weights are <span class="math notranslate nohighlight">\(\lambda_{1,2}\)</span>, <span class="math notranslate nohighlight">\(\lambda_{1,3}\)</span>, and <span class="math notranslate nohighlight">\(\lambda_{1,4}\)</span></p></li>
</ul>
<p>We apply the sigmoid activation in each of the output nodes</p>
<p>Each output is computed by applying a linear transformation followed by a <strong>sigmoid</strong> activation, <span class="math notranslate nohighlight">\(\sigma\)</span>,</p>
<div class="math notranslate nohighlight">
\[
O_2 = \sigma(\lambda_{1,2} \cdot z + b)
\]</div>
<div class="math notranslate nohighlight">
\[
O_3 = \sigma(\lambda_{1,3} \cdot z + b)
\]</div>
<div class="math notranslate nohighlight">
\[
O_4 = \sigma(\lambda_{1,4} \cdot z + b)
\]</div>
<p>where,</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(\lambda_{1,j}\)</span></strong> - are the transpose convolution kernel weights</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(b\)</span></strong> - is the shared bias, single bias term for the output layer</p></li>
</ul>
<p>We can also write the generator forward pass in matrix notation as,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
O_2 \\
O_3 \\
O_4
\end{bmatrix}
=
\sigma\left(
\begin{bmatrix}
\lambda_{1,2} \\
\lambda_{1,3} \\
\lambda_{1,4}
\end{bmatrix}
z + 
\begin{bmatrix}
b \\
b \\
b
\end{bmatrix}
\right)
\end{split}\]</div>
<p>where the sigmoid activation is applied element-wise.</p>
</section>
<section id="discriminator-forward-pass">
<h2>Discriminator Forward Pass</h2>
<p>Now let’s walk-through the discriminator, going from an image, real or fake, to a probability of real. The discriminator receives the image, over 3 input nodes, <span class="math notranslate nohighlight">\(I_5\)</span>, <span class="math notranslate nohighlight">\(I_6\)</span>, and <span class="math notranslate nohighlight">\(I_7\)</span>. In the case of a fake image,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
I_5 \\
I_6 \\
I_7
\end{bmatrix}
=
\begin{bmatrix}
O_2 \\
O_3 \\
O_4
\end{bmatrix}
\end{split}\]</div>
<p>and in the case of a real image,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
I_5 \\
I_6 \\
I_7
\end{bmatrix}
=
\begin{bmatrix}
I_5^{real} \\
I_6^{real} \\
I_7^{real}
\end{bmatrix}
\end{split}\]</div>
<p>Since we have only 1 layer and the convolution kernel is 3 with an input of 3 once again there is no translation!</p>
<ul class="simple">
<li><p>we just take input image, <span class="math notranslate nohighlight">\(I_5\)</span>, <span class="math notranslate nohighlight">\(I_6\)</span>, and <span class="math notranslate nohighlight">\(I_7\)</span>, and apply the convolutional kernel weights, <span class="math notranslate nohighlight">\(\lambda_{5,8}\)</span>, <span class="math notranslate nohighlight">\(\lambda_{6,8}\)</span>, and <span class="math notranslate nohighlight">\(\lambda_{7,8}\)</span>, and add the bias term, <span class="math notranslate nohighlight">\(c\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
D_8 = \sigma\left(
\lambda_{5,8} \cdot I_5 +
\lambda_{6,8} \cdot I_6 +
\lambda_{7,8} \cdot I_7 + c
\right)
\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda_{i,8}\)</span> are the convolutional kernel weights to go from input image to next feature map, only 1 value, our output probability</p></li>
<li><p><span class="math notranslate nohighlight">\(c\)</span> is the bias term</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma(x) = \dfrac{1}{1 + e^{-x}}\)</span> is the sigmoid activation function</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(D_8 \in [0, 1]\)</span> represents the probability assigned by the discriminator that the input is <strong>real</strong> (i.e. not a fake from the generator).</p>
<p>We can also write the discriminator forward pass in matrix notation as,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
D_8 = \sigma\left(
\begin{bmatrix}
\lambda_{5,8} &amp; \lambda_{6,8} &amp; \lambda_{7,8}
\end{bmatrix}
\cdot
\begin{bmatrix}
I_5 \\
I_6 \\
I_7
\end{bmatrix}
+ c
\right)
\end{split}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda_{5,8}, \lambda_{6,8}, \lambda_{7,8}\)</span> are scalar weights</p></li>
<li><p><span class="math notranslate nohighlight">\(I_5, I_6, I_7\)</span> are the input values (i.e., outputs of the generator, <span class="math notranslate nohighlight">\(O_2, O_3, O_4\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(c\)</span> is the bias term</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma(x) = \dfrac{1}{1 + e^{-x}}\)</span> is the sigmoid function</p></li>
</ul>
</section>
<section id="discriminator-loss">
<h2>Discriminator Loss</h2>
<p>Binary cross-entropy is a loss function used for binary classification tasks where the output is a probability between 0 and 1, and the target label is either 0 or 1.</p>
<ul class="simple">
<li><p><strong>Prediction</strong> (model output) - <span class="math notranslate nohighlight">\(\hat{y} \in (0, 1)\)</span>, the output of <span class="math notranslate nohighlight">\(D_8\)</span>, the discriminator’s classification, probability that the image is real</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y} = D_8
\]</div>
<ul class="simple">
<li><p><strong>True label</strong> (ground truth) - <span class="math notranslate nohighlight">\(y \in \{0, 1\}\)</span>, 0 if the image is from the generator, fake, and 1 if the image is from the real training data</p></li>
</ul>
<p>Now we can define the <strong>binary cross-entropy loss</strong> as,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{BCE}}(y, \hat{y}) = - \left[ y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y}) \right]
\]</div>
<p>now we can further specify,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\log(\hat{y})\)</span> is the log-likelihood of the positive prediction</p></li>
<li><p><span class="math notranslate nohighlight">\(log(1 - \hat{y})\)</span> is the log-likelihood of the negative prediction</p></li>
</ul>
<p>how does binary cross-entropy behave?</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(y = 1\)</span> (real image), then:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = -\log(\hat{y}) \quad \text{(we want } \hat{y} \to 1)
\]</div>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(y = 0\)</span> (fake image from the generator), then:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = -\log(1 - \hat{y}) \quad \text{(we want } \hat{y} \to 0)
\]</div>
<p>We can summarize as,</p>
<ul class="simple">
<li><p>the loss is <strong>low</strong> when the model’s prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span> is <strong>close to the true label</strong>, low probability of real for a fake image and high probability of real for a real image</p></li>
<li><p>the loss becomes <strong>very large</strong> if the model is <strong>confident and wrong</strong>, due to the logarithm, i.e., very low probability or real for a real image and very high probability of real for a fake image</p></li>
<li><p>the sigmoid activation ensures that the output, <span class="math notranslate nohighlight">\(\hat{y}\)</span> is a valid probability</p></li>
</ul>
</section>
<section id="discriminator-loss-derivative">
<h2>Discriminator Loss Derivative</h2>
<p>To perform backpropagation we need to calculate the loss derivative. Let’s do this for the input of the activation function as our output node, <span class="math notranslate nohighlight">\(D_8\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dz}
\]</div>
<ul class="simple">
<li><p>define <span class="math notranslate nohighlight">\(z\)</span> as the input for the sigmoid activation as output node, <span class="math notranslate nohighlight">\(D_8\)</span>.</p></li>
<li><p>as you see we do this because it results in a very simple, efficient result.</p></li>
<li><p>recall, the sigmoid function,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}
\]</div>
<p>We will use the chain rule, so we only need to solve the parts,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dz} = \frac{d\mathcal{L}}{d\hat{y}} \cdot \frac{d\hat{y}}{dz}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\frac{d\mathcal{L}}{d\hat{y}}\)</span> - partial derivative of binary cross-entropy loss given the discriminator output <span class="math notranslate nohighlight">\(\hat{y}\)</span> (<span class="math notranslate nohighlight">\(D_8\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{d\hat{y}}{dz}\)</span> - partial derivative of the discriminator output <span class="math notranslate nohighlight">\(\hat{y}\)</span> (<span class="math notranslate nohighlight">\(D_8\)</span>) given the sigmoid activation input</p></li>
</ul>
<p>Now we can solve the first part, partial derivative of loss with respect to the discriminator output, <span class="math notranslate nohighlight">\(\hat{y}\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{d\hat{y}} = -\left( \frac{y}{\hat{y}} - \frac{1 - y}{1 - \hat{y}} \right)
\]</div>
<p>now we can solve the second part, the partial derivative of the discriminator output <span class="math notranslate nohighlight">\(\hat{y}\)</span> (<span class="math notranslate nohighlight">\(D_8\)</span>) given the sigmoid activation input, it is just the sigmoid derivative,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{dz} = \hat{y}(1 - \hat{y})
\]</div>
<p>and we can combine these by the chain rule as,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dz} = -\left( \frac{y}{\hat{y}} - \frac{1 - y}{1 - \hat{y}} \right) \cdot \hat{y}(1 - \hat{y})
\]</div>
<p>We are almost there, we only need to simplify the result, first we distribute, <span class="math notranslate nohighlight">\(\hat{y}(1 - \hat{y})\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dz} = -\left[ y(1 - \hat{y}) - (1 - y)\hat{y} \right]
\]</div>
<p>and then simplify it further,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dz} = -\left[ y - y\hat{y} - \hat{y} + y\hat{y} \right]
= -\left[ y - \hat{y} \right]
= \hat{y} - y
\]</div>
<p>I said this would get simple! Our partial derivative of our loss with respect to the input to the output node sigmoid activation function, <span class="math notranslate nohighlight">\(z\)</span>, is,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dz} = \hat{y} - y
\]</div>
<p>This result shows the gradient is just the <strong>error</strong> — the difference between predicted and true values.</p>
<p>Now we can make this simple interpretation,</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(\hat{y} &gt; y\)</span>, the model overestimates <span class="math notranslate nohighlight">\(\rightarrow\)</span> gradient is positive <span class="math notranslate nohighlight">\(\rightarrow\)</span> lower prediction by moving in the negative gradient</p></li>
<li><p>if <span class="math notranslate nohighlight">\(\hat{y} &lt; y\)</span>, the model underestimates <span class="math notranslate nohighlight">\(\rightarrow\)</span> gradient is negative <span class="math notranslate nohighlight">\(\rightarrow\)</span> increase prediction by moving in the negative gradient</p></li>
</ul>
<p>I know that a title this section “Discriminator Loss Derivative”, but excuse me for performing just a little bit of backpropagation (to before sigmoid activation).</p>
<ul class="simple">
<li><p>next we carry on with back propagation to the discriminator weights and biases</p></li>
</ul>
</section>
<section id="discriminator-back-propagation">
<h2>Discriminator Back Propagation</h2>
<p>For compact notation, let’s use matrix notation and define the input to the <span class="math notranslate nohighlight">\(D_8\)</span> activation, <span class="math notranslate nohighlight">\(z\)</span>, as,</p>
<div class="math notranslate nohighlight">
\[
z = \mathbf{w}^\top \mathbf{x} + c \quad \Rightarrow \quad \frac{dz}{d\mathbf{w}} = \mathbf{x}
\]</div>
<p>Now we can extend our use of the chain rule to,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{d\mathbf{w}} = \frac{d\mathcal{L}}{dz} \cdot \frac{dz}{d\mathbf{w}} = (\hat{y} - y) \cdot \mathbf{x}
\]</div>
<p>So for each of our discriminator weights, <span class="math notranslate nohighlight">\(\lambda_{5,8}\)</span>, <span class="math notranslate nohighlight">\(\lambda_{6,8}\)</span>, and <span class="math notranslate nohighlight">\(\lambda_{7,8}\)</span> we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{d\lambda_{5,8}} = (\hat{y} - y) \cdot I_5
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{d\lambda_{6,8}} = (\hat{y} - y) \cdot I_6
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{d\lambda_{7,8}} = (\hat{y} - y) \cdot I_7
\]</div>
<p>and for the bias, <span class="math notranslate nohighlight">\(c\)</span>, we calculate the next component for the chain rule as,</p>
<div class="math notranslate nohighlight">
\[
\frac{dz}{dc} = 1
\]</div>
<p>so we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dc} = \frac{d\mathcal{L}}{dz} \cdot \frac{dz}{dc} = (\hat{y} - y) \cdot 1 = \hat{y} - y
\]</div>
<p>The backpropagation for our very simple discriminator is quite simple, we can summarize for the weights,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{d\mathbf{w}} = (\hat{y} - y) \cdot \mathbf{x}
\]</div>
<p>and for the bias,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dc} = \hat{y} - y
\]</div>
<p>Let’s write these out for all of our discriminators parameters,
$<span class="math notranslate nohighlight">\(
\begin{aligned}
\frac{d\mathcal{L}}{d\lambda_{5,8}} &amp;= (\hat{y} - y) \cdot I_5 \\
\frac{d\mathcal{L}}{d\lambda_{6,8}} &amp;= (\hat{y} - y) \cdot I_6 \\
\frac{d\mathcal{L}}{d\lambda_{7,8}} &amp;= (\hat{y} - y) \cdot I_7 \\
\frac{d\mathcal{L}}{dc} &amp;= \hat{y} - y
\end{aligned}
\)</span>$</p>
</section>
<section id="generator-loss-derivative-and-back-propagation-through-the-discriminator">
<h2>Generator Loss Derivative and Back Propagation Through the Discriminator</h2>
<p>Recall that the goal of the generator is to make fake images the discriminator assigns as a high probability of a real image, i.e., to fool the discriminator</p>
<ul class="simple">
<li><p>the generator produces a fake image,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\tilde{\mathbf{x}} = G(z) 
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> where ( z ) is a latent vector (e.g., sampled from Uniform[0.4, 1]).</p>
<ul class="simple">
<li><p>the discriminator evaluates this fake sample and returns:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y} = D(\tilde{\mathbf{x}}) \in (0, 1)
\]</div>
<p>Now we can calculate the binary cross-entropy for the generator as,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_G = -\log(\hat{y})
\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{y} = D(G(z))\)</span>, the discriminator’s evaluation of the generator’s fake image, <span class="math notranslate nohighlight">\(z\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}\)</span> is the probability assigned by the discriminator to the fake being real</p></li>
</ul>
<p>This is equivalent to cross-entropy with <strong>target label <span class="math notranslate nohighlight">\(y = 1\)</span></strong>, note here we do a trick, from the generator’s perspective it’s images are real, so we are using <span class="math notranslate nohighlight">\(y=1\)</span>, i.e., real images for the fake images!</p>
<p>You may get confused if you look at the original GAN loss above, this is called the non-saturating generator loss.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Loss Type</p></th>
<th class="head"><p>Expression</p></th>
<th class="head"><p>Comment</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Original GAN</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{E}[\log(1 - D(G(z)))]\)</span></p></td>
<td><p>Theoretical, can cause vanishing gradients</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Non-saturating</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(-\mathbb{E}[\log(D(G(z)))]\)</span></p></td>
<td><p>Practical, stronger gradients, commonly used</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>so instead of minimizing the original generator loss we are maximizing the non-saturating generator loss.</p></li>
</ul>
<p>Let’s show how to back propagate through the entire discriminator with the chain rule.</p>
<ul class="simple">
<li><p>we want the generator loss gradient with respect generator output,</p></li>
</ul>
<p>Given <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}} = G(z)\)</span>, our fake image, we want the partial derivative of the loss given our fake image,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\tilde{\mathbf{x}}}
\]</div>
<p>and by the chain rule,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\tilde{\mathbf{x}}} = \frac{d\mathcal{L}_G}{d\hat{y}} \cdot \frac{d\hat{y}}{d\tilde{\mathbf{x}}}
\]</div>
<p>This is how the discriminator’s belief <span class="math notranslate nohighlight">\(\hat{y}\)</span>​ about “fakeness” changes with changes in <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}\)</span> the fake image.</p>
<p>Now we are ready to back propagate the generator loss through the discriminator, let’s start with our generator loss (from above),</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_G = -\log(\hat{y})
\]</div>
<p>and when we perform the partial derivative,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\hat{y}} = -\frac{1}{\hat{y}}
\]</div>
<p>Now, recall the discriminator’s forward pass is,</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = \sigma(\mathbf{w}^\top \tilde{\mathbf{x}} + c)
\]</div>
<p>so we can calculate the partial derivative of the discriminator’s output with respect to the generator’s fake image as,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{d\tilde{\mathbf{x}}} = \hat{y}(1 - \hat{y}) \cdot \mathbf{w}
\]</div>
<p>now we combine these with the chain rule as,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\tilde{\mathbf{x}}}
= \left( -\frac{1}{\hat{y}} \right) \cdot \left( \hat{y}(1 - \hat{y}) \cdot \mathbf{w} \right)
= -(1 - \hat{y}) \cdot \mathbf{w}
\]</div>
<p>The gradient of the generator’s loss with respect to the output image <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}\)</span> is,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\tilde{\mathbf{x}}} = -(1 - \hat{y}) \cdot \mathbf{w}
\]</div>
<p>We can add some interpretations of this result,</p>
<ul class="simple">
<li><p>when <span class="math notranslate nohighlight">\(\hat{y}\)</span> is close to 0 <span class="math notranslate nohighlight">\(\rightarrow\)</span> discriminator easily spots fake <span class="math notranslate nohighlight">\(\rightarrow\)</span> large gradient <span class="math notranslate nohighlight">\(\rightarrow\)</span> generator updates more.</p></li>
<li><p>when <span class="math notranslate nohighlight">\(\hat{y}\)</span> is close to 1 <span class="math notranslate nohighlight">\(\rightarrow\)</span> generator is fooling the discriminator <span class="math notranslate nohighlight">\(\rightarrow\)</span> gradient is small.</p></li>
</ul>
<p>This guides the generator to tweak its output to increase <span class="math notranslate nohighlight">\(\hat{y}\)</span> — i.e., fool the discriminator.</p>
<p>To further clarify, for our example let’s compute how the discriminator’s output <span class="math notranslate nohighlight">\(\hat{y}\)</span> changes with respect to the generator outputs <span class="math notranslate nohighlight">\(O_5, O_6, O_7\)</span>, instead of the <span class="math notranslate nohighlight">\(w\)</span> vector notation used above.</p>
<ul class="simple">
<li><p>if we apply the chain rule we get,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{dO_i} = \frac{d\hat{y}}{dz} \cdot \frac{dz}{dO_i}
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> for each of the components we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{dz} = \hat{y}(1 - \hat{y})
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{dz}{dO_5} = \lambda_{5,8}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{dz}{dO_6} = \lambda_{6,8}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{dz}{dO_7} = \lambda_{7,8}
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> substituting in the chain rule we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{dO_5} = \hat{y}(1 - \hat{y}) \cdot \lambda_{5,8}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{dO_6} = \hat{y}(1 - \hat{y}) \cdot \lambda_{6,8}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{dO_7} = \hat{y}(1 - \hat{y}) \cdot \lambda_{7,8}
\]</div>
</section>
<section id="backpropagation-through-generator-to-weights-and-bias">
<h2>Backpropagation Through Generator to Weights and Bias</h2>
<p>We now propagate through the generators sigmoid activation in each of the output nodes,</p>
<div class="math notranslate nohighlight">
\[
\frac{dO_i}{dz_i} = O_i (1 - O_i)
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(O_i = \sigma(z_i)\)</span>, where <span class="math notranslate nohighlight">\(z_i\)</span> is the input for the output nodes, pre-activation, and <span class="math notranslate nohighlight">\(O_i\)</span> is output for the output nodes, post-activation</p></li>
</ul>
<p>We Apply chain rule,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{dz_i} = \frac{d\mathcal{L}_G}{dO_i} \cdot \frac{dO_i}{dz_i}
= \frac{d\mathcal{L}_G}{dO_i} \cdot O_i (1 - O_i)
\]</div>
<p>Recall,</p>
<div class="math notranslate nohighlight">
\[
z_i = \lambda_{1,i} \cdot L_1 + b
\]</div>
<p>so we can calculate the generator’s weights partial derivatives as,</p>
<div class="math notranslate nohighlight">
\[
\frac{dz_i}{d\lambda_{1,i}} = L_1
\]</div>
<p>and the generator’s bias partial derivative as,</p>
<div class="math notranslate nohighlight">
\[
\frac{dz_i}{db} = 1
\]</div>
<p>Now we can put this all together with the chain rule, the partial derivatives of the generator loss with respect to the generator weights are,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\lambda_{1,i}} =
\frac{d\mathcal{L}_G}{dz_i} \cdot \frac{dz_i}{d\lambda_{1,i}} =
\left( \frac{d\mathcal{L}_G}{dO_i} \cdot O_i (1 - O_i) \right) \cdot L_1
\]</div>
<p>and the partial derivative of the generator loss with respect to the generator bias is,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{db} =
\sum_{i=5}^7 \frac{d\mathcal{L}_G}{dz_i} \cdot \frac{dz_i}{db} =
\sum_{i=5}^7 \left( \frac{d\mathcal{L}_G}{dO_i} \cdot O_i (1 - O_i) \right)
\]</div>
<p>For clarity, let’s write this out for each of our generator’s weights,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\lambda_{1,2}} =
-(1 - \hat{y}) \cdot \lambda_{5,8} \cdot O_5 (1 - O_5) \cdot L_1
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\lambda_{1,3}} =
-(1 - \hat{y}) \cdot \lambda_{6,8} \cdot O_6 (1 - O_6) \cdot L_1
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\lambda_{1,4}} =
-(1 - \hat{y}) \cdot \lambda_{7,8} \cdot O_7 (1 - O_7) \cdot L_1
\]</div>
<p>and for our generator’s bias,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{db} =
-(1 - \hat{y}) \cdot \left[
\lambda_{5,8} \cdot O_5(1 - O_5)
+ \lambda_{6,8} \cdot O_6(1 - O_6)
+ \lambda_{7,8} \cdot O_7(1 - O_7)
\right]
\]</div>
<p>Let’s make some interpretations,</p>
<ul class="simple">
<li><p>the generator’s weights and bias gradients scale with how much the discriminator is fooled (<span class="math notranslate nohighlight">\(1 - \hat{y}\)</span>)</p></li>
<li><p>the generator learns to tweak <span class="math notranslate nohighlight">\(\lambda_{1,i}\)</span> and <span class="math notranslate nohighlight">\(b\)</span> to push the fake images, <span class="math notranslate nohighlight">\(O_5\)</span>, <span class="math notranslate nohighlight">\(O_6\)</span> and <span class="math notranslate nohighlight">\(O_7\)</span> in directions that increase <span class="math notranslate nohighlight">\(\hat{y}\)</span></p></li>
<li><p>this flow of error gives the generator a signal to <strong>fool the discriminator more effectively</strong> without ever seeing a real image!</p></li>
</ul>
</section>
<section id="simple-gan-training-workflow">
<h2>Simple GAN Training Workflow</h2>
<p>We start with initialization of the generator and discriminator weights and bias and setting the training hyperparameters.</p>
<ol class="arabic simple">
<li><p>Generate the Synthethic, “Real Images” for training</p></li>
</ol>
<ul class="simple">
<li><p>sample <span class="math notranslate nohighlight">\(N\)</span> real 3-node, 1D images <span class="math notranslate nohighlight">\(\mathbf{I} = \{(I_{5,i}, I_{6,i}, I_{7,i})\}_{i=1}^N\)</span></p></li>
<li><p>use the synthetic training data function:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Real images} \sim \text{linear decreasing trend} + \text{noise}
\]</div>
<ol class="arabic simple" start="2">
<li><p><strong>Initialize generator weights and bias</strong> - the weights,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\{\lambda_{1,2}, \lambda_{1,3}, \lambda_{1,4}, b\} \leftarrow \text{small random values}
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> and the bias,</p>
<div class="math notranslate nohighlight">
\[
b \leftarrow 0.0
\]</div>
<ol class="arabic simple" start="3">
<li><p><strong>Initialize discriminator weights and bias</strong> - the weights,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\{\lambda_{5,8}, \lambda_{6,8}, \lambda_{7,8}, c\} \leftarrow \text{small random values}
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> and the bias,</p>
<div class="math notranslate nohighlight">
\[
c \leftarrow 0.0
\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Set model training hyperparameters</strong> - this includes,</p></li>
</ol>
<ul class="simple">
<li><p>Learning Rates - for the generator, <span class="math notranslate nohighlight">\(\eta_G\)</span>, and discriminator, <span class="math notranslate nohighlight">\(\eta_D\)</span></p></li>
<li><p>Batch Size - in this example we are assuming batch size equal to the number of real images</p></li>
<li><p>Epochs - number of training iterations</p></li>
</ul>
<ol class="arabic simple" start="5">
<li><p><strong>Train the discriminator</strong></p></li>
</ol>
<ul class="simple">
<li><p>combine real and fake inputs into a batch of size <span class="math notranslate nohighlight">\(2N\)</span> and inlcude labels <span class="math notranslate nohighlight">\(y_i = 1\)</span> for real, <span class="math notranslate nohighlight">\(y_i = 0\)</span> for fake</p></li>
<li><p>compute discriminator outputs <span class="math notranslate nohighlight">\(\hat{y}_i = D(I_{5,i}, I_{6,i}, I_{7,i})\)</span></p></li>
<li><p>calculate discriminator loss and gradients using:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \lambda_{j,8}}, \quad \frac{\partial \mathcal{L}}{\partial c}
\]</div>
<ul class="simple">
<li><p>update discriminator weights and bias:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lambda_{j,8} \leftarrow \lambda_{j,8} - \eta_D \times \frac{\partial \mathcal{L}}{\partial \lambda_{j,8}}, \quad
  c \leftarrow c - \eta_D \times \frac{\partial \mathcal{L}}{\partial c}
\]</div>
<ol class="arabic simple" start="5">
<li><p><strong>Train the generator</strong></p></li>
</ol>
<ul class="simple">
<li><p>Compute generator output fake images and pass to the discriminator to evaluate the outputs on these fakes, <span class="math notranslate nohighlight">\(D_8\)</span> same as <span class="math notranslate nohighlight">\(y\)</span></p></li>
<li><p>Calculate generator loss gradients using,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}_G}{\partial \lambda_{1,j}}, \quad \frac{\partial \mathcal{L}_G}{\partial b}
\]</div>
<ul class="simple">
<li><p>Update generator weights and bias,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lambda_{1,j} \leftarrow \lambda_{1,j} - \eta_G \times \frac{\partial \mathcal{L}_G}{\partial \lambda_{1,j}}, \quad
  b \leftarrow b - \eta_G \times \frac{\partial \mathcal{L}_G}{\partial b}
\]</div>
<ol class="arabic simple" start="6">
<li><p><strong>Repeat Until Convergence</strong> - or stop criteria is met, such as maximum number of training epochs, return to step 5.</p></li>
</ol>
<p>Here a summary of the training loop,</p>
<ol class="arabic simple">
<li><p>Generate real data batch</p></li>
<li><p>Generate fake data batch</p></li>
<li><p>Update discriminator to better distinguish real/fake</p></li>
<li><p>Update generator to fool discriminator</p></li>
<li><p>Repeat</p></li>
</ol>
<p>This adversarial training loop lets the generator learn to create data mimicking the real distribution, and the discriminator improve in spotting fakes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">train_gan</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">lr_g</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">lr_d</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span> <span class="c1"># function for training the GAN</span>
    <span class="n">weights_epoch_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">weights_g_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Initialize weights</span>
    <span class="n">weights_g</span> <span class="o">=</span> <span class="n">initialize_generator_weights</span><span class="p">()</span>
    <span class="n">weights_d</span> <span class="o">=</span> <span class="n">initialize_discriminator_weights</span><span class="p">()</span>
    
    <span class="c1"># Tracking losses</span>
    <span class="n">generator_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">discriminator_losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">real_images</span> <span class="o">=</span> <span class="n">generate_real_data</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="c1"># one set of images</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># Step 1: Generate real data</span>

        <span class="n">I5_real</span><span class="p">,</span> <span class="n">I6_real</span><span class="p">,</span> <span class="n">I7_real</span> <span class="o">=</span> <span class="n">real_images</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">real_images</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">real_images</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">y_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># Step 2: Generate fake data</span>
        <span class="n">L1_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">fake_images</span> <span class="o">=</span> <span class="n">generator_forward</span><span class="p">(</span><span class="n">L1_fake</span><span class="p">,</span> <span class="n">weights_g</span><span class="p">,</span><span class="n">return_pre_activation</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">I5_fake</span><span class="p">,</span> <span class="n">I6_fake</span><span class="p">,</span> <span class="n">I7_fake</span> <span class="o">=</span> <span class="n">fake_images</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">fake_images</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">fake_images</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">y_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># Combine for discriminator training</span>
        <span class="n">I5_combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">I5_real</span><span class="p">,</span> <span class="n">I5_fake</span><span class="p">])</span>
        <span class="n">I6_combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">I6_real</span><span class="p">,</span> <span class="n">I6_fake</span><span class="p">])</span>
        <span class="n">I7_combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">I7_real</span><span class="p">,</span> <span class="n">I7_fake</span><span class="p">])</span>
        <span class="n">y_combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">y_real</span><span class="p">,</span> <span class="n">y_fake</span><span class="p">])</span>

        <span class="c1"># Step 3: Train discriminator</span>
        <span class="n">grads_d</span> <span class="o">=</span> <span class="n">discriminator_gradients</span><span class="p">(</span><span class="n">I5_combined</span><span class="p">,</span> <span class="n">I6_combined</span><span class="p">,</span> <span class="n">I7_combined</span><span class="p">,</span> <span class="n">y_combined</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">weights_d</span><span class="p">:</span>
            <span class="n">weights_d</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">lr_d</span> <span class="o">*</span> <span class="n">grads_d</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

        <span class="c1"># Step 4: Train generator</span>
        <span class="n">L1_gen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">grads_g</span> <span class="o">=</span> <span class="n">generator_gradients</span><span class="p">(</span><span class="n">L1_gen</span><span class="p">,</span> <span class="n">weights_g</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">weights_g</span><span class="p">:</span>
            <span class="n">weights_g</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">lr_g</span> <span class="o">*</span> <span class="n">grads_g</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="c1"># if epoch in [1000,2500,5000]: # save the weights to visualize model improvement over epochs</span>
            <span class="c1"># weights_g_list.append(weights_g)</span>
        
        <span class="c1"># Step 5: Calculate and store losses</span>
        <span class="n">y_pred_real</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">discriminator_forward</span><span class="p">(</span><span class="n">I5_real</span><span class="p">,</span> <span class="n">I6_real</span><span class="p">,</span> <span class="n">I7_real</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">)</span>
        <span class="n">y_pred_fake</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">discriminator_forward</span><span class="p">(</span><span class="n">I5_fake</span><span class="p">,</span> <span class="n">I6_fake</span><span class="p">,</span> <span class="n">I7_fake</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">)</span>
        <span class="n">loss_d_real</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred_real</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span>
        <span class="n">loss_d_fake</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred_fake</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span>
        <span class="n">loss_d</span> <span class="o">=</span> <span class="n">loss_d_real</span> <span class="o">+</span> <span class="n">loss_d_fake</span>
        <span class="n">discriminator_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_d</span><span class="p">)</span>

        <span class="n">y_pred_gen</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">discriminator_forward</span><span class="p">(</span><span class="o">*</span><span class="n">generator_forward</span><span class="p">(</span><span class="n">L1_gen</span><span class="p">,</span> <span class="n">weights_g</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">)</span>
        <span class="n">loss_g</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred_gen</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span>
        <span class="n">generator_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_g</span><span class="p">)</span>

        <span class="c1"># Print progress</span>
        <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">%</span> <span class="mi">5000</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="n">epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">weights_epoch_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
            <span class="n">weights_g_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">weights_g</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">: D_loss = </span><span class="si">{</span><span class="n">loss_d</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, G_loss = </span><span class="si">{</span><span class="n">loss_g</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="c1"># # Final output</span>
    <span class="c1"># print("\nTraining complete.\nFinal Generator Weights:")</span>
    <span class="c1"># for k, v in weights_g.items():</span>
    <span class="c1">#     print(f"  {k}: {v:.4f}")</span>
    
    <span class="c1"># print("\nFinal Discriminator Weights:")</span>
    <span class="c1"># for k, v in weights_d.items():</span>
    <span class="c1">#     print(f"  {k}: {v:.4f}")</span>

    <span class="k">return</span> <span class="n">weights_g</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">,</span> <span class="n">generator_losses</span><span class="p">,</span> <span class="n">discriminator_losses</span><span class="p">,</span> <span class="n">real_images</span><span class="p">,</span> <span class="n">weights_g_list</span><span class="p">,</span> <span class="n">weights_epoch_list</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">final_weights_g</span><span class="p">,</span> <span class="n">final_weights_d</span><span class="p">,</span> <span class="n">loss_g</span><span class="p">,</span> <span class="n">loss_d</span><span class="p">,</span> <span class="n">real_images</span><span class="p">,</span> <span class="n">weights_g_list</span><span class="p">,</span> <span class="n">weights_epoch_list</span> <span class="o">=</span> <span class="n">train_gan</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">lr_g</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">lr_d</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">loss_g</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Generator'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">loss_d</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Discriminator'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Loss'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Epoch'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 0: D_loss = 1.3433, G_loss = 0.6666
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 5000: D_loss = 1.0994, G_loss = 0.7903
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 10000: D_loss = 1.0035, G_loss = 0.8478
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 15000: D_loss = 0.9899, G_loss = 0.8271
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 20000: D_loss = 1.0441, G_loss = 0.8124
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 25000: D_loss = 1.0621, G_loss = 0.8129
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 30000: D_loss = 1.1218, G_loss = 0.8277
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 35000: D_loss = 1.1224, G_loss = 0.8411
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 40000: D_loss = 1.1631, G_loss = 0.7914
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 45000: D_loss = 1.1779, G_loss = 0.7685
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 49999: D_loss = 1.1607, G_loss = 0.7617
</pre></div>
</div>
<img alt="_images/0e5b9d59df3b7c13cf96ada6a8409c005cf108794aa5c6d5da5188ad5ef20ded.png" src="../Images/686c89ab95fc15d0556a9ed4d7f3bed6.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/0e5b9d59df3b7c13cf96ada6a8409c005cf108794aa5c6d5da5188ad5ef20ded.png"/>
</div>
</div>
</section>
<section id="visualize-real-images-and-trained-generator-fake-images">
<h2>Visualize Real Images and Trained Generator Fake Images</h2>
<p>Let’s check a set of fake images from our trained generator against the real images.</p>
<ul class="simple">
<li><p>recall the generator never saw these images, the discriminator saw the real and fake images and told the generator how good or bad were the generator’s fake images.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">L1_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span> 
<span class="n">trained_fake</span> <span class="o">=</span> <span class="n">generator_forward</span><span class="p">(</span><span class="n">L1_test</span><span class="p">,</span> <span class="n">weights_g_list</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">untrained_fake</span> <span class="o">=</span> <span class="n">generator_forward</span><span class="p">(</span><span class="n">L1_test</span><span class="p">,</span> <span class="n">weights_g_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">real_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">real_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Real Images'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Value'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Node'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Pixel'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.2</span><span class="p">,</span><span class="mf">0.8</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trained_fake</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">trained_fake</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Trained Generator Fake Images'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Value'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Node'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="sa">r</span><span class="s1">'$O_2$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$O_3$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$O_4$'</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.2</span><span class="p">,</span><span class="mf">0.8</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/765558367eae336a177e7a0b9b0a86037b878b20114ad05070fc072fd5f13404.png" src="../Images/5e411f7e4a786c08b02c80dadc431416.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/765558367eae336a177e7a0b9b0a86037b878b20114ad05070fc072fd5f13404.png"/>
</div>
</div>
</section>
<section id="visualize-real-images-and-generator-fake-images-over-training-epochs">
<h2>Visualize Real Images and Generator Fake Images Over Training Epochs</h2>
<p>It is interesting to see how our generator’s fake images evolve over the training epochs.</p>
<ul class="simple">
<li><p>as first the fake images are random due to the random initialization of the generator’s weights and bias</p></li>
<li><p>as the training proceeds the generator learns to improve the fake images.</p></li>
</ul>
<p>I include the real images at the end for comparison.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">weights_g</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">weights_g_list</span><span class="p">):</span>
    <span class="n">fake</span> <span class="o">=</span> <span class="n">generator_forward</span><span class="p">(</span><span class="n">L1_test</span><span class="p">,</span> <span class="n">weights_g_list</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fake</span><span class="p">[</span><span class="n">j</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fake</span><span class="p">[</span><span class="n">j</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Fake Images: Training Epoch '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">weights_epoch_list</span><span class="p">[</span><span class="n">i</span><span class="p">]));</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Value'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Output Node'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="sa">r</span><span class="s1">'$O_2$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$O_3$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$O_4$'</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.2</span><span class="p">,</span><span class="mf">0.8</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">12</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">real_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">real_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Real Images'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Value'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Pixel'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.2</span><span class="p">,</span><span class="mf">0.8</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">3.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c9fb21bf43af473100eacd88fe058bc353b01c81bec63e5cf3719df9229a386d.png" src="../Images/ad9bdf6d9a437102f9bba86e2f345ae0.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/c9fb21bf43af473100eacd88fe058bc353b01c81bec63e5cf3719df9229a386d.png"/>
</div>
</div>
</section>
<section id="comments">
<h2>Comments</h2>
<p>This was a basic treatment of generative adversarial networks. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a> and the YouTube lecture links at the start of this chapter with resource links in the videos’ descriptions.</p>
<p>I hope this is helpful,</p>
<p><em>Michael</em></p>
</section>
<section id="about-the-author">
<h2>About the Author</h2>
<figure style="text-align: center;">
  <img src="../Images/eb709b2c0a0c715da01ae0165efdf3b2.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/intro/michael_pyrcz_officeshot_jacket.jpg"/>
  <figcaption style="text-align: center;"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.
</figcaption>
</figure>
<p>Michael Pyrcz is a professor in the <a class="reference external" href="https://cockrell.utexas.edu/faculty-directory/alphabetical/p">Cockrell School of Engineering</a>, and the <a class="reference external" href="https://www.jsg.utexas.edu/researcher/michael_pyrcz/">Jackson School of Geosciences</a>, at <a class="reference external" href="https://www.utexas.edu/">The University of Texas at Austin</a>, where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,</p>
<ul class="simple">
<li><p>the principal investigator of the <a class="reference external" href="https://fri.cns.utexas.edu/energy-analytics">Energy Analytics</a> freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin</p></li>
<li><p>an associate editor for <a class="reference external" href="https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board">Computers and Geosciences</a>, and a board member for <a class="reference external" href="https://link.springer.com/journal/11004/editorial-board">Mathematical Geosciences</a>, the International Association for Mathematical Geosciences.</p></li>
</ul>
<p>Michael has written over 70 <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en">peer-reviewed publications</a>, a <a class="reference external" href="https://pypi.org/project/geostatspy/">Python package</a> for spatial data analytics, co-authored a textbook on spatial data analytics, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a> and author of two recently released e-books, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy</a> and <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html">Applied Machine Learning in Python: a Hands-on Guide with Code</a>.</p>
<p>All of Michael’s university lectures are available on his <a class="reference external" href="https://www.youtube.com/@GeostatsGuyLectures">YouTube Channel</a> with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his <a class="reference external" href="https://github.com/GeostatsGuy">GitHub account</a>, to support any interested students and working professionals with evergreen content. To find out more about Michael’s work and shared educational resources visit his <span class="xref myst">Website</span>.</p>
</section>
<section id="want-to-work-together">
<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I’d be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>I’m always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
</section>
&#13;

<h2>Motivation</h2>
<p>What if we put together machines? Working in a competitive, adversarial manner?</p>
<ul class="simple">
<li><p>Could we make a more powerful machine learning model that learns its own loss function!</p></li>
<li><p>Could we make images that don’t collapse to exact reproduction of the images in the training set?</p></li>
</ul>
<p>Generative neural networks are very powerful, nature inspired computing deep learning method to make fake, but realistic, images by application of convolutional neural networks, an analogy of visual cortex that extend the ability of our artificial neural networks to better work with images.</p>
<p>Nature inspired computing is looking to nature for inspiration to develop novel problem-solving methods,</p>
<ul class="simple">
<li><p><strong>artificial neural networks</strong> are inspired by biological neural networks</p></li>
<li><p><strong>nodes</strong> in our model are artificial neurons, simple processors</p></li>
<li><p><strong>connections</strong> between nodes are artificial synapses</p></li>
<li><p><strong>perceptive fields</strong> regularization to improve generalization and efficiency</p></li>
</ul>
<p>intelligence emerges from many connected simple processors. For the remainder of this chapter, I will used the terms nodes and connections to describe our convolutional neural network.</p>
&#13;

<h2>Artificial and Convolutional Neural Networks</h2>
<p>If you have not, take this opportunity to review my previous chapters in the e-book on,</p>
<p><a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ANN.html">Artificial Neural Networks</a></p>
<p>The <strong>main takeaways</strong> from my artificial neural network chapter are as follows,</p>
<ul class="simple">
<li><p><strong>architecture of a neural network</strong>, including its fundamental components, nodes (neurons) and the weighted connections between them.</p></li>
<li><p><strong>forward pass</strong> computation through the network, where each node computes a weighted sum of its inputs (including a bias term), followed by the application of a nonlinear activation function.</p></li>
<li><p><strong>computation of the error derivative</strong>, which is then backpropagated through the network via the chain rule to determine the gradients of the loss function with respect to each weight and bias.</p></li>
<li><p><strong>aggregation of these gradients</strong> across all samples in a training batch, typically by averaging, to update the model parameters.</p></li>
<li><p><strong>iterative training process</strong>, where the model is trained over multiple batches and epochs (passes over all the data) to continually refine the weights and biases until the model achieves an acceptable error rate on the test data.</p></li>
</ul>
<p><a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_CNN.html">Convolutional Neural Networks</a></p>
<p>The main takeaways from my convolutional neural network chapter are as follows,</p>
<ul class="simple">
<li><p><strong>regularization</strong> of image data with receptive fields to preserve spatial information and to avoid overfit.</p></li>
<li><p><strong>convolutional kernels</strong> with learnable weights to extraction information from images.</p></li>
</ul>
<p>For both of these chapters, I have included links to my recorded lectures and to neural networks built from scratch with NumPy only!</p>
&#13;

<h2>Generative Adversarial Networks</h2>
<p>If we start with a convolutional neural network and we flip it, i.e., reverse the order of the operations,</p>
<ul class="simple">
<li><p>we have a machine that maps from a 1D vector of values, to an image, i.e., we can generate fake images by randomly assigning latent values</p></li>
<li><p>to accomplish this instead of convolution operations with activation, we have transpose convolution operations with activation to move to the next feature map</p></li>
<li><p>recall we also perform non-linear activation at each feature map to prevent network collapse</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/edf859adfdeba17f094dbc0415d2bb06.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/GAN/flipcnn.png"/>
  <figcaption style="text-align: center;"> A convolutional neural network flipped and convolution replaced with transpose convolution to go from a 1D random latent vector to a random image.
</figcaption>
</figure>
<p>But how do we train this flipped convolutional neural network to make good images?</p>
<ul class="simple">
<li><p>we could take training images and score the difference between our generated fake images, for example, with a pixel-wise squared error (L2 norm)</p></li>
<li><p>but if we did this, our machine learning model would only learn how to make this image or a limited set of training images and that would not be useful</p></li>
</ul>
<p>We want to make a diverse set of image realizations, that look and behave correctly. This is the simulation paradigm at the heart of geostatistics,</p>
<ul class="simple">
<li><p>to learn more about the simulation paradigm from geostatistics, see my <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/GeostatsPy_simulation.html">Simulation Chapter</a> from my free, online e-book, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book">Applied Geostatistics in Python</a>.</p></li>
</ul>
<p>Instead of a typically loss function, we apply a classification convolutional neural network to map from the image to a probability of a real image, i.e., our loss function is effectively a network that learns to score the loss during training.</p>
<p>We have 2 neural networks in our GAN,</p>
<ul class="simple">
<li><p><strong>generator</strong> - flipped convolutional neural network that makes random fake images</p></li>
<li><p><strong>discriminator</strong> - classification convolutional neural netwrok that calculates the probability that an image is real</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/464dd59ea97be3602e14c9ece74a40a6.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/GAN/ganschematic.png"/>
  <figcaption style="text-align: center;"> A convolutional neural network flipped and convolution replaced with transpose convolution to go from a 1D random latent vector to a random image.
</figcaption>
</figure>
&#13;

<h2>Indirect, Adversarial Learning</h2>
<p>How do we train these two coupled networks? We call each network an agent and we train them competively, e.g., they compete while learning!</p>
<ul class="simple">
<li><p>agent 1, Generator, is not trained to minimize an loss function with respect to training data (training image), no MSE!</p></li>
<li><p>instead the agent 1, Generator, is trained to fool agent 2, Discriminator</p></li>
<li><p>agent 2, Discriminator, is learning at the same time to tell the difference between the real training images and the fakes from agent 1, generator</p></li>
</ul>
<p>Each agent has their own competitive goals,</p>
<ul class="simple">
<li><p>Generator – make fakes that Discriminator classifies as real</p></li>
<li><p>Discriminator – correctly classify fake and real images</p></li>
</ul>
<p>Note, the generator never sees the real images, but by learning to fool the discriminator learns to make images like the real training images.</p>
<p>The GAN loss function is stated as,</p>
<div class="math notranslate nohighlight">
\[
\min_{\theta_G} \, \max_{\theta_D} \;
\mathbb{E}_{\mathbf{y} \sim p_{\text{data}}} \left[ \log D_{\theta_D}(\mathbf{y}) \right]
+
\mathbb{E}_{\mathbf{x} \sim p_{\mathbf{x}}} \left[ \log \left( 1 - D_{\theta_D}(G_{\theta_G}(\mathbf{x})) \right) \right]
\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta_D\)</span> - parameters (weights, biases) of the <strong>discriminator</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_G\)</span> - parameters of the <strong>generator</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(D_{\theta_D}(\cdot)\)</span> - discriminator output, given the discriminator parameters <span class="math notranslate nohighlight">\(\theta_D\)</span> (probability input is real)</p></li>
<li><p><span class="math notranslate nohighlight">\(G_{\theta_G}(\mathbf{x})\)</span> - output of the  Generator output given latent input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y} \sim p_{\text{data}}\)</span> - training images from the <strong>real image set</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x} \sim p_{\mathbf{x}}\)</span> - latent input sampled from known prior (e.g. uniform or normal)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[\cdot]\)</span> - expectation over data (i.e., average over all samples)</p></li>
<li><p><span class="math notranslate nohighlight">\(\log D(\cdot)\)</span> - log-likelihood that the discriminator assigns input as real</p></li>
<li><p><span class="math notranslate nohighlight">\(\log(1 - D(G(\cdot)))\)</span> - log-likelihood that discriminator assigns fake to generator’s output</p></li>
</ul>
<p>The discriminator wants to <strong>maximize</strong>,</p>
<div class="math notranslate nohighlight">
\[
\log D(\mathbf{y}) + \log(1 - D(G(\mathbf{x})))
\]</div>
<ul class="simple">
<li><p>tries to <strong>correctly predicts real</strong> training images as real, <span class="math notranslate nohighlight">\(\log D(\mathbf{y}) \rightarrow 0.0\)</span></p></li>
<li><p>and to <strong>correctly predicts generated</strong> fake training images as not real, <span class="math notranslate nohighlight">\(\log(1 - D(G(\mathbf{x}))) \rightarrow 0.0\)</span></p></li>
</ul>
<p>The generator want to <strong>minimize</strong>,</p>
<div class="math notranslate nohighlight">
\[
\log(1 - D(G(\mathbf{x})))
\]</div>
<ul class="simple">
<li><p>tries to <strong>fool the discriminator</strong>, discriminator classifies fake training images as  real, <span class="math notranslate nohighlight">\(\log(1 - D(G(\mathbf{x}))) \rightarrow -\infty\)</span></p></li>
</ul>
<p>To assist with understanding the GAN loss function and the system of competing agents, consider these end members,</p>
<ol class="arabic simple">
<li><p><strong>Perfect Discriminator</strong> - if the discriminator is perfect,</p></li>
</ol>
<ul class="simple">
<li><p>all real training images are classified as real, <span class="math notranslate nohighlight">\(D(\mathbf{y}) = 1\)</span></p></li>
<li><p>all fake images from the generator are classified as real, <span class="math notranslate nohighlight">\(D(G(\mathbf{x})) = 0\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\quad\)</span> then the discriminator loss is,</p>
<div class="math notranslate nohighlight">
\[
\log(1) + \log(1 - 0) = 0 + \log(1) = 0
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> this sounds like good news, i.e., the generator will then improve to catch up with the discriminator, but what actually happens is,</p>
<ul class="simple">
<li><p>generator receives <strong>no loss gradients</strong>, because the generators gradients,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial \log(1 - D(G(z)))}{\partial \theta_G}
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> if <span class="math notranslate nohighlight">\(D(G(z)) \to 0\)</span>, this derivative becomes <strong>zero</strong>, so training stalls and <strong>the generator doesn’t learn</strong>,</p>
<ul class="simple">
<li><p>this is practically solved by substituting <strong>non-saturating generator loss</strong> for the generator,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L_G = -\mathbb{E}_{z \sim p_z}[\log D(G(z))]
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> if <span class="math notranslate nohighlight">\(D(G(z)) \to 0\)</span>, then <span class="math notranslate nohighlight">\(\log D(G(z)) \to -\infty\)</span>, so the gradient becomes <strong>large</strong>, giving the generator a strong learning signal.</p>
<ol class="arabic simple" start="2">
<li><p><strong>Perfect Generator</strong> - if the generator is perfect,</p></li>
</ol>
<ul class="simple">
<li><p>all fake images have the same distribution as the real training images, <span class="math notranslate nohighlight">\(G(\mathbf{x}) \sim p_{\text{data}}\)</span></p></li>
<li><p>the best the discriminator can do is to assign a anive classification, <span class="math notranslate nohighlight">\(D(\cdot) = 0.5\)</span>, for all fake and real training images</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\quad\)</span> then the loss is,</p>
<div class="math notranslate nohighlight">
\[
\log(0.5) + \log(1 - 0.5) = \log(0.5) + \log(0.5) = -\log 4
\]</div>
<ul class="simple">
<li><p>discriminator is <strong>maximally confused</strong></p></li>
<li><p>this is a <strong>Nash equilibrium</strong> for the GAN, because no player can improve their outcome by unilaterally changing their strategy, assuming the other player’s strategy stays the same, generator is already making perfect images and discriminator can only guess naively, 50/50 real and fake.</p></li>
</ul>
&#13;

<h2>Import Required Packages</h2>
<p>We will also need some standard packages. These should have been installed with Anaconda 3.</p>
<ul class="simple">
<li><p>recall our goal is to build a convolutional neural network by-hand with only basic math and array operations, so we only need NumPy along with matplotlib for plotting.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="o">%</span><span class="k">matplotlib</span> inline                                         
<span class="n">suppress_warnings</span> <span class="o">=</span> <span class="kc">True</span>                                      <span class="c1"># toggle to supress warnings</span>
<span class="kn">import</span> <span class="nn">os</span>                                                     <span class="c1"># set working directory</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                                            <span class="c1"># arrays and matrix math</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>                               <span class="c1"># for plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">patches</span>                          <span class="c1"># fancy box around agents</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span> <span class="n">AutoMinorLocator</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="n">FuncFormatter</span>
<span class="kn">import</span> <span class="nn">copy</span>                                                   <span class="c1"># deep copy dictionaries</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>                                         <span class="c1"># default color bar, no bias and friendly for color vision defeciency</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># grid behind plotting elements</span>
<span class="k">if</span> <span class="n">suppress_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>  
    <span class="kn">import</span> <span class="nn">warnings</span>                                           <span class="c1"># supress any warnings for this demonstration</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span> 
<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>                                                     <span class="c1"># random number seed for workflow repeatability</span>
</pre></div>
</div>
</div>
</div>
<p>If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing ‘python -m pip install [package-name]’. More assistance is available with the respective package docs.</p>
&#13;

<h2>Declare Functions</h2>
<p>Here’s the functions to make, train and visualize our generative adversarial network, including the steps,</p>
<ul class="simple">
<li><p>make a simple set of synthetic data</p></li>
<li><p>initialize the weights in our generator and discriminator</p></li>
<li><p>apply our generator and discrimintor</p></li>
<li><p>calculate the error derivative and update the generator and discriminator weights and biases</p></li>
</ul>
<p>Here’s a list of the functions,</p>
<ol class="arabic simple">
<li><p><strong>generate_real_data</strong> -  synthetic data generator</p></li>
<li><p><strong>initialize_generator_weights</strong> - assign small random weights and bias for generator</p></li>
<li><p><strong>initialize_discriminator_weights</strong> - assign small random weights and bias for discriminator</p></li>
<li><p><strong>generator_forward</strong> - calculate a set of fake data with the generator given a set of latent values and the current weights and biases</p></li>
<li><p><strong>discriminator_forward</strong> - calculate the probability of a real image over a set of images and return a 1D ndarray of probabilities</p></li>
<li><p><strong>sigmoid</strong> - activation function to apply in the generator and discriminator</p></li>
<li><p><strong>generator_gradients</strong> - compute generator gradients averaged over the batch</p></li>
<li><p><strong>discriminator_gradients</strong> - compute generator gradients averaged over the batch</p></li>
</ol>
<p>Here are the functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>                                               <span class="c1"># sigmoid activation function</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">generate_real_data</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">slope_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">),</span> <span class="n">residual_std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">):</span> <span class="c1"># make a synthetic training image set</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Generate real 3-node images with decreasing linear trend plus noise.</span>
<span class="sd">    Standardize each to have mean 0.5.</span>
<span class="sd">    Returns shape (batch_size, 3)</span>
<span class="sd">    """</span>
    <span class="n">slopes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">slope_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">slope_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">base</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># node positions for linear trend</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">trend</span> <span class="o">=</span> <span class="n">slopes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">base</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">residual_std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">trend</span> <span class="o">+</span> <span class="n">residual</span>
        <span class="n">sample</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>                       <span class="c1"># standardize to mean 0.5</span>
        <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="k">def</span> <span class="nf">initialize_generator_weights</span><span class="p">():</span>                           <span class="c1"># initialize the generator weights and return as a dictionary</span>
    <span class="c1"># Small random weights and bias for generator</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">'lambda_12'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">'lambda_13'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">'lambda_14'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">'b'</span><span class="p">:</span> <span class="mf">0.0</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">generator_forward</span><span class="p">(</span><span class="n">L1_latent</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span><span class="n">return_pre_activation</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> <span class="c1"># given latent vector and generator weights return a set of fake images</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    L1_latent: ndarray shape (batch_size,)</span>
<span class="sd">    weights: dict with keys 'lambda_1_2', 'lambda_1_3', 'lambda_1_4', 'b'</span>
<span class="sd">    Returns output ndarray shape (batch_size, 3)</span>
<span class="sd">    """</span>
    <span class="n">O2in</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'lambda_12'</span><span class="p">]</span> <span class="o">*</span> <span class="n">L1_latent</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'b'</span><span class="p">]</span>
    <span class="n">O3in</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'lambda_13'</span><span class="p">]</span> <span class="o">*</span> <span class="n">L1_latent</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'b'</span><span class="p">]</span>
    <span class="n">O4in</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'lambda_14'</span><span class="p">]</span> <span class="o">*</span> <span class="n">L1_latent</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'b'</span><span class="p">]</span>

    <span class="n">O2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">O2in</span><span class="p">)</span>
    <span class="n">O3</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">O3in</span><span class="p">)</span>
    <span class="n">O4</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">O4in</span><span class="p">)</span>
    
    <span class="n">Oin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">O2in</span><span class="p">,</span> <span class="n">O3in</span><span class="p">,</span> <span class="n">O4in</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">O2</span><span class="p">,</span> <span class="n">O3</span><span class="p">,</span> <span class="n">O4</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">;</span> 

    <span class="k">if</span> <span class="n">return_pre_activation</span><span class="p">:</span> 
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">O2in</span><span class="p">,</span> <span class="n">O3in</span><span class="p">,</span> <span class="n">O4in</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">O2</span><span class="p">,</span> <span class="n">O3</span><span class="p">,</span> <span class="n">O4</span><span class="p">])</span><span class="o">.</span><span class="n">T</span> <span class="c1"># shape (batch_size, 3)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">O2</span><span class="p">,</span> <span class="n">O3</span><span class="p">,</span> <span class="n">O4</span><span class="p">])</span><span class="o">.</span><span class="n">T</span> <span class="c1"># shape (batch_size, 3)</span>

<span class="k">def</span> <span class="nf">initialize_discriminator_weights</span><span class="p">():</span>                       <span class="c1"># initialize the discriminator weights and return as a dictionary               </span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">'lambda_58'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">'lambda_68'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">'lambda_78'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">'c'</span><span class="p">:</span> <span class="mf">0.0</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">discriminator_forward</span><span class="p">(</span><span class="n">I5</span><span class="p">,</span> <span class="n">I6</span><span class="p">,</span> <span class="n">I7</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>               <span class="c1"># given a set of images return the discriminator probability of real image</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Inputs: I5, I6, I7 shape (batch_size,)</span>
<span class="sd">    weights: dict with keys 'lambda_58', 'lambda_68', 'lambda_78', 'c'</span>
<span class="sd">    Returns probability ndarray shape (batch_size,)</span>
<span class="sd">    """</span>
    <span class="n">dO8in</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="s1">'lambda_58'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I5</span> <span class="o">+</span> 
         <span class="n">weights</span><span class="p">[</span><span class="s1">'lambda_68'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I6</span> <span class="o">+</span> 
         <span class="n">weights</span><span class="p">[</span><span class="s1">'lambda_78'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I7</span> <span class="o">+</span> 
         <span class="n">weights</span><span class="p">[</span><span class="s1">'c'</span><span class="p">])</span>
    <span class="n">dO8</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">dO8in</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dO8</span><span class="p">,</span> <span class="n">dO8in</span>

<span class="k">def</span> <span class="nf">discriminator_gradients</span><span class="p">(</span><span class="n">I5</span><span class="p">,</span> <span class="n">I6</span><span class="p">,</span> <span class="n">I7</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>     <span class="c1"># given set of images, true labels, and discriminator weights return the gradients</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Compute discriminator gradients averaged over batch.</span>
<span class="sd">    y_true: labels (1 for real, 0 for fake), shape (batch_size,)</span>
<span class="sd">    """</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y_pred</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">discriminator_forward</span><span class="p">(</span><span class="n">I5</span><span class="p">,</span> <span class="n">I6</span><span class="p">,</span> <span class="n">I7</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
    <span class="n">dO8in</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span>                                  <span class="c1"># shape (batch_size,), note this solution integrates the sigmoid activation at O8</span>
    
    <span class="n">grad_lambda_58</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO8in</span> <span class="o">*</span> <span class="n">I5</span><span class="p">)</span>
    <span class="n">grad_lambda_68</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO8in</span> <span class="o">*</span> <span class="n">I6</span><span class="p">)</span>
    <span class="n">grad_lambda_78</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO8in</span> <span class="o">*</span> <span class="n">I7</span><span class="p">)</span>
    <span class="n">grad_c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO8in</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">'lambda_58'</span><span class="p">:</span> <span class="n">grad_lambda_58</span><span class="p">,</span>
        <span class="s1">'lambda_68'</span><span class="p">:</span> <span class="n">grad_lambda_68</span><span class="p">,</span>
        <span class="s1">'lambda_78'</span><span class="p">:</span> <span class="n">grad_lambda_78</span><span class="p">,</span>
        <span class="s1">'c'</span><span class="p">:</span> <span class="n">grad_c</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">generator_gradients</span><span class="p">(</span><span class="n">L1_latent</span><span class="p">,</span> <span class="n">weights_g</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">):</span>     <span class="c1"># given latent vector, generator and discriminator weights return the gradients</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Compute gradients of generator weights using discriminator feedback.</span>
<span class="sd">    L1_latent: shape (batch_size,)</span>
<span class="sd">    weights_g: generator weights dict</span>
<span class="sd">    weights_d: discriminator weights dict</span>
<span class="sd">    """</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">L1_latent</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1">#O = generator_forward(L1_latent, weights_g)               # generator outputs, shape (batch_size, 3)</span>
    <span class="n">O_in</span><span class="p">,</span> <span class="n">O</span> <span class="o">=</span> <span class="n">generator_forward</span><span class="p">(</span><span class="n">L1_latent</span><span class="p">,</span> <span class="n">weights_g</span><span class="p">,</span> <span class="n">return_pre_activation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">O2_in</span><span class="p">,</span> <span class="n">O3_in</span><span class="p">,</span> <span class="n">O4_in</span> <span class="o">=</span> <span class="n">O_in</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">O_in</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">O_in</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span>
    <span class="n">O2</span><span class="p">,</span> <span class="n">O3</span><span class="p">,</span> <span class="n">O4</span> <span class="o">=</span> <span class="n">O</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">O</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">O</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">I5</span><span class="p">,</span> <span class="n">I6</span><span class="p">,</span> <span class="n">I7</span> <span class="o">=</span> <span class="n">O</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">O</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">O</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
    
    <span class="n">y_pred</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">discriminator_forward</span><span class="p">(</span><span class="n">I5</span><span class="p">,</span> <span class="n">I6</span><span class="p">,</span> <span class="n">I7</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">)</span>  <span class="c1"># discriminator forward pass</span>
    
    <span class="n">dO8in</span> <span class="o">=</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="mi">1</span>                                           <span class="c1"># gradient of loss w.r.t discriminator logit for generator loss, shape (batch_size,)</span>
    
    <span class="n">dO2</span> <span class="o">=</span> <span class="n">dO8in</span> <span class="o">*</span> <span class="n">weights_d</span><span class="p">[</span><span class="s1">'lambda_58'</span><span class="p">]</span>                         <span class="c1"># gradients w.r.t generator outputs</span>
    <span class="n">dO3</span> <span class="o">=</span> <span class="n">dO8in</span> <span class="o">*</span> <span class="n">weights_d</span><span class="p">[</span><span class="s1">'lambda_68'</span><span class="p">]</span>
    <span class="n">dO4</span> <span class="o">=</span> <span class="n">dO8in</span> <span class="o">*</span> <span class="n">weights_d</span><span class="p">[</span><span class="s1">'lambda_78'</span><span class="p">]</span>

    <span class="n">dO2in</span> <span class="o">=</span> <span class="n">dO2</span> <span class="o">*</span> <span class="n">O2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">O2</span><span class="p">)</span>                                  <span class="c1"># Backprop through generator sigmoid activation</span>
    <span class="n">dO3in</span> <span class="o">=</span> <span class="n">dO3</span> <span class="o">*</span> <span class="n">O3</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">O3</span><span class="p">)</span>
    <span class="n">dO4in</span> <span class="o">=</span> <span class="n">dO4</span> <span class="o">*</span> <span class="n">O4</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">O4</span><span class="p">)</span>
    
    <span class="n">grad_lambda_12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO2in</span> <span class="o">*</span> <span class="n">L1_latent</span><span class="p">)</span>                 <span class="c1"># gradients w.r.t generator weights and bias</span>
    <span class="n">grad_lambda_13</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO3in</span> <span class="o">*</span> <span class="n">L1_latent</span><span class="p">)</span>
    <span class="n">grad_lambda_14</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO4in</span> <span class="o">*</span> <span class="n">L1_latent</span><span class="p">)</span>
    <span class="n">grad_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dO2in</span> <span class="o">+</span> <span class="n">dO3in</span> <span class="o">+</span> <span class="n">dO4in</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">'lambda_12'</span><span class="p">:</span> <span class="n">grad_lambda_12</span><span class="p">,</span>
        <span class="s1">'lambda_13'</span><span class="p">:</span> <span class="n">grad_lambda_13</span><span class="p">,</span>
        <span class="s1">'lambda_14'</span><span class="p">:</span> <span class="n">grad_lambda_14</span><span class="p">,</span>
        <span class="s1">'b'</span><span class="p">:</span> <span class="n">grad_b</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">fancybox</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">xy</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">text_color</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span> <span class="c1"># a dashed fancy box for the GAN plot</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Draws a dashed, rounded rectangle on a given axes.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - ax: The matplotlib axes to draw on</span>
<span class="sd">    - xy: (x, y) tuple for bottom-left corner of the box</span>
<span class="sd">    - width: Width of the box</span>
<span class="sd">    - height: Height of the box</span>
<span class="sd">    - label: Optional label text to display centered above the box</span>
<span class="sd">    - edgecolor: Border color of the box</span>
<span class="sd">    - text_color: Color of the label text (defaults to edgecolor)</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="n">text_color</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">text_color</span> <span class="o">=</span> <span class="n">edgecolor</span>

    <span class="n">box</span> <span class="o">=</span> <span class="n">patches</span><span class="o">.</span><span class="n">FancyBboxPatch</span><span class="p">(</span>                             <span class="c1"># draw box</span>
        <span class="n">xy</span><span class="p">,</span>
        <span class="n">width</span><span class="p">,</span>
        <span class="n">height</span><span class="p">,</span>
        <span class="n">boxstyle</span><span class="o">=</span><span class="s2">"round,pad=0.02"</span><span class="p">,</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">edgecolor</span><span class="o">=</span><span class="n">edgecolor</span><span class="p">,</span>
        <span class="n">facecolor</span><span class="o">=</span><span class="s2">"none"</span><span class="p">,</span>
        <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">box</span><span class="p">)</span>

    <span class="n">x_center</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">width</span> <span class="o">/</span> <span class="mi">2</span>                              <span class="c1"># add label text above the box</span>
    <span class="n">y_top</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">height</span> <span class="o">+</span> <span class="mf">0.02</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x_center</span><span class="p">,</span> <span class="n">y_top</span> <span class="o">+</span> <span class="mf">0.02</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'bottom'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">text_color</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add_grid2</span><span class="p">():</span>                                              <span class="c1"># add grid lines</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks </span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Set the Working Directory</h2>
<p>I always like to do this so I don’t lose files and to simplify subsequent read and writes (avoid including the full address each time).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1">#os.chdir("c:/PGE383")                                        # set the working directory</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Visualize the Generative Adversarial Network</h2>
<p>We are implementing a minimal Generative Adversarial Network (GAN) with the 2 agents,</p>
<ul class="simple">
<li><p><strong>Generator</strong> - that produces 3-node outputs (like tiny 1D images) from a single latent input</p></li>
<li><p><strong>Discriminator</strong> - that evaluates these outputs to distinguish between <strong>real</strong> and <strong>fake</strong> samples.</p></li>
</ul>
<p>Now let’s define the parts of the <strong>Generator</strong>,</p>
<ul class="simple">
<li><p><strong>Latent Node</strong> - <span class="math notranslate nohighlight">\(L_1\)</span>, a single random value with uniform distribution, <span class="math notranslate nohighlight">\(U[0.4,1.0]\)</span>. Note we set the minimum as 0.4 to stay away from 0.0 or negative values as these would remove the slope or flip the slope of the fakes.</p></li>
<li><p><strong>Generator Weights</strong> - <span class="math notranslate nohighlight">\(\lambda_{1,2}\)</span>, <span class="math notranslate nohighlight">\(\lambda_{1,3}\)</span> and <span class="math notranslate nohighlight">\(\lambda_{1,4}\)</span> for the connections from latent to each of the output nodes. This is the simplest possible tranpose convolution, with a kernel size is 3, output nodes is 3 and latent node is 1, so the kernel does not translate. I did this to greatly simplify the book keeping, but the concepts could be extended to a more realistic convolution / tranpose convolution architectures for more realistic images sizes problem.</p></li>
<li><p><strong>Generator Bias</strong> - <span class="math notranslate nohighlight">\(b\)</span>, a single, constant bias over the output layer (output image), the nodes, <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, and <span class="math notranslate nohighlight">\(O_4\)</span></p></li>
<li><p><strong>Generator Output Nodes</strong> - <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, and <span class="math notranslate nohighlight">\(O_4\)</span>, the single and last feature map in our very simple generator; therefore, the output a 1D image with 3 nodes or pixels that are passed to the <strong>Discriminator</strong> input nodes, <span class="math notranslate nohighlight">\(I_5\)</span>, <span class="math notranslate nohighlight">\(I_6\)</span>, and <span class="math notranslate nohighlight">\(I_7\)</span></p></li>
<li><p><strong>Discriminator Input Nodes</strong> - <span class="math notranslate nohighlight">\(I_5\)</span>, <span class="math notranslate nohighlight">\(I_6\)</span>, and <span class="math notranslate nohighlight">\(I_7\)</span>, that receive the real images or the fake images from the generator output nodes, <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, <span class="math notranslate nohighlight">\(O_4\)</span></p></li>
<li><p><strong>Discriminator Weights</strong> - <span class="math notranslate nohighlight">\(\lambda_{5,8}\)</span>, <span class="math notranslate nohighlight">\(\lambda_{6,8}\)</span>, and <span class="math notranslate nohighlight">\(\lambda_{7,8}\)</span> for the connections from input nodes (input image) to the output (descision) node, <span class="math notranslate nohighlight">\(D_8\)</span></p></li>
<li><p><strong>Discriminator Bias</strong> - <span class="math notranslate nohighlight">\(c\)</span>, bias applied at the output (descision) node, <span class="math notranslate nohighlight">\(D_8\)</span></p></li>
</ul>
<p>Now let’s visualize this very simple generative adversarial network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'darksalmon'</span><span class="p">,</span><span class="s1">'deepskyblue'</span><span class="p">,</span><span class="s1">'gold'</span><span class="p">]</span>                  <span class="c1"># line colors for latent to fake to probability flow</span>
<span class="n">colors_real</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'orangered'</span><span class="p">,</span><span class="s1">'dodgerblue'</span><span class="p">,</span><span class="s1">'goldenrod'</span><span class="p">]</span>          <span class="c1"># line colors for real image to probability flow</span>

<span class="k">def</span> <span class="nf">draw_gan_architecture_full</span><span class="p">():</span>                             <span class="c1"># function to draw the GAN demonstrated in this workflow</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$L_1$"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="c1"># generator latent node</span>
            <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s2">"white"</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s2">"black"</span><span class="p">))</span>

    <span class="n">gen_outputs</span> <span class="o">=</span> <span class="p">[</span>                                           <span class="c1"># generator output nodes locations and labels</span>
        <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$O_2$"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$\lambda_{1,2}$"</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$O_3$"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$\lambda_{1,3}$"</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$O_4$"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$\lambda_{1,4}$"</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gen_outputs</span><span class="p">):</span>   <span class="c1"># generator output node labels</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
                <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s2">"white"</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s2">"black"</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>   <span class="c1"># generator output node connections</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-&gt;"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">((</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.11</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'bottom'</span><span class="p">)</span>

    <span class="n">disc_inputs</span> <span class="o">=</span> <span class="p">[</span>                                           <span class="c1"># discriminator input nodes locations and labels</span>
        <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$I_5$"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$\lambda_{5,8}$"</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$I_6$"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$\lambda_{6,8}$"</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$I_7$"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$\lambda_{7,8}$"</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">disc_inputs</span><span class="p">):</span>        <span class="c1"># discriminator input node labels</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
                <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s2">"white"</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s2">"black"</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">gen_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span> <span class="c1"># discriminator input node connections</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-&gt;"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$D_8$"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="c1"># discriminator decision node label</span>
            <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s2">"white"</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s2">"black"</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">disc_inputs</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span><span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>  <span class="c1"># discriminator output node connections</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-&gt;"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">((</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.7</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'bottom'</span><span class="p">)</span>

    <span class="n">real_inputs</span> <span class="o">=</span> <span class="p">[</span>                                           <span class="c1"># real data path below generator</span>
        <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$I_5^</span><span class="si">{real}</span><span class="s2">$"</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$I_6^</span><span class="si">{real}</span><span class="s2">$"</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="sa">r</span><span class="s2">"$I_7^</span><span class="si">{real}</span><span class="s2">$"</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">real_inputs</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="c1"># label real data node labels</span>
                <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s2">"white"</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s2">"black"</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span>                                       <span class="c1"># arrow to discriminator inputs</span>
                    <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">disc_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">disc_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span> 
                    <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-&gt;"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors_real</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">"--"</span><span class="p">))</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.57</span><span class="p">,</span> <span class="s2">"Latent"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>    <span class="c1"># GAN part labels</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.57</span><span class="p">,</span> <span class="s2">"Classification"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.26</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.18</span><span class="p">,</span> <span class="s2">"Real Images"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.26</span><span class="p">,</span> <span class="mf">0.38</span><span class="p">,</span> <span class="s2">"Fake Images"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">)</span>

    <span class="n">fancybox</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.07</span><span class="p">,</span> <span class="mf">0.24</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Generator"</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">"grey"</span><span class="p">)</span> <span class="c1"># fancy boxes around generator and discriminator</span>
    <span class="n">fancybox</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.48</span><span class="p">,</span> <span class="mf">0.24</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.29</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Discriminator"</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">"grey"</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.68</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="s1">'c'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>           <span class="c1"># draw the biases in O2, O3, O4 and D8</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.695</span><span class="p">,</span> <span class="mf">0.47</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.685</span><span class="p">,</span> <span class="mf">0.42</span><span class="p">),</span><span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.27</span><span class="p">,</span><span class="mf">0.63</span><span class="p">,</span><span class="s1">'b'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.28</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.29</span><span class="p">,</span> <span class="mf">0.68</span><span class="p">),</span><span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.27</span><span class="p">,</span><span class="mf">0.43</span><span class="p">,</span><span class="s1">'b'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.28</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.29</span><span class="p">,</span> <span class="mf">0.48</span><span class="p">),</span><span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.27</span><span class="p">,</span><span class="mf">0.23</span><span class="p">,</span><span class="s1">'b'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.28</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.29</span><span class="p">,</span> <span class="mf">0.28</span><span class="p">),</span><span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">draw_gan_architecture_full</span><span class="p">()</span>                                  <span class="c1"># draw the GAN         </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/565edb7221aa6743a530bb98f80b562615b08280d5d3d4a52aee7ff15cefcaf2.png" src="../Images/cdcce92e4f0534109850166492b3b2ef.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/565edb7221aa6743a530bb98f80b562615b08280d5d3d4a52aee7ff15cefcaf2.png"/>
</div>
</div>
<p>Just a couple more comments about my network nomenclature. My goal is to maximize simplicity and clarity,</p>
&#13;

<h2>Comments on Network Nomenclature</h2>
<ul class="simple">
<li><p><strong>Network Nodes and Connections</strong> - I choose to use unique numbers for all nodes, <span class="math notranslate nohighlight">\(L_1\)</span>, <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, <span class="math notranslate nohighlight">\(\ldots\)</span> instead of <span class="math notranslate nohighlight">\(L_1\)</span>, <span class="math notranslate nohighlight">\(O_1\)</span>, <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(\ldots\)</span> to simplify the notation for the weights; therefore, when I say <span class="math notranslate nohighlight">\(\lambda_{5,8}\)</span> you know exactly where this weight is applied in the network.</p></li>
<li><p><strong>Node Outputs</strong> - I use the node label to also describe the output from the node, for example <span class="math notranslate nohighlight">\(O_2\)</span> is a node in the generator’s output layer and also the signal or value output from that node.</p></li>
<li><p><strong>Pre- and Post-activation</strong> - at our nodes <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, <span class="math notranslate nohighlight">\(O_4\)</span>, and <span class="math notranslate nohighlight">\(D_8\)</span> we have the node input before activation and the node output after activation, I use the notation <span class="math notranslate nohighlight">\(O_{2_{in}}\)</span>, <span class="math notranslate nohighlight">\(O_{3_{in}}\)</span>, <span class="math notranslate nohighlight">\(O_{4_{in}}\)</span> and <span class="math notranslate nohighlight">\(D_{8_{in}}\)</span> for the pre-activation input and <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, <span class="math notranslate nohighlight">\(O_4\)</span>, and <span class="math notranslate nohighlight">\(D_8\)</span> for the post-activation node output. This is important because with back propagation we have to step through the nodes, going from post-activation to pre-activation.</p></li>
<li><p><strong>Latent</strong> - while publications often use <span class="math notranslate nohighlight">\(z\)</span> notation for the latent values, to be consistent with my notion above, I use <span class="math notranslate nohighlight">\(L_1\)</span> for the latent value, i.e., the output from my latent node, <span class="math notranslate nohighlight">\(L_1\)</span>.</p></li>
</ul>
<p>Now let’s walk-through all the parts of our example GAN and show all the math.</p>
&#13;

<h2>Sigmoid Activation</h2>
<p>For reference, let’s visualize the sigmoid activation function,</p>
<ul class="simple">
<li><p><strong>activation</strong> - the non-linear transformation, this is the sigmoid activation</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
x_{out} = \sigma(x_{in}) = \dfrac{1}{1 + e^{-x_{in}}}
\]</div>
<ul class="simple">
<li><p><strong>activation derrivative</strong> - essential for back propogation</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sigma'(x_{in}) = \sigma(x_{out})(1 - \sigma(x_{out}))
\]</div>
<p>note, for convenience the derrivative of the sigmoid activation function with respect to the input is posed for the output.</p>
<ul class="simple">
<li><p>as we back-propogate backwards over the activation function we can us the output to step back through the activated network node</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">amin</span> <span class="o">=</span> <span class="o">-</span><span class="mf">5.0</span><span class="p">;</span> <span class="n">amax</span> <span class="o">=</span> <span class="mf">5.0</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">amin</span><span class="p">,</span><span class="n">amax</span><span class="p">,</span><span class="mi">100</span><span class="p">);</span> <span class="n">output</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">amin</span><span class="p">,</span><span class="n">amax</span><span class="p">,</span><span class="mi">100</span><span class="p">)));</span> <span class="n">derriv</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">output</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'forestgreen'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Activation'</span><span class="p">);</span> <span class="n">add_grid2</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Input'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Output'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">derriv</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'deepskyblue'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Derrivative'</span><span class="p">);</span> <span class="n">add_grid2</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Input'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Output'</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Sigmoid Activation'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">amin</span><span class="p">,</span><span class="n">amax</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f7323a3bf2b5671f3b7f629a5e2dd8966903903e490d8b510d1b437b922d20ad.png" src="../Images/6aef20df3b024801181a361544b98363.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/f7323a3bf2b5671f3b7f629a5e2dd8966903903e490d8b510d1b437b922d20ad.png"/>
</div>
</div>
<p>Let’s make some observations about the sigmoid activation and its derrivative,</p>
<ul class="simple">
<li><p><strong>sigmoid outputs</strong> - are bounded (0,1) approaching both limits asymptotically</p></li>
<li><p><strong>vanishing gradients</strong> - as the</p></li>
</ul>
&#13;

<h2>Generator Forward Pass</h2>
<p>First, let’s walk through the generator to go from a latent value to a fake image. The generator takes a latent input,</p>
<div class="math notranslate nohighlight">
\[
L_1 \sim \mathcal{U}(0.4, 1)
\]</div>
<ul class="simple">
<li><p>recall, our simple generator has only one layer <span class="math notranslate nohighlight">\(L1\)</span>, with only 3 outputs, <span class="math notranslate nohighlight">\(O_2\)</span>, <span class="math notranslate nohighlight">\(O_3\)</span>, and <span class="math notranslate nohighlight">\(O_4\)</span>, representing the fake image.</p></li>
</ul>
<p>Then latent value, <span class="math notranslate nohighlight">\(L_1\)</span>, is passed through the transpose convolution kernel to the output,</p>
<ul class="simple">
<li><p>our transpose convolution kernel has a size of 3, the same size as our output, so we don’t see it translate it, resulting in greatly simplified book keeping!</p></li>
<li><p>the tranpose convolution kernel weights are <span class="math notranslate nohighlight">\(\lambda_{1,2}\)</span>, <span class="math notranslate nohighlight">\(\lambda_{1,3}\)</span>, and <span class="math notranslate nohighlight">\(\lambda_{1,4}\)</span></p></li>
</ul>
<p>We apply the sigmoid activation in each of the output nodes</p>
<p>Each output is computed by applying a linear transformation followed by a <strong>sigmoid</strong> activation, <span class="math notranslate nohighlight">\(\sigma\)</span>,</p>
<div class="math notranslate nohighlight">
\[
O_2 = \sigma(\lambda_{1,2} \cdot z + b)
\]</div>
<div class="math notranslate nohighlight">
\[
O_3 = \sigma(\lambda_{1,3} \cdot z + b)
\]</div>
<div class="math notranslate nohighlight">
\[
O_4 = \sigma(\lambda_{1,4} \cdot z + b)
\]</div>
<p>where,</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(\lambda_{1,j}\)</span></strong> - are the transpose convolution kernel weights</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(b\)</span></strong> - is the shared bias, single bias term for the output layer</p></li>
</ul>
<p>We can also write the generator forward pass in matrix notation as,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
O_2 \\
O_3 \\
O_4
\end{bmatrix}
=
\sigma\left(
\begin{bmatrix}
\lambda_{1,2} \\
\lambda_{1,3} \\
\lambda_{1,4}
\end{bmatrix}
z + 
\begin{bmatrix}
b \\
b \\
b
\end{bmatrix}
\right)
\end{split}\]</div>
<p>where the sigmoid activation is applied element-wise.</p>
&#13;

<h2>Discriminator Forward Pass</h2>
<p>Now let’s walk-through the discriminator, going from an image, real or fake, to a probability of real. The discriminator receives the image, over 3 input nodes, <span class="math notranslate nohighlight">\(I_5\)</span>, <span class="math notranslate nohighlight">\(I_6\)</span>, and <span class="math notranslate nohighlight">\(I_7\)</span>. In the case of a fake image,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
I_5 \\
I_6 \\
I_7
\end{bmatrix}
=
\begin{bmatrix}
O_2 \\
O_3 \\
O_4
\end{bmatrix}
\end{split}\]</div>
<p>and in the case of a real image,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
I_5 \\
I_6 \\
I_7
\end{bmatrix}
=
\begin{bmatrix}
I_5^{real} \\
I_6^{real} \\
I_7^{real}
\end{bmatrix}
\end{split}\]</div>
<p>Since we have only 1 layer and the convolution kernel is 3 with an input of 3 once again there is no translation!</p>
<ul class="simple">
<li><p>we just take input image, <span class="math notranslate nohighlight">\(I_5\)</span>, <span class="math notranslate nohighlight">\(I_6\)</span>, and <span class="math notranslate nohighlight">\(I_7\)</span>, and apply the convolutional kernel weights, <span class="math notranslate nohighlight">\(\lambda_{5,8}\)</span>, <span class="math notranslate nohighlight">\(\lambda_{6,8}\)</span>, and <span class="math notranslate nohighlight">\(\lambda_{7,8}\)</span>, and add the bias term, <span class="math notranslate nohighlight">\(c\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
D_8 = \sigma\left(
\lambda_{5,8} \cdot I_5 +
\lambda_{6,8} \cdot I_6 +
\lambda_{7,8} \cdot I_7 + c
\right)
\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda_{i,8}\)</span> are the convolutional kernel weights to go from input image to next feature map, only 1 value, our output probability</p></li>
<li><p><span class="math notranslate nohighlight">\(c\)</span> is the bias term</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma(x) = \dfrac{1}{1 + e^{-x}}\)</span> is the sigmoid activation function</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(D_8 \in [0, 1]\)</span> represents the probability assigned by the discriminator that the input is <strong>real</strong> (i.e. not a fake from the generator).</p>
<p>We can also write the discriminator forward pass in matrix notation as,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
D_8 = \sigma\left(
\begin{bmatrix}
\lambda_{5,8} &amp; \lambda_{6,8} &amp; \lambda_{7,8}
\end{bmatrix}
\cdot
\begin{bmatrix}
I_5 \\
I_6 \\
I_7
\end{bmatrix}
+ c
\right)
\end{split}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda_{5,8}, \lambda_{6,8}, \lambda_{7,8}\)</span> are scalar weights</p></li>
<li><p><span class="math notranslate nohighlight">\(I_5, I_6, I_7\)</span> are the input values (i.e., outputs of the generator, <span class="math notranslate nohighlight">\(O_2, O_3, O_4\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(c\)</span> is the bias term</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma(x) = \dfrac{1}{1 + e^{-x}}\)</span> is the sigmoid function</p></li>
</ul>
&#13;

<h2>Discriminator Loss</h2>
<p>Binary cross-entropy is a loss function used for binary classification tasks where the output is a probability between 0 and 1, and the target label is either 0 or 1.</p>
<ul class="simple">
<li><p><strong>Prediction</strong> (model output) - <span class="math notranslate nohighlight">\(\hat{y} \in (0, 1)\)</span>, the output of <span class="math notranslate nohighlight">\(D_8\)</span>, the discriminator’s classification, probability that the image is real</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y} = D_8
\]</div>
<ul class="simple">
<li><p><strong>True label</strong> (ground truth) - <span class="math notranslate nohighlight">\(y \in \{0, 1\}\)</span>, 0 if the image is from the generator, fake, and 1 if the image is from the real training data</p></li>
</ul>
<p>Now we can define the <strong>binary cross-entropy loss</strong> as,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{BCE}}(y, \hat{y}) = - \left[ y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y}) \right]
\]</div>
<p>now we can further specify,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\log(\hat{y})\)</span> is the log-likelihood of the positive prediction</p></li>
<li><p><span class="math notranslate nohighlight">\(log(1 - \hat{y})\)</span> is the log-likelihood of the negative prediction</p></li>
</ul>
<p>how does binary cross-entropy behave?</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(y = 1\)</span> (real image), then:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = -\log(\hat{y}) \quad \text{(we want } \hat{y} \to 1)
\]</div>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(y = 0\)</span> (fake image from the generator), then:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = -\log(1 - \hat{y}) \quad \text{(we want } \hat{y} \to 0)
\]</div>
<p>We can summarize as,</p>
<ul class="simple">
<li><p>the loss is <strong>low</strong> when the model’s prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span> is <strong>close to the true label</strong>, low probability of real for a fake image and high probability of real for a real image</p></li>
<li><p>the loss becomes <strong>very large</strong> if the model is <strong>confident and wrong</strong>, due to the logarithm, i.e., very low probability or real for a real image and very high probability of real for a fake image</p></li>
<li><p>the sigmoid activation ensures that the output, <span class="math notranslate nohighlight">\(\hat{y}\)</span> is a valid probability</p></li>
</ul>
&#13;

<h2>Discriminator Loss Derivative</h2>
<p>To perform backpropagation we need to calculate the loss derivative. Let’s do this for the input of the activation function as our output node, <span class="math notranslate nohighlight">\(D_8\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dz}
\]</div>
<ul class="simple">
<li><p>define <span class="math notranslate nohighlight">\(z\)</span> as the input for the sigmoid activation as output node, <span class="math notranslate nohighlight">\(D_8\)</span>.</p></li>
<li><p>as you see we do this because it results in a very simple, efficient result.</p></li>
<li><p>recall, the sigmoid function,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}
\]</div>
<p>We will use the chain rule, so we only need to solve the parts,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dz} = \frac{d\mathcal{L}}{d\hat{y}} \cdot \frac{d\hat{y}}{dz}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\frac{d\mathcal{L}}{d\hat{y}}\)</span> - partial derivative of binary cross-entropy loss given the discriminator output <span class="math notranslate nohighlight">\(\hat{y}\)</span> (<span class="math notranslate nohighlight">\(D_8\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{d\hat{y}}{dz}\)</span> - partial derivative of the discriminator output <span class="math notranslate nohighlight">\(\hat{y}\)</span> (<span class="math notranslate nohighlight">\(D_8\)</span>) given the sigmoid activation input</p></li>
</ul>
<p>Now we can solve the first part, partial derivative of loss with respect to the discriminator output, <span class="math notranslate nohighlight">\(\hat{y}\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{d\hat{y}} = -\left( \frac{y}{\hat{y}} - \frac{1 - y}{1 - \hat{y}} \right)
\]</div>
<p>now we can solve the second part, the partial derivative of the discriminator output <span class="math notranslate nohighlight">\(\hat{y}\)</span> (<span class="math notranslate nohighlight">\(D_8\)</span>) given the sigmoid activation input, it is just the sigmoid derivative,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{dz} = \hat{y}(1 - \hat{y})
\]</div>
<p>and we can combine these by the chain rule as,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dz} = -\left( \frac{y}{\hat{y}} - \frac{1 - y}{1 - \hat{y}} \right) \cdot \hat{y}(1 - \hat{y})
\]</div>
<p>We are almost there, we only need to simplify the result, first we distribute, <span class="math notranslate nohighlight">\(\hat{y}(1 - \hat{y})\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dz} = -\left[ y(1 - \hat{y}) - (1 - y)\hat{y} \right]
\]</div>
<p>and then simplify it further,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dz} = -\left[ y - y\hat{y} - \hat{y} + y\hat{y} \right]
= -\left[ y - \hat{y} \right]
= \hat{y} - y
\]</div>
<p>I said this would get simple! Our partial derivative of our loss with respect to the input to the output node sigmoid activation function, <span class="math notranslate nohighlight">\(z\)</span>, is,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dz} = \hat{y} - y
\]</div>
<p>This result shows the gradient is just the <strong>error</strong> — the difference between predicted and true values.</p>
<p>Now we can make this simple interpretation,</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(\hat{y} &gt; y\)</span>, the model overestimates <span class="math notranslate nohighlight">\(\rightarrow\)</span> gradient is positive <span class="math notranslate nohighlight">\(\rightarrow\)</span> lower prediction by moving in the negative gradient</p></li>
<li><p>if <span class="math notranslate nohighlight">\(\hat{y} &lt; y\)</span>, the model underestimates <span class="math notranslate nohighlight">\(\rightarrow\)</span> gradient is negative <span class="math notranslate nohighlight">\(\rightarrow\)</span> increase prediction by moving in the negative gradient</p></li>
</ul>
<p>I know that a title this section “Discriminator Loss Derivative”, but excuse me for performing just a little bit of backpropagation (to before sigmoid activation).</p>
<ul class="simple">
<li><p>next we carry on with back propagation to the discriminator weights and biases</p></li>
</ul>
&#13;

<h2>Discriminator Back Propagation</h2>
<p>For compact notation, let’s use matrix notation and define the input to the <span class="math notranslate nohighlight">\(D_8\)</span> activation, <span class="math notranslate nohighlight">\(z\)</span>, as,</p>
<div class="math notranslate nohighlight">
\[
z = \mathbf{w}^\top \mathbf{x} + c \quad \Rightarrow \quad \frac{dz}{d\mathbf{w}} = \mathbf{x}
\]</div>
<p>Now we can extend our use of the chain rule to,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{d\mathbf{w}} = \frac{d\mathcal{L}}{dz} \cdot \frac{dz}{d\mathbf{w}} = (\hat{y} - y) \cdot \mathbf{x}
\]</div>
<p>So for each of our discriminator weights, <span class="math notranslate nohighlight">\(\lambda_{5,8}\)</span>, <span class="math notranslate nohighlight">\(\lambda_{6,8}\)</span>, and <span class="math notranslate nohighlight">\(\lambda_{7,8}\)</span> we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{d\lambda_{5,8}} = (\hat{y} - y) \cdot I_5
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{d\lambda_{6,8}} = (\hat{y} - y) \cdot I_6
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{d\lambda_{7,8}} = (\hat{y} - y) \cdot I_7
\]</div>
<p>and for the bias, <span class="math notranslate nohighlight">\(c\)</span>, we calculate the next component for the chain rule as,</p>
<div class="math notranslate nohighlight">
\[
\frac{dz}{dc} = 1
\]</div>
<p>so we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dc} = \frac{d\mathcal{L}}{dz} \cdot \frac{dz}{dc} = (\hat{y} - y) \cdot 1 = \hat{y} - y
\]</div>
<p>The backpropagation for our very simple discriminator is quite simple, we can summarize for the weights,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{d\mathbf{w}} = (\hat{y} - y) \cdot \mathbf{x}
\]</div>
<p>and for the bias,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}}{dc} = \hat{y} - y
\]</div>
<p>Let’s write these out for all of our discriminators parameters,
$<span class="math notranslate nohighlight">\(
\begin{aligned}
\frac{d\mathcal{L}}{d\lambda_{5,8}} &amp;= (\hat{y} - y) \cdot I_5 \\
\frac{d\mathcal{L}}{d\lambda_{6,8}} &amp;= (\hat{y} - y) \cdot I_6 \\
\frac{d\mathcal{L}}{d\lambda_{7,8}} &amp;= (\hat{y} - y) \cdot I_7 \\
\frac{d\mathcal{L}}{dc} &amp;= \hat{y} - y
\end{aligned}
\)</span>$</p>
&#13;

<h2>Generator Loss Derivative and Back Propagation Through the Discriminator</h2>
<p>Recall that the goal of the generator is to make fake images the discriminator assigns as a high probability of a real image, i.e., to fool the discriminator</p>
<ul class="simple">
<li><p>the generator produces a fake image,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\tilde{\mathbf{x}} = G(z) 
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> where ( z ) is a latent vector (e.g., sampled from Uniform[0.4, 1]).</p>
<ul class="simple">
<li><p>the discriminator evaluates this fake sample and returns:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y} = D(\tilde{\mathbf{x}}) \in (0, 1)
\]</div>
<p>Now we can calculate the binary cross-entropy for the generator as,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_G = -\log(\hat{y})
\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{y} = D(G(z))\)</span>, the discriminator’s evaluation of the generator’s fake image, <span class="math notranslate nohighlight">\(z\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}\)</span> is the probability assigned by the discriminator to the fake being real</p></li>
</ul>
<p>This is equivalent to cross-entropy with <strong>target label <span class="math notranslate nohighlight">\(y = 1\)</span></strong>, note here we do a trick, from the generator’s perspective it’s images are real, so we are using <span class="math notranslate nohighlight">\(y=1\)</span>, i.e., real images for the fake images!</p>
<p>You may get confused if you look at the original GAN loss above, this is called the non-saturating generator loss.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Loss Type</p></th>
<th class="head"><p>Expression</p></th>
<th class="head"><p>Comment</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Original GAN</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{E}[\log(1 - D(G(z)))]\)</span></p></td>
<td><p>Theoretical, can cause vanishing gradients</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Non-saturating</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(-\mathbb{E}[\log(D(G(z)))]\)</span></p></td>
<td><p>Practical, stronger gradients, commonly used</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>so instead of minimizing the original generator loss we are maximizing the non-saturating generator loss.</p></li>
</ul>
<p>Let’s show how to back propagate through the entire discriminator with the chain rule.</p>
<ul class="simple">
<li><p>we want the generator loss gradient with respect generator output,</p></li>
</ul>
<p>Given <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}} = G(z)\)</span>, our fake image, we want the partial derivative of the loss given our fake image,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\tilde{\mathbf{x}}}
\]</div>
<p>and by the chain rule,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\tilde{\mathbf{x}}} = \frac{d\mathcal{L}_G}{d\hat{y}} \cdot \frac{d\hat{y}}{d\tilde{\mathbf{x}}}
\]</div>
<p>This is how the discriminator’s belief <span class="math notranslate nohighlight">\(\hat{y}\)</span>​ about “fakeness” changes with changes in <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}\)</span> the fake image.</p>
<p>Now we are ready to back propagate the generator loss through the discriminator, let’s start with our generator loss (from above),</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_G = -\log(\hat{y})
\]</div>
<p>and when we perform the partial derivative,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\hat{y}} = -\frac{1}{\hat{y}}
\]</div>
<p>Now, recall the discriminator’s forward pass is,</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = \sigma(\mathbf{w}^\top \tilde{\mathbf{x}} + c)
\]</div>
<p>so we can calculate the partial derivative of the discriminator’s output with respect to the generator’s fake image as,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{d\tilde{\mathbf{x}}} = \hat{y}(1 - \hat{y}) \cdot \mathbf{w}
\]</div>
<p>now we combine these with the chain rule as,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\tilde{\mathbf{x}}}
= \left( -\frac{1}{\hat{y}} \right) \cdot \left( \hat{y}(1 - \hat{y}) \cdot \mathbf{w} \right)
= -(1 - \hat{y}) \cdot \mathbf{w}
\]</div>
<p>The gradient of the generator’s loss with respect to the output image <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}\)</span> is,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\tilde{\mathbf{x}}} = -(1 - \hat{y}) \cdot \mathbf{w}
\]</div>
<p>We can add some interpretations of this result,</p>
<ul class="simple">
<li><p>when <span class="math notranslate nohighlight">\(\hat{y}\)</span> is close to 0 <span class="math notranslate nohighlight">\(\rightarrow\)</span> discriminator easily spots fake <span class="math notranslate nohighlight">\(\rightarrow\)</span> large gradient <span class="math notranslate nohighlight">\(\rightarrow\)</span> generator updates more.</p></li>
<li><p>when <span class="math notranslate nohighlight">\(\hat{y}\)</span> is close to 1 <span class="math notranslate nohighlight">\(\rightarrow\)</span> generator is fooling the discriminator <span class="math notranslate nohighlight">\(\rightarrow\)</span> gradient is small.</p></li>
</ul>
<p>This guides the generator to tweak its output to increase <span class="math notranslate nohighlight">\(\hat{y}\)</span> — i.e., fool the discriminator.</p>
<p>To further clarify, for our example let’s compute how the discriminator’s output <span class="math notranslate nohighlight">\(\hat{y}\)</span> changes with respect to the generator outputs <span class="math notranslate nohighlight">\(O_5, O_6, O_7\)</span>, instead of the <span class="math notranslate nohighlight">\(w\)</span> vector notation used above.</p>
<ul class="simple">
<li><p>if we apply the chain rule we get,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{dO_i} = \frac{d\hat{y}}{dz} \cdot \frac{dz}{dO_i}
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> for each of the components we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{dz} = \hat{y}(1 - \hat{y})
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{dz}{dO_5} = \lambda_{5,8}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{dz}{dO_6} = \lambda_{6,8}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{dz}{dO_7} = \lambda_{7,8}
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> substituting in the chain rule we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{dO_5} = \hat{y}(1 - \hat{y}) \cdot \lambda_{5,8}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{dO_6} = \hat{y}(1 - \hat{y}) \cdot \lambda_{6,8}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d\hat{y}}{dO_7} = \hat{y}(1 - \hat{y}) \cdot \lambda_{7,8}
\]</div>
&#13;

<h2>Backpropagation Through Generator to Weights and Bias</h2>
<p>We now propagate through the generators sigmoid activation in each of the output nodes,</p>
<div class="math notranslate nohighlight">
\[
\frac{dO_i}{dz_i} = O_i (1 - O_i)
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(O_i = \sigma(z_i)\)</span>, where <span class="math notranslate nohighlight">\(z_i\)</span> is the input for the output nodes, pre-activation, and <span class="math notranslate nohighlight">\(O_i\)</span> is output for the output nodes, post-activation</p></li>
</ul>
<p>We Apply chain rule,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{dz_i} = \frac{d\mathcal{L}_G}{dO_i} \cdot \frac{dO_i}{dz_i}
= \frac{d\mathcal{L}_G}{dO_i} \cdot O_i (1 - O_i)
\]</div>
<p>Recall,</p>
<div class="math notranslate nohighlight">
\[
z_i = \lambda_{1,i} \cdot L_1 + b
\]</div>
<p>so we can calculate the generator’s weights partial derivatives as,</p>
<div class="math notranslate nohighlight">
\[
\frac{dz_i}{d\lambda_{1,i}} = L_1
\]</div>
<p>and the generator’s bias partial derivative as,</p>
<div class="math notranslate nohighlight">
\[
\frac{dz_i}{db} = 1
\]</div>
<p>Now we can put this all together with the chain rule, the partial derivatives of the generator loss with respect to the generator weights are,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\lambda_{1,i}} =
\frac{d\mathcal{L}_G}{dz_i} \cdot \frac{dz_i}{d\lambda_{1,i}} =
\left( \frac{d\mathcal{L}_G}{dO_i} \cdot O_i (1 - O_i) \right) \cdot L_1
\]</div>
<p>and the partial derivative of the generator loss with respect to the generator bias is,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{db} =
\sum_{i=5}^7 \frac{d\mathcal{L}_G}{dz_i} \cdot \frac{dz_i}{db} =
\sum_{i=5}^7 \left( \frac{d\mathcal{L}_G}{dO_i} \cdot O_i (1 - O_i) \right)
\]</div>
<p>For clarity, let’s write this out for each of our generator’s weights,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\lambda_{1,2}} =
-(1 - \hat{y}) \cdot \lambda_{5,8} \cdot O_5 (1 - O_5) \cdot L_1
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\lambda_{1,3}} =
-(1 - \hat{y}) \cdot \lambda_{6,8} \cdot O_6 (1 - O_6) \cdot L_1
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{d\lambda_{1,4}} =
-(1 - \hat{y}) \cdot \lambda_{7,8} \cdot O_7 (1 - O_7) \cdot L_1
\]</div>
<p>and for our generator’s bias,</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathcal{L}_G}{db} =
-(1 - \hat{y}) \cdot \left[
\lambda_{5,8} \cdot O_5(1 - O_5)
+ \lambda_{6,8} \cdot O_6(1 - O_6)
+ \lambda_{7,8} \cdot O_7(1 - O_7)
\right]
\]</div>
<p>Let’s make some interpretations,</p>
<ul class="simple">
<li><p>the generator’s weights and bias gradients scale with how much the discriminator is fooled (<span class="math notranslate nohighlight">\(1 - \hat{y}\)</span>)</p></li>
<li><p>the generator learns to tweak <span class="math notranslate nohighlight">\(\lambda_{1,i}\)</span> and <span class="math notranslate nohighlight">\(b\)</span> to push the fake images, <span class="math notranslate nohighlight">\(O_5\)</span>, <span class="math notranslate nohighlight">\(O_6\)</span> and <span class="math notranslate nohighlight">\(O_7\)</span> in directions that increase <span class="math notranslate nohighlight">\(\hat{y}\)</span></p></li>
<li><p>this flow of error gives the generator a signal to <strong>fool the discriminator more effectively</strong> without ever seeing a real image!</p></li>
</ul>
&#13;

<h2>Simple GAN Training Workflow</h2>
<p>We start with initialization of the generator and discriminator weights and bias and setting the training hyperparameters.</p>
<ol class="arabic simple">
<li><p>Generate the Synthethic, “Real Images” for training</p></li>
</ol>
<ul class="simple">
<li><p>sample <span class="math notranslate nohighlight">\(N\)</span> real 3-node, 1D images <span class="math notranslate nohighlight">\(\mathbf{I} = \{(I_{5,i}, I_{6,i}, I_{7,i})\}_{i=1}^N\)</span></p></li>
<li><p>use the synthetic training data function:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Real images} \sim \text{linear decreasing trend} + \text{noise}
\]</div>
<ol class="arabic simple" start="2">
<li><p><strong>Initialize generator weights and bias</strong> - the weights,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\{\lambda_{1,2}, \lambda_{1,3}, \lambda_{1,4}, b\} \leftarrow \text{small random values}
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> and the bias,</p>
<div class="math notranslate nohighlight">
\[
b \leftarrow 0.0
\]</div>
<ol class="arabic simple" start="3">
<li><p><strong>Initialize discriminator weights and bias</strong> - the weights,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\{\lambda_{5,8}, \lambda_{6,8}, \lambda_{7,8}, c\} \leftarrow \text{small random values}
\]</div>
<p><span class="math notranslate nohighlight">\(\quad\)</span> and the bias,</p>
<div class="math notranslate nohighlight">
\[
c \leftarrow 0.0
\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Set model training hyperparameters</strong> - this includes,</p></li>
</ol>
<ul class="simple">
<li><p>Learning Rates - for the generator, <span class="math notranslate nohighlight">\(\eta_G\)</span>, and discriminator, <span class="math notranslate nohighlight">\(\eta_D\)</span></p></li>
<li><p>Batch Size - in this example we are assuming batch size equal to the number of real images</p></li>
<li><p>Epochs - number of training iterations</p></li>
</ul>
<ol class="arabic simple" start="5">
<li><p><strong>Train the discriminator</strong></p></li>
</ol>
<ul class="simple">
<li><p>combine real and fake inputs into a batch of size <span class="math notranslate nohighlight">\(2N\)</span> and inlcude labels <span class="math notranslate nohighlight">\(y_i = 1\)</span> for real, <span class="math notranslate nohighlight">\(y_i = 0\)</span> for fake</p></li>
<li><p>compute discriminator outputs <span class="math notranslate nohighlight">\(\hat{y}_i = D(I_{5,i}, I_{6,i}, I_{7,i})\)</span></p></li>
<li><p>calculate discriminator loss and gradients using:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \lambda_{j,8}}, \quad \frac{\partial \mathcal{L}}{\partial c}
\]</div>
<ul class="simple">
<li><p>update discriminator weights and bias:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lambda_{j,8} \leftarrow \lambda_{j,8} - \eta_D \times \frac{\partial \mathcal{L}}{\partial \lambda_{j,8}}, \quad
  c \leftarrow c - \eta_D \times \frac{\partial \mathcal{L}}{\partial c}
\]</div>
<ol class="arabic simple" start="5">
<li><p><strong>Train the generator</strong></p></li>
</ol>
<ul class="simple">
<li><p>Compute generator output fake images and pass to the discriminator to evaluate the outputs on these fakes, <span class="math notranslate nohighlight">\(D_8\)</span> same as <span class="math notranslate nohighlight">\(y\)</span></p></li>
<li><p>Calculate generator loss gradients using,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}_G}{\partial \lambda_{1,j}}, \quad \frac{\partial \mathcal{L}_G}{\partial b}
\]</div>
<ul class="simple">
<li><p>Update generator weights and bias,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lambda_{1,j} \leftarrow \lambda_{1,j} - \eta_G \times \frac{\partial \mathcal{L}_G}{\partial \lambda_{1,j}}, \quad
  b \leftarrow b - \eta_G \times \frac{\partial \mathcal{L}_G}{\partial b}
\]</div>
<ol class="arabic simple" start="6">
<li><p><strong>Repeat Until Convergence</strong> - or stop criteria is met, such as maximum number of training epochs, return to step 5.</p></li>
</ol>
<p>Here a summary of the training loop,</p>
<ol class="arabic simple">
<li><p>Generate real data batch</p></li>
<li><p>Generate fake data batch</p></li>
<li><p>Update discriminator to better distinguish real/fake</p></li>
<li><p>Update generator to fool discriminator</p></li>
<li><p>Repeat</p></li>
</ol>
<p>This adversarial training loop lets the generator learn to create data mimicking the real distribution, and the discriminator improve in spotting fakes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">train_gan</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">lr_g</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">lr_d</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span> <span class="c1"># function for training the GAN</span>
    <span class="n">weights_epoch_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">weights_g_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Initialize weights</span>
    <span class="n">weights_g</span> <span class="o">=</span> <span class="n">initialize_generator_weights</span><span class="p">()</span>
    <span class="n">weights_d</span> <span class="o">=</span> <span class="n">initialize_discriminator_weights</span><span class="p">()</span>
    
    <span class="c1"># Tracking losses</span>
    <span class="n">generator_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">discriminator_losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">real_images</span> <span class="o">=</span> <span class="n">generate_real_data</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="c1"># one set of images</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># Step 1: Generate real data</span>

        <span class="n">I5_real</span><span class="p">,</span> <span class="n">I6_real</span><span class="p">,</span> <span class="n">I7_real</span> <span class="o">=</span> <span class="n">real_images</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">real_images</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">real_images</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">y_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># Step 2: Generate fake data</span>
        <span class="n">L1_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">fake_images</span> <span class="o">=</span> <span class="n">generator_forward</span><span class="p">(</span><span class="n">L1_fake</span><span class="p">,</span> <span class="n">weights_g</span><span class="p">,</span><span class="n">return_pre_activation</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">I5_fake</span><span class="p">,</span> <span class="n">I6_fake</span><span class="p">,</span> <span class="n">I7_fake</span> <span class="o">=</span> <span class="n">fake_images</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">fake_images</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">fake_images</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">y_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># Combine for discriminator training</span>
        <span class="n">I5_combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">I5_real</span><span class="p">,</span> <span class="n">I5_fake</span><span class="p">])</span>
        <span class="n">I6_combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">I6_real</span><span class="p">,</span> <span class="n">I6_fake</span><span class="p">])</span>
        <span class="n">I7_combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">I7_real</span><span class="p">,</span> <span class="n">I7_fake</span><span class="p">])</span>
        <span class="n">y_combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">y_real</span><span class="p">,</span> <span class="n">y_fake</span><span class="p">])</span>

        <span class="c1"># Step 3: Train discriminator</span>
        <span class="n">grads_d</span> <span class="o">=</span> <span class="n">discriminator_gradients</span><span class="p">(</span><span class="n">I5_combined</span><span class="p">,</span> <span class="n">I6_combined</span><span class="p">,</span> <span class="n">I7_combined</span><span class="p">,</span> <span class="n">y_combined</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">weights_d</span><span class="p">:</span>
            <span class="n">weights_d</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">lr_d</span> <span class="o">*</span> <span class="n">grads_d</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

        <span class="c1"># Step 4: Train generator</span>
        <span class="n">L1_gen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">grads_g</span> <span class="o">=</span> <span class="n">generator_gradients</span><span class="p">(</span><span class="n">L1_gen</span><span class="p">,</span> <span class="n">weights_g</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">weights_g</span><span class="p">:</span>
            <span class="n">weights_g</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">lr_g</span> <span class="o">*</span> <span class="n">grads_g</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="c1"># if epoch in [1000,2500,5000]: # save the weights to visualize model improvement over epochs</span>
            <span class="c1"># weights_g_list.append(weights_g)</span>
        
        <span class="c1"># Step 5: Calculate and store losses</span>
        <span class="n">y_pred_real</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">discriminator_forward</span><span class="p">(</span><span class="n">I5_real</span><span class="p">,</span> <span class="n">I6_real</span><span class="p">,</span> <span class="n">I7_real</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">)</span>
        <span class="n">y_pred_fake</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">discriminator_forward</span><span class="p">(</span><span class="n">I5_fake</span><span class="p">,</span> <span class="n">I6_fake</span><span class="p">,</span> <span class="n">I7_fake</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">)</span>
        <span class="n">loss_d_real</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred_real</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span>
        <span class="n">loss_d_fake</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred_fake</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span>
        <span class="n">loss_d</span> <span class="o">=</span> <span class="n">loss_d_real</span> <span class="o">+</span> <span class="n">loss_d_fake</span>
        <span class="n">discriminator_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_d</span><span class="p">)</span>

        <span class="n">y_pred_gen</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">discriminator_forward</span><span class="p">(</span><span class="o">*</span><span class="n">generator_forward</span><span class="p">(</span><span class="n">L1_gen</span><span class="p">,</span> <span class="n">weights_g</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">)</span>
        <span class="n">loss_g</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred_gen</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span>
        <span class="n">generator_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_g</span><span class="p">)</span>

        <span class="c1"># Print progress</span>
        <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">%</span> <span class="mi">5000</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="n">epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">weights_epoch_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
            <span class="n">weights_g_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">weights_g</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">: D_loss = </span><span class="si">{</span><span class="n">loss_d</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, G_loss = </span><span class="si">{</span><span class="n">loss_g</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="c1"># # Final output</span>
    <span class="c1"># print("\nTraining complete.\nFinal Generator Weights:")</span>
    <span class="c1"># for k, v in weights_g.items():</span>
    <span class="c1">#     print(f"  {k}: {v:.4f}")</span>
    
    <span class="c1"># print("\nFinal Discriminator Weights:")</span>
    <span class="c1"># for k, v in weights_d.items():</span>
    <span class="c1">#     print(f"  {k}: {v:.4f}")</span>

    <span class="k">return</span> <span class="n">weights_g</span><span class="p">,</span> <span class="n">weights_d</span><span class="p">,</span> <span class="n">generator_losses</span><span class="p">,</span> <span class="n">discriminator_losses</span><span class="p">,</span> <span class="n">real_images</span><span class="p">,</span> <span class="n">weights_g_list</span><span class="p">,</span> <span class="n">weights_epoch_list</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">final_weights_g</span><span class="p">,</span> <span class="n">final_weights_d</span><span class="p">,</span> <span class="n">loss_g</span><span class="p">,</span> <span class="n">loss_d</span><span class="p">,</span> <span class="n">real_images</span><span class="p">,</span> <span class="n">weights_g_list</span><span class="p">,</span> <span class="n">weights_epoch_list</span> <span class="o">=</span> <span class="n">train_gan</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">lr_g</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">lr_d</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">loss_g</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Generator'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">loss_d</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Discriminator'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Loss'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Epoch'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 0: D_loss = 1.3433, G_loss = 0.6666
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 5000: D_loss = 1.0994, G_loss = 0.7903
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 10000: D_loss = 1.0035, G_loss = 0.8478
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 15000: D_loss = 0.9899, G_loss = 0.8271
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 20000: D_loss = 1.0441, G_loss = 0.8124
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 25000: D_loss = 1.0621, G_loss = 0.8129
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 30000: D_loss = 1.1218, G_loss = 0.8277
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 35000: D_loss = 1.1224, G_loss = 0.8411
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 40000: D_loss = 1.1631, G_loss = 0.7914
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 45000: D_loss = 1.1779, G_loss = 0.7685
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 49999: D_loss = 1.1607, G_loss = 0.7617
</pre></div>
</div>
<img alt="_images/0e5b9d59df3b7c13cf96ada6a8409c005cf108794aa5c6d5da5188ad5ef20ded.png" src="../Images/686c89ab95fc15d0556a9ed4d7f3bed6.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/0e5b9d59df3b7c13cf96ada6a8409c005cf108794aa5c6d5da5188ad5ef20ded.png"/>
</div>
</div>
&#13;

<h2>Visualize Real Images and Trained Generator Fake Images</h2>
<p>Let’s check a set of fake images from our trained generator against the real images.</p>
<ul class="simple">
<li><p>recall the generator never saw these images, the discriminator saw the real and fake images and told the generator how good or bad were the generator’s fake images.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">L1_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span> 
<span class="n">trained_fake</span> <span class="o">=</span> <span class="n">generator_forward</span><span class="p">(</span><span class="n">L1_test</span><span class="p">,</span> <span class="n">weights_g_list</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">untrained_fake</span> <span class="o">=</span> <span class="n">generator_forward</span><span class="p">(</span><span class="n">L1_test</span><span class="p">,</span> <span class="n">weights_g_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">real_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">real_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Real Images'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Value'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Node'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Pixel'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.2</span><span class="p">,</span><span class="mf">0.8</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trained_fake</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">trained_fake</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Trained Generator Fake Images'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Value'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Node'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="sa">r</span><span class="s1">'$O_2$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$O_3$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$O_4$'</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.2</span><span class="p">,</span><span class="mf">0.8</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/765558367eae336a177e7a0b9b0a86037b878b20114ad05070fc072fd5f13404.png" src="../Images/5e411f7e4a786c08b02c80dadc431416.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/765558367eae336a177e7a0b9b0a86037b878b20114ad05070fc072fd5f13404.png"/>
</div>
</div>
&#13;

<h2>Visualize Real Images and Generator Fake Images Over Training Epochs</h2>
<p>It is interesting to see how our generator’s fake images evolve over the training epochs.</p>
<ul class="simple">
<li><p>as first the fake images are random due to the random initialization of the generator’s weights and bias</p></li>
<li><p>as the training proceeds the generator learns to improve the fake images.</p></li>
</ul>
<p>I include the real images at the end for comparison.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">weights_g</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">weights_g_list</span><span class="p">):</span>
    <span class="n">fake</span> <span class="o">=</span> <span class="n">generator_forward</span><span class="p">(</span><span class="n">L1_test</span><span class="p">,</span> <span class="n">weights_g_list</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fake</span><span class="p">[</span><span class="n">j</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fake</span><span class="p">[</span><span class="n">j</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Fake Images: Training Epoch '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">weights_epoch_list</span><span class="p">[</span><span class="n">i</span><span class="p">]));</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Value'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Output Node'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="sa">r</span><span class="s1">'$O_2$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$O_3$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$O_4$'</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.2</span><span class="p">,</span><span class="mf">0.8</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">12</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">real_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">real_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Real Images'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Value'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Pixel'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.2</span><span class="p">,</span><span class="mf">0.8</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">3.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c9fb21bf43af473100eacd88fe058bc353b01c81bec63e5cf3719df9229a386d.png" src="../Images/ad9bdf6d9a437102f9bba86e2f345ae0.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/c9fb21bf43af473100eacd88fe058bc353b01c81bec63e5cf3719df9229a386d.png"/>
</div>
</div>
&#13;

<h2>Comments</h2>
<p>This was a basic treatment of generative adversarial networks. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a> and the YouTube lecture links at the start of this chapter with resource links in the videos’ descriptions.</p>
<p>I hope this is helpful,</p>
<p><em>Michael</em></p>
&#13;

<h2>About the Author</h2>
<figure style="text-align: center;">
  <img src="../Images/eb709b2c0a0c715da01ae0165efdf3b2.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/intro/michael_pyrcz_officeshot_jacket.jpg"/>
  <figcaption style="text-align: center;"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.
</figcaption>
</figure>
<p>Michael Pyrcz is a professor in the <a class="reference external" href="https://cockrell.utexas.edu/faculty-directory/alphabetical/p">Cockrell School of Engineering</a>, and the <a class="reference external" href="https://www.jsg.utexas.edu/researcher/michael_pyrcz/">Jackson School of Geosciences</a>, at <a class="reference external" href="https://www.utexas.edu/">The University of Texas at Austin</a>, where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,</p>
<ul class="simple">
<li><p>the principal investigator of the <a class="reference external" href="https://fri.cns.utexas.edu/energy-analytics">Energy Analytics</a> freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin</p></li>
<li><p>an associate editor for <a class="reference external" href="https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board">Computers and Geosciences</a>, and a board member for <a class="reference external" href="https://link.springer.com/journal/11004/editorial-board">Mathematical Geosciences</a>, the International Association for Mathematical Geosciences.</p></li>
</ul>
<p>Michael has written over 70 <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en">peer-reviewed publications</a>, a <a class="reference external" href="https://pypi.org/project/geostatspy/">Python package</a> for spatial data analytics, co-authored a textbook on spatial data analytics, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a> and author of two recently released e-books, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy</a> and <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html">Applied Machine Learning in Python: a Hands-on Guide with Code</a>.</p>
<p>All of Michael’s university lectures are available on his <a class="reference external" href="https://www.youtube.com/@GeostatsGuyLectures">YouTube Channel</a> with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his <a class="reference external" href="https://github.com/GeostatsGuy">GitHub account</a>, to support any interested students and working professionals with evergreen content. To find out more about Michael’s work and shared educational resources visit his <span class="xref myst">Website</span>.</p>
&#13;

<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I’d be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>I’m always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
    
</body>
</html>