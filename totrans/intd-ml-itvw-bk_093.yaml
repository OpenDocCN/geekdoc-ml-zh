- en: 8.1.2 Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/8.1.2-questions.html](https://huyenchip.com/ml-interviews-book/contents/8.1.2-questions.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[E] What are the basic assumptions to be made for linear regression?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What happens if we don’t apply feature scaling to logistic regression?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What are the algorithms you’d use when developing the prototype of a fraud
    detection model?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature selection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Why do we use feature selection?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] What are some of the algorithms for feature selection? Pros and cons of
    each.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: k-means clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] How would you choose the value of k?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] If the labels are known, how would you evaluate the performance of your
    k-means clustering algorithm?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] How would you do it if the labels aren’t known?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H] Given the following dataset, can you predict how K-means clustering works
    on it? Explain.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![k-means clustering](../Images/8ca03d741d2431482b90658ce75bae5b.png "image_tooltip")'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: k-nearest neighbor classification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] How would you choose the value of k?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What happens when you increase or decrease the value of k?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] How does the value of k impact the bias and variance?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: k-means and GMM are both powerful clustering algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Compare the two.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] When would you choose one over another?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Hint**: Here’s an example of how K-means and GMM algorithms perform on the
    artificial mouse dataset.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![k-means clustering vs. gaussian mixture model](../Images/defccceebaa2c20384ab102a0cb3cfbc.png
    "image_tooltip")'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Image from [Mohamad Ghassany’s course on Machine Learning](https://www.mghassany.com/MLcourse/gaussian-mixture-models-em.html)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Bagging and boosting are two popular ensembling methods. Random forest is a
    bagging example while XGBoost is a boosting example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] What are some of the fundamental differences between bagging and boosting
    algorithms?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] How are they used in deep learning?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Given this directed graph.![Adjacency matrix](../Images/7a269af62de8405ddc2ba8f041e3d690.png
    "image_tooltip")
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Construct its adjacency matrix.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] How would this matrix change if the graph is now undirected?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] What can you say about the adjacency matrices of two isomorphic graphs?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Imagine we build a user-item collaborative filtering system to recommend to
    each user items similar to the items they’ve bought before.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] You can build either a user-item matrix or an item-item matrix. What are
    the pros and cons of each approach?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] How would you handle a new user who hasn’t made any purchases in the past?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Is feature scaling necessary for kernel methods?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Naive Bayes classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] How is Naive Bayes classifier naive?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Let’s try to construct a Naive Bayes classifier to classify whether a tweet
    has a positive or negative sentiment. We have four training samples:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **Tweet** | **Label** |'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
- en: '| This makes me so upset | Negative |'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
- en: '| This puppy makes me happy | Positive |'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
- en: '| Look at this happy hamster | Positive |'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
- en: '| No hamsters allowed in my house | Negative |'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
- en: According to your classifier, what's sentiment of the sentence `The hamster
    is upset with the puppy`?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Two popular algorithms for winning Kaggle solutions are Light GBM and XGBoost.
    They are both gradient boosting algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What is gradient boosting?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] What problems is gradient boosting good for?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: SVM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What’s linear separation? Why is it desirable when we use SVM?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] How well would vanilla SVM work on this dataset?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Adjacency matrix](../Images/28ebf3793aeb63ffb66fbd2ea3b9166b.png "image_tooltip")'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: '[M] How well would vanilla SVM work on this dataset?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Adjacency matrix](../Images/7e618f095e074e364a89f262b97d5855.png "image_tooltip")'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: '[M] How well would vanilla SVM work on this dataset?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Adjacency matrix](../Images/22476012f7e6c58e01c258ca4feff488.png "image_tooltip")'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: '*This book was created by [Chip Huyen](https://huyenchip.com) with the help
    of wonderful friends. For feedback, errata, and suggestions, the author can be
    reached [here](https://huyenchip.com/communication/). Copyright ©2021 Chip Huyen.*'
  prefs: []
  type: TYPE_NORMAL
