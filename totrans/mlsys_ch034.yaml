- en: Object Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../media/file412.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*DALL·E 3 Prompt: Cartoon in the style of the 1940s or 1950s showcasing a spacious
    industrial warehouse interior. A conveyor belt is prominently featured, carrying
    a mixture of toy wheels and boxes. The wheels are distinguishable with their bright
    yellow centers and black tires. The boxes are white cubes painted with alternating
    black and white patterns. At the end of the moving conveyor stands a retro-styled
    robot, equipped with tools and sensors, diligently classifying and counting the
    arriving wheels and boxes. The overall aesthetic is reminiscent of mid-century
    animation with bold lines and a classic color palette.*'
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This continuation of Image Classification on Nicla Vision is now exploring **Object
    Detection**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file413.png)'
  prefs: []
  type: TYPE_IMG
- en: Object Detection versus Image Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main task with Image Classification models is to produce a list of the
    most probable object categories present on an image, for example, to identify
    a tabby cat just after his dinner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file414.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But what happens when the cat jumps near the wine glass? The model still only
    recognizes the predominant category on the image, the tabby cat:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file415.png)'
  prefs: []
  type: TYPE_IMG
- en: And what happens if there is not a dominant category on the image?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file416.png)'
  prefs: []
  type: TYPE_IMG
- en: The model identifies the above image utterly wrong as an “ashcan,” possibly
    due to the color tonalities.
  prefs: []
  type: TYPE_NORMAL
- en: The model used in all previous examples is MobileNet, which was trained with
    a large dataset, *ImageNet*.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To solve this issue, we need another type of model, where not only **multiple
    categories** (or labels) can be found but also **where** the objects are located
    on a given image.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can imagine, such models are much more complicated and bigger, for example,
    the **MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset.** This
    pre-trained object detection model is designed to locate up to 10 objects within
    an image, outputting a bounding box for each object detected. The below image
    is the result of such a model running on a Raspberry Pi:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file417.png)'
  prefs: []
  type: TYPE_IMG
- en: Those models used for object detection (such as the MobileNet SSD or YOLO) usually
    have several MB in size, which is OK for Raspberry Pi but unsuitable for use with
    embedded devices, where the RAM is usually lower than 1 Mbyte.
  prefs: []
  type: TYPE_NORMAL
- en: 'An innovative solution for Object Detection: FOMO'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Edge Impulse launched in 2022, **FOMO** (Faster Objects, More Objects)](https://docs.edgeimpulse.com/docs/edge-impulse-studio/learning-blocks/object-detection/fomo-object-detection-for-constrained-devices),
    a novel solution for performing object detection on embedded devices, not only
    on the Nicla Vision (Cortex M7) but also on Cortex M4F CPUs (Arduino Nano33 and
    OpenMV M4 series) and the Espressif ESP32 devices (ESP-CAM and XIAO ESP32S3 Sense).'
  prefs: []
  type: TYPE_NORMAL
- en: In this Hands-On lab, we will explore using FOMO with Object Detection, not
    entering many details about the model itself. To understand more about how the
    model works, you can go into the [official FOMO announcement](https://www.edgeimpulse.com/blog/announcing-fomo-faster-objects-more-objects)
    by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.
  prefs: []
  type: TYPE_NORMAL
- en: The Object Detection Project Goal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All Machine Learning projects need to start with a detailed goal. Let’s assume
    we are in an industrial facility and must sort and count **wheels** and special
    **boxes**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file418.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, we should perform a multi-label classification, where each
    image can have three classes:'
  prefs: []
  type: TYPE_NORMAL
- en: Background (No objects)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Box
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wheel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some not labeled image samples that we should use to detect the objects
    (wheels and boxes):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file419.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We are interested in which object is in the image, its location (centroid),
    and how many we can find on it. The object’s size is not detected with FOMO, as
    with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.
  prefs: []
  type: TYPE_NORMAL
- en: We will develop the project using the Nicla Vision for image capture and model
    inference. The ML project will be developed using the Edge Impulse Studio. But
    before starting the object detection project in the Studio, let’s create a *raw
    dataset* (not labeled) with images that contain the objects to be detected.
  prefs: []
  type: TYPE_NORMAL
- en: Data Collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For image capturing, we can use:'
  prefs: []
  type: TYPE_NORMAL
- en: Web Serial Camera tool,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge Impulse Studio,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenMV IDE,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A smartphone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, we will use the **OpenMV IDE**.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting Dataset with OpenMV IDE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we create a folder on the computer where the data will be saved, for
    example, “data.” Next, on the OpenMV IDE, we go to Tools > Dataset Editor and
    select New Dataset to start the dataset collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file420.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Edge impulse suggests that the objects should be similar in size and not overlap
    for better performance. This is OK in an industrial facility, where the camera
    should be fixed, keeping the same distance from the objects to be detected. Despite
    that, we will also try using mixed sizes and positions to see the result.
  prefs: []
  type: TYPE_NORMAL
- en: We will not create separate folders for our images because each contains multiple
    labels.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Connect the Nicla Vision to the OpenMV IDE and run the `dataset_capture_script.py`.
    Clicking on the Capture Image button will start capturing images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file421.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We suggest using around 50 images to mix the objects and vary the number of
    each appearing on the scene. Try to capture different angles, backgrounds, and
    light conditions.
  prefs: []
  type: TYPE_NORMAL
- en: The stored images use a QVGA frame size <semantics><mrow><mn>320</mn><mo>×</mo><mn>240</mn></mrow><annotation
    encoding="application/x-tex">320\times 240</annotation></semantics> and RGB565
    (color pixel format).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: After capturing your dataset, close the Dataset Editor Tool on the `Tools >
    Dataset Editor`.
  prefs: []
  type: TYPE_NORMAL
- en: Edge Impulse Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setup the project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Go to [Edge Impulse Studio,](https://www.edgeimpulse.com/) enter your credentials
    at **Login** (or create an account), and start a new project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you can clone the project developed for this hands-on: [NICLA_Vision_Object_Detection](https://studio.edgeimpulse.com/public/292737/latest).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: On the Project `Dashboard`, go to **Project info** and select **Bounding boxes
    (object detection),** and at the right-top of the page, select `Target`, **Arduino
    Nicla Vision (Cortex-M7)**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file422.png)'
  prefs: []
  type: TYPE_IMG
- en: Uploading the unlabeled data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On Studio, go to the `Data acquisition` tab, and on the `UPLOAD DATA` section,
    upload from your computer files captured.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file423.png)'
  prefs: []
  type: TYPE_IMG
- en: You can leave for the Studio to split your data automatically between Train
    and Test or do it manually.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../media/file424.png)'
  prefs: []
  type: TYPE_IMG
- en: All the unlabeled images (51) were uploaded, but they still need to be labeled
    appropriately before being used as a dataset in the project. The Studio has a
    tool for that purpose, which you can find in the link `Labeling queue (51)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways you can use to perform AI-assisted labeling on the Edge
    Impulse Studio (free version):'
  prefs: []
  type: TYPE_NORMAL
- en: Using yolov5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking objects between frames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge Impulse launched an [auto-labeling feature](https://docs.edgeimpulse.com/docs/edge-impulse-studio/data-acquisition/auto-labeler)
    for Enterprise customers, easing labeling tasks in object detection projects.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ordinary objects can quickly be identified and labeled using an existing library
    of pre-trained object detection models from YOLOv5 (trained with the COCO dataset).
    But since, in our case, the objects are not part of COCO datasets, we should select
    the option of `tracking objects`. With this option, once you draw bounding boxes
    and label the images in one frame, the objects will be tracked automatically from
    frame to frame, *partially* labeling the new ones (not all are correctly labeled).
  prefs: []
  type: TYPE_NORMAL
- en: If you already have a labeled dataset containing bounding boxes, import your
    data using the EI uploader.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Labeling the Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Starting with the first image of your unlabeled data, use your mouse to drag
    a box around an object to add a label. Then click **Save labels** to advance to
    the next item.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file425.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Continue with this process until the queue is empty. At the end, all images
    should have the objects labeled as those samples below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file426.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, review the labeled samples on the `Data acquisition` tab. If one of the
    labels is wrong, it can be edited using the *`three dots`* menu after the sample
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file427.png)'
  prefs: []
  type: TYPE_IMG
- en: We will be guided to replace the wrong label and correct the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file428.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Impulse Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this phase, we should define how to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-processing** consists of resizing the individual images from `320 x 240`
    to `96 x 96` and squashing them (squared form, without cropping). Afterward, the
    images are converted from RGB to Grayscale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Design a Model,** in this case, “Object Detection.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../media/file429.png)'
  prefs: []
  type: TYPE_IMG
- en: Preprocessing all dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, select **Color depth** as `Grayscale`, suitable for use with
    FOMO models and Save `parameters`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file430.png)'
  prefs: []
  type: TYPE_IMG
- en: The Studio moves automatically to the next section, `Generate features`, where
    all samples will be pre-processed, resulting in a dataset with individual <semantics><mrow><mn>96</mn><mo>×</mo><mn>96</mn><mo>×</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">96\times 96\times 1</annotation></semantics> images
    or 9,216 features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file431.png)'
  prefs: []
  type: TYPE_IMG
- en: The feature explorer shows that all samples evidence a good separation after
    the feature generation.
  prefs: []
  type: TYPE_NORMAL
- en: One of the samples (46) is apparently in the wrong space, but clicking on it
    confirms that the labeling is correct.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Model Design, Training, and Test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35)
    designed to coarsely segment an image into a grid of **background** vs **objects
    of interest** (here, *boxes* and *wheels*).
  prefs: []
  type: TYPE_NORMAL
- en: FOMO is an innovative machine learning model for object detection, which can
    use up to 30 times less energy and memory than traditional models like Mobilenet
    SSD and YOLOv5\. FOMO can operate on microcontrollers with less than 200 KB of
    RAM. The main reason this is possible is that while other models calculate the
    object’s size by drawing a square around it (bounding box), FOMO ignores the size
    of the image, providing only the information about where the object is located
    in the image, by means of its centroid coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: How FOMO works?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FOMO takes the image in grayscale and divides it into blocks of pixels using
    a factor of 8\. For the input of 96x96, the grid would be <semantics><mrow><mn>12</mn><mo>×</mo><mn>12</mn></mrow><annotation
    encoding="application/x-tex">12\times 12</annotation></semantics> <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>96</mn><mi>/</mi><mn>8</mn><mo>=</mo><mn>12</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(96/8=12)</annotation></semantics>.
    Next, FOMO will run a classifier through each pixel block to calculate the probability
    that there is a box or a wheel in each of them and, subsequently, determine the
    regions that have the highest probability of containing the object (If a pixel
    block has no objects, it will be classified as *background*). From the overlap
    of the final region, the FOMO provides the coordinates (related to the image dimensions)
    of the centroid of this region.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file432.png)'
  prefs: []
  type: TYPE_IMG
- en: For training, we should select a pre-trained model. Let’s use the **`FOMO (Faster
    Objects, More Objects) MobileNetV2 0.35`.** This model uses around 250 KB of RAM
    and 80 KB of ROM (Flash), which suits well with our board since it has 1 MB of
    RAM and ROM.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file433.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Regarding the training hyper-parameters, the model will be trained with:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Epochs: 60,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batch size: 32'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning Rate: 0.001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For validation during training, 20% of the dataset (*validation_dataset*) will
    be spared. For the remaining 80% (*train_dataset*), we will apply Data Augmentation,
    which will randomly flip, change the size and brightness of the image, and crop
    them, artificially increasing the number of samples on the dataset for training.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the model ends with an F1 score of around 91% (validation) and
    93% (test data).
  prefs: []
  type: TYPE_NORMAL
- en: Note that FOMO automatically added a 3rd label background to the two previously
    defined (*box* and *wheel*).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../media/file434.png)'
  prefs: []
  type: TYPE_IMG
- en: In object detection tasks, accuracy is generally not the primary [evaluation
    metric](https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/).
    Object detection involves classifying objects and providing bounding boxes around
    them, making it a more complex problem than simple classification. The issue is
    that we do not have the bounding box, only the centroids. In short, using accuracy
    as a metric could be misleading and may not provide a complete understanding of
    how well the model is performing. Because of that, we will use the F1 score.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Test model with “Live Classification”
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since Edge Impulse officially supports the Nicla Vision, let’s connect it to
    the Studio. For that, follow the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the [last EI Firmware](https://cdn.edgeimpulse.com/firmware/arduino-nicla-vision.zip)
    and unzip it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open the zip file on your computer and select the uploader related to your OS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put the Nicla-Vision on Boot Mode, pressing the reset button twice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execute the specific batch code for your OS to upload the binary (`arduino-nicla-vision.bin`)
    to your board.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Go to `Live classification` section at EI Studio, and using *webUSB,* connect
    your Nicla Vision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file435.png)'
  prefs: []
  type: TYPE_IMG
- en: Once connected, you can use the Nicla to capture actual images to be tested
    by the trained model on Edge Impulse Studio.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file436.png)'
  prefs: []
  type: TYPE_IMG
- en: One thing to note is that the model can produce false positives and negatives.
    This can be minimized by defining a proper `Confidence Threshold` (use the `three
    dots` menu for the setup). Try with 0.8 or more.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Select `OpenMV Firmware` on the Deploy Tab and press `[Build]`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file437.png)'
  prefs: []
  type: TYPE_IMG
- en: When you try to connect the Nicla with the OpenMV IDE again, it will try to
    update its FW. Choose the option `Load a specific firmware` instead. Or go to
    `Tools > Runs Bootloader (Load Firmware).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file438.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will find a ZIP file on your computer from the Studio. Open it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file439.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Load the .bin file to your board:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file440.png)'
  prefs: []
  type: TYPE_IMG
- en: After the download is finished, a pop-up message will be displayed. `Press OK`,
    and open the script **ei_object_detection.py** downloaded from the Studio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: If a Pop-up appears saying that the FW is out of date, press `[NO]`,
    to upgrade it.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Before running the script, let’s change a few lines. Note that you can leave
    the window definition as <semantics><mrow><mn>240</mn><mo>×</mo><mn>240</mn></mrow><annotation
    encoding="application/x-tex">240\times 240</annotation></semantics> and the camera
    capturing images as QVGA/RGB. The captured image will be pre-processed by the
    FW deployed from Edge Impulse
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Redefine the minimum confidence, for example, to 0.8 to minimize false positives
    and negatives.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Change if necessary, the color of the circles that will be used to display the
    detected object’s centroid for a better contrast.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Keep the remaining code as it is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'and press the `green Play button` to run the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file441.png)'
  prefs: []
  type: TYPE_IMG
- en: From the camera’s view, we can see the objects with their centroids marked with
    12 pixel-fixed circles (each circle has a distinct color, depending on its class).
    On the Serial Terminal, the model shows the labels detected and their position
    on the image window <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>240</mn><mo>×</mo><mn>240</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(240\times
    240)</annotation></semantics>.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that the coordinate origin is in the upper left corner.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../media/file442.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the frames per second rate is around 8 fps (similar to what we got
    with the Image Classification project). This happens because FOMO is cleverly
    built over a CNN model, not with an object detection model like the SSD MobileNet
    or YOLO. For example, when running a MobileNetV2 SSD FPN-Lite <semantics><mrow><mn>320</mn><mo>×</mo><mn>320</mn></mrow><annotation
    encoding="application/x-tex">320\times 320</annotation></semantics> model on a
    Raspberry Pi 4, the latency is around 5 times higher (around 1.5 fps)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a short video showing the inference results: [https://youtu.be/JbpoqRp3BbM](https://youtu.be/JbpoqRp3BbM)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'FOMO is a significant leap in the image processing space, as Louis Moreau and
    Mat Kelcey put it during its launch in 2022:'
  prefs: []
  type: TYPE_NORMAL
- en: FOMO is a ground-breaking algorithm that brings real-time object detection,
    tracking, and counting to microcontrollers for the first time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Multiple possibilities exist for exploring object detection (and, more precisely,
    counting them) on embedded devices. This can be very useful on projects counting
    bees, for example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file443.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Edge Impulse Project](https://studio.edgeimpulse.com/public/292737/latest)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
