- en: Keyword Spotting (KWS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../media/file444.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*DALL·E 3 Prompt: 1950s style cartoon scene set in a vintage audio research
    room. Two Afro-American female scientists are at the center. One holds a magnifying
    glass, closely examining ancient circuitry, while the other takes notes. On their
    wooden table, there are multiple boards with sensors, notably featuring a microphone.
    Behind these boards, a computer with a large, rounded back displays the Arduino
    IDE. The IDE showcases code for LED pin assignments and machine learning inference
    for voice command detection. A distinct window in the IDE, the Serial Monitor,
    reveals outputs indicating the spoken commands ‘yes’ and ‘no’. The room ambiance
    is nostalgic with vintage lamps, classic audio analysis tools, and charts depicting
    FFT graphs and time-domain curves.*'
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having already explored the Nicla Vision board in the *Image Classification*
    and *Object Detection* applications, we are now shifting our focus to voice-activated
    applications with a project on Keyword Spotting (KWS).
  prefs: []
  type: TYPE_NORMAL
- en: As introduced in the *Feature Engineering for Audio Classification* Hands-On
    tutorial, Keyword Spotting (KWS) is integrated into many voice recognition systems,
    enabling devices to respond to specific words or phrases. While this technology
    underpins popular devices like Google Assistant or Amazon Alexa, it’s equally
    applicable and feasible on smaller, low-power devices. This tutorial will guide
    you through implementing a KWS system using TinyML on the Nicla Vision development
    board equipped with a digital microphone.
  prefs: []
  type: TYPE_NORMAL
- en: Our model will be designed to recognize keywords that can trigger device wake-up
    or specific actions, bringing them to life with voice-activated commands.
  prefs: []
  type: TYPE_NORMAL
- en: How does a voice assistant work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As said, *voice assistants* on the market, like Google Home or Amazon Echo-Dot,
    only react to humans when they are “waked up” by particular keywords such as ”
    Hey Google” on the first one and “Alexa” on the second.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file445.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, recognizing voice commands is based on a multi-stage model or
    Cascade Detection.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file446.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Stage 1**: A small microprocessor inside the Echo Dot or Google Home continuously
    listens, waiting for the keyword to be spotted, using a TinyML model at the edge
    (KWS application).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 2**: Only when triggered by the KWS application on Stage 1 is the data
    sent to the cloud and processed on a larger model.'
  prefs: []
  type: TYPE_NORMAL
- en: The video below shows an example of a Google Assistant being programmed on a
    Raspberry Pi (Stage 2), with an Arduino Nano 33 BLE as the TinyML device (Stage
    1).
  prefs: []
  type: TYPE_NORMAL
- en: '[https://youtu.be/e_OPgcnsyvM](https://youtu.be/e_OPgcnsyvM)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore the above Google Assistant project, please see the tutorial: [Building
    an Intelligent Voice Assistant From Scratch](https://www.hackster.io/mjrobot/building-an-intelligent-voice-assistant-from-scratch-2199c3).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this KWS project, we will focus on Stage 1 (KWS or Keyword Spotting), where
    we will use the Nicla Vision, which has a digital microphone that will be used
    to spot the keyword.
  prefs: []
  type: TYPE_NORMAL
- en: The KWS Hands-On Project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The diagram below gives an idea of how the final KWS application should work
    (during inference):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file447.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Our KWS application will recognize four classes of sound:'
  prefs: []
  type: TYPE_NORMAL
- en: '**YES** (Keyword 1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NO** (Keyword 2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NOISE** (no words spoken; only background noise is present)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UNKNOWN** (a mix of different words than YES and NO)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For real-world projects, it is always advisable to include other sounds besides
    the keywords, such as “Noise” (or Background) and “Unknown.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Machine Learning workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main component of the KWS application is its model. So, we must train such
    a model with our specific keywords, noise, and other words (the “unknown”):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file448.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The critical component of any Machine Learning Workflow is the **dataset**.
    Once we have decided on specific keywords, in our case (*YES* and NO), we can
    take advantage of the dataset developed by Pete Warden, [“Speech Commands: A Dataset
    for Limited-Vocabulary Speech Recognition](https://arxiv.org/pdf/1804.03209.pdf).”
    This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop,
    and go. In words such as *yes* and *no,* we can get 1,500 samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download a small portion of the dataset from Edge Studio ([Keyword
    spotting pre-built dataset](https://docs.edgeimpulse.com/docs/pre-built-datasets/keyword-spotting)),
    which includes samples from the four classes we will use in this project: yes,
    no, noise, and background. For this, follow the steps below:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the [keywords dataset.](https://cdn.edgeimpulse.com/datasets/keywords2.zip)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unzip the file to a location of your choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uploading the dataset to the Edge Impulse Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Initiate a new project at Edge Impulse Studio (EIS) and select the `Upload
    Existing Data` tool in the `Data Acquisition` section. Choose the files to be
    uploaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file449.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Define the Label, select `Automatically split between train and test,` and `Upload
    data` to the EIS. Repeat for all classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file450.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The dataset will now appear in the `Data acquisition` section. Note that the
    approximately 6,000 samples (1,500 for each class) are split into Train (4,800)
    and Test (1,200) sets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file451.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Capturing additional Audio Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although we have a lot of data from Pete’s dataset, collecting some words spoken
    by us is advised. When working with accelerometers, creating a dataset with data
    captured by the same type of sensor is essential. In the case of *sound*, this
    is optional because what we will classify is, in reality, *audio* data.
  prefs: []
  type: TYPE_NORMAL
- en: The key difference between sound and audio is the type of energy. Sound is mechanical
    perturbation (longitudinal sound waves) that propagate through a medium, causing
    variations of pressure in it. Audio is an electrical (analog or digital) signal
    representing sound.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When we pronounce a keyword, the sound waves should be converted to audio data.
    The conversion should be done by sampling the signal generated by the microphone
    at a 16 KHz frequency with 16-bit per sample amplitude.
  prefs: []
  type: TYPE_NORMAL
- en: So, any device that can generate audio data with this basic specification (16
    KHz/16 bits) will work fine. As a *device*, we can use the NiclaV, a computer,
    or even your mobile phone.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file452.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using the NiclaV and the Edge Impulse Studio
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As we learned in the chapter *Setup Nicla Vision*, EIS officially supports
    the Nicla Vision, which simplifies the capture of the data from its sensors, including
    the microphone. So, please create a new project on EIS and connect the Nicla to
    it, following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the last updated [EIS Firmware](https://cdn.edgeimpulse.com/firmware/arduino-nicla-vision.zip)
    and unzip it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open the zip file on your computer and select the uploader corresponding to
    your OS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../media/file453.png)'
  prefs: []
  type: TYPE_IMG
- en: Put the NiclaV in Boot Mode by pressing the reset button twice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upload the binary *arduino-nicla-vision.bin* to your board by running the batch
    code corresponding to your OS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go to your project on EIS, and on the `Data Acquisition tab`, select `WebUSB`.
    A window will pop up; choose the option that shows that the `Nicla is paired`
    and press `[Connect]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can choose which sensor data to pick in the `Collect Data` section on the
    `Data Acquisition` tab. Select: `Built-in microphone`, define your `label` (for
    example, *yes*), the sampling `Frequency`[16000Hz], and the `Sample length (in
    milliseconds)`, for example [10s]. `Start sampling`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file454.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Data on Pete’s dataset have a length of 1s, but the recorded samples are 10s
    long and must be split into 1s samples. Click on `three dots` after the sample
    name and select `Split sample`.
  prefs: []
  type: TYPE_NORMAL
- en: A window will pop up with the Split tool.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file455.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once inside the tool, split the data into 1-second (1000 ms) records. If necessary,
    add or remove segments. This procedure should be repeated for all new samples.
  prefs: []
  type: TYPE_NORMAL
- en: Using a smartphone and the EI Studio
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can also use your PC or smartphone to capture audio data, using a sampling
    frequency of 16 KHz and a bit depth of 16.
  prefs: []
  type: TYPE_NORMAL
- en: Go to `Devices`, scan the `QR Code` using your phone, and click on the link.
    A data Collection app will appear in your browser. Select `Collecting Audio`,
    and define your `Label`, data capture `Length,` and `Category`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file456.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Repeat the same procedure used with the NiclaV.
  prefs: []
  type: TYPE_NORMAL
- en: Note that any app, such as [Audacity](https://www.audacityteam.org/), can be
    used for audio recording, provided you use 16 KHz/16-bit depth samples.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Creating Impulse (Pre-Process / Model definition)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*An* **impulse** *takes raw data, uses signal processing to extract features,
    and then uses a learning block to classify new data.*'
  prefs: []
  type: TYPE_NORMAL
- en: Impulse Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../media/file457.jpg)'
  prefs: []
  type: TYPE_IMG
- en: First, we will take the data points with a 1-second window, augmenting the data
    and sliding that window in 500 ms intervals. Note that the option zero-pad data
    is set. It is essential to fill with ‘zeros’ samples smaller than 1 second (in
    some cases, some samples can result smaller than the 1000 ms window on the split
    tool to avoid noise and spikes).
  prefs: []
  type: TYPE_NORMAL
- en: Each 1-second audio sample should be pre-processed and converted to an image
    (for example, <semantics><mrow><mn>13</mn><mo>×</mo><mn>49</mn><mo>×</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">13\times 49\times 1</annotation></semantics>). As
    discussed in the *Feature Engineering for Audio Classification* Hands-On tutorial,
    we will use `Audio (MFCC)`, which extracts features from audio signals using [Mel
    Frequency Cepstral Coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum),
    which are well suited for the human voice, our case here.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we select the `Classification` block to build our model from scratch using
    a Convolution Neural Network (CNN).
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can use the `Transfer Learning (Keyword Spotting)` block,
    which fine-tunes a pre-trained keyword spotting model on your data. This approach
    has good performance with relatively small keyword datasets.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pre-Processing (MFCC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following step is to create the features to be trained in the next phase:'
  prefs: []
  type: TYPE_NORMAL
- en: We could keep the default parameter values, but we will use the DSP `Autotune
    parameters` option.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file458.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will take the `Raw features` (our 1-second, 16 KHz sampled audio data) and
    use the MFCC processing block to calculate the `Processed features`. For every
    16,000 raw features (16,000 <semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    1 second), we will get 637 processed features <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>13</mn><mo>×</mo><mn>49</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">(13\times 49)</annotation></semantics>.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file459.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The result shows that we only used a small amount of memory to pre-process data
    (16 KB) and a latency of 34 ms, which is excellent. For example, on an Arduino
    Nano (Cortex-M4f @ 64 MHz), the same pre-process will take around 480 ms. The
    parameters chosen, such as the `FFT length` [512], will significantly impact the
    latency.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s `Save parameters` and move to the `Generated features` tab, where
    the actual features will be generated. Using [UMAP](https://umap-learn.readthedocs.io/en/latest/),
    a dimension reduction technique, the `Feature explorer` shows how the features
    are distributed on a two-dimensional plot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file460.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The result seems OK, with a visually clear separation between *yes* features
    (in red) and *no* features (in blue). The *unknown* features seem nearer to the
    *no space* than the *yes*. This suggests that the keyword *no* has more propensity
    to false positives.
  prefs: []
  type: TYPE_NORMAL
- en: Going under the hood
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand better how the raw sound is preprocessed, look at the *Feature
    Engineering for Audio Classification* chapter. You can play with the MFCC features
    generation by downloading this [notebook](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_MFCC_Analysis.ipynb)
    from GitHub or [[Opening it In Colab]](https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_MFCC_Analysis.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: Model Design and Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use a simple Convolution Neural Network (CNN) model, tested with 1D
    and 2D convolutions. The basic architecture has two blocks of Convolution + MaxPooling
    ([8] and [16] filters, respectively) and a Dropout of [0.25] for the 1D and [0.5]
    for the 2D. For the last layer, after Flattening, we have [4] neurons, one for
    each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file461.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As hyper-parameters, we will have a `Learning Rate` of [0.005] and a model trained
    by [100] epochs. We will also include a data augmentation method based on [SpecAugment](https://arxiv.org/abs/1904.08779).
    We trained the 1D and the 2D models with the same hyperparameters. The 1D architecture
    had a better overall result (90.5% accuracy when compared with 88% of the 2D,
    so we will use the 1D.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file462.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using 1D convolutions is more efficient because it requires fewer parameters
    than 2D convolutions, making them more suitable for resource-constrained environments.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is also interesting to pay attention to the 1D Confusion Matrix. The F1 Score
    for `yes` is 95%, and for `no`, 91%. That was expected by what we saw with the
    Feature Explorer (`no` and `unknown` at close distance). In trying to improve
    the result, you can inspect closely the results of the samples with an error.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file463.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Listen to the samples that went wrong. For example, for `yes`, most of the mistakes
    were related to a yes pronounced as “yeh”. You can acquire additional samples
    and then retrain your model.
  prefs: []
  type: TYPE_NORMAL
- en: Going under the hood
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to understand what is happening “under the hood,” you can download
    the pre-processed dataset (`MFCC training data`) from the `Dashboard` tab and
    run this [Jupyter Notebook](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_CNN_training.ipynb),
    playing with the code or [[Opening it In Colab]](https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_CNN_training.ipynb).
    For example, you can analyze the accuracy by each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file464.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Testing the model with the data reserved for training (Test Data), we got an
    accuracy of approximately 76%.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file465.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting the F1 score, we can see that for YES, we got 0.90, an excellent
    result since we expect to use this keyword as the primary “trigger” for our KWS
    project. The worst result (0.70) is for UNKNOWN, which is OK.
  prefs: []
  type: TYPE_NORMAL
- en: For NO, we got 0.72, which was expected, but to improve this result, we can
    move the samples that were not correctly classified to the training dataset and
    then repeat the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Live Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can proceed to the project’s next step but also consider that it is possible
    to perform `Live Classification` using the NiclaV or a smartphone to capture live
    samples, testing the trained model before deployment on our device.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy and Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The EIS will package all the needed libraries, preprocessing functions, and
    trained models, downloading them to your computer. Go to the `Deployment` section,
    select `Arduino Library`, and at the bottom, choose `Quantized (Int8)` and press
    `Build`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file466.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When the `Build` button is selected, a zip file will be created and downloaded
    to your computer. On your Arduino IDE, go to the `Sketch` tab, select the option
    `Add .ZIP Library`, and Choose the .zip file downloaded by EIS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file467.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, it is time for a real test. We will make inferences while completely disconnected
    from the EIS. Let’s use the NiclaV code example created when we deployed the Arduino
    Library.
  prefs: []
  type: TYPE_NORMAL
- en: In your Arduino IDE, go to the `File/Examples` tab, look for your project, and
    select `nicla-vision/nicla-vision_microphone` (or `nicla-vision_microphone_continuous`)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file468.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Press the reset button twice to put the NiclaV in boot mode, upload the sketch
    to your board, and test some real inferences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file469.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Post-processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know the model is working since it detects our keywords, let’s modify
    the code to see the result with the NiclaV completely offline (disconnected from
    the PC and powered by a battery, a power bank, or an independent 5V power supply).
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that whenever the keyword YES is detected, the Green LED will light;
    if a NO is heard, the Red LED will light, if it is a UNKNOWN, the Blue LED will
    light; and in the presence of noise (No Keyword), the LEDs will be OFF.
  prefs: []
  type: TYPE_NORMAL
- en: We should modify one of the code examples. Let’s do it now with the `nicla-vision_microphone_continuous`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with initializing the LEDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Create two functions, `turn_off_leds()` function , to turn off all RGB LEDs
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Another `turn_on_led()` function is used to turn on the RGB LEDs according to
    the most probable result of the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And change the `// print the predictions` portion of the code on `loop()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can find the complete code on the [project’s GitHub](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/KWS/nicla_vision_microphone_continuous_LED).
  prefs: []
  type: TYPE_NORMAL
- en: Upload the sketch to your board and test some real inferences. The idea is that
    the Green LED will be ON whenever the keyword YES is detected, the Red will lit
    for a NO, and any other word will turn on the Blue LED. All the LEDs should be
    off if silence or background noise is present. Remember that the same procedure
    can “trigger” an external device to perform a desired action instead of turning
    on an LED, as we saw in the introduction.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://youtu.be/25Rd76OTXLY](https://youtu.be/25Rd76OTXLY)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will find the notebooks and code used in this hands-on tutorial on the [GitHub](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/KWS)
    repository.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Before we finish, consider that Sound Classification is more than just voice.
    For example, you can develop TinyML projects around sound in several areas, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security** (Broken Glass detection, Gunshot)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Industry** (Anomaly Detection)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical** (Snore, Cough, Pulmonary diseases)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nature** (Beehive control, insect sound, pouching mitigation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Subset of Google Speech Commands Dataset](https://cdn.edgeimpulse.com/datasets/keywords2.zip)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KWS MFCC Analysis Colab Notebook](https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_MFCC_Analysis.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KWS_CNN_training Colab Notebook](https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_CNN_training.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Arduino Post-processing Code](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/KWS/nicla_vision_microphone_continuous_LED)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Edge Impulse Project](https://studio.edgeimpulse.com/public/292418/latest)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
