<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Feature Ranking</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Feature Ranking</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_feature_ranking.html">https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_feature_ranking.html</a></blockquote>

<p>Michael J. Pyrcz, Professor, The University of Texas at Austin</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
<p>Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with Code‚Äù.</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite this e-Book as:</p>
<p>Pyrcz, M.J., 2024, <em>Applied Machine Learning in Python: A Hands-on Guide with Code</em> [e-book]. Zenodo. doi:10.5281/zenodo.15169138 <a class="reference external" href="https://doi.org/10.5281/zenodo.15169138"><img alt="DOI" src="../Images/7e4ea662f44af1eae87e87ecbb962ff4.png" data-original-src="https://zenodo.org/badge/863274676.svg"/></a></p>
</div>
<p>The workflows in this book and more are available here:</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite the MachineLearningDemos GitHub Repository as:</p>
<p>Pyrcz, M.J., 2024, <em>MachineLearningDemos: Python Machine Learning Demonstration Workflows Repository</em> (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312. GitHub repository: <a class="github reference external" href="https://github.com/GeostatsGuy/MachineLearningDemos">GeostatsGuy/MachineLearningDemos</a> <a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.13835312"><img alt="DOI" src="../Images/4e3a59c17d684b06a170c4af84e0f631.png" data-original-src="https://zenodo.org/badge/862519860.svg"/></a></p>
</div>
<p>By Michael J. Pyrcz <br/>
¬© Copyright 2024.</p>
<p>This chapter is a tutorial for / demonstration of <strong>Feature Ranking</strong>.</p>
<p><strong>YouTube Lecture</strong>: check out my lectures on:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl">Introduction to Machine Learning</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/embks9p4pb8?si=B2HXm_i0oMSWkBhN">Curse of Dimensionality, Dimensionality Reduction, Principal Component Analysis</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/Yt0o8ukIOKU?si=_ri1NPwKVdhYzgO3">Multidimensional Scaling and Random Projection</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/6QJjZoWknEI?si=p6vp811xWAmzWY3r">Feature Transformations</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/5Q0gemu-h3Q?si=ATG-ue0i2qcc-IVx">Feature Selection</a></p></li>
</ul>
<p>These lectures are all part of my <a class="reference external" href="https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&amp;si=XonjO2wHdXffMpeI">Machine Learning Course</a> on YouTube with linked well-documented Python workflows and interactive dashboards. My goal is to share accessible, actionable, and repeatable educational content. If you want to know about my motivation, check out <a class="reference external" href="https://michaelpyrcz.com/my-story">Michael‚Äôs Story</a>.</p>
<section id="motivation-for-feature-ranking">
<h2>Motivation for Feature Ranking</h2>
<p>There are often many predictor features (input variables), available for us to work with for building our prediction models.</p>
<ul class="simple">
<li><p>there are good reasons to be selective, throwing in every possible feature is not a good idea!</p></li>
</ul>
<p>In general, for the best prediction model, careful selection of the fewest features that provide the most amount of information is the best practice.</p>
<p>Here‚Äôs why:</p>
<ul class="simple">
<li><p><strong>blunders</strong> - more predictor features result in more complicated workflows that require more professional time and have increased opportunity for mistakes in the workflow</p></li>
<li><p><strong>difficult to visualize</strong> - higher dimensional models, i.e., larger number of predictor features, are more difficult to visualize</p></li>
<li><p><strong>model checking</strong> - more complicated models may be more difficult to interrogate, interpret and QC</p></li>
<li><p><strong>predictor feature redundancy</strong> - more likely to have redundant predictor features. The inclusion of highly redundant and collinear or multicollinear features increases model variance, increase model instability and decreases prediction accuracy for testing</p></li>
<li><p><strong>computational time</strong> - more predictor features generally increase the computational time required to train the model and the computational storage, i.e., the model may be less compact and portable</p></li>
<li><p><strong>model overfit</strong> - the risk of overfit increases with the more features, due to increase in model complexity</p></li>
<li><p><strong>model extrapolation</strong> - many predictor features results in high dimensional model space with less data coverage and more likelihood for model extrapolation that may be inaccurate</p></li>
</ul>
<p>The primary concern with many predictor features is the curse of dimensionality. Let‚Äôs summarize the curse!</p>
</section>
<section id="curse-of-dimensionality">
<h2>Curse of Dimensionality</h2>
<ol class="arabic simple">
<li><p><strong>Data and Model Visualization</strong> - we cannot visualize beyond 3D, i.e., access the model fit to data, evaluate interpolation vs. extrapolation.</p></li>
</ol>
<ul class="simple">
<li><p>consider a 5D example shown as a matrix scatter plot, even in this case there is an extreme marginalization to 2D for each plot,</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/ecf50f66114aec17ea35fde1342d66c4.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static\feature_ranking\matrix_scatterplot.png"/>
  <figcaption style="text-align: center;">Example 5D data as a matrix scatter plot.</figcaption>
</figure>
<ol class="arabic simple" start="2">
<li><p><strong>Sampling</strong> - the number of samples sufficient to infer statistics like the joint probability, <span class="math notranslate nohighlight">\(P(x_1,\ldots,x_m)\)</span>.</p></li>
</ol>
<ul class="simple">
<li><p>recall the calculation of a histogram or normalized histogram: we establish bins and calculate frequencies or probabilities in each bin.</p></li>
<li><p>we require a nominal number of data samples for each bin, so we require <span class="math notranslate nohighlight">\(ùëõ=ùëõ_{ùë†/ùëèùëñùëõ} \cdot ùëõ_{ùëèùëñùëõùë†}\)</span> samples in 1D</p></li>
<li><p>but in mD we required <span class="math notranslate nohighlight">\(n\)</span> samples to calculate the discretized joint probability,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
ùëõ=ùëõ_{ùë†/ùëèùëñùëõ} \cdot ùëõ_{ùëèùëñùëõùë†}^m$
\]</div>
<ul class="simple">
<li><p>for example, 10 samples per bin with 35 bins requires 12,250 samples in 2D, and 428,750 samples in 3D</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/bc8823819263f4497ef6baab93a9ee38.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/feature_ranking/bivariate_example.png"/>
  <figcaption style="text-align: center;">Example 2D data with 35 bins for each feature.</figcaption>
</figure>
<ol class="arabic simple" start="3">
<li><p><strong>Sample Coverage</strong> - the range of the sample values cover the predictor feature space.</p></li>
</ol>
<ul class="simple">
<li><p>fraction of the possible solution space that is sampled, for 1 feature we assume 80% coverage</p></li>
<li><p>remember, we usually, directly sample only <span class="math notranslate nohighlight">\(\frac{1}{10^7}\)</span> of the volume of the subsurface</p></li>
<li><p>yes, the concept of coverage is subjective, how much data to cover? What about gaps? etc.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/d8058511a88a482ed34b0cbd9eb34fec.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/feature_ranking/coverage1D.png"/>
  <figcaption style="text-align: center;">Example 2D data with 35 bins for each feature.</figcaption>
</figure>
<ul class="simple">
<li><p>now if there is 80% coverage for 2 features the 2D coverage is 64%</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/8d96453b3f6c2a92a160fe4329a13d4a.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/feature_ranking/coverage2D.png"/>
  <figcaption style="text-align: center;">Example 2D data with 35 bins for each feature.</figcaption>
</figure>
<ul class="simple">
<li><p>coverage is,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
c =  c_1^m
\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Distorted Space</strong> - high dimensional space is distorted.</p></li>
</ol>
<ul class="simple">
<li><p>take the ratio of the volume of an inscribed hypersphere in a hypercube,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\pi^{\frac{m}{2}}}{m 2^{m-1} \Gamma\left(\frac{m}{2}\right)} \to 0 \quad \text{as} \quad m \to \infty
\]</div>
<ul class="simple">
<li><p>recall, <span class="math notranslate nohighlight">\(\Gamma(ùëõ)=(ùëõ‚àí1)!\)</span>.</p></li>
<li><p>high dimensional space is all corners and no middle and most of high dimensional space is far from the middle (all corners!).</p></li>
<li><p>as a result distances in high dimensional space lose sensitivity, i.e., for any random points in the space the expected pairwise distances all become the same,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lim_{m \to \infty} \left( \mathbb{E}\left[\text{dist}_{\text{max}}(m) - \text{dist}_{\text{min}}(m)\right] \right) \to 0
\]</div>
<ul class="simple">
<li><p>the limit of the expectation of the range of pairwise distances over random points in hyper-dimensional space tends to zero. If distances are almost all the same, Euclidian distance is no longer meaningful!</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/8c8d512cca4eb330150d1ba298831543.png" style="display: block; margin: 0 auto; width: 60%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/feature_ranking/distortion_chart.png"/>
  <figcaption style="text-align: center;">The ratio of the volume of a hypersphere within a hypercube.</figcaption>
</figure>
<ul class="simple">
<li><p>here‚Äôs the severity of the distortion for various dimensionalities,</p></li>
</ul>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>m</p></th>
<th class="head"><p>nD / 2D</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2</p></td>
<td><p>1.0</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>0.28</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>0.003</p></td>
</tr>
<tr class="row-odd"><td><p>20</p></td>
<td><p>0.00000003</p></td>
</tr>
</tbody>
</table>
<ol class="arabic simple" start="5">
<li><p><strong>Multicollinearity</strong> - higher dimensional datasets are more likely to have collinearity or multicollinearity.</p></li>
</ol>
<ul class="simple">
<li><p>Feature linearly described by other features resulting in high model variance.</p></li>
</ul>
</section>
<section id="what-is-feature-ranking">
<h2>What is Feature Ranking?</h2>
<p>Feature ranking is a set of metrics that assign relative importance or value to each predictor feature with respect to information contained for inference and importance in predicting a response feature. There are a wide variety of possible methods to accomplish this. My recommendation is a <strong>‚Äòwide-array‚Äô</strong> approach with multiple analyses and metrics, while understanding the assumptions and limitations of each method.</p>
<p>Here‚Äôs the general types of metrics that we consider for feature ranking.</p>
<ol class="arabic simple">
<li><p>Visual Inspection of Data Distributions and Scatter Plots</p></li>
<li><p>Statistical Summaries</p></li>
<li><p>Model-based</p></li>
<li><p>Recursive Feature Elimination</p></li>
</ol>
<p>Also, we should not neglect expert knowledge. If additional information is known about physical processes, causation, reliability and availability of predictor features this should be integrated into assigning feature ranking.</p>
</section>
<section id="load-the-required-libraries">
<h2>Load the Required Libraries</h2>
<p>The following code loads the required libraries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">geostatspy.GSLIB</span> <span class="k">as</span> <span class="nn">GSLIB</span>                              <span class="c1"># GSLIB utilities, visualization and wrapper</span>
<span class="kn">import</span> <span class="nn">geostatspy.geostats</span> <span class="k">as</span> <span class="nn">geostats</span>                        <span class="c1"># GSLIB methods convert to Python  </span>
<span class="kn">import</span> <span class="nn">geostatspy</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'GeostatsPy version: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">geostatspy</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>GeostatsPy version: 0.0.71
</pre></div>
</div>
</div>
</div>
<p>We will also need some standard packages. These should have been installed with Anaconda 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">ignore_warnings</span> <span class="o">=</span> <span class="kc">True</span>                                        <span class="c1"># ignore warnings?</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                                            <span class="c1"># ndarrays for gridded data</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>                                           <span class="c1"># DataFrames for tabular data</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>                             <span class="c1"># remove encoding error</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>                     <span class="c1"># for recursive feature selection</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">mutual_info_regression</span>  <span class="c1"># mutual information</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>             <span class="c1"># linear regression model</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>            <span class="c1"># model-based feature importance</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>                                   <span class="c1"># measures to check our models</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span> <span class="c1"># variance inflation factor</span>
<span class="kn">import</span> <span class="nn">os</span>                                                     <span class="c1"># set working directory, run executables</span>
<span class="kn">import</span> <span class="nn">math</span>                                                   <span class="c1"># basic math operations</span>
<span class="kn">import</span> <span class="nn">random</span>                                                 <span class="c1"># for random numbers</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>                               <span class="c1"># for plotting</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span> <span class="n">AutoMinorLocator</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>                  <span class="c1"># custom color maps</span>
<span class="kn">import</span> <span class="nn">matplotlib.ticker</span> <span class="k">as</span> <span class="nn">mtick</span>                             <span class="c1"># control tick label formatting</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>                                         <span class="c1"># for matrix scatter plots</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>                                       <span class="c1"># summary statistics</span>
<span class="kn">import</span> <span class="nn">numpy.linalg</span> <span class="k">as</span> <span class="nn">linalg</span>                                 <span class="c1"># for linear algebra</span>
<span class="kn">import</span> <span class="nn">scipy.spatial</span> <span class="k">as</span> <span class="nn">sp</span>                                    <span class="c1"># for fast nearest neighbor search</span>
<span class="kn">import</span> <span class="nn">scipy.signal</span> <span class="k">as</span> <span class="nn">signal</span>                                 <span class="c1"># kernel for moving window calculation</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jit</span>                                         <span class="c1"># for numerical speed up</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.weightstats</span> <span class="kn">import</span> <span class="n">DescrStatsW</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># plot all grids below the plot elements</span>
<span class="k">if</span> <span class="n">ignore_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>                                   
    <span class="kn">import</span> <span class="nn">warnings</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>                                         <span class="c1"># color map</span>
</pre></div>
</div>
</div>
</div>
<p>For the Shapley value approach for feature ranking we need an additional package and to start up javascript support.</p>
<ul class="simple">
<li><p>after running this block you should see a hexagon with the text ‚Äòjs‚Äô to indicate that javascript is ready</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">sys</span>
<span class="c1">#!{sys.executable} -m pip install shap</span>
<span class="kn">import</span> <span class="nn">shap</span>
<span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div align="center"><img src="../Images/70b822753245ba6bb888425de8eb62b5.png"/></div></div></div>
</div>
<p>If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available with the respective package docs.</p>
</section>
<section id="design-custom-color-map">
<h2>Design Custom Color Map</h2>
<p>Accounting for significance by masking nonsignificant values</p>
<ul class="simple">
<li><p>for demonstration only currently, could be updated for each plot based on results confidence and uncertainty</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">my_colormap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'RdBu_r'</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>                  <span class="c1"># make a custom colormap</span>
<span class="n">newcolors</span> <span class="o">=</span> <span class="n">my_colormap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>               <span class="c1"># define colormap space</span>
<span class="n">white</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">250</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">250</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">250</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>              <span class="c1"># define white color (4 channel)</span>
<span class="c1">#newcolors[26:230, :] = white                                 # mask all correlations less than abs(0.8)</span>
<span class="c1">#newcolors[56:200, :] = white                                 # mask all correlations less than abs(0.6)</span>
<span class="n">newcolors</span><span class="p">[</span><span class="mi">76</span><span class="p">:</span><span class="mi">180</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">white</span>                                  <span class="c1"># mask all correlations less than abs(0.4)</span>
<span class="n">signif</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">newcolors</span><span class="p">)</span>                            <span class="c1"># assign as listed colormap</span>
         
<span class="n">my_colormap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'inferno'</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>                 <span class="c1"># make a custom colormap</span>
<span class="n">newcolors</span> <span class="o">=</span> <span class="n">my_colormap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>               <span class="c1"># define colormap space</span>
<span class="n">white</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">250</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">250</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">250</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>              <span class="c1"># define white color (4 channel)</span>
<span class="c1">#newcolors[26:230, :] = white                                 # mask all correlations less than abs(0.8)</span>
<span class="n">newcolors</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">12</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">white</span>                                    <span class="c1"># mask all correlations less than abs(0.6)</span>
<span class="c1">#newcolors[86:170, :] = white                                 # mask all correlations less than abs(0.4)</span>
<span class="n">sign1</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">newcolors</span><span class="p">)</span>                             <span class="c1"># assign as listed colormap</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="declare-functions">
<h2>Declare Functions</h2>
<p>Here‚Äôs a couple of functions to assist with calculating metrics for ranking and other plots:</p>
<ul class="simple">
<li><p><strong>plot_corr</strong> - plot a correlation matrix</p></li>
<li><p><strong>partial_corr</strong> - partial correlation coefficient</p></li>
<li><p><strong>semipar_corr</strong> - semipartial correlation coefficient</p></li>
<li><p><strong>mutual_matrix</strong> - mutual information matrix, matrix of all pairwise mutual information</p></li>
<li><p><strong>mutual_information_objective</strong> - my modified version of the MRMR loss function (Ixy - average(Ixx)) for feature ranking (uses all other predictor features)</p></li>
<li><p><strong>delta_mutual_information_quotient</strong> - change in mutual information quotient by adding and removing a specific feature (uses all other predictor features for the comparison)</p></li>
<li><p><strong>weighted_avg_and_std</strong> - average and standard deviation account for data weights</p></li>
<li><p><strong>weighted_percentile</strong> - percentile accounting for data weights</p></li>
<li><p><strong>histogram_bounds</strong> - add confidence intervals to histograms</p></li>
<li><p><strong>add_grid</strong> - convenience function to add major and minor gridlines to improve plot interpretability</p></li>
</ul>
<p>Here are the functions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">mmin</span><span class="p">,</span><span class="n">mmax</span><span class="p">,</span><span class="n">nominal</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span> <span class="c1"># feature ranking plot</span>
    <span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">);</span> <span class="n">mask_low</span> <span class="o">=</span> <span class="n">nominal</span><span class="o">-</span><span class="n">mask</span><span class="o">*</span><span class="p">(</span><span class="n">nominal</span><span class="o">-</span><span class="n">mmin</span><span class="p">);</span> <span class="n">mask_high</span> <span class="o">=</span> <span class="n">nominal</span><span class="o">+</span><span class="n">mask</span><span class="o">*</span><span class="p">(</span><span class="n">mmax</span><span class="o">-</span><span class="n">nominal</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">mpred</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span><span class="s1">'r--'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mpred</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&lt;</span> <span class="n">nominal</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'dodgerblue'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mpred</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&gt;</span> <span class="n">nominal</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'lightcoral'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">mpred</span><span class="p">,</span><span class="n">mask_low</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&lt;</span> <span class="n">mask_low</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">mpred</span><span class="p">,</span><span class="n">mask_high</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&gt;</span> <span class="n">mask_high</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">mmin</span><span class="p">,</span><span class="n">mmax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">mpred</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">();</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mf">270.0</span><span class="p">)</span>
    <span class="k">return</span>

<span class="k">def</span> <span class="nf">plot_corr</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">limits</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span>                 <span class="c1"># plots a graphical correlation matrix </span>
    <span class="n">my_colormap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'RdBu_r'</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>          
    <span class="n">newcolors</span> <span class="o">=</span> <span class="n">my_colormap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
    <span class="n">white</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">white_low</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">128</span> <span class="o">-</span> <span class="n">mask</span><span class="o">*</span><span class="mi">128</span><span class="p">);</span> <span class="n">white_high</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">128</span><span class="o">+</span><span class="n">mask</span><span class="o">*</span><span class="mi">128</span><span class="p">)</span>
    <span class="n">newcolors</span><span class="p">[</span><span class="n">white_low</span><span class="p">:</span><span class="n">white_high</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">white</span>                <span class="c1"># mask all correlations less than abs(0.8)</span>
    <span class="n">newcmp</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">newcolors</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="n">fignum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">vmin</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="n">limits</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">limits</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">newcmp</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">);</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s1">'bottom'</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">tick_bottom</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">orientation</span> <span class="o">=</span> <span class="s1">'vertical'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mf">270.0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">partial_corr</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>                                          <span class="c1"># partial correlation by Fabian Pedregosa-Izquierdo, f@bianp.net</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">P_corr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="n">P_corr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
            <span class="n">idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">idx</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">beta_i</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">C</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">],</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">beta_j</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">C</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">],</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">res_j</span> <span class="o">=</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span> <span class="n">beta_i</span><span class="p">)</span>
            <span class="n">res_i</span> <span class="o">=</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta_j</span><span class="p">)</span>
            <span class="n">corr</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">res_i</span><span class="p">,</span> <span class="n">res_j</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">P_corr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span>
            <span class="n">P_corr</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span>
    <span class="k">return</span> <span class="n">P_corr</span>

<span class="k">def</span> <span class="nf">semipartial_corr</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>                                      <span class="c1"># Michael Pyrcz modified the function above by Fabian Pedregosa-Izquierdo, f@bianp.net for semipartial correlation</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">P_corr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="n">P_corr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
            <span class="n">idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">idx</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">beta_i</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">C</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">],</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">res_j</span> <span class="o">=</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span> <span class="n">beta_i</span><span class="p">)</span>
            <span class="n">res_i</span> <span class="o">=</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> 
            <span class="n">corr</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">res_i</span><span class="p">,</span> <span class="n">res_j</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">P_corr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span>
            <span class="n">P_corr</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span>
    <span class="k">return</span> <span class="n">P_corr</span>

<span class="k">def</span> <span class="nf">mutual_matrix</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">features</span><span class="p">):</span>                               <span class="c1"># calculate mutual information matrix</span>
    <span class="n">mutual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ifeature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">jfeature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
                <span class="n">mutual</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mutual</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">mutual</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ifeature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
        <span class="n">mutual</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">return</span> <span class="n">mutual</span>

<span class="k">def</span> <span class="nf">mutual_information_objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>                        <span class="c1"># modified from MRMR loss function, Ixy - average(Ixx)</span>
    <span class="n">mutual_information_quotient</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">icol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">Vx</span> <span class="o">=</span> <span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">Ixx_mat</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">mcol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">m</span><span class="p">:</span>
                <span class="n">Ixx_mat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>
        <span class="n">Wx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">Ixx_mat</span><span class="p">)</span>
        <span class="n">mutual_information_quotient</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Vx</span><span class="o">/</span><span class="n">Wx</span><span class="p">)</span>
    <span class="n">mutual_information_quotient</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">mutual_information_quotient</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mutual_information_quotient</span>

<span class="k">def</span> <span class="nf">delta_mutual_information_quotient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>                   <span class="c1"># standard mutual information quotient</span>
    <span class="n">delta_mutual_information_quotient</span> <span class="o">=</span> <span class="p">[]</span>               
    
    <span class="n">Ixy</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">mcol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">Ixy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>
    <span class="n">Vs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">Ixy</span><span class="p">)</span>
    <span class="n">Ixx</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">mcol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">ncol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
            <span class="n">Ixx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>
    <span class="n">Ws</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">Ixx</span><span class="p">)</span> 
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">icol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>          
        <span class="n">Ixy_s</span> <span class="o">=</span> <span class="p">[]</span>                                          
        <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">mcol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">m</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>
                <span class="n">Ixy_s</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>
        <span class="n">Vs_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">Ixy_s</span><span class="p">)</span>
        <span class="n">Ixx_s</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">mcol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">m</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">ncol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">n</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>
                        <span class="n">Ixx_s</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>                  
        <span class="n">Ws_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">Ixx_s</span><span class="p">)</span>
        <span class="n">delta_mutual_information_quotient</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">Vs</span><span class="o">/</span><span class="n">Ws</span><span class="p">)</span><span class="o">-</span><span class="p">(</span><span class="n">Vs_s</span><span class="o">/</span><span class="n">Ws_s</span><span class="p">))</span>
    
    <span class="n">delta_mutual_information_quotient</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">delta_mutual_information_quotient</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  
    <span class="k">return</span> <span class="n">delta_mutual_information_quotient</span>

<span class="k">def</span> <span class="nf">weighted_avg_and_std</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>                    <span class="c1"># calculate weighted statistics (Eric O Lebigot, stack overflow)</span>
    <span class="n">average</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">((</span><span class="n">values</span><span class="o">-</span><span class="n">average</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">average</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">weighted_percentile</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">perc</span><span class="p">):</span>                 <span class="c1"># calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) </span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> 
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> 
    <span class="n">cdf</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">perc</span><span class="p">,</span> <span class="n">cdf</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">histogram_bounds</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">color</span><span class="p">):</span>                   <span class="c1"># add uncertainty bounds to a histogram          </span>
    <span class="n">p10</span> <span class="o">=</span> <span class="n">weighted_percentile</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="mf">0.1</span><span class="p">);</span> <span class="n">avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">);</span> <span class="n">p90</span> <span class="o">=</span> <span class="n">weighted_percentile</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="mf">0.9</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">p10</span><span class="p">,</span><span class="n">p10</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">45</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">'dashed'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">avg</span><span class="p">,</span><span class="n">avg</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">45</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">p90</span><span class="p">,</span><span class="n">p90</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">45</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">'dashed'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add_grid</span><span class="p">():</span>                                               <span class="c1"># add major and minor gridlines</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="set-the-working-directory">
<h2>Set the Working Directory</h2>
<p>I always like to do this so I don‚Äôt lose files and to simplify subsequent read and writes (avoid including the full address each time).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1">#os.chdir("d:/PGE383")                                   # set the working directory</span>
</pre></div>
</div>
</div>
</div>
<p>You will have to update the part in quotes with your own working directory and the format is different on a Mac (e.g. ‚Äú~/PGE‚Äù).</p>
</section>
<section id="loading-tabular-data">
<h2>Loading Tabular Data</h2>
<p>Here‚Äôs the command to load our comma delimited data file in to a Pandas‚Äô DataFrame object.</p>
<p>Let‚Äôs load the provided multivariate, spatial dataset ‚Äòunconv_MV.csv‚Äô. This dataset has variables from 1,000 unconventional wells including:</p>
<ul class="simple">
<li><p>well average porosity</p></li>
<li><p>log transform of permeability (to linearize the relationships with other variables)</p></li>
<li><p>acoustic impedance (kg/m^3 x m/s x 10^6)</p></li>
<li><p>brittleness ratio (%)</p></li>
<li><p>total organic carbon (%)</p></li>
<li><p>vitrinite reflectance (%)</p></li>
<li><p>initial production 90 day average (MCFPD).</p></li>
</ul>
<p>Note, the dataset is synthetic.</p>
<p>We load it with the pandas ‚Äòread_csv‚Äô function into a DataFrame we called ‚Äòmy_data‚Äô and then preview it to make sure it loaded correctly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">idata</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">if</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv'</span><span class="p">)</span> <span class="c1"># load data from Dr. Pyrcz's GitHub repository  </span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="s1">'Prod'</span>                                             <span class="c1"># specify the response feature</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">deep</span> <span class="o">=</span> <span class="kc">True</span><span class="p">);</span> <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'Well'</span><span class="p">,</span><span class="n">response</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="s1">'columns'</span><span class="p">)</span> <span class="c1"># make predictor and response DataFrames</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">response</span><span class="p">]</span>
    
    <span class="n">features</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="p">[</span><span class="n">Y</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>               <span class="c1"># store the names of the features</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">name</span>
    
    <span class="n">xmin</span> <span class="o">=</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">10.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.9</span><span class="p">];</span> <span class="n">xmax</span> <span class="o">=</span> <span class="p">[</span><span class="mf">24.0</span><span class="p">,</span><span class="mf">10.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">,</span><span class="mf">85.0</span><span class="p">,</span><span class="mf">2.2</span><span class="p">,</span><span class="mf">2.9</span><span class="p">]</span> <span class="c1"># set the minimum and maximum values for plotting</span>
    <span class="n">Ymin</span> <span class="o">=</span> <span class="mf">500.0</span><span class="p">;</span> <span class="n">Ymax</span> <span class="o">=</span> <span class="mf">9000.0</span>
    
    <span class="n">predlabel</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Porosity (%)'</span><span class="p">,</span><span class="s1">'Permeability (mD)'</span><span class="p">,</span><span class="s1">'Acoustic Impedance (kg/m2s*10^6)'</span><span class="p">,</span><span class="s1">'Brittleness Ratio (%)'</span><span class="p">,</span> <span class="c1"># set the names for plotting</span>
                 <span class="s1">'Total Organic Carbon (%)'</span><span class="p">,</span><span class="s1">'Vitrinite Reflectance (%)'</span><span class="p">]</span>
    <span class="n">resplabel</span> <span class="o">=</span> <span class="s1">'Normalized Initial Production (MCFPD)'</span>
    
    <span class="n">predtitle</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Porosity'</span><span class="p">,</span><span class="s1">'Permeability'</span><span class="p">,</span><span class="s1">'Acoustic Impedance'</span><span class="p">,</span><span class="s1">'Brittleness Ratio'</span><span class="p">,</span> <span class="c1"># set the units for plotting</span>
                 <span class="s1">'Total Organic Carbon'</span><span class="p">,</span><span class="s1">'Vitrinite Reflectance'</span><span class="p">]</span>
    <span class="n">resptitle</span> <span class="o">=</span> <span class="s1">'Normalized Initial Production'</span>
    
    <span class="n">featurelabel</span> <span class="o">=</span> <span class="n">predlabel</span> <span class="o">+</span> <span class="p">[</span><span class="n">resplabel</span><span class="p">]</span>                        <span class="c1"># make feature labels and titles for concise code</span>
    <span class="n">featuretitle</span> <span class="o">=</span> <span class="n">predtitle</span> <span class="o">+</span> <span class="p">[</span><span class="n">resptitle</span><span class="p">]</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
    
<span class="c1"># elif idata == 1:</span>
<span class="c1">#     names = {'Porosity':'Por'}</span>
    
<span class="c1">#     df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository  </span>
<span class="c1">#     df = df.rename(columns=names)</span>
<span class="c1">#     df['Por'] = df['Por'] * 100.0; df['AI'] = df['AI'] / 1000.0; </span>
<span class="c1">#     df.drop('Unnamed: 0',axis=1,inplace=True) </span>
    
<span class="c1">#     features = df.columns.values.tolist()                          # store the names of the features</span>

<span class="c1">#     xmin = [0.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax = [10000.0,10000.0,1.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting</span>
    
<span class="c1">#     flabel = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',</span>
<span class="c1">#               'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting</span>

<span class="c1">#     ftitle = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',</span>
<span class="c1">#               'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']</span>

<span class="k">elif</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>  
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv'</span><span class="p">)</span> <span class="c1"># load data from Dr. Pyrcz's GitHub repository  </span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="s1">'CumulativeOil'</span>                                             <span class="c1"># specify the response feature</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">deep</span> <span class="o">=</span> <span class="kc">True</span><span class="p">);</span> <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'Well_ID'</span><span class="p">,</span><span class="s1">'X'</span><span class="p">,</span><span class="s1">'Y'</span><span class="p">,</span><span class="n">response</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="s1">'columns'</span><span class="p">)</span> <span class="c1"># make predictor and response DataFrames</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">response</span><span class="p">]</span>
    
    <span class="n">features</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="p">[</span><span class="n">Y</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>               <span class="c1"># store the names of the features</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">name</span>

    <span class="n">xmin</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">6.5</span><span class="p">,</span><span class="mf">1.4</span><span class="p">,</span><span class="mf">1600.0</span><span class="p">,</span><span class="mf">10.0</span><span class="p">,</span><span class="mf">1300.0</span><span class="p">,</span><span class="mf">1.6</span><span class="p">];</span> <span class="n">xmax</span> <span class="o">=</span> <span class="p">[</span><span class="mf">75.0</span><span class="p">,</span><span class="mf">10000.0</span><span class="p">,</span><span class="mf">10000.0</span><span class="p">,</span><span class="mf">19.0</span><span class="p">,</span><span class="mf">500.0</span><span class="p">,</span><span class="mf">8.3</span><span class="p">,</span><span class="mf">3.6</span><span class="p">,</span><span class="mf">6200.0</span><span class="p">,</span><span class="mf">50.0</span><span class="p">,</span><span class="mf">2000.0</span><span class="p">,</span><span class="mf">12.0</span><span class="p">]</span> <span class="c1"># set the minimum and maximum values for plotting</span>
    <span class="n">Ymin</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span> <span class="n">Ymax</span> <span class="o">=</span> <span class="mf">3000.0</span>
    
    <span class="n">predlabel</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Well (ID)'</span><span class="p">,</span><span class="s1">'X (m)'</span><span class="p">,</span><span class="s1">'Y (m)'</span><span class="p">,</span><span class="s1">'Porosity (fraction)'</span><span class="p">,</span><span class="s1">'Permeability (mD)'</span><span class="p">,</span><span class="s1">'Acoustic Impedance (kg/m2s*10^6)'</span><span class="p">,</span>
              <span class="s1">'Density (g/cm^3)'</span><span class="p">,</span><span class="s1">'Compressible velocity (m/s)'</span><span class="p">,</span><span class="s1">'Youngs modulus (GPa)'</span><span class="p">,</span> <span class="s1">'Shear velocity (m/s)'</span><span class="p">,</span> <span class="s1">'Shear modulus (GPa)'</span><span class="p">]</span> 
    <span class="n">resplabel</span> <span class="o">=</span> <span class="s1">'Cumulative Production (MSTB)'</span>
    
    <span class="n">predtitle</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Well'</span><span class="p">,</span><span class="s1">'X'</span><span class="p">,</span><span class="s1">'Y'</span><span class="p">,</span><span class="s1">'Porosity'</span><span class="p">,</span><span class="s1">'Permeability'</span><span class="p">,</span><span class="s1">'Acoustic Impedance'</span><span class="p">,</span>
              <span class="s1">'Density (g/cm^3)'</span><span class="p">,</span><span class="s1">'Compressible velocity'</span><span class="p">,</span><span class="s1">'Youngs modulus'</span><span class="p">,</span> <span class="s1">'Shear velocity'</span><span class="p">,</span> <span class="s1">'Shear modulus'</span><span class="p">]</span> 
    <span class="n">resptitle</span> <span class="o">=</span> <span class="s1">'Cumulative Production'</span>
    
    <span class="n">featurelabel</span> <span class="o">=</span> <span class="n">predlabel</span> <span class="o">+</span> <span class="p">[</span><span class="n">resplabel</span><span class="p">]</span>                        <span class="c1"># make feature labels and titles for concise code</span>
    <span class="n">featuretitle</span> <span class="o">=</span> <span class="n">predtitle</span> <span class="o">+</span> <span class="p">[</span><span class="n">resptitle</span><span class="p">]</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span/><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">SSLCertVerificationError</span><span class="g g-Whitespace">                  </span>Traceback (most recent call last)
<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:1317,</span> in <span class="ni">AbstractHTTPHandler.do_open</span><span class="nt">(self, http_class, req, **http_conn_args)</span>
<span class="g g-Whitespace">   </span><span class="mi">1316</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1317</span>     <span class="n">h</span><span class="o">.</span><span class="n">request</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">get_method</span><span class="p">(),</span> <span class="n">req</span><span class="o">.</span><span class="n">selector</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">headers</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1318</span>               <span class="n">encode_chunked</span><span class="o">=</span><span class="n">req</span><span class="o">.</span><span class="n">has_header</span><span class="p">(</span><span class="s1">'Transfer-encoding'</span><span class="p">))</span>
<span class="g g-Whitespace">   </span><span class="mi">1319</span> <span class="k">except</span> <span class="ne">OSError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span> <span class="c1"># timeout error</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1230,</span> in <span class="ni">HTTPConnection.request</span><span class="nt">(self, method, url, body, headers, encode_chunked)</span>
<span class="g g-Whitespace">   </span><span class="mi">1229</span><span class="w"> </span><span class="sd">"""Send a complete request to the server."""</span>
<span class="ne">-&gt; </span><span class="mi">1230</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_request</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">headers</span><span class="p">,</span> <span class="n">encode_chunked</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1276,</span> in <span class="ni">HTTPConnection._send_request</span><span class="nt">(self, method, url, body, headers, encode_chunked)</span>
<span class="g g-Whitespace">   </span><span class="mi">1275</span>     <span class="n">body</span> <span class="o">=</span> <span class="n">_encode</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="s1">'body'</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1276</span> <span class="bp">self</span><span class="o">.</span><span class="n">endheaders</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">encode_chunked</span><span class="o">=</span><span class="n">encode_chunked</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1225,</span> in <span class="ni">HTTPConnection.endheaders</span><span class="nt">(self, message_body, encode_chunked)</span>
<span class="g g-Whitespace">   </span><span class="mi">1224</span>     <span class="k">raise</span> <span class="n">CannotSendHeader</span><span class="p">()</span>
<span class="ne">-&gt; </span><span class="mi">1225</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_output</span><span class="p">(</span><span class="n">message_body</span><span class="p">,</span> <span class="n">encode_chunked</span><span class="o">=</span><span class="n">encode_chunked</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1004,</span> in <span class="ni">HTTPConnection._send_output</span><span class="nt">(self, message_body, encode_chunked)</span>
<span class="g g-Whitespace">   </span><span class="mi">1003</span> <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[:]</span>
<span class="ne">-&gt; </span><span class="mi">1004</span> <span class="bp">self</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1006</span> <span class="k">if</span> <span class="n">message_body</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1007</span> 
<span class="g g-Whitespace">   </span><span class="mi">1008</span>     <span class="c1"># create a consistent interface to message_body</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:944,</span> in <span class="ni">HTTPConnection.send</span><span class="nt">(self, data)</span>
<span class="g g-Whitespace">    </span><span class="mi">943</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_open</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">944</span>     <span class="bp">self</span><span class="o">.</span><span class="n">connect</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">945</span> <span class="k">else</span><span class="p">:</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1399,</span> in <span class="ni">HTTPSConnection.connect</span><span class="nt">(self)</span>
<span class="g g-Whitespace">   </span><span class="mi">1397</span>     <span class="n">server_hostname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">host</span>
<span class="ne">-&gt; </span><span class="mi">1399</span> <span class="bp">self</span><span class="o">.</span><span class="n">sock</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">wrap_socket</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sock</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1400</span>                                       <span class="n">server_hostname</span><span class="o">=</span><span class="n">server_hostname</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\ssl.py:500,</span> in <span class="ni">SSLContext.wrap_socket</span><span class="nt">(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)</span>
<span class="g g-Whitespace">    </span><span class="mi">494</span> <span class="k">def</span> <span class="nf">wrap_socket</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sock</span><span class="p">,</span> <span class="n">server_side</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">495</span>                 <span class="n">do_handshake_on_connect</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">496</span>                 <span class="n">suppress_ragged_eofs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">497</span>                 <span class="n">server_hostname</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">498</span>     <span class="c1"># SSLSocket class handles server_hostname encoding before it calls</span>
<span class="g g-Whitespace">    </span><span class="mi">499</span>     <span class="c1"># ctx._wrap_socket()</span>
<span class="ne">--&gt; </span><span class="mi">500</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sslsocket_class</span><span class="o">.</span><span class="n">_create</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">501</span>         <span class="n">sock</span><span class="o">=</span><span class="n">sock</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">502</span>         <span class="n">server_side</span><span class="o">=</span><span class="n">server_side</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">503</span>         <span class="n">do_handshake_on_connect</span><span class="o">=</span><span class="n">do_handshake_on_connect</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">504</span>         <span class="n">suppress_ragged_eofs</span><span class="o">=</span><span class="n">suppress_ragged_eofs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">505</span>         <span class="n">server_hostname</span><span class="o">=</span><span class="n">server_hostname</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">506</span>         <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">507</span>         <span class="n">session</span><span class="o">=</span><span class="n">session</span>
<span class="g g-Whitespace">    </span><span class="mi">508</span>     <span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\ssl.py:1040,</span> in <span class="ni">SSLSocket._create</span><span class="nt">(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)</span>
<span class="g g-Whitespace">   </span><span class="mi">1039</span>             <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"do_handshake_on_connect should not be specified for non-blocking sockets"</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1040</span>         <span class="bp">self</span><span class="o">.</span><span class="n">do_handshake</span><span class="p">()</span>
<span class="g g-Whitespace">   </span><span class="mi">1041</span> <span class="k">except</span> <span class="p">(</span><span class="ne">OSError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\ssl.py:1309,</span> in <span class="ni">SSLSocket.do_handshake</span><span class="nt">(self, block)</span>
<span class="g g-Whitespace">   </span><span class="mi">1308</span>         <span class="bp">self</span><span class="o">.</span><span class="n">settimeout</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1309</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_sslobj</span><span class="o">.</span><span class="n">do_handshake</span><span class="p">()</span>
<span class="g g-Whitespace">   </span><span class="mi">1310</span> <span class="k">finally</span><span class="p">:</span>

<span class="ne">SSLCertVerificationError</span>: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)

<span class="n">During</span> <span class="n">handling</span> <span class="n">of</span> <span class="n">the</span> <span class="n">above</span> <span class="n">exception</span><span class="p">,</span> <span class="n">another</span> <span class="n">exception</span> <span class="n">occurred</span><span class="p">:</span>

<span class="ne">URLError</span><span class="g g-Whitespace">                                  </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">7</span><span class="p">],</span> <span class="n">line</span> <span class="mi">3</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">idata</span> <span class="o">=</span> <span class="mi">0</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="k">if</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="ne">----&gt; </span><span class="mi">3</span>     <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv'</span><span class="p">)</span> <span class="c1"># load data from Dr. Pyrcz's GitHub repository  </span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>     <span class="n">response</span> <span class="o">=</span> <span class="s1">'Prod'</span>                                             <span class="c1"># specify the response feature</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span>     <span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">deep</span> <span class="o">=</span> <span class="kc">True</span><span class="p">);</span> <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'Well'</span><span class="p">,</span><span class="n">response</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="s1">'columns'</span><span class="p">)</span> <span class="c1"># make predictor and response DataFrames</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:912,</span> in <span class="ni">read_csv</span><span class="nt">(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)</span>
<span class="g g-Whitespace">    </span><span class="mi">899</span> <span class="n">kwds_defaults</span> <span class="o">=</span> <span class="n">_refine_defaults_read</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">900</span>     <span class="n">dialect</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">901</span>     <span class="n">delimiter</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">908</span>     <span class="n">dtype_backend</span><span class="o">=</span><span class="n">dtype_backend</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">909</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">910</span> <span class="n">kwds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwds_defaults</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">912</span> <span class="k">return</span> <span class="n">_read</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="p">,</span> <span class="n">kwds</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:577,</span> in <span class="ni">_read</span><span class="nt">(filepath_or_buffer, kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">574</span> <span class="n">_validate_names</span><span class="p">(</span><span class="n">kwds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"names"</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">576</span> <span class="c1"># Create the parser.</span>
<span class="ne">--&gt; </span><span class="mi">577</span> <span class="n">parser</span> <span class="o">=</span> <span class="n">TextFileReader</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">579</span> <span class="k">if</span> <span class="n">chunksize</span> <span class="ow">or</span> <span class="n">iterator</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">580</span>     <span class="k">return</span> <span class="n">parser</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:1407,</span> in <span class="ni">TextFileReader.__init__</span><span class="nt">(self, f, engine, **kwds)</span>
<span class="g g-Whitespace">   </span><span class="mi">1404</span>     <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">[</span><span class="s2">"has_index_names"</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwds</span><span class="p">[</span><span class="s2">"has_index_names"</span><span class="p">]</span>
<span class="g g-Whitespace">   </span><span class="mi">1406</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span><span class="p">:</span> <span class="n">IOHandles</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="ne">-&gt; </span><span class="mi">1407</span> <span class="bp">self</span><span class="o">.</span><span class="n">_engine</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_engine</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:1661,</span> in <span class="ni">TextFileReader._make_engine</span><span class="nt">(self, f, engine)</span>
<span class="g g-Whitespace">   </span><span class="mi">1659</span>     <span class="k">if</span> <span class="s2">"b"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mode</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1660</span>         <span class="n">mode</span> <span class="o">+=</span> <span class="s2">"b"</span>
<span class="ne">-&gt; </span><span class="mi">1661</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="o">=</span> <span class="n">get_handle</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1662</span>     <span class="n">f</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1663</span>     <span class="n">mode</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1664</span>     <span class="n">encoding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"encoding"</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1665</span>     <span class="n">compression</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"compression"</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1666</span>     <span class="n">memory_map</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"memory_map"</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1667</span>     <span class="n">is_text</span><span class="o">=</span><span class="n">is_text</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1668</span>     <span class="n">errors</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"encoding_errors"</span><span class="p">,</span> <span class="s2">"strict"</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1669</span>     <span class="n">storage_options</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"storage_options"</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1670</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1671</span> <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="g g-Whitespace">   </span><span class="mi">1672</span> <span class="n">f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span><span class="o">.</span><span class="n">handle</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:716,</span> in <span class="ni">get_handle</span><span class="nt">(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">713</span>     <span class="n">codecs</span><span class="o">.</span><span class="n">lookup_error</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">715</span> <span class="c1"># open URLs</span>
<span class="ne">--&gt; </span><span class="mi">716</span> <span class="n">ioargs</span> <span class="o">=</span> <span class="n">_get_filepath_or_buffer</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">717</span>     <span class="n">path_or_buf</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">718</span>     <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">719</span>     <span class="n">compression</span><span class="o">=</span><span class="n">compression</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">720</span>     <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">721</span>     <span class="n">storage_options</span><span class="o">=</span><span class="n">storage_options</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">722</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">724</span> <span class="n">handle</span> <span class="o">=</span> <span class="n">ioargs</span><span class="o">.</span><span class="n">filepath_or_buffer</span>
<span class="g g-Whitespace">    </span><span class="mi">725</span> <span class="n">handles</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">BaseBuffer</span><span class="p">]</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:368,</span> in <span class="ni">_get_filepath_or_buffer</span><span class="nt">(filepath_or_buffer, encoding, compression, mode, storage_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">366</span> <span class="c1"># assuming storage_options is to be interpreted as headers</span>
<span class="g g-Whitespace">    </span><span class="mi">367</span> <span class="n">req_info</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">storage_options</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">368</span> <span class="k">with</span> <span class="n">urlopen</span><span class="p">(</span><span class="n">req_info</span><span class="p">)</span> <span class="k">as</span> <span class="n">req</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">369</span>     <span class="n">content_encoding</span> <span class="o">=</span> <span class="n">req</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"Content-Encoding"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">370</span>     <span class="k">if</span> <span class="n">content_encoding</span> <span class="o">==</span> <span class="s2">"gzip"</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">371</span>         <span class="c1"># Override compression based on Content-Encoding header</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:270,</span> in <span class="ni">urlopen</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">264</span><span class="w"> </span><span class="sd">"""</span>
<span class="g g-Whitespace">    </span><span class="mi">265</span><span class="sd"> Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of</span>
<span class="g g-Whitespace">    </span><span class="mi">266</span><span class="sd"> the stdlib.</span>
<span class="g g-Whitespace">    </span><span class="mi">267</span><span class="sd"> """</span>
<span class="g g-Whitespace">    </span><span class="mi">268</span> <span class="kn">import</span> <span class="nn">urllib.request</span>
<span class="ne">--&gt; </span><span class="mi">270</span> <span class="k">return</span> <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:222,</span> in <span class="ni">urlopen</span><span class="nt">(url, data, timeout, cafile, capath, cadefault, context)</span>
<span class="g g-Whitespace">    </span><span class="mi">220</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">221</span>     <span class="n">opener</span> <span class="o">=</span> <span class="n">_opener</span>
<span class="ne">--&gt; </span><span class="mi">222</span> <span class="k">return</span> <span class="n">opener</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">timeout</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:525,</span> in <span class="ni">OpenerDirector.open</span><span class="nt">(self, fullurl, data, timeout)</span>
<span class="g g-Whitespace">    </span><span class="mi">522</span>     <span class="n">req</span> <span class="o">=</span> <span class="n">meth</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">524</span> <span class="n">sys</span><span class="o">.</span><span class="n">audit</span><span class="p">(</span><span class="s1">'urllib.Request'</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">full_url</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">headers</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">get_method</span><span class="p">())</span>
<span class="ne">--&gt; </span><span class="mi">525</span> <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_open</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">527</span> <span class="c1"># post-process response</span>
<span class="g g-Whitespace">    </span><span class="mi">528</span> <span class="n">meth_name</span> <span class="o">=</span> <span class="n">protocol</span><span class="o">+</span><span class="s2">"_response"</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:542,</span> in <span class="ni">OpenerDirector._open</span><span class="nt">(self, req, data)</span>
<span class="g g-Whitespace">    </span><span class="mi">539</span>     <span class="k">return</span> <span class="n">result</span>
<span class="g g-Whitespace">    </span><span class="mi">541</span> <span class="n">protocol</span> <span class="o">=</span> <span class="n">req</span><span class="o">.</span><span class="n">type</span>
<span class="ne">--&gt; </span><span class="mi">542</span> <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_chain</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle_open</span><span class="p">,</span> <span class="n">protocol</span><span class="p">,</span> <span class="n">protocol</span> <span class="o">+</span>
<span class="g g-Whitespace">    </span><span class="mi">543</span>                           <span class="s1">'_open'</span><span class="p">,</span> <span class="n">req</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">544</span> <span class="k">if</span> <span class="n">result</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">545</span>     <span class="k">return</span> <span class="n">result</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:502,</span> in <span class="ni">OpenerDirector._call_chain</span><span class="nt">(self, chain, kind, meth_name, *args)</span>
<span class="g g-Whitespace">    </span><span class="mi">500</span> <span class="k">for</span> <span class="n">handler</span> <span class="ow">in</span> <span class="n">handlers</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">501</span>     <span class="n">func</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">handler</span><span class="p">,</span> <span class="n">meth_name</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">502</span>     <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">503</span>     <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">504</span>         <span class="k">return</span> <span class="n">result</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:1360,</span> in <span class="ni">HTTPSHandler.https_open</span><span class="nt">(self, req)</span>
<span class="g g-Whitespace">   </span><span class="mi">1359</span> <span class="k">def</span> <span class="nf">https_open</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">req</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1360</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_open</span><span class="p">(</span><span class="n">http</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">HTTPSConnection</span><span class="p">,</span> <span class="n">req</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1361</span>         <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_context</span><span class="p">,</span> <span class="n">check_hostname</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_check_hostname</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:1320,</span> in <span class="ni">AbstractHTTPHandler.do_open</span><span class="nt">(self, http_class, req, **http_conn_args)</span>
<span class="g g-Whitespace">   </span><span class="mi">1317</span>         <span class="n">h</span><span class="o">.</span><span class="n">request</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">get_method</span><span class="p">(),</span> <span class="n">req</span><span class="o">.</span><span class="n">selector</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">headers</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1318</span>                   <span class="n">encode_chunked</span><span class="o">=</span><span class="n">req</span><span class="o">.</span><span class="n">has_header</span><span class="p">(</span><span class="s1">'Transfer-encoding'</span><span class="p">))</span>
<span class="g g-Whitespace">   </span><span class="mi">1319</span>     <span class="k">except</span> <span class="ne">OSError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span> <span class="c1"># timeout error</span>
<span class="ne">-&gt; </span><span class="mi">1320</span>         <span class="k">raise</span> <span class="n">URLError</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1321</span>     <span class="n">r</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">getresponse</span><span class="p">()</span>
<span class="g g-Whitespace">   </span><span class="mi">1322</span> <span class="k">except</span><span class="p">:</span>

<span class="ne">URLError</span>: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)&gt;
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can also establish the feature ranges for plotting. We could calculate the feature range directly from the data with code like this:</p></li>
</ul>
<div class="highlight-p notranslate"><div class="highlight"><pre><span/>Pormin = np.min(df['Por'].values)                             # extract ndarray of data table column
Pormax = np.max(df['Por'].values)                             # and calculate min and max
</pre></div>
</div>
<p>but, this would not result in easy to understand color bars and axis scales, let‚Äôs pick convenient round numbers. We will also declare feature labels for ease of plotting.</p>
</section>
<section id="visualize-the-dataframe">
<h2>Visualize the DataFrame</h2>
<p>Visualizing the DataFrame is useful first check of the data.</p>
<ul class="simple">
<li><p>many things can go wrong, e.g., we loaded the wrong data, all the features did not load, etc.</p></li>
</ul>
<p>We can preview by utilizing the ‚Äòhead‚Äô DataFrame member function (with a nice and clean format, see below).</p>
<ul class="simple">
<li><p>add parameter ‚Äòn=13‚Äô to see the first 13 rows of the dataset.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>                                                 <span class="c1"># we could also use this command for a table preview </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Well</th>
      <th>Por</th>
      <th>Perm</th>
      <th>AI</th>
      <th>Brittle</th>
      <th>TOC</th>
      <th>VR</th>
      <th>Prod</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>12.08</td>
      <td>2.92</td>
      <td>2.80</td>
      <td>81.40</td>
      <td>1.16</td>
      <td>2.31</td>
      <td>1695.360819</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>12.38</td>
      <td>3.53</td>
      <td>3.22</td>
      <td>46.17</td>
      <td>0.89</td>
      <td>1.88</td>
      <td>3007.096063</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>14.02</td>
      <td>2.59</td>
      <td>4.01</td>
      <td>72.80</td>
      <td>0.89</td>
      <td>2.72</td>
      <td>2531.938259</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>17.67</td>
      <td>6.75</td>
      <td>2.63</td>
      <td>39.81</td>
      <td>1.08</td>
      <td>1.88</td>
      <td>5288.514854</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>17.52</td>
      <td>4.57</td>
      <td>3.18</td>
      <td>10.94</td>
      <td>1.51</td>
      <td>1.90</td>
      <td>2859.469624</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>14.53</td>
      <td>4.81</td>
      <td>2.69</td>
      <td>53.60</td>
      <td>0.94</td>
      <td>1.67</td>
      <td>4017.374438</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>13.49</td>
      <td>3.60</td>
      <td>2.93</td>
      <td>63.71</td>
      <td>0.80</td>
      <td>1.85</td>
      <td>2952.812773</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>11.58</td>
      <td>3.03</td>
      <td>3.25</td>
      <td>53.00</td>
      <td>0.69</td>
      <td>1.93</td>
      <td>2670.933846</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>12.52</td>
      <td>2.72</td>
      <td>2.43</td>
      <td>65.77</td>
      <td>0.95</td>
      <td>1.98</td>
      <td>2474.048178</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>13.25</td>
      <td>3.94</td>
      <td>3.71</td>
      <td>66.20</td>
      <td>1.14</td>
      <td>2.65</td>
      <td>2722.893266</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>15.04</td>
      <td>4.39</td>
      <td>2.22</td>
      <td>61.11</td>
      <td>1.08</td>
      <td>1.77</td>
      <td>3828.247174</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>16.19</td>
      <td>6.30</td>
      <td>2.29</td>
      <td>49.10</td>
      <td>1.53</td>
      <td>1.86</td>
      <td>5095.810104</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>16.82</td>
      <td>5.42</td>
      <td>2.80</td>
      <td>66.65</td>
      <td>1.17</td>
      <td>1.98</td>
      <td>4091.637316</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="summary-statistics-for-tabular-data">
<h2>Summary Statistics for Tabular Data</h2>
<p>There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames. The describe command provides count, mean, minimum, maximum, and quartiles all in a nice data table.</p>
<ul class="simple">
<li><p>We use transpose just to flip the table so that features are on the rows and the statistics are on the columns.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>                                     <span class="c1"># calculate summary statistics for the data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Well</th>
      <td>200.0</td>
      <td>100.500000</td>
      <td>57.879185</td>
      <td>1.000000</td>
      <td>50.750000</td>
      <td>100.500000</td>
      <td>150.250000</td>
      <td>200.000000</td>
    </tr>
    <tr>
      <th>Por</th>
      <td>200.0</td>
      <td>14.991150</td>
      <td>2.971176</td>
      <td>6.550000</td>
      <td>12.912500</td>
      <td>15.070000</td>
      <td>17.402500</td>
      <td>23.550000</td>
    </tr>
    <tr>
      <th>Perm</th>
      <td>200.0</td>
      <td>4.330750</td>
      <td>1.731014</td>
      <td>1.130000</td>
      <td>3.122500</td>
      <td>4.035000</td>
      <td>5.287500</td>
      <td>9.870000</td>
    </tr>
    <tr>
      <th>AI</th>
      <td>200.0</td>
      <td>2.968850</td>
      <td>0.566885</td>
      <td>1.280000</td>
      <td>2.547500</td>
      <td>2.955000</td>
      <td>3.345000</td>
      <td>4.630000</td>
    </tr>
    <tr>
      <th>Brittle</th>
      <td>200.0</td>
      <td>48.161950</td>
      <td>14.129455</td>
      <td>10.940000</td>
      <td>37.755000</td>
      <td>49.510000</td>
      <td>58.262500</td>
      <td>84.330000</td>
    </tr>
    <tr>
      <th>TOC</th>
      <td>200.0</td>
      <td>0.990450</td>
      <td>0.481588</td>
      <td>-0.190000</td>
      <td>0.617500</td>
      <td>1.030000</td>
      <td>1.350000</td>
      <td>2.180000</td>
    </tr>
    <tr>
      <th>VR</th>
      <td>200.0</td>
      <td>1.964300</td>
      <td>0.300827</td>
      <td>0.930000</td>
      <td>1.770000</td>
      <td>1.960000</td>
      <td>2.142500</td>
      <td>2.870000</td>
    </tr>
    <tr>
      <th>Prod</th>
      <td>200.0</td>
      <td>3864.407081</td>
      <td>1553.277558</td>
      <td>839.822063</td>
      <td>2686.227611</td>
      <td>3604.303506</td>
      <td>4752.637555</td>
      <td>8590.384044</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Ranking features is really an effort to understand the features and their relationships with each other. We will start with basic data visualization and move to more complicated methods such are partial correlation and recursive feature elimination.</p>
</section>
<section id="coverage">
<h2>Coverage</h2>
<p>Let‚Äôs start with the concept of feature coverage.</p>
<ul class="simple">
<li><p>If a feature is available over a small proportion of the samples then we may not want to include it as it will result in issues with feature imputation, estimation of missing data.</p></li>
<li><p>By removing a couple features with poor coverage we may improve our model because there are limitations with feature imputation, feature imputation can actually impose bias in statistics and additional error in our prediction models</p></li>
<li><p>if likewise deletion is applied to deal with missing values, features with low coverage result in a lot of removed data!</p></li>
</ul>
<p>Let‚Äôs start with a bar chart with the proportion of missing records:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s1">'bar'</span><span class="p">)</span>                <span class="c1"># calculate DataFrame with percentage missing by feature</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Percentage of Missing Values'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Data Completeness'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6b56bb06fe01cd87f3513b8ebe3a71bb409619f34b70e4dd2bd50d61e5c4e62d.png" src="../Images/e5a834b70ca47ad151aee5749adc53ce.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/6b56bb06fe01cd87f3513b8ebe3a71bb409619f34b70e4dd2bd50d61e5c4e62d.png"/>
</div>
</div>
<p>For the provided example dataset the plot should be empty. There are no missing data so the ‚ÄòProportion of Missing Records‚Äô is 0.0 for all features.</p>
<p>If you wanted to test this plot with some missing data, run this code first:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">proportion_NaN</span> <span class="o">=</span> <span class="mf">0.1</span>                                    <span class="c1"># proportion of values in DataFrame to remove</span>

<span class="n">remove</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">proportion_NaN</span>    <span class="c1"># make the boolean array for removal</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Fraction of removed values in mask ndarray = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">remove</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">remove</span><span class="o">.</span><span class="n">size</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span> <span class="o">+</span> <span class="s1">'.'</span><span class="p">)</span>

<span class="n">df_mask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">mask</span><span class="p">(</span><span class="n">remove</span><span class="p">)</span>                               <span class="c1"># make a new DataFrame with specified proportion removed</span>
</pre></div>
</div>
<p>Remove this code and reload the data to continue to get consistent results with the discussions below.</p>
<p>This does not tell the whole story. For example, if 20% of feature A is missing and 20% of feature B is missing are those the same and different samples. This has a huge impact if you perform likewise deletion.</p>
<ul class="simple">
<li><p>If there is not too much data then we can actually visualize data coverage over all samples and features in a boolean table like this.</p></li>
<li><p>This method may identify specific samples with many missing features that may be removed to improve overall coverage or other trends or structures in the missing data that may result in sampling bias.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df_temp</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                  <span class="c1"># make a deep copy of the DataFrame</span>
<span class="n">df_bool</span> <span class="o">=</span> <span class="n">df_temp</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span>                                    <span class="c1"># true is value, false if NaN</span>
<span class="c1">#df_bool = df_bool.set_index(df_temp.pop('UWI'))              # set the index / feature for the heat map y column</span>
<span class="n">heat</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">df_bool</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="p">[</span><span class="s1">'r'</span><span class="p">,</span><span class="s1">'w'</span><span class="p">],</span> <span class="n">annot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">'.0f'</span><span class="p">,</span><span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">linecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># make the binary heat map, no bins</span>
<span class="n">heat</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">heat</span><span class="o">.</span><span class="n">get_xticklabels</span><span class="p">(),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">heat</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">heat</span><span class="o">.</span><span class="n">get_yticklabels</span><span class="p">(),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">heat</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Data Completeness Heatmap'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span> <span class="n">heat</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Feature'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">);</span> <span class="n">heat</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Sample (Index)'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.8</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.6</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5849f3ca4f8301f49db8799952b591b62475a866fcaa2646497f042118e36aa0.png" src="../Images/9b8b2b5fe0360995f89df5950e3ed23d.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/5849f3ca4f8301f49db8799952b591b62475a866fcaa2646497f042118e36aa0.png"/>
</div>
</div>
<p>Once again this plot should be quite boring for the provided dataset with perfect coverage, every cell should be filled in red.</p>
<ul class="simple">
<li><p>add the code to remove some records to test this plot. White cells are missing records.</p></li>
</ul>
<section id="feature-imputation">
<h3>Feature Imputation</h3>
<p>See the chapter on feature imputation to learn what to do about missing data.</p>
<p>For now a concise treatment here, we will just apply likewise deletion and move on.</p>
<ul class="simple">
<li><p>we remove all samples with any missing feature values. While this is quite simple, it is a sledge hammer approach to ensure perfect coverage required by feature ranking methods that we are about to demonstrate. Please check out the other methods in the linked workflow above.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">how</span><span class="o">=</span><span class="s1">'any'</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                      <span class="c1"># likewise deletion</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="summary-statistics">
<h2>Summary Statistics</h2>
<p>In any multivariate work we should start with the univariate analysis, summary statistics of one variable at a time. The summary statistic ranking method is qualitative, we are asking:</p>
<ul class="simple">
<li><p>Are there data issues?</p></li>
<li><p>Do we trust the features? Do we trust the features all equally?</p></li>
<li><p>Are there issues that need to be taken care of before we develop any multivariate workflows?</p></li>
</ul>
<p>There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames. The describe command provides count, mean, minimum, maximum, and quartiles all in a compact data table. We use transpose() command to flip the table so that features are on the rows and the statistics are on the columns.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>                                     <span class="c1"># DataFrame summary statistics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Well</th>
      <td>200.0</td>
      <td>100.500000</td>
      <td>57.879185</td>
      <td>1.000000</td>
      <td>50.750000</td>
      <td>100.500000</td>
      <td>150.250000</td>
      <td>200.000000</td>
    </tr>
    <tr>
      <th>Por</th>
      <td>200.0</td>
      <td>14.991150</td>
      <td>2.971176</td>
      <td>6.550000</td>
      <td>12.912500</td>
      <td>15.070000</td>
      <td>17.402500</td>
      <td>23.550000</td>
    </tr>
    <tr>
      <th>Perm</th>
      <td>200.0</td>
      <td>4.330750</td>
      <td>1.731014</td>
      <td>1.130000</td>
      <td>3.122500</td>
      <td>4.035000</td>
      <td>5.287500</td>
      <td>9.870000</td>
    </tr>
    <tr>
      <th>AI</th>
      <td>200.0</td>
      <td>2.968850</td>
      <td>0.566885</td>
      <td>1.280000</td>
      <td>2.547500</td>
      <td>2.955000</td>
      <td>3.345000</td>
      <td>4.630000</td>
    </tr>
    <tr>
      <th>Brittle</th>
      <td>200.0</td>
      <td>48.161950</td>
      <td>14.129455</td>
      <td>10.940000</td>
      <td>37.755000</td>
      <td>49.510000</td>
      <td>58.262500</td>
      <td>84.330000</td>
    </tr>
    <tr>
      <th>TOC</th>
      <td>200.0</td>
      <td>0.990450</td>
      <td>0.481588</td>
      <td>-0.190000</td>
      <td>0.617500</td>
      <td>1.030000</td>
      <td>1.350000</td>
      <td>2.180000</td>
    </tr>
    <tr>
      <th>VR</th>
      <td>200.0</td>
      <td>1.964300</td>
      <td>0.300827</td>
      <td>0.930000</td>
      <td>1.770000</td>
      <td>1.960000</td>
      <td>2.142500</td>
      <td>2.870000</td>
    </tr>
    <tr>
      <th>Prod</th>
      <td>200.0</td>
      <td>3864.407081</td>
      <td>1553.277558</td>
      <td>839.822063</td>
      <td>2686.227611</td>
      <td>3604.303506</td>
      <td>4752.637555</td>
      <td>8590.384044</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Summary statistics are a critical first step in data checking.</p>
<ul class="simple">
<li><p>this includes the number of valid (non-null) values for each feature (count removes all np.NaN from the totals for each variable).</p></li>
<li><p>we can see the general behaviors such as central tendency, mean, and dispersion, variance.</p></li>
<li><p>we can identify issue with negative values, extreme values, and values that are outside the range of plausible values for each property.</p></li>
<li><p>The data looks to be in pretty good shape and for brevity we skip outlier detection. Let‚Äôs look at the univariate distributions.</p></li>
</ul>
</section>
<section id="univariate-distributions">
<h2>Univariate Distributions</h2>
<p>As with summary statistics, this ranking method is a qualitative check for issues with the data and to assess our confidence with each feature. It is better to not include a feature with low confidence of quality as it may be misleading (while adding to model complexity as discussed previously).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nbins</span> <span class="o">=</span> <span class="mi">20</span>                                                    <span class="c1"># number of histogram bins</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>                        <span class="c1"># plot histograms with central tendency and P10 and P90 labeled</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">nbins</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># histogram_bounds(values=df[feature].values,weights=np.ones(len(df)),color='red')</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">feature</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">featuretitle</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span> 
    <span class="c1"># if feature == resp:   </span>
    <span class="c1">#     plt.xlim([Ymin,Ymax])    </span>
    <span class="c1"># else:</span>
    <span class="c1">#     plt.xlim([xmin[i],xmax[i]]) </span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">4.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1b5c32c2236778514e0fd18a00eb66559f6030647523aca11be1a1e520a5693c.png" src="../Images/8a0c89fa0d885a7643839d04c23ff8be.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/1b5c32c2236778514e0fd18a00eb66559f6030647523aca11be1a1e520a5693c.png"/>
</div>
</div>
<p>The univariate distributions look good:</p>
<ul class="simple">
<li><p>there are no obvious outliers</p></li>
<li><p>the permeability is positively skewed, as is often observed</p></li>
<li><p>the corrected TOC has a small spike, but it‚Äôs reasonable</p></li>
</ul>
</section>
<section id="bivariate-distributions">
<h2>Bivariate Distributions</h2>
<p>Matrix scatter plots are a very efficient method to observe the bivariate relationships between the variables.</p>
<ul class="simple">
<li><p>this is another opportunity through data visualization to identify data issues</p></li>
<li><p>we can assess if we have collinearity, specifically simpler form between two features at a time.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pairgrid</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">PairGrid</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="c1"># matrix scatter plots</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_upper</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_diag</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'k'</span><span class="p">)</span><span class="c1"># Map a density plot to the lower triangle</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_lower</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">,</span> 
                              <span class="n">shade</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">shade_lowest</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_levels</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/283204472327a4b7c8b5d3fcf91848aec9161f0646166a8de22e8d42f8dbdaf5.png" src="../Images/eb1c7e7e9920c8c27be71cd70df81661.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/283204472327a4b7c8b5d3fcf91848aec9161f0646166a8de22e8d42f8dbdaf5.png"/>
</div>
</div>
<p>This plot communicates a lot of information. How could we use this plot for variable ranking?</p>
<ul class="simple">
<li><p>we can identify features that are closely related to each other, e.g., if two features have almost a perfect monotonic linear or near linear relationship we should remove one immediately. This is a simple case of collinearity that will likely result in model instability as discussed above.</p></li>
<li><p>we can check for linear vs. non-linear relationships. If we observe nonlinear bivariate relationships this will impact the choice of methods, and the quality of results from methods that assume linear relationships for variable ranking.</p></li>
<li><p>we can identify constraint relationships and heteroscedasticity between variables. Once again these may restrict our ranking methods and also encourage us to retains specific features to retain these features in the resulting model.</p></li>
</ul>
<p>Yet, we must remember that bivariate visualization and analysis is not sufficient to understand all the multivariate relationships in the data, e.g., multicollinearity includes strong linear relationships between 2 or more features. These may be hard to see with only bivariate plots.</p>
</section>
<section id="pairwise-covariance">
<h2>Pairwise Covariance</h2>
<p>Pairwise covariance provides a measure of the strength of the linear relationship between each predictor feature and the response feature. At this point, we specify that the goal of this study is to predict production, our response variable, from the other available predictor features. We are thinking predictively now, not inferentially, we want to estimate the function, <span class="math notranslate nohighlight">\(\hat{f}\)</span>, to accomplish this:</p>
<div class="math notranslate nohighlight">
\[
Y = \hat{f}(X_1,\ldots,X_n) 
\]</div>
<p>where <span class="math notranslate nohighlight">\(Y\)</span> is our response feature and <span class="math notranslate nohighlight">\(X_1,\ldots,X_n\)</span> are our predictor features. If we retained all of our predictor features to predict the response we would have:</p>
<div class="math notranslate nohighlight">
\[
Prod = \hat{f}(Por,Perm,AI,Brittle,TOC,VR) 
\]</div>
<p>Now back to the covariance, the covariance is defined as:</p>
<div class="math notranslate nohighlight">
\[
C_{xy}  = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{(n-1)}
\]</div>
<p>Covariance:</p>
<ul class="simple">
<li><p>measures the linear relationship</p></li>
<li><p>sensitive to the dispersion / variance of both the predictor and response</p></li>
</ul>
<p>We can use the follow command to build a covariance matrix:</p>
<div class="highlight-p notranslate"><div class="highlight"><pre><span/>df.iloc[:,1:8].cov()                                    # covariance matrix sliced predictors vs. response
</pre></div>
</div>
<p>the output is a new Pandas DataFrame, so we can slice the last column to get a Pandas series (ndarray with names) with the covariances between all predictors features and the response.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">covariance</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span> <span class="c1"># calculate covariance matrix and slice for only pred - resp</span>
<span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">,</span><span class="s1">'Covariance Matrix'</span><span class="p">,</span><span class="mf">4000.0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>          <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">covariance</span><span class="p">,</span><span class="o">-</span><span class="mf">20000.0</span><span class="p">,</span><span class="mf">20000.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Covariance with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Covariance'</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.6</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ff711b73e2baba3be2993b907d5023462afb3ab86692953cb31870078fc6969f.png" src="../Images/89e89e3a083da28954418714216aa9d0.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/ff711b73e2baba3be2993b907d5023462afb3ab86692953cb31870078fc6969f.png"/>
</div>
</div>
<p>The covariance is useful, but as you can see the magnitude is quite variable.</p>
<ul class="simple">
<li><p>the covarince magnitudes are a function of each feature‚Äôs feature and feature variance is somewhat arbitrary.</p></li>
<li><p>for example, what is the variance of porosity in fraction vs. percentage or permeability in Darcy vs. milliDarcy. We can show that if we apply a constant multiplier, <span class="math notranslate nohighlight">\(c\)</span>, to a feature, <span class="math notranslate nohighlight">\(X\)</span>, that the variance will change according to this relationship (the proof is based on expectation formulation of variance):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sigma_{cX}^2 = c^2 \cdot \sigma_{X}^2
\]</div>
<p>By moving from percentage to fraction we decrease the variance of porosity by a factor of 10,000! The variance of each feature is potentially arbitrary, with the exception when all the features are in the same units.</p>
<p>Pairwise correlations are standardized covariances; therefore, avoids this arbitrary magnitude issue.</p>
</section>
<section id="pairwise-correlation-coefficient">
<h2>Pairwise Correlation Coefficient</h2>
<p>Pairwise correlation coefficient provides a measure of the strength of the linear relationship between each predictor feature and the response feature.</p>
<div class="math notranslate nohighlight">
\[
\rho_{xy}  = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{(n-1)\sigma_x \sigma_y}, \, -1.0 \le \rho_{xy} \le 1.0
\]</div>
<p>The correlation coefficient:</p>
<ul class="simple">
<li><p>measures the linear relationship</p></li>
<li><p>removes the sensitivity to the dispersion / variance of both the predictor and response features, by normalizing by the product of the standard deviation of each feature</p></li>
</ul>
<p>We can use the follow command to build a correlation matrix:</p>
<div class="highlight-p notranslate"><div class="highlight"><pre><span/>df.iloc[:,1:8].corr()
</pre></div>
</div>
<p>the output is a new Pandas DataFrame, so we can slice the last column to get a Pandas series (ndarray with names) with the correlations between all predictors features and the response.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">correlation</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span> <span class="c1"># calculate covariance matrix and slice for only pred - resp</span>
<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="s1">'Correlation Matrix'</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>           <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8de550142d1a57e5f8c2443095641ca1f3d8907168b1d668112afc3f7f49b625.png" src="../Images/cf2f8ebbbb9381f4ae232eefb7ce2e7a.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/8de550142d1a57e5f8c2443095641ca1f3d8907168b1d668112afc3f7f49b625.png"/>
</div>
</div>
<p>From the correlation matrix we can observe:</p>
<ul class="simple">
<li><p>We see that porosity, permeability and total organic carbon have the strongest linear relationships with production.</p></li>
<li><p>Acoustic impedance has weak negative relationships with production.</p></li>
<li><p>Brittleness is very close to 0.0. If you review the brittleness vs. production scatterplot, you‚Äôll observe a complicated non-linear relationship. There is a brittleness ratio sweet spot for production (rock that is not too soft nor too hard)!</p></li>
</ul>
<p>We could also look at the full correlation matrix to evaluate the potential for redundancy between predictor features.</p>
<ul class="simple">
<li><p>strong degree of correlation between porosity and permeability and porosity and TOC</p></li>
<li><p>strong degree of negative correlation between TOC and acoustic impedance</p></li>
</ul>
<p>We are still limited to a strict linear relationship.  The rank correlation allows us to relax this assumption.</p>
</section>
<section id="pairwise-spearman-rank-correlation-coefficient">
<h2>Pairwise Spearman Rank Correlation Coefficient</h2>
<p>The rank correlation coefficient applies the rank transform to the data prior to calculating the correlation coefficient. To calculate the rank transform simply replace the data values with the rank <span class="math notranslate nohighlight">\(R_x = 1,\dots,n\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the maximum value and <span class="math notranslate nohighlight">\(1\)</span> is the minimum value.</p>
<div class="math notranslate nohighlight">
\[
\rho_{R_x R_y}  = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i} - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le 1.0
\]</div>
<div class="math notranslate nohighlight">
\[
x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall \, i \gt j 
\]</div>
<div class="math notranslate nohighlight">
\[
R_{x_i} = i
\]</div>
<p>The rank correlation:</p>
<ul class="simple">
<li><p>measures the monotonic relationship, relaxes the linear assumption</p></li>
<li><p>removes the sensitivity to the dispersion / variance of both the predictor and response, by normalizing by the product of the standard deviation of each.</p></li>
</ul>
<p>We can use the follow command to build a rank correlation matrix and calculate the p-value:</p>
<div class="highlight-p notranslate"><div class="highlight"><pre><span/>stats.spearmanr(df.iloc[:,1:8])
</pre></div>
</div>
<p>the output is a new Pandas DataFrame, so we can slice the last column to get a Pandas series (ndarray with names) with
the correlations between all predictors features and the response.</p>
<p>Also, we get a very convenient <em>pval</em> 2D ndarray with the two-sided (two-tail summing symmetric over both tails) p-value for a hypothesis test with:</p>
<div class="math notranslate nohighlight">
\[
H_o: \rho_{R_x R_y} = 0
\]</div>
<div class="math notranslate nohighlight">
\[
H_1: \rho_{R_x R_y} \ne 0
\]</div>
<p>Let‚Äôs keep the p-values between all the predictor features and our response feature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">rank_correlation</span><span class="p">,</span> <span class="n">rank_correlation_pval</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">spearmanr</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span> <span class="c1"># calculate the rank correlation coefficient</span>
<span class="n">rank_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rank_correlation</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">rank_correlation</span> <span class="o">=</span> <span class="n">rank_correlation</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>
<span class="n">rank_correlation_pval</span> <span class="o">=</span> <span class="n">rank_correlation_pval</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Rank Correlation p-value:</span><span class="se">\n</span><span class="s2">"</span><span class="p">);</span> <span class="nb">print</span><span class="p">(</span><span class="n">rank_correlation_pval</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">rank_matrix</span><span class="p">,</span><span class="s1">'Rank Correlation Matrix'</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>      <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">rank_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Rank Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Rank Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Rank Correlation p-value:

[2.43279911e-02 1.34135205e-01 1.18844068e-10 2.71646948e-04
 2.11367755e-06 0.00000000e+00 3.29170847e-04]
</pre></div>
</div>
<img alt="_images/dd0d78eac8d8429bd13249c35c5d50933da7bf91abf6d4cb58614489675c598e.png" src="../Images/609c07d08205a2c92204da55d19ad62a.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/dd0d78eac8d8429bd13249c35c5d50933da7bf91abf6d4cb58614489675c598e.png"/>
</div>
</div>
<p>There matrix and line plots indicate that the rank correlation coefficients are similar to the correlation coefficients indicating that nonlinearity and outliers are not likely impacting the correlation-based feature ranking.</p>
<p>With regard to rank correlation p-values,</p>
<ul class="simple">
<li><p>at a typical alpha value of 0.05, only the rank correlation between brittleness and production does not fail the hypothesis test; therefore, is not significantly different than 0.0.</p></li>
</ul>
<p>It is useful to look at the difference between the correlation coefficient and rank correlation coefficient.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>                                              <span class="c1"># plot correlation matrix with significance colormap</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">rank_matrix</span><span class="o">.</span><span class="n">values</span>
<span class="n">diff_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">diff_matrix</span><span class="p">,</span><span class="s1">'Correlation - Rank Correlation'</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span>

<span class="n">corr_diff</span> <span class="o">=</span> <span class="n">correlation</span> <span class="o">-</span> <span class="n">rank_correlation</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">corr_diff</span><span class="p">,</span><span class="o">-</span><span class="mf">0.20</span><span class="p">,</span><span class="mf">0.20</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Correlation Coefficient - Rank Correlation Coefficient'</span><span class="p">,</span><span class="s1">'Correlation Diffference'</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e6d73185fa9381a8bc24c2f7002331e6bbb2de3276c7073a45074a6ef2fae98b.png" src="../Images/9010e510d3b644ed069447aad1564797.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/e6d73185fa9381a8bc24c2f7002331e6bbb2de3276c7073a45074a6ef2fae98b.png"/>
</div>
</div>
<p>Here are some interesting observations:</p>
<ul class="simple">
<li><p>correlation of porosity and vitrinite reflectance with production improve when we reduce the impact of linearity and outliers</p></li>
<li><p>correlation of brittleness with production worsen when we reduce the impact of linearity and outliers</p></li>
</ul>
<p>All of these methods up to now have considered one feature at a time. We can also consider methods that consider all features jointly to ‚Äòisolate‚Äô the influence of each feature.</p>
</section>
<section id="partial-correlation-coefficient">
<h2>Partial Correlation Coefficient</h2>
<p>This is a linear correlation coefficient that controls for the effects all the remaining variables, <span class="math notranslate nohighlight">\(\rho_{XY.Z}\)</span> and <span class="math notranslate nohighlight">\(\rho_{YX.Z}\)</span> is the partial correlation between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span>, after controlling for <span class="math notranslate nohighlight">\(Z\)</span>.</p>
<p>To calculate the partial correlation coefficient between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(Z_i, \forall \quad i = 1,\ldots, m-1\)</span> remaining features we use the following steps:</p>
<ol class="arabic simple">
<li><p>perform linear, least-squares regression to predict <span class="math notranslate nohighlight">\(X\)</span> from <span class="math notranslate nohighlight">\(Z_i, \forall \quad i = 1,\ldots, m-1\)</span>. <span class="math notranslate nohighlight">\(X\)</span> is regressed on the predictors to calculate the estimate, <span class="math notranslate nohighlight">\(X^*\)</span></p></li>
<li><p>calculate the residuals in Step #1, <span class="math notranslate nohighlight">\(X-X^*\)</span>, where <span class="math notranslate nohighlight">\(X^* = f(Z_{1,\ldots,m-1})\)</span>, linear regression model</p></li>
<li><p>perform linear, least-squares regression to predict <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(Z_i, \forall \quad i = 1,\ldots, m-1\)</span>. <span class="math notranslate nohighlight">\(Y\)</span> is regressed on the predictors to calculate the estimate, <span class="math notranslate nohighlight">\(Y^*\)</span></p></li>
<li><p>calculate the residuals in Step #3, <span class="math notranslate nohighlight">\(Y-Y^*\)</span>, where <span class="math notranslate nohighlight">\(Y^* = f(Z_{1,\ldots,m-1})\)</span>, linear regression model</p></li>
<li><p>calculate the correlation coefficient between the residuals from Steps #2 and #4, <span class="math notranslate nohighlight">\(\rho_{X-X^*,Y-Y^*}\)</span></p></li>
</ol>
<p>The partial correlation, provides a measure of the linear relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> while controlling for the effect of <span class="math notranslate nohighlight">\(Z\)</span> other features on both, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.  We use the function declared previously taken from Fabian Pedregosa-Izquierdo, <a class="reference external" href="mailto:f%40bianp.net">f<span>@</span>bianp<span>.</span>net</a>. The original code is on GitHub at <a class="reference external" href="https://git.io/fhyHB">https://git.io/fhyHB</a>.</p>
<p>To use this method we must assume:</p>
<ol class="arabic simple">
<li><p>two variables to compare, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></p></li>
<li><p>other variables to control, <span class="math notranslate nohighlight">\(Z_{1,\ldots,m-2}\)</span></p></li>
<li><p>linear relationships between all variables</p></li>
<li><p>no significant outliers</p></li>
<li><p>approximately bivariate normality between the variables</p></li>
</ol>
<p>We are in pretty good shape, but we have some departures from bivariate normality. We could consider Gaussian univariate transforms to improve this. This option is provided later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">partial_correlation</span> <span class="o">=</span> <span class="n">partial_corr</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span> <span class="c1"># calculate the partial correlation coefficients</span>
<span class="n">partial_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">partial_correlation</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">partial_correlation</span> <span class="o">=</span> <span class="n">partial_correlation</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span> <span class="c1"># extract a single row and remove production with itself        </span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">partial_matrix</span><span class="p">,</span><span class="s1">'Partial Correlation Matrix'</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">partial_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Partial Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Partial Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/685abb597e65278976ed6245ea39ffba1c566acd98a0496f9a20ae9b472f4e07.png" src="../Images/11649099249c19a8d0134ee44bd96661.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/685abb597e65278976ed6245ea39ffba1c566acd98a0496f9a20ae9b472f4e07.png"/>
</div>
</div>
<p>Now we see a lot of new things about the unique contributions of each predictor feature!</p>
<ul class="simple">
<li><p>porosity and permeability are strongly correlated with each other so they are penalized severely</p></li>
<li><p>acoustic impedance‚Äôs and vitrinite reflectance‚Äôs absolute correlation are increased reflecting their unique contributions</p></li>
<li><p>total organic carbon flipped signs!  When we control for all other variables, it has a negative relationship with production.</p></li>
</ul>
<p>With the partial correlation coefficients we have controlled for the influence of all other predictor features on both the specific predictor and the response features. The semipartial correlation filters out the influence of all other predictor features on the raw response variable.</p>
</section>
<section id="semipartial-correlation-coefficient">
<h2>Semipartial Correlation Coefficient</h2>
<p>This is a linear correlation coefficient that controls for the effects all the remaining features, <span class="math notranslate nohighlight">\(Z\)</span> on <span class="math notranslate nohighlight">\(X\)</span>, and then calculates the correlation between the residual <span class="math notranslate nohighlight">\(X^*-X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.  Note: we do not control for influence of <span class="math notranslate nohighlight">\(Z\)</span> features on the response feature, <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>To calculate the semipartial correlation coefficient between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(Z_i, \forall \quad i = 1,\ldots, m-1\)</span> remaining features we use the following steps:</p>
<ol class="arabic simple">
<li><p>perform linear, least-squares regression to predict <span class="math notranslate nohighlight">\(X\)</span> from <span class="math notranslate nohighlight">\(Z_i, \forall \quad i = 1,\ldots, m-1\)</span>. <span class="math notranslate nohighlight">\(X\)</span> is regressed on the remaining predictor features to calculate the estimate, <span class="math notranslate nohighlight">\(X^*\)</span></p></li>
<li><p>calculate the residuals in Step #1, <span class="math notranslate nohighlight">\(X-X^*\)</span>, where <span class="math notranslate nohighlight">\(X^* = f(Z_{1,\ldots,m-1})\)</span>, linear regression model</p></li>
<li><p>calculate the correlation coefficient between the residuals from Steps #2 and <span class="math notranslate nohighlight">\(Y\)</span> response feature, <span class="math notranslate nohighlight">\(\rho_{X-X^*,Y}\)</span></p></li>
</ol>
<p>The semipartial correlation coefficient, provides a measure of the linear relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> while controlling for the effect of <span class="math notranslate nohighlight">\(Z\)</span> other predictor features on the predictor feature, <span class="math notranslate nohighlight">\(X\)</span>, to get the unique contribution of <span class="math notranslate nohighlight">\(X\)</span> with respect to <span class="math notranslate nohighlight">\(Y\)</span>. We use a modified version of the partial correlation function that we declared previously. The original code is on GitHub at <a class="reference external" href="https://git.io/fhyHB">https://git.io/fhyHB</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">semipartial_correlation</span> <span class="o">=</span> <span class="n">semipartial_corr</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span>    <span class="c1"># calculate the semi-partial correlation coefficients</span>
<span class="n">semipartial_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">semipartial_correlation</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">semipartial_correlation</span> <span class="o">=</span> <span class="n">semipartial_correlation</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>    <span class="c1"># extract a single row and remove production with itself</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">semipartial_matrix</span><span class="p">,</span><span class="s1">'Semi-partial Correlation Matrix'</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">semipartial_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Semipartial Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Semipartial Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8288d5546785bae96f5c850214ddd5be4c148d6210b1b7d05ca2f86640b6b443.png" src="../Images/9bc790699675e3bdbd9e8ed625051fa1.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/8288d5546785bae96f5c850214ddd5be4c148d6210b1b7d05ca2f86640b6b443.png"/>
</div>
</div>
<p>More information to consider:</p>
<ul class="simple">
<li><p>porosity, permeability and vitrinite reflectance are the most important by this feature ranking method</p></li>
<li><p>all other predictor features have quite low correlations</p></li>
</ul>
<p>This is a good moment to stop and take stock of all the results from the quantitative methods.  We will plot them all together.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># plt.subplot(151)</span>
<span class="c1"># feature_rank_plot(features,covariance,-5000.0,5000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.1)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">rank_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Rank Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Rank Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">partial_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Partial Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Partial Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># plt.subplot(155)</span>
<span class="c1"># feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8e1ed4eede67a45f3fd9cf848e281724927b517c683e5207ab574696b9952bde.png" src="../Images/9b87cdad7bc65f987a9f65bfe7febc3b.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/8e1ed4eede67a45f3fd9cf848e281724927b517c683e5207ab574696b9952bde.png"/>
</div>
</div>
<p>I think we are converging on porosity, permeability and vitrinite reflectance as the most important variables with respect to linear relationships with the production.</p>
</section>
<section id="feature-ranking-with-feature-transformations">
<h2>Feature Ranking with Feature Transformations</h2>
<p>There are many reasons to perform feature transformations (see the associated chapter) and as mentioned above for partial and semipartial correlation a distribution transformation may assist with compliance to metric assumptions.</p>
<ul class="simple">
<li><p>As an exercise and check, let‚Äôs standardize all the features and repeat the previously calculated quantitative methods. We know this will have an impact on covariance, what about the other metrics?</p></li>
</ul>
<p>There is a bunch of code to get this done, but it isn‚Äôt too complicated. First, lets make a new DataFrame with all variables standardized. Then we can make a minor edit (change the DataFrame name) and reuse the code from above. You can choose between:</p>
<ol class="arabic simple">
<li><p>Standardization - affine correction to scale the distributions to have <span class="math notranslate nohighlight">\(\overline{x} = 0\)</span> and <span class="math notranslate nohighlight">\(\sigma_x = 1.0\)</span>.</p></li>
<li><p>Normal Score Transform - distribution transform of each feature to standard normal, Gaussian shape with <span class="math notranslate nohighlight">\(\overline{x} = 0\)</span> and <span class="math notranslate nohighlight">\(\sigma_x = 1.0\)</span>.</p></li>
</ol>
<p>Use this block to perform affine correction of the features:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># dfS = pd.DataFrame()                                        # affine correction, standardization to a mean of 0 and variance of 1 </span>
<span class="c1"># dfS['Well'] = df['Well'].values</span>
<span class="c1"># dfS['Por'] = GSLIB.affine(df['Por'].values,0.0,1.0)</span>
<span class="c1"># dfS['Perm'] = GSLIB.affine(df['Perm'].values,0.0,1.0)</span>
<span class="c1"># dfS['AI'] = GSLIB.affine(df['AI'].values,0.0,1.0)</span>
<span class="c1"># dfS['Brittle'] = GSLIB.affine(df['Brittle'].values,0.0,1.0)</span>
<span class="c1"># dfS['TOC'] = GSLIB.affine(df['TOC'].values,0.0,1.0)</span>
<span class="c1"># dfS['VR'] = GSLIB.affine(df['VR'].values,0.0,1.0)</span>
<span class="c1"># dfS['Prod'] = GSLIB.affine(df['Prod'].values,0.0,1.0)</span>
<span class="c1"># dfS.head()</span>
</pre></div>
</div>
</div>
</div>
<p>Use this block to perform normal score transform of the features:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">dfS</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>                                          <span class="c1"># Gaussian transform, standardization to a mean of 0 and variance of 1 </span>

<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
    <span class="n">dfS</span><span class="p">[</span><span class="n">feature</span><span class="p">],</span><span class="n">d1</span><span class="p">,</span><span class="n">d2</span> <span class="o">=</span> <span class="n">geostats</span><span class="o">.</span><span class="n">nscore</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">)</span>

<span class="n">dfS</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Por</th>
      <th>Perm</th>
      <th>AI</th>
      <th>Brittle</th>
      <th>TOC</th>
      <th>VR</th>
      <th>Prod</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.964092</td>
      <td>-0.780664</td>
      <td>-0.285841</td>
      <td>2.432379</td>
      <td>0.312053</td>
      <td>1.114651</td>
      <td>-1.780464</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.832725</td>
      <td>-0.378580</td>
      <td>0.446827</td>
      <td>-0.195502</td>
      <td>-0.272809</td>
      <td>-0.325239</td>
      <td>-0.392079</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.312053</td>
      <td>-1.069155</td>
      <td>1.722384</td>
      <td>2.004654</td>
      <td>-0.272809</td>
      <td>2.241403</td>
      <td>-0.832725</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.730638</td>
      <td>1.325516</td>
      <td>-0.531604</td>
      <td>-0.590284</td>
      <td>0.131980</td>
      <td>-0.325239</td>
      <td>0.815126</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.698283</td>
      <td>0.298921</td>
      <td>0.365149</td>
      <td>-2.870033</td>
      <td>1.047216</td>
      <td>-0.259823</td>
      <td>-0.531604</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Regardless of the transformation that you chose it is best practice to check the summary statistics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">dfS</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>                                                <span class="c1"># check the summary statistics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Por</th>
      <th>Perm</th>
      <th>AI</th>
      <th>Brittle</th>
      <th>TOC</th>
      <th>VR</th>
      <th>Prod</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>200.000000</td>
      <td>200.000000</td>
      <td>2.000000e+02</td>
      <td>2.000000e+02</td>
      <td>200.000000</td>
      <td>200.000000</td>
      <td>2.000000e+02</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>-0.009700</td>
      <td>0.010306</td>
      <td>9.732356e-03</td>
      <td>8.028717e-05</td>
      <td>0.014152</td>
      <td>0.017360</td>
      <td>1.617223e-03</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.040456</td>
      <td>1.005488</td>
      <td>1.000221e+00</td>
      <td>1.000278e+00</td>
      <td>0.989223</td>
      <td>1.000401</td>
      <td>9.949811e-01</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-4.991462</td>
      <td>-3.355431</td>
      <td>-2.782502e+00</td>
      <td>-2.870033e+00</td>
      <td>-2.336891</td>
      <td>-2.899210</td>
      <td>-2.483589e+00</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>-0.670577</td>
      <td>-0.647337</td>
      <td>-6.588985e-01</td>
      <td>-6.705770e-01</td>
      <td>-0.670577</td>
      <td>-0.651072</td>
      <td>-6.705770e-01</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.006267</td>
      <td>0.006267</td>
      <td>8.881784e-16</td>
      <td>8.881784e-16</td>
      <td>0.018807</td>
      <td>0.006267</td>
      <td>8.881784e-16</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.670577</td>
      <td>0.678574</td>
      <td>6.705770e-01</td>
      <td>6.705770e-01</td>
      <td>0.682378</td>
      <td>0.682642</td>
      <td>6.705770e-01</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2.807034</td>
      <td>2.807034</td>
      <td>2.807034e+00</td>
      <td>2.807034e+00</td>
      <td>2.807034</td>
      <td>2.807034</td>
      <td>2.807034e+00</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We should also check the matrix scatter plot again.</p>
<ul class="simple">
<li><p>If you performed normal score transform, you have standardized the mean and variance and correct the univariate shape of the distribution, but the bivariate relationships still depart from Gaussian.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pairgrid</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">PairGrid</span><span class="p">(</span><span class="n">dfS</span><span class="p">)</span> <span class="c1"># matrix scatter plots</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_upper</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_diag</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'k'</span><span class="p">)</span><span class="c1"># Map a density plot to the lower triangle</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_lower</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">,</span> 
                              <span class="n">shade</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">shade_lowest</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_levels</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/884680b3106f0f9bc10b64a1888d213dedcd55860acea49e6f5bd179d1604868.png" src="../Images/8568ba1fa581044cc577242f378dd1db.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/884680b3106f0f9bc10b64a1888d213dedcd55860acea49e6f5bd179d1604868.png"/>
</div>
</div>
<p>This is the new DataFrame with standardized variables. Now we repeat the previous calculations.</p>
<ul class="simple">
<li><p>We will be more efficient this time and use quite compact code.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">stand_covariance</span> <span class="o">=</span> <span class="n">dfS</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">dfS</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>
<span class="n">stand_correlation</span> <span class="o">=</span> <span class="n">dfS</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">dfS</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>

<span class="n">stand_rank_correlation</span><span class="p">,</span> <span class="n">stand_rank_correlation_pval</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">spearmanr</span><span class="p">(</span><span class="n">dfS</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">dfS</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span>
<span class="n">stand_rank_correlation</span> <span class="o">=</span> <span class="n">stand_rank_correlation</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>
<span class="n">stand_partial_correlation</span> <span class="o">=</span> <span class="n">partial_corr</span><span class="p">(</span><span class="n">dfS</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">dfS</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span> <span class="c1"># calculate the partial correlation coefficients</span>
<span class="n">stand_partial_correlation</span> <span class="o">=</span> <span class="n">stand_partial_correlation</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>
<span class="n">stand_semipartial_correlation</span> <span class="o">=</span> <span class="n">semipartial_corr</span><span class="p">(</span><span class="n">dfS</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">dfS</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span>    <span class="c1"># calculate the semi-partial correlation coefficients</span>
<span class="n">stand_semipartial_correlation</span> <span class="o">=</span> <span class="n">stand_semipartial_correlation</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span> 
</pre></div>
</div>
</div>
</div>
<p>and repeat the previous summary plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># plt.subplot(2,5,1)</span>
<span class="c1"># feature_rank_plot(features,covariance,-5000.0,5000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.5)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">rank_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Rank Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Rank Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">partial_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Partial Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Partial Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># plt.subplot(2,5,5)</span>
<span class="c1"># feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)</span>

<span class="c1"># plt.subplot(2,5,6)</span>
<span class="c1"># feature_rank_plot(features,stand_covariance,-1.0,1.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance of Standardized',0.5)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">stand_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Correlation of Standardized'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">stand_rank_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Rank Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Rank Correlation of Standardized'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">stand_partial_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Partial Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Partial Correlation of Standardized'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># plt.subplot(2,5,10)</span>
<span class="c1"># feature_rank_plot(features,stand_semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation of Standardized',0.5)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e6320b5768883e13feea467d4888f04edce7671ecd0ba7a92874bc94656cd1a2.png" src="../Images/8eeed3569969856a648e8f7bf97ddb8b.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/e6320b5768883e13feea467d4888f04edce7671ecd0ba7a92874bc94656cd1a2.png"/>
</div>
</div>
<p>What can you observe:</p>
<ul class="simple">
<li><p>covariance is now equal to correlation coefficient</p></li>
<li><p>the semipartial correlations are sensitive to the feature standardization (affine correlation or normal score transform).</p></li>
</ul>
</section>
<section id="conditional-statistics">
<h2>Conditional Statistics</h2>
<p>We will separate the wells into low, mid and high production and access the difference in the conditional statistics.</p>
<ul class="simple">
<li><p>This will provide a more flexible method to compare the relationship between each feature and production</p></li>
<li><p>If the conditional statistics change significantly then that feature is informative</p></li>
</ul>
<p>We are going to make a single violin plot over all of our features</p>
<ul class="simple">
<li><p>We need a categorical feature for production, so we truncate production to High or Low with this code,</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">df</span><span class="p">[</span><span class="s1">'tProd'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'Prod'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mi">4000</span><span class="p">,</span> <span class="s1">'High'</span><span class="p">,</span> <span class="s1">'Low'</span><span class="p">)</span> 
</pre></div>
</div>
<ul class="simple">
<li><p>We will need to standardize all of our features so we can observe their relative differences together</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">'Por'</span><span class="p">,</span><span class="s1">'Perm'</span><span class="p">,</span><span class="s1">'AI'</span><span class="p">,</span><span class="s1">'Brittle'</span><span class="p">,</span><span class="s1">'TOC'</span><span class="p">,</span><span class="s1">'VR'</span><span class="p">]]</span>
<span class="n">x_stand</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>      
</pre></div>
</div>
<ul class="simple">
<li><p>This code extracted the features into a new DataFrame ‚Äòx‚Äô, then applied the standardization operation on each column (feature)</p></li>
<li><p>Then we add the truncated production feature into the standardized features</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">[</span><span class="s1">'tProd'</span><span class="p">],</span><span class="n">x_stand</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">6</span><span class="p">]],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>We can then apply the melt command to unpivot the DataFrame</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">id_vars</span><span class="o">=</span><span class="s2">"tProd"</span><span class="p">,</span><span class="n">var_name</span><span class="o">=</span><span class="s2">"Predictors"</span><span class="p">,</span><span class="n">value_name</span><span class="o">=</span><span class="s1">'Standardized_Value'</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>We now have a long DataFrame (6 features x 200 samples = 12000 rows) with:</p>
<ul>
<li><p>production: Low or High</p></li>
<li><p>features: Por, Perm, AI, Brittle, TOC or VR</p></li>
<li><p>standardized feature value</p></li>
</ul>
</li>
</ul>
<p>We can then build our violin plot</p>
<ul class="simple">
<li><p>x is our predictor features</p></li>
<li><p>y is the standardized values for the predictor features (all now in one column)</p></li>
<li><p>hue is the production level High or Low</p></li>
<li><p>split is True so the violins are split in half</p></li>
<li><p>inner is quartiles for P25, P50 and P75 are plotted as dashed lines</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">threshold</span> <span class="o">=</span> <span class="mf">2000.0</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'tProd'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">resp</span><span class="p">]</span><span class="o">&gt;=</span><span class="n">threshold</span><span class="p">,</span> <span class="s1">'High'</span><span class="p">,</span> <span class="s1">'Low'</span><span class="p">)</span>       <span class="c1"># make a high and low production categorical feature</span>

<span class="n">x_temp</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span>
<span class="n">x_temp_stand</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_temp</span> <span class="o">-</span> <span class="n">x_temp</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">x_temp</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>      <span class="c1"># standardization by feature</span>
<span class="n">x_temp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">[</span><span class="s1">'tProd'</span><span class="p">],</span><span class="n">x_temp_stand</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)]],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># add the production categorical feature to the DataFrame</span>
<span class="n">x_temp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">x_temp</span><span class="p">,</span><span class="n">id_vars</span><span class="o">=</span><span class="s2">"tProd"</span><span class="p">,</span><span class="n">var_name</span><span class="o">=</span><span class="s2">"Predictor Feature"</span><span class="p">,</span><span class="n">value_name</span><span class="o">=</span><span class="s1">'Standardized Predictor Feature'</span><span class="p">)</span> <span class="c1"># unpivot the DataFrame</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Predictor Feature"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Standardized Predictor Feature"</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">"tProd"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">x_temp</span><span class="p">,</span><span class="n">split</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inner</span><span class="o">=</span><span class="s2">"quart"</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">"Set2"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Conditional Distributions by Production'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/39636127cd144d1449668e2c0eee3c8e122afe9891a46cc3e4610d38ff16390a.png" src="../Images/97e5b28483156d276669ee0ba4b67c7a.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/39636127cd144d1449668e2c0eee3c8e122afe9891a46cc3e4610d38ff16390a.png"/>
</div>
</div>
<p>From the violin plot we can observe that the conditional distributions of porosity, permeability, TOC have the most variation between low and high production wells.</p>
<p>We can replace the plot with box and whisker plots of the conditional distributions.</p>
<ul class="simple">
<li><p>Box and whisker plots improve our ability to observe the conditional P25, P75 and the upper and lower bounds from the Tukey outlier test.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Predictor Feature"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Standardized Predictor Feature"</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">"tProd"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">x_temp</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'tProd'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2121ce938f6088b2c90acec866c9ff94b488c33826983e13a20d4e1721016352.png" src="../Images/5cb6b0c21042f1de1888484ca5e64a81.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/2121ce938f6088b2c90acec866c9ff94b488c33826983e13a20d4e1721016352.png"/>
</div>
</div>
<p>From the conditional box plot we can observe that the conditional distributions of porosity, permeability, TOC have the most variation between low and high production wells.</p>
<ul class="simple">
<li><p>We can observed the outliers in porosity, permeability (upper tail), total organic carbon (lower tail) and vitrinite reflectance.</p></li>
</ul>
</section>
<section id="variance-inflation-factor-vif">
<h2>Variance Inflation Factor (VIF)</h2>
<p>A measure of linear multicollinearity between a predictor feature (<span class="math notranslate nohighlight">\(X_i\)</span>) a nd all other predictor features (<span class="math notranslate nohighlight">\(X_j, \forall j \ne i\)</span>).</p>
<p>First we calculate a linear regression for a predictor feature given all the other predictor features.</p>
<div class="math notranslate nohighlight">
\[
X_i = \sum_{j, j \ne i}^m X_j + \epsilon
\]</div>
<p>From this model we determine the coefficient of determination, <span class="math notranslate nohighlight">\(R^2\)</span>, known as variance explained.</p>
<p>Then we calculate the Variance Inflation Factor as:</p>
<div class="math notranslate nohighlight">
\[
VIF = \frac{1}{1 - R^2}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">vif_values</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">vif_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>

<span class="n">vif_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">vif_values</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">vif_values</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>                  <span class="c1"># find indices for descending order</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                        <span class="c1"># plot the feature importance </span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Variance Inflation Factor"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">vif_values</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span>
       <span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pred</span><span class="p">)[</span><span class="n">indices</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">);</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Variance Inflation Factor'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/eaaa1c71dedcaed7ee83a0dfce8fdfb50ad17facc2a2c38a51caf6c6481cb547.png" src="../Images/5692efa445f9167d3d0d5a6d8f3e8bcb.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/eaaa1c71dedcaed7ee83a0dfce8fdfb50ad17facc2a2c38a51caf6c6481cb547.png"/>
</div>
</div>
<p>Vitrinite reflectance has the most linear redundancy while permeability has the least linear redundancy with other predictor features.</p>
<ul class="simple">
<li><p>remember, high variance inflation factor is bad</p></li>
<li><p>recall that variance inflation factor does not integrate the relationship between each predictor feature and the response feature.</p></li>
<li><p>typically, variance inflation factor is used as a screening tool to remove features that have too much redundancy with other predictor features.</p></li>
</ul>
<p>Now let‚Äôs cover model-based feature ranking methods.</p>
</section>
<section id="b-coefficients-beta-weights">
<h2><span class="math notranslate nohighlight">\(B\)</span> Coefficients / Beta Weights</h2>
<p>We could also consider <span class="math notranslate nohighlight">\(B\)</span> coefficients.  These are the linear regression coefficients without standardization of the variables. Let‚Äôs use the linear regression method that is available in the SciPy package.</p>
<p>The estimator for <span class="math notranslate nohighlight">\(Y\)</span> is simply the linear equation:</p>
<p>\begin{equation}
Y^* = \sum_{i=1}^{m} b_i X_i + c
\end{equation}</p>
<p>The <span class="math notranslate nohighlight">\(b_i\)</span> coefficients are solved to minimize the squared error between the estimates, <span class="math notranslate nohighlight">\(Y^*\)</span> and the values in the training dataset, <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>                                      <span class="c1"># instantiate a linear regression model                   </span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">pred</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">resp</span><span class="p">])</span>                                    <span class="c1"># train the model</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="o">-</span><span class="mf">1000.0</span><span class="p">,</span><span class="mf">1000.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, B Coefficients with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="sa">r</span><span class="s1">'Linear Regression Slope, $b_1$'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ddb18df2725eafa50c95853e6bec78f8aa249726ae339ee0d874966578afdf95.png" src="../Images/7da7b523b4ae881b20a379148ea78eea.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/ddb18df2725eafa50c95853e6bec78f8aa249726ae339ee0d874966578afdf95.png"/>
</div>
</div>
<p>The output is the <span class="math notranslate nohighlight">\(b\)</span> coefficients, ordered over our features from <span class="math notranslate nohighlight">\(b_i, i = 1,\ldots,n\)</span> and then the intercept, <span class="math notranslate nohighlight">\(c\)</span>, that I have removed to avoid confusion.</p>
<ul class="simple">
<li><p>we see the negative contribution of AI and TOC</p></li>
<li><p>the results are very sensitive to the magnitudes of the variances of the predictor features.</p></li>
</ul>
<p>We can remove this sensitivity by working with standardized features.</p>
</section>
<section id="beta-coefficients-beta-weights">
<h2><span class="math notranslate nohighlight">\(\beta\)</span> Coefficients / Beta Weights</h2>
<p><span class="math notranslate nohighlight">\(\beta\)</span> coefficients are calculated as the linear regression of the coefficients after we have standardized the predictor and response features to have a variance of one.</p>
<p>\begin{equation}
\sigma^2_{X^s_i} = 1.0 \quad \forall \quad i = 1,\ldots,m, \quad \sigma^2_{Y^s} = 1.0
\end{equation}</p>
<p>The estimator for <span class="math notranslate nohighlight">\(Y^s\)</span> standardized is simply the linear equation:</p>
<p>\begin{equation}
Y^{s*} = \sum_{i=1}^{m} \beta_i X^s_i + c
\end{equation}</p>
<p>It is convenient that we have just standardized all our variables to have a variance of 1.0 just recently (see above). Let‚Äôs use the same linear regression method again on the standardized features to get <span class="math notranslate nohighlight">\(\beta\)</span> coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dfS</span><span class="p">[</span><span class="n">pred</span><span class="p">],</span><span class="n">dfS</span><span class="p">[</span><span class="n">resp</span><span class="p">])</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="sa">r</span><span class="s1">'Feature Ranking, $\beta$ Coefficients with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="sa">r</span><span class="s1">'Standardized Linear Regression Slope, $b_1$'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/29565b4c74d530743a2aacdb6e0cda302fe0e962b6635c8cc1f909311289fcd9.png" src="../Images/63a59eb20ac3161240452ef8f6fcd97a.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/29565b4c74d530743a2aacdb6e0cda302fe0e962b6635c8cc1f909311289fcd9.png"/>
</div>
</div>
<p>Some observations:</p>
<ul class="simple">
<li><p>the change between <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> coefficients is not just a constant scaling on the ranking metrics, because the linear model coefficients are also sensitive to the ranges and magnitudes of the features.</p></li>
<li><p>with beta coefficients porosity, acoustic impedance and total organic carbon have higher rank for estimating production</p></li>
</ul>
</section>
<section id="feature-importance">
<h2>Feature Importance</h2>
<p>A variety of machine learning methods provide measures of feature importance, for example decision trees the reduction in mean square error through inclusion of each feature and is summarized as:</p>
<div class="math notranslate nohighlight">
\[
FI(x) = \sum_{t \in T_f} \frac{N_t}{N} \Delta_{MSE_t}
\]</div>
<p>where <span class="math notranslate nohighlight">\(T_f\)</span> are all nodes with feature <span class="math notranslate nohighlight">\(x\)</span> as the split, <span class="math notranslate nohighlight">\(N_t\)</span> is the number of training samples reaching node <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(N\)</span> is the total number of samples in the dataset and <span class="math notranslate nohighlight">\(\Delta_{MSE_t}\)</span> is the reduction in MSE with the <span class="math notranslate nohighlight">\(t\)</span> split.</p>
<p>Note, feature importance can be calculated in a similar manner to MSE above for the case of classification trees with <strong>Gini Impurity</strong>.</p>
<p>Let‚Äôs look at the feature importance from a random forest regression model fit to our data.</p>
<ul class="simple">
<li><p>We instantiate a random forest with default hyperparameters. This results in unlimited complexity, over-trained trees in our forest. The averaging of these trees takes care of the overfit issue.</p></li>
<li><p>Then we train our random forest and extract the feature importances, calculated as the expectated feature importance over all the trees in the forest.</p></li>
<li><p>we can also extract the feature importances over all the trees in the forest and summarize with the standard deviation to access the robustness, uncertainty of our feature importance measure</p></li>
</ul>
<p>For more information check out my lecture on <a class="reference external" href="https://www.youtube.com/watch?v=m5_wk310fho&amp;list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&amp;index=39">random forest</a> predictive machine learning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Code modified from https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization</span>
<span class="n">lab_enc</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">();</span> <span class="n">Y_encoded</span> <span class="o">=</span> <span class="n">lab_enc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="c1"># this removes an encoding error </span>

<span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span>                 <span class="c1"># instantiate the random forest  </span>
<span class="n">random_forest</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">Y_encoded</span><span class="p">))</span> <span class="c1"># fit the random forest</span>
<span class="n">importance_rank</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">feature_importances_</span>    <span class="c1"># extract the expected feature importances</span>

<span class="n">importance_rank_stand</span> <span class="o">=</span> <span class="n">importance_rank</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">importance_rank</span><span class="p">)</span>                          <span class="c1"># calculate relative mutual information</span>

<span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">([</span><span class="n">tree</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># calculate stdev over trees</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importance_rank</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>             <span class="c1"># find indices for descending order</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                        <span class="c1"># plot the feature importance </span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Random Forest-based Feature importances"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">importance_rank</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span>
       <span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">std</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Feature Importance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/633da2b9b4e395da57c78aa4f82399344b016071733ef9fb8569d30c70d92604.png" src="../Images/4221de8488fdf2ce8f7716c22e1ea9d8.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/633da2b9b4e395da57c78aa4f82399344b016071733ef9fb8569d30c70d92604.png"/>
</div>
</div>
<p>There is more we can do with model-based methods. We will actually test models to assess the incremental impact of each predictor feature! We will try this with recursive feature elimination.</p>
<p>Let‚Äôs plot the results from the <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> coefficients and compare with the previous results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">231</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">rank_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Rank Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Rank Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">232</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">partial_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Partial Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Partial Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">234</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span><span class="o">-</span><span class="mf">1000.0</span><span class="p">,</span><span class="mf">1000.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, B Coefficients with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'B Coefficients'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">235</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Beta Coefficients with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Beta Coefficients'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">236</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">importance_rank_stand</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Feature Importance with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Standardized Feature Importance'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/28592d6d0a57887d32a07cc893157689a5862f636dd4b208cc9b42b3dc9dd964.png" src="../Images/9b736db6a7b146115943a1775ce2cc0d.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/28592d6d0a57887d32a07cc893157689a5862f636dd4b208cc9b42b3dc9dd964.png"/>
</div>
</div>
</section>
<section id="mutual-information">
<h2>Mutual Information</h2>
<p>Mutual information is a generalized approach that quantifies the mutual dependence between two features.</p>
<ul class="simple">
<li><p>quantifies the amount of information gained from observing one feature about the other</p></li>
<li><p>avoids any assumption about the form of the relationship (e.g. no assumption of linear relationship)</p></li>
<li><p>compares the joint probabilities to the product of the marginal probabilities</p></li>
</ul>
<p>For discrete or binned continuous features <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, mutual information is calculated as:</p>
<div class="math notranslate nohighlight">
\[
I(X;Y) = \sum_{y \in Y} \sum_{x \in X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right)
\]</div>
<p>recall, given independence between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<div class="math notranslate nohighlight">
\[
P_{X,Y}(x,y) = P_X(x) \cdot P_Y(y)
\]</div>
<p>therefore if the two features are independent then the <span class="math notranslate nohighlight">\(log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) = 0\)</span></p>
<p>The joint probability <span class="math notranslate nohighlight">\(P_{X,Y}(x,y)\)</span> is a weighting term on the sum and enforces closure.</p>
<ul class="simple">
<li><p>parts of the joint distribution with greater density have greater impact on the mutual information metric</p></li>
</ul>
<p>For continuous (and nonbinned) features we can applied the integral form.</p>
<div class="math notranslate nohighlight">
\[
I(X;Y) = \int_{Y} \int_{X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) dx dy
\]</div>
<p>We get a sorted list of the indices in decreasing order of importance with the command</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p>the slice reverses the order, for descending order of feature importance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">pred</span><span class="p">]</span>                            <span class="c1"># separate DataFrames for predictor and response features</span>
<span class="n">y_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">resp</span><span class="p">]</span>

<span class="n">mi</span> <span class="o">=</span> <span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x_df</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">y_df</span><span class="p">))</span>              <span class="c1"># calculate mutual information</span>
<span class="n">mi</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">mi</span><span class="p">)</span>                                        <span class="c1"># calculate relative mutual information</span>

<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">mi</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>                          <span class="c1"># find indices for descending order</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Feature ranking:"</span><span class="p">)</span>                               <span class="c1"># write out the feature importances</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="si">%d</span><span class="s2">. feature </span><span class="si">%s</span><span class="s2"> = </span><span class="si">%f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices</span><span class="p">][</span><span class="n">f</span><span class="p">],</span> <span class="n">mi</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="n">f</span><span class="p">]]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                        <span class="c1"># plot the relative mutual information </span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Mutual Information"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mi</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span>
       <span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Mutual Information'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Feature ranking:
1. feature Por = 1.000000
2. feature Perm = 0.345842
3. feature TOC = 0.272418
4. feature Brittle = 0.073310
5. feature AI = 0.059024
6. feature VR = 0.000000
</pre></div>
</div>
<img alt="_images/704e3c6fd50614dcf67edb8dd01d1392f4509c1017dae51fcd1a26281de97377.png" src="../Images/d12dd6dd61aa76402a54c096acd0768b.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/704e3c6fd50614dcf67edb8dd01d1392f4509c1017dae51fcd1a26281de97377.png"/>
</div>
</div>
<section id="mutual-information-accounting-for-relevance-and-redundancy">
<h3>Mutual Information Accounting For Relevance and Redundancy</h3>
<p>The standard Maximum Relevance - Minumum Redundancy (MRMR) objective function considers a subset of predictor features, i.e., to score predictor feature subsets as metric to identify the most informative subset of predictor features.</p>
<ul class="simple">
<li><p>the approach calculates the average mutual information between the subset of predictor features and the response feature minus the average mutual information between the subset of predictor features.</p></li>
</ul>
<p>\begin{equation}
MID = \frac{1}{|S|}{\sum_{\alpha \in S} I(X_{\alpha},Y) } - \frac{1}{|S|^2} {\sum_{\alpha \in S}^m \sum_{\beta \in S}^m I(X_{\alpha},X_{\beta})}
\end{equation}</p>
<p>as a measure of <span class="math notranslate nohighlight">\(relevance - redundancy\)</span> or</p>
<p>\begin{equation}
MIQ = \frac{ \frac{1}{|S|}{\sum_{\alpha \in S}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|^2} {\sum_{\alpha \in S}^m \sum_{\beta \in S}^m I(X_{\alpha},X_{\beta})} }
\end{equation}</p>
<ul class="simple">
<li><p>as a measure of <span class="math notranslate nohighlight">\(\frac{relevance}{redundancy}\)</span>.</p></li>
</ul>
</section>
</section>
<section id="mutual-information-accounting-for-relevance-and-redundancy-ofat-variants">
<h2>Mutual Information Accounting For Relevance and Redundancy OFAT Variants</h2>
<p>I propose that for one-feature-at-a-time (OFAT) predictor feature ranking (predictor feature subset, <span class="math notranslate nohighlight">\(S = [X_i]\)</span> and <span class="math notranslate nohighlight">\(|S| = 1\)</span>) we modify this to the following calculation:</p>
<ul class="simple">
<li><p><strong>relevance</strong> - the mutual information between the selected predictor feature, <span class="math notranslate nohighlight">\(X_i\)</span>, and the response feature, <span class="math notranslate nohighlight">\(Y\)</span></p></li>
<li><p><strong>redundancy</strong> - the average mutual information between the selected predictor feature, <span class="math notranslate nohighlight">\(X_i\)</span>, and the remaining predictor features, <span class="math notranslate nohighlight">\(X_{\alpha}, \alpha \ne i\)</span>.</p></li>
<li><p>we use the quotient form of the calculation from Gulgezen, Cataltepe and Yu (2009).</p></li>
</ul>
<p>Our modified version of the Maximum Relevance - Minumum Redundancy (MRMR) objective function for OFAT ranking scores the selected predictor feature <span class="math notranslate nohighlight">\(X_i\)</span>‚Äôs <strong>relevance</strong> as its mutual information with the response feature:</p>
<p>\begin{equation}
I(X_i,Y)
\end{equation}</p>
<p>and <strong>redundancy</strong> between the selected predictor feature, <span class="math notranslate nohighlight">\(X_i\)</span>, and the remaining predictor features:</p>
<p>\begin{equation}
\frac{1}{|S|-1} \sum_{\alpha=1, \alpha \ne i}^m I(X_i,X_{\alpha})
\end{equation}</p>
<p>were <span class="math notranslate nohighlight">\(X\)</span> are predictor features, <span class="math notranslate nohighlight">\(Y\)</span> is the response feature, <span class="math notranslate nohighlight">\(X_i\)</span> is the specific predictor feature being scored and <span class="math notranslate nohighlight">\(|S|\)</span> is the number of predictor features and <span class="math notranslate nohighlight">\(I()\)</span> is mutual information between the indicated features. One formulation is a simple difference, relevance minus redundancy,</p>
<div class="math notranslate nohighlight">
\[
\Phi_{\Delta}(X_i,Y) = I(X_{\alpha},Y) - \frac{1}{|S|-1} \sum_{\beta=1, \alpha \ne \beta}^m I( X_{\alpha},X_{\beta} ) 
\]</div>
<p>an alternative is a ratio,</p>
<div class="math notranslate nohighlight">
\[
\Phi_{r}(X_i,Y) = \frac{ I(X_i,Y) }{ \frac{1}{|S|-1} \sum_{\alpha=1, \alpha \ne i}^m I(X_i,X_{\alpha})}
\]</div>
<p>Here the feature ranks for the mutual information relevance minus redundancy, <span class="math notranslate nohighlight">\(\Phi_{\Delta}(X_i,Y)\)</span>, approach.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">obj_mutual</span> <span class="o">=</span> <span class="n">mutual_information_objective</span><span class="p">(</span><span class="n">x_df</span><span class="p">,</span><span class="n">y_df</span><span class="p">)</span>
<span class="n">indices_obj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">obj_mutual</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>              <span class="c1"># find indices for descending order</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                        <span class="c1"># plot the relative mutual information </span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"One-at-a-Time MRMR Objective Function for Mutual Information-based Feature Selection"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">obj_mutual</span><span class="p">[</span><span class="n">indices_obj</span><span class="p">],</span>
       <span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s2">"black"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices_obj</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Feature Importance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c0b176e4990bdb326b638222358d06af771613f6ae47153d42a7e2614c7d3c28.png" src="../Images/ce31fdb26712840a6f374091a92555c3.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/c0b176e4990bdb326b638222358d06af771613f6ae47153d42a7e2614c7d3c28.png"/>
</div>
</div>
<section id="delta-mutual-information-quotient-accounting-for-relevance-and-redundancy">
<h3>Delta Mutual Information Quotient Accounting for Relevance and Redundancy</h3>
<p>We use adapt the mutual information quotient from Gulgezen, Cataltepe and Yu (2009) to develop an OFAT ranking metric.</p>
<p>The standard MRMR objective function that scores the subset of features‚Äô <strong>relevance</strong> between the subset of predictor features and the response feature:</p>
<p>\begin{equation}
\frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) }
\end{equation}</p>
<p>and <strong>redundancy</strong> between the subset of predictor features:</p>
<p>\begin{equation}
\frac{1}{|S|^2} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})}
\end{equation}</p>
<p>To find the most informative subset of predictor features we must find the subset of features that maximize relevance while minimizing redundancy. We can accomplish this by maximizing either of these two formulations,</p>
<p>\begin{equation}
MID = \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } - \frac{1}{|S|^2} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})}
\end{equation}</p>
<p>or</p>
<p>\begin{equation}
MIQ = \frac{ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|^2} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} }
\end{equation}</p>
<p>I suggest feature ranking through the calculation of the change in <span class="math notranslate nohighlight">\(MIQ\)</span> via inclusion and removal of a specific predictor feature (<span class="math notranslate nohighlight">\(X_i\)</span>).</p>
<p>\begin{equation}
\Delta MIQ_i = \frac{ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|^2} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} } - \frac{ \frac{1}{|S|}{\sum_{\alpha=1,\alpha \ne i}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|^2} {\sum_{\alpha=1,\alpha \ne i}^m \sum_{\beta=1,\beta \ne i}^m I(X_{\alpha},X_{\beta})} }
\end{equation}</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">delta_mutual_information</span> <span class="o">=</span> <span class="n">delta_mutual_information_quotient</span><span class="p">(</span><span class="n">x_df</span><span class="p">,</span><span class="n">y_df</span><span class="p">)</span>

<span class="n">indices_delta_mutual_information</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">delta_mutual_information</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># find indices for descending order</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot the relative mutual information </span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Delta Mutual Information Quotient"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">delta_mutual_information</span><span class="p">[</span><span class="n">indices_delta_mutual_information</span><span class="p">],</span>
       <span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span><span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices_delta_mutual_information</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Feature Importance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/03d7d53a3d4abf4c562eeb53cbb1d74427a39a38a569d1e60c953a3ce9fe55a8.png" src="../Images/21f9cee66dbe4d7f6bc8cac154c1ad85.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/03d7d53a3d4abf4c562eeb53cbb1d74427a39a38a569d1e60c953a3ce9fe55a8.png"/>
</div>
</div>
<p>It is intructive to compare delta mutual information and variance inflation factor ranking. Both of these methods account for predictor feature redundancy.</p>
<ul class="simple">
<li><p>but VIF assumes linearity and does not account for relevance</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="n">delta_mutual_information</span><span class="p">),</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="o">-</span><span class="n">vif_values</span><span class="p">),</span><span class="n">c</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="n">delta_mutual_information</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="o">-</span><span class="n">vif_values</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="mf">0.1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Delta Mutual Information Rank'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Variance Inflation Factor Rank'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Variance Inflation Factor vs. Delta Mutual Information Ranking'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mf">0.1</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],[</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s1">'coral'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">'dodgerblue'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/753de82205baf137ac8f92671732ef8708fc686e982b161ca2b05ba1095ee90e.png" src="../Images/3028e58a56f928ebb26de84cfb6e1f54.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/753de82205baf137ac8f92671732ef8708fc686e982b161ca2b05ba1095ee90e.png"/>
</div>
</div>
<p>From mutual information we can observe that porosity, permeability then total organic carbon and brittleness have the greatest departure from general independence.</p>
</section>
</section>
<section id="summary-of-all-bivariate-metrics">
<h2>Summary of All Bivariate Metrics</h2>
<p>We have a wide array of criteria to rank our features.</p>
<ul class="simple">
<li><p>the <span class="math notranslate nohighlight">\(B\)</span> coefficient have the same issue as covariance, sensitivity to the univariate variance</p></li>
<li><p>the <span class="math notranslate nohighlight">\(\beta\)</span> coefficients remove this sensitivity and are consistent with previous results.</p></li>
</ul>
<p>Given all of these methods, I would rank the variables as:</p>
<ol class="arabic simple">
<li><p>Porosity</p></li>
<li><p>Vitrinite Reflectance</p></li>
<li><p>Acoustic Impedance</p></li>
<li><p>Permeability</p></li>
<li><p>Total Organic Carbon</p></li>
<li><p>Brittleness</p></li>
</ol>
<p>I have assigned these ranks by observing the general trend in these metrics. Of course, we could make a more quantitative score and rank by weighting each method.</p>
<p>As mentioned before, we should not neglect expert knowledge. If additional information is known about physical processes, causation, and reliability and availability of variables this should be integrated into assigning ranks.</p>
<p>We include a bonus method here, recursive feature elimination, but only provide a simple linear regression model example. More could be done with more complicated models.</p>
</section>
<section id="recursive-feature-elimination">
<h2>Recursive Feature Elimination</h2>
<p>Recursive Feature Elimination (RFE) method works by recursively removing features and building a model with the remaining features.</p>
<ul class="simple">
<li><p>for the first step, all features are used to build a model and the features are ranked by feature importance or the <span class="math notranslate nohighlight">\(\beta\)</span> coefficient</p></li>
<li><p>the least important feature is pruned and the model is rebuilt</p></li>
<li><p>this is repeated until there is only one feature remaining</p></li>
</ul>
<p>In this code we make a prediction model based on multilinear regression and indicate that we want to find the best feature based on recursive feature elimination. The algorithm assigns rank <span class="math notranslate nohighlight">\(1,\ldots,m\)</span> for all features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">rfe_linear</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">LinearRegression</span><span class="p">(),</span><span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># set up RFE linear regression model</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'const'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>                                <span class="c1"># let's add one's for the constant term</span>
<span class="n">rfe_linear</span> <span class="o">=</span> <span class="n">rfe_linear</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">resp</span><span class="p">]))</span> <span class="c1"># recursive elimination</span>
<span class="n">dfS</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'const'</span><span class="p">,</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>                               <span class="c1"># remove the ones</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Recursive Feature Elimination: Multilinear Regression'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Rank #'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="s1">' '</span> <span class="o">+</span> <span class="n">pred</span><span class="p">[</span><span class="n">rfe_linear</span><span class="o">.</span><span class="n">ranking_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Recursive Feature Elimination: Multilinear Regression
Rank #1 Brittle
Rank #2 TOC
Rank #3 AI
Rank #4 VR
Rank #5 Por
Rank #6 Perm
</pre></div>
</div>
</div>
</div>
<p>The advantages with the recursive elimination method:</p>
<ul class="simple">
<li><p>the actual model can be used in assessing feature ranks</p></li>
<li><p>the ranking is based on accuracy of the estimate</p></li>
</ul>
<p>but this method is sensitive to:</p>
<ul class="simple">
<li><p>choice of model</p></li>
<li><p>training dataset</p></li>
</ul>
<p>The feature ranks are quite different from our previous methods.  Many have moved from the previous assessment. Perhaps we should use a more flexible modeling method.</p>
<p>Let‚Äôs repeat this method with a more flexible machine learning method, a decision tree regression model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>            
<span class="kn">import</span> <span class="nn">geostatspy.GSLIB</span> <span class="k">as</span> <span class="nn">GSLIB</span>                              <span class="c1"># GSLIB utilities, visualization and wrapper</span>
<span class="n">rfe_rf</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span><span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># set up RFE linear regression model</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'const'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>                                <span class="c1"># let's add one's for the constant term</span>

<span class="n">lab_enc</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">();</span> <span class="n">Y_encoded</span> <span class="o">=</span> <span class="n">lab_enc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

<span class="n">rfe_rf</span> <span class="o">=</span> <span class="n">rfe_rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">Y_encoded</span><span class="p">))</span>                    <span class="c1"># recursive elimination</span>
<span class="n">dfS</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'const'</span><span class="p">,</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>                               <span class="c1"># remove the ones</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Recursive Feature Elimination: Random Forest Regression'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Rank #'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="s1">' '</span> <span class="o">+</span> <span class="n">pred</span><span class="p">[</span><span class="n">rfe_rf</span><span class="o">.</span><span class="n">ranking_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Recursive Feature Elimination: Random Forest Regression
Rank #1 Por
Rank #2 VR
Rank #3 Brittle
Rank #4 Perm
Rank #5 TOC
Rank #6 AI
</pre></div>
</div>
</div>
</div>
<p>Once again, recursive feature elimination for feature ranking is sensitive to the accuracy of the model.</p>
<ul class="simple">
<li><p>the actual prediction model must have its associated hyperparameters tuned and the model accuracy checked.</p></li>
<li><p>for example, in this case the multilinear regression feature ranks are unreliable due to the poor accuracy of the linear model.</p></li>
</ul>
</section>
<section id="shapley-values-for-feature-ranking">
<h2>Shapley Values for Feature Ranking</h2>
<p>Let‚Äôs take a random subset of the data, as background values to evaluate our model.</p>
<ul class="simple">
<li><p>we subset for faster calculation</p></li>
<li><p>we should evaluate / enforce efficient coverage of the predictor feature space</p></li>
</ul>
<p>Since Shapley values are model based, we must start with building a model</p>
<section id="build-a-random-forest-model">
<h3>Build a Random Forest Model</h3>
<p>Since Shapley is model based we need to build a model</p>
<ul class="simple">
<li><p>Let‚Äôs start with a good random forest model, observe Shapley and then return here and modify the model</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">73093</span>                                                  <span class="c1"># set the random forest hyperparameters</span>

<span class="c1"># #Underfit random forest</span>
<span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1">#Overfit random forest</span>
<span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">6</span>

<span class="c1"># #Good random forest</span>
<span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">rfr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">)</span>
<span class="n">rfr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">)</span>

<span class="n">Y_hat</span> <span class="o">=</span> <span class="n">predict_train</span> <span class="o">=</span> <span class="n">rfr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">MSE</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">Y_hat</span><span class="p">)</span>
<span class="n">Var_Explained</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">explained_variance_score</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">Y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Mean Squared Error on Training = '</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">MSE</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="s1">', Variance Explained ='</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">Var_Explained</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

<span class="n">importances</span> <span class="o">=</span> <span class="n">rfr</span><span class="o">.</span><span class="n">feature_importances_</span>               <span class="c1"># expected (global) importance over the forest fore each predictor feature</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">([</span><span class="n">rfr</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">rfr</span><span class="o">.</span><span class="n">estimators_</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">Y_hat</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Random Forest Model'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Actual Production (MCFPD)'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Estimated Production (MCFPD)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">7000</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">7000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">7000</span><span class="p">,</span><span class="mi">7000</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">head_length</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">head_width</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Feature Importances"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="n">pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">],</span><span class="n">rfr</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">std</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">)</span>
<span class="c1">#plt.xticks(range(X.shape[1]), indices)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Feature Importance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Mean Squared Error on Training =  428100.87 , Variance Explained = 0.82
</pre></div>
</div>
<img alt="_images/cbc4aa1b1542fb9bf3527dc1f3f615180d1f3b63d21eb259a11b916d03595cea.png" src="../Images/6262d930974113cf78a782fd287a8df6.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/cbc4aa1b1542fb9bf3527dc1f3f615180d1f3b63d21eb259a11b916d03595cea.png"/>
</div>
</div>
</section>
</section>
<section id="calculate-shapley-values">
<h2>Calculate Shapley Values</h2>
<p>Let‚Äôs select some background data at random to calculate local Shapley values and then summarize with global Shapley measures.</p>
<p>Background Samples are selected as a random subset from all the data. Why not just use all the data as background?</p>
<ul class="simple">
<li><p><strong>Shapley values can be computationally expensive to calculate</strong>, we need all the combinations of models to get all the predictions for marginal contributions that are summarized as Shapley values</p></li>
<li><p><strong>The original data may be sampled in a biased manner</strong>, then we would want to ensure that the background data are representative, i.e., sampled from the original data to reduce bias to avoid bias in our feature importance assessment</p></li>
<li><p><strong>Generalization vs. specific prediction cases</strong>, if all the data are used as background we get an overall data assessment of feature importance, but we may want to carefully select data to explore specific prediction cases</p></li>
</ul>
<p>For simplicity here, we just select randomly selection <span class="math notranslate nohighlight">\(n\)</span> data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">background</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">nsamples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">73073</span><span class="p">)</span> 
<span class="n">model_explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">rfr</span><span class="p">)</span>
<span class="n">shap_values</span> <span class="o">=</span> <span class="n">model_explainer</span><span class="o">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">background</span><span class="p">)</span> <span class="c1"># global Shapley Measures</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="local-shapley-values">
<h2>Local Shapley Values</h2>
<p>Let‚Äôs start by looking at the local Shapley values to demonstrate the concept of efficiency.</p>
<ul class="simple">
<li><p>first let‚Äôs confirm that the output from the shap function is a <span class="math notranslate nohighlight">\(\left[n_{background}, m\right]\)</span> nd array.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">shap_values</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(50, 6)
</pre></div>
</div>
</div>
</div>
<p>We have the local Shapley values for each prediction for the background cases. Let‚Äôs visualize one to demonstrate this.</p>
<ul class="simple">
<li><p>I coded this custom visualization to clearly communicate local Shapley values and the concept of efficiency.</p></li>
<li><p>We start at the average of the training response feature and add the local Shapely values for each predictor feature to reach the prediction.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nback</span> <span class="o">=</span> <span class="mi">7</span>

<span class="n">resp_avg</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">Y_hat</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">rfr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">background</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="n">nback</span><span class="p">]])</span>

<span class="n">current</span> <span class="o">=</span> <span class="n">resp_avg</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="p">,</span><span class="n">current</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.3</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="n">current</span><span class="p">],[</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.3</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="p">,</span><span class="n">current</span><span class="o">+</span><span class="mi">2</span><span class="p">],[</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.2</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">current</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">0.5</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="s1">'blue'</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="p">,</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]],[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="p">,</span><span class="n">current</span><span class="p">],[</span><span class="n">i</span><span class="o">+</span><span class="mf">0.6</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">],</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]],[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">1.3</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]],[</span><span class="n">i</span><span class="o">+</span><span class="mf">1.2</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">1.3</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">],</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="mi">2</span><span class="p">],[</span><span class="n">i</span><span class="o">+</span><span class="mf">1.3</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">1.2</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'+ '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">],</span><span class="mi">0</span><span class="p">)),[</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">1.1</span><span class="p">],</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'- '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]),</span><span class="mi">0</span><span class="p">)),[</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">1.1</span><span class="p">],</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="p">,</span><span class="n">current</span><span class="p">],[</span><span class="n">i</span><span class="o">+</span><span class="mf">0.7</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="n">current</span><span class="p">],[</span><span class="n">i</span><span class="o">+</span><span class="mf">0.9</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="p">,</span><span class="n">current</span><span class="o">+</span><span class="mi">2</span><span class="p">],[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">0.9</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">resp_avg</span><span class="p">,</span><span class="n">resp_avg</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mf">1.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">yhat</span><span class="p">,</span><span class="n">yhat</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mf">1.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Response Feature, Training Average'</span><span class="p">,[</span><span class="n">resp_avg</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Model Prediction'</span><span class="p">,[</span><span class="n">yhat</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mi">2</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="sa">r</span><span class="s1">'None / $\overline</span><span class="si">{y}</span><span class="s1">$'</span><span class="p">]</span> <span class="o">+</span> <span class="n">pred</span> <span class="o">+</span> <span class="p">[</span><span class="sa">r</span><span class="s1">'$\hat</span><span class="si">{y}</span><span class="s1">=f(X)$'</span><span class="p">])</span>
<span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Production (MCFPD)'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Local Shapley Values, Background Index: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">nback</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4687606001e63cc73cb21d138e664ae5506aa6589d98a6bb132426a1eeacd218.png" src="../Images/223f16f325ce6407f83bcbcad31a87a9.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/4687606001e63cc73cb21d138e664ae5506aa6589d98a6bb132426a1eeacd218.png"/>
</div>
</div>
<p>Now I show you the built in plotting methods to communicate the same thing with the shap Python Package.</p>
</section>
<section id="shapley-force-plot">
<h2>Shapley Force Plot</h2>
<p>We can simulaneously visualize all of the Shapley values for all of the sample data in the order of the background data set.</p>
<ul class="simple">
<li><p>blue indicates reduction in the predicted production and red indicates increase in predicted production</p></li>
</ul>
<p>We are visualizing over all background sample data at once. Reorder by original sample ordering and select the nback index to compare to the above plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">shap</span><span class="o">.</span><span class="n">force_plot</span><span class="p">(</span><span class="n">model_explainer</span><span class="o">.</span><span class="n">expected_value</span><span class="p">,</span><span class="n">shap_values</span><span class="p">,</span><span class="n">background</span><span class="p">,</span><span class="n">out_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Production'</span><span class="p">],</span><span class="n">feature_names</span><span class="o">=</span><span class="n">pred</span><span class="p">,)</span>  
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<div id="i38ZAMPLKNLGY5L30ROKQ">
<div style="color: #900; text-align: center;">
  <b>Visualization omitted, Javascript library not loaded!</b><br/>
  Have you run `initjs()` in this notebook? If this notebook was from another
  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing
  this notebook on github the Javascript has been stripped for security. If you are using
  JupyterLab this error is because a JupyterLab extension has not yet been written.
</div></div>
 </div></div>
</div>
</section>
<section id="local-force-plot">
<h2>Local Force Plot</h2>
<p>We pick a specific sample from the background and visualize the force plot.</p>
<ul class="simple">
<li><p>We can see the genesis of the plot above, Shapley values for all features given a local set of values in sample <span class="math notranslate nohighlight">\(i\)</span>, (<span class="math notranslate nohighlight">\(x_i\)</span>).</p></li>
</ul>
<p>Compare this result to the above custom plot that I made and you will see that it communicates the same information.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">shap</span><span class="o">.</span><span class="n">force_plot</span><span class="p">(</span><span class="n">model_explainer</span><span class="o">.</span><span class="n">expected_value</span><span class="p">,</span><span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">],</span><span class="n">background</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="n">nback</span><span class="p">]],</span><span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<div id="i4LGR5S1DL4ZR5IW69141">
<div style="color: #900; text-align: center;">
  <b>Visualization omitted, Javascript library not loaded!</b><br/>
  Have you run `initjs()` in this notebook? If this notebook was from another
  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing
  this notebook on github the Javascript has been stripped for security. If you are using
  JupyterLab this error is because a JupyterLab extension has not yet been written.
</div></div>
 </div></div>
</div>
<p>Appreciation to Xuesong Ma for the suggestion to improve the above local Shapley value content and visualizations.</p>
</section>
<section id="global-shapley-values">
<h2>Global Shapley Values</h2>
<p>Let‚Äôs review the global Shapley measures.</p>
<ul class="simple">
<li><p>sorted bar chart of the arithmetic average of the absolute SHAP value over the background data</p></li>
<li><p>sorted plot of the SHAP value over the background data</p></li>
<li><p>plot of the SHAP value over the background data as a violin plot</p></li>
</ul>
<p>Note: all of these methods are applying the global average (<span class="math notranslate nohighlight">\(E[X_i]\)</span>) for each feature to impute for those cases not including feature <span class="math notranslate nohighlight">\(i\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">pred</span><span class="p">,</span> <span class="n">shap_values</span> <span class="o">=</span> <span class="n">shap_values</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">background</span><span class="p">,</span> <span class="n">plot_type</span><span class="o">=</span><span class="s2">"bar"</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s2">"darkorange"</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Predictor Features'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">pred</span><span class="p">,</span> <span class="n">shap_values</span> <span class="o">=</span> <span class="n">shap_values</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">background</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">pred</span><span class="p">,</span> <span class="n">shap_values</span> <span class="o">=</span> <span class="n">shap_values</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">background</span><span class="p">,</span><span class="n">plot_type</span> <span class="o">=</span> <span class="s2">"violin"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2f143182e5f4f722358c7743d0e8d7cd49f8f2a4e3afecee9fcacac06e45af95.png" src="../Images/e3945b1cd0e515c4f1adfcc33e2436d8.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/2f143182e5f4f722358c7743d0e8d7cd49f8f2a4e3afecee9fcacac06e45af95.png"/>
</div>
</div>
<p>The the center and right plots show the Shapley values for each feature over all the randomly selected background samples, while the plot on the left is the bar chart of the mean absolute Shapley values.</p>
<ul class="simple">
<li><p>Porosity, Permeability and TOC are the top features</p></li>
</ul>
</section>
<section id="comments">
<h2>Comments</h2>
<p>This was a basic treatment of feature ranking. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a> and the YouTube lecture links at the start of this chapter with resource links in the videos‚Äô descriptions.</p>
<p>I hope this is helpful,</p>
<p><em>Michael</em></p>
</section>
<section id="about-the-author">
<h2>About the Author</h2>
<figure style="text-align: center;">
  <img src="../Images/eb709b2c0a0c715da01ae0165efdf3b2.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/intro/michael_pyrcz_officeshot_jacket.jpg"/>
  <figcaption style="text-align: center;"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.
</figcaption>
</figure>
<p>Michael Pyrcz is a professor in the <a class="reference external" href="https://cockrell.utexas.edu/faculty-directory/alphabetical/p">Cockrell School of Engineering</a>, and the <a class="reference external" href="https://www.jsg.utexas.edu/researcher/michael_pyrcz/">Jackson School of Geosciences</a>, at <a class="reference external" href="https://www.utexas.edu/">The University of Texas at Austin</a>, where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,</p>
<ul class="simple">
<li><p>the principal investigator of the <a class="reference external" href="https://fri.cns.utexas.edu/energy-analytics">Energy Analytics</a> freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin</p></li>
<li><p>an associate editor for <a class="reference external" href="https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board">Computers and Geosciences</a>, and a board member for <a class="reference external" href="https://link.springer.com/journal/11004/editorial-board">Mathematical Geosciences</a>, the International Association for Mathematical Geosciences.</p></li>
</ul>
<p>Michael has written over 70 <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en">peer-reviewed publications</a>, a <a class="reference external" href="https://pypi.org/project/geostatspy/">Python package</a> for spatial data analytics, co-authored a textbook on spatial data analytics, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a> and author of two recently released e-books, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy</a> and <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html">Applied Machine Learning in Python: a Hands-on Guide with Code</a>.</p>
<p>All of Michael‚Äôs university lectures are available on his <a class="reference external" href="https://www.youtube.com/@GeostatsGuyLectures">YouTube Channel</a> with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his <a class="reference external" href="https://github.com/GeostatsGuy">GitHub account</a>, to support any interested students and working professionals with evergreen content. To find out more about Michael‚Äôs work and shared educational resources visit his <span class="xref myst">Website</span>.</p>
</section>
<section id="want-to-work-together">
<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>I‚Äôm always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
</section>
&#13;

<h2>Motivation for Feature Ranking</h2>
<p>There are often many predictor features (input variables), available for us to work with for building our prediction models.</p>
<ul class="simple">
<li><p>there are good reasons to be selective, throwing in every possible feature is not a good idea!</p></li>
</ul>
<p>In general, for the best prediction model, careful selection of the fewest features that provide the most amount of information is the best practice.</p>
<p>Here‚Äôs why:</p>
<ul class="simple">
<li><p><strong>blunders</strong> - more predictor features result in more complicated workflows that require more professional time and have increased opportunity for mistakes in the workflow</p></li>
<li><p><strong>difficult to visualize</strong> - higher dimensional models, i.e., larger number of predictor features, are more difficult to visualize</p></li>
<li><p><strong>model checking</strong> - more complicated models may be more difficult to interrogate, interpret and QC</p></li>
<li><p><strong>predictor feature redundancy</strong> - more likely to have redundant predictor features. The inclusion of highly redundant and collinear or multicollinear features increases model variance, increase model instability and decreases prediction accuracy for testing</p></li>
<li><p><strong>computational time</strong> - more predictor features generally increase the computational time required to train the model and the computational storage, i.e., the model may be less compact and portable</p></li>
<li><p><strong>model overfit</strong> - the risk of overfit increases with the more features, due to increase in model complexity</p></li>
<li><p><strong>model extrapolation</strong> - many predictor features results in high dimensional model space with less data coverage and more likelihood for model extrapolation that may be inaccurate</p></li>
</ul>
<p>The primary concern with many predictor features is the curse of dimensionality. Let‚Äôs summarize the curse!</p>
&#13;

<h2>Curse of Dimensionality</h2>
<ol class="arabic simple">
<li><p><strong>Data and Model Visualization</strong> - we cannot visualize beyond 3D, i.e., access the model fit to data, evaluate interpolation vs. extrapolation.</p></li>
</ol>
<ul class="simple">
<li><p>consider a 5D example shown as a matrix scatter plot, even in this case there is an extreme marginalization to 2D for each plot,</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/ecf50f66114aec17ea35fde1342d66c4.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static\feature_ranking\matrix_scatterplot.png"/>
  <figcaption style="text-align: center;">Example 5D data as a matrix scatter plot.</figcaption>
</figure>
<ol class="arabic simple" start="2">
<li><p><strong>Sampling</strong> - the number of samples sufficient to infer statistics like the joint probability, <span class="math notranslate nohighlight">\(P(x_1,\ldots,x_m)\)</span>.</p></li>
</ol>
<ul class="simple">
<li><p>recall the calculation of a histogram or normalized histogram: we establish bins and calculate frequencies or probabilities in each bin.</p></li>
<li><p>we require a nominal number of data samples for each bin, so we require <span class="math notranslate nohighlight">\(ùëõ=ùëõ_{ùë†/ùëèùëñùëõ} \cdot ùëõ_{ùëèùëñùëõùë†}\)</span> samples in 1D</p></li>
<li><p>but in mD we required <span class="math notranslate nohighlight">\(n\)</span> samples to calculate the discretized joint probability,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
ùëõ=ùëõ_{ùë†/ùëèùëñùëõ} \cdot ùëõ_{ùëèùëñùëõùë†}^m$
\]</div>
<ul class="simple">
<li><p>for example, 10 samples per bin with 35 bins requires 12,250 samples in 2D, and 428,750 samples in 3D</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/bc8823819263f4497ef6baab93a9ee38.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/feature_ranking/bivariate_example.png"/>
  <figcaption style="text-align: center;">Example 2D data with 35 bins for each feature.</figcaption>
</figure>
<ol class="arabic simple" start="3">
<li><p><strong>Sample Coverage</strong> - the range of the sample values cover the predictor feature space.</p></li>
</ol>
<ul class="simple">
<li><p>fraction of the possible solution space that is sampled, for 1 feature we assume 80% coverage</p></li>
<li><p>remember, we usually, directly sample only <span class="math notranslate nohighlight">\(\frac{1}{10^7}\)</span> of the volume of the subsurface</p></li>
<li><p>yes, the concept of coverage is subjective, how much data to cover? What about gaps? etc.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/d8058511a88a482ed34b0cbd9eb34fec.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/feature_ranking/coverage1D.png"/>
  <figcaption style="text-align: center;">Example 2D data with 35 bins for each feature.</figcaption>
</figure>
<ul class="simple">
<li><p>now if there is 80% coverage for 2 features the 2D coverage is 64%</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/8d96453b3f6c2a92a160fe4329a13d4a.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/feature_ranking/coverage2D.png"/>
  <figcaption style="text-align: center;">Example 2D data with 35 bins for each feature.</figcaption>
</figure>
<ul class="simple">
<li><p>coverage is,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
c =  c_1^m
\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Distorted Space</strong> - high dimensional space is distorted.</p></li>
</ol>
<ul class="simple">
<li><p>take the ratio of the volume of an inscribed hypersphere in a hypercube,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\pi^{\frac{m}{2}}}{m 2^{m-1} \Gamma\left(\frac{m}{2}\right)} \to 0 \quad \text{as} \quad m \to \infty
\]</div>
<ul class="simple">
<li><p>recall, <span class="math notranslate nohighlight">\(\Gamma(ùëõ)=(ùëõ‚àí1)!\)</span>.</p></li>
<li><p>high dimensional space is all corners and no middle and most of high dimensional space is far from the middle (all corners!).</p></li>
<li><p>as a result distances in high dimensional space lose sensitivity, i.e., for any random points in the space the expected pairwise distances all become the same,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lim_{m \to \infty} \left( \mathbb{E}\left[\text{dist}_{\text{max}}(m) - \text{dist}_{\text{min}}(m)\right] \right) \to 0
\]</div>
<ul class="simple">
<li><p>the limit of the expectation of the range of pairwise distances over random points in hyper-dimensional space tends to zero. If distances are almost all the same, Euclidian distance is no longer meaningful!</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/8c8d512cca4eb330150d1ba298831543.png" style="display: block; margin: 0 auto; width: 60%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/feature_ranking/distortion_chart.png"/>
  <figcaption style="text-align: center;">The ratio of the volume of a hypersphere within a hypercube.</figcaption>
</figure>
<ul class="simple">
<li><p>here‚Äôs the severity of the distortion for various dimensionalities,</p></li>
</ul>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>m</p></th>
<th class="head"><p>nD / 2D</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2</p></td>
<td><p>1.0</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>0.28</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>0.003</p></td>
</tr>
<tr class="row-odd"><td><p>20</p></td>
<td><p>0.00000003</p></td>
</tr>
</tbody>
</table>
<ol class="arabic simple" start="5">
<li><p><strong>Multicollinearity</strong> - higher dimensional datasets are more likely to have collinearity or multicollinearity.</p></li>
</ol>
<ul class="simple">
<li><p>Feature linearly described by other features resulting in high model variance.</p></li>
</ul>
&#13;

<h2>What is Feature Ranking?</h2>
<p>Feature ranking is a set of metrics that assign relative importance or value to each predictor feature with respect to information contained for inference and importance in predicting a response feature. There are a wide variety of possible methods to accomplish this. My recommendation is a <strong>‚Äòwide-array‚Äô</strong> approach with multiple analyses and metrics, while understanding the assumptions and limitations of each method.</p>
<p>Here‚Äôs the general types of metrics that we consider for feature ranking.</p>
<ol class="arabic simple">
<li><p>Visual Inspection of Data Distributions and Scatter Plots</p></li>
<li><p>Statistical Summaries</p></li>
<li><p>Model-based</p></li>
<li><p>Recursive Feature Elimination</p></li>
</ol>
<p>Also, we should not neglect expert knowledge. If additional information is known about physical processes, causation, reliability and availability of predictor features this should be integrated into assigning feature ranking.</p>
&#13;

<h2>Load the Required Libraries</h2>
<p>The following code loads the required libraries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">geostatspy.GSLIB</span> <span class="k">as</span> <span class="nn">GSLIB</span>                              <span class="c1"># GSLIB utilities, visualization and wrapper</span>
<span class="kn">import</span> <span class="nn">geostatspy.geostats</span> <span class="k">as</span> <span class="nn">geostats</span>                        <span class="c1"># GSLIB methods convert to Python  </span>
<span class="kn">import</span> <span class="nn">geostatspy</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'GeostatsPy version: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">geostatspy</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>GeostatsPy version: 0.0.71
</pre></div>
</div>
</div>
</div>
<p>We will also need some standard packages. These should have been installed with Anaconda 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">ignore_warnings</span> <span class="o">=</span> <span class="kc">True</span>                                        <span class="c1"># ignore warnings?</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                                            <span class="c1"># ndarrays for gridded data</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>                                           <span class="c1"># DataFrames for tabular data</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>                             <span class="c1"># remove encoding error</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>                     <span class="c1"># for recursive feature selection</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">mutual_info_regression</span>  <span class="c1"># mutual information</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>             <span class="c1"># linear regression model</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>            <span class="c1"># model-based feature importance</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>                                   <span class="c1"># measures to check our models</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span> <span class="c1"># variance inflation factor</span>
<span class="kn">import</span> <span class="nn">os</span>                                                     <span class="c1"># set working directory, run executables</span>
<span class="kn">import</span> <span class="nn">math</span>                                                   <span class="c1"># basic math operations</span>
<span class="kn">import</span> <span class="nn">random</span>                                                 <span class="c1"># for random numbers</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>                               <span class="c1"># for plotting</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span> <span class="n">AutoMinorLocator</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>                  <span class="c1"># custom color maps</span>
<span class="kn">import</span> <span class="nn">matplotlib.ticker</span> <span class="k">as</span> <span class="nn">mtick</span>                             <span class="c1"># control tick label formatting</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>                                         <span class="c1"># for matrix scatter plots</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>                                       <span class="c1"># summary statistics</span>
<span class="kn">import</span> <span class="nn">numpy.linalg</span> <span class="k">as</span> <span class="nn">linalg</span>                                 <span class="c1"># for linear algebra</span>
<span class="kn">import</span> <span class="nn">scipy.spatial</span> <span class="k">as</span> <span class="nn">sp</span>                                    <span class="c1"># for fast nearest neighbor search</span>
<span class="kn">import</span> <span class="nn">scipy.signal</span> <span class="k">as</span> <span class="nn">signal</span>                                 <span class="c1"># kernel for moving window calculation</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jit</span>                                         <span class="c1"># for numerical speed up</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.weightstats</span> <span class="kn">import</span> <span class="n">DescrStatsW</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># plot all grids below the plot elements</span>
<span class="k">if</span> <span class="n">ignore_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>                                   
    <span class="kn">import</span> <span class="nn">warnings</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>                                         <span class="c1"># color map</span>
</pre></div>
</div>
</div>
</div>
<p>For the Shapley value approach for feature ranking we need an additional package and to start up javascript support.</p>
<ul class="simple">
<li><p>after running this block you should see a hexagon with the text ‚Äòjs‚Äô to indicate that javascript is ready</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">sys</span>
<span class="c1">#!{sys.executable} -m pip install shap</span>
<span class="kn">import</span> <span class="nn">shap</span>
<span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div align="center"><img src="../Images/70b822753245ba6bb888425de8eb62b5.png"/></div></div></div>
</div>
<p>If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available with the respective package docs.</p>
&#13;

<h2>Design Custom Color Map</h2>
<p>Accounting for significance by masking nonsignificant values</p>
<ul class="simple">
<li><p>for demonstration only currently, could be updated for each plot based on results confidence and uncertainty</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">my_colormap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'RdBu_r'</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>                  <span class="c1"># make a custom colormap</span>
<span class="n">newcolors</span> <span class="o">=</span> <span class="n">my_colormap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>               <span class="c1"># define colormap space</span>
<span class="n">white</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">250</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">250</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">250</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>              <span class="c1"># define white color (4 channel)</span>
<span class="c1">#newcolors[26:230, :] = white                                 # mask all correlations less than abs(0.8)</span>
<span class="c1">#newcolors[56:200, :] = white                                 # mask all correlations less than abs(0.6)</span>
<span class="n">newcolors</span><span class="p">[</span><span class="mi">76</span><span class="p">:</span><span class="mi">180</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">white</span>                                  <span class="c1"># mask all correlations less than abs(0.4)</span>
<span class="n">signif</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">newcolors</span><span class="p">)</span>                            <span class="c1"># assign as listed colormap</span>
         
<span class="n">my_colormap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'inferno'</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>                 <span class="c1"># make a custom colormap</span>
<span class="n">newcolors</span> <span class="o">=</span> <span class="n">my_colormap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>               <span class="c1"># define colormap space</span>
<span class="n">white</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">250</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">250</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">250</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>              <span class="c1"># define white color (4 channel)</span>
<span class="c1">#newcolors[26:230, :] = white                                 # mask all correlations less than abs(0.8)</span>
<span class="n">newcolors</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">12</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">white</span>                                    <span class="c1"># mask all correlations less than abs(0.6)</span>
<span class="c1">#newcolors[86:170, :] = white                                 # mask all correlations less than abs(0.4)</span>
<span class="n">sign1</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">newcolors</span><span class="p">)</span>                             <span class="c1"># assign as listed colormap</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Declare Functions</h2>
<p>Here‚Äôs a couple of functions to assist with calculating metrics for ranking and other plots:</p>
<ul class="simple">
<li><p><strong>plot_corr</strong> - plot a correlation matrix</p></li>
<li><p><strong>partial_corr</strong> - partial correlation coefficient</p></li>
<li><p><strong>semipar_corr</strong> - semipartial correlation coefficient</p></li>
<li><p><strong>mutual_matrix</strong> - mutual information matrix, matrix of all pairwise mutual information</p></li>
<li><p><strong>mutual_information_objective</strong> - my modified version of the MRMR loss function (Ixy - average(Ixx)) for feature ranking (uses all other predictor features)</p></li>
<li><p><strong>delta_mutual_information_quotient</strong> - change in mutual information quotient by adding and removing a specific feature (uses all other predictor features for the comparison)</p></li>
<li><p><strong>weighted_avg_and_std</strong> - average and standard deviation account for data weights</p></li>
<li><p><strong>weighted_percentile</strong> - percentile accounting for data weights</p></li>
<li><p><strong>histogram_bounds</strong> - add confidence intervals to histograms</p></li>
<li><p><strong>add_grid</strong> - convenience function to add major and minor gridlines to improve plot interpretability</p></li>
</ul>
<p>Here are the functions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">mmin</span><span class="p">,</span><span class="n">mmax</span><span class="p">,</span><span class="n">nominal</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span> <span class="c1"># feature ranking plot</span>
    <span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">);</span> <span class="n">mask_low</span> <span class="o">=</span> <span class="n">nominal</span><span class="o">-</span><span class="n">mask</span><span class="o">*</span><span class="p">(</span><span class="n">nominal</span><span class="o">-</span><span class="n">mmin</span><span class="p">);</span> <span class="n">mask_high</span> <span class="o">=</span> <span class="n">nominal</span><span class="o">+</span><span class="n">mask</span><span class="o">*</span><span class="p">(</span><span class="n">mmax</span><span class="o">-</span><span class="n">nominal</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">mpred</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span><span class="s1">'r--'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mpred</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&lt;</span> <span class="n">nominal</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'dodgerblue'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mpred</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&gt;</span> <span class="n">nominal</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'lightcoral'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">mpred</span><span class="p">,</span><span class="n">mask_low</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&lt;</span> <span class="n">mask_low</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">mpred</span><span class="p">,</span><span class="n">mask_high</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&gt;</span> <span class="n">mask_high</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">mmin</span><span class="p">,</span><span class="n">mmax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">mpred</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">();</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mf">270.0</span><span class="p">)</span>
    <span class="k">return</span>

<span class="k">def</span> <span class="nf">plot_corr</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">limits</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span>                 <span class="c1"># plots a graphical correlation matrix </span>
    <span class="n">my_colormap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'RdBu_r'</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>          
    <span class="n">newcolors</span> <span class="o">=</span> <span class="n">my_colormap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
    <span class="n">white</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">white_low</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">128</span> <span class="o">-</span> <span class="n">mask</span><span class="o">*</span><span class="mi">128</span><span class="p">);</span> <span class="n">white_high</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">128</span><span class="o">+</span><span class="n">mask</span><span class="o">*</span><span class="mi">128</span><span class="p">)</span>
    <span class="n">newcolors</span><span class="p">[</span><span class="n">white_low</span><span class="p">:</span><span class="n">white_high</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">white</span>                <span class="c1"># mask all correlations less than abs(0.8)</span>
    <span class="n">newcmp</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">newcolors</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="n">fignum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">vmin</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="n">limits</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">limits</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">newcmp</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">);</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s1">'bottom'</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">tick_bottom</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">orientation</span> <span class="o">=</span> <span class="s1">'vertical'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mf">270.0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">partial_corr</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>                                          <span class="c1"># partial correlation by Fabian Pedregosa-Izquierdo, f@bianp.net</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">P_corr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="n">P_corr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
            <span class="n">idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">idx</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">beta_i</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">C</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">],</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">beta_j</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">C</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">],</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">res_j</span> <span class="o">=</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span> <span class="n">beta_i</span><span class="p">)</span>
            <span class="n">res_i</span> <span class="o">=</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta_j</span><span class="p">)</span>
            <span class="n">corr</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">res_i</span><span class="p">,</span> <span class="n">res_j</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">P_corr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span>
            <span class="n">P_corr</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span>
    <span class="k">return</span> <span class="n">P_corr</span>

<span class="k">def</span> <span class="nf">semipartial_corr</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>                                      <span class="c1"># Michael Pyrcz modified the function above by Fabian Pedregosa-Izquierdo, f@bianp.net for semipartial correlation</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">P_corr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="n">P_corr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
            <span class="n">idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">idx</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">beta_i</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">C</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">],</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">res_j</span> <span class="o">=</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span> <span class="n">beta_i</span><span class="p">)</span>
            <span class="n">res_i</span> <span class="o">=</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> 
            <span class="n">corr</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">res_i</span><span class="p">,</span> <span class="n">res_j</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">P_corr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span>
            <span class="n">P_corr</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span>
    <span class="k">return</span> <span class="n">P_corr</span>

<span class="k">def</span> <span class="nf">mutual_matrix</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">features</span><span class="p">):</span>                               <span class="c1"># calculate mutual information matrix</span>
    <span class="n">mutual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ifeature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">jfeature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
                <span class="n">mutual</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mutual</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">mutual</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ifeature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
        <span class="n">mutual</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">return</span> <span class="n">mutual</span>

<span class="k">def</span> <span class="nf">mutual_information_objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>                        <span class="c1"># modified from MRMR loss function, Ixy - average(Ixx)</span>
    <span class="n">mutual_information_quotient</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">icol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">Vx</span> <span class="o">=</span> <span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">Ixx_mat</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">mcol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">m</span><span class="p">:</span>
                <span class="n">Ixx_mat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>
        <span class="n">Wx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">Ixx_mat</span><span class="p">)</span>
        <span class="n">mutual_information_quotient</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Vx</span><span class="o">/</span><span class="n">Wx</span><span class="p">)</span>
    <span class="n">mutual_information_quotient</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">mutual_information_quotient</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mutual_information_quotient</span>

<span class="k">def</span> <span class="nf">delta_mutual_information_quotient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>                   <span class="c1"># standard mutual information quotient</span>
    <span class="n">delta_mutual_information_quotient</span> <span class="o">=</span> <span class="p">[]</span>               
    
    <span class="n">Ixy</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">mcol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">Ixy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>
    <span class="n">Vs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">Ixy</span><span class="p">)</span>
    <span class="n">Ixx</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">mcol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">ncol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
            <span class="n">Ixx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>
    <span class="n">Ws</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">Ixx</span><span class="p">)</span> 
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">icol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>          
        <span class="n">Ixy_s</span> <span class="o">=</span> <span class="p">[]</span>                                          
        <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">mcol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">m</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>
                <span class="n">Ixy_s</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>
        <span class="n">Vs_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">Ixy_s</span><span class="p">)</span>
        <span class="n">Ixx_s</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">mcol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">m</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">ncol</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">n</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>
                        <span class="n">Ixx_s</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>                  
        <span class="n">Ws_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">Ixx_s</span><span class="p">)</span>
        <span class="n">delta_mutual_information_quotient</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">Vs</span><span class="o">/</span><span class="n">Ws</span><span class="p">)</span><span class="o">-</span><span class="p">(</span><span class="n">Vs_s</span><span class="o">/</span><span class="n">Ws_s</span><span class="p">))</span>
    
    <span class="n">delta_mutual_information_quotient</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">delta_mutual_information_quotient</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  
    <span class="k">return</span> <span class="n">delta_mutual_information_quotient</span>

<span class="k">def</span> <span class="nf">weighted_avg_and_std</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>                    <span class="c1"># calculate weighted statistics (Eric O Lebigot, stack overflow)</span>
    <span class="n">average</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">((</span><span class="n">values</span><span class="o">-</span><span class="n">average</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">average</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">weighted_percentile</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">perc</span><span class="p">):</span>                 <span class="c1"># calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) </span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> 
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> 
    <span class="n">cdf</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">perc</span><span class="p">,</span> <span class="n">cdf</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">histogram_bounds</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">color</span><span class="p">):</span>                   <span class="c1"># add uncertainty bounds to a histogram          </span>
    <span class="n">p10</span> <span class="o">=</span> <span class="n">weighted_percentile</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="mf">0.1</span><span class="p">);</span> <span class="n">avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">);</span> <span class="n">p90</span> <span class="o">=</span> <span class="n">weighted_percentile</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="mf">0.9</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">p10</span><span class="p">,</span><span class="n">p10</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">45</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">'dashed'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">avg</span><span class="p">,</span><span class="n">avg</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">45</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">p90</span><span class="p">,</span><span class="n">p90</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">45</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">'dashed'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add_grid</span><span class="p">():</span>                                               <span class="c1"># add major and minor gridlines</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Set the Working Directory</h2>
<p>I always like to do this so I don‚Äôt lose files and to simplify subsequent read and writes (avoid including the full address each time).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1">#os.chdir("d:/PGE383")                                   # set the working directory</span>
</pre></div>
</div>
</div>
</div>
<p>You will have to update the part in quotes with your own working directory and the format is different on a Mac (e.g. ‚Äú~/PGE‚Äù).</p>
&#13;

<h2>Loading Tabular Data</h2>
<p>Here‚Äôs the command to load our comma delimited data file in to a Pandas‚Äô DataFrame object.</p>
<p>Let‚Äôs load the provided multivariate, spatial dataset ‚Äòunconv_MV.csv‚Äô. This dataset has variables from 1,000 unconventional wells including:</p>
<ul class="simple">
<li><p>well average porosity</p></li>
<li><p>log transform of permeability (to linearize the relationships with other variables)</p></li>
<li><p>acoustic impedance (kg/m^3 x m/s x 10^6)</p></li>
<li><p>brittleness ratio (%)</p></li>
<li><p>total organic carbon (%)</p></li>
<li><p>vitrinite reflectance (%)</p></li>
<li><p>initial production 90 day average (MCFPD).</p></li>
</ul>
<p>Note, the dataset is synthetic.</p>
<p>We load it with the pandas ‚Äòread_csv‚Äô function into a DataFrame we called ‚Äòmy_data‚Äô and then preview it to make sure it loaded correctly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">idata</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">if</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv'</span><span class="p">)</span> <span class="c1"># load data from Dr. Pyrcz's GitHub repository  </span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="s1">'Prod'</span>                                             <span class="c1"># specify the response feature</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">deep</span> <span class="o">=</span> <span class="kc">True</span><span class="p">);</span> <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'Well'</span><span class="p">,</span><span class="n">response</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="s1">'columns'</span><span class="p">)</span> <span class="c1"># make predictor and response DataFrames</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">response</span><span class="p">]</span>
    
    <span class="n">features</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="p">[</span><span class="n">Y</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>               <span class="c1"># store the names of the features</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">name</span>
    
    <span class="n">xmin</span> <span class="o">=</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">10.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.9</span><span class="p">];</span> <span class="n">xmax</span> <span class="o">=</span> <span class="p">[</span><span class="mf">24.0</span><span class="p">,</span><span class="mf">10.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">,</span><span class="mf">85.0</span><span class="p">,</span><span class="mf">2.2</span><span class="p">,</span><span class="mf">2.9</span><span class="p">]</span> <span class="c1"># set the minimum and maximum values for plotting</span>
    <span class="n">Ymin</span> <span class="o">=</span> <span class="mf">500.0</span><span class="p">;</span> <span class="n">Ymax</span> <span class="o">=</span> <span class="mf">9000.0</span>
    
    <span class="n">predlabel</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Porosity (%)'</span><span class="p">,</span><span class="s1">'Permeability (mD)'</span><span class="p">,</span><span class="s1">'Acoustic Impedance (kg/m2s*10^6)'</span><span class="p">,</span><span class="s1">'Brittleness Ratio (%)'</span><span class="p">,</span> <span class="c1"># set the names for plotting</span>
                 <span class="s1">'Total Organic Carbon (%)'</span><span class="p">,</span><span class="s1">'Vitrinite Reflectance (%)'</span><span class="p">]</span>
    <span class="n">resplabel</span> <span class="o">=</span> <span class="s1">'Normalized Initial Production (MCFPD)'</span>
    
    <span class="n">predtitle</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Porosity'</span><span class="p">,</span><span class="s1">'Permeability'</span><span class="p">,</span><span class="s1">'Acoustic Impedance'</span><span class="p">,</span><span class="s1">'Brittleness Ratio'</span><span class="p">,</span> <span class="c1"># set the units for plotting</span>
                 <span class="s1">'Total Organic Carbon'</span><span class="p">,</span><span class="s1">'Vitrinite Reflectance'</span><span class="p">]</span>
    <span class="n">resptitle</span> <span class="o">=</span> <span class="s1">'Normalized Initial Production'</span>
    
    <span class="n">featurelabel</span> <span class="o">=</span> <span class="n">predlabel</span> <span class="o">+</span> <span class="p">[</span><span class="n">resplabel</span><span class="p">]</span>                        <span class="c1"># make feature labels and titles for concise code</span>
    <span class="n">featuretitle</span> <span class="o">=</span> <span class="n">predtitle</span> <span class="o">+</span> <span class="p">[</span><span class="n">resptitle</span><span class="p">]</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
    
<span class="c1"># elif idata == 1:</span>
<span class="c1">#     names = {'Porosity':'Por'}</span>
    
<span class="c1">#     df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository  </span>
<span class="c1">#     df = df.rename(columns=names)</span>
<span class="c1">#     df['Por'] = df['Por'] * 100.0; df['AI'] = df['AI'] / 1000.0; </span>
<span class="c1">#     df.drop('Unnamed: 0',axis=1,inplace=True) </span>
    
<span class="c1">#     features = df.columns.values.tolist()                          # store the names of the features</span>

<span class="c1">#     xmin = [0.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax = [10000.0,10000.0,1.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting</span>
    
<span class="c1">#     flabel = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',</span>
<span class="c1">#               'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting</span>

<span class="c1">#     ftitle = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',</span>
<span class="c1">#               'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']</span>

<span class="k">elif</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>  
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv'</span><span class="p">)</span> <span class="c1"># load data from Dr. Pyrcz's GitHub repository  </span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="s1">'CumulativeOil'</span>                                             <span class="c1"># specify the response feature</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">deep</span> <span class="o">=</span> <span class="kc">True</span><span class="p">);</span> <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'Well_ID'</span><span class="p">,</span><span class="s1">'X'</span><span class="p">,</span><span class="s1">'Y'</span><span class="p">,</span><span class="n">response</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="s1">'columns'</span><span class="p">)</span> <span class="c1"># make predictor and response DataFrames</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">response</span><span class="p">]</span>
    
    <span class="n">features</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="p">[</span><span class="n">Y</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>               <span class="c1"># store the names of the features</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">name</span>

    <span class="n">xmin</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">6.5</span><span class="p">,</span><span class="mf">1.4</span><span class="p">,</span><span class="mf">1600.0</span><span class="p">,</span><span class="mf">10.0</span><span class="p">,</span><span class="mf">1300.0</span><span class="p">,</span><span class="mf">1.6</span><span class="p">];</span> <span class="n">xmax</span> <span class="o">=</span> <span class="p">[</span><span class="mf">75.0</span><span class="p">,</span><span class="mf">10000.0</span><span class="p">,</span><span class="mf">10000.0</span><span class="p">,</span><span class="mf">19.0</span><span class="p">,</span><span class="mf">500.0</span><span class="p">,</span><span class="mf">8.3</span><span class="p">,</span><span class="mf">3.6</span><span class="p">,</span><span class="mf">6200.0</span><span class="p">,</span><span class="mf">50.0</span><span class="p">,</span><span class="mf">2000.0</span><span class="p">,</span><span class="mf">12.0</span><span class="p">]</span> <span class="c1"># set the minimum and maximum values for plotting</span>
    <span class="n">Ymin</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span> <span class="n">Ymax</span> <span class="o">=</span> <span class="mf">3000.0</span>
    
    <span class="n">predlabel</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Well (ID)'</span><span class="p">,</span><span class="s1">'X (m)'</span><span class="p">,</span><span class="s1">'Y (m)'</span><span class="p">,</span><span class="s1">'Porosity (fraction)'</span><span class="p">,</span><span class="s1">'Permeability (mD)'</span><span class="p">,</span><span class="s1">'Acoustic Impedance (kg/m2s*10^6)'</span><span class="p">,</span>
              <span class="s1">'Density (g/cm^3)'</span><span class="p">,</span><span class="s1">'Compressible velocity (m/s)'</span><span class="p">,</span><span class="s1">'Youngs modulus (GPa)'</span><span class="p">,</span> <span class="s1">'Shear velocity (m/s)'</span><span class="p">,</span> <span class="s1">'Shear modulus (GPa)'</span><span class="p">]</span> 
    <span class="n">resplabel</span> <span class="o">=</span> <span class="s1">'Cumulative Production (MSTB)'</span>
    
    <span class="n">predtitle</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Well'</span><span class="p">,</span><span class="s1">'X'</span><span class="p">,</span><span class="s1">'Y'</span><span class="p">,</span><span class="s1">'Porosity'</span><span class="p">,</span><span class="s1">'Permeability'</span><span class="p">,</span><span class="s1">'Acoustic Impedance'</span><span class="p">,</span>
              <span class="s1">'Density (g/cm^3)'</span><span class="p">,</span><span class="s1">'Compressible velocity'</span><span class="p">,</span><span class="s1">'Youngs modulus'</span><span class="p">,</span> <span class="s1">'Shear velocity'</span><span class="p">,</span> <span class="s1">'Shear modulus'</span><span class="p">]</span> 
    <span class="n">resptitle</span> <span class="o">=</span> <span class="s1">'Cumulative Production'</span>
    
    <span class="n">featurelabel</span> <span class="o">=</span> <span class="n">predlabel</span> <span class="o">+</span> <span class="p">[</span><span class="n">resplabel</span><span class="p">]</span>                        <span class="c1"># make feature labels and titles for concise code</span>
    <span class="n">featuretitle</span> <span class="o">=</span> <span class="n">predtitle</span> <span class="o">+</span> <span class="p">[</span><span class="n">resptitle</span><span class="p">]</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span/><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">SSLCertVerificationError</span><span class="g g-Whitespace">                  </span>Traceback (most recent call last)
<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:1317,</span> in <span class="ni">AbstractHTTPHandler.do_open</span><span class="nt">(self, http_class, req, **http_conn_args)</span>
<span class="g g-Whitespace">   </span><span class="mi">1316</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1317</span>     <span class="n">h</span><span class="o">.</span><span class="n">request</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">get_method</span><span class="p">(),</span> <span class="n">req</span><span class="o">.</span><span class="n">selector</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">headers</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1318</span>               <span class="n">encode_chunked</span><span class="o">=</span><span class="n">req</span><span class="o">.</span><span class="n">has_header</span><span class="p">(</span><span class="s1">'Transfer-encoding'</span><span class="p">))</span>
<span class="g g-Whitespace">   </span><span class="mi">1319</span> <span class="k">except</span> <span class="ne">OSError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span> <span class="c1"># timeout error</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1230,</span> in <span class="ni">HTTPConnection.request</span><span class="nt">(self, method, url, body, headers, encode_chunked)</span>
<span class="g g-Whitespace">   </span><span class="mi">1229</span><span class="w"> </span><span class="sd">"""Send a complete request to the server."""</span>
<span class="ne">-&gt; </span><span class="mi">1230</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_request</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">headers</span><span class="p">,</span> <span class="n">encode_chunked</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1276,</span> in <span class="ni">HTTPConnection._send_request</span><span class="nt">(self, method, url, body, headers, encode_chunked)</span>
<span class="g g-Whitespace">   </span><span class="mi">1275</span>     <span class="n">body</span> <span class="o">=</span> <span class="n">_encode</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="s1">'body'</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1276</span> <span class="bp">self</span><span class="o">.</span><span class="n">endheaders</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">encode_chunked</span><span class="o">=</span><span class="n">encode_chunked</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1225,</span> in <span class="ni">HTTPConnection.endheaders</span><span class="nt">(self, message_body, encode_chunked)</span>
<span class="g g-Whitespace">   </span><span class="mi">1224</span>     <span class="k">raise</span> <span class="n">CannotSendHeader</span><span class="p">()</span>
<span class="ne">-&gt; </span><span class="mi">1225</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_output</span><span class="p">(</span><span class="n">message_body</span><span class="p">,</span> <span class="n">encode_chunked</span><span class="o">=</span><span class="n">encode_chunked</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1004,</span> in <span class="ni">HTTPConnection._send_output</span><span class="nt">(self, message_body, encode_chunked)</span>
<span class="g g-Whitespace">   </span><span class="mi">1003</span> <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[:]</span>
<span class="ne">-&gt; </span><span class="mi">1004</span> <span class="bp">self</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1006</span> <span class="k">if</span> <span class="n">message_body</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1007</span> 
<span class="g g-Whitespace">   </span><span class="mi">1008</span>     <span class="c1"># create a consistent interface to message_body</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:944,</span> in <span class="ni">HTTPConnection.send</span><span class="nt">(self, data)</span>
<span class="g g-Whitespace">    </span><span class="mi">943</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_open</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">944</span>     <span class="bp">self</span><span class="o">.</span><span class="n">connect</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">945</span> <span class="k">else</span><span class="p">:</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1399,</span> in <span class="ni">HTTPSConnection.connect</span><span class="nt">(self)</span>
<span class="g g-Whitespace">   </span><span class="mi">1397</span>     <span class="n">server_hostname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">host</span>
<span class="ne">-&gt; </span><span class="mi">1399</span> <span class="bp">self</span><span class="o">.</span><span class="n">sock</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">wrap_socket</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sock</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1400</span>                                       <span class="n">server_hostname</span><span class="o">=</span><span class="n">server_hostname</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\ssl.py:500,</span> in <span class="ni">SSLContext.wrap_socket</span><span class="nt">(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)</span>
<span class="g g-Whitespace">    </span><span class="mi">494</span> <span class="k">def</span> <span class="nf">wrap_socket</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sock</span><span class="p">,</span> <span class="n">server_side</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">495</span>                 <span class="n">do_handshake_on_connect</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">496</span>                 <span class="n">suppress_ragged_eofs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">497</span>                 <span class="n">server_hostname</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">498</span>     <span class="c1"># SSLSocket class handles server_hostname encoding before it calls</span>
<span class="g g-Whitespace">    </span><span class="mi">499</span>     <span class="c1"># ctx._wrap_socket()</span>
<span class="ne">--&gt; </span><span class="mi">500</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sslsocket_class</span><span class="o">.</span><span class="n">_create</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">501</span>         <span class="n">sock</span><span class="o">=</span><span class="n">sock</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">502</span>         <span class="n">server_side</span><span class="o">=</span><span class="n">server_side</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">503</span>         <span class="n">do_handshake_on_connect</span><span class="o">=</span><span class="n">do_handshake_on_connect</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">504</span>         <span class="n">suppress_ragged_eofs</span><span class="o">=</span><span class="n">suppress_ragged_eofs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">505</span>         <span class="n">server_hostname</span><span class="o">=</span><span class="n">server_hostname</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">506</span>         <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">507</span>         <span class="n">session</span><span class="o">=</span><span class="n">session</span>
<span class="g g-Whitespace">    </span><span class="mi">508</span>     <span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\ssl.py:1040,</span> in <span class="ni">SSLSocket._create</span><span class="nt">(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)</span>
<span class="g g-Whitespace">   </span><span class="mi">1039</span>             <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"do_handshake_on_connect should not be specified for non-blocking sockets"</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1040</span>         <span class="bp">self</span><span class="o">.</span><span class="n">do_handshake</span><span class="p">()</span>
<span class="g g-Whitespace">   </span><span class="mi">1041</span> <span class="k">except</span> <span class="p">(</span><span class="ne">OSError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\ssl.py:1309,</span> in <span class="ni">SSLSocket.do_handshake</span><span class="nt">(self, block)</span>
<span class="g g-Whitespace">   </span><span class="mi">1308</span>         <span class="bp">self</span><span class="o">.</span><span class="n">settimeout</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1309</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_sslobj</span><span class="o">.</span><span class="n">do_handshake</span><span class="p">()</span>
<span class="g g-Whitespace">   </span><span class="mi">1310</span> <span class="k">finally</span><span class="p">:</span>

<span class="ne">SSLCertVerificationError</span>: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)

<span class="n">During</span> <span class="n">handling</span> <span class="n">of</span> <span class="n">the</span> <span class="n">above</span> <span class="n">exception</span><span class="p">,</span> <span class="n">another</span> <span class="n">exception</span> <span class="n">occurred</span><span class="p">:</span>

<span class="ne">URLError</span><span class="g g-Whitespace">                                  </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">7</span><span class="p">],</span> <span class="n">line</span> <span class="mi">3</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">idata</span> <span class="o">=</span> <span class="mi">0</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="k">if</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="ne">----&gt; </span><span class="mi">3</span>     <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv'</span><span class="p">)</span> <span class="c1"># load data from Dr. Pyrcz's GitHub repository  </span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>     <span class="n">response</span> <span class="o">=</span> <span class="s1">'Prod'</span>                                             <span class="c1"># specify the response feature</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span>     <span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">deep</span> <span class="o">=</span> <span class="kc">True</span><span class="p">);</span> <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'Well'</span><span class="p">,</span><span class="n">response</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="s1">'columns'</span><span class="p">)</span> <span class="c1"># make predictor and response DataFrames</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:912,</span> in <span class="ni">read_csv</span><span class="nt">(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)</span>
<span class="g g-Whitespace">    </span><span class="mi">899</span> <span class="n">kwds_defaults</span> <span class="o">=</span> <span class="n">_refine_defaults_read</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">900</span>     <span class="n">dialect</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">901</span>     <span class="n">delimiter</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">908</span>     <span class="n">dtype_backend</span><span class="o">=</span><span class="n">dtype_backend</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">909</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">910</span> <span class="n">kwds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwds_defaults</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">912</span> <span class="k">return</span> <span class="n">_read</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="p">,</span> <span class="n">kwds</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:577,</span> in <span class="ni">_read</span><span class="nt">(filepath_or_buffer, kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">574</span> <span class="n">_validate_names</span><span class="p">(</span><span class="n">kwds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"names"</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">576</span> <span class="c1"># Create the parser.</span>
<span class="ne">--&gt; </span><span class="mi">577</span> <span class="n">parser</span> <span class="o">=</span> <span class="n">TextFileReader</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">579</span> <span class="k">if</span> <span class="n">chunksize</span> <span class="ow">or</span> <span class="n">iterator</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">580</span>     <span class="k">return</span> <span class="n">parser</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:1407,</span> in <span class="ni">TextFileReader.__init__</span><span class="nt">(self, f, engine, **kwds)</span>
<span class="g g-Whitespace">   </span><span class="mi">1404</span>     <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">[</span><span class="s2">"has_index_names"</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwds</span><span class="p">[</span><span class="s2">"has_index_names"</span><span class="p">]</span>
<span class="g g-Whitespace">   </span><span class="mi">1406</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span><span class="p">:</span> <span class="n">IOHandles</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="ne">-&gt; </span><span class="mi">1407</span> <span class="bp">self</span><span class="o">.</span><span class="n">_engine</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_engine</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:1661,</span> in <span class="ni">TextFileReader._make_engine</span><span class="nt">(self, f, engine)</span>
<span class="g g-Whitespace">   </span><span class="mi">1659</span>     <span class="k">if</span> <span class="s2">"b"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mode</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1660</span>         <span class="n">mode</span> <span class="o">+=</span> <span class="s2">"b"</span>
<span class="ne">-&gt; </span><span class="mi">1661</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="o">=</span> <span class="n">get_handle</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1662</span>     <span class="n">f</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1663</span>     <span class="n">mode</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1664</span>     <span class="n">encoding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"encoding"</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1665</span>     <span class="n">compression</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"compression"</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1666</span>     <span class="n">memory_map</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"memory_map"</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1667</span>     <span class="n">is_text</span><span class="o">=</span><span class="n">is_text</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1668</span>     <span class="n">errors</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"encoding_errors"</span><span class="p">,</span> <span class="s2">"strict"</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1669</span>     <span class="n">storage_options</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"storage_options"</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1670</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1671</span> <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="g g-Whitespace">   </span><span class="mi">1672</span> <span class="n">f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span><span class="o">.</span><span class="n">handle</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:716,</span> in <span class="ni">get_handle</span><span class="nt">(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">713</span>     <span class="n">codecs</span><span class="o">.</span><span class="n">lookup_error</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">715</span> <span class="c1"># open URLs</span>
<span class="ne">--&gt; </span><span class="mi">716</span> <span class="n">ioargs</span> <span class="o">=</span> <span class="n">_get_filepath_or_buffer</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">717</span>     <span class="n">path_or_buf</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">718</span>     <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">719</span>     <span class="n">compression</span><span class="o">=</span><span class="n">compression</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">720</span>     <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">721</span>     <span class="n">storage_options</span><span class="o">=</span><span class="n">storage_options</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">722</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">724</span> <span class="n">handle</span> <span class="o">=</span> <span class="n">ioargs</span><span class="o">.</span><span class="n">filepath_or_buffer</span>
<span class="g g-Whitespace">    </span><span class="mi">725</span> <span class="n">handles</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">BaseBuffer</span><span class="p">]</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:368,</span> in <span class="ni">_get_filepath_or_buffer</span><span class="nt">(filepath_or_buffer, encoding, compression, mode, storage_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">366</span> <span class="c1"># assuming storage_options is to be interpreted as headers</span>
<span class="g g-Whitespace">    </span><span class="mi">367</span> <span class="n">req_info</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">storage_options</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">368</span> <span class="k">with</span> <span class="n">urlopen</span><span class="p">(</span><span class="n">req_info</span><span class="p">)</span> <span class="k">as</span> <span class="n">req</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">369</span>     <span class="n">content_encoding</span> <span class="o">=</span> <span class="n">req</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"Content-Encoding"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">370</span>     <span class="k">if</span> <span class="n">content_encoding</span> <span class="o">==</span> <span class="s2">"gzip"</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">371</span>         <span class="c1"># Override compression based on Content-Encoding header</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:270,</span> in <span class="ni">urlopen</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">264</span><span class="w"> </span><span class="sd">"""</span>
<span class="g g-Whitespace">    </span><span class="mi">265</span><span class="sd"> Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of</span>
<span class="g g-Whitespace">    </span><span class="mi">266</span><span class="sd"> the stdlib.</span>
<span class="g g-Whitespace">    </span><span class="mi">267</span><span class="sd"> """</span>
<span class="g g-Whitespace">    </span><span class="mi">268</span> <span class="kn">import</span> <span class="nn">urllib.request</span>
<span class="ne">--&gt; </span><span class="mi">270</span> <span class="k">return</span> <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:222,</span> in <span class="ni">urlopen</span><span class="nt">(url, data, timeout, cafile, capath, cadefault, context)</span>
<span class="g g-Whitespace">    </span><span class="mi">220</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">221</span>     <span class="n">opener</span> <span class="o">=</span> <span class="n">_opener</span>
<span class="ne">--&gt; </span><span class="mi">222</span> <span class="k">return</span> <span class="n">opener</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">timeout</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:525,</span> in <span class="ni">OpenerDirector.open</span><span class="nt">(self, fullurl, data, timeout)</span>
<span class="g g-Whitespace">    </span><span class="mi">522</span>     <span class="n">req</span> <span class="o">=</span> <span class="n">meth</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">524</span> <span class="n">sys</span><span class="o">.</span><span class="n">audit</span><span class="p">(</span><span class="s1">'urllib.Request'</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">full_url</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">headers</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">get_method</span><span class="p">())</span>
<span class="ne">--&gt; </span><span class="mi">525</span> <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_open</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">527</span> <span class="c1"># post-process response</span>
<span class="g g-Whitespace">    </span><span class="mi">528</span> <span class="n">meth_name</span> <span class="o">=</span> <span class="n">protocol</span><span class="o">+</span><span class="s2">"_response"</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:542,</span> in <span class="ni">OpenerDirector._open</span><span class="nt">(self, req, data)</span>
<span class="g g-Whitespace">    </span><span class="mi">539</span>     <span class="k">return</span> <span class="n">result</span>
<span class="g g-Whitespace">    </span><span class="mi">541</span> <span class="n">protocol</span> <span class="o">=</span> <span class="n">req</span><span class="o">.</span><span class="n">type</span>
<span class="ne">--&gt; </span><span class="mi">542</span> <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_chain</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle_open</span><span class="p">,</span> <span class="n">protocol</span><span class="p">,</span> <span class="n">protocol</span> <span class="o">+</span>
<span class="g g-Whitespace">    </span><span class="mi">543</span>                           <span class="s1">'_open'</span><span class="p">,</span> <span class="n">req</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">544</span> <span class="k">if</span> <span class="n">result</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">545</span>     <span class="k">return</span> <span class="n">result</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:502,</span> in <span class="ni">OpenerDirector._call_chain</span><span class="nt">(self, chain, kind, meth_name, *args)</span>
<span class="g g-Whitespace">    </span><span class="mi">500</span> <span class="k">for</span> <span class="n">handler</span> <span class="ow">in</span> <span class="n">handlers</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">501</span>     <span class="n">func</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">handler</span><span class="p">,</span> <span class="n">meth_name</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">502</span>     <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">503</span>     <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">504</span>         <span class="k">return</span> <span class="n">result</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:1360,</span> in <span class="ni">HTTPSHandler.https_open</span><span class="nt">(self, req)</span>
<span class="g g-Whitespace">   </span><span class="mi">1359</span> <span class="k">def</span> <span class="nf">https_open</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">req</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1360</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_open</span><span class="p">(</span><span class="n">http</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">HTTPSConnection</span><span class="p">,</span> <span class="n">req</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1361</span>         <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_context</span><span class="p">,</span> <span class="n">check_hostname</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_check_hostname</span><span class="p">)</span>

<span class="nn">File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:1320,</span> in <span class="ni">AbstractHTTPHandler.do_open</span><span class="nt">(self, http_class, req, **http_conn_args)</span>
<span class="g g-Whitespace">   </span><span class="mi">1317</span>         <span class="n">h</span><span class="o">.</span><span class="n">request</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">get_method</span><span class="p">(),</span> <span class="n">req</span><span class="o">.</span><span class="n">selector</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">headers</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1318</span>                   <span class="n">encode_chunked</span><span class="o">=</span><span class="n">req</span><span class="o">.</span><span class="n">has_header</span><span class="p">(</span><span class="s1">'Transfer-encoding'</span><span class="p">))</span>
<span class="g g-Whitespace">   </span><span class="mi">1319</span>     <span class="k">except</span> <span class="ne">OSError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span> <span class="c1"># timeout error</span>
<span class="ne">-&gt; </span><span class="mi">1320</span>         <span class="k">raise</span> <span class="n">URLError</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1321</span>     <span class="n">r</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">getresponse</span><span class="p">()</span>
<span class="g g-Whitespace">   </span><span class="mi">1322</span> <span class="k">except</span><span class="p">:</span>

<span class="ne">URLError</span>: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)&gt;
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can also establish the feature ranges for plotting. We could calculate the feature range directly from the data with code like this:</p></li>
</ul>
<div class="highlight-p notranslate"><div class="highlight"><pre><span/>Pormin = np.min(df['Por'].values)                             # extract ndarray of data table column
Pormax = np.max(df['Por'].values)                             # and calculate min and max
</pre></div>
</div>
<p>but, this would not result in easy to understand color bars and axis scales, let‚Äôs pick convenient round numbers. We will also declare feature labels for ease of plotting.</p>
&#13;

<h2>Visualize the DataFrame</h2>
<p>Visualizing the DataFrame is useful first check of the data.</p>
<ul class="simple">
<li><p>many things can go wrong, e.g., we loaded the wrong data, all the features did not load, etc.</p></li>
</ul>
<p>We can preview by utilizing the ‚Äòhead‚Äô DataFrame member function (with a nice and clean format, see below).</p>
<ul class="simple">
<li><p>add parameter ‚Äòn=13‚Äô to see the first 13 rows of the dataset.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>                                                 <span class="c1"># we could also use this command for a table preview </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Well</th>
      <th>Por</th>
      <th>Perm</th>
      <th>AI</th>
      <th>Brittle</th>
      <th>TOC</th>
      <th>VR</th>
      <th>Prod</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>12.08</td>
      <td>2.92</td>
      <td>2.80</td>
      <td>81.40</td>
      <td>1.16</td>
      <td>2.31</td>
      <td>1695.360819</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>12.38</td>
      <td>3.53</td>
      <td>3.22</td>
      <td>46.17</td>
      <td>0.89</td>
      <td>1.88</td>
      <td>3007.096063</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>14.02</td>
      <td>2.59</td>
      <td>4.01</td>
      <td>72.80</td>
      <td>0.89</td>
      <td>2.72</td>
      <td>2531.938259</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>17.67</td>
      <td>6.75</td>
      <td>2.63</td>
      <td>39.81</td>
      <td>1.08</td>
      <td>1.88</td>
      <td>5288.514854</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>17.52</td>
      <td>4.57</td>
      <td>3.18</td>
      <td>10.94</td>
      <td>1.51</td>
      <td>1.90</td>
      <td>2859.469624</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>14.53</td>
      <td>4.81</td>
      <td>2.69</td>
      <td>53.60</td>
      <td>0.94</td>
      <td>1.67</td>
      <td>4017.374438</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>13.49</td>
      <td>3.60</td>
      <td>2.93</td>
      <td>63.71</td>
      <td>0.80</td>
      <td>1.85</td>
      <td>2952.812773</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>11.58</td>
      <td>3.03</td>
      <td>3.25</td>
      <td>53.00</td>
      <td>0.69</td>
      <td>1.93</td>
      <td>2670.933846</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>12.52</td>
      <td>2.72</td>
      <td>2.43</td>
      <td>65.77</td>
      <td>0.95</td>
      <td>1.98</td>
      <td>2474.048178</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>13.25</td>
      <td>3.94</td>
      <td>3.71</td>
      <td>66.20</td>
      <td>1.14</td>
      <td>2.65</td>
      <td>2722.893266</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>15.04</td>
      <td>4.39</td>
      <td>2.22</td>
      <td>61.11</td>
      <td>1.08</td>
      <td>1.77</td>
      <td>3828.247174</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>16.19</td>
      <td>6.30</td>
      <td>2.29</td>
      <td>49.10</td>
      <td>1.53</td>
      <td>1.86</td>
      <td>5095.810104</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>16.82</td>
      <td>5.42</td>
      <td>2.80</td>
      <td>66.65</td>
      <td>1.17</td>
      <td>1.98</td>
      <td>4091.637316</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
&#13;

<h2>Summary Statistics for Tabular Data</h2>
<p>There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames. The describe command provides count, mean, minimum, maximum, and quartiles all in a nice data table.</p>
<ul class="simple">
<li><p>We use transpose just to flip the table so that features are on the rows and the statistics are on the columns.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>                                     <span class="c1"># calculate summary statistics for the data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Well</th>
      <td>200.0</td>
      <td>100.500000</td>
      <td>57.879185</td>
      <td>1.000000</td>
      <td>50.750000</td>
      <td>100.500000</td>
      <td>150.250000</td>
      <td>200.000000</td>
    </tr>
    <tr>
      <th>Por</th>
      <td>200.0</td>
      <td>14.991150</td>
      <td>2.971176</td>
      <td>6.550000</td>
      <td>12.912500</td>
      <td>15.070000</td>
      <td>17.402500</td>
      <td>23.550000</td>
    </tr>
    <tr>
      <th>Perm</th>
      <td>200.0</td>
      <td>4.330750</td>
      <td>1.731014</td>
      <td>1.130000</td>
      <td>3.122500</td>
      <td>4.035000</td>
      <td>5.287500</td>
      <td>9.870000</td>
    </tr>
    <tr>
      <th>AI</th>
      <td>200.0</td>
      <td>2.968850</td>
      <td>0.566885</td>
      <td>1.280000</td>
      <td>2.547500</td>
      <td>2.955000</td>
      <td>3.345000</td>
      <td>4.630000</td>
    </tr>
    <tr>
      <th>Brittle</th>
      <td>200.0</td>
      <td>48.161950</td>
      <td>14.129455</td>
      <td>10.940000</td>
      <td>37.755000</td>
      <td>49.510000</td>
      <td>58.262500</td>
      <td>84.330000</td>
    </tr>
    <tr>
      <th>TOC</th>
      <td>200.0</td>
      <td>0.990450</td>
      <td>0.481588</td>
      <td>-0.190000</td>
      <td>0.617500</td>
      <td>1.030000</td>
      <td>1.350000</td>
      <td>2.180000</td>
    </tr>
    <tr>
      <th>VR</th>
      <td>200.0</td>
      <td>1.964300</td>
      <td>0.300827</td>
      <td>0.930000</td>
      <td>1.770000</td>
      <td>1.960000</td>
      <td>2.142500</td>
      <td>2.870000</td>
    </tr>
    <tr>
      <th>Prod</th>
      <td>200.0</td>
      <td>3864.407081</td>
      <td>1553.277558</td>
      <td>839.822063</td>
      <td>2686.227611</td>
      <td>3604.303506</td>
      <td>4752.637555</td>
      <td>8590.384044</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Ranking features is really an effort to understand the features and their relationships with each other. We will start with basic data visualization and move to more complicated methods such are partial correlation and recursive feature elimination.</p>
&#13;

<h2>Coverage</h2>
<p>Let‚Äôs start with the concept of feature coverage.</p>
<ul class="simple">
<li><p>If a feature is available over a small proportion of the samples then we may not want to include it as it will result in issues with feature imputation, estimation of missing data.</p></li>
<li><p>By removing a couple features with poor coverage we may improve our model because there are limitations with feature imputation, feature imputation can actually impose bias in statistics and additional error in our prediction models</p></li>
<li><p>if likewise deletion is applied to deal with missing values, features with low coverage result in a lot of removed data!</p></li>
</ul>
<p>Let‚Äôs start with a bar chart with the proportion of missing records:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s1">'bar'</span><span class="p">)</span>                <span class="c1"># calculate DataFrame with percentage missing by feature</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Percentage of Missing Values'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Data Completeness'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6b56bb06fe01cd87f3513b8ebe3a71bb409619f34b70e4dd2bd50d61e5c4e62d.png" src="../Images/e5a834b70ca47ad151aee5749adc53ce.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/6b56bb06fe01cd87f3513b8ebe3a71bb409619f34b70e4dd2bd50d61e5c4e62d.png"/>
</div>
</div>
<p>For the provided example dataset the plot should be empty. There are no missing data so the ‚ÄòProportion of Missing Records‚Äô is 0.0 for all features.</p>
<p>If you wanted to test this plot with some missing data, run this code first:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">proportion_NaN</span> <span class="o">=</span> <span class="mf">0.1</span>                                    <span class="c1"># proportion of values in DataFrame to remove</span>

<span class="n">remove</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">proportion_NaN</span>    <span class="c1"># make the boolean array for removal</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Fraction of removed values in mask ndarray = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">remove</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">remove</span><span class="o">.</span><span class="n">size</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span> <span class="o">+</span> <span class="s1">'.'</span><span class="p">)</span>

<span class="n">df_mask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">mask</span><span class="p">(</span><span class="n">remove</span><span class="p">)</span>                               <span class="c1"># make a new DataFrame with specified proportion removed</span>
</pre></div>
</div>
<p>Remove this code and reload the data to continue to get consistent results with the discussions below.</p>
<p>This does not tell the whole story. For example, if 20% of feature A is missing and 20% of feature B is missing are those the same and different samples. This has a huge impact if you perform likewise deletion.</p>
<ul class="simple">
<li><p>If there is not too much data then we can actually visualize data coverage over all samples and features in a boolean table like this.</p></li>
<li><p>This method may identify specific samples with many missing features that may be removed to improve overall coverage or other trends or structures in the missing data that may result in sampling bias.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df_temp</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                  <span class="c1"># make a deep copy of the DataFrame</span>
<span class="n">df_bool</span> <span class="o">=</span> <span class="n">df_temp</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span>                                    <span class="c1"># true is value, false if NaN</span>
<span class="c1">#df_bool = df_bool.set_index(df_temp.pop('UWI'))              # set the index / feature for the heat map y column</span>
<span class="n">heat</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">df_bool</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="p">[</span><span class="s1">'r'</span><span class="p">,</span><span class="s1">'w'</span><span class="p">],</span> <span class="n">annot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">'.0f'</span><span class="p">,</span><span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">linecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># make the binary heat map, no bins</span>
<span class="n">heat</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">heat</span><span class="o">.</span><span class="n">get_xticklabels</span><span class="p">(),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">heat</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">heat</span><span class="o">.</span><span class="n">get_yticklabels</span><span class="p">(),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">heat</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Data Completeness Heatmap'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span> <span class="n">heat</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Feature'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">);</span> <span class="n">heat</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Sample (Index)'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.8</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.6</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5849f3ca4f8301f49db8799952b591b62475a866fcaa2646497f042118e36aa0.png" src="../Images/9b8b2b5fe0360995f89df5950e3ed23d.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/5849f3ca4f8301f49db8799952b591b62475a866fcaa2646497f042118e36aa0.png"/>
</div>
</div>
<p>Once again this plot should be quite boring for the provided dataset with perfect coverage, every cell should be filled in red.</p>
<ul class="simple">
<li><p>add the code to remove some records to test this plot. White cells are missing records.</p></li>
</ul>
<section id="feature-imputation">
<h3>Feature Imputation</h3>
<p>See the chapter on feature imputation to learn what to do about missing data.</p>
<p>For now a concise treatment here, we will just apply likewise deletion and move on.</p>
<ul class="simple">
<li><p>we remove all samples with any missing feature values. While this is quite simple, it is a sledge hammer approach to ensure perfect coverage required by feature ranking methods that we are about to demonstrate. Please check out the other methods in the linked workflow above.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">how</span><span class="o">=</span><span class="s1">'any'</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                      <span class="c1"># likewise deletion</span>
</pre></div>
</div>
</div>
</div>
</section>
&#13;

<h3>Feature Imputation</h3>
<p>See the chapter on feature imputation to learn what to do about missing data.</p>
<p>For now a concise treatment here, we will just apply likewise deletion and move on.</p>
<ul class="simple">
<li><p>we remove all samples with any missing feature values. While this is quite simple, it is a sledge hammer approach to ensure perfect coverage required by feature ranking methods that we are about to demonstrate. Please check out the other methods in the linked workflow above.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">how</span><span class="o">=</span><span class="s1">'any'</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                      <span class="c1"># likewise deletion</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Summary Statistics</h2>
<p>In any multivariate work we should start with the univariate analysis, summary statistics of one variable at a time. The summary statistic ranking method is qualitative, we are asking:</p>
<ul class="simple">
<li><p>Are there data issues?</p></li>
<li><p>Do we trust the features? Do we trust the features all equally?</p></li>
<li><p>Are there issues that need to be taken care of before we develop any multivariate workflows?</p></li>
</ul>
<p>There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames. The describe command provides count, mean, minimum, maximum, and quartiles all in a compact data table. We use transpose() command to flip the table so that features are on the rows and the statistics are on the columns.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>                                     <span class="c1"># DataFrame summary statistics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Well</th>
      <td>200.0</td>
      <td>100.500000</td>
      <td>57.879185</td>
      <td>1.000000</td>
      <td>50.750000</td>
      <td>100.500000</td>
      <td>150.250000</td>
      <td>200.000000</td>
    </tr>
    <tr>
      <th>Por</th>
      <td>200.0</td>
      <td>14.991150</td>
      <td>2.971176</td>
      <td>6.550000</td>
      <td>12.912500</td>
      <td>15.070000</td>
      <td>17.402500</td>
      <td>23.550000</td>
    </tr>
    <tr>
      <th>Perm</th>
      <td>200.0</td>
      <td>4.330750</td>
      <td>1.731014</td>
      <td>1.130000</td>
      <td>3.122500</td>
      <td>4.035000</td>
      <td>5.287500</td>
      <td>9.870000</td>
    </tr>
    <tr>
      <th>AI</th>
      <td>200.0</td>
      <td>2.968850</td>
      <td>0.566885</td>
      <td>1.280000</td>
      <td>2.547500</td>
      <td>2.955000</td>
      <td>3.345000</td>
      <td>4.630000</td>
    </tr>
    <tr>
      <th>Brittle</th>
      <td>200.0</td>
      <td>48.161950</td>
      <td>14.129455</td>
      <td>10.940000</td>
      <td>37.755000</td>
      <td>49.510000</td>
      <td>58.262500</td>
      <td>84.330000</td>
    </tr>
    <tr>
      <th>TOC</th>
      <td>200.0</td>
      <td>0.990450</td>
      <td>0.481588</td>
      <td>-0.190000</td>
      <td>0.617500</td>
      <td>1.030000</td>
      <td>1.350000</td>
      <td>2.180000</td>
    </tr>
    <tr>
      <th>VR</th>
      <td>200.0</td>
      <td>1.964300</td>
      <td>0.300827</td>
      <td>0.930000</td>
      <td>1.770000</td>
      <td>1.960000</td>
      <td>2.142500</td>
      <td>2.870000</td>
    </tr>
    <tr>
      <th>Prod</th>
      <td>200.0</td>
      <td>3864.407081</td>
      <td>1553.277558</td>
      <td>839.822063</td>
      <td>2686.227611</td>
      <td>3604.303506</td>
      <td>4752.637555</td>
      <td>8590.384044</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Summary statistics are a critical first step in data checking.</p>
<ul class="simple">
<li><p>this includes the number of valid (non-null) values for each feature (count removes all np.NaN from the totals for each variable).</p></li>
<li><p>we can see the general behaviors such as central tendency, mean, and dispersion, variance.</p></li>
<li><p>we can identify issue with negative values, extreme values, and values that are outside the range of plausible values for each property.</p></li>
<li><p>The data looks to be in pretty good shape and for brevity we skip outlier detection. Let‚Äôs look at the univariate distributions.</p></li>
</ul>
&#13;

<h2>Univariate Distributions</h2>
<p>As with summary statistics, this ranking method is a qualitative check for issues with the data and to assess our confidence with each feature. It is better to not include a feature with low confidence of quality as it may be misleading (while adding to model complexity as discussed previously).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nbins</span> <span class="o">=</span> <span class="mi">20</span>                                                    <span class="c1"># number of histogram bins</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>                        <span class="c1"># plot histograms with central tendency and P10 and P90 labeled</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">nbins</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># histogram_bounds(values=df[feature].values,weights=np.ones(len(df)),color='red')</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">feature</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">featuretitle</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span> 
    <span class="c1"># if feature == resp:   </span>
    <span class="c1">#     plt.xlim([Ymin,Ymax])    </span>
    <span class="c1"># else:</span>
    <span class="c1">#     plt.xlim([xmin[i],xmax[i]]) </span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">4.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1b5c32c2236778514e0fd18a00eb66559f6030647523aca11be1a1e520a5693c.png" src="../Images/8a0c89fa0d885a7643839d04c23ff8be.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/1b5c32c2236778514e0fd18a00eb66559f6030647523aca11be1a1e520a5693c.png"/>
</div>
</div>
<p>The univariate distributions look good:</p>
<ul class="simple">
<li><p>there are no obvious outliers</p></li>
<li><p>the permeability is positively skewed, as is often observed</p></li>
<li><p>the corrected TOC has a small spike, but it‚Äôs reasonable</p></li>
</ul>
&#13;

<h2>Bivariate Distributions</h2>
<p>Matrix scatter plots are a very efficient method to observe the bivariate relationships between the variables.</p>
<ul class="simple">
<li><p>this is another opportunity through data visualization to identify data issues</p></li>
<li><p>we can assess if we have collinearity, specifically simpler form between two features at a time.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pairgrid</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">PairGrid</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="c1"># matrix scatter plots</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_upper</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_diag</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'k'</span><span class="p">)</span><span class="c1"># Map a density plot to the lower triangle</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_lower</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">,</span> 
                              <span class="n">shade</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">shade_lowest</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_levels</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/283204472327a4b7c8b5d3fcf91848aec9161f0646166a8de22e8d42f8dbdaf5.png" src="../Images/eb1c7e7e9920c8c27be71cd70df81661.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/283204472327a4b7c8b5d3fcf91848aec9161f0646166a8de22e8d42f8dbdaf5.png"/>
</div>
</div>
<p>This plot communicates a lot of information. How could we use this plot for variable ranking?</p>
<ul class="simple">
<li><p>we can identify features that are closely related to each other, e.g., if two features have almost a perfect monotonic linear or near linear relationship we should remove one immediately. This is a simple case of collinearity that will likely result in model instability as discussed above.</p></li>
<li><p>we can check for linear vs. non-linear relationships. If we observe nonlinear bivariate relationships this will impact the choice of methods, and the quality of results from methods that assume linear relationships for variable ranking.</p></li>
<li><p>we can identify constraint relationships and heteroscedasticity between variables. Once again these may restrict our ranking methods and also encourage us to retains specific features to retain these features in the resulting model.</p></li>
</ul>
<p>Yet, we must remember that bivariate visualization and analysis is not sufficient to understand all the multivariate relationships in the data, e.g., multicollinearity includes strong linear relationships between 2 or more features. These may be hard to see with only bivariate plots.</p>
&#13;

<h2>Pairwise Covariance</h2>
<p>Pairwise covariance provides a measure of the strength of the linear relationship between each predictor feature and the response feature. At this point, we specify that the goal of this study is to predict production, our response variable, from the other available predictor features. We are thinking predictively now, not inferentially, we want to estimate the function, <span class="math notranslate nohighlight">\(\hat{f}\)</span>, to accomplish this:</p>
<div class="math notranslate nohighlight">
\[
Y = \hat{f}(X_1,\ldots,X_n) 
\]</div>
<p>where <span class="math notranslate nohighlight">\(Y\)</span> is our response feature and <span class="math notranslate nohighlight">\(X_1,\ldots,X_n\)</span> are our predictor features. If we retained all of our predictor features to predict the response we would have:</p>
<div class="math notranslate nohighlight">
\[
Prod = \hat{f}(Por,Perm,AI,Brittle,TOC,VR) 
\]</div>
<p>Now back to the covariance, the covariance is defined as:</p>
<div class="math notranslate nohighlight">
\[
C_{xy}  = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{(n-1)}
\]</div>
<p>Covariance:</p>
<ul class="simple">
<li><p>measures the linear relationship</p></li>
<li><p>sensitive to the dispersion / variance of both the predictor and response</p></li>
</ul>
<p>We can use the follow command to build a covariance matrix:</p>
<div class="highlight-p notranslate"><div class="highlight"><pre><span/>df.iloc[:,1:8].cov()                                    # covariance matrix sliced predictors vs. response
</pre></div>
</div>
<p>the output is a new Pandas DataFrame, so we can slice the last column to get a Pandas series (ndarray with names) with the covariances between all predictors features and the response.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">covariance</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span> <span class="c1"># calculate covariance matrix and slice for only pred - resp</span>
<span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">,</span><span class="s1">'Covariance Matrix'</span><span class="p">,</span><span class="mf">4000.0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>          <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">covariance</span><span class="p">,</span><span class="o">-</span><span class="mf">20000.0</span><span class="p">,</span><span class="mf">20000.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Covariance with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Covariance'</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.6</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ff711b73e2baba3be2993b907d5023462afb3ab86692953cb31870078fc6969f.png" src="../Images/89e89e3a083da28954418714216aa9d0.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/ff711b73e2baba3be2993b907d5023462afb3ab86692953cb31870078fc6969f.png"/>
</div>
</div>
<p>The covariance is useful, but as you can see the magnitude is quite variable.</p>
<ul class="simple">
<li><p>the covarince magnitudes are a function of each feature‚Äôs feature and feature variance is somewhat arbitrary.</p></li>
<li><p>for example, what is the variance of porosity in fraction vs. percentage or permeability in Darcy vs. milliDarcy. We can show that if we apply a constant multiplier, <span class="math notranslate nohighlight">\(c\)</span>, to a feature, <span class="math notranslate nohighlight">\(X\)</span>, that the variance will change according to this relationship (the proof is based on expectation formulation of variance):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sigma_{cX}^2 = c^2 \cdot \sigma_{X}^2
\]</div>
<p>By moving from percentage to fraction we decrease the variance of porosity by a factor of 10,000! The variance of each feature is potentially arbitrary, with the exception when all the features are in the same units.</p>
<p>Pairwise correlations are standardized covariances; therefore, avoids this arbitrary magnitude issue.</p>
&#13;

<h2>Pairwise Correlation Coefficient</h2>
<p>Pairwise correlation coefficient provides a measure of the strength of the linear relationship between each predictor feature and the response feature.</p>
<div class="math notranslate nohighlight">
\[
\rho_{xy}  = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{(n-1)\sigma_x \sigma_y}, \, -1.0 \le \rho_{xy} \le 1.0
\]</div>
<p>The correlation coefficient:</p>
<ul class="simple">
<li><p>measures the linear relationship</p></li>
<li><p>removes the sensitivity to the dispersion / variance of both the predictor and response features, by normalizing by the product of the standard deviation of each feature</p></li>
</ul>
<p>We can use the follow command to build a correlation matrix:</p>
<div class="highlight-p notranslate"><div class="highlight"><pre><span/>df.iloc[:,1:8].corr()
</pre></div>
</div>
<p>the output is a new Pandas DataFrame, so we can slice the last column to get a Pandas series (ndarray with names) with the correlations between all predictors features and the response.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">correlation</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span> <span class="c1"># calculate covariance matrix and slice for only pred - resp</span>
<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="s1">'Correlation Matrix'</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>           <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8de550142d1a57e5f8c2443095641ca1f3d8907168b1d668112afc3f7f49b625.png" src="../Images/cf2f8ebbbb9381f4ae232eefb7ce2e7a.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/8de550142d1a57e5f8c2443095641ca1f3d8907168b1d668112afc3f7f49b625.png"/>
</div>
</div>
<p>From the correlation matrix we can observe:</p>
<ul class="simple">
<li><p>We see that porosity, permeability and total organic carbon have the strongest linear relationships with production.</p></li>
<li><p>Acoustic impedance has weak negative relationships with production.</p></li>
<li><p>Brittleness is very close to 0.0. If you review the brittleness vs. production scatterplot, you‚Äôll observe a complicated non-linear relationship. There is a brittleness ratio sweet spot for production (rock that is not too soft nor too hard)!</p></li>
</ul>
<p>We could also look at the full correlation matrix to evaluate the potential for redundancy between predictor features.</p>
<ul class="simple">
<li><p>strong degree of correlation between porosity and permeability and porosity and TOC</p></li>
<li><p>strong degree of negative correlation between TOC and acoustic impedance</p></li>
</ul>
<p>We are still limited to a strict linear relationship.  The rank correlation allows us to relax this assumption.</p>
&#13;

<h2>Pairwise Spearman Rank Correlation Coefficient</h2>
<p>The rank correlation coefficient applies the rank transform to the data prior to calculating the correlation coefficient. To calculate the rank transform simply replace the data values with the rank <span class="math notranslate nohighlight">\(R_x = 1,\dots,n\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the maximum value and <span class="math notranslate nohighlight">\(1\)</span> is the minimum value.</p>
<div class="math notranslate nohighlight">
\[
\rho_{R_x R_y}  = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i} - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le 1.0
\]</div>
<div class="math notranslate nohighlight">
\[
x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall \, i \gt j 
\]</div>
<div class="math notranslate nohighlight">
\[
R_{x_i} = i
\]</div>
<p>The rank correlation:</p>
<ul class="simple">
<li><p>measures the monotonic relationship, relaxes the linear assumption</p></li>
<li><p>removes the sensitivity to the dispersion / variance of both the predictor and response, by normalizing by the product of the standard deviation of each.</p></li>
</ul>
<p>We can use the follow command to build a rank correlation matrix and calculate the p-value:</p>
<div class="highlight-p notranslate"><div class="highlight"><pre><span/>stats.spearmanr(df.iloc[:,1:8])
</pre></div>
</div>
<p>the output is a new Pandas DataFrame, so we can slice the last column to get a Pandas series (ndarray with names) with
the correlations between all predictors features and the response.</p>
<p>Also, we get a very convenient <em>pval</em> 2D ndarray with the two-sided (two-tail summing symmetric over both tails) p-value for a hypothesis test with:</p>
<div class="math notranslate nohighlight">
\[
H_o: \rho_{R_x R_y} = 0
\]</div>
<div class="math notranslate nohighlight">
\[
H_1: \rho_{R_x R_y} \ne 0
\]</div>
<p>Let‚Äôs keep the p-values between all the predictor features and our response feature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">rank_correlation</span><span class="p">,</span> <span class="n">rank_correlation_pval</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">spearmanr</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span> <span class="c1"># calculate the rank correlation coefficient</span>
<span class="n">rank_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rank_correlation</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">rank_correlation</span> <span class="o">=</span> <span class="n">rank_correlation</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>
<span class="n">rank_correlation_pval</span> <span class="o">=</span> <span class="n">rank_correlation_pval</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Rank Correlation p-value:</span><span class="se">\n</span><span class="s2">"</span><span class="p">);</span> <span class="nb">print</span><span class="p">(</span><span class="n">rank_correlation_pval</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">rank_matrix</span><span class="p">,</span><span class="s1">'Rank Correlation Matrix'</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>      <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">rank_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Rank Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Rank Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Rank Correlation p-value:

[2.43279911e-02 1.34135205e-01 1.18844068e-10 2.71646948e-04
 2.11367755e-06 0.00000000e+00 3.29170847e-04]
</pre></div>
</div>
<img alt="_images/dd0d78eac8d8429bd13249c35c5d50933da7bf91abf6d4cb58614489675c598e.png" src="../Images/609c07d08205a2c92204da55d19ad62a.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/dd0d78eac8d8429bd13249c35c5d50933da7bf91abf6d4cb58614489675c598e.png"/>
</div>
</div>
<p>There matrix and line plots indicate that the rank correlation coefficients are similar to the correlation coefficients indicating that nonlinearity and outliers are not likely impacting the correlation-based feature ranking.</p>
<p>With regard to rank correlation p-values,</p>
<ul class="simple">
<li><p>at a typical alpha value of 0.05, only the rank correlation between brittleness and production does not fail the hypothesis test; therefore, is not significantly different than 0.0.</p></li>
</ul>
<p>It is useful to look at the difference between the correlation coefficient and rank correlation coefficient.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>                                              <span class="c1"># plot correlation matrix with significance colormap</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">rank_matrix</span><span class="o">.</span><span class="n">values</span>
<span class="n">diff_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">diff_matrix</span><span class="p">,</span><span class="s1">'Correlation - Rank Correlation'</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span>

<span class="n">corr_diff</span> <span class="o">=</span> <span class="n">correlation</span> <span class="o">-</span> <span class="n">rank_correlation</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">corr_diff</span><span class="p">,</span><span class="o">-</span><span class="mf">0.20</span><span class="p">,</span><span class="mf">0.20</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Correlation Coefficient - Rank Correlation Coefficient'</span><span class="p">,</span><span class="s1">'Correlation Diffference'</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e6d73185fa9381a8bc24c2f7002331e6bbb2de3276c7073a45074a6ef2fae98b.png" src="../Images/9010e510d3b644ed069447aad1564797.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/e6d73185fa9381a8bc24c2f7002331e6bbb2de3276c7073a45074a6ef2fae98b.png"/>
</div>
</div>
<p>Here are some interesting observations:</p>
<ul class="simple">
<li><p>correlation of porosity and vitrinite reflectance with production improve when we reduce the impact of linearity and outliers</p></li>
<li><p>correlation of brittleness with production worsen when we reduce the impact of linearity and outliers</p></li>
</ul>
<p>All of these methods up to now have considered one feature at a time. We can also consider methods that consider all features jointly to ‚Äòisolate‚Äô the influence of each feature.</p>
&#13;

<h2>Partial Correlation Coefficient</h2>
<p>This is a linear correlation coefficient that controls for the effects all the remaining variables, <span class="math notranslate nohighlight">\(\rho_{XY.Z}\)</span> and <span class="math notranslate nohighlight">\(\rho_{YX.Z}\)</span> is the partial correlation between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span>, after controlling for <span class="math notranslate nohighlight">\(Z\)</span>.</p>
<p>To calculate the partial correlation coefficient between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(Z_i, \forall \quad i = 1,\ldots, m-1\)</span> remaining features we use the following steps:</p>
<ol class="arabic simple">
<li><p>perform linear, least-squares regression to predict <span class="math notranslate nohighlight">\(X\)</span> from <span class="math notranslate nohighlight">\(Z_i, \forall \quad i = 1,\ldots, m-1\)</span>. <span class="math notranslate nohighlight">\(X\)</span> is regressed on the predictors to calculate the estimate, <span class="math notranslate nohighlight">\(X^*\)</span></p></li>
<li><p>calculate the residuals in Step #1, <span class="math notranslate nohighlight">\(X-X^*\)</span>, where <span class="math notranslate nohighlight">\(X^* = f(Z_{1,\ldots,m-1})\)</span>, linear regression model</p></li>
<li><p>perform linear, least-squares regression to predict <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(Z_i, \forall \quad i = 1,\ldots, m-1\)</span>. <span class="math notranslate nohighlight">\(Y\)</span> is regressed on the predictors to calculate the estimate, <span class="math notranslate nohighlight">\(Y^*\)</span></p></li>
<li><p>calculate the residuals in Step #3, <span class="math notranslate nohighlight">\(Y-Y^*\)</span>, where <span class="math notranslate nohighlight">\(Y^* = f(Z_{1,\ldots,m-1})\)</span>, linear regression model</p></li>
<li><p>calculate the correlation coefficient between the residuals from Steps #2 and #4, <span class="math notranslate nohighlight">\(\rho_{X-X^*,Y-Y^*}\)</span></p></li>
</ol>
<p>The partial correlation, provides a measure of the linear relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> while controlling for the effect of <span class="math notranslate nohighlight">\(Z\)</span> other features on both, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.  We use the function declared previously taken from Fabian Pedregosa-Izquierdo, <a class="reference external" href="mailto:f%40bianp.net">f<span>@</span>bianp<span>.</span>net</a>. The original code is on GitHub at <a class="reference external" href="https://git.io/fhyHB">https://git.io/fhyHB</a>.</p>
<p>To use this method we must assume:</p>
<ol class="arabic simple">
<li><p>two variables to compare, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></p></li>
<li><p>other variables to control, <span class="math notranslate nohighlight">\(Z_{1,\ldots,m-2}\)</span></p></li>
<li><p>linear relationships between all variables</p></li>
<li><p>no significant outliers</p></li>
<li><p>approximately bivariate normality between the variables</p></li>
</ol>
<p>We are in pretty good shape, but we have some departures from bivariate normality. We could consider Gaussian univariate transforms to improve this. This option is provided later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">partial_correlation</span> <span class="o">=</span> <span class="n">partial_corr</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span> <span class="c1"># calculate the partial correlation coefficients</span>
<span class="n">partial_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">partial_correlation</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">partial_correlation</span> <span class="o">=</span> <span class="n">partial_correlation</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span> <span class="c1"># extract a single row and remove production with itself        </span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">partial_matrix</span><span class="p">,</span><span class="s1">'Partial Correlation Matrix'</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">partial_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Partial Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Partial Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/685abb597e65278976ed6245ea39ffba1c566acd98a0496f9a20ae9b472f4e07.png" src="../Images/11649099249c19a8d0134ee44bd96661.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/685abb597e65278976ed6245ea39ffba1c566acd98a0496f9a20ae9b472f4e07.png"/>
</div>
</div>
<p>Now we see a lot of new things about the unique contributions of each predictor feature!</p>
<ul class="simple">
<li><p>porosity and permeability are strongly correlated with each other so they are penalized severely</p></li>
<li><p>acoustic impedance‚Äôs and vitrinite reflectance‚Äôs absolute correlation are increased reflecting their unique contributions</p></li>
<li><p>total organic carbon flipped signs!  When we control for all other variables, it has a negative relationship with production.</p></li>
</ul>
<p>With the partial correlation coefficients we have controlled for the influence of all other predictor features on both the specific predictor and the response features. The semipartial correlation filters out the influence of all other predictor features on the raw response variable.</p>
&#13;

<h2>Semipartial Correlation Coefficient</h2>
<p>This is a linear correlation coefficient that controls for the effects all the remaining features, <span class="math notranslate nohighlight">\(Z\)</span> on <span class="math notranslate nohighlight">\(X\)</span>, and then calculates the correlation between the residual <span class="math notranslate nohighlight">\(X^*-X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.  Note: we do not control for influence of <span class="math notranslate nohighlight">\(Z\)</span> features on the response feature, <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>To calculate the semipartial correlation coefficient between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(Z_i, \forall \quad i = 1,\ldots, m-1\)</span> remaining features we use the following steps:</p>
<ol class="arabic simple">
<li><p>perform linear, least-squares regression to predict <span class="math notranslate nohighlight">\(X\)</span> from <span class="math notranslate nohighlight">\(Z_i, \forall \quad i = 1,\ldots, m-1\)</span>. <span class="math notranslate nohighlight">\(X\)</span> is regressed on the remaining predictor features to calculate the estimate, <span class="math notranslate nohighlight">\(X^*\)</span></p></li>
<li><p>calculate the residuals in Step #1, <span class="math notranslate nohighlight">\(X-X^*\)</span>, where <span class="math notranslate nohighlight">\(X^* = f(Z_{1,\ldots,m-1})\)</span>, linear regression model</p></li>
<li><p>calculate the correlation coefficient between the residuals from Steps #2 and <span class="math notranslate nohighlight">\(Y\)</span> response feature, <span class="math notranslate nohighlight">\(\rho_{X-X^*,Y}\)</span></p></li>
</ol>
<p>The semipartial correlation coefficient, provides a measure of the linear relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> while controlling for the effect of <span class="math notranslate nohighlight">\(Z\)</span> other predictor features on the predictor feature, <span class="math notranslate nohighlight">\(X\)</span>, to get the unique contribution of <span class="math notranslate nohighlight">\(X\)</span> with respect to <span class="math notranslate nohighlight">\(Y\)</span>. We use a modified version of the partial correlation function that we declared previously. The original code is on GitHub at <a class="reference external" href="https://git.io/fhyHB">https://git.io/fhyHB</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">semipartial_correlation</span> <span class="o">=</span> <span class="n">semipartial_corr</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span>    <span class="c1"># calculate the semi-partial correlation coefficients</span>
<span class="n">semipartial_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">semipartial_correlation</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">semipartial_correlation</span> <span class="o">=</span> <span class="n">semipartial_correlation</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>    <span class="c1"># extract a single row and remove production with itself</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">semipartial_matrix</span><span class="p">,</span><span class="s1">'Semi-partial Correlation Matrix'</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">semipartial_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Semipartial Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Semipartial Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8288d5546785bae96f5c850214ddd5be4c148d6210b1b7d05ca2f86640b6b443.png" src="../Images/9bc790699675e3bdbd9e8ed625051fa1.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/8288d5546785bae96f5c850214ddd5be4c148d6210b1b7d05ca2f86640b6b443.png"/>
</div>
</div>
<p>More information to consider:</p>
<ul class="simple">
<li><p>porosity, permeability and vitrinite reflectance are the most important by this feature ranking method</p></li>
<li><p>all other predictor features have quite low correlations</p></li>
</ul>
<p>This is a good moment to stop and take stock of all the results from the quantitative methods.  We will plot them all together.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># plt.subplot(151)</span>
<span class="c1"># feature_rank_plot(features,covariance,-5000.0,5000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.1)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">rank_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Rank Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Rank Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">partial_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Partial Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Partial Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># plt.subplot(155)</span>
<span class="c1"># feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8e1ed4eede67a45f3fd9cf848e281724927b517c683e5207ab574696b9952bde.png" src="../Images/9b87cdad7bc65f987a9f65bfe7febc3b.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/8e1ed4eede67a45f3fd9cf848e281724927b517c683e5207ab574696b9952bde.png"/>
</div>
</div>
<p>I think we are converging on porosity, permeability and vitrinite reflectance as the most important variables with respect to linear relationships with the production.</p>
&#13;

<h2>Feature Ranking with Feature Transformations</h2>
<p>There are many reasons to perform feature transformations (see the associated chapter) and as mentioned above for partial and semipartial correlation a distribution transformation may assist with compliance to metric assumptions.</p>
<ul class="simple">
<li><p>As an exercise and check, let‚Äôs standardize all the features and repeat the previously calculated quantitative methods. We know this will have an impact on covariance, what about the other metrics?</p></li>
</ul>
<p>There is a bunch of code to get this done, but it isn‚Äôt too complicated. First, lets make a new DataFrame with all variables standardized. Then we can make a minor edit (change the DataFrame name) and reuse the code from above. You can choose between:</p>
<ol class="arabic simple">
<li><p>Standardization - affine correction to scale the distributions to have <span class="math notranslate nohighlight">\(\overline{x} = 0\)</span> and <span class="math notranslate nohighlight">\(\sigma_x = 1.0\)</span>.</p></li>
<li><p>Normal Score Transform - distribution transform of each feature to standard normal, Gaussian shape with <span class="math notranslate nohighlight">\(\overline{x} = 0\)</span> and <span class="math notranslate nohighlight">\(\sigma_x = 1.0\)</span>.</p></li>
</ol>
<p>Use this block to perform affine correction of the features:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># dfS = pd.DataFrame()                                        # affine correction, standardization to a mean of 0 and variance of 1 </span>
<span class="c1"># dfS['Well'] = df['Well'].values</span>
<span class="c1"># dfS['Por'] = GSLIB.affine(df['Por'].values,0.0,1.0)</span>
<span class="c1"># dfS['Perm'] = GSLIB.affine(df['Perm'].values,0.0,1.0)</span>
<span class="c1"># dfS['AI'] = GSLIB.affine(df['AI'].values,0.0,1.0)</span>
<span class="c1"># dfS['Brittle'] = GSLIB.affine(df['Brittle'].values,0.0,1.0)</span>
<span class="c1"># dfS['TOC'] = GSLIB.affine(df['TOC'].values,0.0,1.0)</span>
<span class="c1"># dfS['VR'] = GSLIB.affine(df['VR'].values,0.0,1.0)</span>
<span class="c1"># dfS['Prod'] = GSLIB.affine(df['Prod'].values,0.0,1.0)</span>
<span class="c1"># dfS.head()</span>
</pre></div>
</div>
</div>
</div>
<p>Use this block to perform normal score transform of the features:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">dfS</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>                                          <span class="c1"># Gaussian transform, standardization to a mean of 0 and variance of 1 </span>

<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
    <span class="n">dfS</span><span class="p">[</span><span class="n">feature</span><span class="p">],</span><span class="n">d1</span><span class="p">,</span><span class="n">d2</span> <span class="o">=</span> <span class="n">geostats</span><span class="o">.</span><span class="n">nscore</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">)</span>

<span class="n">dfS</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Por</th>
      <th>Perm</th>
      <th>AI</th>
      <th>Brittle</th>
      <th>TOC</th>
      <th>VR</th>
      <th>Prod</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.964092</td>
      <td>-0.780664</td>
      <td>-0.285841</td>
      <td>2.432379</td>
      <td>0.312053</td>
      <td>1.114651</td>
      <td>-1.780464</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.832725</td>
      <td>-0.378580</td>
      <td>0.446827</td>
      <td>-0.195502</td>
      <td>-0.272809</td>
      <td>-0.325239</td>
      <td>-0.392079</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.312053</td>
      <td>-1.069155</td>
      <td>1.722384</td>
      <td>2.004654</td>
      <td>-0.272809</td>
      <td>2.241403</td>
      <td>-0.832725</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.730638</td>
      <td>1.325516</td>
      <td>-0.531604</td>
      <td>-0.590284</td>
      <td>0.131980</td>
      <td>-0.325239</td>
      <td>0.815126</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.698283</td>
      <td>0.298921</td>
      <td>0.365149</td>
      <td>-2.870033</td>
      <td>1.047216</td>
      <td>-0.259823</td>
      <td>-0.531604</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Regardless of the transformation that you chose it is best practice to check the summary statistics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">dfS</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>                                                <span class="c1"># check the summary statistics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Por</th>
      <th>Perm</th>
      <th>AI</th>
      <th>Brittle</th>
      <th>TOC</th>
      <th>VR</th>
      <th>Prod</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>200.000000</td>
      <td>200.000000</td>
      <td>2.000000e+02</td>
      <td>2.000000e+02</td>
      <td>200.000000</td>
      <td>200.000000</td>
      <td>2.000000e+02</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>-0.009700</td>
      <td>0.010306</td>
      <td>9.732356e-03</td>
      <td>8.028717e-05</td>
      <td>0.014152</td>
      <td>0.017360</td>
      <td>1.617223e-03</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.040456</td>
      <td>1.005488</td>
      <td>1.000221e+00</td>
      <td>1.000278e+00</td>
      <td>0.989223</td>
      <td>1.000401</td>
      <td>9.949811e-01</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-4.991462</td>
      <td>-3.355431</td>
      <td>-2.782502e+00</td>
      <td>-2.870033e+00</td>
      <td>-2.336891</td>
      <td>-2.899210</td>
      <td>-2.483589e+00</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>-0.670577</td>
      <td>-0.647337</td>
      <td>-6.588985e-01</td>
      <td>-6.705770e-01</td>
      <td>-0.670577</td>
      <td>-0.651072</td>
      <td>-6.705770e-01</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.006267</td>
      <td>0.006267</td>
      <td>8.881784e-16</td>
      <td>8.881784e-16</td>
      <td>0.018807</td>
      <td>0.006267</td>
      <td>8.881784e-16</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.670577</td>
      <td>0.678574</td>
      <td>6.705770e-01</td>
      <td>6.705770e-01</td>
      <td>0.682378</td>
      <td>0.682642</td>
      <td>6.705770e-01</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2.807034</td>
      <td>2.807034</td>
      <td>2.807034e+00</td>
      <td>2.807034e+00</td>
      <td>2.807034</td>
      <td>2.807034</td>
      <td>2.807034e+00</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We should also check the matrix scatter plot again.</p>
<ul class="simple">
<li><p>If you performed normal score transform, you have standardized the mean and variance and correct the univariate shape of the distribution, but the bivariate relationships still depart from Gaussian.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pairgrid</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">PairGrid</span><span class="p">(</span><span class="n">dfS</span><span class="p">)</span> <span class="c1"># matrix scatter plots</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_upper</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_diag</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'k'</span><span class="p">)</span><span class="c1"># Map a density plot to the lower triangle</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_lower</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">,</span> 
                              <span class="n">shade</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">shade_lowest</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_levels</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/884680b3106f0f9bc10b64a1888d213dedcd55860acea49e6f5bd179d1604868.png" src="../Images/8568ba1fa581044cc577242f378dd1db.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/884680b3106f0f9bc10b64a1888d213dedcd55860acea49e6f5bd179d1604868.png"/>
</div>
</div>
<p>This is the new DataFrame with standardized variables. Now we repeat the previous calculations.</p>
<ul class="simple">
<li><p>We will be more efficient this time and use quite compact code.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">stand_covariance</span> <span class="o">=</span> <span class="n">dfS</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">dfS</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>
<span class="n">stand_correlation</span> <span class="o">=</span> <span class="n">dfS</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">dfS</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>

<span class="n">stand_rank_correlation</span><span class="p">,</span> <span class="n">stand_rank_correlation_pval</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">spearmanr</span><span class="p">(</span><span class="n">dfS</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">dfS</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span>
<span class="n">stand_rank_correlation</span> <span class="o">=</span> <span class="n">stand_rank_correlation</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>
<span class="n">stand_partial_correlation</span> <span class="o">=</span> <span class="n">partial_corr</span><span class="p">(</span><span class="n">dfS</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">dfS</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span> <span class="c1"># calculate the partial correlation coefficients</span>
<span class="n">stand_partial_correlation</span> <span class="o">=</span> <span class="n">stand_partial_correlation</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span>
<span class="n">stand_semipartial_correlation</span> <span class="o">=</span> <span class="n">semipartial_corr</span><span class="p">(</span><span class="n">dfS</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">dfS</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_indexer</span><span class="p">(</span><span class="n">features</span><span class="p">)])</span>    <span class="c1"># calculate the semi-partial correlation coefficients</span>
<span class="n">stand_semipartial_correlation</span> <span class="o">=</span> <span class="n">stand_semipartial_correlation</span><span class="p">[:,</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)]</span> 
</pre></div>
</div>
</div>
</div>
<p>and repeat the previous summary plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># plt.subplot(2,5,1)</span>
<span class="c1"># feature_rank_plot(features,covariance,-5000.0,5000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.5)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">rank_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Rank Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Rank Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">partial_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Partial Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Partial Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># plt.subplot(2,5,5)</span>
<span class="c1"># feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)</span>

<span class="c1"># plt.subplot(2,5,6)</span>
<span class="c1"># feature_rank_plot(features,stand_covariance,-1.0,1.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance of Standardized',0.5)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">stand_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Correlation of Standardized'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">stand_rank_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Rank Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Rank Correlation of Standardized'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">stand_partial_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Partial Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Partial Correlation of Standardized'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># plt.subplot(2,5,10)</span>
<span class="c1"># feature_rank_plot(features,stand_semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation of Standardized',0.5)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e6320b5768883e13feea467d4888f04edce7671ecd0ba7a92874bc94656cd1a2.png" src="../Images/8eeed3569969856a648e8f7bf97ddb8b.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/e6320b5768883e13feea467d4888f04edce7671ecd0ba7a92874bc94656cd1a2.png"/>
</div>
</div>
<p>What can you observe:</p>
<ul class="simple">
<li><p>covariance is now equal to correlation coefficient</p></li>
<li><p>the semipartial correlations are sensitive to the feature standardization (affine correlation or normal score transform).</p></li>
</ul>
&#13;

<h2>Conditional Statistics</h2>
<p>We will separate the wells into low, mid and high production and access the difference in the conditional statistics.</p>
<ul class="simple">
<li><p>This will provide a more flexible method to compare the relationship between each feature and production</p></li>
<li><p>If the conditional statistics change significantly then that feature is informative</p></li>
</ul>
<p>We are going to make a single violin plot over all of our features</p>
<ul class="simple">
<li><p>We need a categorical feature for production, so we truncate production to High or Low with this code,</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">df</span><span class="p">[</span><span class="s1">'tProd'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'Prod'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mi">4000</span><span class="p">,</span> <span class="s1">'High'</span><span class="p">,</span> <span class="s1">'Low'</span><span class="p">)</span> 
</pre></div>
</div>
<ul class="simple">
<li><p>We will need to standardize all of our features so we can observe their relative differences together</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">'Por'</span><span class="p">,</span><span class="s1">'Perm'</span><span class="p">,</span><span class="s1">'AI'</span><span class="p">,</span><span class="s1">'Brittle'</span><span class="p">,</span><span class="s1">'TOC'</span><span class="p">,</span><span class="s1">'VR'</span><span class="p">]]</span>
<span class="n">x_stand</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>      
</pre></div>
</div>
<ul class="simple">
<li><p>This code extracted the features into a new DataFrame ‚Äòx‚Äô, then applied the standardization operation on each column (feature)</p></li>
<li><p>Then we add the truncated production feature into the standardized features</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">[</span><span class="s1">'tProd'</span><span class="p">],</span><span class="n">x_stand</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">6</span><span class="p">]],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>We can then apply the melt command to unpivot the DataFrame</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">id_vars</span><span class="o">=</span><span class="s2">"tProd"</span><span class="p">,</span><span class="n">var_name</span><span class="o">=</span><span class="s2">"Predictors"</span><span class="p">,</span><span class="n">value_name</span><span class="o">=</span><span class="s1">'Standardized_Value'</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>We now have a long DataFrame (6 features x 200 samples = 12000 rows) with:</p>
<ul>
<li><p>production: Low or High</p></li>
<li><p>features: Por, Perm, AI, Brittle, TOC or VR</p></li>
<li><p>standardized feature value</p></li>
</ul>
</li>
</ul>
<p>We can then build our violin plot</p>
<ul class="simple">
<li><p>x is our predictor features</p></li>
<li><p>y is the standardized values for the predictor features (all now in one column)</p></li>
<li><p>hue is the production level High or Low</p></li>
<li><p>split is True so the violins are split in half</p></li>
<li><p>inner is quartiles for P25, P50 and P75 are plotted as dashed lines</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">threshold</span> <span class="o">=</span> <span class="mf">2000.0</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'tProd'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">resp</span><span class="p">]</span><span class="o">&gt;=</span><span class="n">threshold</span><span class="p">,</span> <span class="s1">'High'</span><span class="p">,</span> <span class="s1">'Low'</span><span class="p">)</span>       <span class="c1"># make a high and low production categorical feature</span>

<span class="n">x_temp</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span>
<span class="n">x_temp_stand</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_temp</span> <span class="o">-</span> <span class="n">x_temp</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">x_temp</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>      <span class="c1"># standardization by feature</span>
<span class="n">x_temp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">[</span><span class="s1">'tProd'</span><span class="p">],</span><span class="n">x_temp_stand</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)]],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># add the production categorical feature to the DataFrame</span>
<span class="n">x_temp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">x_temp</span><span class="p">,</span><span class="n">id_vars</span><span class="o">=</span><span class="s2">"tProd"</span><span class="p">,</span><span class="n">var_name</span><span class="o">=</span><span class="s2">"Predictor Feature"</span><span class="p">,</span><span class="n">value_name</span><span class="o">=</span><span class="s1">'Standardized Predictor Feature'</span><span class="p">)</span> <span class="c1"># unpivot the DataFrame</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Predictor Feature"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Standardized Predictor Feature"</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">"tProd"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">x_temp</span><span class="p">,</span><span class="n">split</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inner</span><span class="o">=</span><span class="s2">"quart"</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">"Set2"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Conditional Distributions by Production'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/39636127cd144d1449668e2c0eee3c8e122afe9891a46cc3e4610d38ff16390a.png" src="../Images/97e5b28483156d276669ee0ba4b67c7a.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/39636127cd144d1449668e2c0eee3c8e122afe9891a46cc3e4610d38ff16390a.png"/>
</div>
</div>
<p>From the violin plot we can observe that the conditional distributions of porosity, permeability, TOC have the most variation between low and high production wells.</p>
<p>We can replace the plot with box and whisker plots of the conditional distributions.</p>
<ul class="simple">
<li><p>Box and whisker plots improve our ability to observe the conditional P25, P75 and the upper and lower bounds from the Tukey outlier test.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Predictor Feature"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Standardized Predictor Feature"</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">"tProd"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">x_temp</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'tProd'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2121ce938f6088b2c90acec866c9ff94b488c33826983e13a20d4e1721016352.png" src="../Images/5cb6b0c21042f1de1888484ca5e64a81.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/2121ce938f6088b2c90acec866c9ff94b488c33826983e13a20d4e1721016352.png"/>
</div>
</div>
<p>From the conditional box plot we can observe that the conditional distributions of porosity, permeability, TOC have the most variation between low and high production wells.</p>
<ul class="simple">
<li><p>We can observed the outliers in porosity, permeability (upper tail), total organic carbon (lower tail) and vitrinite reflectance.</p></li>
</ul>
&#13;

<h2>Variance Inflation Factor (VIF)</h2>
<p>A measure of linear multicollinearity between a predictor feature (<span class="math notranslate nohighlight">\(X_i\)</span>) a nd all other predictor features (<span class="math notranslate nohighlight">\(X_j, \forall j \ne i\)</span>).</p>
<p>First we calculate a linear regression for a predictor feature given all the other predictor features.</p>
<div class="math notranslate nohighlight">
\[
X_i = \sum_{j, j \ne i}^m X_j + \epsilon
\]</div>
<p>From this model we determine the coefficient of determination, <span class="math notranslate nohighlight">\(R^2\)</span>, known as variance explained.</p>
<p>Then we calculate the Variance Inflation Factor as:</p>
<div class="math notranslate nohighlight">
\[
VIF = \frac{1}{1 - R^2}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">vif_values</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">vif_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>

<span class="n">vif_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">vif_values</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">vif_values</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>                  <span class="c1"># find indices for descending order</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                        <span class="c1"># plot the feature importance </span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Variance Inflation Factor"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">vif_values</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span>
       <span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pred</span><span class="p">)[</span><span class="n">indices</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">);</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Variance Inflation Factor'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/eaaa1c71dedcaed7ee83a0dfce8fdfb50ad17facc2a2c38a51caf6c6481cb547.png" src="../Images/5692efa445f9167d3d0d5a6d8f3e8bcb.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/eaaa1c71dedcaed7ee83a0dfce8fdfb50ad17facc2a2c38a51caf6c6481cb547.png"/>
</div>
</div>
<p>Vitrinite reflectance has the most linear redundancy while permeability has the least linear redundancy with other predictor features.</p>
<ul class="simple">
<li><p>remember, high variance inflation factor is bad</p></li>
<li><p>recall that variance inflation factor does not integrate the relationship between each predictor feature and the response feature.</p></li>
<li><p>typically, variance inflation factor is used as a screening tool to remove features that have too much redundancy with other predictor features.</p></li>
</ul>
<p>Now let‚Äôs cover model-based feature ranking methods.</p>
&#13;

<h2><span class="math notranslate nohighlight">\(B\)</span> Coefficients / Beta Weights</h2>
<p>We could also consider <span class="math notranslate nohighlight">\(B\)</span> coefficients.  These are the linear regression coefficients without standardization of the variables. Let‚Äôs use the linear regression method that is available in the SciPy package.</p>
<p>The estimator for <span class="math notranslate nohighlight">\(Y\)</span> is simply the linear equation:</p>
<p>\begin{equation}
Y^* = \sum_{i=1}^{m} b_i X_i + c
\end{equation}</p>
<p>The <span class="math notranslate nohighlight">\(b_i\)</span> coefficients are solved to minimize the squared error between the estimates, <span class="math notranslate nohighlight">\(Y^*\)</span> and the values in the training dataset, <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>                                      <span class="c1"># instantiate a linear regression model                   </span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">pred</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">resp</span><span class="p">])</span>                                    <span class="c1"># train the model</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="o">-</span><span class="mf">1000.0</span><span class="p">,</span><span class="mf">1000.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, B Coefficients with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="sa">r</span><span class="s1">'Linear Regression Slope, $b_1$'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ddb18df2725eafa50c95853e6bec78f8aa249726ae339ee0d874966578afdf95.png" src="../Images/7da7b523b4ae881b20a379148ea78eea.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/ddb18df2725eafa50c95853e6bec78f8aa249726ae339ee0d874966578afdf95.png"/>
</div>
</div>
<p>The output is the <span class="math notranslate nohighlight">\(b\)</span> coefficients, ordered over our features from <span class="math notranslate nohighlight">\(b_i, i = 1,\ldots,n\)</span> and then the intercept, <span class="math notranslate nohighlight">\(c\)</span>, that I have removed to avoid confusion.</p>
<ul class="simple">
<li><p>we see the negative contribution of AI and TOC</p></li>
<li><p>the results are very sensitive to the magnitudes of the variances of the predictor features.</p></li>
</ul>
<p>We can remove this sensitivity by working with standardized features.</p>
&#13;

<h2><span class="math notranslate nohighlight">\(\beta\)</span> Coefficients / Beta Weights</h2>
<p><span class="math notranslate nohighlight">\(\beta\)</span> coefficients are calculated as the linear regression of the coefficients after we have standardized the predictor and response features to have a variance of one.</p>
<p>\begin{equation}
\sigma^2_{X^s_i} = 1.0 \quad \forall \quad i = 1,\ldots,m, \quad \sigma^2_{Y^s} = 1.0
\end{equation}</p>
<p>The estimator for <span class="math notranslate nohighlight">\(Y^s\)</span> standardized is simply the linear equation:</p>
<p>\begin{equation}
Y^{s*} = \sum_{i=1}^{m} \beta_i X^s_i + c
\end{equation}</p>
<p>It is convenient that we have just standardized all our variables to have a variance of 1.0 just recently (see above). Let‚Äôs use the same linear regression method again on the standardized features to get <span class="math notranslate nohighlight">\(\beta\)</span> coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dfS</span><span class="p">[</span><span class="n">pred</span><span class="p">],</span><span class="n">dfS</span><span class="p">[</span><span class="n">resp</span><span class="p">])</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="sa">r</span><span class="s1">'Feature Ranking, $\beta$ Coefficients with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="sa">r</span><span class="s1">'Standardized Linear Regression Slope, $b_1$'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/29565b4c74d530743a2aacdb6e0cda302fe0e962b6635c8cc1f909311289fcd9.png" src="../Images/63a59eb20ac3161240452ef8f6fcd97a.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/29565b4c74d530743a2aacdb6e0cda302fe0e962b6635c8cc1f909311289fcd9.png"/>
</div>
</div>
<p>Some observations:</p>
<ul class="simple">
<li><p>the change between <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> coefficients is not just a constant scaling on the ranking metrics, because the linear model coefficients are also sensitive to the ranges and magnitudes of the features.</p></li>
<li><p>with beta coefficients porosity, acoustic impedance and total organic carbon have higher rank for estimating production</p></li>
</ul>
&#13;

<h2>Feature Importance</h2>
<p>A variety of machine learning methods provide measures of feature importance, for example decision trees the reduction in mean square error through inclusion of each feature and is summarized as:</p>
<div class="math notranslate nohighlight">
\[
FI(x) = \sum_{t \in T_f} \frac{N_t}{N} \Delta_{MSE_t}
\]</div>
<p>where <span class="math notranslate nohighlight">\(T_f\)</span> are all nodes with feature <span class="math notranslate nohighlight">\(x\)</span> as the split, <span class="math notranslate nohighlight">\(N_t\)</span> is the number of training samples reaching node <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(N\)</span> is the total number of samples in the dataset and <span class="math notranslate nohighlight">\(\Delta_{MSE_t}\)</span> is the reduction in MSE with the <span class="math notranslate nohighlight">\(t\)</span> split.</p>
<p>Note, feature importance can be calculated in a similar manner to MSE above for the case of classification trees with <strong>Gini Impurity</strong>.</p>
<p>Let‚Äôs look at the feature importance from a random forest regression model fit to our data.</p>
<ul class="simple">
<li><p>We instantiate a random forest with default hyperparameters. This results in unlimited complexity, over-trained trees in our forest. The averaging of these trees takes care of the overfit issue.</p></li>
<li><p>Then we train our random forest and extract the feature importances, calculated as the expectated feature importance over all the trees in the forest.</p></li>
<li><p>we can also extract the feature importances over all the trees in the forest and summarize with the standard deviation to access the robustness, uncertainty of our feature importance measure</p></li>
</ul>
<p>For more information check out my lecture on <a class="reference external" href="https://www.youtube.com/watch?v=m5_wk310fho&amp;list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&amp;index=39">random forest</a> predictive machine learning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Code modified from https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization</span>
<span class="n">lab_enc</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">();</span> <span class="n">Y_encoded</span> <span class="o">=</span> <span class="n">lab_enc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="c1"># this removes an encoding error </span>

<span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span>                 <span class="c1"># instantiate the random forest  </span>
<span class="n">random_forest</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">Y_encoded</span><span class="p">))</span> <span class="c1"># fit the random forest</span>
<span class="n">importance_rank</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">feature_importances_</span>    <span class="c1"># extract the expected feature importances</span>

<span class="n">importance_rank_stand</span> <span class="o">=</span> <span class="n">importance_rank</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">importance_rank</span><span class="p">)</span>                          <span class="c1"># calculate relative mutual information</span>

<span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">([</span><span class="n">tree</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># calculate stdev over trees</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importance_rank</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>             <span class="c1"># find indices for descending order</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                        <span class="c1"># plot the feature importance </span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Random Forest-based Feature importances"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">importance_rank</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span>
       <span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">std</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Feature Importance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/633da2b9b4e395da57c78aa4f82399344b016071733ef9fb8569d30c70d92604.png" src="../Images/4221de8488fdf2ce8f7716c22e1ea9d8.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/633da2b9b4e395da57c78aa4f82399344b016071733ef9fb8569d30c70d92604.png"/>
</div>
</div>
<p>There is more we can do with model-based methods. We will actually test models to assess the incremental impact of each predictor feature! We will try this with recursive feature elimination.</p>
<p>Let‚Äôs plot the results from the <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> coefficients and compare with the previous results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">231</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">rank_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Rank Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Rank Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">232</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">partial_correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Partial Correlation with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Partial Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">234</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span><span class="o">-</span><span class="mf">1000.0</span><span class="p">,</span><span class="mf">1000.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, B Coefficients with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'B Coefficients'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">235</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Beta Coefficients with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Beta Coefficients'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">236</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">importance_rank_stand</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Feature Importance with '</span> <span class="o">+</span> <span class="n">resp</span><span class="p">,</span><span class="s1">'Standardized Feature Importance'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/28592d6d0a57887d32a07cc893157689a5862f636dd4b208cc9b42b3dc9dd964.png" src="../Images/9b736db6a7b146115943a1775ce2cc0d.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/28592d6d0a57887d32a07cc893157689a5862f636dd4b208cc9b42b3dc9dd964.png"/>
</div>
</div>
&#13;

<h2>Mutual Information</h2>
<p>Mutual information is a generalized approach that quantifies the mutual dependence between two features.</p>
<ul class="simple">
<li><p>quantifies the amount of information gained from observing one feature about the other</p></li>
<li><p>avoids any assumption about the form of the relationship (e.g. no assumption of linear relationship)</p></li>
<li><p>compares the joint probabilities to the product of the marginal probabilities</p></li>
</ul>
<p>For discrete or binned continuous features <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, mutual information is calculated as:</p>
<div class="math notranslate nohighlight">
\[
I(X;Y) = \sum_{y \in Y} \sum_{x \in X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right)
\]</div>
<p>recall, given independence between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<div class="math notranslate nohighlight">
\[
P_{X,Y}(x,y) = P_X(x) \cdot P_Y(y)
\]</div>
<p>therefore if the two features are independent then the <span class="math notranslate nohighlight">\(log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) = 0\)</span></p>
<p>The joint probability <span class="math notranslate nohighlight">\(P_{X,Y}(x,y)\)</span> is a weighting term on the sum and enforces closure.</p>
<ul class="simple">
<li><p>parts of the joint distribution with greater density have greater impact on the mutual information metric</p></li>
</ul>
<p>For continuous (and nonbinned) features we can applied the integral form.</p>
<div class="math notranslate nohighlight">
\[
I(X;Y) = \int_{Y} \int_{X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) dx dy
\]</div>
<p>We get a sorted list of the indices in decreasing order of importance with the command</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p>the slice reverses the order, for descending order of feature importance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">pred</span><span class="p">]</span>                            <span class="c1"># separate DataFrames for predictor and response features</span>
<span class="n">y_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">resp</span><span class="p">]</span>

<span class="n">mi</span> <span class="o">=</span> <span class="n">mutual_info_regression</span><span class="p">(</span><span class="n">x_df</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">y_df</span><span class="p">))</span>              <span class="c1"># calculate mutual information</span>
<span class="n">mi</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">mi</span><span class="p">)</span>                                        <span class="c1"># calculate relative mutual information</span>

<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">mi</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>                          <span class="c1"># find indices for descending order</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Feature ranking:"</span><span class="p">)</span>                               <span class="c1"># write out the feature importances</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="si">%d</span><span class="s2">. feature </span><span class="si">%s</span><span class="s2"> = </span><span class="si">%f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices</span><span class="p">][</span><span class="n">f</span><span class="p">],</span> <span class="n">mi</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="n">f</span><span class="p">]]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                        <span class="c1"># plot the relative mutual information </span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Mutual Information"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mi</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span>
       <span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Mutual Information'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Feature ranking:
1. feature Por = 1.000000
2. feature Perm = 0.345842
3. feature TOC = 0.272418
4. feature Brittle = 0.073310
5. feature AI = 0.059024
6. feature VR = 0.000000
</pre></div>
</div>
<img alt="_images/704e3c6fd50614dcf67edb8dd01d1392f4509c1017dae51fcd1a26281de97377.png" src="../Images/d12dd6dd61aa76402a54c096acd0768b.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/704e3c6fd50614dcf67edb8dd01d1392f4509c1017dae51fcd1a26281de97377.png"/>
</div>
</div>
<section id="mutual-information-accounting-for-relevance-and-redundancy">
<h3>Mutual Information Accounting For Relevance and Redundancy</h3>
<p>The standard Maximum Relevance - Minumum Redundancy (MRMR) objective function considers a subset of predictor features, i.e., to score predictor feature subsets as metric to identify the most informative subset of predictor features.</p>
<ul class="simple">
<li><p>the approach calculates the average mutual information between the subset of predictor features and the response feature minus the average mutual information between the subset of predictor features.</p></li>
</ul>
<p>\begin{equation}
MID = \frac{1}{|S|}{\sum_{\alpha \in S} I(X_{\alpha},Y) } - \frac{1}{|S|^2} {\sum_{\alpha \in S}^m \sum_{\beta \in S}^m I(X_{\alpha},X_{\beta})}
\end{equation}</p>
<p>as a measure of <span class="math notranslate nohighlight">\(relevance - redundancy\)</span> or</p>
<p>\begin{equation}
MIQ = \frac{ \frac{1}{|S|}{\sum_{\alpha \in S}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|^2} {\sum_{\alpha \in S}^m \sum_{\beta \in S}^m I(X_{\alpha},X_{\beta})} }
\end{equation}</p>
<ul class="simple">
<li><p>as a measure of <span class="math notranslate nohighlight">\(\frac{relevance}{redundancy}\)</span>.</p></li>
</ul>
</section>
&#13;

<h3>Mutual Information Accounting For Relevance and Redundancy</h3>
<p>The standard Maximum Relevance - Minumum Redundancy (MRMR) objective function considers a subset of predictor features, i.e., to score predictor feature subsets as metric to identify the most informative subset of predictor features.</p>
<ul class="simple">
<li><p>the approach calculates the average mutual information between the subset of predictor features and the response feature minus the average mutual information between the subset of predictor features.</p></li>
</ul>
<p>\begin{equation}
MID = \frac{1}{|S|}{\sum_{\alpha \in S} I(X_{\alpha},Y) } - \frac{1}{|S|^2} {\sum_{\alpha \in S}^m \sum_{\beta \in S}^m I(X_{\alpha},X_{\beta})}
\end{equation}</p>
<p>as a measure of <span class="math notranslate nohighlight">\(relevance - redundancy\)</span> or</p>
<p>\begin{equation}
MIQ = \frac{ \frac{1}{|S|}{\sum_{\alpha \in S}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|^2} {\sum_{\alpha \in S}^m \sum_{\beta \in S}^m I(X_{\alpha},X_{\beta})} }
\end{equation}</p>
<ul class="simple">
<li><p>as a measure of <span class="math notranslate nohighlight">\(\frac{relevance}{redundancy}\)</span>.</p></li>
</ul>
&#13;

<h2>Mutual Information Accounting For Relevance and Redundancy OFAT Variants</h2>
<p>I propose that for one-feature-at-a-time (OFAT) predictor feature ranking (predictor feature subset, <span class="math notranslate nohighlight">\(S = [X_i]\)</span> and <span class="math notranslate nohighlight">\(|S| = 1\)</span>) we modify this to the following calculation:</p>
<ul class="simple">
<li><p><strong>relevance</strong> - the mutual information between the selected predictor feature, <span class="math notranslate nohighlight">\(X_i\)</span>, and the response feature, <span class="math notranslate nohighlight">\(Y\)</span></p></li>
<li><p><strong>redundancy</strong> - the average mutual information between the selected predictor feature, <span class="math notranslate nohighlight">\(X_i\)</span>, and the remaining predictor features, <span class="math notranslate nohighlight">\(X_{\alpha}, \alpha \ne i\)</span>.</p></li>
<li><p>we use the quotient form of the calculation from Gulgezen, Cataltepe and Yu (2009).</p></li>
</ul>
<p>Our modified version of the Maximum Relevance - Minumum Redundancy (MRMR) objective function for OFAT ranking scores the selected predictor feature <span class="math notranslate nohighlight">\(X_i\)</span>‚Äôs <strong>relevance</strong> as its mutual information with the response feature:</p>
<p>\begin{equation}
I(X_i,Y)
\end{equation}</p>
<p>and <strong>redundancy</strong> between the selected predictor feature, <span class="math notranslate nohighlight">\(X_i\)</span>, and the remaining predictor features:</p>
<p>\begin{equation}
\frac{1}{|S|-1} \sum_{\alpha=1, \alpha \ne i}^m I(X_i,X_{\alpha})
\end{equation}</p>
<p>were <span class="math notranslate nohighlight">\(X\)</span> are predictor features, <span class="math notranslate nohighlight">\(Y\)</span> is the response feature, <span class="math notranslate nohighlight">\(X_i\)</span> is the specific predictor feature being scored and <span class="math notranslate nohighlight">\(|S|\)</span> is the number of predictor features and <span class="math notranslate nohighlight">\(I()\)</span> is mutual information between the indicated features. One formulation is a simple difference, relevance minus redundancy,</p>
<div class="math notranslate nohighlight">
\[
\Phi_{\Delta}(X_i,Y) = I(X_{\alpha},Y) - \frac{1}{|S|-1} \sum_{\beta=1, \alpha \ne \beta}^m I( X_{\alpha},X_{\beta} ) 
\]</div>
<p>an alternative is a ratio,</p>
<div class="math notranslate nohighlight">
\[
\Phi_{r}(X_i,Y) = \frac{ I(X_i,Y) }{ \frac{1}{|S|-1} \sum_{\alpha=1, \alpha \ne i}^m I(X_i,X_{\alpha})}
\]</div>
<p>Here the feature ranks for the mutual information relevance minus redundancy, <span class="math notranslate nohighlight">\(\Phi_{\Delta}(X_i,Y)\)</span>, approach.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">obj_mutual</span> <span class="o">=</span> <span class="n">mutual_information_objective</span><span class="p">(</span><span class="n">x_df</span><span class="p">,</span><span class="n">y_df</span><span class="p">)</span>
<span class="n">indices_obj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">obj_mutual</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>              <span class="c1"># find indices for descending order</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                        <span class="c1"># plot the relative mutual information </span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"One-at-a-Time MRMR Objective Function for Mutual Information-based Feature Selection"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">obj_mutual</span><span class="p">[</span><span class="n">indices_obj</span><span class="p">],</span>
       <span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s2">"black"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices_obj</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Feature Importance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c0b176e4990bdb326b638222358d06af771613f6ae47153d42a7e2614c7d3c28.png" src="../Images/ce31fdb26712840a6f374091a92555c3.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/c0b176e4990bdb326b638222358d06af771613f6ae47153d42a7e2614c7d3c28.png"/>
</div>
</div>
<section id="delta-mutual-information-quotient-accounting-for-relevance-and-redundancy">
<h3>Delta Mutual Information Quotient Accounting for Relevance and Redundancy</h3>
<p>We use adapt the mutual information quotient from Gulgezen, Cataltepe and Yu (2009) to develop an OFAT ranking metric.</p>
<p>The standard MRMR objective function that scores the subset of features‚Äô <strong>relevance</strong> between the subset of predictor features and the response feature:</p>
<p>\begin{equation}
\frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) }
\end{equation}</p>
<p>and <strong>redundancy</strong> between the subset of predictor features:</p>
<p>\begin{equation}
\frac{1}{|S|^2} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})}
\end{equation}</p>
<p>To find the most informative subset of predictor features we must find the subset of features that maximize relevance while minimizing redundancy. We can accomplish this by maximizing either of these two formulations,</p>
<p>\begin{equation}
MID = \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } - \frac{1}{|S|^2} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})}
\end{equation}</p>
<p>or</p>
<p>\begin{equation}
MIQ = \frac{ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|^2} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} }
\end{equation}</p>
<p>I suggest feature ranking through the calculation of the change in <span class="math notranslate nohighlight">\(MIQ\)</span> via inclusion and removal of a specific predictor feature (<span class="math notranslate nohighlight">\(X_i\)</span>).</p>
<p>\begin{equation}
\Delta MIQ_i = \frac{ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|^2} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} } - \frac{ \frac{1}{|S|}{\sum_{\alpha=1,\alpha \ne i}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|^2} {\sum_{\alpha=1,\alpha \ne i}^m \sum_{\beta=1,\beta \ne i}^m I(X_{\alpha},X_{\beta})} }
\end{equation}</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">delta_mutual_information</span> <span class="o">=</span> <span class="n">delta_mutual_information_quotient</span><span class="p">(</span><span class="n">x_df</span><span class="p">,</span><span class="n">y_df</span><span class="p">)</span>

<span class="n">indices_delta_mutual_information</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">delta_mutual_information</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># find indices for descending order</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot the relative mutual information </span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Delta Mutual Information Quotient"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">delta_mutual_information</span><span class="p">[</span><span class="n">indices_delta_mutual_information</span><span class="p">],</span>
       <span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span><span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices_delta_mutual_information</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Feature Importance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/03d7d53a3d4abf4c562eeb53cbb1d74427a39a38a569d1e60c953a3ce9fe55a8.png" src="../Images/21f9cee66dbe4d7f6bc8cac154c1ad85.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/03d7d53a3d4abf4c562eeb53cbb1d74427a39a38a569d1e60c953a3ce9fe55a8.png"/>
</div>
</div>
<p>It is intructive to compare delta mutual information and variance inflation factor ranking. Both of these methods account for predictor feature redundancy.</p>
<ul class="simple">
<li><p>but VIF assumes linearity and does not account for relevance</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="n">delta_mutual_information</span><span class="p">),</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="o">-</span><span class="n">vif_values</span><span class="p">),</span><span class="n">c</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="n">delta_mutual_information</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="o">-</span><span class="n">vif_values</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="mf">0.1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Delta Mutual Information Rank'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Variance Inflation Factor Rank'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Variance Inflation Factor vs. Delta Mutual Information Ranking'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mf">0.1</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],[</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s1">'coral'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">'dodgerblue'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/753de82205baf137ac8f92671732ef8708fc686e982b161ca2b05ba1095ee90e.png" src="../Images/3028e58a56f928ebb26de84cfb6e1f54.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/753de82205baf137ac8f92671732ef8708fc686e982b161ca2b05ba1095ee90e.png"/>
</div>
</div>
<p>From mutual information we can observe that porosity, permeability then total organic carbon and brittleness have the greatest departure from general independence.</p>
</section>
&#13;

<h3>Delta Mutual Information Quotient Accounting for Relevance and Redundancy</h3>
<p>We use adapt the mutual information quotient from Gulgezen, Cataltepe and Yu (2009) to develop an OFAT ranking metric.</p>
<p>The standard MRMR objective function that scores the subset of features‚Äô <strong>relevance</strong> between the subset of predictor features and the response feature:</p>
<p>\begin{equation}
\frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) }
\end{equation}</p>
<p>and <strong>redundancy</strong> between the subset of predictor features:</p>
<p>\begin{equation}
\frac{1}{|S|^2} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})}
\end{equation}</p>
<p>To find the most informative subset of predictor features we must find the subset of features that maximize relevance while minimizing redundancy. We can accomplish this by maximizing either of these two formulations,</p>
<p>\begin{equation}
MID = \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } - \frac{1}{|S|^2} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})}
\end{equation}</p>
<p>or</p>
<p>\begin{equation}
MIQ = \frac{ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|^2} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} }
\end{equation}</p>
<p>I suggest feature ranking through the calculation of the change in <span class="math notranslate nohighlight">\(MIQ\)</span> via inclusion and removal of a specific predictor feature (<span class="math notranslate nohighlight">\(X_i\)</span>).</p>
<p>\begin{equation}
\Delta MIQ_i = \frac{ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|^2} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} } - \frac{ \frac{1}{|S|}{\sum_{\alpha=1,\alpha \ne i}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|^2} {\sum_{\alpha=1,\alpha \ne i}^m \sum_{\beta=1,\beta \ne i}^m I(X_{\alpha},X_{\beta})} }
\end{equation}</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">delta_mutual_information</span> <span class="o">=</span> <span class="n">delta_mutual_information_quotient</span><span class="p">(</span><span class="n">x_df</span><span class="p">,</span><span class="n">y_df</span><span class="p">)</span>

<span class="n">indices_delta_mutual_information</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">delta_mutual_information</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># find indices for descending order</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot the relative mutual information </span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Delta Mutual Information Quotient"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">delta_mutual_information</span><span class="p">[</span><span class="n">indices_delta_mutual_information</span><span class="p">],</span>
       <span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span><span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices_delta_mutual_information</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Feature Importance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/03d7d53a3d4abf4c562eeb53cbb1d74427a39a38a569d1e60c953a3ce9fe55a8.png" src="../Images/21f9cee66dbe4d7f6bc8cac154c1ad85.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/03d7d53a3d4abf4c562eeb53cbb1d74427a39a38a569d1e60c953a3ce9fe55a8.png"/>
</div>
</div>
<p>It is intructive to compare delta mutual information and variance inflation factor ranking. Both of these methods account for predictor feature redundancy.</p>
<ul class="simple">
<li><p>but VIF assumes linearity and does not account for relevance</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="n">delta_mutual_information</span><span class="p">),</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="o">-</span><span class="n">vif_values</span><span class="p">),</span><span class="n">c</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="n">delta_mutual_information</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="o">-</span><span class="n">vif_values</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="mf">0.1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Delta Mutual Information Rank'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Variance Inflation Factor Rank'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Variance Inflation Factor vs. Delta Mutual Information Ranking'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mf">0.1</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],[</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s1">'coral'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">'dodgerblue'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/753de82205baf137ac8f92671732ef8708fc686e982b161ca2b05ba1095ee90e.png" src="../Images/3028e58a56f928ebb26de84cfb6e1f54.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/753de82205baf137ac8f92671732ef8708fc686e982b161ca2b05ba1095ee90e.png"/>
</div>
</div>
<p>From mutual information we can observe that porosity, permeability then total organic carbon and brittleness have the greatest departure from general independence.</p>
&#13;

<h2>Summary of All Bivariate Metrics</h2>
<p>We have a wide array of criteria to rank our features.</p>
<ul class="simple">
<li><p>the <span class="math notranslate nohighlight">\(B\)</span> coefficient have the same issue as covariance, sensitivity to the univariate variance</p></li>
<li><p>the <span class="math notranslate nohighlight">\(\beta\)</span> coefficients remove this sensitivity and are consistent with previous results.</p></li>
</ul>
<p>Given all of these methods, I would rank the variables as:</p>
<ol class="arabic simple">
<li><p>Porosity</p></li>
<li><p>Vitrinite Reflectance</p></li>
<li><p>Acoustic Impedance</p></li>
<li><p>Permeability</p></li>
<li><p>Total Organic Carbon</p></li>
<li><p>Brittleness</p></li>
</ol>
<p>I have assigned these ranks by observing the general trend in these metrics. Of course, we could make a more quantitative score and rank by weighting each method.</p>
<p>As mentioned before, we should not neglect expert knowledge. If additional information is known about physical processes, causation, and reliability and availability of variables this should be integrated into assigning ranks.</p>
<p>We include a bonus method here, recursive feature elimination, but only provide a simple linear regression model example. More could be done with more complicated models.</p>
&#13;

<h2>Recursive Feature Elimination</h2>
<p>Recursive Feature Elimination (RFE) method works by recursively removing features and building a model with the remaining features.</p>
<ul class="simple">
<li><p>for the first step, all features are used to build a model and the features are ranked by feature importance or the <span class="math notranslate nohighlight">\(\beta\)</span> coefficient</p></li>
<li><p>the least important feature is pruned and the model is rebuilt</p></li>
<li><p>this is repeated until there is only one feature remaining</p></li>
</ul>
<p>In this code we make a prediction model based on multilinear regression and indicate that we want to find the best feature based on recursive feature elimination. The algorithm assigns rank <span class="math notranslate nohighlight">\(1,\ldots,m\)</span> for all features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">rfe_linear</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">LinearRegression</span><span class="p">(),</span><span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># set up RFE linear regression model</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'const'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>                                <span class="c1"># let's add one's for the constant term</span>
<span class="n">rfe_linear</span> <span class="o">=</span> <span class="n">rfe_linear</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">resp</span><span class="p">]))</span> <span class="c1"># recursive elimination</span>
<span class="n">dfS</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'const'</span><span class="p">,</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>                               <span class="c1"># remove the ones</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Recursive Feature Elimination: Multilinear Regression'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Rank #'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="s1">' '</span> <span class="o">+</span> <span class="n">pred</span><span class="p">[</span><span class="n">rfe_linear</span><span class="o">.</span><span class="n">ranking_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Recursive Feature Elimination: Multilinear Regression
Rank #1 Brittle
Rank #2 TOC
Rank #3 AI
Rank #4 VR
Rank #5 Por
Rank #6 Perm
</pre></div>
</div>
</div>
</div>
<p>The advantages with the recursive elimination method:</p>
<ul class="simple">
<li><p>the actual model can be used in assessing feature ranks</p></li>
<li><p>the ranking is based on accuracy of the estimate</p></li>
</ul>
<p>but this method is sensitive to:</p>
<ul class="simple">
<li><p>choice of model</p></li>
<li><p>training dataset</p></li>
</ul>
<p>The feature ranks are quite different from our previous methods.  Many have moved from the previous assessment. Perhaps we should use a more flexible modeling method.</p>
<p>Let‚Äôs repeat this method with a more flexible machine learning method, a decision tree regression model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>            
<span class="kn">import</span> <span class="nn">geostatspy.GSLIB</span> <span class="k">as</span> <span class="nn">GSLIB</span>                              <span class="c1"># GSLIB utilities, visualization and wrapper</span>
<span class="n">rfe_rf</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span><span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># set up RFE linear regression model</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'const'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>                                <span class="c1"># let's add one's for the constant term</span>

<span class="n">lab_enc</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">();</span> <span class="n">Y_encoded</span> <span class="o">=</span> <span class="n">lab_enc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

<span class="n">rfe_rf</span> <span class="o">=</span> <span class="n">rfe_rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">Y_encoded</span><span class="p">))</span>                    <span class="c1"># recursive elimination</span>
<span class="n">dfS</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'const'</span><span class="p">,</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>                               <span class="c1"># remove the ones</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Recursive Feature Elimination: Random Forest Regression'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Rank #'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="s1">' '</span> <span class="o">+</span> <span class="n">pred</span><span class="p">[</span><span class="n">rfe_rf</span><span class="o">.</span><span class="n">ranking_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Recursive Feature Elimination: Random Forest Regression
Rank #1 Por
Rank #2 VR
Rank #3 Brittle
Rank #4 Perm
Rank #5 TOC
Rank #6 AI
</pre></div>
</div>
</div>
</div>
<p>Once again, recursive feature elimination for feature ranking is sensitive to the accuracy of the model.</p>
<ul class="simple">
<li><p>the actual prediction model must have its associated hyperparameters tuned and the model accuracy checked.</p></li>
<li><p>for example, in this case the multilinear regression feature ranks are unreliable due to the poor accuracy of the linear model.</p></li>
</ul>
&#13;

<h2>Shapley Values for Feature Ranking</h2>
<p>Let‚Äôs take a random subset of the data, as background values to evaluate our model.</p>
<ul class="simple">
<li><p>we subset for faster calculation</p></li>
<li><p>we should evaluate / enforce efficient coverage of the predictor feature space</p></li>
</ul>
<p>Since Shapley values are model based, we must start with building a model</p>
<section id="build-a-random-forest-model">
<h3>Build a Random Forest Model</h3>
<p>Since Shapley is model based we need to build a model</p>
<ul class="simple">
<li><p>Let‚Äôs start with a good random forest model, observe Shapley and then return here and modify the model</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">73093</span>                                                  <span class="c1"># set the random forest hyperparameters</span>

<span class="c1"># #Underfit random forest</span>
<span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1">#Overfit random forest</span>
<span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">6</span>

<span class="c1"># #Good random forest</span>
<span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">rfr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">)</span>
<span class="n">rfr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">)</span>

<span class="n">Y_hat</span> <span class="o">=</span> <span class="n">predict_train</span> <span class="o">=</span> <span class="n">rfr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">MSE</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">Y_hat</span><span class="p">)</span>
<span class="n">Var_Explained</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">explained_variance_score</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">Y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Mean Squared Error on Training = '</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">MSE</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="s1">', Variance Explained ='</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">Var_Explained</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

<span class="n">importances</span> <span class="o">=</span> <span class="n">rfr</span><span class="o">.</span><span class="n">feature_importances_</span>               <span class="c1"># expected (global) importance over the forest fore each predictor feature</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">([</span><span class="n">rfr</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">rfr</span><span class="o">.</span><span class="n">estimators_</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">Y_hat</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Random Forest Model'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Actual Production (MCFPD)'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Estimated Production (MCFPD)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">7000</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">7000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">7000</span><span class="p">,</span><span class="mi">7000</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">head_length</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">head_width</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Feature Importances"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="n">pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">],</span><span class="n">rfr</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">std</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">)</span>
<span class="c1">#plt.xticks(range(X.shape[1]), indices)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Feature Importance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Mean Squared Error on Training =  428100.87 , Variance Explained = 0.82
</pre></div>
</div>
<img alt="_images/cbc4aa1b1542fb9bf3527dc1f3f615180d1f3b63d21eb259a11b916d03595cea.png" src="../Images/6262d930974113cf78a782fd287a8df6.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/cbc4aa1b1542fb9bf3527dc1f3f615180d1f3b63d21eb259a11b916d03595cea.png"/>
</div>
</div>
</section>
&#13;

<h3>Build a Random Forest Model</h3>
<p>Since Shapley is model based we need to build a model</p>
<ul class="simple">
<li><p>Let‚Äôs start with a good random forest model, observe Shapley and then return here and modify the model</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">73093</span>                                                  <span class="c1"># set the random forest hyperparameters</span>

<span class="c1"># #Underfit random forest</span>
<span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1">#Overfit random forest</span>
<span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">6</span>

<span class="c1"># #Good random forest</span>
<span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">rfr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">)</span>
<span class="n">rfr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">)</span>

<span class="n">Y_hat</span> <span class="o">=</span> <span class="n">predict_train</span> <span class="o">=</span> <span class="n">rfr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">MSE</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">Y_hat</span><span class="p">)</span>
<span class="n">Var_Explained</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">explained_variance_score</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">Y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Mean Squared Error on Training = '</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">MSE</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="s1">', Variance Explained ='</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">Var_Explained</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

<span class="n">importances</span> <span class="o">=</span> <span class="n">rfr</span><span class="o">.</span><span class="n">feature_importances_</span>               <span class="c1"># expected (global) importance over the forest fore each predictor feature</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">([</span><span class="n">rfr</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">rfr</span><span class="o">.</span><span class="n">estimators_</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">Y_hat</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Random Forest Model'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Actual Production (MCFPD)'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Estimated Production (MCFPD)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">7000</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">7000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">7000</span><span class="p">,</span><span class="mi">7000</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">head_length</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">head_width</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Feature Importances"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="n">pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">],</span><span class="n">rfr</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s2">"darkorange"</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">std</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">)</span>
<span class="c1">#plt.xticks(range(X.shape[1]), indices)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Feature Importance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Mean Squared Error on Training =  428100.87 , Variance Explained = 0.82
</pre></div>
</div>
<img alt="_images/cbc4aa1b1542fb9bf3527dc1f3f615180d1f3b63d21eb259a11b916d03595cea.png" src="../Images/6262d930974113cf78a782fd287a8df6.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/cbc4aa1b1542fb9bf3527dc1f3f615180d1f3b63d21eb259a11b916d03595cea.png"/>
</div>
</div>
&#13;

<h2>Calculate Shapley Values</h2>
<p>Let‚Äôs select some background data at random to calculate local Shapley values and then summarize with global Shapley measures.</p>
<p>Background Samples are selected as a random subset from all the data. Why not just use all the data as background?</p>
<ul class="simple">
<li><p><strong>Shapley values can be computationally expensive to calculate</strong>, we need all the combinations of models to get all the predictions for marginal contributions that are summarized as Shapley values</p></li>
<li><p><strong>The original data may be sampled in a biased manner</strong>, then we would want to ensure that the background data are representative, i.e., sampled from the original data to reduce bias to avoid bias in our feature importance assessment</p></li>
<li><p><strong>Generalization vs. specific prediction cases</strong>, if all the data are used as background we get an overall data assessment of feature importance, but we may want to carefully select data to explore specific prediction cases</p></li>
</ul>
<p>For simplicity here, we just select randomly selection <span class="math notranslate nohighlight">\(n\)</span> data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">background</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">nsamples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">73073</span><span class="p">)</span> 
<span class="n">model_explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">rfr</span><span class="p">)</span>
<span class="n">shap_values</span> <span class="o">=</span> <span class="n">model_explainer</span><span class="o">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">background</span><span class="p">)</span> <span class="c1"># global Shapley Measures</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Local Shapley Values</h2>
<p>Let‚Äôs start by looking at the local Shapley values to demonstrate the concept of efficiency.</p>
<ul class="simple">
<li><p>first let‚Äôs confirm that the output from the shap function is a <span class="math notranslate nohighlight">\(\left[n_{background}, m\right]\)</span> nd array.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">shap_values</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(50, 6)
</pre></div>
</div>
</div>
</div>
<p>We have the local Shapley values for each prediction for the background cases. Let‚Äôs visualize one to demonstrate this.</p>
<ul class="simple">
<li><p>I coded this custom visualization to clearly communicate local Shapley values and the concept of efficiency.</p></li>
<li><p>We start at the average of the training response feature and add the local Shapely values for each predictor feature to reach the prediction.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nback</span> <span class="o">=</span> <span class="mi">7</span>

<span class="n">resp_avg</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">Y_hat</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">rfr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">background</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="n">nback</span><span class="p">]])</span>

<span class="n">current</span> <span class="o">=</span> <span class="n">resp_avg</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="p">,</span><span class="n">current</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.3</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="n">current</span><span class="p">],[</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.3</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="p">,</span><span class="n">current</span><span class="o">+</span><span class="mi">2</span><span class="p">],[</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.2</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">current</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">0.5</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="s1">'blue'</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="p">,</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]],[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="p">,</span><span class="n">current</span><span class="p">],[</span><span class="n">i</span><span class="o">+</span><span class="mf">0.6</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">],</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]],[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">1.3</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]],[</span><span class="n">i</span><span class="o">+</span><span class="mf">1.2</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">1.3</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">],</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="mi">2</span><span class="p">],[</span><span class="n">i</span><span class="o">+</span><span class="mf">1.3</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">1.2</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'+ '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">],</span><span class="mi">0</span><span class="p">)),[</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">1.1</span><span class="p">],</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'- '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]),</span><span class="mi">0</span><span class="p">)),[</span><span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">1.1</span><span class="p">],</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">current</span> <span class="o">+</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="p">,</span><span class="n">current</span><span class="p">],[</span><span class="n">i</span><span class="o">+</span><span class="mf">0.7</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="n">current</span><span class="p">],[</span><span class="n">i</span><span class="o">+</span><span class="mf">0.9</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">current</span><span class="p">,</span><span class="n">current</span><span class="o">+</span><span class="mi">2</span><span class="p">],[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mf">0.9</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">resp_avg</span><span class="p">,</span><span class="n">resp_avg</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mf">1.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">yhat</span><span class="p">,</span><span class="n">yhat</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mf">1.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Response Feature, Training Average'</span><span class="p">,[</span><span class="n">resp_avg</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Model Prediction'</span><span class="p">,[</span><span class="n">yhat</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mi">2</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="sa">r</span><span class="s1">'None / $\overline</span><span class="si">{y}</span><span class="s1">$'</span><span class="p">]</span> <span class="o">+</span> <span class="n">pred</span> <span class="o">+</span> <span class="p">[</span><span class="sa">r</span><span class="s1">'$\hat</span><span class="si">{y}</span><span class="s1">=f(X)$'</span><span class="p">])</span>
<span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">+</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Production (MCFPD)'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Feature'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Local Shapley Values, Background Index: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">nback</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4687606001e63cc73cb21d138e664ae5506aa6589d98a6bb132426a1eeacd218.png" src="../Images/223f16f325ce6407f83bcbcad31a87a9.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/4687606001e63cc73cb21d138e664ae5506aa6589d98a6bb132426a1eeacd218.png"/>
</div>
</div>
<p>Now I show you the built in plotting methods to communicate the same thing with the shap Python Package.</p>
&#13;

<h2>Shapley Force Plot</h2>
<p>We can simulaneously visualize all of the Shapley values for all of the sample data in the order of the background data set.</p>
<ul class="simple">
<li><p>blue indicates reduction in the predicted production and red indicates increase in predicted production</p></li>
</ul>
<p>We are visualizing over all background sample data at once. Reorder by original sample ordering and select the nback index to compare to the above plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">shap</span><span class="o">.</span><span class="n">force_plot</span><span class="p">(</span><span class="n">model_explainer</span><span class="o">.</span><span class="n">expected_value</span><span class="p">,</span><span class="n">shap_values</span><span class="p">,</span><span class="n">background</span><span class="p">,</span><span class="n">out_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Production'</span><span class="p">],</span><span class="n">feature_names</span><span class="o">=</span><span class="n">pred</span><span class="p">,)</span>  
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<div id="i38ZAMPLKNLGY5L30ROKQ">
<div style="color: #900; text-align: center;">
  <b>Visualization omitted, Javascript library not loaded!</b><br/>
  Have you run `initjs()` in this notebook? If this notebook was from another
  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing
  this notebook on github the Javascript has been stripped for security. If you are using
  JupyterLab this error is because a JupyterLab extension has not yet been written.
</div></div>
 </div></div>
</div>
&#13;

<h2>Local Force Plot</h2>
<p>We pick a specific sample from the background and visualize the force plot.</p>
<ul class="simple">
<li><p>We can see the genesis of the plot above, Shapley values for all features given a local set of values in sample <span class="math notranslate nohighlight">\(i\)</span>, (<span class="math notranslate nohighlight">\(x_i\)</span>).</p></li>
</ul>
<p>Compare this result to the above custom plot that I made and you will see that it communicates the same information.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">shap</span><span class="o">.</span><span class="n">force_plot</span><span class="p">(</span><span class="n">model_explainer</span><span class="o">.</span><span class="n">expected_value</span><span class="p">,</span><span class="n">shap_values</span><span class="p">[</span><span class="n">nback</span><span class="p">],</span><span class="n">background</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="n">nback</span><span class="p">]],</span><span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<div id="i4LGR5S1DL4ZR5IW69141">
<div style="color: #900; text-align: center;">
  <b>Visualization omitted, Javascript library not loaded!</b><br/>
  Have you run `initjs()` in this notebook? If this notebook was from another
  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing
  this notebook on github the Javascript has been stripped for security. If you are using
  JupyterLab this error is because a JupyterLab extension has not yet been written.
</div></div>
 </div></div>
</div>
<p>Appreciation to Xuesong Ma for the suggestion to improve the above local Shapley value content and visualizations.</p>
&#13;

<h2>Global Shapley Values</h2>
<p>Let‚Äôs review the global Shapley measures.</p>
<ul class="simple">
<li><p>sorted bar chart of the arithmetic average of the absolute SHAP value over the background data</p></li>
<li><p>sorted plot of the SHAP value over the background data</p></li>
<li><p>plot of the SHAP value over the background data as a violin plot</p></li>
</ul>
<p>Note: all of these methods are applying the global average (<span class="math notranslate nohighlight">\(E[X_i]\)</span>) for each feature to impute for those cases not including feature <span class="math notranslate nohighlight">\(i\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">pred</span><span class="p">,</span> <span class="n">shap_values</span> <span class="o">=</span> <span class="n">shap_values</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">background</span><span class="p">,</span> <span class="n">plot_type</span><span class="o">=</span><span class="s2">"bar"</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s2">"darkorange"</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Predictor Features'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">pred</span><span class="p">,</span> <span class="n">shap_values</span> <span class="o">=</span> <span class="n">shap_values</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">background</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">pred</span><span class="p">,</span> <span class="n">shap_values</span> <span class="o">=</span> <span class="n">shap_values</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">background</span><span class="p">,</span><span class="n">plot_type</span> <span class="o">=</span> <span class="s2">"violin"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2f143182e5f4f722358c7743d0e8d7cd49f8f2a4e3afecee9fcacac06e45af95.png" src="../Images/e3945b1cd0e515c4f1adfcc33e2436d8.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/2f143182e5f4f722358c7743d0e8d7cd49f8f2a4e3afecee9fcacac06e45af95.png"/>
</div>
</div>
<p>The the center and right plots show the Shapley values for each feature over all the randomly selected background samples, while the plot on the left is the bar chart of the mean absolute Shapley values.</p>
<ul class="simple">
<li><p>Porosity, Permeability and TOC are the top features</p></li>
</ul>
&#13;

<h2>Comments</h2>
<p>This was a basic treatment of feature ranking. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a> and the YouTube lecture links at the start of this chapter with resource links in the videos‚Äô descriptions.</p>
<p>I hope this is helpful,</p>
<p><em>Michael</em></p>
&#13;

<h2>About the Author</h2>
<figure style="text-align: center;">
  <img src="../Images/eb709b2c0a0c715da01ae0165efdf3b2.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/intro/michael_pyrcz_officeshot_jacket.jpg"/>
  <figcaption style="text-align: center;"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.
</figcaption>
</figure>
<p>Michael Pyrcz is a professor in the <a class="reference external" href="https://cockrell.utexas.edu/faculty-directory/alphabetical/p">Cockrell School of Engineering</a>, and the <a class="reference external" href="https://www.jsg.utexas.edu/researcher/michael_pyrcz/">Jackson School of Geosciences</a>, at <a class="reference external" href="https://www.utexas.edu/">The University of Texas at Austin</a>, where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,</p>
<ul class="simple">
<li><p>the principal investigator of the <a class="reference external" href="https://fri.cns.utexas.edu/energy-analytics">Energy Analytics</a> freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin</p></li>
<li><p>an associate editor for <a class="reference external" href="https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board">Computers and Geosciences</a>, and a board member for <a class="reference external" href="https://link.springer.com/journal/11004/editorial-board">Mathematical Geosciences</a>, the International Association for Mathematical Geosciences.</p></li>
</ul>
<p>Michael has written over 70 <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en">peer-reviewed publications</a>, a <a class="reference external" href="https://pypi.org/project/geostatspy/">Python package</a> for spatial data analytics, co-authored a textbook on spatial data analytics, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a> and author of two recently released e-books, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy</a> and <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html">Applied Machine Learning in Python: a Hands-on Guide with Code</a>.</p>
<p>All of Michael‚Äôs university lectures are available on his <a class="reference external" href="https://www.youtube.com/@GeostatsGuyLectures">YouTube Channel</a> with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his <a class="reference external" href="https://github.com/GeostatsGuy">GitHub account</a>, to support any interested students and working professionals with evergreen content. To find out more about Michael‚Äôs work and shared educational resources visit his <span class="xref myst">Website</span>.</p>
&#13;

<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>I‚Äôm always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
    
</body>
</html>