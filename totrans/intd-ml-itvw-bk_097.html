<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>8.2.3 Reinforcement learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>8.2.3 Reinforcement learning</h1>
<blockquote>åŽŸæ–‡ï¼š<a href="https://huyenchip.com/ml-interviews-book/contents/8.2.3-reinforcement-learning.html">https://huyenchip.com/ml-interviews-book/contents/8.2.3-reinforcement-learning.html</a></blockquote>
                                
                                
<blockquote>
<p>ðŸŒ³ <strong>Tip</strong> ðŸŒ³<br/>
To refresh your knowledge on deep RL, checkout <a href="https://spinningup.openai.com/en/latest/" target="_blank">Spinning Up in Deep RL</a> (OpenAI)</p>
</blockquote>
<ol>
<li>[E] Explain the explore vs exploit tradeoff with examples.</li>
<li>[E] How would a finite or infinite horizon affect our algorithms?</li>
<li>[E] Why do we need the discount term for objective functions?</li>
<li><p>[E] Fill in the empty circles using the minimax algorithm.</p>
<center>
    <img src="../Images/24e7be9d8e9ad4ff02ee1ee2e821bff8.png" width="45%" alt="Minimax algorithm" title="image_tooltip" data-original-src="https://huyenchip.com/ml-interviews-book/contents/images/image34.png"/>
</center>
</li>
<li><p>[M] Fill in the alpha and beta values as you traverse the minimax tree from left to right.</p>
<center>
    <img src="../Images/e5c5778c32bf07f69195be40a13b8d14.png" width="80%" alt="Alpha-beta pruning" title="image_tooltip" data-original-src="https://huyenchip.com/ml-interviews-book/contents/images/image35.png"/>
</center>
</li>
<li><p>[E] Given a policy, derive the reward function.</p>
</li>
<li>[M] Pros and cons of on-policy vs. off-policy.</li>
<li>[M] Whatâ€™s the difference between model-based and model-free? Which one is more data-efficient?</li>
</ol>

                                
                                    
</body>
</html>