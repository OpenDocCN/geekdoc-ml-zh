- en: AGI Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AGI系统
- en: '*DALL·E 3 Prompt: A futuristic visualization showing the evolution from current
    ML systems to AGI. The image depicts a technical visualization with three distinct
    zones: in the foreground, familiar ML components like neural networks, GPUs, and
    data pipelines; in the middle ground, emerging systems like large language models
    and multi-agent architectures forming interconnected constellations; and in the
    background, a luminous horizon suggesting AGI. The scene uses a gradient from
    concrete technical blues and greens in the foreground to abstract golden and white
    light at the horizon. Circuit patterns and data flows connect all elements, showing
    how today’s building blocks evolve into tomorrow’s intelligence. The style is
    technical yet aspirational, suitable for an advanced textbook.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3 提示：一幅描绘从当前机器学习系统到通用人工智能（AGI）演变的未来派可视化图像。该图像展示了一个技术可视化，有三个不同的区域：在前景，熟悉的机器学习组件，如神经网络、GPU和数据管道；在中景，新兴的系统，如大型语言模型和多智能体架构形成相互连接的星座；在背景，一个发光的地平线暗示着AGI。场景使用从前景的混凝土技术蓝绿色渐变到地平线的抽象金色和白色光线。电路图案和数据流连接所有元素，展示了今天的构建块如何演变成明天的智能。这种风格既技术又充满希望，适合高级教科书。*'
- en: '![](../media/file320.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file320.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*Why must machine learning systems practitioners understand emerging trends
    and anticipate technological evolution rather than simply mastering current implementations?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么机器学习系统从业者必须理解新兴趋势并预见技术演变，而不是仅仅掌握当前的实施情况？*'
- en: Machine learning systems operate in a rapidly evolving technological landscape
    where yesterday’s cutting-edge approaches become tomorrow’s legacy systems, demanding
    practitioners who can anticipate and adapt to rapid shifts. Unlike mature engineering
    disciplines, ML systems face continuous disruption from algorithmic breakthroughs,
    hardware advances, and changing computational paradigms reshaping system architecture
    requirements. Understanding emerging trends enables engineers to make forward-looking
    design decisions extending system lifespans, avoiding technological dead ends,
    and positioning infrastructure for future capabilities. This anticipatory mindset
    becomes critical as organizations invest heavily in ML systems expected to operate
    for years while underlying technology continues evolving rapidly. Studying frontier
    developments helps practitioners develop strategic thinking necessary to build
    adaptive systems, evaluate emerging technologies against current implementations,
    and make informed decisions about when and how to incorporate innovations into
    production environments.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统运行在一个快速发展的技术环境中，昨天的尖端方法可能成为明天的过时系统，这要求从业者能够预见并适应快速的变化。与成熟的工程学科不同，机器学习系统面临着来自算法突破、硬件进步和不断变化的计算范式对系统架构要求的持续颠覆。理解新兴趋势使工程师能够做出前瞻性的设计决策，延长系统寿命，避免技术死胡同，并为未来的能力定位基础设施。随着组织在预计将运行多年且底层技术持续快速发展的机器学习系统上大量投资，这种预见性思维变得至关重要。研究前沿发展有助于从业者发展必要的战略思维，以构建适应性系统，评估新兴技术与当前实施情况，并就何时以及如何将创新纳入生产环境做出明智的决策。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Define artificial general intelligence (AGI) and distinguish it from narrow
    AI through domain generality, knowledge transfer, and continuous learning capabilities
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义通用人工智能（AGI）并区分它和窄人工智能，通过领域普遍性、知识迁移和持续学习能力
- en: Analyze how current AI limitations (lack of causal reasoning, persistent memory,
    and cross-domain transfer) constrain progress toward AGI
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析当前人工智能限制（缺乏因果推理、持久记忆和跨领域迁移）如何限制向AGI的进步
- en: Compare competing AGI paradigms (scaling hypothesis, neurosymbolic approaches,
    embodied intelligence, multi-agent systems) and evaluate their engineering trade-offs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较竞争的AGI范式（规模假设、神经符号方法、具身智能、多智能体系统）并评估它们的工程权衡
- en: Design compound AI system architectures that integrate specialized components
    for enhanced capabilities beyond monolithic models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计复合人工智能系统架构，集成专用组件以增强超越单体模型的能力
- en: Evaluate emerging architectural paradigms (state space models, energy-based
    models, neuromorphic computing) for their potential to overcome transformer limitations
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估新兴的架构范式（状态空间模型、基于能量的模型、神经形态计算）克服Transformer限制的潜力
- en: Assess advanced training methodologies (RLHF, Constitutional AI, continual learning)
    for developing aligned and adaptive compound systems
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估高级训练方法（RLHF、宪法AI、持续学习）以开发对齐和自适应的复合系统
- en: Identify critical technical barriers to AGI development including context limitations,
    energy constraints, reasoning capabilities, and alignment challenges
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定AGI发展的关键技术障碍，包括上下文限制、能源约束、推理能力和对齐挑战
- en: Synthesize infrastructure requirements across optimization, hardware acceleration,
    and operations for AGI-scale systems
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 综合优化、硬件加速和运营方面的基础设施需求，以适应AGI规模系统
- en: From Specialized AI to General Intelligence
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从专用人工智能到通用智能
- en: When tasked with planning a complex, multi-day project, ChatGPT generates plausible
    sounding plans that often contain logical flaws[1](#fn1). When asked to recall
    details from previous conversations, it fails due to lack of persistent memory.
    When required to explain why a particular solution works through first principles
    reasoning, it reproduces learned patterns rather than demonstrating genuine comprehension.
    These failures represent not simple bugs but fundamental architectural limitations.
    Contemporary models lack persistent memory, causal reasoning, and planning capabilities,
    the very attributes that define general intelligence.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当被要求规划一个复杂的多日项目时，ChatGPT生成听起来合理的计划，但往往包含逻辑错误[1](#fn1)。当要求回忆先前的对话细节时，由于缺乏持久记忆而失败。当需要通过第一性原理推理解释为什么某个解决方案有效时，它重现了学习到的模式，而不是展示真正的理解。这些失败不仅仅是简单的错误，而是根本性的架构限制。当代模型缺乏持久记忆、因果推理和规划能力，这些正是定义通用智能的属性。
- en: Exploring the engineering roadmap from today’s specialized systems to tomorrow’s
    Artificial General Intelligence (AGI), we frame it as a complex systems integration
    challenge. While contemporary large-scale systems demonstrate capabilities across
    diverse domains from natural language understanding to multimodal reasoning they
    remain limited by their architectures. The field of machine learning systems has
    reached a critical juncture where the convergence of engineering principles enables
    us to envision systems that transcend these limitations, requiring new theoretical
    frameworks and engineering methodologies.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 探索从今天的专用系统到明天的通用人工智能（AGI）的工程路线图，我们将它视为一个复杂的系统集成挑战。虽然当代大规模系统在自然语言理解到多模态推理等多个领域展示了能力，但它们仍然受限于其架构。机器学习系统领域已经达到一个关键转折点，工程原则的汇聚使我们能够展望超越这些限制的系统，需要新的理论框架和工程方法。
- en: This chapter examines the trajectory from contemporary specialized systems toward
    artificial general intelligence through the lens of systems engineering principles
    established throughout this textbook. The central thesis argues that artificial
    general intelligence constitutes primarily a systems integration challenge rather
    than an algorithmic breakthrough, requiring coordination of heterogeneous computational
    components, adaptive memory architectures, and continuous learning mechanisms
    that operate across arbitrary domains without task-specific optimization.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过本教科书建立的系统工程原则的视角，考察了从当代专用系统向人工智能的轨迹。中心论点认为，通用人工智能主要构成一个系统集成挑战，而不是算法突破，需要协调异构计算组件、自适应内存架构和跨任意领域无特定任务优化的持续学习机制。
- en: The analysis proceeds along three interconnected research directions that define
    the contemporary frontier in intelligent systems. First, we investigate artificial
    general intelligence as a systems integration problem, examining how current limitations
    in causal reasoning, knowledge incorporation, and cross-domain transfer constrain
    progress toward domain-general intelligence. Second, we analyze compound AI systems
    as practical architectures that transcend monolithic model limitations through
    orchestration of specialized components, offering immediate pathways toward enhanced
    capabilities. Third, we explore emerging computational paradigms including energy-based
    models, state space architectures, and neuromorphic computing that promise different
    approaches to learning and inference.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 分析沿着三个相互关联的研究方向进行，这些方向定义了当代智能系统的前沿。首先，我们研究通用人工智能作为一个系统集成问题，探讨当前在因果推理、知识融合和跨领域迁移中的局限性如何制约向领域通用智能的进步。其次，我们分析复合人工智能系统作为超越单体模型限制的实用架构，通过协调专用组件提供通往增强能力的直接途径。第三，我们探索包括基于能量的模型、状态空间架构和神经形态计算等新兴计算范式，这些范式承诺不同的学习和推理方法。
- en: These developments carry profound implications for every domain of machine learning
    systems engineering. Data engineering must accommodate multimodal, streaming,
    and synthetically generated content at scales that challenge existing pipeline
    architectures. Training infrastructure requires coordination of heterogeneous
    computational substrates combining symbolic and statistical learning paradigms.
    Model optimization must preserve emergent capabilities while ensuring deployment
    across diverse hardware configurations. Operational systems must maintain reliability,
    safety, and alignment properties as capabilities approach and potentially exceed
    human cognitive performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发展对机器学习系统工程的每个领域都产生了深远的影响。数据工程必须适应多模态、流式和合成生成内容，其规模挑战现有的管道架构。训练基础设施需要协调异构的计算基础，结合符号和统计学习范式。模型优化必须在保持涌现能力的同时，确保部署在多样化的硬件配置中。操作系统必须在能力接近甚至可能超过人类认知性能时，保持可靠性、安全性和对齐属性。
- en: The significance of these frontiers extends beyond technical considerations
    to encompass strategic implications for practitioners designing systems intended
    to operate over extended timescales. Contemporary architectural decisions regarding
    data representation, computational resource allocation, and system modularity
    will determine whether artificial general intelligence emerges through incremental
    progress or requires paradigm shifts. The engineering principles governing these
    choices will shape the trajectory of artificial intelligence development and its
    integration with human cognitive systems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些前沿的重要性不仅超越了技术考虑，还包括对设计旨在在长期时间尺度上运行的系统的实践者的战略影响。关于数据表示、计算资源分配和系统模块化的当代架构决策将决定通用人工智能是通过渐进式进步还是需要范式转变而出现。指导这些选择的工程原则将塑造人工智能发展的轨迹及其与人类认知系统的集成。
- en: Rather than engaging in speculative futurism, this chapter grounds its analysis
    in systematic extensions of established engineering methodologies. The path toward
    artificial general intelligence emerges through disciplined application of systems
    thinking, scaled integration of proven techniques, and careful attention to emergent
    behaviors arising from complex component interactions. This approach positions
    artificial general intelligence as an achievable engineering objective that builds
    incrementally upon existing capabilities while recognizing the qualitative challenges
    inherent in transcending narrow domain specialization.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是参与推测性的未来主义，本章将分析建立在系统扩展现有工程方法论的基础上。通往通用人工智能的道路是通过系统思维的纪律性应用、已验证技术的规模化集成以及对复杂组件交互产生的涌现行为的细致关注而出现的。这种方法将通用人工智能定位为一个可实现的工程目标，它逐步建立在现有能力之上，同时认识到超越狭窄领域专业化的定性挑战。
- en: 'Defining AGI: Intelligence as a Systems Problem'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义AGI：将智能视为一个系统问题
- en: '***Artificial General Intelligence (AGI)*** represents computational systems
    that match human cognitive capabilities across all domains through *domain generality*,
    *knowledge transfer*, and *continuous learning*, rather than excelling at narrow,
    task-specific applications.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用人工智能（AGI）**代表的是通过**领域通用性**、**知识迁移**和**持续学习**，在所有领域与人类认知能力相匹配的计算系统，而不是在狭窄的任务特定应用中表现出色。'
- en: AGI emerges as primarily a systems engineering challenge. While ChatGPT and
    Claude demonstrate strong capabilities within language domains, and specialized
    systems defeat world champions at chess and Go, true AGI requires integrating
    perception, reasoning, planning, and action within architectures that adapt without
    boundaries[2](#fn2).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通用人工智能（AGI）的出现主要是一个系统工程挑战。虽然ChatGPT和Claude在语言领域表现出强大的能力，而专业系统在象棋和国际象棋中击败了世界冠军，但真正的AGI需要将感知、推理、规划和行动整合到没有边界的架构中[2](#fn2)。
- en: 'Consider the cognitive architecture underlying human intelligence. The brain
    coordinates specialized subsystems through hierarchical integration: sensory cortices
    process multimodal input, the hippocampus consolidates episodic memories, the
    prefrontal cortex orchestrates executive control, and the cerebellum refines motor
    predictions. Each subsystem operates with distinct computational principles, yet
    they combine seamlessly to produce unified behavior. This biological blueprint
    suggests that AGI will emerge not from scaling single architectures, but from
    orchestrating specialized components, precisely the compound systems approach
    we explore throughout this chapter.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑人类智能背后的认知架构。大脑通过分层集成协调专门的子系统：感觉皮层处理多模态输入，海马体巩固情景记忆，前额叶皮层协调执行控制，小脑细化运动预测。每个子系统以不同的计算原理运行，但它们无缝结合，产生统一的行为。这个生物蓝图表明，AGI不会从扩展单个架构中产生，而是从协调专门的组件中产生，这正是我们在本章中探讨的复合系统方法。
- en: 'Current systems excel at pattern matching but lack causal understanding. When
    ChatGPT solves a physics problem, it leverages statistical correlations from training
    data rather than modeling physical laws. When DALL-E generates an image, it combines
    learned visual patterns without understanding three-dimensional structure or lighting
    physics. These limitations stem from architectural constraints: transformers process
    information through attention mechanisms optimized for sequence modeling, not
    causal reasoning or spatial understanding.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当前系统在模式匹配方面表现出色，但缺乏因果理解。当ChatGPT解决物理问题时，它利用训练数据中的统计相关性，而不是建模物理定律。当DALL-E生成图像时，它结合了学习到的视觉模式，但没有理解三维结构或光照物理。这些局限性源于架构约束：变压器通过优化于序列建模的注意力机制处理信息，而不是因果推理或空间理解。
- en: Energy-based models offer an alternative framework that could bridge this gap,
    providing optimization-driven reasoning that mimics how biological systems solve
    problems through energy minimization (detailed in [Section 20.5.2](ch026.xhtml#sec-agi-systems-energybased-models-learning-optimization-e4c6)).
    Rather than predicting the most probable next token, these systems find configurations
    that minimize global energy functions, potentially enabling genuine reasoning
    about cause and effect.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基于能量的模型提供了一个可以弥合这一差距的替代框架，它提供了一种通过优化驱动的推理，模仿生物系统通过能量最小化解决问题的方式（详见[第20.5.2节](ch026.xhtml#sec-agi-systems-energybased-models-learning-optimization-e4c6)）。这些系统不是预测最可能的下一个标记，而是找到最小化全局能量函数的配置，这可能使真正关于因果关系的推理成为可能。
- en: 'The path from today’s specialized systems to tomorrow’s general intelligence
    requires advances across every domain covered in this textbook: distributed training
    ([Chapter 8](ch014.xhtml#sec-ai-training)) must coordinate heterogeneous architectures,
    hardware acceleration ([Chapter 11](ch017.xhtml#sec-ai-acceleration)) must support
    diverse computational patterns, and data engineering ([Chapter 6](ch012.xhtml#sec-data-engineering))
    must synthesize causal training examples. Most critically, [Chapter 2](ch008.xhtml#sec-ml-systems)
    integration principles must evolve to orchestrate different representational frameworks.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从今天的专业系统到明天的通用智能之路，需要在本书涵盖的每个领域取得进展：分布式训练（[第8章](ch014.xhtml#sec-ai-training)）必须协调异构架构，硬件加速（[第11章](ch017.xhtml#sec-ai-acceleration)）必须支持不同的计算模式，以及数据工程（[第6章](ch012.xhtml#sec-data-engineering)）必须综合因果训练示例。最重要的是，[第2章](ch008.xhtml#sec-ml-systems)的集成原则必须演变，以协调不同的表示框架。
- en: 'Contemporary AGI research divides into four competing paradigms, each offering
    different answers to the question: What computational approach will achieve artificial
    general intelligence? These paradigms represent more than academic debates; they
    suggest radically different engineering paths, resource requirements, and timeline
    expectations.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当代AGI研究分为四个相互竞争的范例，每个范例都对以下问题提供不同的答案：哪种计算方法将实现通用人工智能？这些范例不仅代表了学术辩论，还暗示了截然不同的工程路径、资源需求和时间预期。
- en: The Scaling Hypothesis
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缩放假设
- en: 'The scaling hypothesis, championed by OpenAI and Anthropic, posits that AGI
    will emerge through continued scaling of transformer architectures ([Kaplan et
    al. 2020](ch058.xhtml#ref-kaplan2020scaling)). This approach extrapolates from
    observed scaling laws that reveal consistent, predictable relationships between
    model performance and three key factors: parameter count N, dataset size D, and
    compute budget C. Empirically, test loss follows power law relationships: L(N)
    ∝ N^(-α) for parameters, L(D) ∝ D^(-β) for data, and L(C) ∝ C^(-γ) for compute,
    where α ≈ 0.076, β ≈ 0.095, and γ ≈ 0.050 ([Kaplan et al. 2020](ch058.xhtml#ref-kaplan2020scaling)).
    These smooth, predictable curves suggest that each 10× increase in parameters
    yields measurable capability improvements across diverse tasks, from language
    understanding to reasoning and code generation.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放假设，由OpenAI和Anthropic倡导，认为AGI将通过持续缩放Transformer架构而出现([Kaplan et al. 2020](ch058.xhtml#ref-kaplan2020scaling))。这种方法从观察到的缩放定律外推，这些定律揭示了模型性能与三个关键因素之间的稳定、可预测的关系：参数数量N、数据集大小D和计算预算C。经验上，测试损失遵循幂律关系：对于参数，L(N)
    ∝ N^(-α)，对于数据，L(D) ∝ D^(-β)，对于计算，L(C) ∝ C^(-γ)，其中α ≈ 0.076，β ≈ 0.095，γ ≈ 0.050
    ([Kaplan et al. 2020](ch058.xhtml#ref-kaplan2020scaling))。这些平滑、可预测的曲线表明，参数数量每增加10倍，就能在从语言理解到推理和代码生成等不同任务中带来可测量的能力提升。
- en: 'Recent developments have expanded the scaling hypothesis beyond training time
    compute to include inference time compute. OpenAI’s o1 and o3 reasoning models
    demonstrate that allowing models to “think longer” during inference through explicit
    chain of thought reasoning and search over solution paths can dramatically improve
    performance on complex reasoning tasks. This suggests a new scaling dimension:
    rather than solely investing compute in larger models, allocating compute to extended
    inference enables models to tackle problems requiring multi-step reasoning, planning,
    and self-verification. The systems implications are significant, as inference
    time scaling requires different infrastructure optimizations than training time
    scaling.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的发展已经将缩放假设从训练时间的计算扩展到推理时间的计算。OpenAI的o1和o3推理模型表明，通过显式的思维链推理和解决方案路径的搜索，允许模型在推理过程中“思考更长”可以显著提高复杂推理任务的表现。这表明一个新的缩放维度：与其仅仅投资于更大的模型，不如将计算分配给扩展的推理，使模型能够处理需要多步推理、规划和自我验证的问题。系统的影响是显著的，因为推理时间的缩放需要与训练时间的缩放不同的基础设施优化。
- en: 'The extrapolation becomes striking when projected to AGI scale. If these scaling
    laws continue, AGI training would require approximately 2.5 × 10²⁶ FLOPs[3](#fn3),
    a 250× increase over GPT-4’s estimated compute budget. This represents not merely
    quantitative scaling but a qualitative bet: that sufficient scale will induce
    emergent capabilities like robust reasoning, planning, and knowledge integration
    that current models lack.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当将外推扩展到通用人工智能（AGI）规模时，这种外推变得引人注目。如果这些缩放定律持续存在，AGI的训练将需要大约2.5 × 10²⁶浮点运算（FLOPs）[3](#fn3)，这比GPT-4的估计计算预算增加了250倍。这不仅仅代表数量的缩放，更是一种质的赌注：足够的规模将诱导出当前模型所缺乏的如稳健推理、规划和知识整合等涌现能力。
- en: 'Such scale requires datacenter coordination ([Chapter 8](ch014.xhtml#sec-ai-training))
    and higher hardware utilization ([Chapter 11](ch017.xhtml#sec-ai-acceleration))
    to make training economically feasible. The sheer magnitude drives exploration
    of post-Moore’s Law architectures: 3D chip stacking for higher transistor density,
    optical interconnects for reduced communication overhead, and processing-in-memory
    to minimize data movement.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的规模需要数据中心协调([第8章](ch014.xhtml#sec-ai-training))和更高的硬件利用率([第11章](ch017.xhtml#sec-ai-acceleration))，以使训练在经济上可行。巨大的规模推动了摩尔定律之后的架构探索：3D芯片堆叠以实现更高的晶体管密度，光互连以减少通信开销，以及在内存中处理以最小化数据移动。
- en: Hybrid Neurosymbolic Architectures
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合神经符号架构
- en: 'Yet the scaling hypothesis faces a key challenge: current transformers excel
    at correlation but struggle with causation. When ChatGPT explains why planes fly,
    it reproduces patterns from training data rather than understanding aerodynamic
    principles. This limitation motivates the second paradigm.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管规模假设面临一个关键挑战：当前变压器在相关性方面表现出色，但在因果关系方面却力不从心。当ChatGPT解释为什么飞机能飞时，它只是从训练数据中复制模式，而不是理解空气动力学原理。这种局限性促使第二个范式产生。
- en: Hybrid neurosymbolic systems combine neural networks for perception and pattern
    recognition with symbolic engines for reasoning and planning. This approach argues
    that pure scaling cannot achieve AGI because statistical learning differs from
    logical reasoning ([Marcus 2020](ch058.xhtml#ref-marcus2020next)). Where neural
    networks excel at pattern matching across high dimensional spaces, symbolic systems
    provide verifiable logical inference, constraint satisfaction, and causal reasoning
    through explicit rule manipulation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 混合神经符号系统结合了用于感知和模式识别的神经网络与用于推理和规划的符号引擎。这种方法认为纯粹的规模扩展无法实现通用人工智能（AGI），因为统计学习与逻辑推理不同（[Marcus
    2020](ch058.xhtml#ref-marcus2020next)）。在神经网络擅长于高维空间中的模式匹配时，符号系统通过显式规则操作提供可验证的逻辑推理、约束满足和因果推理。
- en: 'AlphaGeometry ([Trinh et al. 2024](ch058.xhtml#ref-alphageometry2024)) exemplifies
    this integration through complementary strengths. The neural component, a transformer
    trained on 100 million synthetic geometry problems, learns to suggest promising
    construction steps (adding auxiliary lines, identifying similar triangles) that
    would advance toward a proof. The symbolic component, a deduction engine implementing
    classical geometry axioms, rigorously verifies each suggested step and systematically
    explores logical consequences. This division of labor mirrors human mathematical
    reasoning: intuition suggests promising directions while formal logic validates
    correctness. The system solved 25 of 30 International Mathematical Olympiad geometry
    problems, matching the performance of an average gold medalist while producing
    human readable proofs verifiable through symbolic rules.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGeometry ([Trinh et al. 2024](ch058.xhtml#ref-alphageometry2024)) 通过互补优势体现了这种整合。神经网络部分，一个在1亿个合成几何问题上训练的变压器，学会了提出有希望的构建步骤（添加辅助线、识别相似三角形），这些步骤将有助于证明。符号部分，一个实现经典几何公理的演绎引擎，严格验证每个建议的步骤并系统地探索逻辑后果。这种劳动分工反映了人类的数学推理：直觉提出有希望的方向，而形式逻辑验证正确性。该系统解决了30个国际数学奥林匹克几何问题中的25个，其表现与平均金牌得主相当，同时产生了通过符号规则可验证的人类可读证明。
- en: 'Engineering neurosymbolic systems requires reconciling two computational paradigms.
    Neural components operate on continuous representations optimized through gradient
    descent, while symbolic components manipulate discrete symbols through logical
    inference. The integration challenge spans multiple levels: representation alignment
    (mapping between vector embeddings and symbolic structures), computation coordination
    (scheduling GPU-optimized neural operations alongside CPU-based symbolic reasoning),
    and learning synchronization (backpropagating through non-differentiable symbolic
    operations). Framework infrastructure from [Chapter 7](ch013.xhtml#sec-ai-frameworks)
    must evolve to support these heterogeneous computations within unified training
    loops.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 工程神经符号系统需要调和两种计算范式。神经网络部分在通过梯度下降优化的连续表示上操作，而符号部分通过逻辑推理操作离散符号。整合挑战跨越多个层面：表示对齐（向量嵌入与符号结构之间的映射）、计算协调（在基于GPU优化的神经网络操作与基于CPU的符号推理之间进行调度）和学习同步（通过非可微分的符号操作进行反向传播）。第七章中的框架基础设施必须发展以支持这些异构计算在统一的训练循环中。
- en: Embodied Intelligence
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 具身智能
- en: Both scaling and neurosymbolic approaches assume intelligence can emerge from
    disembodied computation. The third paradigm challenges this assumption, arguing
    that genuine intelligence requires physical grounding in the world. This perspective
    emerged from robotics research observing that even simple insects navigating complex
    terrain demonstrate behaviors that pure symbolic reasoning struggles to replicate,
    suggesting sensorimotor coupling provides fundamental scaffolding for intelligence.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 规模和神经符号方法都假设智能可以从不具身的计算中产生。第三个范式挑战了这个假设，认为真正的智能需要在世界中具有物理基础。这种观点源于机器人研究，观察到即使是简单的昆虫在复杂地形中导航也表现出纯符号推理难以复制的行为，这表明感觉运动耦合为智能提供了基本支撑。
- en: The embodied intelligence paradigm, rooted in Brooks’ subsumption architecture
    ([Brooks 1986](ch058.xhtml#ref-brooks1986robust)) and Pfeifer’s morphological
    computation ([Pfeifer and Bongard 2006](ch058.xhtml#ref-pfeifer2007body)), contends
    that intelligence requires sensorimotor grounding through continuous perception-action
    loops. Abstract reasoning, this view holds, emerges from physical interaction
    rather than disembodied computation. Consider how humans learn “heavy” not through
    verbal definition but through physical experience lifting objects, developing
    intuitive physics through embodied interaction. Language models can recite that
    “rocks are heavier than feathers” without understanding weight through sensorimotor
    experience, potentially limiting their reasoning about physical scenarios.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 具身智能范式，根植于Brooks的吸收架构([Brooks 1986](ch058.xhtml#ref-brooks1986robust))和Pfeifer的形态计算([Pfeifer和Bongard
    2006](ch058.xhtml#ref-pfeifer2007body))，主张智能需要通过连续的感知-动作循环进行传感器-运动定位。这种观点认为，抽象推理是从物理交互中产生的，而不是从无身体的计算中产生的。考虑人类如何通过举起物体而不是通过口头定义来学习“重”，通过具身交互发展直观物理学。语言模型可以背诵“石头比羽毛重”，但没有通过传感器-运动体验理解重量，这可能会限制它们对物理场景的推理。
- en: 'RT-2 (Robotics Transformer 2) ([Brohan et al. 2023](ch058.xhtml#ref-rt2023robotics))
    demonstrates early progress bridging this gap through vision-language-action models.
    By fine-tuning PaLM-E, a 562B parameter vision-language model, on robotic manipulation
    datasets containing millions of robot trajectories, RT-2 achieves 62% success
    on novel tasks compared to 32% for vision-only baselines. Critically, it transfers
    internet-scale knowledge to physical tasks: when shown a picture of an extinct
    animal and asked to “pick up the extinct animal,” it correctly identifies and
    grasps a toy dinosaur, demonstrating semantic understanding grounded in physical
    capability. The architecture processes images through a visual encoder, concatenates
    with language instructions, and outputs discretized robot actions (joint positions,
    gripper states) that the control system executes. This end-to-end learning from
    pixels to actions represents a departure from traditional robotics pipelines separating
    perception, planning, and control into distinct modules.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: RT-2（机器人变压器2）([Brohan等人2023](ch058.xhtml#ref-rt2023robotics))通过视觉-语言-动作模型展示了弥合这一差距的早期进展。通过在包含数百万个机器人轨迹的机器人操作数据集上微调PaLM-E，一个562B参数的视觉-语言模型，RT-2在新型任务上实现了62%的成功率，而仅视觉基线为32%。关键的是，它将互联网规模的知识转移到物理任务中：当展示一张灭绝动物的图片并要求“拿起灭绝动物”时，它正确地识别并抓起了一个玩具恐龙，展示了基于物理能力的语义理解。该架构通过视觉编码器处理图像，与语言指令连接，并输出离散的机器人动作（关节位置，夹爪状态），控制系统执行这些动作。从像素到动作的端到端学习代表了从传统机器人管道中分离感知、规划和控制为不同模块的一种转变。
- en: 'Embodied systems face unique engineering constraints absent in purely digital
    intelligence, creating a challenging design space. Real-time control loops demand
    sub-100 ms inference latency for stable manipulation, requiring on-device deployment
    from [Chapter 14](ch020.xhtml#sec-ondevice-learning) rather than cloud inference
    where network round-trip latency alone exceeds control budgets. The control hierarchy
    operates at multiple timescales: high-level task planning (10-100 Hz, “grasp the
    cup”), mid-level motion planning (100-1000 Hz, trajectory generation), and low-level
    control (1000+ Hz, motor commands with proprioceptive feedback). Each layer must
    complete inference within its cycle time while maintaining safety constraints
    that prevent self-collision, workspace violations, or excessive forces that could
    damage objects or injure humans.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 具身系统面临独特的工程约束，这在纯数字智能中是不存在的，从而形成了一个具有挑战性的设计空间。实时控制回路要求在100毫秒以下的推理延迟，以实现稳定的操作，需要从[第14章](ch020.xhtml#sec-ondevice-learning)在设备上部署，而不是在云端进行推理，因为网络往返延迟本身就超过了控制预算。控制层次在多个时间尺度上运行：高级任务规划（10-100赫兹，“抓住杯子”），中级运动规划（100-1000赫兹，轨迹生成），以及低级控制（1000+赫兹，带有本体感觉反馈的电机命令）。每一层必须在它的周期时间内完成推理，同时保持安全约束，以防止自碰撞、工作空间违规或可能损坏物体或伤害人类的过度力量。
- en: 'Power constraints impose severe limitations compared to datacenter systems.
    A mobile robot operates on 100-500 W total power budget (batteries, actuators,
    sensors, computation) versus a datacenter’s megawatts for model inference alone.
    Boston Dynamics’ Atlas humanoid robot dedicates approximately 1 kW to hydraulic
    actuation and 100-200W to onboard computation, forcing aggressive model compression
    and efficient architectures. This drives neuromorphic computing interest: Intel’s
    Loihi ([Mike Davies et al. 2018](ch058.xhtml#ref-davies2018loihi)) processes visual
    attention tasks at 1000× lower power than GPUs, making it viable for battery-powered
    systems. The power-performance trade-off becomes critical: running a 7B parameter
    model at 10 Hz for real-time inference requires 50-100W on mobile GPUs, consuming
    substantial battery capacity that reduces operational time from hours to minutes.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 功率限制与数据中心系统相比施加了严重的限制。一个移动机器人的总功率预算为100-500瓦（电池、执行器、传感器、计算）与数据中心仅用于模型推理的兆瓦相比。波士顿动力学的Atlas类人机器人将大约1千瓦用于液压执行，100-200瓦用于机载计算，迫使模型压缩和高效架构。这推动了神经形态计算的兴趣：英特尔的Loihi（[Mike
    Davies等人 2018](ch058.xhtml#ref-davies2018loihi)）在1000倍低于GPU的功率下处理视觉注意力任务，使其适用于电池供电系统。功率性能权衡变得至关重要：以10赫兹的频率运行7B参数模型进行实时推理，在移动GPU上需要50-100瓦，消耗大量电池容量，将操作时间从小时减少到分钟。
- en: 'Safety-critical operation necessitates formal verification methods beyond the
    statistical guarantees of pure learning systems. When Tesla’s Full Self-Driving
    operates on public roads or surgical robots manipulate near vital organs, probabilistic
    “probably safe most of the time” proves insufficient. Embodied AGI requires certified
    behavior: provable bounds on states the system can enter, guaranteed response
    times for emergency stops, and verified fallback behaviors when learning-based
    components fail. This motivates hybrid architectures combining learned policies
    for nominal operation with hard-coded safety controllers that activate on anomaly
    detection, verified through formal methods proving the combined system satisfies
    safety specifications. The verification challenge intensifies with learning: continual
    adaptation from experience must preserve safety properties even as policies evolve.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 安全关键操作需要超出纯学习系统统计保证的正式验证方法。当特斯拉的全自动驾驶在公共道路上运行或手术机器人操作接近生命器官时，概率性的“大多数时候可能是安全的”证明是不够的。具身AGI需要经过认证的行为：系统可以进入的状态的可证明界限，紧急停止的保证响应时间，以及当基于学习的组件失败时的验证回退行为。这促使混合架构结合了用于正常操作的学习策略和硬编码的安全控制器，后者在异常检测时激活，并通过形式方法验证，证明组合系统满足安全规范。随着学习的进行，验证挑战加剧：从经验中持续的适应必须保持安全属性，即使策略在演变。
- en: These constraints, while daunting, may prove advantageous for AGI development.
    Biological intelligence evolved under similar limitations, achieving remarkable
    efficiency through sensorimotor grounding. Efficient AGI might emerge from resource-constrained
    embodied systems rather than datacenter-scale models, with physical interaction
    providing the inductive bias necessary for sample-efficient learning. The embodiment
    hypothesis suggests that intelligence fundamentally arises from agents acting
    in environments under resource constraints, making embodied approaches not just
    one path to AGI but potentially a necessary component of any truly general intelligence.
    For compound systems, this suggests integrating embodied components that handle
    physical reasoning, grounding abstract concepts in sensorimotor experience even
    within predominantly digital architectures.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这些限制虽然令人畏惧，但可能对AGI（通用人工智能）的发展有利。生物智能在类似的限制下进化，通过感觉运动基础实现了显著的效率。高效的AGI可能从资源受限的具身系统中产生，而不是数据中心规模的模式，物理交互提供了必要的归纳偏差，以实现样本高效的学习。具身假设表明，智能从根本上源于在资源受限的环境中行动的智能体，这使得具身方法不仅是通往AGI的一条路径，而且可能是任何真正通用智能的必要组成部分。对于复合系统，这表明需要整合处理物理推理的具身组件，即使在以数字架构为主的情况下，也要将抽象概念扎根于感觉运动经验中。
- en: Multi-Agent Systems and Emergent Intelligence
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多智能体系统和涌现智能
- en: The fourth paradigm challenges the assumption that intelligence must reside
    within a single entity. Multi-agent approaches posit that AGI will emerge from
    interactions between multiple specialized agents, each with distinct capabilities,
    operating within shared environments. This perspective draws inspiration from
    biological systems, where ant colonies, bee hives, and human societies demonstrate
    collective intelligence exceeding individual capabilities. No single ant comprehends
    the colony’s architectural plans, yet coordinated local interactions produce sophisticated
    nest structures.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第四范式挑战了智能必须存在于单一实体中的假设。多智能体方法认为，通用人工智能将来自多个专业智能体之间的交互，每个智能体都有独特的功能，在共享环境中操作。这种观点从生物系统中汲取灵感，例如蚂蚁群体、蜂巢和人类社会展示了超越个体能力的集体智慧。没有一只蚂蚁能理解群体的建筑计划，但协调的局部互动产生了复杂的巢穴结构。
- en: 'OpenAI’s hide-and-seek agents ([Baker et al. 2019](ch058.xhtml#ref-baker2019emergent))
    demonstrated how competition drives emergent complexity without explicit programming.
    Hider agents learned to build fortresses using movable blocks, prompting seeker
    agents to discover tool use, pushing boxes to climb walls. This sparked an arms
    race: hiders learned to lock tools away, seekers learned to exploit physics glitches.
    Each capability emerged purely from competitive pressure, not human specification,
    suggesting that multi-agent interaction could bootstrap increasingly sophisticated
    behaviors toward general intelligence.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的捉迷藏智能体（[Baker等人，2019](ch058.xhtml#ref-baker2019emergent)）展示了竞争如何在没有明确编程的情况下驱动涌现的复杂性。隐藏智能体学会了使用可移动的方块建造堡垒，促使寻找智能体发现工具的使用，推动箱子爬墙。这引发了一场军备竞赛：隐藏智能体学会了将工具锁起来，寻找智能体学会了利用物理漏洞。每种能力纯粹是从竞争压力中产生的，而不是人类的指定，这表明多智能体交互可以启动越来越复杂的行为，朝着通用智能发展。
- en: From a systems engineering perspective, multi-agent AGI introduces challenges
    reminiscent of distributed computing but with fundamental differences. Like distributed
    systems, multi-agent architectures require robust communication protocols, consensus
    mechanisms, and fault tolerance from [Chapter 13](ch019.xhtml#sec-ml-operations).
    However, where traditional distributed systems coordinate identical nodes executing
    predetermined algorithms, AGI agents must coordinate heterogeneous reasoning processes,
    resolve conflicting world models, and align divergent objectives. Projects like
    AutoGPT ([Richards et al. 2023](ch058.xhtml#ref-autogpt2023)) demonstrate early
    autonomous agent capabilities, orchestrating web searches, code execution, and
    tool use to accomplish complex tasks, though current implementations remain limited
    by context window constraints and error accumulation across multi-step plans.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统工程的角度来看，多智能体通用人工智能引入了类似于分布式计算但具有根本差异的挑战。就像分布式系统一样，多智能体架构需要健壮的通信协议、共识机制和容错性，如第13章所述。[第13章](ch019.xhtml#sec-ml-operations)。然而，与传统分布式系统协调执行预定算法的相同节点不同，通用人工智能智能体必须协调异构推理过程，解决冲突的世界模型，并使不同的目标保持一致。像AutoGPT（[Richards等人，2023](ch058.xhtml#ref-autogpt2023)）这样的项目展示了早期自主智能体的能力，通过协调网络搜索、代码执行和工具使用来完成复杂任务，尽管当前的实施仍然受到上下文窗口限制和多步骤计划中错误累积的限制。
- en: 'These four paradigms (scaling, neurosymbolic, embodied, and multi-agent) need
    not be mutually exclusive. Indeed, the most promising path forward may combine
    insights from each: substantial computational resources applied to hybrid architectures
    that ground abstract reasoning in physical or simulated embodiment, with multiple
    specialized agents coordinating to solve complex problems. Such convergence points
    toward compound AI systems, the architectural framework that could unite these
    paradigms into practical implementations.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这四种范式（扩展、神经符号、具身和多智能体）并不需要相互排斥。实际上，最有前景的发展道路可能结合了每个范式的见解：将大量计算资源应用于混合架构，将抽象推理建立在物理或模拟具身之上，同时多个专业智能体协调解决复杂问题。这种融合指向了复合人工智能系统，这是一个可以将这些范式统一到实际应用中的架构框架。
- en: The Compound AI Systems Framework
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复合人工智能系统框架
- en: 'The trajectory toward AGI favors “Compound AI Systems” ([Zaharia et al. 2024](ch058.xhtml#ref-berkeley2024compound)):
    multiple specialized components operating in concert rather than monolithic models.
    This architectural paradigm represents the organizing principle for understanding
    how today’s building blocks assemble into tomorrow’s intelligent systems.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 向通用人工智能（AGI）的轨迹倾向于“复合人工智能系统”([Zaharia 等人 2024](ch058.xhtml#ref-berkeley2024compound))：多个专业组件协同工作而不是单一模型。这种架构范式代表了理解今天的基本构建块如何组装成明天智能系统的组织原则。
- en: 'Modern AI assistants already demonstrate this compound architecture. ChatGPT
    integrates a language model for text generation, a code interpreter for computation,
    web search for current information, and DALL-E for image creation. Each component
    excels at its specialized task while a central orchestrator coordinates their
    interactions through several mechanisms: intent classification determines which
    components to activate based on user queries, result aggregation combines outputs
    from multiple components into coherent responses, and error handling routes failed
    operations to alternative components or triggers user clarification.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现代人工智能助手已经展示了这种复合架构。ChatGPT集成了用于文本生成的语言模型、用于计算的代码解释器、用于获取当前信息的网络搜索以及用于图像创建的DALL-E。每个组件在其专业任务上表现出色，而中央协调器通过多种机制协调它们的交互：意图分类根据用户查询确定要激活哪些组件，结果聚合将多个组件的输出组合成连贯的响应，错误处理将失败的运算路由到替代组件或触发用户澄清。
- en: 'When analyzing stock market trends, the orchestration unfolds through multiple
    stages. First, the language model parses the user request to extract key information
    (ticker symbols, time ranges, analysis types). Second, it generates API calls
    to web search for current prices and retrieves relevant financial news. Third,
    the code interpreter receives this data and executes statistical analysis through
    Python scripts, computing moving averages, volatility measures, or correlation
    analyses. Fourth, the language model synthesizes these quantitative results with
    contextual information into natural language explanations. Fifth, if the user
    requests visualizations, the system routes to code generation for matplotlib charts.
    This orchestration achieves results no single component could produce: web search
    lacks analytical capabilities, code execution cannot interpret results, and the
    language model alone cannot access real time data.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析股市趋势时，协调通过多个阶段展开。首先，语言模型解析用户请求以提取关键信息（股票代码、时间范围、分析类型）。其次，它生成API调用以网络搜索获取当前价格并检索相关财经新闻。第三，代码解释器接收这些数据并通过Python脚本执行统计分析，计算移动平均数、波动性指标或相关性分析。第四，语言模型将这些定量结果与上下文信息综合成自然语言解释。第五，如果用户请求可视化，系统将路由到代码生成以生成matplotlib图表。这种协调实现了单个组件无法产生的结果：网络搜索缺乏分析能力，代码执行无法解释结果，而语言模型单独无法访问实时数据。
- en: 'The organizational analogy illuminates this architecture. A single, monolithic
    AGI resembles attempting to have one individual perform all functions within an
    enterprise: strategy, accounting, marketing, engineering, and legal work. This
    approach neither scales nor provides specialized expertise. A compound AI system
    mirrors a well structured organization with a chief executive (the orchestrator)
    who sets strategy and delegates tasks. Specialized departments handle distinct
    functions: research libraries manage knowledge retrieval, legal teams implement
    safety and alignment filters, and engineering teams provide specialized tools
    and models. Intelligence emerges from coordinated work across these specialized
    components rather than from a single, all knowing entity.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 组织类比阐明了这种架构。一个单一、统一的AGI类似于试图让一个人在企业内部执行所有职能：战略、会计、营销、工程和法律工作。这种方法既无法扩展，也无法提供专业化的专业知识。复合人工智能系统类似于一个结构良好的组织，有一个首席执行官（协调器）制定战略并委派任务。专业部门处理不同的职能：研究图书馆管理知识检索，法律团队实施安全和一致性过滤器，工程团队提供专业工具和模型。智能是从这些专业组件的协调工作中产生的，而不是从单一的全知实体中产生的。
- en: The compound approach offers five key advantages over monolithic models. First,
    modularity enables components to update independently without full system retraining.
    When OpenAI improves code interpretation, they swap that module without touching
    the language model, similar to upgrading a graphics card without replacing the
    entire computer. Second, specialization allows each component to optimize for
    its specific task. A dedicated retrieval system using vector databases outperforms
    a language model attempting to memorize all knowledge, just as specialized ASICs
    outperform general purpose CPUs for particular computations. Third, interpretability
    emerges from traceable decision paths through component interactions. When a system
    makes an error, engineers can identify whether retrieval, reasoning, or generation
    failed, which remains impossible with opaque end to end models. Fourth, scalability
    permits new capabilities to integrate without architectural overhauls. Adding
    voice recognition or robotic control becomes a matter of adding modules rather
    than retraining trillion parameter models. Fifth, safety benefits from multiple
    specialized validators constraining outputs at each stage. A toxicity filter checks
    generated text, a factuality verifier validates claims, and a safety monitor prevents
    harmful actions. This creates layered defense rather than relying on a single
    model to behave correctly.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 复合方法相较于单体模型具有五大关键优势。首先，模块化使得组件可以独立更新而无需对整个系统进行重新训练。当OpenAI改进代码解释时，他们只需更换那个模块，而不必触及语言模型，这就像升级显卡而不更换整个电脑一样。其次，专业化允许每个组件针对其特定任务进行优化。使用向量数据库的专用检索系统比试图记住所有知识的语言模型表现更出色，就像专用ASIC比通用CPU在特定计算上表现更出色一样。第三，可解释性来自于通过组件交互的可追溯决策路径。当系统出错时，工程师可以确定是检索、推理还是生成失败，这在不可透见的端到端模型中是不可能的。第四，可扩展性允许在不进行架构大修的情况下集成新功能。添加语音识别或机器人控制只需添加模块，而不是重新训练万亿参数模型。第五，安全性得益于在每个阶段对输出进行约束的多个专用验证器。一个毒性过滤器检查生成的文本，一个事实验证器验证声明，一个安全监控器防止有害行为。这创建了一个分层防御系统，而不是依赖于单个模型正确行为。
- en: These advantages explain why every major AI lab now pursues compound architectures.
    Google’s Gemini 2.0 combines multimodal understanding with native tool use and
    agentic capabilities. Anthropic’s Claude 3.5 integrates constitutional AI components,
    computer use capabilities, and extended context windows enabling sophisticated
    multi-step workflows. OpenAI’s ChatGPT orchestrates plugins, code execution, image
    generation, and web browsing through unified interfaces. The rapid evolution of
    these systems, from single-purpose assistants to multi-capable agents, demonstrates
    that compound architecture adoption accelerates as capabilities mature. The engineering
    principles established throughout this textbook, from distributed systems to workflow
    orchestration, now converge to enable these compound systems.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优势解释了为什么现在每个主要的AI实验室都在追求复合架构。谷歌的Gemini 2.0结合了多模态理解和原生工具使用以及代理能力。Anthropic的Claude
    3.5集成了宪法AI组件、计算机使用能力以及扩展的上下文窗口，从而实现复杂的多步骤工作流程。OpenAI的ChatGPT通过统一的界面协调插件、代码执行、图像生成和网络浏览。这些系统从单一用途助手到多能力代理的快速演变表明，随着能力的成熟，复合架构的采用正在加速。本书中从分布式系统到工作流程编排建立的所有工程原则现在正汇聚起来，以使这些复合系统成为可能。
- en: Building Blocks for Compound Intelligence
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复合智能的构建模块
- en: The evolution from monolithic models to compound AI systems requires advances
    in how we engineer data, integrate components, and scale infrastructure. These
    building blocks represent the critical enablers that will determine whether compound
    intelligence can achieve the flexibility and capability needed for artificial
    general intelligence. Each component addresses specific limitations of current
    approaches while creating new engineering challenges that span data availability,
    system integration, and computational scaling.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从单体模型到复合人工智能系统的演变需要我们在数据工程、组件集成和基础设施扩展方面的进步。这些构建块代表了复合智能能否实现人工智能通用智能所需灵活性和能力的决定性推动力。每个组件都针对当前方法的特定局限性进行设计，同时创造跨越数据可用性、系统集成和计算扩展的新工程挑战。
- en: '[Figure 20.5](ch026.xhtml#fig-compound-ai-system) illustrates how these building
    blocks integrate within the compound AI architecture: specialized data engineering
    components feed content to the Knowledge Retrieval system, dynamic architectures
    enable the LLM Orchestrator to route computations efficiently through mixture-of-experts
    patterns, and advanced training paradigms power the Safety Filters that implement
    constitutional AI principles. Understanding these building blocks individually
    and their integration collectively provides the foundation for engineering tomorrow’s
    intelligent systems.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[图20.5](ch026.xhtml#fig-compound-ai-system)说明了这些构建块如何在复合人工智能架构中整合：专门的数据工程组件向知识检索系统提供内容，动态架构使LLM编排器能够通过专家混合模式有效地路由计算，而高级训练范式为实施宪法人工智能原则的安全过滤器提供动力。理解这些构建块各自的作用以及它们的整体整合，为构建明天的智能系统提供了基础。'
- en: Data Engineering at Scale
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大规模数据工程
- en: 'Data engineering represents the first and most critical building block. Compound
    AI systems require advanced data engineering to feed their specialized components,
    yet machine learning faces a data availability crisis. The scale becomes apparent
    when examining model requirements progression: GPT-3 consumed 300 billion tokens
    (OpenAI), GPT-4 likely used over 10 trillion tokens (scaling law extrapolations[4](#fn4)),
    yet research estimates suggest only 4.6-17 trillion high-quality tokens exist
    across the entire internet[5](#fn5). This progression reveals a critical bottleneck:
    at current consumption rates, traditional web-scraped text data may be exhausted
    by 2026, forcing exploration of synthetic data generation and alternative scaling
    paths ([Sevilla et al. 2022a](ch058.xhtml#ref-epoch2022compute)).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程是第一个也是最关键的构建块。复合人工智能系统需要高级数据工程来为其专门的组件提供支持，然而机器学习面临着数据可用性的危机。当检查模型需求的发展进程时，规模变得明显：GPT-3消耗了3000亿个标记（OpenAI），GPT-4可能使用了超过1000万亿个标记（根据扩展定律[4](#fn4)），然而研究估计整个互联网上只有4.6-17万亿高质量标记[5](#fn5)。这一进程揭示了一个关键瓶颈：在当前的消费速度下，传统的网络爬虫文本数据可能在2026年耗尽，迫使探索合成数据生成和替代扩展路径（[Sevilla等人2022a](ch058.xhtml#ref-epoch2022compute)）。
- en: 'Three data engineering approaches address this challenge through compound system
    design:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 三种数据工程方法通过复合系统设计来应对这一挑战：
- en: Self-Supervised Learning Components
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自监督学习组件
- en: Self-supervised learning enables compound AI systems to transcend the labeled
    data bottleneck. While supervised learning requires human annotations for every
    example, self-supervised methods extract knowledge from data structure itself
    by learning from the inherent patterns, relationships, and regularities present
    in raw information.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习使复合人工智能系统能够超越标记数据的瓶颈。虽然监督学习需要对每个例子进行人工标注，但自监督方法通过从原始信息中固有的模式、关系和规律中学习，从数据结构本身提取知识。
- en: The biological precedent is informative. Human brains process approximately
    10¹¹ bits per second of sensory input but receive fewer than 10⁴ bits per second
    of explicit feedback, meaning 99.99% of learning occurs through self-supervised
    pattern extraction[6](#fn6). A child learns object permanence not from labeled
    examples but from observing objects disappear and reappear. They grasp physics
    not from equations but from watching things fall, roll, and collide.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 生物学的先例是有启发性的。人脑每秒处理大约10¹¹位感官输入，但每秒只接收少于10⁴位的明确反馈，这意味着99.99%的学习是通过自监督模式提取[6](#fn6)进行的。孩子不是从标记的例子中学习物体恒存性，而是通过观察物体消失和再次出现。他们不是从方程式中学习物理，而是通过观察物体坠落、滚动和碰撞。
- en: Yann LeCun calls self-supervised learning the “dark matter” of intelligence
    ([Yann LeCun 2022](ch058.xhtml#ref-lecun2022path)), invisible yet constituting
    most of the learning universe. Current language models barely scratch this surface
    through next-token prediction, a primitive form that learns statistical correlations
    rather than causal understanding. When ChatGPT predicts “apple” after “red,” it
    leverages co-occurrence statistics, not an understanding that apples possess the
    property of redness.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Yann LeCun称自监督学习为“智能的暗物质”([Yann LeCun 2022](ch058.xhtml#ref-lecun2022path))，虽然看不见，但构成了学习宇宙的大部分。当前的语模模型仅仅通过下一个标记预测来触及这一表面，这是一种原始形式，它学习的是统计相关性而不是因果理解。当ChatGPT在“red”之后预测“apple”时，它利用的是共现统计，而不是苹果具有红色属性的理解。
- en: The Joint Embedding Predictive Architecture (JEPA)[7](#fn7) demonstrates a more
    sophisticated approach. Instead of predicting raw pixels or tokens, JEPA learns
    abstract representations of world states. Shown a video of a ball rolling down
    a ramp, JEPA doesn’t predict pixel values frame-by-frame. Instead, it learns representations
    encoding trajectory, momentum, and collision dynamics, concepts transferable across
    different objects and scenarios. This abstraction achieves 3× better sample efficiency
    than pixel prediction while learning genuinely reusable knowledge.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 联合嵌入预测架构（JEPA）[7](#fn7)展示了一种更复杂的方法。JEPA不是预测原始像素或标记，而是学习世界状态的抽象表示。当JEPA看到一个球沿着斜坡滚动的视频时，它不会逐帧预测像素值。相反，它学习表示编码轨迹、动量和碰撞动力学，这些概念可以跨不同对象和场景转移。这种抽象比像素预测实现了3倍的样本效率，同时学习到真正可重用的知识。
- en: 'For compound systems, self-supervised learning enables each specialized component
    to develop expertise from its natural data domain. A vision module learns from
    images, a language module from text, a dynamics module from video, all without
    manual labeling. The engineering challenge involves coordinating these diverse
    learning processes: ensuring representations align across modalities, preventing
    catastrophic forgetting when components update, and maintaining consistency as
    the system scales. Framework infrastructure from [Chapter 7](ch013.xhtml#sec-ai-frameworks)
    must evolve to support these heterogeneous self-supervised objectives within unified
    training loops.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复合系统，自监督学习使每个专业组件能够从其自然数据域中发展专业知识。视觉模块从图像中学习，语言模块从文本中学习，动力学模块从视频中学习，所有这些都不需要人工标记。工程挑战在于协调这些不同的学习过程：确保表示在模态之间对齐，防止组件更新时的灾难性遗忘，并在系统扩展时保持一致性。第七章（ch013.xhtml#sec-ai-frameworks）中的框架基础设施必须发展，以支持在统一训练循环中这些异构的自监督目标。
- en: Synthetic Data Generation
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 合成数据生成
- en: 'Compound systems generate their own training data through guided synthesis
    rather than relying solely on human-generated content. This approach seems paradoxical:
    how can models learn from themselves without degrading into model collapse, where
    generated data increasingly reflects model biases rather than ground truth? The
    answer lies in three complementary mechanisms that prevent quality degradation.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 复合系统通过引导合成而不是完全依赖人工生成的内容来生成自己的训练数据。这种方法看似矛盾：模型如何从自己那里学习而不退化成模型崩溃，即生成数据越来越多地反映模型偏差而不是真实情况？答案在于三个互补的机制，这些机制可以防止质量下降。
- en: First, verification through external ground truth constrains generation. Microsoft’s
    Phi models ([Gunasekar et al. 2023](ch058.xhtml#ref-gunasekar2023textbooks)) generate
    synthetic textbook problems but verify solutions through symbolic execution, mathematical
    proof checkers, or code compilation. A generated algebra problem must have a unique,
    verifiable solution; a programming exercise must compile and pass test cases.
    This creates a feedback loop where generators learn to produce not merely plausible
    examples but verifiable correct ones.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过外部真实数据进行验证限制了生成。微软的Phi模型（[Gunasekar等人2023](ch058.xhtml#ref-gunasekar2023textbooks)）生成合成教科书问题，但通过符号执行、数学证明检查器或代码编译来验证解决方案。生成的代数问题必须有一个唯一、可验证的解；编程练习必须编译并通过测试用例。这创建了一个反馈循环，其中生成器学习产生不仅可能是合理的例子，而且是可验证的正确例子。
- en: 'Second, curriculum-based synthesis starts with simple, tractable examples and
    progressively increases complexity. Phi-2 (2.7B parameters) matches GPT-3.5 (175B)
    performance because its synthetic training data follows pedagogical progression:
    basic arithmetic before calculus, simple functions before recursion, concrete
    examples before abstract reasoning. This structured curriculum enables smaller
    models to achieve capabilities requiring 65× more parameters when trained on unstructured
    web data.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，基于课程的合成从简单、易于处理的例子开始，并逐步增加复杂性。Phi-2（2.7B个参数）的性能与GPT-3.5（175B）相当，因为其合成的训练数据遵循教学进程：先从基础算术开始，然后是微积分，再是简单函数，然后是递归，最后是具体例子，最后是抽象推理。这种结构化的课程使得在无结构网络数据上训练时，较小的模型能够实现需要65倍更多参数的能力。
- en: 'Third, ensemble verification uses multiple independent models to filter synthetic
    data. When generating training examples, outputs must satisfy multiple distinct
    critic models trained on different data distributions. This prevents systematic
    biases: if one generator consistently produces examples favoring particular patterns,
    ensemble critics trained on diverse data will identify and reject these biased
    samples. Anthropic’s Constitutional AI demonstrates this through iterative refinement:
    one component generates responses, multiple critics evaluate them against different
    principles (helpfulness, harmlessness, factual accuracy), and synthesis produces
    improved versions satisfying all criteria simultaneously.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，集成验证使用多个独立的模型来过滤合成数据。在生成训练示例时，输出必须满足多个不同的、基于不同数据分布训练的评论员模型。这防止了系统偏差：如果一个生成器持续产生有利于特定模式的示例，那么在多样化数据上训练的集成评论员将识别并拒绝这些有偏差的样本。Anthropic的宪法AI通过迭代优化展示了这一点：一个组件生成响应，多个评论员根据不同的原则（有用性、无害性、事实准确性）对其进行评估，综合产生满足所有标准的改进版本。
- en: For compound systems, this enables specialized data generation components that
    create domain-specific training examples calibrated to other component needs.
    A reasoning component might generate step by step solutions for a verification
    component to check, while a code generation component produces programs for an
    execution component to validate.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复合系统，这使专门的数据生成组件能够创建针对其他组件需求的特定领域训练示例。一个推理组件可能为验证组件生成逐步解决方案以进行检查，而代码生成组件为执行组件生成程序以进行验证。
- en: Self-Play Components
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自我玩耍组件
- en: 'AlphaGo Zero ([Silver et al. 2017](ch058.xhtml#ref-silver2017mastering)) demonstrated
    a key principle for compound systems: components can bootstrap expertise through
    self-competition without human data. Starting from completely random play, it
    achieved superhuman Go performance in 72 hours purely through self-play reinforcement
    learning. The mechanism relies on three technical elements that enable bootstrapping
    from zero knowledge.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo Zero（[Silver等人2017](ch058.xhtml#ref-silver2017mastering)）展示了复合系统的一个关键原则：组件可以通过自我竞争在没有人类数据的情况下建立专业知识。从完全随机的游戏开始，它仅通过自我玩耍的强化学习在72小时内实现了超人类的围棋表现。该机制依赖于三个技术元素，这些元素能够从零知识开始建立专业知识。
- en: First, self-play provides automatic curriculum adaptation through opponent strength
    tracking. Unlike supervised learning with fixed datasets, self-play continuously
    adjusts difficulty as both competing agents improve. When AlphaGo Zero plays against
    itself, each game reflects current skill level, creating training examples calibrated
    to just beyond current capabilities. Early games explore basic patterns; later
    games reveal subtle tactical nuances impossible to specify through human instruction.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第一，自我玩耍通过对手强度跟踪提供自动课程适应性。与使用固定数据集的监督学习不同，自我玩耍随着竞争代理的改进而持续调整难度。当AlphaGo Zero与自己对战时，每场比赛都反映了当前的技能水平，创建了定位于略高于当前能力的训练示例。早期游戏探索基本模式；后期游戏揭示了通过人类指令无法指定的微妙战术细节。
- en: 'Second, search-guided exploration expands the effective training distribution
    beyond what current policy can generate. Monte Carlo Tree Search simulates thousands
    of possible futures from each position, discovering strong moves the current policy
    would not consider. These search-enhanced decisions become training targets, pulling
    policy toward superhuman play through iterative improvement. This creates a virtuous
    cycle: better policy enables more accurate search, which discovers better training
    targets, which improve policy further.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，搜索引导的探索扩展了有效训练分布，使其超越当前策略可以生成的范围。蒙特卡洛树搜索从每个位置模拟成千上万种可能的结果，发现当前策略不会考虑的强力走法。这些搜索增强的决策成为训练目标，通过迭代改进将策略推向超人类水平。这创造了一个良性循环：更好的策略使搜索更准确，发现更好的训练目标，进而进一步提高策略。
- en: Third, outcome verification provides unambiguous learning signals. Game outcomes
    (win/loss in Go, solution correctness in coding, debate victory in reasoning)
    offer clear supervision without human annotation. A model that generates code
    can test millions of candidate programs against test suites, learning from successes
    and failures without human evaluation. DeepMind’s AlphaCode generates over one
    million programs per competition problem, filtering through compilation errors
    and test failures to identify correct solutions, thereby learning both from successful
    programs (positive examples) and systematic failure patterns (negative examples).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，结果验证提供了明确的 学习信号。游戏结果（围棋中的胜负、编码中的解决方案正确性、推理中的辩论胜利）提供了清晰的监督，无需人工标注。一个生成代码的模型可以对测试套件中的数百万个候选程序进行测试，从成功和失败中学习，而无需人工评估。DeepMind的AlphaCode针对每个比赛问题生成超过一百万个程序，通过编译错误和测试失败进行筛选，以识别正确解决方案，从而从成功的程序（正例）和系统性的失败模式（负例）中学习。
- en: This principle extends beyond games to create specialized system components
    for compound architectures. OpenAI’s debate models argue opposing sides of questions,
    with a judge model determining which argument better supports truth, creating
    training data for both argumentation and evaluation. Anthropic’s models critique
    their own outputs through self-generated critiques evaluated for quality, bootstrapping
    improved responses. These self-play patterns enable compound systems to generate
    domain-specific training data without expensive human supervision.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这一原则不仅适用于游戏，还可以为复合架构创建专门的系统组件。OpenAI的辩论模型争论问题的对立面，由裁判模型决定哪个论点更好地支持真理，为论点和评估创建训练数据。Anthropic的模型通过自我生成的、经过质量评估的评论来批判自己的输出，从而启动改进的响应。这些自播放模式使复合系统能够在无需昂贵的人类监督的情况下生成特定领域的训练数据。
- en: 'Implementing this approach in compound systems requires data pipelines handling
    dynamic generation at scale: managing continuous streams of self-generated examples,
    filtering for quality through automated verification, and preventing mode collapse
    through diversity metrics. The engineering challenge involves orchestrating multiple
    self-playing components while maintaining exploration diversity and preventing
    system-wide convergence to suboptimal patterns or adversarial equilibria.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在复合系统中实施这种方法需要处理大规模动态生成的数据管道：管理连续的自生成示例流，通过自动化验证进行质量筛选，并通过多样性指标防止模式崩溃。工程挑战在于在保持探索多样性并防止系统整体收敛到次优模式或对抗性均衡的同时，协调多个自播放组件。
- en: Web-Scale Data Processing
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Web-Scale Data Processing
- en: 'High-quality curated text may be limited, but self-supervised learning, synthetic
    generation, and self-play create new data sources. The internet’s long tail contains
    untapped resources for compound systems: GitHub repositories, academic papers,
    technical documentation, and specialized forums. Common Crawl contains 250 billion
    pages, GitHub hosts 200M+ repositories, arXiv contains 2M+ papers, and Reddit
    has 3B+ comments, combining to over 100 trillion tokens of varied quality. The
    challenge lies in extraction and quality assessment rather than availability.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 高质量精选文本可能有限，但自监督学习、合成生成和自播放创造了新的数据来源。互联网的长尾包含复合系统未开发的资源：GitHub仓库、学术论文、技术文档和专门论坛。Common
    Crawl包含2500亿页，GitHub托管2000万多个仓库，arXiv包含200万多篇论文，Reddit有30亿条评论，合计超过1000万亿个不同质量的标记。挑战在于提取和质量评估，而不是可用性。
- en: 'Modern compound systems employ sophisticated filtering pipelines ([Figure 20.1](ch026.xhtml#fig-frontier-data-pipeline))
    where specialized components handle different aspects: deduplication removes 30-60%
    redundancy in web crawls, quality classifiers trained on curated data identify
    high-value content, and domain-specific extractors process code, mathematics,
    and scientific text. This processing intensity exemplifies the data engineering
    challenge: GPT-4’s training likely processed over 100 trillion raw tokens to extract
    10-13 trillion training tokens, representing approximately 90% total data reduction:
    30% from deduplication, then 80-90% of remaining data from quality filtering.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现代复合系统采用复杂的过滤管道（[图20.1](ch026.xhtml#fig-frontier-data-pipeline)），其中专门的组件处理不同的方面：去重减少了网络爬取中的30-60%冗余，基于精选数据的质量分类器识别高价值内容，特定领域的提取器处理代码、数学和科学文本。这种处理强度体现了数据工程挑战：GPT-4的训练可能处理了超过1000万亿个原始标记，以提取10-13万亿个训练标记，这代表了大约90%的总数据减少：30%来自去重，然后是剩余数据的80-90%来自质量筛选。
- en: This represents a shift from batch processing to continuous, adaptive data curation
    where multiple specialized components work together to transform raw internet
    data into training-ready content.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表了从批量处理到连续、自适应数据管理的转变，其中多个专业组件协同工作，将原始互联网数据转换为训练准备好的内容。
- en: '![](../media/file321.svg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file321.svg)'
- en: 'Figure 20.1: **Data Engineering Pipeline for Frontier Models**: The multi-stage
    pipeline transforms 100+ trillion raw tokens into 10-13 trillion high-quality
    training tokens. Each stage applies increasingly sophisticated filtering, with
    synthetic generation augmenting the final dataset. This pipeline represents the
    evolution from simple web scraping to intelligent data curation systems.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.1：**前沿模型的数据工程流程**：多阶段流程将100+万亿个原始标记转换为10-13万亿个高质量训练标记。每个阶段应用越来越复杂的过滤，合成生成增强了最终数据集。这个流程代表了从简单的网络抓取到智能数据管理系统的演变。
- en: 'The pipeline in [Figure 20.1](ch026.xhtml#fig-frontier-data-pipeline) reveals
    an important insight: the bottleneck isn’t data availability but processing capacity.
    Starting with 111.5 trillion raw tokens, aggressive filtering reduces this to
    just 10-13 trillion training tokens, with over 90% of data discarded. For ML engineers,
    this means that improving filter quality could be more impactful than gathering
    more raw data. A 10% improvement in the quality filter’s precision could yield
    an extra trillion high-quality tokens, equivalent to doubling the amount of books
    available.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[图20.1](ch026.xhtml#fig-frontier-data-pipeline)中的流程图揭示了一个重要的见解：瓶颈不是数据可用性，而是处理能力。从111.5万亿个原始标记开始，积极的过滤将这一数字减少到仅10-13万亿个训练标记，超过90%的数据被丢弃。对于机器学习工程师来说，这意味着提高过滤器的质量可能比收集更多原始数据更有影响。质量过滤器精确度提高10%可以额外产生一个万亿个高质量标记，相当于将可用的书籍数量翻倍。'
- en: These data engineering approaches (synthetic generation, self-play, and advanced
    harvesting) represent the first building block of compound AI systems. They transform
    data limitations from barriers into opportunities for innovation, with specialized
    components generating, filtering, and processing data streams continuously.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据工程方法（合成生成、自我对弈和高级采集）代表了复合人工智能系统的第一个构建块。它们将数据限制从障碍转变为创新的机遇，专用组件持续生成、过滤和处理数据流。
- en: Generating high-quality training data only addresses part of the compound systems
    challenge. The next building block involves architectural innovations that enable
    efficient computation across specialized components while maintaining system coherence.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 仅生成高质量的训练数据只解决了复合系统挑战的一部分。下一个构建块涉及架构创新，这些创新能够使专用组件在保持系统连贯性的同时进行高效计算。
- en: Dynamic Architectures for Compound Systems
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 复合系统的动态架构
- en: Compound systems require dynamic approaches that can adapt computation based
    on task requirements and input characteristics. This section explores architectural
    innovations that enable efficient specialization through selective computation
    and sophisticated routing mechanisms. Mixture of experts and similar approaches
    allow systems to activate only relevant components for each task, improving computational
    efficiency while maintaining system capability.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 复合系统需要动态方法，可以根据任务需求和输入特征调整计算。本节探讨了通过选择性计算和复杂的路由机制实现高效专业化的架构创新。专家混合和类似方法允许系统仅激活每个任务相关的组件，提高计算效率同时保持系统能力。
- en: Specialization Through Selective Computation
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过选择性计算实现专业化
- en: 'Compound systems face a fundamental efficiency challenge: not all components
    need to activate for every task. A mathematics question requires different processing
    than language translation or code generation, yet dense monolithic models activate
    all parameters for every input regardless of task requirements.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 复合系统面临一个基本的效率挑战：不是所有组件都需要在每个任务中激活。数学问题需要与语言翻译或代码生成不同的处理，然而密集的单体模型无论任务需求如何，都会为每个输入激活所有参数。
- en: 'Consider GPT-3 ([T. Brown et al. 2020](ch058.xhtml#ref-brown2020language))
    processing the prompt “What is 2+2?”. All 175 billion parameters activate despite
    this requiring only arithmetic reasoning, not language translation, code generation,
    or commonsense reasoning. This activation requires 350GB memory and 350 GFLOPs
    per token of forward pass computation. Activation analysis through gradient attribution
    reveals that only 10-20% of parameters contribute meaningfully to any given prediction,
    suggesting 80-90% computational waste for typical inputs. The situation worsens
    at scale: a hypothetical 1 trillion parameter dense model would require 2TB memory
    and 2 TFLOPs per token, with similar utilization inefficiency.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑GPT-3 ([T. Brown et al. 2020](ch058.xhtml#ref-brown2020language))处理提示“2+2等于多少？”。尽管这只需要算术推理，而不是语言翻译、代码生成或常识推理，但所有1750亿个参数都会被激活。这种激活需要每条前向传递计算350GB内存和350
    GFLOPs。通过梯度归因的激活分析揭示，只有10-20%的参数对任何给定预测有实质性贡献，这表明对于典型输入有80-90%的计算浪费。在规模扩大时，情况变得更糟：一个假设的1万亿参数密集模型将需要每条2TB内存和每条2
    TFLOPs，具有类似的利用效率低下。
- en: This inefficiency compounds across three dimensions. Memory bandwidth limits
    how quickly parameters load from HBM to compute units, creating bottlenecks even
    when compute units sit idle. Power consumption scales with activated parameters
    regardless of contribution, burning energy for computations that minimally influence
    outputs. Latency increases linearly with model size for dense architectures, making
    real-time applications infeasible beyond certain scales.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种低效在三个维度上累积。内存带宽限制参数从HBM加载到计算单元的速度，即使在计算单元空闲时也会造成瓶颈。功耗与激活的参数成正比，无论其贡献如何，都会为对输出影响最小的计算消耗能量。对于密集架构，延迟与模型大小成线性关系，使得超过一定规模后实时应用变得不可行。
- en: 'The biological precedent suggests alternative approaches. The human brain contains
    approximately 86 billion neurons but does not activate all for every task. Visual
    processing primarily engages occipital cortex, language engages temporal regions,
    and motor control engages frontal areas. This sparse, task-specific activation
    enables energy efficiency: the brain operates on 20 watts despite complexity rivaling
    trillion parameter models in connectivity density.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 生物先例表明了替代方法。人脑大约含有860亿个神经元，但并非每个任务都会激活所有神经元。视觉处理主要涉及枕叶皮层，语言处理涉及颞叶区域，运动控制涉及额叶区域。这种稀疏的、特定于任务的激活实现了能源效率：尽管其复杂性可与连接密度达到万亿参数模型的复杂度相媲美，但大脑在20瓦的功率下运行。
- en: These observations motivate architectural designs enabling selective activation
    of system components. Rather than activating all parameters, compound systems
    should route inputs to relevant specialized components, activating only the subset
    necessary for each specific task. This selective computation promises order of
    magnitude improvements in efficiency, latency, and scalability.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观察结果激励了能够实现系统组件选择性激活的架构设计。复合系统不应激活所有参数，而应将输入路由到相关的专业组件，仅激活每个特定任务所需的子集。这种选择性计算有望在效率、延迟和可扩展性方面实现数量级的改进。
- en: Expert Routing in Compound Systems
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 复合系统中的专家路由
- en: 'The Mixture of Experts (MoE) architecture ([Fedus, Zoph, and Shazeer 2021b](ch058.xhtml#ref-fedus2022switch))
    demonstrates the compound systems principle at the model level: specialized components
    activated through intelligent routing. Rather than processing every input through
    all parameters, MoE models consist of multiple expert networks, each specializing
    in different problem types. A routing mechanism (learned gating function) determines
    which experts process each input, as illustrated in [Figure 20.2](ch026.xhtml#fig-moe-routing).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 混合专家（MoE）架构([Fedus, Zoph, and Shazeer 2021b](ch058.xhtml#ref-fedus2022switch))在模型级别上展示了复合系统原理：通过智能路由激活的专业组件。MoE模型由多个专家网络组成，每个网络专门处理不同类型的问题。一个路由机制（学习到的门控函数）确定哪个专家处理每个输入，如图[图20.2](ch026.xhtml#fig-moe-routing)所示。
- en: The router computes probabilities for each expert using learned linear transformations
    followed by softmax, typically selecting the top-2 experts per token. Load balancing
    losses ensure uniform expert utilization to prevent collapse to few specialists.
    This pattern extends naturally to compound systems where different models, tools,
    or processing pipelines are routed based on input characteristics.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 路由器使用学习到的线性变换后跟 softmax 计算每个专家的概率，通常每个标记选择前两个专家。负载均衡损失确保专家的均匀使用，以防止收敛到少数专家。这种模式自然扩展到复合系统，其中不同的模型、工具或处理管道根据输入特征进行路由。
- en: As shown in [Figure 20.2](ch026.xhtml#fig-moe-routing), when a token enters
    the system, the router evaluates which experts are most relevant. For “2+2=”,
    the router assigns high weights (0.7) to arithmetic specialists while giving zero
    weight to vision or language experts. For “Bonjour means”, it activates translation
    experts instead. GPT-4 ([OpenAI et al. 2023](ch058.xhtml#ref-openai2023gpt4))
    is rumored to use eight expert models of approximately 220B parameters each (unconfirmed
    by OpenAI), activating only two per token, reducing active computation to 280B
    parameters while maintaining 1.8T total capacity with 5-7x inference speedup.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 20.2](ch026.xhtml#fig-moe-routing) 所示，当标记进入系统时，路由器评估哪些专家最相关。对于“2+2=”，路由器将高权重（0.7）分配给算术专家，而对视觉或语言专家则给予零权重。对于“Bonjour
    means”，它激活了翻译专家。据传言，GPT-4 ([OpenAI 等人 2023](ch058.xhtml#ref-openai2023gpt4)) 使用了大约
    220B 参数的八个专家模型（由 OpenAI 未确认），每个标记激活两个，将活跃计算减少到 280B 参数，同时保持 1.8T 总容量，并实现 5-7 倍的推理速度提升。
- en: 'This introduces systems challenges: load balancing across experts, preventing
    collapse where all routing converges to few experts, and managing irregular memory
    access patterns. For compound systems, these same challenges apply to routing
    between different models, databases, and processing pipelines, requiring sophisticated
    orchestration infrastructure.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这引入了系统挑战：专家之间的负载均衡，防止所有路由都收敛到少数专家，以及管理不规则的内存访问模式。对于复合系统，这些相同的挑战也适用于不同模型、数据库和处理管道之间的路由，需要复杂的编排基础设施。
- en: '![](../media/file322.svg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file322.svg)'
- en: 'Figure 20.2: **Mixture of Experts (MoE) Routing**: Conditional computation
    through learned routing enables efficient scaling to trillions of parameters.
    The router (gating function) determines which experts process each token, activating
    only relevant specialists. This sparse activation pattern reduces computational
    cost while maintaining model capacity, though it introduces load balancing and
    memory access challenges.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.2：**专家混合（MoE）路由**：通过学习路由进行条件计算，能够高效地扩展到万亿参数。路由器（门控函数）确定哪些专家处理每个标记，仅激活相关专家。这种稀疏激活模式降低了计算成本，同时保持了模型容量，尽管它引入了负载均衡和内存访问挑战。
- en: External Memory for Compound Systems
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 外部内存用于复合系统
- en: Beyond routing efficiency, compound systems require memory architectures that
    scale beyond individual model constraints. As detailed in [Section 20.5.1](ch026.xhtml#sec-agi-systems-state-space-models-efficient-longcontext-processing-7ece),
    transformers face quadratic memory scaling with sequence length, limiting knowledge
    access during inference and preventing long-context reasoning across system components.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 除了路由效率之外，复合系统还需要内存架构，其扩展能力超越单个模型限制。如 [第 20.5.1 节](ch026.xhtml#sec-agi-systems-state-space-models-efficient-longcontext-processing-7ece)
    详细所述，变换器面临与序列长度成二次关系的内存扩展，限制了推理期间的知识访问，并阻止系统组件之间的长上下文推理。
- en: Retrieval-Augmented Generation (RAG)[8](#fn8) addresses this by creating external
    memory stores accessible to multiple system components. Instead of encoding all
    knowledge in parameters, specialized retrieval components query databases containing
    billions of documents, incorporating relevant information into generation processes.
    This transforms the architecture from purely parametric to hybrid parametric-nonparametric
    systems ([Borgeaud et al. 2021](ch058.xhtml#ref-borgeaud2022improving)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建多个系统组件可访问的外部内存存储，检索增强生成（RAG）[8](#fn8) 解决了这个问题。它不是将所有知识编码在参数中，而是专门的检索组件查询包含数十亿文档的数据库，将相关信息纳入生成过程。这使架构从纯参数化转变为混合参数化-非参数化系统
    ([Borgeaud 等人 2021](ch058.xhtml#ref-borgeaud2022improving))。
- en: For compound systems, this enables shared knowledge bases accessible to different
    specialized components, efficient similarity search across diverse content types,
    and coordinated retrieval that supports complex multi-step reasoning processes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复合系统来说，这使不同的专门组件可以访问共享的知识库，在多样化的内容类型之间进行高效的相似性搜索，并支持复杂的多步推理过程。
- en: Modular Reasoning Architectures
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模块化推理架构
- en: 'Multi-step reasoning exemplifies the compound systems advantage: breaking complex
    problems into verifiable components. While monolithic models can answer simple
    questions directly, multi-step problems produce compounding errors (90% accuracy
    per step yields only 59% overall accuracy for 5-step problems). GPT-3 ([T. Brown
    et al. 2020](ch058.xhtml#ref-brown2020language)) exhibits 40-60% error rates on
    complex reasoning, primarily from intermediate step failures.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 多步推理体现了复合系统优势：将复杂问题分解为可验证的组件。虽然单体模型可以直接回答简单问题，但多步问题会产生累积错误（每步90%的准确率对于5步问题来说，整体准确率只有59%）。GPT-3
    ([T. Brown等人 2020](ch058.xhtml#ref-brown2020language)) 在复杂推理上的错误率在40-60%，主要来自中间步骤的失败。
- en: Chain-of-thought prompting ([Wei et al. 2022](ch058.xhtml#ref-wei2022chain))
    and modular reasoning architectures address this through decomposition where different
    components handle different reasoning stages. Rather than generating answers directly,
    specialized components produce intermediate reasoning steps that verification
    components can check and correct. Chain-of-thought prompting improves GSM8K accuracy
    from 17.9% to 58.1%, with step verification reaching 78.2%.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链提示（[Wei等人 2022](ch058.xhtml#ref-wei2022chain)）和模块化推理架构通过分解来解决这个问题，其中不同的组件处理不同的推理阶段。而不是直接生成答案，专门的组件产生中间推理步骤，验证组件可以检查和纠正。思维链提示将GSM8K的准确率从17.9%提高到58.1%，步骤验证达到78.2%。
- en: 'This architectural approach, decomposing complex tasks across specialized components
    with verification, represents the core compound systems pattern: multiple specialists
    collaborating through structured interfaces rather than monolithic processing.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构方法，通过验证将复杂任务分解到专门的组件中，代表了核心复合系统模式：多个专家通过结构化接口而不是单体处理进行协作。
- en: These innovations demonstrate the transition from static architectures toward
    dynamic compound systems that route computation, access external memory, and decompose
    reasoning across specialized components. This architectural foundation enables
    the sophisticated orchestration required for AGI-scale intelligence.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这些创新展示了从静态架构向动态复合系统的转变，这些系统可以路由计算、访问外部内存，并在专门的组件之间分解推理。这种架构基础为AGI规模智能所需的复杂编排提供了可能。
- en: Dynamic architectures provide sophisticated orchestration mechanisms, yet they
    operate within the computational constraints of their underlying paradigms. Transformers,
    the foundation of current breakthroughs, face scaling limitations that compound
    systems must eventually transcend. Before examining how to train and deploy compound
    systems, we must understand the alternative architectural paradigms that could
    form their computational substrate.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 动态架构提供了复杂的编排机制，但它们在其底层范例的计算约束内运行。Transformer，当前突破的基础，面临着扩展限制，复合系统必须最终超越这些限制。在考察如何训练和部署复合系统之前，我们必须了解可能形成其计算基础的替代架构范例。
- en: Alternative Architectures for AGI
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适用于通用人工智能（AGI）的替代架构
- en: 'The dynamic architectures explored above extend transformer capabilities while
    preserving their core computational pattern: attention mechanisms that compare
    every input element with every other element. This quadratic scaling creates an
    inherent bottleneck as context lengths grow. Processing a 100,000 token document
    requires 10 billion pairwise comparisons, which is computationally expensive and
    economically prohibitive for many applications.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上文所探讨的动态架构在扩展Transformer能力的同时，保留了其核心计算模式：将每个输入元素与每个其他元素进行比较的注意力机制。这种二次扩展随着上下文长度的增加而产生了固有的瓶颈。处理一个包含10万个标记的文档需要100亿个成对比较，这在计算上非常昂贵，对于许多应用来说经济上也是不可行的。
- en: The autoregressive generation pattern limits transformers to sequential, left-to-right
    processing that cannot easily revise earlier decisions based on later constraints.
    These limitations suggest that achieving AGI may require architectural innovations
    beyond scaling current paradigms.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归生成模式限制了Transformer只能进行顺序的、从左到右的处理，无法根据后续约束轻松修改早期决策。这些限制表明，实现AGI可能需要超越当前范例的架构创新。
- en: 'This section examines three emerging paradigms that address transformer limitations
    through different computational principles: state space models for efficient long-context
    processing, energy-based models for optimization-driven reasoning, and world models
    for causal understanding. Each represents a potential building block for future
    compound intelligence systems.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了三种新兴范式，它们通过不同的计算原理来解决 Transformer 的局限性：用于高效长期上下文处理的状态空间模型、基于能量的模型用于优化驱动推理，以及用于因果理解的领域模型。每个都代表未来复合智能系统潜在的建筑模块。
- en: 'State Space Models: Efficient Long-Context Processing'
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 状态空间模型：高效的长期上下文处理
- en: 'Transformers’ attention mechanism compares every token with every other token,
    creating quadratic scaling: a 100,000 token context requires 10 billion comparisons
    (100K × 100K pairwise attention scores). This O(n²) memory and computation complexity
    limits context windows and makes processing book-length documents, multi-hour
    conversations, or entire codebases prohibitively expensive for real-time applications.
    The quadratic bottleneck emerges from the attention matrix A = softmax(QKᵀ/√d)
    where Q, K ∈ ℝⁿˣᵈ must compute all n² pairwise similarities.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的注意力机制将每个标记与每个其他标记进行比较，创建二次缩放：100,000 个标记的上下文需要 1,000 亿次比较（100K
    × 100K 对注意力分数）。这种 O(n²) 的内存和计算复杂度限制了上下文窗口，使得处理书籍长度的文档、多小时的对话或整个代码库对于实时应用来说成本过高。二次瓶颈来自于注意力矩阵
    A = softmax(QKᵀ/√d)，其中 Q, K ∈ ℝⁿˣᵈ 必须计算所有 n² 对相似度。
- en: 'State space models offer a compelling alternative by processing sequences in
    O(n) time through recurrent hidden state updates rather than attention over all
    prior tokens. The fundamental idea draws from control theory: maintain a compressed
    latent state h ∈ ℝᵈ that summarizes all previous inputs, updating it incrementally
    as new tokens arrive. Mathematically, state space models implement continuous-time
    dynamics discretized for sequence processing:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间模型通过在 O(n) 时间内通过循环隐藏状态更新来处理序列，而不是对所有先前标记进行注意力处理，从而提供了一个有吸引力的替代方案。其基本思想来源于控制理论：维护一个压缩的潜在状态
    h ∈ ℝᵈ，该状态总结了所有先前输入，并在新标记到达时增量更新它。从数学上讲，状态空间模型实现了连续时间动力学，用于序列处理：
- en: '**Continuous form:** <semantics><mtable><mtr><mtd columnalign="right" style="text-align:
    right"><mover><mi>h</mi><mo accent="true">̇</mo></mover><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd
    columnalign="left" style="text-align: left"><mo>=</mo><mi>A</mi><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>B</mi><mi>x</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd
    columnalign="right" style="text-align: right"><mi>y</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd
    columnalign="left" style="text-align: left"><mo>=</mo><mi>C</mi><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>D</mi><mi>x</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable>
    <annotation encoding="application/x-tex">\begin{aligned} \dot{h}(t) &= Ah(t) +
    Bx(t) \\ y(t) &= Ch(t) + Dx(t) \end{aligned}</annotation></semantics>'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续形式**：<semantics><mtable><mtr><mtd columnalign="right" style="text-align:
    right"><mover><mi>h</mi><mo accent="true">̇</mo></mover><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd
    columnalign="left" style="text-align: left"><mo>=</mo><mi>A</mi><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>B</mi><mi>x</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd
    columnalign="right" style="text-align: right"><mi>y</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd
    columnalign="left" style="text-align: left"><mo>=</mo><mi>C</mi><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>D</mi><mi>x</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable>
    <annotation encoding="application/x-tex">\begin{aligned} \dot{h}(t) &= Ah(t) +
    Bx(t) \\ y(t) &= Ch(t) + Dx(t) \end{aligned}</annotation></semantics>'
- en: '**Discretized form:** <semantics><mtable><mtr><mtd columnalign="right" style="text-align:
    right"><msub><mi>h</mi><mi>t</mi></msub></mtd><mtd columnalign="left" style="text-align:
    left"><mo>=</mo><mover><mi>A</mi><mo accent="true">‾</mo></mover><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mover><mi>B</mi><mo
    accent="true">‾</mo></mover><msub><mi>x</mi><mi>t</mi></msub></mtd></mtr><mtr><mtd
    columnalign="right" style="text-align: right"><msub><mi>y</mi><mi>t</mi></msub></mtd><mtd
    columnalign="left" style="text-align: left"><mo>=</mo><mover><mi>C</mi><mo accent="true">‾</mo></mover><msub><mi>h</mi><mi>t</mi></msub><mo>+</mo><mover><mi>D</mi><mo
    accent="true">‾</mo></mover><msub><mi>x</mi><mi>t</mi></msub></mtd></mtr></mtable>
    <annotation encoding="application/x-tex">\begin{aligned} h_t &= \bar{A}h_{t-1}
    + \bar{B}x_t \\ y_t &= \bar{C}h_t + \bar{D}x_t \end{aligned}</annotation></semantics>'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**离散形式:** <semantics><mtable><mtr><mtd columnalign="right" style="text-align:
    right"><msub><mi>h</mi><mi>t</mi></msub></mtd><mtd columnalign="left" style="text-align:
    left"><mo>=</mo><mover><mi>A</mi><mo accent="true">‾</mo></mover><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mover><mi>B</mi><mo
    accent="true">‾</mo></mover><msub><mi>x</mi><mi>t</mi></msub></mtd></mtr><mtr><mtd
    columnalign="right" style="text-align: right"><msub><mi>y</mi><mi>t</mi></msub></mtd><mtd
    columnalign="left" style="text-align: left"><mo>=</mo><mover><mi>C</mi><mo accent="true">‾</mo></mover><msub><mi>h</mi><mi>t</mi></msub><mo>+</mo><mover><mi>D</mi><mo
    accent="true">‾</mo></mover><msub><mi>x</mi><mi>t</mi></msub></mtd></mtr></mtable>
    <annotation encoding="application/x-tex">\begin{aligned} h_t &= \bar{A}h_{t-1}
    + \bar{B}x_t \\ y_t &= \bar{C}h_t + \bar{D}x_t \end{aligned}</annotation></semantics>'
- en: where x ∈ ℝ is the input token, h ∈ ℝᵈ is the hidden state, y ∈ ℝ is the output,
    and {A, B, C, D} are learned parameters mapping between these spaces. Unlike RNN
    hidden states that suffer from vanishing/exploding gradients, state space formulations
    leverage structured matrices (diagonal, low-rank, or Toeplitz) that enable stable
    long-range dependencies through careful initialization and parameterization.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 x ∈ ℝ 是输入标记，h ∈ ℝᵈ 是隐藏状态，y ∈ ℝ 是输出，{A, B, C, D} 是学习参数，它们在这些空间之间进行映射。与受梯度消失/爆炸问题困扰的RNN隐藏状态不同，状态空间公式利用结构化矩阵（对角线、低秩或Toeplitz）通过仔细的初始化和参数化，通过稳定的长距离依赖性。
- en: 'The technical breakthrough enabling competitive performance came from selective
    state spaces where the recurrence parameters themselves depend on the input: Āₜ
    = f_A(xₜ), B̄ₜ = f_B(xₜ), making the state transition input-dependent rather than
    fixed. This selectivity allows the model to dynamically adjust which information
    to remember or forget based on current input content. When processing “The trophy
    doesn’t fit in the suitcase because it’s too big,” the model can selectively maintain
    “trophy” in state while discarding less relevant words, with the selection driven
    by learned input-dependent gating similar to LSTM forget gates but within the
    state space framework. This approach resembles maintaining a running summary that
    adapts its compression strategy based on content importance rather than blindly
    summarizing everything equally.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 实现具有竞争力性能的技术突破来自于选择性的状态空间，其中递归参数本身依赖于输入：Āₜ = f_A(xₜ), B̄ₜ = f_B(xₜ)，这使得状态转换输入依赖而非固定。这种选择性允许模型根据当前输入内容动态调整要记住或忘记的信息。当处理“奖杯太大，放不进手提箱”时，模型可以选择性地在状态中维持“奖杯”信息，同时丢弃不那么相关的词语，选择过程由学习到的输入依赖的门控机制驱动，类似于LSTM的遗忘门，但处于状态空间框架内。这种方法类似于维护一个运行摘要，根据内容的重要性调整其压缩策略，而不是盲目地平均总结一切。
- en: Models like Mamba ([A. Gu and Dao 2023](ch058.xhtml#ref-gu2023mamba)), RWKV
    ([Peng et al. 2023](ch058.xhtml#ref-peng2023rwkv)), and Liquid Time-constant Networks
    ([Hasani et al. 2020](ch058.xhtml#ref-hasani2020liquid)) demonstrate that this
    approach can match transformer performance on many tasks while scaling linearly
    rather than quadratically with sequence length. Using selective state spaces with
    input-dependent parameters, Mamba achieves 5× better throughput on long sequences
    (100K+ tokens) compared to transformers. Mamba-7B matches transformer-7B performance
    on text while using 5× less memory for 100K token sequences. Subsequent developments
    including Mamba-2 have further improved both efficiency and quality, while hybrid
    architectures combining state space layers with attention (as in Jamba) suggest
    that the future may involve complementary mechanisms rather than wholesale architectural
    replacement. RWKV combines the efficient inference of RNNs with the parallelizable
    training of transformers, while Liquid Time-constant Networks adapt their dynamics
    based on input, showing particular promise for time-series and continuous control
    tasks.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 像Mamba ([A. Gu and Dao 2023](ch058.xhtml#ref-gu2023mamba))、RWKV ([Peng et al.
    2023](ch058.xhtml#ref-peng2023rwkv))和Liquid Time-constant Networks ([Hasani et
    al. 2020](ch058.xhtml#ref-hasani2020liquid))这样的模型表明，这种方法可以在许多任务上匹配Transformer的性能，同时与序列长度成线性而不是平方关系进行扩展。使用具有输入相关参数的选择性状态空间，Mamba在长序列（100K+标记）上的吞吐量比Transformer提高了5倍。Mamba-7B在文本上的性能与Transformer-7B相当，但在100K标记序列上使用的内存减少了5倍。后续的发展包括Mamba-2，进一步提高了效率和质量，而将状态空间层与注意（如在Jamba中）结合的混合架构表明，未来可能涉及互补机制而不是全面架构替换。RWKV结合了RNN的高效推理和Transformer的可并行训练，而Liquid
    Time-constant Networks根据输入调整其动态，显示出在时间序列和连续控制任务上的特别前景。
- en: Systems engineering implications are significant. Linear scaling enables processing
    book-length contexts, multi-hour conversations, or entire codebases within single
    model calls. This requires rethinking data loading strategies (handling MB-scale
    inputs), memory management (streaming rather than batch processing), and distributed
    inference patterns optimized for sequential processing rather than parallel attention.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 系统工程的影响是显著的。线性扩展使得在单个模型调用中处理书籍长度的上下文、多小时的对话或整个代码库成为可能。这需要重新思考数据加载策略（处理MB级输入）、内存管理（流式处理而不是批量处理）以及针对顺序处理而非并行注意力的分布式推理模式。
- en: 'State space models remain experimental. Transformers benefit from years of
    optimization across the entire ML systems stack, from specialized hardware kernels
    (FlashAttention, optimized CUDA implementations) to distributed training frameworks
    (tensor parallelism, pipeline parallelism from [Chapter 8](ch014.xhtml#sec-ai-training))
    to deployment infrastructure. Alternative architectures must not only match transformer
    capabilities but also justify the engineering effort required to rebuild this
    optimization ecosystem. For compound systems, hybrid approaches may prove most
    practical: transformers for tasks benefiting from parallel attention, state space
    models for long-context sequential processing, coordinated through the orchestration
    patterns explored in [Section 20.3](ch026.xhtml#sec-agi-systems-compound-ai-systems-framework-2a31).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间模型仍然是实验性的。Transformer得益于整个机器学习系统堆栈多年的优化，从专用硬件内核（FlashAttention、优化的CUDA实现）到分布式训练框架（张量并行性、来自[第8章](ch014.xhtml#sec-ai-training)的流水线并行性）再到部署基础设施。替代架构不仅必须与Transformer的能力相匹配，还必须证明重建此优化生态系统的工程努力是合理的。对于复合系统，混合方法可能最为实用：Transformer用于受益于并行注意力的任务，状态空间模型用于长上下文顺序处理，通过在[第20.3节](ch026.xhtml#sec-agi-systems-compound-ai-systems-framework-2a31)中探索的编排模式进行协调。
- en: 'Energy-Based Models: Learning Through Optimization'
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于能量的模型：通过优化进行学习
- en: 'Current language models generate text by predicting one token at a time, conditioning
    each prediction on all previous tokens. This autoregressive approach has key limitations
    for complex reasoning: it cannot easily revise earlier decisions based on later
    constraints, struggles with problems requiring global optimization, and tends
    to produce locally coherent but globally inconsistent outputs.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当前语言模型通过一次预测一个标记，并将每个预测条件化在所有先前标记的基础上来生成文本。这种自回归方法在复杂推理方面存在关键限制：它不能轻易根据后续约束来修订早期决策，难以处理需要全局优化的问题，并且往往产生局部一致但全局不一致的输出。
- en: 'Energy-based models (EBMs) offer a different approach: learning an energy function
    <semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">E(x)</annotation></semantics>
    that assigns low energy to probable or desirable configurations <semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics> and high energy to improbable
    ones. Rather than directly generating outputs, EBMs perform inference through
    optimization, finding configurations that minimize energy. This paradigm enables
    several capabilities unavailable to autoregressive models through its fundamentally
    different computational structure.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基于能量的模型（EBMs）提供了一种不同的方法：学习一个能量函数 <semantics><mrow><mi>E</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">E(x)</annotation></semantics>，它将低能量分配给可能的或期望的配置 <semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics>，并将高能量分配给不可能的配置。EBMs不是直接生成输出，而是通过优化进行推理，寻找最小化能量的配置。这种范式通过其根本不同的计算结构，使得自回归模型无法实现的能力成为可能。
- en: 'First, EBMs enable global optimization by considering multiple interacting
    constraints simultaneously rather than making sequential local decisions. When
    planning a multi-step project where earlier decisions constrain later options,
    autoregressive models must commit to steps sequentially without revising based
    on downstream consequences. An EBM can formulate the entire plan as an optimization
    problem where the energy function captures constraint satisfaction across all
    steps, then search for globally optimal solutions through gradient descent or
    sampling methods. For problems requiring planning, constraint satisfaction, or
    multi-step reasoning where local decisions create global suboptimality, this holistic
    optimization proves essential. Sudoku exemplifies this: filling squares sequentially
    often leads to contradictions requiring backtracking, while formulating valid
    completions as low-energy states enables efficient solution through constraint
    propagation.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，EBMs通过同时考虑多个相互作用的约束条件，而不是做出序列性的局部决策，从而实现全局优化。当规划一个多步骤项目，其中早期决策限制了后续选项时，自回归模型必须按顺序承诺步骤，而不会根据下游后果进行修订。EBM可以将整个计划作为优化问题来制定，其中能量函数捕捉所有步骤的约束满足情况，然后通过梯度下降或采样方法搜索全局最优解。对于需要规划、约束满足或多步骤推理的问题，其中局部决策导致全局次优，这种整体优化证明是至关重要的。数独就是一个例子：按顺序填充方格往往会导致需要回溯的矛盾，而将有效的完成作为低能量状态，可以通过约束传播实现高效的解决方案。
- en: 'Second, the energy landscape naturally represents multiple valid solutions
    with different energy levels, enabling exploration of solution diversity. Unlike
    autoregressive models that commit to single generation paths through greedy decoding
    or limited beam search, EBMs maintain probability distributions over the entire
    solution space. When designing molecules with desired properties, multiple chemical
    structures might satisfy constraints with varying trade-offs. The energy function
    assigns scores to each candidate structure, with inference sampling diverse low-energy
    configurations rather than collapsing to single outputs. This supports creative
    applications where diversity matters: generating multiple plot variations for
    a story, exploring architectural design alternatives, or proposing candidate drug
    molecules for synthesis and testing.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，能量景观自然地表示了具有不同能量水平的多个有效解，从而能够探索解的多样性。与通过贪婪解码或有限的束搜索承诺单一生成路径的自回归模型不同，EBMs在整个解空间上维护概率分布。在设计具有所需特性的分子时，多个化学结构可能以不同的权衡满足约束条件。能量函数为每个候选结构分配分数，通过推理采样多样化的低能量配置，而不是收敛到单一输出。这支持了创意应用，其中多样性很重要：为故事生成多个情节变体，探索建筑设计的替代方案，或提出候选药物分子进行合成和测试。
- en: 'Third, EBMs support bidirectional reasoning that propagates information both
    forward and backward through inference. Autoregressive generation flows unidirectionally
    from start to end, unable to revise earlier decisions based on later constraints.
    EBMs perform inference through iterative refinement that can modify any part of
    the output to reduce global energy. When writing poetry where the final line must
    rhyme with the first, EBMs can adjust earlier lines to enable satisfying conclusions.
    This bidirectional capability extends to causal reasoning: inferring probable
    causes from observed effects, planning actions that achieve desired outcomes,
    and debugging code by working backward from error symptoms to root causes. The
    inference procedure treats all variables symmetrically, enabling flexible reasoning
    in any direction needed.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，EBMs 支持双向推理，通过推理将信息向前和向后传播。自回归生成流程从开始到结束单向传播，无法根据后续约束修改早期决策。EBMs 通过迭代细化进行推理，可以修改输出中的任何部分以降低全局能量。在写诗时，如果最后一行必须与第一行押韵，EBMs
    可以调整早期行以实现令人满意的结论。这种双向能力扩展到因果推理：从观察到的效果推断可能的因果关系，规划实现预期结果的行为，以及通过从错误症状追溯到根本原因来调试代码。推理过程对待所有变量都是对称的，使得在任何方向上都能进行灵活推理。
- en: 'Fourth, energy levels provide principled uncertainty quantification through
    the Boltzmann distribution p(x) ∝ exp(-E(x)/T) where temperature T controls confidence
    calibration. Solutions with energy far above the minimum receive exponentially
    lower probability, providing natural confidence scores. This supports robust decision
    making in uncertain environments: when multiple completion options have similar
    low energies, the model expresses uncertainty rather than overconfidently committing
    to arbitrary choices. For safety-critical applications like medical diagnosis
    or autonomous vehicle control, knowing when the model is uncertain enables deferring
    to human judgment rather than blindly executing potentially incorrect decisions.
    The energy-based framework inherently provides the uncertainty estimates that
    autoregressive models must learn separately through ensemble methods or Bayesian
    approximations.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，能量水平通过玻尔兹曼分布 p(x) ∝ exp(-E(x)/T) 提供了基于原理的不确定性量化，其中温度 T 控制置信度校准。能量远高于最小值的解决方案具有指数级降低的概率，提供自然的置信度分数。这支持在不确定环境中的稳健决策：当多个完成选项具有相似的低能量时，模型表达不确定性而不是过度自信地做出任意选择。对于医疗诊断或自动驾驶车辆控制等安全关键应用，知道模型何时不确定可以启用依赖人类判断而不是盲目执行可能不正确的决策。基于能量的框架本质上提供了自回归模型必须通过集成方法或贝叶斯近似单独学习的不确定性估计。
- en: Systems engineering challenges are considerable. Inference requires solving
    optimization problems that can be computationally expensive, particularly for
    high-dimensional spaces. Training EBMs often involves contrastive learning methods
    requiring negative example generation through MCMC sampling[9](#fn9) or other
    computationally intensive procedures. The optimization landscapes can contain
    many local minima, requiring sophisticated inference algorithms.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 系统工程挑战相当大。推理需要解决可能计算成本高昂的优化问题，尤其是对于高维空间。训练 EBMs 通常涉及对比学习方法，需要通过 MCMC 抽样[9](#fn9)
    或其他计算密集型程序生成负例。优化景观可能包含许多局部最小值，需要复杂的推理算法。
- en: These challenges create opportunities for systems innovation. Specialized hardware
    for optimization (quantum annealers, optical computers) could provide computational
    advantages for EBM inference. Hierarchical energy models could decompose complex
    problems into tractable subproblems. Hybrid architectures could combine fast autoregressive
    generation with EBM refinement for improved solution quality.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战为系统创新创造了机会。用于优化的专用硬件（量子退火器、光学计算机）可以为 EBM 推理提供计算优势。分层能量模型可以将复杂问题分解为可处理的子问题。混合架构可以将快速自回归生成与
    EBM 精细化相结合，以改善解决方案质量。
- en: In compound AI systems, EBMs could serve as specialized reasoning components
    handling constraint satisfaction, planning, and verification tasks, domains where
    optimization-based approaches excel. While autoregressive models generate fluent
    text, EBMs ensure logical consistency and constraint adherence. This division
    of labor leverages each approach’s strengths while mitigating weaknesses, exemplifying
    the compound systems principle explored in [Section 20.3](ch026.xhtml#sec-agi-systems-compound-ai-systems-framework-2a31).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在复合AI系统中，EBMs可以作为专门的推理组件处理约束满足、规划和验证任务，这些领域是优化方法表现优异的领域。虽然自回归模型生成流畅的文本，但EBMs确保逻辑一致性和约束遵守。这种劳动分工利用了每种方法的优点，同时减轻了弱点，体现了[第20.3节](ch026.xhtml#sec-agi-systems-compound-ai-systems-framework-2a31)中探讨的复合系统原则。
- en: World Models and Predictive Learning
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 世界模型与预测学习
- en: 'Building on the self-supervised learning principles established in [Section 20.4.1.1](ch026.xhtml#sec-agi-systems-selfsupervised-learning-components-e6d8),
    true AGI requires world models: learned internal representations of how environments
    work that support prediction, planning, and causal reasoning across diverse domains.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于[第20.4.1.1节](ch026.xhtml#sec-agi-systems-selfsupervised-learning-components-e6d8)中建立的自监督学习原则，真正的AGI需要世界模型：学习环境如何工作的内部表示，支持跨多个领域的预测、规划和因果推理。
- en: 'World models are internal simulations that capture causal relationships enabling
    systems to predict consequences of actions, reason about counterfactuals, and
    plan sequences toward goals. While current AI predicts surface patterns in data
    through next-token prediction, world models understand underlying mechanisms.
    Consider the difference: a language model learns that “rain” and “wet” frequently
    co-occur in text, achieving statistical association. A world model learns that
    rain causes wetness through absorption and surface wetting, enabling predictions
    about novel scenarios (Will a covered object get wet in rain? No, because the
    cover blocks causal mechanism) that pure statistical models cannot make.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 世界模型是内部模拟，它捕捉因果关系，使系统能够预测行动的后果，对反事实进行推理，并规划向目标序列。虽然当前的AI通过预测下一个标记来通过数据预测表面模式，但世界模型理解底层机制。考虑一下区别：一个语言模型学习到“rain”（雨）和“wet”（湿）在文本中经常同时出现，实现了统计关联。而世界模型学习到雨通过吸收和表面湿润导致湿，能够对新型场景进行预测（被覆盖的物体在雨中会湿吗？不会，因为覆盖物阻止了因果机制），这是纯统计模型无法做到的。
- en: 'The technical distinction manifests in representation structure. Autoregressive
    models maintain probability distributions over sequences: P(x₁, x₂, …, xₙ) = ∏ᵢ
    P(xᵢ | x₁, …, xᵢ₋₁), predicting each token given history. World models instead
    learn latent dynamics: sₜ₊₁ = f(sₜ, aₜ) mapping current state sₜ and action aₜ
    to next state, with separate observation model o = g(s) rendering states to observations.
    This factorization enables forward simulation (predicting long-term consequences),
    inverse models (inferring actions that produced observed outcomes), and counterfactual
    reasoning (what would happen if action differed).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上的区别体现在表示结构上。自回归模型在序列上维持概率分布：P(x₁, x₂, …, xₙ) = ∏ᵢ P(xᵢ | x₁, …, xᵢ₋₁)，根据历史预测每个标记。世界模型则学习潜在动态：sₜ₊₁
    = f(sₜ, aₜ) 将当前状态 sₜ 和动作 aₜ 映射到下一个状态，通过独立的观察模型 o = g(s) 将状态渲染为观察。这种分解使得可以进行前向模拟（预测长期后果）、逆模型（推断产生观察结果的动作），以及反事实推理（如果动作不同会发生什么）。
- en: 'DeepMind’s MuZero ([Schrittwieser et al. 2020](ch058.xhtml#ref-schrittwieser2020mastering))
    demonstrates world model principles in game playing. Rather than learning rules
    explicitly, MuZero learns three functions: representation (mapping observations
    to hidden states), dynamics (predicting next hidden state from current state and
    action), and prediction (estimating value and policy from hidden state). Starting
    without game rules, it discovers that certain piece configurations lead to winning
    outcomes, enabling superhuman play in chess, shogi, and Go through learned causal
    models rather than explicit rule specification.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind的MuZero([Schrittwieser等人2020](ch058.xhtml#ref-schrittwieser2020mastering))在游戏中的表现展示了世界模型原则。MuZero不是通过显式学习规则，而是学习三个函数：表示（将观察映射到隐藏状态）、动态（从当前状态和动作预测下一个隐藏状态），以及预测（从隐藏状态估计价值和策略）。它从没有游戏规则开始，发现某些棋子配置会导致胜利的结果，通过学习因果模型而不是显式规则规范，实现了在象棋、将棋和围棋中的超人类表现。
- en: This paradigm shift leverages the Joint Embedding Predictive Architecture (JEPA)
    framework introduced earlier, moving beyond autoregressive generation toward predictive
    intelligence that understands causality. Instead of generating text tokens sequentially,
    future AGI systems predict consequences of actions in abstract representation
    spaces. For robotics, this means predicting how objects move when pushed (physics
    world model). For language, this means predicting how conversations evolve based
    on speaking strategies (social world model). For reasoning, this means predicting
    how mathematical statements follow from axioms (logical world model).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这种范式转变利用了之前介绍的联合嵌入预测架构（JEPA）框架，超越了自回归生成，朝着理解因果关系的预测智能发展。未来的AGI系统不再按顺序生成文本标记，而是在抽象表示空间中预测行动的后果。对于机器人学，这意味着预测物体被推动时的运动（物理世界模型）。对于语言，这意味着根据说话策略预测对话如何演变（社会世界模型）。对于推理，这意味着预测数学陈述如何从公理中得出（逻辑世界模型）。
- en: 'Systems engineering challenges span multiple dimensions. Data requirements
    grow substantially: learning accurate world models requires petabytes of multimodal
    interaction data capturing diverse causal patterns, far exceeding text-only training.
    Architecture design must support temporal synchronization across multiple sensory
    modalities (vision at 30 Hz, audio at 16 kHz, proprioception at 1 kHz), requiring
    careful buffer management and alignment. Training procedures must enable continuous
    learning from streaming data without catastrophic forgetting (challenges explored
    in [Section 20.6.0.4](ch026.xhtml#sec-agi-systems-continual-learning-lifelong-adaptation-7aee)),
    updating world models as environments change while preserving previously learned
    causal relationships.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 系统工程挑战涉及多个维度。数据需求大幅增长：学习准确的世界模型需要PB级的多模态交互数据，以捕捉多样的因果模式，远超过仅文本的训练。架构设计必须支持多个感官模态之间的时序同步（视觉30Hz，音频16kHz，本体感觉1kHz），这需要仔细的缓冲管理和对齐。训练程序必须能够从流数据中持续学习，而不会发生灾难性遗忘（在[第20.6.0.4节](ch026.xhtml#sec-agi-systems-continual-learning-lifelong-adaptation-7aee)中探讨的挑战），在环境变化时更新世界模型，同时保留之前学习到的因果关系。
- en: 'Verification poses unique challenges. Evaluating world models requires testing
    causal predictions, not just statistical accuracy. A model predicting “umbrellas
    appear when it rains” achieves high statistical accuracy but fails causally, as
    umbrellas don’t cause rain. Testing requires intervention experiments: if the
    model believes rain causes umbrellas, removing umbrellas shouldn’t affect predicted
    rain. Implementing such causal testing at scale demands sophisticated evaluation
    infrastructure beyond standard ML benchmarking.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 验证提出了独特的挑战。评估世界模型需要测试因果预测，而不仅仅是统计准确性。一个预测“下雨时会出现雨伞”的模型虽然达到了高统计准确性，但在因果上却失败了，因为雨伞不会导致下雨。测试需要干预实验：如果模型认为雨会导致雨伞，那么移除雨伞不应该影响预测的雨。在规模上实施这种因果测试需要超越标准机器学习基准测试的复杂评估基础设施。
- en: In compound systems, world model components provide causal understanding and
    planning capabilities while other components handle perception, action selection,
    or communication. This specialization enables developing robust world models for
    specific domains (physical laws for robotics, social dynamics for dialogue, logical
    rules for mathematics) while maintaining flexibility to combine them for complex,
    multi-domain reasoning tasks. A household robot might use physical world models
    to predict object trajectories, social world models to anticipate human actions,
    and planning algorithms to sequence manipulation steps achieving desired outcomes.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在复合系统中，世界模型组件提供因果理解和规划能力，而其他组件则处理感知、动作选择或通信。这种专业化使得能够为特定领域（如机器人学的物理定律、对话的社会动力学、数学的逻辑规则）开发出鲁棒的世界模型，同时保持灵活性，以便将它们组合用于复杂的多领域推理任务。一个家庭机器人可能会使用物理世界模型来预测物体轨迹，社会世界模型来预测人类行为，以及规划算法来按顺序执行操作步骤，以实现期望的结果。
- en: Hybrid Architecture Integration Strategies
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合架构集成策略
- en: The paradigms explored above address complementary transformer limitations through
    different computational approaches, yet none represents a complete replacement.
    Transformers excel at parallel processing and fluent natural language generation
    but suffer quadratic memory scaling and sequential generation constraints. State
    space models achieve linear complexity but lack transformers’ expressive attention
    patterns. Energy-based models enable global optimization but require expensive
    inference. World models provide causal reasoning but demand extensive multimodal
    training data. The path forward lies not in choosing one paradigm but orchestrating
    hybrid compound systems that leverage each architecture’s strengths while mitigating
    weaknesses.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 上文探讨的范例通过不同的计算方法解决了互补的转换器限制，但没有任何一个代表完全的替代。转换器在并行处理和流畅的自然语言生成方面表现出色，但遭受二次内存缩放和顺序生成约束。状态空间模型实现线性复杂度，但缺乏转换器的表达注意力模式。基于能量的模型实现全局优化，但需要昂贵的推理。世界模型提供因果推理，但需要广泛的多模态训练数据。前进的道路不在于选择一个范例，而在于编排混合复合系统，利用每个架构的优势同时减轻弱点。
- en: Several integration patterns emerge from current research. Cascade architectures
    route inputs sequentially through specialized components, with each stage refining
    outputs from previous stages. A language understanding pipeline might use transformers
    for initial parsing, world models for causal inference about described events,
    and energy-based models for constraint checking and consistency verification.
    This sequential specialization enables sophisticated reasoning pipelines where
    each component contributes distinct capabilities.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 从当前研究中出现了几种集成模式。级联架构按顺序将输入路由到专用组件，每个阶段都细化来自前一阶段的输出。一个语言理解管道可能使用转换器进行初始解析，使用世界模型对描述的事件进行因果推理，并使用基于能量的模型进行约束检查和一致性验证。这种顺序专业化使复杂的推理管道成为可能，其中每个组件都贡献了独特的功能。
- en: 'Parallel ensemble approaches combine multiple architectures processing inputs
    simultaneously, with results aggregated through learned weighting or voting mechanisms.
    A question-answering system might generate candidate answers using transformers,
    score them using energy-based models evaluating logical consistency, and rank
    them using world models predicting downstream consequences. This redundancy provides
    robustness: if one architecture fails on particular inputs, others may succeed.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 并行集成方法结合多个架构同时处理输入，通过学习加权或投票机制汇总结果。一个问答系统可能使用转换器生成候选答案，使用基于能量的模型评估逻辑一致性进行评分，并使用预测下游后果的世界模型进行排名。这种冗余提供了鲁棒性：如果一个架构在特定输入上失败，其他架构可能成功。
- en: Hierarchical decomposition assigns architectures to different abstraction levels.
    High level planning might use world models to predict long-term consequences,
    mid level execution might use transformers for action generation, and low level
    control might use state space models for real-time response. This vertical integration
    enables systems to reason at multiple timescales simultaneously, from millisecond
    reflexes to multi-hour plans.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 层次分解将架构分配到不同的抽象级别。高级规划可能使用世界模型来预测长期后果，中级执行可能使用转换器进行动作生成，而低级控制可能使用状态空间模型进行实时响应。这种垂直集成使系统能够同时在不同时间尺度上进行推理，从毫秒级的反射到多小时的计划。
- en: The most sophisticated integration strategy involves dynamic routing based on
    input characteristics and task requirements. An orchestrator analyzes incoming
    requests and selects appropriate architectural components adaptively. Mathematical
    proofs route to symbolic reasoners augmented by transformer hint generation. Creative
    writing tasks route to transformers optimized for fluent generation. Long document
    summarization routes to state space models handling extended contexts. Physical
    manipulation planning routes to world models predicting object dynamics. This
    adaptive specialization requires meta-learning systems that learn which architectures
    excel for particular task distributions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最复杂的集成策略涉及基于输入特性和任务要求的动态路由。协调器分析传入的请求并自适应地选择适当的架构组件。基于数学证明的路由到由转换器提示生成的增强符号推理器。创意写作任务路由到优化流畅生成的转换器。长文档摘要路由到处理扩展上下文的状态空间模型。物理操作规划路由到预测对象动态的世界模型。这种自适应专业化需要元学习系统，这些系统学习哪些架构在特定任务分布中表现卓越。
- en: 'Implementation challenges compound with architectural heterogeneity. Training
    procedures must accommodate different computational patterns: transformers parallelize
    across sequence positions, recurrent models process sequentially, and energy-based
    models require iterative optimization. Gradient computation differs fundamentally:
    transformers backpropagate through deterministic operations, world models backpropagate
    through learned dynamics, and energy-based models require contrastive estimation.
    Framework infrastructure from [Chapter 7](ch013.xhtml#sec-ai-frameworks) must
    evolve to support these diverse training paradigms within unified pipelines.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 实施挑战与架构异构性叠加。训练程序必须适应不同的计算模式：Transformer在序列位置上并行化，循环模型按顺序处理，基于能量的模型需要迭代优化。梯度计算存在根本差异：Transformer通过确定性操作进行反向传播，世界模型通过学习到的动态进行反向传播，基于能量的模型需要对比估计。来自[第7章](ch013.xhtml#sec-ai-frameworks)的框架基础设施必须发展以支持这些多样化的训练范例在统一管道内。
- en: Hardware acceleration presents similar challenges. Transformers map efficiently
    to GPU tensor cores optimized for dense matrix multiplication. State space models
    benefit from sequential processing engines with optimized memory access patterns.
    Energy-based models require optimization hardware accelerating iterative refinement.
    Compound systems must orchestrate computation across heterogeneous accelerators,
    routing different architectural components to appropriate hardware substrates
    while minimizing data movement overhead.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件加速也面临着类似的挑战。Transformer可以高效地映射到针对密集矩阵乘法优化的GPU张量核心。状态空间模型受益于具有优化内存访问模式的顺序处理引擎。基于能量的模型需要优化硬件来加速迭代细化。复合系统必须在异构加速器之间协调计算，将不同的架构组件路由到适当的硬件基础之上，同时最小化数据移动开销。
- en: Deployment and monitoring infrastructure must track diverse failure modes across
    architectural components. Transformer failures typically manifest as fluency degradation
    or factual errors. Energy-based model failures appear as optimization convergence
    issues or constraint violations. World model failures show as incorrect causal
    predictions or planning breakdowns. Observability systems from [Chapter 13](ch019.xhtml#sec-ml-operations)
    must detect and diagnose failures across these different failure semantics, requiring
    architectural-specific monitoring strategies within unified operational frameworks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 部署和监控基础设施必须跟踪架构组件中的各种故障模式。Transformer的故障通常表现为流畅度下降或事实错误。基于能量的模型故障表现为优化收敛问题或约束违规。世界模型故障表现为错误的因果预测或计划崩溃。来自[第13章](ch019.xhtml#sec-ml-operations)的可观察性系统必须检测和诊断这些不同故障语义中的故障，需要在统一操作框架内采用针对特定架构的监控策略。
- en: The compound AI systems framework from [Section 20.3](ch026.xhtml#sec-agi-systems-compound-ai-systems-framework-2a31)
    provides organizing principles for managing this architectural heterogeneity.
    By treating each paradigm as a specialized component with well defined interfaces,
    compound systems enable architectural diversity while maintaining system coherence.
    The following sections on training methodologies, infrastructure requirements,
    and operational practices apply across these architectural paradigms, though specific
    implementations vary based on computational substrate.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[第20.3节](ch026.xhtml#sec-agi-systems-compound-ai-systems-framework-2a31)的复合人工智能系统框架为管理这种架构异构性提供了组织原则。通过将每个范例视为具有良好定义接口的专用组件，复合系统能够在保持系统一致性的同时实现架构多样性。以下关于训练方法、基础设施要求和操作实践的章节适用于这些架构范例，尽管具体实现取决于计算基础。
- en: Training Methodologies for Compound Systems
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复合系统的训练方法
- en: The development of compound systems requires sophisticated training methodologies
    that go beyond traditional machine learning approaches. Training systems with
    multiple specialized components while ensuring alignment with human values and
    intentions requires sophisticated approaches. Reinforcement learning from human
    feedback can be applied to compound architectures, and continuous learning enables
    these systems to improve through deployment and interaction.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 复合系统的发展需要超越传统机器学习方法的复杂训练方法。在确保与人类价值观和意图对齐的同时，训练具有多个专用组件的系统需要复杂的方法。从人类反馈中进行的强化学习可以应用于复合架构，而持续学习使这些系统能够通过部署和交互来改进。
- en: Alignment Across Components
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 组件间的对齐
- en: 'Compound systems face an alignment challenge that builds upon responsible AI
    principles ([Chapter 17](ch023.xhtml#sec-responsible-ai)) while extending beyond
    current safety frameworks to address systems that may exceed human capabilities:
    each specialized component must align with human values while the orchestrator
    must coordinate these components appropriately. Traditional supervised learning
    creates a mismatch where models trained on internet text learn to predict what
    humans write, not what humans want. GPT-3 completions for sensitive historical
    prompts varied significantly, with some evaluations showing concerning outputs
    in a minority of cases, accurately reflecting web content distribution rather
    than truth.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 复合系统面临的对齐挑战建立在负责任的AI原则([第17章](ch023.xhtml#sec-responsible-ai))之上，同时超越了当前的安全框架，以解决可能超出人类能力范围的系统：每个专门的组件必须与人类价值观保持一致，而协调者必须适当地协调这些组件。传统的监督学习在模型上产生了不匹配，这些模型是在互联网文本上训练的，学会预测人类会写什么，而不是人类想要什么。GPT-3对敏感历史提示的完成结果差异很大，一些评估显示在少数情况下有令人担忧的输出，准确地反映了网络内容分布而不是真相。
- en: 'For compound systems, misalignment in any component can compromise the entire
    system: a search component that retrieves biased information, a reasoning component
    that perpetuates harmful stereotypes, or a safety filter that fails to catch problematic
    content.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复合系统，任何组件的不对齐都可能损害整个系统：一个检索有偏见信息的搜索组件，一个持续有害刻板印象的推理组件，或者一个未能捕捉到问题内容的过滤安全组件。
- en: Human Feedback for Component Training
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 组件训练的人类反馈
- en: Addressing these alignment challenges, Reinforcement Learning from Human Feedback
    (RLHF) ([Christiano et al. 2017](ch058.xhtml#ref-christiano2017deep); [Ouyang
    et al. 2022](ch058.xhtml#ref-ouyang2022training)) addresses alignment through
    multi-stage training that compounds naturally to system-level alignment. Rather
    than training on text prediction alone, RLHF creates specialized components within
    the training pipeline itself.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些对齐挑战，从人类反馈中学习强化学习（RLHF）([Christiano 等人 2017](ch058.xhtml#ref-christiano2017deep);
    [Ouyang 等人 2022](ch058.xhtml#ref-ouyang2022training)）通过多阶段训练来解决对齐问题，这种训练自然地累积到系统级对齐。RLHF不仅仅是在文本预测上进行训练，而是在训练流程本身中创建了专门的组件。
- en: The process exemplifies compound systems design through three distinct stages,
    each with specific technical requirements. Stage 1 begins with supervised fine-tuning
    on high-quality demonstrations. Human annotators write example responses to prompts
    demonstrating desired behavior, providing approximately 10,000-100,000 demonstrations
    across diverse tasks. This initial fine-tuning transforms a base language model
    (trained purely on text prediction) into an instruction-following assistant, though
    without understanding human preferences for different response qualities.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程通过三个不同的阶段展示了复合系统设计，每个阶段都有具体的技术要求。第一阶段从对高质量演示的监督微调开始。人类注释者编写示例响应，以提示展示所需的行为，为各种任务提供大约10,000-100,000个演示。这种初始微调将基础语言模型（仅基于文本预测训练）转变为遵循指令的助手，尽管它并不理解人类对不同响应质量的不同偏好。
- en: Stage 2 collects comparative feedback to train a reward model. Rather than rating
    responses on absolute scales (difficult for humans to calibrate consistently),
    annotators compare multiple model outputs for the same prompt, selecting which
    response better satisfies criteria like helpfulness, harmlessness, and honesty.
    The system generates 4-10 candidate responses per prompt, with humans ranking
    or doing pairwise comparisons. From these comparisons, a separate reward model
    learns to predict human preferences, mapping any response to a scalar reward score
    estimating human judgment. This reward model achieves approximately 70-75% agreement
    with held-out human preferences, providing automated quality assessment without
    requiring human evaluation of every output.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段收集比较反馈来训练奖励模型。而不是在绝对尺度上对响应进行评分（对人类来说难以一致校准），注释者会比较同一提示下的多个模型输出，选择哪个响应更好地满足如有用性、无害性和诚实性等标准。系统为每个提示生成4-10个候选响应，由人类进行排名或成对比较。从这些比较中，一个独立的奖励模型学习预测人类偏好，将任何响应映射到一个标量奖励分数，该分数估计了人类的判断。这个奖励模型与保留的人类偏好达到约70-75%的一致性，提供了自动化的质量评估，无需对每个输出进行人工评估。
- en: 'Stage 3 applies reinforcement learning to optimize policy using the learned
    reward model. Proximal Policy Optimization (PPO) ([Schulman et al. 2017](ch058.xhtml#ref-schulman2017proximal))
    fine-tunes the language model to maximize expected reward while preventing excessive
    deviation from the supervised fine-tuned initialization through KL divergence
    penalties. This constraint proves critical: without it, models exploit reward
    model weaknesses, generating nonsensical outputs that fool the reward predictor
    but fail true human judgment. The KL penalty β controls this trade-off, typically
    set to 0.01-0.1, allowing meaningful improvement while maintaining coherent outputs.
    Each reinforcement learning step generates responses, computes rewards, and updates
    policy gradients, iterating until convergence ([Figure 20.3](ch026.xhtml#fig-rlhf-pipeline)).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 第3阶段应用强化学习，使用学习到的奖励模型来优化策略。近端策略优化（PPO）([Schulman et al. 2017](ch058.xhtml#ref-schulman2017proximal))
    微调语言模型以最大化预期奖励，同时通过KL散度惩罚防止过度偏离监督微调的初始化。这个约束至关重要：没有它，模型会利用奖励模型的弱点，生成无意义的输出，这些输出会欺骗奖励预测器，但无法通过真正的人类判断。KL惩罚β控制这个权衡，通常设置为0.01-0.1，允许有意义的改进同时保持连贯的输出。每个强化学习步骤生成响应，计算奖励，并更新策略梯度，迭代直到收敛([图20.3](ch026.xhtml#fig-rlhf-pipeline))。
- en: '![](../media/file323.svg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file323.svg)'
- en: 'Figure 20.3: **RLHF Training Pipeline**: The three-stage process transforms
    base language models into aligned assistants. Stage 1 uses human demonstrations
    for initial fine-tuning. Stage 2 collects human preferences to train a reward
    model. Stage 3 applies reinforcement learning (PPO) to optimize for human preferences
    while preventing mode collapse through KL divergence penalties.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.3：**RLHF训练流程**：三个阶段的过程将基础语言模型转化为对齐助手。第1阶段使用人类演示进行初始微调。第2阶段收集人类偏好以训练奖励模型。第3阶段应用强化学习（PPO）以优化人类偏好，同时通过KL散度惩罚防止模式崩溃。
- en: 'The engineering complexity of [Figure 20.3](ch026.xhtml#fig-rlhf-pipeline)
    is substantial. Each stage requires distinct infrastructure: Stage 1 needs demonstration
    collection systems, Stage 2 demands ranking interfaces that present multiple outputs
    side-by-side, and Stage 3 requires careful hyperparameter tuning to prevent the
    policy from diverging too far from the original model (the KL penalty shown).
    The feedback loop at the bottom represents continuous iteration, with models often
    going through multiple rounds of RLHF, each round requiring fresh human data to
    prevent overfitting to the reward model.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[图20.3](ch026.xhtml#fig-rlhf-pipeline) 的工程复杂性相当大。每个阶段都需要不同的基础设施：第1阶段需要演示收集系统，第2阶段需要能够并排展示多个输出的排名接口，第3阶段则需要仔细调整超参数，以防止策略偏离原始模型太远（如图中所示的KL惩罚）。底部的反馈循环代表持续迭代，模型通常要经过多个RLHF的循环，每个循环都需要新鲜的人类数据以防止对奖励模型过度拟合。'
- en: 'This approach yields significant improvements: InstructGPT ([Ouyang et al.
    2022](ch058.xhtml#ref-ouyang2022training)) with 1.3B parameters outperforms GPT-3
    with 175B parameters in human evaluations[10](#fn10), demonstrating that alignment
    matters more than scale for user satisfaction. For ML engineers, this means that
    investing in alignment infrastructure can be more valuable than scaling compute:
    a 100x smaller aligned model outperforms a larger unaligned one.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法带来了显著的改进：具有1.3B参数的InstructGPT ([Ouyang et al. 2022](ch058.xhtml#ref-ouyang2022training))
    在人类评估中优于具有175B参数的GPT-3[10](#fn10)，这表明对齐比规模对用户满意度更重要。对于机器学习工程师来说，这意味着投资于对齐基础设施可能比扩展计算更有价值：一个100倍更小的对齐模型优于一个更大的未对齐模型。
- en: 'Constitutional AI: Value-Aligned Learning'
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 宪法AI：价值对齐学习
- en: 'Human feedback remains expensive and inconsistent: different annotators provide
    conflicting preferences, and scaling human oversight to billions of interactions
    proves challenging[11](#fn11). Constitutional AI ([Y. Bai et al. 2022](ch058.xhtml#ref-bai2022constitutional))
    addresses these limitations through automated preference learning.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 人类反馈仍然昂贵且不一致：不同的注释者提供相互冲突的偏好，将人类监督扩展到数十亿次的交互非常具有挑战性[11](#fn11)。宪法AI ([Y. Bai
    et al. 2022](ch058.xhtml#ref-bai2022constitutional)) 通过自动偏好学习来解决这些限制。
- en: Instead of human rankings, Constitutional AI uses a set of principles (a “constitution”)
    to guide model behavior[12](#fn12). The model generates responses, critiques its
    own outputs against these principles, and revises responses iteratively. This
    self-improvement loop removes the human bottleneck while maintaining alignment
    objectives.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类排名不同，宪法AI使用一套原则（一个“宪法”）来指导模型行为[12](#fn12)。模型生成响应，对其输出与这些原则进行批评，并迭代地修改响应。这个自我改进循环消除了人类瓶颈，同时保持了对齐目标。
- en: '![](../media/file324.svg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file324.svg)'
- en: 'Figure 20.4: **Constitutional AI Self-Improvement Loop**: The iterative refinement
    process eliminates human feedback bottlenecks. Each cycle evaluates outputs against
    constitutional principles, generates critiques, and produces improved versions.
    After 5 iterations, harmful content reduces by 95% while maintaining helpfulness.
    The final outputs become training data for the next model generation.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.4：**宪法AI自我改进循环**：迭代精炼过程消除了人类反馈瓶颈。每个循环都会将输出与宪法原则进行比较，生成批评，并产生改进版本。经过5次迭代，有害内容减少了95%，同时保持了有用性。最终输出成为下一代模型训练的数据。
- en: The approach leverages optimization techniques from [Chapter 10](ch016.xhtml#sec-model-optimizations)
    by having the model distill its own knowledge through principled self-refinement
    ([Figure 20.4](ch026.xhtml#fig-constitutional-ai)), similar to knowledge distillation
    but guided by constitutional objectives rather than teacher models.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法利用第10章（ch016.xhtml#sec-model-optimizations）中的优化技术，通过模型通过原则性的自我精炼来提炼自己的知识（[图20.4](ch026.xhtml#fig-constitutional-ai)），类似于知识蒸馏，但由宪法目标而不是教师模型引导。
- en: 'Continual Learning: Lifelong Adaptation'
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 持续学习：终身适应
- en: 'Deployed models face a limitation: they cannot learn from user interactions
    without retraining. Each conversation provides valuable feedback (corrections,
    clarifications, new information) but models remain frozen after training[13](#fn13).
    This creates an ever-widening gap between training data and current reality.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的模型面临一个限制：它们无法在不重新训练的情况下从用户交互中学习。每次对话都提供了有价值的反馈（纠正、澄清、新信息），但模型在训练后仍然保持冻结状态[13](#fn13)。这导致训练数据和当前现实之间的差距不断拉大。
- en: 'Continual learning aims to update models from ongoing interactions while preventing
    catastrophic forgetting: the phenomenon where learning new information erases
    previous knowledge[14](#fn14). Standard gradient descent overwrites parameters
    without discrimination, destroying prior learning.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 持续学习旨在在防止灾难性遗忘的同时，从持续交互中更新模型：灾难性遗忘是指学习新信息会抹去先前知识的现象[14](#fn14)。标准的梯度下降法不加区分地覆盖参数，破坏了先前的学习。
- en: Solutions require memory management inspired by [Chapter 14](ch020.xhtml#sec-ondevice-learning)
    that protect important knowledge while enabling new learning. Elastic Weight Consolidation
    (EWC) ([Kirkpatrick et al. 2017](ch058.xhtml#ref-kirkpatrick2017overcoming)) addresses
    this by identifying which neural network parameters were critical for previous
    tasks, then penalizing changes to those specific weights when learning new tasks.
    The technique computes the Fisher Information Matrix to measure parameter importance.
    Parameters with high Fisher information contributed significantly to previous
    performance and should be preserved. Progressive Neural Networks take a different
    approach by adding entirely new pathways for new knowledge while freezing original
    pathways, ensuring previous capabilities remain intact. Memory replay techniques
    periodically rehearse examples from previous tasks during new training, maintaining
    performance through continued practice rather than architectural constraints.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案需要受第14章（ch020.xhtml#sec-ondevice-learning）启发的内存管理，在保护重要知识的同时，使新的学习成为可能。弹性权重巩固（EWC）([Kirkpatrick等人2017](ch058.xhtml#ref-kirkpatrick2017overcoming))通过识别哪些神经网络参数对先前任务至关重要，然后在学习新任务时对这些特定权重的变化进行惩罚来解决这一问题。该技术计算Fisher信息矩阵来衡量参数的重要性。具有高Fisher信息的参数对先前性能的贡献很大，应该被保留。渐进式神经网络采取不同的方法，通过为新的知识添加全新的路径，同时冻结原始路径，确保先前能力保持完整。记忆回放技术在新训练期间定期复习先前任务中的示例，通过持续练习而不是架构约束来维持性能。
- en: These training innovations (alignment through human feedback, principled self-improvement,
    and continual adaptation) transform the training paradigms from [Chapter 8](ch014.xhtml#sec-ai-training)
    into dynamic learning systems that improve through deployment rather than remaining
    static after training.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这些训练创新（通过人类反馈进行对齐、原则性自我改进和持续适应）将训练范式从[第8章](ch014.xhtml#sec-ai-training)转变为动态学习系统，通过部署而不是训练后保持静态来提高性能。
- en: Production Infrastructure for AGI-Scale Systems
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AGI规模系统的生产基础设施
- en: 'The preceding subsections examined novel challenges for AGI: data engineering
    at scale, dynamic architectures, and training paradigms for compound intelligence.
    These represent areas where AGI demands new approaches beyond current practice.
    Three additional building blocks (optimization, hardware, and operations) prove
    equally critical for AGI systems. Rather than requiring entirely new techniques,
    these domains apply and extend the comprehensive frameworks developed in earlier
    chapters.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的子节讨论了针对通用人工智能（AGI）的新挑战：大规模数据工程、动态架构和复合智能的训练范式。这些领域代表了AGI需要超越现有实践的新方法。另外三个构建块（优化、硬件和运营）对于AGI系统同样至关重要。这些领域并非需要全新的技术，而是应用并扩展了在前面章节中开发的综合框架。
- en: 'This section briefly surveys how optimization ([Chapter 10](ch016.xhtml#sec-model-optimizations)),
    hardware acceleration ([Chapter 11](ch017.xhtml#sec-ai-acceleration)), and MLOps
    ([Chapter 13](ch019.xhtml#sec-ml-operations)) evolve for AGI-scale systems. The
    key insight: while the scale and coordination challenges intensify substantially,
    the underlying engineering principles remain consistent with those mastered throughout
    this textbook.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要概述了优化（[第10章](ch016.xhtml#sec-model-optimizations)）、硬件加速（[第11章](ch017.xhtml#sec-ai-acceleration)）和MLOps（[第13章](ch019.xhtml#sec-ml-operations)）在AGI规模系统中的演变。关键洞察：尽管规模和协调挑战显著加剧，但基础工程原理与本书全书中掌握的原则保持一致。
- en: 'Optimization: Dynamic Intelligence Allocation'
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化：动态智能分配
- en: The optimization techniques from [Chapter 10](ch016.xhtml#sec-model-optimizations)
    take on new significance for AGI, evolving from static compression to dynamic
    intelligence allocation across compound system components. Current models waste
    computation by activating all parameters for every input. When GPT-4 answers “2+2=4”,
    it activates the same trillion parameters used for reasoning about quantum mechanics,
    like using a supercomputer for basic arithmetic. AGI systems require selective
    activation based on input complexity to avoid this inefficiency.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[第10章](ch016.xhtml#sec-model-optimizations)中的优化技术对于AGI具有新的意义，从静态压缩演变为在复合系统组件之间动态分配智能。当前模型通过为每个输入激活所有参数来浪费计算。当GPT-4回答“2+2=4”时，它激活了用于量子力学推理的相同万亿参数，就像使用超级计算机进行基本算术一样。AGI系统需要根据输入复杂性进行选择性激活，以避免这种低效。'
- en: 'Mixture-of-experts architectures (explored in [Section 20.4.2.2](ch026.xhtml#sec-agi-systems-expert-routing-compound-systems-0e3e))
    demonstrate one approach to sparse and adaptive computation: routing inputs through
    relevant subsets of model capacity. Extending this principle, adaptive computation
    allocates computational time dynamically based on problem difficulty, spending
    seconds on simple queries but extensive resources on complex reasoning tasks.
    This requires systems engineering for real-time difficulty assessment and graceful
    scaling across computational budgets.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合架构（在[第20.4.2.2节](ch026.xhtml#sec-agi-systems-expert-routing-compound-systems-0e3e)中探讨）展示了一种实现稀疏和自适应计算的方法：通过模型容量的相关子集路由输入。扩展这一原则，自适应计算根据问题难度动态分配计算时间，简单查询只需几秒钟，而复杂推理任务则需大量资源。这需要系统工程来实时评估难度并优雅地跨计算预算进行扩展。
- en: 'Rather than building monolithic models, AGI systems can employ distillation
    cascades where large frontier models teach progressively smaller, specialized
    variants. This mirrors human organizations: junior staff handle routine work while
    senior experts tackle complex problems. The knowledge distillation techniques
    from [Chapter 10](ch016.xhtml#sec-model-optimizations) enable creating model families
    that maintain capabilities while reducing computational requirements for common
    tasks. The systems engineering challenge involves orchestrating these hierarchies
    and routing problems to appropriate computational levels.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与构建单体模型不同，AGI系统可以采用蒸馏级联，其中大型前沿模型逐步教授更小、更专业的变体。这反映了人类组织：初级员工处理常规工作，而高级专家解决复杂问题。第10章（ch016.xhtml#sec-model-optimizations）中的知识蒸馏技术使得创建能够保持能力同时降低常见任务计算需求的模型系列成为可能。系统工程挑战在于协调这些层次并将问题路由到适当的计算级别。
- en: The optimization principles from [Chapter 10](ch016.xhtml#sec-model-optimizations)
    (pruning, quantization, distillation) remain foundational; AGI systems simply
    apply them dynamically across compound architectures rather than statically to
    individual models.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 第10章（ch016.xhtml#sec-model-optimizations）中的优化原则（剪枝、量化、蒸馏）仍然是基础性的；AGI系统只是将它们动态地应用于复合架构，而不是静态地应用于单个模型。
- en: 'Hardware: Scaling Beyond Moore’s Law'
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件：超越摩尔定律的扩展
- en: The hardware acceleration principles from [Chapter 11](ch017.xhtml#sec-ai-acceleration)
    provide foundations, but AGI-scale requirements demand post-Moore’s Law architectures
    as traditional silicon scaling ([Koomey et al. 2011](ch058.xhtml#ref-koomey2011web))
    slows from approximately 30-50% annual transistor density improvements (1970-2010)
    to roughly 10-20% annually (2010-2025)[15](#fn15).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 第11章（ch017.xhtml#sec-ai-acceleration）中的硬件加速原则提供了基础，但AGI规模的需求要求超越摩尔定律的架构，因为传统的硅扩展（[Koomey
    et al. 2011](ch058.xhtml#ref-koomey2011web)）从大约每年30-50%的晶体管密度提升（1970-2010）放缓到大约每年10-20%（2010-2025）[15](#fn15)。
- en: Training GPT-4 class models already requires extensive parallelism coordinating
    thousands of GPUs through the tensor, pipeline, and data parallelism techniques
    from [Chapter 8](ch014.xhtml#sec-ai-training). AGI systems require 100-1000× this
    scale, requiring architectural innovations across multiple fronts.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GPT-4级模型已经需要广泛的并行性，通过第8章（ch014.xhtml#sec-ai-training）中的张量、流水线和数据并行技术协调数千个GPU。AGI系统需要100-1000倍的这种规模，需要多个方面的架构创新。
- en: 3D chip stacking and chiplets build density through vertical integration and
    modular composition rather than horizontal shrinking. Samsung’s 176-layer 3D NAND
    and AMD’s multi-chiplet EPYC processors demonstrate feasibility[16](#fn16). For
    AGI, this enables mixing specialized processors (matrix units, memory controllers,
    networking chips) in optimal ratios while managing thermal challenges through
    advanced cooling.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 3D芯片堆叠和芯片组通过垂直集成和模块化组合来提高密度，而不是水平缩小。三星的176层3D NAND和AMD的多芯片组EPYC处理器证明了可行性[16](#fn16)。对于AGI来说，这允许以最佳比例混合专用处理器（矩阵单元、内存控制器、网络芯片），并通过先进的冷却技术管理热挑战。
- en: Communication and memory bottlenecks require novel solutions through optical
    interconnects and processing-in-memory architectures. Silicon photonics enables
    100 Tbps bandwidth with 10× lower energy than electrical interconnects, critical
    when coordinating 100,000+ processors[17](#fn17). Processing-in-memory reduces
    data movement energy by 100× by computing directly where data resides, addressing
    the memory wall limiting current accelerator efficiency.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通信和内存瓶颈需要通过光互连和内存中处理架构等新颖解决方案。硅光子学可以实现比电互连低10倍的能量，100 Tbps的带宽，这对于协调10万个以上的处理器至关重要[17](#fn17)。内存中处理将数据移动能量降低100倍，通过直接在数据所在位置进行计算，解决了限制当前加速器效率的内存墙问题。
- en: Longer-term pathways emerge through neuromorphic and quantum-hybrid systems.
    Intel’s Loihi ([Mike Davies et al. 2018](ch058.xhtml#ref-davies2018loihi)) and
    IBM’s TrueNorth demonstrate 1000× energy efficiency for event-driven workloads
    through brain-inspired architectures. Quantum-classical hybrids could accelerate
    combinatorial optimization (neural architecture search, hyperparameter tuning)
    while classical systems handle gradient computation[18](#fn18). Programming these
    heterogeneous systems requires sophisticated middleware to decompose AGI workflows
    across different computational paradigms.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过神经形态和量子混合系统，出现了更长期的发展路径。英特尔Loihi([Mike Davies等人 2018](ch058.xhtml#ref-davies2018loihi))和IBM的TrueNorth通过脑启发架构展示了事件驱动工作负载的1000倍能效。量子-经典混合系统可以加速组合优化（神经网络架构搜索、超参数调整），而经典系统处理梯度计算[18](#fn18)。编程这些异构系统需要复杂的中间件来分解AGI工作流程跨越不同的计算范式。
- en: The hardware acceleration principles from [Chapter 11](ch017.xhtml#sec-ai-acceleration)
    (parallelism, memory hierarchy optimization, specialized compute units) remain
    foundational. AGI systems extend these through post-Moore’s Law innovations while
    requiring unprecedented orchestration across heterogeneous architectures.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 第11章[第11章](ch017.xhtml#sec-ai-acceleration)中的硬件加速原则（并行性、内存层次优化、专用计算单元）仍然是基础性的。AGI系统通过摩尔定律之后的创新扩展了这些原则，同时需要跨异构架构前所未有的编排。
- en: 'Operations: Continuous System Evolution'
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 操作：持续系统进化
- en: The MLOps principles from [Chapter 13](ch019.xhtml#sec-ml-operations) become
    critical as AGI systems evolve from static models to dynamic, continuously learning
    entities. Three operational challenges intensify at AGI scale and transform how
    we think about model deployment and maintenance.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AGI系统从静态模型发展到动态、持续学习的实体，MLOps原则[第13章](ch019.xhtml#sec-ml-operations)变得至关重要。在AGI规模下，三个操作挑战加剧，并转变了我们对模型部署和维护的看法。
- en: Continuous learning systems update from user interactions in real-time while
    maintaining safety and reliability. This transforms operations from discrete deployments
    (v1.0, v1.1, v2.0) to continuous evolution where models change constantly. Traditional
    version control, rollback strategies, and reproducibility guarantees require rethinking.
    The operational infrastructure must support live model updates without service
    interruption while maintaining safety invariants, a challenge absent in static
    model deployment covered in [Chapter 13](ch019.xhtml#sec-ml-operations).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 持续学习系统在保持安全性和可靠性的同时，实时地从用户交互中更新。这使操作从离散部署（v1.0、v1.1、v2.0）转变为持续进化，其中模型不断变化。传统的版本控制、回滚策略和可重复性保证需要重新思考。操作基础设施必须支持在不停机的情况下进行实时模型更新，同时保持安全性不变量，这是在[第13章](ch019.xhtml#sec-ml-operations)中涵盖的静态模型部署所不具备的挑战。
- en: Testing and validation grow complex when comparing personalized model variants
    across millions of users. Traditional A/B testing from [Chapter 13](ch019.xhtml#sec-ml-operations)
    assumes consistent experiences per variant; AGI systems introduce complications
    where each user may receive a slightly different model. Emergent behaviors can
    appear suddenly as capabilities scale, requiring detection of subtle performance
    regressions across diverse use cases. The monitoring and observability principles
    from [Chapter 13](ch019.xhtml#sec-ml-operations) provide foundations but must
    extend to detect capability changes rather than just performance metrics.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较数百万用户的个性化模型变体时，测试和验证变得复杂。第13章[第13章](ch019.xhtml#sec-ml-operations)中的传统A/B测试假设每个变体都有一致的经验；AGI系统引入了复杂性，其中每个用户可能收到一个略有不同的模型。随着能力的扩展，突然出现的涌现行为可能需要检测不同用例中的微妙性能退化。第13章[第13章](ch019.xhtml#sec-ml-operations)中的监控和可观察性原则提供了基础，但必须扩展以检测能力变化，而不仅仅是性能指标。
- en: Safety monitoring demands real-time detection of harmful outputs, prompt injections,
    and adversarial attacks across billions of interactions. Unlike traditional software
    monitoring tracking system metrics (latency, throughput, error rates), AI safety
    monitoring requires understanding semantic content, user intent, and potential
    harm. This necessitates new tooling combining the robustness principles from [Chapter 16](ch022.xhtml#sec-robust-ai),
    security practices from [Chapter 15](ch021.xhtml#sec-security-privacy), and responsible
    AI frameworks from [Chapter 17](ch023.xhtml#sec-responsible-ai). The operational
    challenge involves deploying these safety systems at scale while maintaining sub-second
    response times.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 安全监控需要实时检测有害输出、及时注入和对抗攻击，这些在数十亿次的交互中发生。与传统的软件监控跟踪系统指标（延迟、吞吐量、错误率）不同，AI安全监控需要理解语义内容、用户意图和潜在危害。这需要结合来自[第16章](ch022.xhtml#sec-robust-ai)的鲁棒性原则、来自[第15章](ch021.xhtml#sec-security-privacy)的安全实践和来自[第17章](ch023.xhtml#sec-responsible-ai)的有责任感的AI框架的新工具。运营挑战在于大规模部署这些安全系统的同时，保持亚秒级的响应时间。
- en: The MLOps principles from [Chapter 13](ch019.xhtml#sec-ml-operations) (CI/CD,
    monitoring, incident response) remain essential; AGI systems simply apply them
    to continuously evolving, personalized models requiring semantic rather than purely
    metric-based validation.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[第13章](ch019.xhtml#sec-ml-operations)（持续集成/持续部署、监控、事件响应）的MLOps原则仍然至关重要；AGI系统只是将它们应用于持续演变、个性化的模型，这些模型需要基于语义而非纯粹基于指标的有效性验证。
- en: Integrated System Architecture Design
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成系统架构设计
- en: The six building blocks examined (data engineering, dynamic architectures, training
    paradigms, optimization, hardware, and operations) must work in concert for compound
    AI systems, but integration proves far more challenging than simply assembling
    components. Successful architectures require carefully designed interfaces, coordinated
    optimization across layers, and holistic understanding of how building blocks
    interact to create emergent capabilities or cascade failures.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 被考察的六个构建块（数据工程、动态架构、训练范式、优化、硬件和运营）必须协同工作，以实现复合AI系统，但集成比简单地组装组件更具挑战性。成功的架构需要精心设计的接口、跨层的协调优化，以及全面理解构建块如何相互作用以创建涌现能力或级联故障。
- en: 'Consider data flow through an integrated compound system serving a complex
    user query. Novel data engineering pipelines from [Section 20.4.1](ch026.xhtml#sec-agi-systems-data-engineering-scale-91a0)
    continuously generate synthetic training examples, curate web-scale corpora, and
    enable self-play learning that produce specialized training datasets for different
    components. These datasets feed into dynamic architectures from [Section 20.4.2](ch026.xhtml#sec-agi-systems-dynamic-architectures-compound-systems-fca0)
    where mixture-of-experts models route different aspects of queries to specialized
    components: mathematical reasoning to quantitative experts, creative writing to
    language specialists, code generation to programming-focused modules. Each expert
    was trained using methodologies from [Section 20.6](ch026.xhtml#sec-agi-systems-training-methodologies-compound-systems-e3fa)
    including RLHF alignment, constitutional AI self-improvement, and continual learning
    that adapts to user feedback. Optimization techniques from [Section 20.6.1.1](ch026.xhtml#sec-agi-systems-optimization-dynamic-intelligence-allocation-369a)
    enable deploying these components efficiently through quantization reducing memory
    footprints, pruning eliminating redundant parameters, and distillation transferring
    knowledge to smaller deployment models. This optimized model ensemble runs on
    heterogeneous hardware from [Section 20.6.1.2](ch026.xhtml#sec-agi-systems-hardware-scaling-beyond-moores-law-5e96)
    combining GPU clusters for transformer inference, neuromorphic chips for event-driven
    perception, and specialized accelerators for symbolic reasoning. Finally, evolved
    MLOps from [Section 20.6.1.3](ch026.xhtml#sec-agi-systems-operations-continuous-system-evolution-ed9b)
    monitors this complex deployment through semantic validation, handles component
    failures gracefully, and supports continuous learning updates without service
    interruption.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑通过一个集成复合系统处理复杂用户查询时的数据流。来自[第20.4.1节](ch026.xhtml#sec-agi-systems-data-engineering-scale-91a0)的新颖数据工程管道持续生成合成训练示例，整理网络规模语料库，并启用自我玩耍学习，为不同组件生成专门的训练数据集。这些数据集输入到来自[第20.4.2节](ch026.xhtml#sec-agi-systems-dynamic-architectures-compound-systems-fca0)的动态架构中，其中混合专家模型将查询的不同方面路由到专门的组件：数学推理到定量专家，创意写作到语言专家，代码生成到编程模块。每个专家都使用来自[第20.6节](ch026.xhtml#sec-agi-systems-training-methodologies-compound-systems-e3fa)的方法进行训练，包括RLHF对齐、宪法AI自我改进和持续学习，以适应用户反馈。来自[第20.6.1.1节](ch026.xhtml#sec-agi-systems-optimization-dynamic-intelligence-allocation-369a)的优化技术通过量化减少内存占用、剪枝消除冗余参数和蒸馏将知识转移到更小的部署模型，使这些组件能够高效部署。这个优化模型集在来自[第20.6.1.2节](ch026.xhtml#sec-agi-systems-hardware-scaling-beyond-moores-law-5e96)的异构硬件上运行，结合GPU集群进行Transformer推理、神经形态芯片进行事件驱动感知和针对符号推理的专用加速器。最后，来自[第20.6.1.3节](ch026.xhtml#sec-agi-systems-operations-continuous-system-evolution-ed9b)的进化MLOps通过语义验证监控这个复杂的部署，优雅地处理组件故障，并支持无服务中断的持续学习更新。
- en: 'The critical insight: these building blocks cannot be developed in isolation.
    Data engineering decisions constrain which architectural patterns prove feasible;
    model architectures determine optimization opportunities; hardware capabilities
    bound achievable performance; operational requirements feed back to influence
    architectural choices. This creates a tightly coupled design space where co-optimization
    across building blocks often yields greater improvements than optimizing any single
    component.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 关键洞见：这些构建块不能孤立地发展。数据工程决策限制了哪些架构模式是可行的；模型架构决定了优化机会；硬件能力限制了可达到的性能；运营需求反馈影响架构选择。这创造了一个紧密耦合的设计空间，其中跨构建块的协同优化往往比优化任何单个组件带来更大的改进。
- en: 'Concretely, three integration patterns emerged from production compound systems,
    each representing different trade-offs in the building block design space. The
    horizontal integration pattern distributes specialized components across a shared
    infrastructure layer. All components access common data pipelines, deploy on homogeneous
    hardware clusters, and integrate through standardized APIs. This pattern maximizes
    resource sharing and operational simplicity but limits per-component optimization.
    Google’s Gemini exemplifies this approach: multimodal encoders, reasoning modules,
    and tool integrations all run on TPU clusters, sharing training infrastructure
    and deployment frameworks. The advantage lies in operational efficiency: one team
    manages the infrastructure serving all components. The limitation manifests when
    component-specific optimizations (neuromorphic hardware for vision, symbolic accelerators
    for logic) cannot be leveraged within the homogeneous substrate.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，从生产复合系统中出现了三种整合模式，每种模式都代表了构建块设计空间中的不同权衡。横向整合模式将专业组件分布在共享基础设施层。所有组件访问公共数据管道，在同质硬件集群上部署，并通过标准化API进行集成。这种模式最大化了资源共享和操作简单性，但限制了每个组件的优化。Google的Gemini是这种方法的例证：多模态编码器、推理模块和工具集成都在TPU集群上运行，共享训练基础设施和部署框架。其优势在于操作效率：一个团队管理着服务于所有组件的基础设施。当组件特定的优化（用于视觉的神经形态硬件、用于逻辑的符号加速器）无法在均匀的基材中利用时，这种限制就会显现出来。
- en: 'The vertical integration pattern customizes the entire stack for each specialized
    component. A reasoning component might train on synthetic data from formal logic
    generators, use energy-based architectures optimized for constraint satisfaction,
    deploy on quantum-classical hybrid hardware accelerating combinatorial search,
    and include custom verification in its operational monitoring. A separate vision
    component trains on self-supervised video prediction, uses convolutional or vision
    transformer architectures, deploys on neuromorphic chips for efficient event processing,
    and monitors for distribution shift in visual inputs. This pattern enables maximal
    per-component optimization at the cost of operational complexity managing heterogeneous
    systems. Meta’s approach with different specialized models for different modalities
    and tasks exemplifies vertical integration: each capability area receives custom
    treatment across the entire stack.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直整合模式为每个专业组件定制整个堆栈。一个推理组件可能会在形式逻辑生成器的合成数据上训练，使用基于能量的架构以优化约束满足，部署在量子经典混合硬件上加速组合搜索，并在其操作监控中包含定制验证。一个独立的视觉组件在自监督视频预测上训练，使用卷积或视觉转换器架构，部署在神经形态芯片上进行高效的事件处理，并监控视觉输入中的分布变化。这种模式以管理异构系统的操作复杂性为代价，实现了每个组件的最大优化。Meta针对不同模态和任务采用不同专用模型的方法，是垂直整合的例证：每个能力领域在整个堆栈中都得到定制处理。
- en: 'The hierarchical integration pattern combines horizontal and vertical approaches
    through layered abstraction. Lower layers provide shared infrastructure (data
    pipelines, training clusters, deployment platforms) while higher layers enable
    component-specific customization (architectural choices, optimization strategies,
    operational policies). Foundation model providers exemplify this: they offer base
    models trained on massive infrastructure (horizontal), which developers fine-tune
    with custom data and optimization (vertical), deployed on shared serving infrastructure
    (horizontal), with custom monitoring and guardrails (vertical). This pattern balances
    operational efficiency with optimization flexibility but introduces complexity
    at abstraction boundaries where the shared infrastructure must accommodate diverse
    customization needs.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 层次整合模式通过分层抽象结合了横向和纵向方法。底层提供共享基础设施（数据管道、训练集群、部署平台），而高层则实现组件特定的定制（架构选择、优化策略、操作策略）。基础模型提供商是这种模式的例证：他们提供基于大规模基础设施训练的基础模型（横向），开发者使用定制数据和优化进行微调（纵向），部署在共享服务基础设施上（横向），并具有定制监控和护栏（纵向）。这种模式在操作效率和优化灵活性之间取得平衡，但在抽象边界引入了复杂性，因为共享基础设施必须满足多样化的定制需求。
- en: Choosing among these patterns requires understanding system requirements and
    organizational capabilities. Horizontal integration suits organizations with strong
    infrastructure teams but limited AI specialization, accepting some performance
    sacrifice for operational simplicity. Vertical integration benefits organizations
    with deep AI expertise across multiple domains, able to manage complexity for
    maximal performance. Hierarchical integration serves platforms supporting diverse
    use cases, providing standard infrastructure while enabling customization.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些模式之间进行选择需要理解系统需求和组织能力。横向整合适合拥有强大基础设施团队但AI专业能力有限的组织，为了运营简单，可以接受一些性能上的牺牲。纵向整合有利于在多个领域拥有深厚AI专业知识的组织，能够管理复杂性以实现最大性能。分层整合服务于支持多种用例的平台，提供标准基础设施同时允许定制。
- en: 'The engineering challenge intensifies with scale. A research prototype might
    manually integrate building blocks through ad-hoc scripts and configuration files.
    Production systems serving millions of users require robust integration frameworks:
    declarative specifications defining how components interact, automated deployment
    pipelines validating cross-building-block consistency, monitoring systems detecting
    integration failures, and update mechanisms coordinating changes across building
    blocks without breaking dependencies. These frameworks themselves become substantial
    engineering artifacts, often rivaling individual building blocks in complexity.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 随着规模的扩大，工程挑战加剧。一个研究原型可能通过临时脚本和配置文件手动集成构建块。服务于数百万用户的生产系统需要强大的集成框架：声明性规范定义组件如何交互，自动部署管道验证跨构建块的一致性，监控系统检测集成故障，以及更新机制协调构建块之间的变化，而不会破坏依赖关系。这些框架本身成为重要的工程成果，通常在复杂性上与单个构建块相媲美。
- en: Critically, the engineering principles developed throughout this textbook provide
    foundations for all six building blocks. AGI development extends rather than replaces
    these principles, applying them at unprecedented scale and coordination complexity.
    The data engineering principles from [Chapter 6](ch012.xhtml#sec-data-engineering)
    scale to petabyte corpora. The distributed training techniques from [Chapter 8](ch014.xhtml#sec-ai-training)
    coordinate million-GPU clusters. The optimization methods from [Chapter 10](ch016.xhtml#sec-model-optimizations)
    enable trillion-parameter deployment. The operational practices from [Chapter 13](ch019.xhtml#sec-ml-operations)
    ensure reliable compound system operation. AGI systems engineering builds incrementally
    upon these foundations rather than requiring revolutionary new approaches, though
    the scale and coordination demands push existing techniques to their limits and
    sometimes beyond.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，本书中开发出的工程原理为所有六个构建块提供了基础。AGI 的发展是扩展而不是取代这些原理，它们在前所未有的规模和协调复杂性上应用这些原理。来自[第6章](ch012.xhtml#sec-data-engineering)的数据工程原理适用于PB级语料库。来自[第8章](ch014.xhtml#sec-ai-training)的分布式训练技术协调百万GPU集群。来自[第10章](ch016.xhtml#sec-model-optimizations)的优化方法使万亿参数部署成为可能。来自[第13章](ch019.xhtml#sec-ml-operations)的运营实践确保了可靠的多系统操作。AGI
    系统工程是在这些基础上逐步构建的，而不是需要革命性的新方法，尽管规模和协调需求将现有技术推向极限，有时甚至超越极限。
- en: Production Deployment of Compound AI Systems
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复合AI系统的生产部署
- en: 'The preceding sections established the building blocks required for compound
    AI systems: novel data sources and training paradigms, architectural alternatives
    addressing transformer limitations, and infrastructure supporting heterogeneous
    components. These building blocks provide the raw materials for AGI development.
    This section examines how to assemble these materials into functioning systems
    through orchestration patterns that coordinate specialized components at production
    scale.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的章节建立了复合AI系统所需的构建块：新颖的数据来源和训练范式、解决Transformer限制的架构替代方案以及支持异构组件的基础设施。这些构建块为AGI开发提供了原材料。本节将探讨如何通过协调生产规模下专用组件的编排模式，将这些材料组装成功能系统。
- en: The compound AI systems framework provides the conceptual foundation, but implementing
    these systems at scale requires sophisticated orchestration infrastructure. Production
    systems like GPT-4 ([OpenAI et al. 2023](ch058.xhtml#ref-openai2023gpt4)) tool
    integration, Gemini ([G. Team et al. 2023](ch058.xhtml#ref-team2023gemini)) search
    augmentation, and Claude’s constitutional AI ([Y. Bai et al. 2022](ch058.xhtml#ref-bai2022constitutional))
    implementation demonstrate how specialized components coordinate to achieve capabilities
    beyond individual model limits. The engineering complexity involves managing component
    interactions, handling failures gracefully, and maintaining system coherence as
    components evolve independently. Understanding these implementation patterns bridges
    the gap between conceptual frameworks and operational reality.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 复合人工智能系统框架提供了概念基础，但在规模上实施这些系统需要复杂的协调基础设施。如GPT-4 ([OpenAI等，2023](ch058.xhtml#ref-openai2023gpt4))
    工具集成、Gemini ([G. Team等，2023](ch058.xhtml#ref-team2023gemini)) 搜索增强和Claude的宪法人工智能
    ([Y. Bai等，2022](ch058.xhtml#ref-bai2022constitutional)) 实施展示了如何通过专用组件的协调来实现超越单个模型限制的能力。工程复杂性包括管理组件交互、优雅处理故障以及随着组件独立演变而保持系统一致性。理解这些实施模式弥合了概念框架和运营现实之间的差距。
- en: '[Figure 20.5](ch026.xhtml#fig-compound-ai-system) illustrates the engineering
    complexity with specific performance metrics: the central orchestrator routes
    user queries to appropriate specialized modules within 10-50 ms decision latency,
    manages bidirectional communication between components through 1-10 GB/s data
    flows depending on modality (text: 1 MB/s, code: 10 MB/s, multimodal: 1 GB/s),
    coordinates iterative refinement processes with 100-500 ms round-trip times per
    component, and maintains conversation state across the entire interaction using
    1-100 GB memory per session. Each component represents distinct engineering challenges
    requiring different optimization strategies (LLM: GPU-optimized inference, Search:
    distributed indexing, Code: secure sandboxing), hardware configurations (orchestrator:
    CPU+memory, retrieval: SSD+bandwidth, compute: GPU clusters), and operational
    practices (sub-second latency SLAs, 99.9% availability, failure isolation). Failure
    modes include component timeouts (10-30 second fallbacks), dependency failures
    (graceful degradation), and coordination deadlocks (circuit breaker patterns).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[图20.5](ch026.xhtml#fig-compound-ai-system)展示了使用特定性能指标来体现的工程复杂性：中心协调器在10-50毫秒的决策延迟内将用户查询路由到适当的专用模块，通过1-10
    GB/s的数据流（根据模式不同：文本：1 MB/s，代码：10 MB/s，多模态：1 GB/s）管理组件间的双向通信，每个组件的迭代优化过程需要100-500毫秒的往返时间，并且每个会话使用1-100
    GB的内存来维护整个交互的会话状态。每个组件代表不同的工程挑战，需要不同的优化策略（LLM：GPU优化的推理，搜索：分布式索引，代码：安全沙箱），硬件配置（协调器：CPU+内存，检索：SSD+带宽，计算：GPU集群），以及操作实践（亚秒级延迟SLA，99.9%可用性，故障隔离）。故障模式包括组件超时（10-30秒的回退），依赖失败（优雅降级），以及协调死锁（断路器模式）。'
- en: '![](../media/file325.svg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file325.svg)'
- en: 'Figure 20.5: **Compound AI System Architecture**: Modern AI assistants integrate
    specialized components through a central orchestrator, enabling capabilities beyond
    monolithic models. Each module handles specific tasks while the LLM coordinates
    information flow, decisions, and responses. This architecture enables independent
    scaling, specialized optimization, and multi-layer safety validation.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.5：**复合人工智能系统架构**：现代人工智能助手通过中心协调器整合专用组件，使能力超越单体模型。每个模块处理特定任务，而LLM协调信息流、决策和响应。这种架构实现了独立扩展、专用优化和多层安全验证。
- en: Orchestration Patterns for Production Systems
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生产系统的协调模式
- en: Implementing compound AI systems at production scale requires sophisticated
    orchestration patterns that coordinate specialized components while maintaining
    reliability and performance. Three fundamental patterns emerged from production
    deployments at organizations like OpenAI, Anthropic, and Google, each addressing
    different aspects of component coordination.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产规模上实施复合人工智能系统需要复杂的协调模式，这些模式在协调专用组件的同时保持可靠性和性能。来自OpenAI、Anthropic和Google等组织的生产部署中出现了三种基本模式，每种模式都针对组件协调的不同方面。
- en: 'The request routing pattern determines which components process each user query
    based on intent classification and capability requirements. When a user asks “What’s
    the weather in Tokyo?”, the orchestrator analyzes the request structure, identifies
    required capabilities (web search for real-time data, location resolution, unit
    conversion), and routes to appropriate components. This routing happens in two
    stages: coarse-grained classification using a small, fast model (10-50ms latency)
    determines broad categories (factual query, creative task, code generation, multimodal
    request), followed by fine-grained routing that selects specific component configurations.
    GPT-4’s tool use implementation exemplifies this: the base model generates function
    calls as structured JSON, a validation layer checks schema compliance, the execution
    engine invokes external APIs with timeout protection, and result integration merges
    outputs back into conversation context. The routing layer maintains a capability
    registry mapping intents to component combinations, updated dynamically as new
    components deploy or existing ones prove unreliable.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 请求路由模式根据意图分类和能力需求确定哪些组件处理每个用户查询。当用户询问“东京的天气怎么样？”时，协调器分析请求结构，识别所需的能力（实时数据网络搜索、位置解析、单位转换），并将请求路由到适当的组件。这种路由分为两个阶段：使用小型快速模型（10-50毫秒延迟）进行粗粒度分类，确定广泛类别（事实查询、创意任务、代码生成、多模态请求），然后是细粒度路由，选择特定的组件配置。GPT-4的工具使用实现展示了这一点：基础模型生成结构化JSON的功能调用，验证层检查模式合规性，执行引擎调用外部API并具有超时保护，结果集成将输出合并回对话上下文中。路由层维护一个能力注册表，将意图映射到组件组合，随着新组件的部署或现有组件的不稳定而动态更新。
- en: 'Component coordination becomes critical when multiple specialized modules must
    work together. The orchestration state machine pattern manages multi-step workflows
    where outputs from one component inform inputs to subsequent components. Consider
    a research query requiring synthesis across multiple sources: the orchestrator
    (1) decomposes the question into sub-queries addressing different aspects, (2)
    dispatches parallel searches across knowledge bases, (3) ranks retrieved passages
    by relevance, (4) feeds top-k passages to the reasoning component with the original
    question, (5) validates generated claims against retrieved evidence, and (6) formats
    the final response with citations. Each transition between stages requires state
    management tracking intermediate results, handling partial failures, and making
    continuation decisions. The orchestrator maintains workflow state in distributed
    memory (Redis, Memcached) enabling recovery from component failures without restarting
    entire pipelines. State checkpointing occurs after each successful stage, allowing
    restart from the last consistent state when components timeout or return errors.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当多个专用模块必须协同工作时，组件协调变得至关重要。协调器状态机模式管理多步骤工作流程，其中某个组件的输出通知后续组件的输入。考虑一个需要跨多个来源综合的研究查询：协调器（1）将问题分解为针对不同方面的子查询，（2）在知识库中并行搜索，（3）按相关性对检索到的段落进行排序，（4）将排名前k的段落反馈给包含原始问题的推理组件，（5）验证生成的声明与检索到的证据的一致性，（6）用引文格式化最终响应。每个阶段之间的过渡都需要状态管理跟踪中间结果，处理部分失败，并做出继续决策。协调器在分布式内存（Redis、Memcached）中维护工作流程状态，使组件失败时无需重启整个管道即可恢复。在每个成功阶段之后进行状态检查点，允许在组件超时或返回错误时从最后一个一致状态重新启动。
- en: 'Error handling and resilience patterns prove essential as component counts
    increase. The circuit breaker pattern prevents cascading failures when components
    become unreliable. When a knowledge retrieval component begins timing out due
    to database overload, the circuit breaker tracks failure rates and automatically
    disables that component after exceeding thresholds (e.g., >30% failures over 60
    seconds). Rather than continuing to overwhelm the failing component, the orchestrator
    routes to fallback strategies: cached responses for common queries, degraded responses
    from the base model alone, or explicit user notification that certain capabilities
    are temporarily unavailable. Circuit state transitions through three phases: closed
    (normal operation), open (failures trigger immediate fallbacks), and half-open
    (periodic testing for recovery). Anthropic’s Claude implementation includes sophisticated
    fallback hierarchies where constitutional AI filters have multiple backup implementations
    at different quality/latency trade-offs, ensuring safety validation even when
    preferred components fail.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组件数量的增加，错误处理和弹性模式变得至关重要。断路器模式可以防止组件不可靠时发生级联故障。当一个知识检索组件由于数据库过载开始超时时，断路器会跟踪失败率，并在超过阈值（例如，60秒内超过30%的失败）后自动禁用该组件。而不是继续压垮失败的组件，协调器会转向回退策略：常见查询的缓存响应、仅从基础模型获得的降级响应，或明确通知用户某些功能暂时不可用。电路状态转换通过三个阶段：关闭（正常操作）、开启（故障触发即时回退）和半开启（定期测试恢复）。Anthropic的Claude实现包括复杂的回退层次结构，其中宪法AI过滤器在不同质量/延迟权衡下有多个备用实现，确保即使在首选组件失败时也能进行安全验证。
- en: 'Production systems implement dynamic component scaling based on load and performance
    characteristics. Different components face different bottlenecks: the base language
    model is compute-bound requiring GPU instances, vector search is memory-bandwidth-bound
    requiring high-IOPS SSDs, and code execution is isolation-bound requiring sandboxed
    containers. The orchestrator monitors component-level metrics (latency distribution,
    throughput, error rates, resource utilization) and signals scaling decisions to
    the deployment infrastructure. When code execution requests spike during peak
    hours, Kubernetes horizontally scales container pools while the orchestrator load-balances
    requests across available instances. This requires sophisticated queuing: high-priority
    requests (paying customers, critical workflows) skip to front of queues while
    batch requests tolerate higher latency. The orchestrator tracks per-user request
    contexts enabling fair scheduling that prevents single users from monopolizing
    shared resources while maintaining quality of service for all users.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 生产系统根据负载和性能特征实现动态组件缩放。不同的组件面临不同的瓶颈：基础语言模型是计算密集型，需要GPU实例；向量搜索是内存带宽密集型，需要高IOPS的SSD；代码执行是隔离密集型，需要沙箱容器。协调器监控组件级指标（延迟分布、吞吐量、错误率、资源利用率）并将缩放决策信号发送到部署基础设施。在高峰时段代码执行请求激增时，Kubernetes水平扩展容器池，而协调器在可用实例之间负载均衡请求。这需要复杂的排队：高优先级请求（付费客户、关键工作流程）跳到队列的前面，而批量请求可以容忍更高的延迟。协调器跟踪每个用户的请求上下文，实现公平调度，防止单个用户垄断共享资源，同时保持所有用户的服务质量。
- en: 'Monitoring and observability become exponentially more complex with compound
    systems. Traditional metrics like latency and throughput prove insufficient when
    failures manifest as semantic degradation rather than hard errors. The system
    might execute successfully (no exceptions thrown, 200 OK responses) yet produce
    poor outputs because retrieval returned irrelevant passages or the reasoning component
    hallucinated connections. Production observability requires semantic monitoring
    tracking content quality alongside system health. This involves multiple validation
    layers: automated fact-checking comparing claims against knowledge bases, consistency
    checking ensuring responses don’t contradict prior statements in conversation,
    safety filtering detecting harmful content generation, and calibration monitoring
    verifying confidence scores match actual accuracy. These validators run asynchronously
    to avoid blocking user responses but feed into continuous quality dashboards enabling
    rapid detection of subtle regressions. When Google’s Bard initially launched,
    semantic monitoring detected that certain query patterns caused increased citation
    errors, triggering investigation revealing retrieval component issues that system
    metrics alone would not have surfaced.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 监控和可观测性在复合系统中变得指数级复杂。当故障表现为语义退化而非硬错误时，传统的指标如延迟和吞吐量证明是不够的。系统可能执行成功（未抛出异常，返回200
    OK响应），但输出质量差，因为检索返回了不相关的段落或推理组件产生了幻觉连接。生产可观测性需要语义监控跟踪内容质量以及系统健康。这涉及到多个验证层：自动事实核查将主张与知识库进行比较，一致性检查确保响应不与对话中的先前陈述相矛盾，安全过滤检测有害内容生成，以及校准监控验证置信度分数与实际准确性相匹配。这些验证器异步运行以避免阻塞用户响应，但它们会输入到持续的质量仪表板中，从而能够快速检测细微的回归。当谷歌的Bard最初推出时，语义监控检测到某些查询模式导致引用错误增加，触发调查揭示了检索组件问题，这些问题仅凭系统指标是不会暴露的。
- en: 'The engineering challenge intensifies with versioning and deployment. In monolithic
    systems, version updates are atomic: deploy new model, route traffic, monitor,
    rollback if necessary. Compound systems have N components evolving independently,
    creating version compatibility complexity. When the base language model updates
    to improve reasoning, does it remain compatible with the existing safety filter
    trained on the old model’s output distribution? Production systems maintain compatibility
    matrices tracking which component versions work together and implement staged
    rollouts that update one component at a time while monitoring for interaction
    regressions. This requires extensive integration testing in staging environments
    that replicate production traffic patterns, A/B testing frameworks comparing compound
    system variants across user cohorts, and automated canary deployment pipelines
    that gradually increase traffic to new configurations while watching for anomalies.
    The operational discipline from [Chapter 13](ch019.xhtml#sec-ml-operations) extends
    to compound systems but with multiplicative complexity: N components create O(N²)
    potential interactions requiring validation.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 随着版本控制和部署，工程挑战加剧。在单体系统中，版本更新是原子的：部署新模型，路由流量，监控，必要时回滚。复合系统有N个组件独立演变，创建了版本兼容性复杂性。当基础语言模型更新以改进推理时，它是否与基于旧模型输出分布训练的安全过滤器保持兼容？生产系统维护兼容性矩阵，跟踪哪些组件版本可以一起工作，并实施分阶段推出，一次更新一个组件同时监控交互回归。这需要在复制生产流量模式的生产环境中进行广泛的集成测试，A/B测试框架比较跨用户群体的复合系统变体，以及自动金丝雀部署管道，逐渐增加新配置的流量同时监控异常。第13章（ch019.xhtml#sec-ml-operations）中的运营纪律扩展到复合系统，但具有乘法复杂性：N个组件创建了O(N²)潜在交互，需要验证。
- en: Remaining Technical Barriers
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剩余的技术障碍
- en: 'The building blocks explored above (data engineering at scale, dynamic architectures,
    alternative paradigms, training methodologies, and infrastructure components)
    represent significant engineering progress toward AGI. Yet an honest assessment
    reveals that these advances, while necessary, remain insufficient. Five critical
    barriers separate current ML systems from artificial general intelligence, each
    representing not just algorithmic challenges but systems engineering problems
    requiring innovation across the entire stack. Understanding these barriers prevents
    overconfidence while guiding research priorities: some barriers may yield to clever
    orchestration of existing building blocks; others demand conceptual innovations
    not yet imagined.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 上文探讨的构建模块（大规模数据工程、动态架构、替代范式、训练方法和基础设施组件）代表了向通用人工智能（AGI）迈出的重大工程进步。然而，诚实的评估表明，这些进步虽然必要，但仍然不足。五个关键障碍将当前的机器学习系统与通用人工智能隔开，每个障碍不仅代表算法挑战，还代表需要在整个堆栈上进行创新的系统工程问题。理解这些障碍可以防止过度自信，同时指导研究重点：一些障碍可能通过巧妙地编排现有构建模块而得到解决；而其他障碍则需要尚未想象的概念创新。
- en: 'Consider concrete failures that reveal the gap: ChatGPT can write code but
    fails to track variable state across a long debugging session. It can explain
    quantum mechanics but cannot learn from user corrections within a conversation.
    It can translate between languages but lacks the cultural context to know when
    literal translation misleads. These represent not minor bugs but fundamental architectural
    limitations interconnecting such that progress on any single barrier proves insufficient.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑具体的失败案例，这些案例揭示了差距：ChatGPT可以编写代码，但在长时间的调试会话中无法跟踪变量状态。它可以解释量子力学，但不能从对话中的用户更正中学习。它可以进行语言翻译，但缺乏文化背景，不知道何时直译会误导。这些问题不仅仅是小错误，而是连接这些系统的基本架构限制，任何单个障碍的进展都证明是不够的。
- en: Memory and Context Limitations
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记忆和上下文限制
- en: 'Human working memory holds approximately seven items, yet long-term memory
    stores lifetime experiences ([Landauer 1986](ch058.xhtml#ref-landauer1986much)).
    Current AI systems invert this: transformer context windows reach 128K tokens
    (approximately 100K words) but cannot maintain information across sessions. This
    creates systems that can process books but cannot remember yesterday’s conversation.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 人类的工作记忆可以容纳大约七个项目，而长期记忆则存储了一生的经验([Landauer 1986](ch058.xhtml#ref-landauer1986much))。当前的AI系统与此相反：transformer上下文窗口达到128K个标记（大约100K个单词），但不能在会话之间保持信息。这导致了可以处理书籍的系统，但不能记住昨天的对话。
- en: The challenge extends beyond storage to organization and retrieval. Human memory
    operates hierarchically (events within days within years) and associatively (smell
    triggering childhood memories). Current systems lack these structures, treating
    all information equally. Vector databases store billions of embeddings but lack
    temporal or semantic organization, while humans retrieve relevant memories from
    decades of experience in milliseconds through associative activation spreading[19](#fn19).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战不仅限于存储，还扩展到了组织和检索。人类的记忆是分层的（日内的事件在年内的层级中）和关联性的（气味触发童年记忆）。当前系统缺乏这些结构，对所有信息一视同仁。向量数据库存储了数十亿个嵌入，但缺乏时间或语义组织，而人类通过关联激活的传播在毫秒内就能从数十年的经验中检索到相关记忆[19](#fn19)。
- en: 'Addressing these memory limitations, building AGI memory systems requires innovations
    from [Chapter 6](ch012.xhtml#sec-data-engineering): hierarchical indexing supporting
    multi-scale retrieval, attention mechanisms that selectively forget irrelevant
    information, and experience consolidation that transfers short-term interactions
    into long-term knowledge. Compound systems may address this through specialized
    memory components with different temporal scales and retrieval mechanisms.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些记忆限制，构建通用人工智能的记忆系统需要从[第6章](ch012.xhtml#sec-data-engineering)中汲取创新：支持多尺度检索的分层索引、选择性遗忘无关信息的注意力机制，以及将短期交互转化为长期知识的经验巩固。复合系统可能通过具有不同时间尺度和检索机制的专用记忆组件来解决这个问题。
- en: Energy Efficiency and Computational Scale
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 能效和计算规模
- en: Energy consumption presents equally daunting challenges. GPT-4 training is estimated
    to have consumed 50-100 GWh of electricity ([Sevilla et al. 2022a](ch058.xhtml#ref-epoch2022compute)),
    enough to power 50,000 homes for a year[20](#fn20). Extrapolating to AGI suggests
    energy requirements exceeding small nations’ output, creating both economic and
    environmental challenges.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 能耗同样提出了令人畏惧的挑战。GPT-4的训练估计消耗了50-100 GWh的电力([Sevilla等2022a](ch058.xhtml#ref-epoch2022compute))，足以为一万户家庭供电一年[20](#fn20)。外推到通用人工智能（AGI）表明，能源需求将超过小国的产出，从而带来经济和环境挑战。
- en: 'The human brain operates on 20 watts while performing computations that would
    require megawatts on current hardware[21](#fn21). This six-order-of-magnitude
    efficiency gap emerges from architectural differences: biological neurons operate
    at ~1 Hz effective compute rates using chemical signaling, while digital processors
    run at GHz frequencies using electronic switching. Despite the frequency disadvantage,
    the brain’s extensive parallelism (10¹¹ neurons with 10¹⁴ connections) and analog
    processing enable efficient pattern recognition that digital systems achieve only
    through brute force computation. This efficiency gap, detailed earlier with specific
    computational metrics in [Section 20.2](ch026.xhtml#sec-agi-systems-defining-agi-intelligence-systems-problem-19b9),
    cannot be closed through incremental improvements. Solutions require reimagining
    of computation, building on [Chapter 18](ch024.xhtml#sec-sustainable-ai): neuromorphic
    architectures that compute with spikes rather than matrix multiplications, reversible
    computing that recycles energy through computation, and algorithmic improvements
    that reduce training iterations by orders of magnitude.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑在20瓦的功率下进行计算，而当前硬件需要兆瓦的功率才能完成这些计算[21](#fn21)。这种六数量级的效率差距源于架构差异：生物神经元以大约1赫兹的有效计算速率使用化学信号进行操作，而数字处理器则以千兆赫兹的频率使用电子开关运行。尽管频率存在劣势，但大脑的广泛并行性（10¹¹个神经元和10¹⁴个连接）以及模拟处理能力使得数字系统只能通过暴力计算才能实现的模式识别变得高效。这种效率差距，如[第20.2节](ch026.xhtml#sec-agi-systems-defining-agi-intelligence-systems-problem-19b9)中详细说明的特定计算指标所示，不能通过渐进式改进来弥补。解决方案需要重新构想计算，建立在[第18章](ch024.xhtml#sec-sustainable-ai)的基础上：使用脉冲而不是矩阵乘法进行计算的神经形态架构，通过计算回收能量的可逆计算，以及通过数量级减少训练迭代的算法改进。
- en: Causal Reasoning and Planning Capabilities
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因果推理和规划能力
- en: Algorithmic limitations remain even with efficient hardware. Current models
    excel at pattern completion but struggle with novel reasoning. Ask ChatGPT to
    plan a trip, and it produces plausible itineraries. Ask it to solve a problem
    requiring new reasoning (proving a novel theorem or designing an experiment) and
    performance degrades rapidly[22](#fn22).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 即使硬件高效，算法限制仍然存在。当前模型在模式完成方面表现出色，但在新颖推理方面却力不从心。要求ChatGPT规划一次旅行，它会生成合理的行程。要求它解决需要新推理的问题（证明新的定理或设计实验）时，性能会迅速下降[22](#fn22)。
- en: 'True reasoning requires capabilities absent from current architectures. Consider
    three key requirements: World models represent internal simulations of how systems
    behave over time—for example, understanding that dropping a ball causes it to
    fall, not just that “dropped” and “fell” co-occur in text. Search mechanisms explore
    solution spaces systematically rather than relying on pattern matching. Finding
    mathematical proofs requires testing hypotheses and backtracking, not just recognizing
    solution patterns. Causal understanding distinguishes correlation from causation,
    recognizing that umbrellas correlate with rain but don’t cause it, while clouds
    do[23](#fn23). These capabilities demand architectural innovations beyond those
    in [Chapter 4](ch010.xhtml#sec-dnn-architectures), potentially hybrid systems
    combining neural networks with symbolic reasoners, or new architectures inspired
    by cognitive science.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的推理需要当前架构中缺乏的能力。考虑三个关键要求：世界模型代表系统随时间行为的内部模拟——例如，理解抛球会导致其下落，而不仅仅是“抛下”和“落下”在文本中同时出现。搜索机制系统地探索解决方案空间，而不是依赖于模式匹配。找到数学证明需要测试假设和回溯，而不仅仅是识别解决方案模式。因果理解区分相关性和因果关系，认识到雨伞与雨相关，但不会导致雨，而云则会[23](#fn23)。这些能力需要超越[第4章](ch010.xhtml#sec-dnn-architectures)中提到的架构创新，可能需要结合神经网络和符号推理器的混合系统，或受认知科学启发的新的架构。
- en: Symbol Grounding and Embodied Intelligence
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 符号接地和具身智能
- en: Language models learn “cat” co-occurs with “meow” and “fur” but have never experienced
    a cat’s warmth or heard its purr. This symbol grounding problem ([Harnad 1990](ch058.xhtml#ref-harnad1990symbol);
    [Searle 1980](ch058.xhtml#ref-searle1980minds)) (connecting symbols to experiences)
    may limit intelligence without embodiment.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型学习“猫”与“喵喵”和“毛皮”同时出现，但从未体验过猫的温暖或听到它的咕噜声。这个符号接地问题([Harnad 1990](ch058.xhtml#ref-harnad1990symbol)；[Searle
    1980](ch058.xhtml#ref-searle1980minds))（将符号与经验联系起来）可能会在没有实体的情况下限制智能。
- en: 'Robotic embodiment introduces systems constraints from [Chapter 14](ch020.xhtml#sec-ondevice-learning):
    real-time inference requirements (sub-100 ms control loops), continuous learning
    from noisy sensor data, and safe exploration in environments where mistakes cause
    physical damage[24](#fn24). These constraints mirror the efficiency challenges
    covered in [Chapter 9](ch015.xhtml#sec-efficient-ai) but with even stricter latency
    and reliability requirements. Yet embodiment might be essential for understanding
    concepts like “heavy,” “smooth,” or “careful” that are grounded in physical experience.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人实体引入了来自[第14章](ch020.xhtml#sec-ondevice-learning)的系统约束：实时推理需求（控制循环小于100毫秒），从嘈杂的传感器数据中进行持续学习，以及在错误会导致物理损坏的环境中安全探索[24](#fn24)。这些约束反映了[第9章](ch015.xhtml#sec-efficient-ai)中涵盖的效率挑战，但具有更严格的延迟和可靠性要求。然而，实体可能对于理解“重”、“光滑”或“小心”等基于物理经验的概念是必不可少的。
- en: AI Alignment and Value Specification
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工智能对齐与价值指定
- en: The most critical barrier involves ensuring AGI systems pursue human values
    rather than optimizing simplified objectives that lead to harmful outcomes[25](#fn25).
    Current reward functions are proxies (maximize engagement, minimize error) that
    can produce unintended behaviors when optimized strongly.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最关键的障碍在于确保通用人工智能系统追求人类价值观，而不是优化导致有害结果简化的目标[25](#fn25)。当前的奖励函数是代理（最大化参与度，最小化错误），当被强烈优化时可能会产生意外的行为。
- en: 'Alignment requires solving multiple interconnected problems: value specification
    (what do humans actually want?), robust optimization (pursuing goals without exploiting
    loopholes), corrigibility (remaining modifiable as capabilities grow), and scalable
    oversight (maintaining control over systems smarter than overseers)[26](#fn26).
    These challenges span technical and philosophical domains, requiring advances
    in interpretability from [Chapter 17](ch023.xhtml#sec-responsible-ai), formal
    verification methods, and new frameworks for specifying and verifying objectives.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐需要解决多个相互关联的问题：价值指定（人类实际上想要什么？）、鲁棒优化（追求目标而不利用漏洞）、可纠正性（随着能力增长而保持可修改性）和可扩展的监督（在比监督者更聪明的系统中保持控制权）[26](#fn26)。这些挑战跨越技术和哲学领域，需要从[第17章](ch023.xhtml#sec-responsible-ai)中推进可解释性、形式化验证方法和新的指定和验证目标框架的框架。
- en: '**The Alignment Tax: Permanent Operational Cost of Safety**'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**对齐税：安全性的永久性运营成本**'
- en: Ensuring AGI systems are safe and aligned with human values requires significant,
    ongoing investment of computational resources, research effort, and human oversight.
    This “alignment tax” represents a permanent operational cost, not a one-time problem
    to be solved. Aligned AGI systems may be intentionally less computationally efficient
    than unaligned ones because a portion of their resources will always be dedicated
    to safety verification, value alignment checks, and self-limitation mechanisms.
    Systems must continuously monitor their own behavior, verify outputs against safety
    constraints, and maintain oversight channels even when these checks introduce
    latency or reduce throughput. This frames alignment not as an engineering hurdle
    to overcome and move past, but as a continuous cost of operating trustworthy intelligent
    systems at scale.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 确保通用人工智能系统安全并与人类价值观一致需要大量的、持续的计算资源投资、研究努力和人类监督。这种“对齐税”代表了一种永久性的运营成本，而不仅仅是一次性需要解决的问题。对齐的通用人工智能系统可能有意地不如未对齐的系统计算效率高，因为它们的部分资源将始终用于安全验证、价值对齐检查和自我限制机制。系统必须持续监控自己的行为，验证输出是否符合安全约束，即使在这些检查引入延迟或降低吞吐量时也要保持监督渠道。这把对齐视为一个持续的运营成本，而不是一个需要克服和超越的工程障碍，而是作为在规模上运营值得信赖的智能系统的持续成本。
- en: '![](../media/file326.svg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file326.svg)'
- en: 'Figure 20.6: **Technical Barriers to AGI**: Five critical challenges must be
    solved simultaneously for artificial general intelligence. Each represents orders-of-magnitude
    gaps: memory systems need persistence across sessions, energy efficiency requires
    1000x improvements, reasoning needs genuine planning beyond pattern matching,
    embodiment demands symbol grounding, and alignment requires value specification.
    Red arrows show critical blocking paths; dashed gray lines indicate key interdependencies.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.6：**通用人工智能的技术障碍**：为了实现通用人工智能，必须同时解决五个关键挑战。每个挑战都代表着数量级的差距：记忆系统需要在会话间保持持久性，能效需要提高1000倍，推理需要超越模式匹配进行真正的规划，具身化需要符号基础，而对齐需要价值指定。红色箭头显示关键阻塞路径；虚线灰色线条表示关键相互依赖关系。
- en: These five barriers form an interconnected web of challenges. Progress on any
    single barrier remains insufficient, as AGI requires coordinated breakthroughs
    across all dimensions, as illustrated in [Figure 20.6](ch026.xhtml#fig-technical-barriers).
    The engineering principles developed throughout this textbook, from data engineering
    ([Chapter 6](ch012.xhtml#sec-data-engineering)) through distributed training ([Chapter 8](ch014.xhtml#sec-ai-training))
    to robust deployment ([Chapter 13](ch019.xhtml#sec-ml-operations)), provide foundations
    for addressing each barrier, though the complete solutions remain unknown.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这五个障碍形成了一个相互关联的挑战网络。任何单个障碍的进展都仍然不足，因为通用人工智能需要跨所有维度的协调突破，如图20.6所示。[图20.6](ch026.xhtml#fig-technical-barriers)展示了这一点。本书从数据工程（[第6章](ch012.xhtml#sec-data-engineering)）到分布式训练（[第8章](ch014.xhtml#sec-ai-training)），再到稳健部署（[第13章](ch019.xhtml#sec-ml-operations)）所发展的工程原理，为解决每个障碍提供了基础，尽管完整的解决方案仍然未知。
- en: The magnitude of these challenges motivates reconsideration of AGI’s organizational
    structure. Rather than overcoming each barrier through monolithic system improvements,
    an alternative approach distributes intelligence across multiple specialized agents
    that collaborate to achieve capabilities exceeding any individual system.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战的规模促使人们重新考虑通用人工智能的组织结构。而不是通过单一系统的整体改进来克服每个障碍，一种替代方法是在多个专业代理之间分配智能，这些代理协作以实现超越任何单个系统的能力。
- en: Emergent Intelligence Through Multi-Agent Coordination
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过多代理协调实现涌现智能
- en: 'The technical barriers outlined above demand orders-of-magnitude breakthroughs
    that may prove elusive for single-agent architectures. Each barrier represents
    a computational or scaling challenge: processing infinite context, achieving biological
    energy efficiency, performing causal reasoning, grounding in physical embodiment,
    and maintaining alignment as capabilities scale. Addressing all barriers simultaneously
    within monolithic systems compounds the difficulty exponentially.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 上文概述的技术障碍要求实现数量级的突破，这可能对单一代理架构来说是难以捉摸的。每个障碍都代表着计算或扩展挑战：处理无限上下文，实现生物能效，进行因果推理，在物理具身化中扎根，以及随着能力的扩展保持对齐。在单一系统中同时解决所有障碍将使难度呈指数级增加。
- en: 'Multi-agent systems offer an alternative paradigm where intelligence emerges
    from interactions between specialized agents rather than residing in any single
    system. This approach aligns with the compound AI systems framework: rather than
    one system solving all problems, specialized components collaborate through structured
    interfaces. Multi-agent systems extend this principle to AGI scale, potentially
    sidestepping some barriers through distribution. Memory limitations dissolve when
    specialized agents maintain domain-specific context. Energy efficiency improves
    through selective activation; only relevant agents engage for each task. Reasoning
    decomposes across specialized agents with verification. Embodiment becomes feasible
    through distributed physical instantiation. Alignment simplifies when specialized
    agents have narrow, verifiable objectives.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理系统提供了一个替代范式，其中智能来自专业代理之间的交互，而不是存在于任何单个系统中。这种方法与复合人工智能系统框架相一致：而不是一个系统解决所有问题，专业组件通过结构化接口协作。多代理系统将这一原则扩展到通用人工智能规模，可能通过分布绕过一些障碍。当专业代理保持特定领域的上下文时，记忆限制就会消失。通过选择性激活提高能效；每个任务只涉及相关的代理。推理通过验证分解到专业代理中。通过分布式物理实现，具身化变得可行。当专业代理有狭窄、可验证的目标时，对齐变得简单。
- en: Yet AGI-scale multi-agent systems introduce new engineering challenges that
    dwarf current distributed systems. Understanding these challenges proves essential
    for evaluating whether multi-agent approaches offer practical pathways to AGI
    or simply replace known barriers with unknown coordination problems.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，AGI规模的多代理系统引入了新的工程挑战，这些挑战远远超过了当前的分布式系统。理解这些挑战对于评估多代理方法是否为AGI提供了实际途径，或者只是用未知的协调问题替换了已知的障碍至关重要。
- en: AGI systems might require coordination between millions of specialized agents
    distributed across continents while today’s distributed systems coordinate thousands
    of servers[27](#fn27). Each agent could be a frontier-model-scale system consuming
    gigawatts of power, making coordination latency and bandwidth major bottlenecks.
    Communication between agents in Tokyo and New York introduces 150 ms round-trip
    delays, unacceptable for real-time reasoning requiring millisecond coordination.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: AGI系统可能需要在跨越大陆的数百万个专业代理之间进行协调，而今天的分布式系统只需要协调数千个服务器[27](#fn27)。每个代理可能都是一个前沿模型规模的系统，消耗千兆瓦的电力，使得协调延迟和带宽成为主要的瓶颈。东京和纽约之间代理之间的通信引入了150毫秒的往返延迟，这对于需要毫秒级协调的实时推理来说是无法接受的。
- en: Addressing these coordination challenges requires first establishing agent specialization
    across different domains. Scientific reasoning agents would process exabytes of
    literature, creative agents would generate multimedia content, strategic planning
    agents would optimize across decades-long timescales, and embodied agents would
    control robotic systems. Each agent excels in its specialty while sharing common
    interfaces that enable coordination. This mirrors how modern software systems
    decompose complex functionality into microservices, but at unprecedented scale
    and complexity.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些协调挑战首先需要在不同领域建立代理的专业化。科学推理代理将处理艾字节级的文献，创意代理将生成多媒体内容，战略规划代理将在数十年的时间尺度上优化，而具身代理将控制机器人系统。每个代理在其专业领域内表现出色，同时共享使协调成为可能的通用接口。这反映了现代软件系统如何将复杂功能分解成微服务，但规模和复杂性都是前所未有的。
- en: The effectiveness of such specialization critically depends on communication
    protocols between agents. Unlike traditional distributed systems that exchange
    simple state updates, AGI agents must communicate rich semantic information including
    partial world models, reasoning chains, uncertainty estimates, and intent representations[28](#fn28).
    The protocols must compress complex cognitive states into network packets while
    preserving semantic fidelity across heterogeneous agent architectures. Current
    internet protocols lack semantic understanding; future AGI networks might require
    content-aware routing that understands reasoning context.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这种专业化的有效性关键取决于代理之间的通信协议。与交换简单状态更新的传统分布式系统不同，AGI代理必须通信丰富的语义信息，包括部分世界模型、推理链、不确定性估计和意图表示[28](#fn28)。这些协议必须将复杂的认知状态压缩成网络数据包，同时在异构代理架构中保持语义的准确性。当前的互联网协议缺乏语义理解；未来的AGI网络可能需要内容感知路由，它能够理解推理上下文。
- en: 'Beyond protocols, network topology design becomes critical for achieving efficient
    communication at scale. Rather than flat network architectures, AGI systems might
    require hierarchical topologies mimicking biological neural organization: local
    agent clusters for rapid coordination, regional hubs for cross-domain integration,
    and global coordination layers for system-wide coherence[29](#fn29). Load balancing
    algorithms must consider not just computational load but semantic affinity, routing
    related reasoning tasks to agents with shared context.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 除了协议之外，网络拓扑设计对于实现大规模高效通信变得至关重要。而不是扁平的网络架构，AGI系统可能需要模仿生物神经组织的分层拓扑：用于快速协调的本地代理集群，用于跨领域集成的区域枢纽，以及用于系统整体一致性的全球协调层[29](#fn29)。负载均衡算法必须考虑不仅仅是计算负载，还要考虑语义亲和力，将相关的推理任务路由到具有共享上下文的代理。
- en: These architectural considerations lead naturally to questions of consensus
    mechanisms, which for AGI agents face complexity beyond traditional distributed
    systems. While blockchain consensus involves simple state transitions, AGI consensus
    must handle conflicting world models, competing reasoning chains, and subjective
    value judgments[30](#fn30). When scientific reasoning agents disagree about experimental
    interpretations, creative agents propose conflicting artistic directions, and
    strategic agents recommend opposing policies, the system needs mechanisms for
    productive disagreement rather than forced consensus. This might involve reputation
    systems that weight agent contributions by past accuracy, voting mechanisms that
    consider argument quality not just agent count, and meta-reasoning systems that
    identify when disagreement indicates genuine uncertainty versus agent malfunction.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构考虑自然地引出关于共识机制的问题，对于AGI代理来说，它们面临的复杂性超出了传统的分布式系统。虽然区块链共识涉及简单的状态转换，但AGI共识必须处理冲突的世界模型、竞争的推理链和主观的价值判断[30](#fn30)。当科学推理代理在实验解释上意见不一致时，创意代理提出冲突的艺术方向，战略代理推荐对立的政策，系统需要机制来促进有建设性的分歧，而不是强制达成共识。这可能涉及根据过去准确性权衡代理贡献的名誉系统，考虑论证质量而不仅仅是代理数量的投票机制，以及识别分歧表明真正的不确定性还是代理故障的元推理系统。
- en: 'Consensus challenges intensify when considering Byzantine fault tolerance,
    which becomes more challenging when agents are not just providing incorrect information
    but potentially pursuing different objectives. Unlike server failures that are
    random, agent failures might be systematic: an agent trained on biased data consistently
    providing skewed recommendations, an agent with misaligned objectives subtly manipulating
    other agents, or an agent compromised by adversarial attacks spreading misinformation[31](#fn31).
    Traditional Byzantine algorithms require 3f+1 honest nodes to tolerate f Byzantine
    nodes, but AGI systems might face sophisticated, coordinated attacks requiring
    novel defense mechanisms.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到拜占庭容错性时，共识挑战加剧，当代理不仅提供错误信息，还可能追求不同目标时，这变得更加困难。与随机的服务器故障不同，代理故障可能是系统的：一个在偏见数据上训练的代理持续提供偏颇的建议，一个目标不一致的代理微妙地操纵其他代理，或者一个被对抗性攻击破坏的代理传播错误信息[31](#fn31)。传统的拜占庭算法需要3f+1个诚实节点来容忍f个拜占庭节点，但AGI系统可能面临复杂、协调的攻击，需要新的防御机制。
- en: 'Finally, resource coordination across millions of agents demands new distributed
    algorithms that move beyond current orchestration frameworks. When multiple reasoning
    chains compete for compute resources, memory bandwidth, and network capacity,
    the system needs real-time resource allocation that considers not just current
    load but predicted reasoning complexity. This requires advances beyond current
    Kubernetes orchestration: predictive load balancing based on reasoning difficulty
    estimation, priority systems that understand reasoning urgency, and graceful degradation
    that maintains system coherence when resources become constrained[32](#fn32).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数百万代理之间的资源协调需要新的分布式算法，这些算法超越了当前的协调框架。当多个推理链竞争计算资源、内存带宽和网络容量时，系统需要考虑不仅当前负载，还要考虑预测推理复杂度的实时资源分配。这需要超越当前Kubernetes协调的进步：基于推理难度估计的预测负载均衡，理解推理紧迫性的优先级系统，以及当资源受限时保持系统一致性的优雅降级[32](#fn32)。
- en: 'The goal is emergent intelligence: capabilities arising from agent interaction
    that no single agent possesses. Like how behaviors emerge from simple rules in
    swarm systems, reasoning might emerge from relatively simple agents working together.
    The whole becomes greater than the sum of its parts, but only through careful
    systems engineering of the coordination mechanisms.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是涌现智能：由代理交互产生的能力，单个代理并不具备。就像在群体系统中，行为从简单的规则中涌现出来一样，推理可能从相对简单的协作代理中产生。整体大于部分之和，但这需要通过仔细的系统工程来协调机制。
- en: This multi-agent approach requires orchestration ([Chapter 5](ch011.xhtml#sec-ai-workflow)),
    robust communication infrastructure, and attention to failure modes where agent
    interactions could lead to unexpected behaviors.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多代理方法需要协调（[第5章](ch011.xhtml#sec-ai-workflow)），强大的通信基础设施，以及对可能导致意外行为的代理交互失败模式的关注。
- en: Engineering Pathways to AGI
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工程通往通用人工智能（AGI）的路径
- en: 'The journey from current AI systems to artificial general intelligence requires
    more than understanding technical possibilities; it demands strategic thinking
    about practical opportunities. The preceding sections surveyed building blocks,
    emerging paradigms, technical barriers, and alternative organizational structures.
    This comprehensive foundation enables addressing the critical question for practicing
    ML systems engineers: how do these frontiers translate into actionable engineering
    decisions?'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 从当前AI系统到通用人工智能的旅程需要更多的不仅仅是理解技术可能性；它需要关于实际机会的战略思考。前面的章节概述了构建块、新兴范式、技术障碍和替代组织结构。这个全面的基础使得能够解决实践ML系统工程师的关键问题：这些前沿如何转化为可操作的工程决策？
- en: 'Understanding AGI’s ultimate challenges proves intellectually valuable but
    operationally insufficient. Engineers need practical guidance connecting AGI frontiers
    to current work: which opportunities merit investment now, which challenges demand
    attention first, and how AGI research informs production system design today.
    This section bridges the gap between AGI’s distant horizons and near-term engineering
    decisions.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 理解AGI的最终挑战在智力上是有价值的，但在操作上是不够的。工程师需要实际指导，将AGI前沿与当前工作联系起来：哪些机会现在值得投资，哪些挑战需要首先关注，以及AGI研究如何影响今天的生产系统设计。本节架起了AGI遥远的前景与近期工程决策之间的差距。
- en: The convergence of these building blocks (data engineering at scale, dynamic
    architectures, alternative paradigms, training methodologies, and post-Moore’s
    Law hardware) creates concrete opportunities for ML systems engineers. These are
    not decades-away possibilities but near-term projects that advance current capabilities
    while building toward AGI. Simultaneously, navigating these opportunities requires
    confronting challenges spanning technical depth, operational complexity, and organizational
    dynamics.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这些构建块的融合（大规模数据工程、动态架构、替代范式、训练方法和摩尔定律之后的硬件）为ML系统工程师创造了具体的机会。这些不是几十年后的可能性，而是近期的项目，它们在提升当前能力的同时，朝着AGI迈进。同时，导航这些机会需要面对跨越技术深度、运营复杂性和组织动态的挑战。
- en: 'This section examines practical pathways from current systems toward AGI-scale
    intelligence through the lens of near-term engineering opportunities and their
    corresponding challenges. The goal: actionable guidance for systems engineers
    positioned to shape AI’s trajectory over the next decade.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过近期工程机会及其相应挑战的视角，探讨了从当前系统向AGI规模智能过渡的实际途径。目标：为处于塑造未来十年AI轨迹位置的系统工程师提供可操作的指导。
- en: 'Opportunity Landscape: Infrastructure to Apps'
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机会景观：基础设施到应用
- en: 'Three opportunity domains emerge from the AGI building blocks: foundational
    infrastructure, enabling technologies, and end-user applications.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 从AGI构建块中出现了三个机会领域：基础基础设施、使能技术和最终用户应用。
- en: Next-generation training platforms address current inefficiencies where GPU
    clusters achieve only 20-40% utilization during training. Improving utilization
    to 70-80% would reduce training costs by 40-60%, worth billions annually. These
    platforms must handle mixture-of-experts models requiring dynamic load balancing,
    dynamic computation graphs demanding just-in-time compilation, and continuous
    learning pipelines needing real-time updates without service interruption. Multi-modal
    processing platforms provide unified handling across text, images, audio, video,
    and sensor data, while edge-cloud hybrid systems blur boundaries between local
    and remote computation through intelligent workload distribution.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 新一代训练平台解决了当前的不效率问题，其中GPU集群在训练期间仅达到20-40%的利用率。将利用率提高到70-80%将降低40-60%的训练成本，每年价值数十亿美元。这些平台必须处理需要动态负载平衡的专家混合模型、需要即时编译的动态计算图以及需要实时更新而不中断服务的持续学习管道。多模态处理平台提供对文本、图像、音频、视频和传感器数据的统一处理，而边缘-云混合系统通过智能工作负载分配模糊了本地和远程计算之间的界限。
- en: Personalized AI systems learn individual workflows and preferences over time,
    enabled by parameter-efficient fine-tuning reducing costs 1000×, retrieval systems
    for personal knowledge bases, and privacy-preserving techniques. Real-time intelligence
    systems enable new paradigms requiring sub-200 ms response times for conversational
    AI, <10 ms for autonomous vehicles, and <1 ms for robotic surgery. Explainable
    AI systems integrate interpretability as first-class constraints, driven by regulatory
    requirements including EU AI Act mandates and medical device approval processes.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 定制化AI系统随着时间的推移学习个人工作流程和偏好，得益于参数高效的微调，成本降低1000倍，个人知识库的检索系统以及隐私保护技术。实时智能系统使新的范式成为可能，这些范式要求对话AI的响应时间小于200 ms，自动驾驶车辆小于10 ms，机器人手术小于1 ms。可解释AI系统将可解释性作为一级约束整合，受到包括欧盟AI法案要求和医疗设备审批流程在内的监管要求驱动。
- en: Workflow automation systems orchestrate multiple AI components for end-to-end
    task completion across scientific discovery, creative production, and software
    development. McKinsey estimates 60-70% of current jobs contain 30%+ automatable
    activities, yet current automation covers <5% of possible workflows primarily
    due to integration complexity rather than capability limitations. These applications
    build upon compound AI systems principles ([Section 20.3](ch026.xhtml#sec-agi-systems-compound-ai-systems-framework-2a31)),
    requiring orchestration infrastructure from [Chapter 5](ch011.xhtml#sec-ai-workflow).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程自动化系统编排多个AI组件，以完成科学发现、创意生产和软件开发的全端任务。麦肯锡估计，目前60-70%的工作包含30%以上的可自动化活动，但当前的自动化主要由于集成复杂性而不是能力限制，仅覆盖了可能的5%的工作流程。这些应用建立在复合AI系统原则（[第20.3节](ch026.xhtml#sec-agi-systems-compound-ai-systems-framework-2a31)）之上，需要来自第5章的编排基础设施。
- en: Engineering Challenges in AGI Development
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AGI开发中的工程挑战
- en: Realizing these opportunities requires addressing challenges that span multiple
    dimensions. Rather than isolated technical problems, these challenges represent
    systemic issues requiring coordinated solutions across the building blocks.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这些机会需要解决跨越多个维度的挑战。这些挑战不是孤立的技术问题，而是需要跨构建块协调解决方案的系统性问题。
- en: 'Technical Challenges: Reliability and Performance'
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术挑战：可靠性和性能
- en: Ultra-high reliability requirements intensify at AGI scale. When training runs
    cost millions of dollars and involve thousands of components, even 99.9% reliability
    means frequent failures destroying weeks of progress. This demands checkpointing
    that restarts from recent states, recovery mechanisms salvaging partial progress,
    and graceful degradation maintaining quality when components fail. Moving from
    99.9% to 99.99% reliability, a 10× reduction in failure rate, proves disproportionately
    expensive, requiring redundancy, predictive failure detection, and fault-tolerant
    algorithms.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在AGI规模下，超高可靠性要求日益加剧。当训练运行成本数百万美元并涉及数千个组件时，即使99.9%的可靠性也意味着频繁的故障，会破坏数周的进展。这需要从最近状态重新启动的检查点、恢复机制来挽救部分进展以及组件失败时的优雅降级。将可靠性从99.9%提升到99.99%，即故障率降低10倍，证明成本不成比例地高昂，需要冗余、预测性故障检测和容错算法。
- en: Heterogeneous system orchestration grows increasingly complex as systems must
    coordinate CPUs for preprocessing, GPUs for matrix operations, TPUs[33](#fn33)
    for inference, quantum processors for optimization, and neuromorphic chips for
    energy-efficient computation. This heterogeneity demands abstractions hiding complexity
    from developers and scheduling algorithms optimizing across different computational
    paradigms. Current frameworks (TensorFlow, PyTorch from [Chapter 7](ch013.xhtml#sec-ai-frameworks))
    assume relatively homogeneous hardware; AGI infrastructure requires new abstractions
    supporting multi-paradigm orchestration.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 随着系统必须协调CPU进行预处理、GPU进行矩阵运算、TPU[33](#fn33)进行推理、量子处理器进行优化以及神经形态芯片进行节能计算，异构系统编排变得越来越复杂。这种异构性要求抽象化，以隐藏复杂性给开发者，并需要优化跨不同计算范式的调度算法。当前的框架（TensorFlow、来自第7章的PyTorch）假设硬件相对同质；AGI基础设施需要支持多范式编排的新抽象。
- en: Quality-efficiency trade-offs sharpen as systems scale. Real-time systems often
    cannot use the most advanced models due to latency constraints—a dilemma that
    intensifies as model capabilities grow. The optimization challenge involves hierarchical
    processing where simple models handle routine cases while advanced models activate
    only when needed, adaptive algorithms adjusting computational depth based on available
    time, and graceful degradation providing approximate results when exact computation
    isn’t possible.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 随着系统规模的扩大，质量-效率权衡变得更加尖锐。实时系统往往不能使用最先进的模型，因为延迟限制——随着模型能力的增长，这种困境加剧。优化挑战涉及分层处理，其中简单模型处理常规案例，而高级模型仅在需要时激活，自适应算法根据可用时间调整计算深度，以及当精确计算不可能时提供近似结果。
- en: 'Operational Challenges: Testing and Deployment'
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运营挑战：测试和部署
- en: Verification and validation for AI-driven workflows proves difficult when errors
    compound through long chains. A small mistake in early stages can invalidate hours
    or days of subsequent work. This requires automated testing understanding AI behavior
    patterns, checkpoint systems enabling rollback from failure points, and confidence
    monitoring triggering human review when uncertainty increases. The testing frameworks
    from [Chapter 13](ch019.xhtml#sec-ml-operations) extend to handle non-deterministic
    AI components and emergent behaviors.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 当错误通过长链累积时，对由AI驱动的流程进行验证和验证变得困难。早期阶段的一个小错误可能会使后续数小时或数天的工怍无效。这需要自动测试理解AI行为模式，检查点系统允许从失败点回滚，以及当不确定性增加时触发人工审查的信心监控。第13章中的测试框架扩展到处理非确定性AI组件和涌现行为。
- en: Trust calibration determines when humans should intervene in automated systems.
    Complete automation often fails, but determining optimal handoff points requires
    understanding both technical capabilities and human factors. The challenge involves
    creating interfaces providing context for human decision-making, developing trust
    calibration so humans know when to intervene, and maintaining human expertise
    in domains where automation becomes dominant. This draws on responsible AI principles
    from [Chapter 17](ch023.xhtml#sec-responsible-ai) regarding human-AI collaboration.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 信任校准决定人类何时应介入自动化系统。完全自动化往往失败，但确定最佳交接点需要理解技术能力和人类因素。挑战包括创建为人类决策提供背景的接口，开发信任校准以便人类知道何时介入，以及在自动化成为主导领域的领域内保持人类专业知识。这借鉴了第17章中关于人机协作的负责任AI原则。
- en: Safety monitoring at the semantic level requires understanding content and intent,
    not just system metrics. AI safety monitoring must detect harmful outputs, prompt
    injections, and adversarial attacks in real-time across billions of interactions—qualitatively
    different from traditional software monitoring tracking latency, throughput, and
    error rates. This necessitates new tooling combining robustness principles ([Chapter 16](ch022.xhtml#sec-robust-ai)),
    security practices ([Chapter 15](ch021.xhtml#sec-security-privacy)), and responsible
    AI frameworks ([Chapter 17](ch023.xhtml#sec-responsible-ai)).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义层面的安全监控需要理解内容意图，而不仅仅是系统指标。AI安全监控必须在数十亿次的交互中实时检测有害输出、提示注入和对抗攻击——这与传统软件监控跟踪延迟、吞吐量和错误率有质的不同。这需要新的工具，结合鲁棒性原则（第16章）、安全实践（第15章）和负责任AI框架（第17章）。
- en: Social and Ethical Considerations
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 社会和伦理考量
- en: AGI systems amplify existing privacy and security challenges ([Chapter 15](ch021.xhtml#sec-security-privacy))
    while introducing new attack vectors through multi-component interactions and
    continuous learning capabilities. Privacy and personalization create difficult
    tensions in system design. Personalization requires user data (conversation histories,
    work patterns, preferences) yet privacy regulations and user expectations increasingly
    demand local processing. The challenge lies in developing federated learning and
    differential privacy techniques that enable personalization while maintaining
    privacy guarantees. Current approaches often sacrifice significant performance
    for privacy protection—a trade-off that must improve for widespread adoption.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: AGI系统放大了现有的隐私和安全挑战（[第15章](ch021.xhtml#sec-security-privacy)），同时通过多组件交互和持续学习能力引入了新的攻击向量。隐私和个人化在系统设计中造成了难以调和的紧张关系。个性化需要用户数据（对话历史、工作模式、偏好），而隐私法规和用户期望越来越要求本地处理。挑战在于开发联邦学习和差分隐私技术，这些技术能够在保持隐私保证的同时实现个性化。当前的方法往往为了隐私保护而牺牲了显著的性能——这种权衡必须得到改善，以便得到广泛的应用。
- en: Filter bubbles and bias amplification risk reinforcing harmful patterns when
    personalized AI systems learn to give users what they want to hear rather than
    what they need to know. This limits exposure to diverse perspectives and challenging
    ideas. Building responsible personalization requires ensuring systems occasionally
    introduce diverse viewpoints, challenge user assumptions rather than confirming
    beliefs, and maintain transparency about personalization processes. This applies
    the responsible AI principles from [Chapter 17](ch023.xhtml#sec-responsible-ai)
    at the personalization layer.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 当个性化AI系统学会向用户提供他们想听的内容而不是他们需要知道的内容时，过滤泡和偏见放大风险可能会强化有害模式。这限制了接触不同观点和挑战性想法的机会。构建负责任的个人化需要确保系统偶尔引入不同的观点，挑战用户的假设而不是确认信念，并保持个人化过程的透明度。这将在个人化层应用[第17章](ch023.xhtml#sec-responsible-ai)中的负责任AI原则。
- en: 'Explainability and performance create tension, forcing choices between model
    accuracy and human interpretability. More interpretable models often sacrifice
    accuracy because constraints required for human understanding may conflict with
    optimal computational patterns. Different stakeholders need different explanations:
    medical professionals want detailed causal reasoning, patients want simple reassuring
    summaries, regulatory auditors need compliance-focused explanations, and researchers
    need technical details enabling reproducibility. Building systems adapting explanations
    appropriately requires combining technical expertise with user experience design.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性和性能之间产生紧张关系，迫使在模型准确性和人类可解释性之间做出选择。更可解释的模型往往牺牲了准确性，因为为了人类理解所需的约束可能与最优计算模式相冲突。不同的利益相关者需要不同的解释：医疗专业人员希望有详细的因果推理，患者希望有简单的安慰性摘要，监管审计员需要以合规性为重点的解释，研究人员需要能够实现可重复性的技术细节。构建能够适当调整解释的系统需要结合技术专长与用户体验设计。
- en: 'The opportunity and challenge landscapes interconnect: infrastructure platforms
    enable personalized and real-time systems, which power automation applications,
    but each opportunity amplifies specific challenges. Successfully navigating this
    landscape requires the systems thinking developed throughout this textbook: understanding
    how components interact, anticipating failure modes, designing for graceful degradation,
    and balancing competing constraints. The engineering principles from data pipelines
    ([Chapter 6](ch012.xhtml#sec-data-engineering)) through distributed training ([Chapter 8](ch014.xhtml#sec-ai-training))
    to robust deployment ([Chapter 13](ch019.xhtml#sec-ml-operations)) provide foundations
    for addressing these challenges at unprecedented scale.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 机遇和挑战的地形相互交织：基础设施平台使个性化实时系统成为可能，这些系统为自动化应用提供动力，但每个机遇都会放大特定的挑战。成功穿越这一地形需要本书中发展起来的系统思维：理解组件如何相互作用，预测故障模式，设计优雅降级，以及平衡相互竞争的约束。从数据管道（[第6章](ch012.xhtml#sec-data-engineering)）到分布式训练（[第8章](ch014.xhtml#sec-ai-training)），再到稳健部署（[第13章](ch019.xhtml#sec-ml-operations)）的工程原则，为以前所未有的规模应对这些挑战提供了基础。
- en: Implications for ML Systems Engineers
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对机器学习系统工程师的影响
- en: ML systems engineers with understanding of this textbook’s content are uniquely
    positioned for AGI development. The competencies developed, from data engineering
    ([Chapter 6](ch012.xhtml#sec-data-engineering)) through distributed training ([Chapter 8](ch014.xhtml#sec-ai-training))
    to model optimization ([Chapter 10](ch016.xhtml#sec-model-optimizations)) and
    robust deployment ([Chapter 13](ch019.xhtml#sec-ml-operations)), constitute essential
    AGI infrastructure requirements. AGI development demands full-stack capabilities
    spanning infrastructure construction, efficient experimentation tools, safety
    and alignment system design, and reproducible complex system interactions.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 理解本书内容的ML系统工程师在AGI开发方面处于独特的位置。从数据工程([第6章](ch012.xhtml#sec-data-engineering))到分布式训练([第8章](ch014.xhtml#sec-ai-training))，再到模型优化([第10章](ch016.xhtml#sec-model-optimizations))和稳健部署([第13章](ch019.xhtml#sec-ml-operations))，这些能力构成了AGI基础设施的基本要求。AGI开发需要涵盖基础设施构建、高效实验工具、安全与对齐系统设计以及可复现的复杂系统交互的全栈能力。
- en: Applying AGI Concepts to Current Practice
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将AGI概念应用于当前实践
- en: Understanding AGI trajectories improves architectural decisions in routine ML
    projects today. The engineering challenges inherent in AGI development directly
    map to the foundational knowledge developed throughout this textbook. [Table 20.1](ch026.xhtml#tbl-agi-chapter-mapping)
    demonstrates how AGI aspirations build upon established ML systems principles,
    reinforcing that the skills needed for AGI development extend current competencies
    rather than replacing them.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 理解AGI的发展轨迹有助于今天在常规ML项目中做出架构决策。AGI发展中固有的工程挑战直接映射到本书全书中开发的基础知识。 [表20.1](ch026.xhtml#tbl-agi-chapter-mapping)
    展示了AGI的愿景如何建立在已建立的ML系统原则之上，强化了AGI发展所需的技能是扩展现有能力而不是取代它们。
- en: 'Table 20.1: **AGI Challenges to Core ML Systems Knowledge**: The technical
    challenges of AGI development directly build upon the foundational engineering
    principles covered throughout this textbook.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '表20.1: **AGI对核心ML系统知识的挑战**: AGI发展的技术挑战直接建立在本书全书中涵盖的基础工程原则之上。'
- en: '| **AGI Challenge** | **Foundational Knowledge in Chapter…** |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| **AGI挑战** | **第…章中的基础知识** |'
- en: '| --- | --- |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Data at Scale** | [Chapter 6](ch012.xhtml#sec-data-engineering): Data Engineering
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| **大规模数据** | [第6章](ch012.xhtml#sec-data-engineering): 数据工程 |'
- en: '| **Training Paradigms** | [Chapter 8](ch014.xhtml#sec-ai-training): AI Training
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| **训练范式** | [第8章](ch014.xhtml#sec-ai-training): AI训练 |'
- en: '| **Dynamic Architectures** | [Chapter 4](ch010.xhtml#sec-dnn-architectures):
    DNN Architectures |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| **动态架构** | [第4章](ch010.xhtml#sec-dnn-architectures): DNN架构 |'
- en: '| **Hardware Scaling** | [Chapter 11](ch017.xhtml#sec-ai-acceleration): AI
    Acceleration |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| **硬件扩展** | [第11章](ch017.xhtml#sec-ai-acceleration): AI加速 |'
- en: '| **Efficiency & Resource** | [Chapter 10](ch016.xhtml#sec-model-optimizations):
    Efficient AI |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| **效率与资源** | [第10章](ch016.xhtml#sec-model-optimizations): 高效AI |'
- en: '| **Management** |  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| **管理** |  |'
- en: '| **Development Frameworks** | [Chapter 7](ch013.xhtml#sec-ai-frameworks):
    Frameworks |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| **开发框架** | [第7章](ch013.xhtml#sec-ai-frameworks): 框架 |'
- en: '| **System Orchestration** | [Chapter 5](ch011.xhtml#sec-ai-workflow): Workflow
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| **系统编排** | [第5章](ch011.xhtml#sec-ai-workflow): 工作流程 |'
- en: '| **Edge Deployment** | [Chapter 14](ch020.xhtml#sec-ondevice-learning): On-device
    Learning |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| **边缘部署** | [第14章](ch020.xhtml#sec-ondevice-learning): 设备端学习 |'
- en: '| **Performance Evaluation** | [Chapter 12](ch018.xhtml#sec-benchmarking-ai):
    Benchmarking AI |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| **性能评估** | [第12章](ch018.xhtml#sec-benchmarking-ai): AI基准测试 |'
- en: '| **Privacy & Security** | [Chapter 15](ch021.xhtml#sec-security-privacy):
    Privacy & Security |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| **隐私与安全** | [第15章](ch021.xhtml#sec-security-privacy): 隐私与安全 |'
- en: '| **Energy Sustainability** | [Chapter 18](ch024.xhtml#sec-sustainable-ai):
    Sustainable AI |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| **能源可持续性** | [第18章](ch024.xhtml#sec-sustainable-ai): 可持续AI |'
- en: '| **Alignment & Safety** | [Chapter 17](ch023.xhtml#sec-responsible-ai): Responsible
    AI |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| **对齐与安全** | [第17章](ch023.xhtml#sec-responsible-ai): 负责任的AI |'
- en: '| **Operations** | [Chapter 13](ch019.xhtml#sec-ml-operations): ML Operations
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| **操作** | [第13章](ch019.xhtml#sec-ml-operations): ML操作 |'
- en: Three key AGI concepts apply directly to current practice. First, compound systems
    with specialized components often outperform single large models while being easier
    to debug, update, and scale—the architecture in [Figure 20.5](ch026.xhtml#fig-compound-ai-system)
    applies whether orchestrating multiple models, integrating external tools, or
    coordinating retrieval with generation. Second, the data pipeline in [Figure 20.1](ch026.xhtml#fig-frontier-data-pipeline)
    shows frontier models discard over 90% of raw data through filtering, suggesting
    most projects under-invest in data cleaning and synthetic generation. Third, the
    RLHF pipeline ([Figure 20.3](ch026.xhtml#fig-rlhf-pipeline)) demonstrates that
    alignment through preference learning proves essential for user satisfaction at
    any scale, from customer service bots to recommendation engines.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 三个关键的AGI概念直接应用于当前实践。首先，具有专用组件的复合系统通常比单个大型模型表现更好，同时更容易调试、更新和扩展——[图20.5](ch026.xhtml#fig-compound-ai-system)中的架构适用于协调多个模型、集成外部工具或协调检索与生成的协调。第二，[图20.1](ch026.xhtml#fig-frontier-data-pipeline)中的数据管道显示前沿模型通过过滤丢弃了超过90%的原始数据，这表明大多数项目在数据清理和合成生成方面的投资不足。第三，RLHF管道([图20.3](ch026.xhtml#fig-rlhf-pipeline))表明，通过偏好学习实现的对齐对于任何规模的用户满意度至关重要，从客户服务机器人到推荐引擎。
- en: The principles covered throughout this textbook provide the foundation; AGI
    frontiers push these principles toward their ultimate expression as distributed
    systems expertise, hardware-software co-design knowledge, and human-AI interaction
    understanding become increasingly critical.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 本教材中涵盖的原则提供了基础；AGI 前沿推动这些原则向其最终表达——分布式系统专业知识、软硬件协同设计知识和人机交互理解变得越来越关键。
- en: Core Design Principles for AGI Systems
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AGI 系统的核心设计原则
- en: 'AGI trajectory remains uncertain. Breakthroughs may emerge from unexpected
    directions: transformers displaced RNNs in 2017 despite decades of LSTM dominance,
    state space models achieve transformer performance with linear complexity, and
    quantum neural networks could provide exponential speedups for specific problems.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: AGI 轨迹仍然不确定。突破可能来自意想不到的方向：2017年，尽管LSTM几十年来占据主导地位，但变压器取代了RNN，状态空间模型以线性复杂度实现了变压器的性能，而量子神经网络可能为特定问题提供指数级的加速。
- en: This uncertainty amplifies systems engineering value. Regardless of architectural
    breakthroughs, successful approaches require efficient data processing pipelines
    handling exabyte-scale datasets, scalable training infrastructure supporting million-GPU
    clusters, optimized model deployment across heterogeneous hardware, robust operational
    practices ensuring 99.99% availability, and integrated safety and ethics frameworks.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不确定性放大了系统工程的价值。无论是否存在架构突破，成功的方法都需要高效的数据处理管道来处理埃字节级的数据集，可扩展的训练基础设施来支持百万GPU集群，优化跨异构硬件的模型部署，确保99.99%可用性的稳健操作实践，以及集成安全和伦理框架。
- en: The systematic approaches to distributed systems, efficient deployment, and
    robust operation covered throughout this textbook remain essential whether AGI
    emerges from scaled transformers, compound systems, or entirely new architectures.
    Engineering principles transcend specific technologies, providing foundations
    for intelligent system construction across any technological trajectory.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 本教材中涵盖的分布式系统、高效部署和稳健操作的系统方法，无论AGI是从扩展的变压器、复合系统还是全新的架构中产生，都仍然是必要的。工程原则超越了特定技术，为任何技术轨迹上的智能系统构建提供了基础。
- en: Fallacies and Pitfalls
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谬误和陷阱
- en: The path toward artificial general intelligence presents unique systems engineering
    challenges where misconceptions about effective approaches have derailed projects,
    wasted resources, and generated unrealistic expectations. Understanding what not
    to do proves as valuable as understanding proper approaches, particularly when
    each fallacy contains enough truth to appear compelling while ignoring crucial
    engineering considerations.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 通往通用人工智能的道路提出了独特的系统工程挑战，其中对有效方法的误解已经使项目脱轨、浪费资源并产生了不切实际的期望。了解不应该做什么与了解正确的方法一样有价值，尤其是当每个谬误都包含足够的真理以显得有说服力，同时忽略了关键工程考虑时。
- en: '**Fallacy:** *AGI will emerge automatically once models reach sufficient scale
    in parameters and training data.*'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *一旦模型在参数和训练数据规模上达到足够大，AGI 将自动出现。*'
- en: This “scale is all you need” misconception leads teams to believe that current
    AI limitations simply reflect insufficient model size and that making models bigger
    inevitably yields AGI. While empirical scaling laws show consistent improvements
    (GPT-3’s 175B parameters significantly outperforming GPT-2’s 1.5B across benchmarks),
    this reasoning ignores that architectural innovation, efficiency improvements,
    and training paradigm advances prove equally essential. The human brain achieves
    intelligence through 86 billion neurons ([Azevedo et al. 2009](ch058.xhtml#ref-azevedo2009equal))
    comparable to mid-sized language models via sophisticated architecture and learning
    mechanisms rather than scale alone, demonstrating 10⁶× better energy efficiency
    than current AI systems. Scaling GPT-3 ([T. Brown et al. 2020](ch058.xhtml#ref-brown2020language))
    from 175B to hypothetical 17.5T parameters would require $10B training costs consuming
    5 GWh equivalent to a small town’s annual electricity, yet would still lack persistent
    memory, efficient continual learning, multimodal grounding, and robust reasoning
    essential for AGI. Effective AGI development requires balancing infrastructure
    investment in larger training runs with research investment in novel architectures
    explored through mixture-of-experts ([Section 20.4.2.2](ch026.xhtml#sec-agi-systems-expert-routing-compound-systems-0e3e)),
    retrieval augmentation ([Section 20.4.2.3](ch026.xhtml#sec-agi-systems-external-memory-compound-systems-648c)),
    and modular reasoning ([Section 20.4.2.4](ch026.xhtml#sec-agi-systems-modular-reasoning-architectures-be96))
    patterns that enable capabilities inaccessible through pure scaling.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“规模就是一切”的误解导致团队相信，当前人工智能的限制仅仅是模型规模不足的反映，而增大模型规模必然会导致AGI的出现。虽然经验性的规模定律显示出持续改进（GPT-3的1750亿参数在基准测试中显著优于GPT-2的15亿参数），但这种推理忽略了架构创新、效率提升和训练范式进步同样至关重要的因素。人脑通过860亿个神经元([Azevedo等人，2009](ch058.xhtml#ref-azevedo2009equal))实现智能，与中等规模的语言模型通过复杂的架构和学习机制相当，而不是仅仅依靠规模，这比当前的人工智能系统展示了10⁶倍的能源效率。将GPT-3([T.
    Brown等人，2020](ch058.xhtml#ref-brown2020language))从1750亿参数扩展到假设的1750万亿参数将需要100亿美元的培训成本，消耗相当于一个小镇年度电力的5
    GWh，但仍然缺乏持续的记忆、高效的持续学习、多模态基础和稳健推理，这些都是AGI所必需的。有效的AGI开发需要平衡在更大规模训练运行中的基础设施投资与通过专家混合([第20.4.2.2节](ch026.xhtml#sec-agi-systems-expert-routing-compound-systems-0e3e))、检索增强([第20.4.2.3节](ch026.xhtml#sec-agi-systems-external-memory-compound-systems-648c))和模块化推理([第20.4.2.4节](ch026.xhtml#sec-agi-systems-modular-reasoning-architectures-be96))模式的研究投资，这些模式能够实现仅通过纯粹扩展无法达到的能力。
- en: '**Fallacy:** *Compound AI systems represent temporary workarounds that true
    AGI will render obsolete.*'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *复合人工智能系统只是临时解决方案，真正的通用人工智能（AGI）将会使其过时。*'
- en: The belief that AGI will be a single unified model making compound systems (combinations
    of models, tools, retrieval, and databases) unnecessary ignores computer science
    principles about modular architectures. While compound systems introduce complexity
    through multiple components, interfaces, and failure modes, modular architectures
    with specialized components enable independent optimization, graceful degradation,
    incremental updates, and debuggable behavior essential for production systems
    at any scale. Even biological intelligence employs specialized neural circuits
    for vision, motor control, language, and memory coordinated through structured
    interfaces rather than monolithic processing. GPT-4’s ([OpenAI et al. 2023](ch058.xhtml#ref-openai2023gpt4))
    code generation accuracy improves from 48% to 89% when augmented with code execution,
    syntax checking, and test validation, compound components that verify and refine
    outputs. This pattern generalizes across retrieval augmentation enabling current
    knowledge access, tool use enabling precise computation, and safety filters ensuring
    appropriate behavior, with these capabilities remaining essential regardless of
    base model size. Production AGI systems require embracing compound architectures
    as core patterns, investing in orchestration infrastructure ([Chapter 5](ch011.xhtml#sec-ai-workflow)),
    component interfaces, and composition patterns that establish organizational practices
    essential for AGI-scale deployment.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 认为AGI将是一个单一统一模型，使得复合系统（模型的组合、工具、检索和数据库）变得不必要的信念，忽视了计算机科学关于模块化架构的原则。虽然复合系统通过多个组件、接口和故障模式引入了复杂性，但具有专用组件的模块化架构能够实现独立优化、优雅降级、增量更新和可调试的行为，这对于任何规模的生成系统都是必不可少的。即使是生物智能也使用专门的神经网络来处理视觉、运动控制、语言和记忆，这些通过结构化接口而不是单一处理进行协调。GPT-4的代码生成精度在增加了代码执行、语法检查和测试验证等复合组件后，从48%提高到89%，这些组件验证和细化了输出。这种模式在检索增强中普遍适用，使得当前知识访问、工具使用实现精确计算以及安全过滤器确保适当行为成为可能，这些能力在基础模型大小方面仍然是必不可少的。生产级AGI系统需要接受复合架构作为核心模式，投资于编排基础设施（[第5章](ch011.xhtml#sec-ai-workflow)）、组件接口和组合模式，这些模式建立了对于AGI规模部署至关重要的组织实践。
- en: '**Fallacy:** *AGI requires entirely new engineering principles making traditional
    software engineering irrelevant.*'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *AGI需要全新的工程原则，使得传统的软件工程变得无关紧要*。'
- en: This misconception assumes that AGI’s unprecedented capabilities necessitate
    abandoning existing ML systems practices for revolutionary approaches different
    from current engineering. AGI extends rather than replaces systems engineering
    fundamentals, with distributed training ([Chapter 8](ch014.xhtml#sec-ai-training)),
    efficient inference ([Chapter 10](ch016.xhtml#sec-model-optimizations)), robust
    deployment ([Chapter 13](ch019.xhtml#sec-ml-operations)), and monitoring remaining
    essential as architectures evolve. Training GPT-4 ([OpenAI et al. 2023](ch058.xhtml#ref-openai2023gpt4))
    required coordinating 25,000 GPUs through sophisticated distributed systems engineering
    applying tensor parallelism, pipeline parallelism, and data parallelism from [Chapter 8](ch014.xhtml#sec-ai-training),
    while AGI-scale systems will demand 100-1000× this coordination. Engineers ignoring
    distributed systems principles in pursuit of “revolutionary AGI engineering” will
    recreate decades of hard-won lessons about consistency, fault tolerance, and performance
    optimization. Effective AGI development requires mastering fundamentals in data
    engineering ([Chapter 6](ch012.xhtml#sec-data-engineering)), training infrastructure,
    optimization, hardware acceleration ([Chapter 11](ch017.xhtml#sec-ai-acceleration)),
    and operations that scale to AGI requirements through strong software engineering
    practices, distributed systems expertise, and MLOps discipline rather than abandoning
    proven principles.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这种误解认为，AGI前所未有的能力需要放弃现有的机器学习系统实践，转而采用与当前工程不同的革命性方法。AGI扩展而非取代系统工程的基本原则，随着架构的演变，分布式训练（[第8章](ch014.xhtml#sec-ai-training)）、高效推理（[第10章](ch016.xhtml#sec-model-optimizations)）、稳健部署（[第13章](ch019.xhtml#sec-ml-operations)）和监控仍然至关重要。训练GPT-4（[OpenAI等，2023](ch058.xhtml#ref-openai2023gpt4)）需要通过复杂的分布式系统工程，应用第8章（ch014.xhtml#sec-ai-training）中的张量并行、流水线并行和数据并行来协调25,000个GPU，而AGI规模系统将需要100-1000倍于此的协调。工程师在追求“革命性的AGI工程”时忽视分布式系统原则，将重新创造几十年前关于一致性、容错性和性能优化的宝贵经验教训。有效的AGI开发需要掌握数据工程（[第6章](ch012.xhtml#sec-data-engineering)）、训练基础设施、优化、硬件加速（[第11章](ch017.xhtml#sec-ai-acceleration)）以及通过强大的软件工程实践、分布式系统专长和MLOps纪律来满足AGI要求的运营，而不是放弃经过验证的原则。
- en: '**Pitfall:** *Treating biological intelligence as a complete template for AGI
    implementation.*'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *将生物智能视为AGI实施的完整模板。*'
- en: Many teams assume that precisely replicating biological neural mechanisms in
    silicon provides the complete path to AGI, attracted by the brain’s remarkable
    energy efficiency (20 W for 10¹⁵ operations/second) and neuromorphic computing’s
    1000× efficiency gains for certain workloads. While biological principles provide
    valuable insights around event-driven computation, hierarchical development, and
    multimodal integration, biological and silicon substrates operate on different
    physics with different strengths. Digital systems excel at precise arithmetic,
    reliable storage, and rapid communication that biological neurons cannot match,
    while biological neurons achieve analog computation, massive parallelism, and
    low-power operation difficult in digital circuits. Neuromorphic chips like Intel’s
    Loihi ([Mike Davies et al. 2018](ch058.xhtml#ref-davies2018loihi)) achieve impressive
    efficiency for event-driven workloads such as object tracking and gesture recognition
    but struggle with dense matrix operations where GPUs excel. Optimal AGI architectures
    likely require hybrid approaches extracting biological principles (sparse activation,
    hierarchical learning, multimodal integration, continual adaptation) while leveraging
    digital strengths (precise arithmetic, reliable storage) rather than direct replication.
    Effective engineering focuses on computational principles like event-driven processing
    and developmental learning stages rather than biological implementation details.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 许多团队认为，在硅中精确复制生物神经机制是通往通用人工智能的完整路径，这吸引了大脑惊人的能效（每秒10¹⁵次操作消耗20瓦）和神经形态计算在特定工作负载上1000倍的效率提升。虽然生物原理在事件驱动计算、分层发展和多模态集成方面提供了宝贵的见解，但生物和硅基板在不同的物理上运行，具有不同的优势。数字系统在精确算术、可靠存储和快速通信方面表现出色，这是生物神经元无法比拟的，而生物神经元则实现了模拟计算、大规模并行和低功耗操作，这在数字电路中很难实现。像英特尔Loihi这样的神经形态芯片（[Mike
    Davies等人，2018](ch058.xhtml#ref-davies2018loihi)）在事件驱动工作负载（如目标跟踪和手势识别）上实现了令人印象深刻的效率，但在GPU擅长的密集矩阵运算方面却力不从心。最佳的通用人工智能架构可能需要混合方法，提取生物原理（稀疏激活、分层学习、多模态集成、持续适应）同时利用数字优势（精确算术、可靠存储），而不是直接复制。有效的工程重点在于计算原理，如事件驱动处理和发育学习阶段，而不是生物实现细节。
- en: Summary
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Artificial intelligence stands at an inflection point where the building blocks
    mastered throughout this textbook assemble into systems of extraordinary capability.
    Large language models demonstrate that engineered scale unlocks emergent intelligence
    through the systematic progression from current achievements to future possibilities
    explored in this chapter.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能正处于一个拐点，本书中掌握的构建块将组装成具有非凡能力的系统。大型语言模型表明，通过从当前成就到本章探索的未来可能性所进行的系统进步，工程规模可以解锁涌现智能。
- en: The narrow AI to AGI transition constitutes a systems engineering challenge
    extending beyond algorithmic innovation to encompass integration of data, compute,
    models, and infrastructure at unprecedented scale. As detailed in [Section 20.2](ch026.xhtml#sec-agi-systems-defining-agi-intelligence-systems-problem-19b9),
    AGI training may require 2.5 × 10²⁶ FLOPs with infrastructure supporting 175,000+
    accelerators consuming 122 MW power and requiring approximately $52 billion in
    hardware costs.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 从窄人工智能（narrow AI）到通用人工智能的过渡构成了一个系统工程挑战，它超越了算法创新，涵盖了在前所未有的规模上整合数据、计算、模型和基础设施。正如[第20.2节](ch026.xhtml#sec-agi-systems-defining-agi-intelligence-systems-problem-19b9)中详细说明的，AGI的训练可能需要2.5
    × 10²⁶浮点运算次数（FLOPs），并需要支持175,000+加速器的基础设施，消耗122兆瓦的电力，并需要大约522亿美元硬件成本。
- en: Compound AI systems provide the architectural foundation for this transition,
    revealing how specialized components solve complex problems through intelligent
    orchestration rather than monolithic scaling.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 复合人工智能系统为这一过渡提供了架构基础，揭示了如何通过智能编排解决复杂问题，而不是通过单体扩展。
- en: '**Key Takeaways**'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: Current AI breakthroughs (LLMs, multimodal models) directly build upon ML systems
    engineering principles established throughout preceding chapters
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前的人工智能突破（如大型语言模型、多模态模型）直接建立在前面章节中确立的机器学习系统工程原则之上
- en: AGI represents systems integration challenges requiring sophisticated orchestration
    across multiple components and technologies
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用人工智能（AGI）代表了需要跨多个组件和技术进行复杂协调的系统集成挑战
- en: Compound AI systems provide practical pathways combining specialized models
    and tools for complex capability achievement
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复合人工智能系统提供了结合专门模型和工具以实现复杂能力目标的实际途径
- en: Engineering competencies developed, from distributed training through efficient
    deployment, constitute essential AGI development requirements
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从分布式训练到高效部署，开发出的工程能力构成了必要的人工智能通用（AGI）开发要求
- en: Future advances emerge from systems engineering improvements equally with algorithmic
    innovations
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来进步来自于系统工程改进与算法创新同等重要
- en: This textbook prepares readers for contribution to this challenge. Understanding
    encompasses data flow through systems ([Chapter 6](ch012.xhtml#sec-data-engineering)),
    model optimization and deployment ([Chapter 10](ch016.xhtml#sec-model-optimizations)),
    hardware acceleration of computation ([Chapter 11](ch017.xhtml#sec-ai-acceleration)),
    and reliable ML system operation at scale ([Chapter 13](ch019.xhtml#sec-ml-operations)).
    These capabilities constitute requirements for next-generation intelligent system
    construction.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这本教科书为读者准备参与这一挑战。理解包括系统中的数据流（[第6章](ch012.xhtml#sec-data-engineering)）、模型优化和部署（[第10章](ch016.xhtml#sec-model-optimizations)）、计算硬件加速（[第11章](ch017.xhtml#sec-ai-acceleration)）以及大规模可靠机器学习系统操作（[第13章](ch019.xhtml#sec-ml-operations)）。这些能力构成了下一代智能系统构建的要求。
- en: AGI arrival timing remains uncertain, whether from scaled transformers or novel
    architectures. Systems engineering principles remain essential regardless of timeline
    or technical approach. Artificial intelligence futures build upon tools and techniques
    covered throughout these chapters, from neural network principles in [Chapter 3](ch009.xhtml#sec-dl-primer)
    to advanced system orchestration in [Chapter 5](ch011.xhtml#sec-ai-workflow).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能通用（AGI）的到来时间仍然不确定，无论是通过扩展的变压器还是新颖的架构。无论时间表或技术方法如何，系统工程原则始终是必要的。人工智能的未来建立在贯穿这些章节的工具和技术之上，从[第3章](ch009.xhtml#sec-dl-primer)中的神经网络原理到[第5章](ch011.xhtml#sec-ai-workflow)中的高级系统编排。
- en: The foundation stands complete, built through systematic mastery of ML systems
    engineering from data pipelines through distributed training to robust deployment.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施已完全建成，这是通过系统性地掌握机器学习系统工程从数据管道到分布式训练再到稳健部署的过程实现的。
- en: '* * *'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
