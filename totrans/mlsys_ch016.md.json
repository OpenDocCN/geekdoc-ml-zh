["```py\nimport torch\nimport torch.nn.utils.prune as prune\n\n# Original dense weight matrix\nweights = torch.tensor(\n    [[0.8, 0.1, -0.7], [0.05, -0.9, 0.03], [-0.6, 0.02, 0.4]]\n)\n\n# Simple magnitude-based pruning: remove weights with magnitude < 0.5\nthreshold = 0.5\nmask = torch.abs(weights) >= threshold\npruned_weights = weights * mask\n\nprint(\"Original:\", weights)\nprint(\"Pruned:\", pruned_weights)\n# Result: 4 out of 9 weights remain (56% sparsity)\n```", "```py\nimport torch\n\n# Original FP32 weights\nweights_fp32 = torch.tensor(\n    [0.127, -0.084, 0.392, -0.203], dtype=torch.float32\n)\nprint(f\"Original FP32: {weights_fp32}\")\nprint(f\"Memory per weight: 32 bits\")\n\n# Simple uniform quantization to INT8 (-128 to 127)\n# Step 1: Find scale factor\nmax_val = weights_fp32.abs().max()\nscale = max_val / 127  # 127 is max positive INT8 value\n\n# Step 2: Quantize using our formula q = round(x/s)\nweights_int8 = torch.round(weights_fp32 / scale).to(torch.int8)\nprint(f\"Quantized INT8: {weights_int8}\")\nprint(f\"Memory per weight: 8 bits (reduced from 32)\")\n\n# Step 3: Dequantize to verify\nweights_dequantized = weights_int8.float() * scale\nprint(f\"Dequantized: {weights_dequantized}\")\nprint(\n    f\"Quantization error: \"\n    f\"{(weights_fp32 - weights_dequantized).abs().mean():.6f}\"\n)\n```", "```py\nimport torch\nfrom torch.quantization import QuantStub, DeQuantStub,\n     prepare_qat\n\n# Define a model with quantization support\nclass QuantizedModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = QuantStub()\n        self.conv = torch.nn.Conv2d(3, 64, 3)\n        self.dequant = DeQuantStub()\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.conv(x)\n        return self.dequant(x)\n\n# Prepare model for quantization-aware training\nmodel = QuantizedModel()\nmodel.qconfig = torch.quantization.get_default_qat_qconfig()\nmodel_prepared = prepare_qat(model)\n```", "```py\nimport torch.nn.utils.prune as prune\n\n# Apply unstructured pruning\nmodule = torch.nn.Linear(10, 10)\nprune.l1_unstructured(module, name=\"weight\", amount=0.3)\n# Prune 30% of weights\n\n# Apply structured pruning\nprune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n```"]