<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>6  Bare-Bones Machine Learning is Insufficient</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>6  Bare-Bones Machine Learning is Insufficient</h1>
<blockquote>原文：<a href="https://ml-science-book.com/insufficient.html">https://ml-science-book.com/insufficient.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part-one.html">Justifying Machine Learning For Science</a></li><li class="breadcrumb-item"><a href="./insufficient.html"><span class="chapter-number">6</span>  <span class="chapter-title">Bare-Bones Machine Learning is Insufficient</span></a></li></ol></nav>
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Even though machine learning has great merits when it comes to prediction, it clashes with other scientific goals like control, explanations, and reasoning. This chapter highlights more concretely and practically the insufficiencies of bare-bones machine learning as a scientific methodology.</p>
<div class="raven-box">
<p>The veteran scientists’ concerns about machine learning were not misplaced. Over time, the tornado system began to act up. False positives and false negatives became more common. Krarah consulted Rattle, but even she was puzzled. Rattle concluded that the problems were not specific to tornado prediction, but systemic to machine learning. They had to be fixed.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../Images/86c10560731ea401cc3d41fe917f8942.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%" data-original-src="https://ml-science-book.com/images/raven-insufficient.jpg"/></p>
</figure>
</div>
</div>
<section id="low-test-error-is-not-enough" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="low-test-error-is-not-enough"><span class="header-section-number">6.1</span> Low test error is not enough</h2>
<p>As we highlighted in <a href="justification.html" class="quarto-xref"><span>Chapter 4</span></a>, machine learning has a transparent notion of what makes a good model: a good model has low error on an unseen test set and in consequence a low generalization error. However, this notion is empty if it is not equipped with an underlying theory of generalization. When can you infer that the model performance generalizes from just knowing the error on a test set? Are there guarantees that certain learning algorithms must yield models with low expected error?</p>
<p>In <a href="generalization.html" class="quarto-xref"><span>Chapter 7</span></a>, we present statistical learning theory as the main contender for a theory of generalization in machine learning.</p>
<p>However, the standard notion of generalization only reflects the predictive capacities of models on data from the same distribution. Extrapolation to completely new scenarios or data from different distributions is not encompassed in the standard notion of generalization. It therefore has limited bite, especially in science, where the goal is to generalize from models to insights about the phenomenon.</p>
<p>In <a href="generalization.html" class="quarto-xref"><span>Chapter 7</span></a>, we therefore also discuss broader conceptions of generalization. <a href="robustness.html" class="quarto-xref"><span>Chapter 11</span></a> on robustness, moreover, shows how to define generalization under distribution shifts.</p>
</section>
<section id="domain-knowledge-is-overlooked" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="domain-knowledge-is-overlooked"><span class="header-section-number">6.2</span> Domain knowledge is overlooked</h2>
<p>Science builds on prior knowledge to produce knowledge. Bare-bones machine learning disrupts both ends of this knowledge flow: You expect the model to figure out relations from the data, and get out an opaque prediction model. The focus shifts from epistemology to utility, from science to engineering. You tinker with the model to improve a metric. If machine learning is fully embraced in its bare-bones form, a lot of domain knowledge is left unused, and there is little new knowledge to gain.</p>
<p>However, coherence with background knowledge makes scientific models more valuable. It is smart to incorporate additional information<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Compare that to statistical modeling or differential equations: These modeling approaches encourage, even require, the formulation of prior knowledge in terms of distribution assumptions and equations. And you get interpretable estimates back that help you better understand the phenomenon you study.</p>
<p>In <a href="domain.html" class="quarto-xref"><span>Chapter 8</span></a>, we argue that you can take a domain-knowledge-driven approach with machine learning as well. Even better: Thanks to the focus on predictive performance, you can evaluate your domain knowledge in terms of predictive performance. <!--With approaches from interpretable machine learning (@sec-interpretability), we may even extract new insights about the studied phenomena.--></p>
</section>
<section id="predictions-cannot-be-easily-explained-and-interpreted" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="predictions-cannot-be-easily-explained-and-interpreted"><span class="header-section-number">6.3</span> Predictions cannot be easily explained and interpreted</h2>
<p>Interpretability enables you to justify models and reason about phenomena. But machine learning models are generally not inherently interpretable, because they may have a complex functional form that is adjusted to data – they are black-boxes. This makes it difficult to understand how the model behaves and what it relies on:</p>
<ul>
<li>What were the most influential features?</li>
<li>Which features did the model learn?</li>
<li>Why did the model make a certain prediction?</li>
</ul>
<p>We show in <a href="interpretability.html" class="quarto-xref"><span>Chapter 9</span></a> how to use interpretability techniques to improve models and gain insights from them.</p>
</section>
<section id="predictive-performance-is-at-odds-with-causality" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="predictive-performance-is-at-odds-with-causality"><span class="header-section-number">6.4</span> Predictive performance is at odds with causality</h2>
<p>The world we live and act in is a gigantic causal mechanism. Bare-bones machine learning models however ignore the causal dimension of things. All they care about is making better predictions, and they rely on whatever statistical dependency to pursue this goal. Do you want to know your COVID risk? Alright, the machine learning model needs your shoe size, salary, and the football ranking of your local club.</p>
<p>But scientists want to separate between causes, effects, or spuriously correlated features. Physicians want to know why certain people have COVID and others don’t. They want to develop drugs and prescribe treatments that make people healthy. To answer these questions, causality must be taken into account.</p>
<p>We’ll show in <a href="causality.html" class="quarto-xref"><span>Chapter 10</span></a> how the combination of causal inference and machine learning allows you to find causal dependencies, learn causal models, and estimate causal effects.</p>
<!--Closely linked to causality is the problem of robustness. We show in chapter @sec-robustness how causal knowledge can help to build more robust ML models.-->
<!--. Causality guides our experiments and understanding causal relations enables control and informed action. Ignoring causal dependencies would be a big mistake as prediction in a static manner often provides poor guidance for actions. For instance, a model may predict you have a high COVID risk and suggest that you can lower your COVID risk by not coughing. However, taking cough syrup does not lower your COVID risk. We discuss the interplay between causality and ML in Chapter ...-->
</section>
<section id="models-lack-robustness-in-deployment" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="models-lack-robustness-in-deployment"><span class="header-section-number">6.5</span> Models lack robustness in deployment</h2>
<p>Bare-bones machine learning provides predictive models, but only for a static environment. This means machine learning models are only accurate, if:</p>
<ul>
<li>they are applied to data that is similar to the training data, and</li>
<li>the data structure remains intact during deployment.</li>
</ul>
<p>In the wild, phenomena are complex. Data changes constantly, external factors enter, measurement devices produce errors, time moves, and observation targets shift. If you want to use machine learning models in your scientific pipeline, you have to make them robust tools under real-world conditions.</p>
<p>We show in <a href="robustness.html" class="quarto-xref"><span>Chapter 11</span></a> what types of robustness you may be interested in and discuss strategies such as data augmentation that help you robustify your model.</p>
</section>
<section id="predictions-come-without-uncertainty-quantification" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="predictions-come-without-uncertainty-quantification"><span class="header-section-number">6.6</span> Predictions come without uncertainty quantification</h2>
<p>Proper uncertainty quantification is crucial when high-stakes decisions are made in the real world. However, bare-bones machine learning models only provide point predictions. Some models, such as Bayesian models, come with built-in uncertainty quantification, but limiting the model class to these models may result in a loss in predictive performance. Perhaps the best-performing model is a random forest. If you choose the best-performing model, you might get one without built-in uncertainty quantification. Even when model outputs look like probabilities because they are between zero and one (yes, we’re talking to you, softmaxers), they often cannot be interpreted as ‘real’ probabilities when models aren’t well calibrated.</p>
<p>We discuss the philosophy behind uncertainty in machine learning, calibration, and model-agnostic methods for uncertainty quantification in <a href="uncertainty.html" class="quarto-xref"><span>Chapter 12</span></a>.</p>
</section>
<section id="no-consensus-on-standards-for-reproducibility" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="no-consensus-on-standards-for-reproducibility"><span class="header-section-number">6.7</span> No consensus on standards for reproducibility</h2>
<p>There are many standards for how to document code, data, and models for others to reproduce results. Reproducibility is key to:</p>
<ul>
<li>being transparent about what you did to achieve high performance,</li>
<li>allowing others to test your work and gain trust in it,</li>
<li>reliably building on your results, and</li>
<li>making your code reusable for potential applications.</li>
</ul>
<p>Unfortunately, many papers that use machine learning in science do not allow for reproducibility <span class="citation" data-cites="mcdermott2021reproducibility"><a href="references.html#ref-mcdermott2021reproducibility" role="doc-biblioref">[1]</a></span>. Often, important information is missing such as on the weight initialization, training epochs, hyperparameters, or random seeds. Also, preprocessing steps might not be listed, and the code is poorly documented or poorly implemented.</p>
<p>We discuss standards for reproducibility of machine learning in science in <a href="reproducibility.html" class="quarto-xref"><span>Chapter 13</span></a>.</p>
</section>
<section id="missing-standards-for-reporting" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="missing-standards-for-reporting"><span class="header-section-number">6.8</span> Missing standards for reporting</h2>
<p>In science, it is of central importance to make transparent why a scientific model is suitable and what data it explains. In machine learning, researchers still need to determine what information is scientifically relevant. Predictive performance on the test set is indeed important. But what about the importance of features? Or information about the data collection or the model selection?</p>
<p>We discuss different standards for reporting model results in chapter <a href="reporting.html" class="quarto-xref"><span>Chapter 14</span></a>. We provide a high-level view of how to group these standards and some best practices for publication.</p>
<!--
## Garbage in, garbage out the importance of data quality in ML 

ML relies heavily on empirical data and less so on theory. While this may be a feature when the theory is questionable, it's a bug when the data is bad. For instance, our data can be noisy, mislabeled, contain only uninteresting features, falsely encoded, too small, or not representative of the population. Bad training data leads to unreliable models. We show how to check data quality and deal with bad data in Chapter...-->
<!--## Distance between data and modeler

Generally, models can hide the data and also the assumptions that are entered during modeling. While this is already present in classical scientific modeling, it is even worse with performance-driven ML. Especially the strong focus on performance leads people to overlook problems such as low-quality data, and missing causal features. We show how to regain connection to your data in chapters @sec-generalization and @sec-interpretability.

## Inadequate guidance in scientific experimenting

One key function of scientific models is guiding scientific experiments. For instance, the model may give interesting or strange predictions for a specific scenario, which incentivizes us to conduct an experiment or investigation into whether the model is right. These interesting predictions are usually extrapolating rather than interpolating, that is they are about situations where we have little to no data. However, as mentioned earlier, ML is bad in extrapolation, and ML extrapolations are generally no guidance for experiments. We show approaches to improve ML model extrapolation skills in chapters @sec-ablation, @sec-domain, and @sec-causality.-->


<div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-mcdermott2021reproducibility" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">M. B. McDermott, S. Wang, N. Marinsek, R. Ranganath, L. Foschini, and M. Ghassemi, <span>“Reproducibility in machine learning for health research: Still a ways to go,”</span> <em>Science Translational Medicine</em>, vol. 13, no. 586, p. eabb1655, 2021, doi: <a href="https://doi.org/10.1126/scitranslmed.abb1655">10.1126/scitranslmed.abb1655</a>.</div>
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr/>
<ol>
<li id="fn1"><p>Usually scientific theories are informed by a lot of historical data; thus, in a certain way incorporating background knowledge increases the data support.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

    
</body>
</html>