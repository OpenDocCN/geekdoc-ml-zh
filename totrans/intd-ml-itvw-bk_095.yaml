- en: 8.2.1 Natural language processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/8.2.1-natural-language-processing.html](https://huyenchip.com/ml-interviews-book/contents/8.2.1-natural-language-processing.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: RNNs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What’s the motivation for RNN?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What’s the motivation for LSTM?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] How would you do dropouts in an RNN?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What’s density estimation? Why do we say a language model is a density
    estimator?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Language models are often referred to as unsupervised learning, but some
    say its mechanism isn’t that different from supervised learning. What are your
    thoughts?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Word embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Why do we need word embeddings?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] What’s the difference between count-based and prediction-based word embeddings?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H] Most word embedding algorithms are based on the assumption that words that
    appear in similar contexts have similar meanings. What are some of the problems
    with context-based word embeddings?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Given 5 documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[M] Given a query Q: “The early bird gets the worm”, find the two top-ranked
    documents according to the TF/IDF rank using the cosine similarity measure and
    the term set {bird, duck, worm, early, get, love}. Are the top-ranked documents
    relevant to the query?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Assume that document D5 goes on to tell more about the duck and the bird
    and mentions “bird” three times, instead of just once. What happens to the rank
    of D5? Is this change in the ranking of D5 a desirable property of TF/IDF? Why?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Your client wants you to train a language model on their dataset but their
    dataset is very small with only about 10,000 tokens. Would you use an n-gram or
    a neural language model?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] For n-gram language models, does increasing the context length (n) improve
    the model’s performance? Why or why not?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] What problems might we encounter when using softmax as the last layer for
    word-level language models? How do we fix it?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What''s the Levenshtein distance of the two words “doctor” and “bottle”?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] BLEU is a popular metric for machine translation. What are the pros and
    cons of BLEU?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H] On the same test set, LM model A has a character-level entropy of 2 while
    LM model A has a word-level entropy of 6\. Which model would you choose to deploy?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Imagine you have to train a NER model on the text corpus A. Would you make
    A case-sensitive or case-insensitive?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Why does removing stop words sometimes hurt a sentiment analysis model?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Many models use relative position embedding instead of absolute position
    embedding. Why is that?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H] Some NLP models use the same weights for both the embedding layer and the
    layer just before softmax. What’s the purpose of this?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
