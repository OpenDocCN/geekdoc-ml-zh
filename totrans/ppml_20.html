<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Chapter 12 Recommending Recommendations: A Recommender System Using Natural Language Understanding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Chapter 12 Recommending Recommendations: A Recommender System Using Natural Language Understanding</h1>
<blockquote>原文：<a href="https://ppml.dev/nlp.html">https://ppml.dev/nlp.html</a></blockquote>
<div id="nlp" class="section level1 hasAnchor" number="12">

<!-- the \chaptermark{} defines a shorter running title that fits in page headers. -->
<p><em>In collaboration with Carlo Lipizzi, Teaching Associate Professor &amp; Program Lead, School of Systems and Enterprises,
Stevens Institute of Technology.</em>
</p>
<p>
In Part <a href="hardware.html#"><strong>1</strong></a> we covered the foundations underlying machine learning pipelines; in Part
<a href="design-code.html#"><strong>2</strong></a> we discussed how to create and maintain them well; and in Part <a href="development-tools.html#"><strong>3</strong></a> we provided a
brief overview of the tools and technologies involved. We now put all this material into context by discussing an
abridged version of a real-world machine learning pipeline that Carlo Lipizzi built for the U.S. Department of Defense.
We base our discussion on Lipizzi et al. <span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span> and the references therein. An adapted version of the
pipeline code and configurations is available at</p>
<blockquote>
<p><a href="https://github.com/pragprogml">https://github.com/pragprogml</a> .</p>
</blockquote>
<p>We first define the scope of the pipeline and put it into the appropriate domain context (Section <a href="nlp.html#nlp-domain">12.1</a>). We
then outline the machine learning models involved and how we can think of them as a data processing pipeline (Section
<a href="nlp.html#nlp-model">12.2</a>). Finally, we sketch a suitable hardware and software infrastructure for the pipeline to run on
(Section <a href="nlp.html#nlp-infra">12.3</a>) and the modules that are most interesting from a software engineering perspective (Section
<a href="nlp.html#nlp-architecture">12.4</a>).</p>
<div id="nlp-domain" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> The Domain Problem<a href="nlp.html#nlp-domain" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>
The first step in creating a machine learning pipeline is to define its scope, starting with the problem it will try to
solve (Section <a href="design-code.html#scoping-pipeline">5.3.1</a>). Lipizzi et al. <span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span> frame it as follows:</p>
<blockquote>
<p>“a system to determine what are the most relevant recommendations that stakeholders are providing to the Defense
Acquisition community […] extracting user-specific relevance from text and recommending a document or part
of it.”</p>
</blockquote>
<p>In other words, we envisage that users will submit one or more documents and that the machine learning pipeline will
rank them in terms of overall relevance, highlighting the most relevant passages in each document at the same time. This
would be an ideal opening in the mission statement document (Section <a href="documenting-code.html#domaindocs">8.4</a>).
</p>
<p>The domain metrics that Lipizzi et al. <span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span> focus on are the <em>relevance of each document</em>, which is
defined as:</p>
<blockquote>
<p>“By counting the number of words with a similarity more than a threshold (such as 0.50) and normalising it with
respect to the number of the words in each document, an average similarity measure is calculated that presents the
level of similarity of the entire document with respect to the entire benchmarks. […] A document with a higher
measure is more relevant or like the benchmarks.”</p>
</blockquote>
<p>and the <em>relevance of individual passages</em>, which is defined as follows:</p>
<blockquote>
<p>“To determine the relevant parts of each recommendation, the document was looked at in segments of words. […] It was
found that with a window of 20 words from the similarity matrix, the actual document (which includes the raw text)
would have a window of 35 words that would make up important and relevant recommendations. To assure high-quality
moving average windows, the threshold of average similarity is set to 0.75. Any window of words above that threshold
is then traced back to the original document and is highlighted.”</p>
</blockquote>
<p>What is the threshold for success? From a domain perspective, we want relevant documents to be ranked consistently
higher than unrelated documents.</p>
<blockquote>
<p>“A good indicator that the model learned is that the control document’s similarity (0.25) was significantly lower than
the worst recommendation document (0.5). This means that the model did an accurate job of learning the domain of
recommendations and finding the parallels in the documents.”</p>
</blockquote>
<p>In statistical terms, we can evaluate the performance of the pipeline using any of the popular measures of rank
agreement. If we have access to a set of documents labelled by domain experts as either relevant or unrelated, a simple
but effective choice may be the hit ratio among the top <span class="math inline">\(k\)</span> documents:
<span class="math display">\[\begin{equation*}
  \mathrm{HR} = \frac{\text{number of relevant documents among the top $k$}}{k}\,.
\end{equation*}\]</span>
Firstly, we may assume that users will only look for relevant results among the first <span class="math inline">\(k\)</span> of documents: after all,
70%–90% of users never go beyond the first page of Google results <span class="citation">(Shelton <a href="#ref-first-page" role="doc-biblioref">2017</a>)</span>. Therefore, the accuracy of the ranking
of later documents is not as important from a domain perspective. Secondly, the labelling of the documents will
inevitably be noisy (Section <a href="design-code.html#data-debt">5.2.1</a>): different domain experts will produce different rankings. Hopefully,
we can estimate HR using a subset of documents that all experts agree are either highly relevant or unrelated. Granular
measures of rank agreement such as Kendall’s <span class="math inline">\(\tau\)</span> or Spearman’s <span class="math inline">\(\rho\)</span> may not be robust against the noise in the
labels, limiting our ability to contrast the performance of different machine learning models. In turn, this may impact
the ability of the monitoring infrastructure (Section <a href="design-code.html#monitoring-pipeline">5.3.6</a>) to automatically trigger model
retraining (Section <a href="design-code.html#model-pipeline">5.3.4</a>) or rollbacks (Section <a href="deploying-code.html#rollback">7.6</a>).

</p>
<p>For the ranking to be well-defined, we should specify what “relevant” means. Lipizzi et al. <span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span>
base the pipeline design on the <em>room theory</em> framework developed in their previous work <span class="citation">(Lipizzi, Borrelli, and de Oliveira Capela <a href="#ref-room-theory" role="doc-biblioref">2021</a>)</span>. The key
idea behind this framework is that we need to make machine learning models aware of the context they operate in
to achieve a semantic understanding of the documents they analyse. The same word may have different meanings in
different domains: disambiguating them and their relevance is essential to move from natural language processing (NLP)
to natural language understanding (NLU). Lipizzi et al. <span class="citation">(Lipizzi, Borrelli, and de Oliveira Capela <a href="#ref-room-theory" role="doc-biblioref">2021</a>)</span> propose to achieve NLU by having domain experts
carefully select the documents the models will be trained on to form a knowledge base for a specific topic. In addition,
experts identify the key terms in the domain and give them weights to encode their relative importance. New documents
will be compared to this knowledge base, as distilled by a machine learning model: the more similar they are to those in
the knowledge base, the more they are considered relevant. Clearly, this approach does not work if we train our models
on a large general-purpose data set like the Wikipedia corpus: as we argued in Section <a href="design-code.html#scoping-pipeline">5.3.1</a>,
identifying what data we need to collect is essential for the pipeline to perform well. In the case of Lipizzi et al.
<span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span>, these data are a corpus of documents on a specific type of goods or services that is within the
purview of the procurement processes of the U.S. Department of Defense. The corpus should be large enough to cover all
the relevant information on that type of goods or services: if it is too small, it may not contain all key terms and
phrases or it may not allow us to model their relationships accurately. On the other hand, if it is too large, it may
lack focus and it may lower the quality of the rankings produced by the models. We should train the machine learning
models in the pipeline using only documents on the exact same topic as those that the pipeline will be ranking (Section
<a href="troubleshooting-code.html#data-problems">9.1</a>). Limiting the focus of the pipeline in this way may also help in preventing models from becoming
stale (Section <a href="design-code.html#data-debt">5.2.1</a>) as quickly, since there will be fewer relevant terms and they will only be used with a
specific technical meaning.</p>
</div>
<div id="nlp-model" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> The Machine Learning Model<a href="nlp.html#nlp-model" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>
The pipeline in Lipizzi et al. <span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span> is relatively straightforward from a machine learning perspective
because it includes just a single model. As a result, it is not susceptible to model feedback loops (Section
<a href="design-code.html#model-debt">5.2.2</a>) and is robust against correction cascades (Sections <a href="design-code.html#model-debt">5.2.2</a> and
<a href="troubleshooting-code.html#troubleshooting-heterogeneous-data">9.1.2</a>). The data and the models interact in the pipeline as follows:
</p>
<ol style="list-style-type: decimal">
<li>The documents that encode the domain knowledge on the acquisition of a specific type of goods or services are
ingested and prepared (Section <a href="design-code.html#data-pipeline">5.3.3</a>) using standard NLP techniques including those described in
Section <a href="types-structures.html#string-types">3.1.3</a>. They are a static data set that is used for training (Section <a href="design-code.html#model-pipeline">5.3.4</a>).</li>
<li>The domain knowledge is distilled from the documents into word embeddings using word2vec <span class="citation">(Rong <a href="#ref-word2vec" role="doc-biblioref">2014</a>)</span> and the list of
key terms with the associated weights provided by the domain experts. The embeddings represent what Lipizzi et al.
<span class="citation">(Lipizzi, Borrelli, and de Oliveira Capela <a href="#ref-room-theory" role="doc-biblioref">2021</a>)</span> call the “room” and are the core of our machine learning model. The key terms are the vocabulary
we pass to word2vec.</li>
<li>Inference (Section <a href="design-code.html#production-pipeline">5.3.5</a>) involves users submitting new documents which are then prepared in the
same way as those in the training set. The relevance of each document is measured as the degree of similarity with
the “room” by pooling cosine distance and scaling it by the document’s length. Therefore, the machine
learning model outputs a scalar number between 0 and 1 which is then used to rank the model.</li>
<li>At the same time, the model parses each new document sequentially, identifies sequences of words with high relevance
and highlights them. Therefore, each inference request also returns a modified version of the document that was
submitted by the user.</li>
</ol>
<p>Training and inference are both computationally efficient. The time complexity (Chapter <a href="algorithms.html#algorithms">4</a>) of training
varies between <span class="math inline">\(O(N \log(V))\)</span> and <span class="math inline">\(O(NV)\)</span>, where <span class="math inline">\(N\)</span> is the number of documents and <span class="math inline">\(V\)</span> is the number of words in the
vocabulary, depending on the implementation of word2vec. Inference is <span class="math inline">\(O(N)\)</span> both for estimating relevance and for
highlighting relevant portions of text, and it can be implemented as a single pass over each document. In addition, word
embeddings can be updated when new documents are available <span class="citation">(Kaji and Kobayashi <a href="#ref-kaji" role="doc-biblioref">2017</a>)</span>: there is no need to relearn them from scratch when
the embeddings become stale.</p>
<p>In practice, Lipizzi et al. <span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span> limit <span class="math inline">\(V\)</span> by asking the domain experts to provide a list of a few hundred
key terms: manually assembling such a list is feasible because we are targeting a single type of goods or services as
discussed in Section <a href="nlp.html#nlp-domain">12.1</a>. The sample size requirements of word2vec are dramatically reduced for the same
reason <span class="citation">(Dusserre and Padró <a href="#ref-word2vec-ssize" role="doc-biblioref">2017</a>)</span>, so both <span class="math inline">\(N\)</span> and <span class="math inline">\(V\)</span> are limited for practical purposes. We can, however, expand the scope by
extending the pipeline to include an ensemble of models (one for each type of goods or services) in which the
appropriate model is selected either (manually) by the user or (automatically) by matching the new document to the
closest “room”. The latter task can reuse the inference module that computes document similarity, so it has linear time
complexity and should not noticeably impact the time complexity of the pipeline.</p>
<p>The inputs and outputs of each of data ingestion, data preparation, model training and inference have well-defined
characteristics that make it easy to construct a suite of software tests based on property-based testing (Section
<a href="troubleshooting-code.html#testing">9.4</a>) and to monitor their behaviour in production (Section <a href="design-code.html#monitoring-pipeline">5.3.6</a>). The models and the
algorithms involved are easy to replace with new ones that have better performance in model evaluation and validation
(Section <a href="design-code.html#model-pipeline">5.3.4</a>) because we can demonstrate them to be functionally equivalent to those we are currently
using. In particular:</p>
<ul>
<li>Data ingestion takes PDFs containing text as inputs and outputs the words therein as a vector of strings.</li>
<li>Data preparation takes a vector of strings as input, performs the operations discussed in Section <a href="types-structures.html#string-types">3.1.3</a>
and outputs a second vector of strings containing only the key terms in the list provided by the domain experts.</li>
<li>Model training takes the output from data preparation as an input and outputs the word embeddings, which can be either
a sparse or a dense matrix (Sections <a href="types-structures.html#matrices">3.2.3</a> and <a href="algorithms.html#bigO-sparsem">4.5.2</a>).</li>
<li>Inference takes the word embeddings and one or more new, preprocessed documents as inputs and outputs a relevance
score (a scalar) and a document with highlights as outputs. The outputs may or may not be ordered in order of
reverse relevance, depending on whether they are meant for programmatic use rr for a dashboard.</li>
</ul>
<p>We should test that data ingestion correctly handles well-formed PDFs, and either fails outright or degrades gracefully
when handed malformed PDFs or PDFs with structured data that cannot be parsed as text (for instance, tables and
equations). Optionally, we could augment data ingestion with bitmapping and OCR to try and salvage such documents. Data
preparation should handle text related to the goods or services within the scope of the pipeline, dropping unrelated
words and rejecting texts in a language different from English. We should also test that model training and inference
complete successfully for boundary and valid inputs (say, documents with no relevant keyword, just one relevant keyword,
all relevant keywords) and to fail for invalid inputs (say, empty documents, <code>NA</code> strings). Finally, we should add
integration and system testing to examine the stability of all outputs and of the pipeline as a whole: submitting PDFs
containing text with small perturbations (replacing a word with a synonym, etc.) should result in very similar
relevance scores. We can do the same with invariants like punctuation and capitalisation, both of which should be
removed during data preparation. These tests and the corresponding monitoring facilities should be designed to cover all
parts of the pipeline, to be as few as possible (Section <a href="troubleshooting-code.html#test-coverage">9.4.6</a>) and to be fast enough to allow for live
monitoring.</p>
<p>Having such a suite of software tests integrated in our CI/CD and monitoring facilities makes it possible to safely plug
in new software and models to upgrade different parts of the pipeline. However, we should measure and log how many
resources the upgraded parts use (Section <a href="design-code.html#monitoring-pipeline">5.3.6</a>). Firstly, we should ensure that the hardware
infrastructure we will draw up in Section <a href="nlp.html#nlp-infra">12.3</a> is sufficient to run them or scale it as appropriate (Section
<a href="hardware.html#hardware-choice">2.4</a>). Secondly, monitoring facilities should still be able to provide real-time feedback. After all,
NLP models are notorious for being resource intensive (Section <a href="troubleshooting-code.html#troubleshooting-costly-models">9.2.3</a>)! For the particular
application in Lipizzi et al. <span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span>, it is particularly important for inference to have low latency because
we envisage that users will expect the documents to be ranked in real time.</p>
</div>
<div id="nlp-infra" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> The Infrastructure<a href="nlp.html#nlp-infra" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>How the software implementation of the pipeline should be divided into modules should be apparent: the data processing
steps we described in Section <a href="nlp.html#nlp-model">12.2</a> map well to the general architecture we discussed in Section
<a href="design-code.html#processing-pipeline">5.3</a>. In order to support it, we should perform some capacity planning and estimate its compute,
memory and storage needs (Section <a href="hardware.html#hardware-choice">2.4</a>).</p>
<p>The pipeline described in Lipizzi et al. <span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span> is not particularly demanding in terms of computing power.
The narrow focus of the “room” on a single type of goods or services means that we can keep our training set small and
limit our storage needs as well. We do not have stringent memory requirements either: the word embeddings are limited in
size because of the limited number of key terms in the vocabulary. Furthermore, we do not need to load the complete
training set into memory to learn them: we can use the documents in smaller batches and learn the embeddings
incrementally <span class="citation">(Kaji and Kobayashi <a href="#ref-kaji" role="doc-biblioref">2017</a>)</span>.</p>
<p>Therefore, at a bare minimum, we need:</p>
<ul>
<li>a machine learning system for model training, optimised for compute and memory;</li>
<li>a set of systems with less memory and compute but good network connectivity to distribute the inference load and keep
latency low;</li>
<li>a storage system to hold the PDF documents used for training, the prepared textual data we extract from them and a
model repository with the embeddings;</li>
<li>a separate system hosting the pipeline orchestrator, the CI/CD infrastructure and the server components of logging and
monitoring (Section <a href="design-code.html#monitoring-pipeline">5.3.6</a>).</li>
</ul>
<p>We should also take care of assigning sufficient resources to all the environments we use (test, pre-production,
production) and of making as similar as possible in their hardware configurations.</p>
<p>The machine learning system dedicated to model training may be a local system: this facilitates experimentation because
observability is more limited on remote systems (Section <a href="hardware.html#hardware-cloud">2.3</a>). Furthermore, we should plan for the
future: we should equip it with GPUs or TPUs to be able to explore more complicated NLP models (with an eye towards
adopting them and replacing word2vec if they perform better) and to accelerate word2vec to the point where increasing
the size of the training set over time becomes a non-issue. It then makes sense for the storage systems holding the raw
and prepared data to be local systems as well, and to be placed in the same facility as that performing model training
to reduce the overhead of data access in model training. Cold storage (Section <a href="hardware.html#hardware-memory">2.1.2</a>) is suitable for
raw data (the PDF documents): we need to access them only when working on data ingestion and preparation, not for
training. Hot storage may be better for the prepared data, again to limit the overhead of data accesses and increase
operational intensity (Section <a href="hardware.html#hardware-using">2.2</a>).</p>
<p>
In contrast, the machine learning systems running the inference modules are better placed in geographically-distributed
cloud instances if the users are spread over the world, which is definitely the case for U.S. Defense personnel, to
reduce latency across the board. We may locate the model repository in the same cloud to facilitate model deployment
(Section <a href="design-code.html#production-pipeline">5.3.5</a> and Chapter <a href="deploying-code.html#deploying-code">7</a>).
</p>
<p>Finally, the orchestrator, the CI/CD infrastructure, and the logging and monitoring facilities should be placed on
completely separate hardware and network connections to ensure they will be available and accessible regardless of any
hardware or software issues affecting the other modules in the pipeline. We should also set them up (or the MLOps
platform, if we are using one in their place) in a clustered configuration to avoid single points of failure and strive
for maximum scalability and reliability. They will be required to restore the pipeline to a functional state, for
instance, by rolling back malfunctioning machine learning models. Keeping the model registry in the cloud makes
replicating it it in different geographical regions easier, increasing its availability and reliability in adverse
scenarios.</p>
<p>How can we design a backup and disaster recovery strategy? That depends on how we manage our infrastructure and on
whether the infrastructure is local, remote or a mix of both. If we can rely on configuration management and we have our
infrastructure completely described as-code, it may be preferable to re-create it from scratch and re-run the CI/CD
pipeline. Just re-running the CI/CD pipeline may be enough to fix minor issues such as a botched module deployment. For
instance, Kubernetes <span class="citation">(The Kubernetes Authors <a href="#ref-kubernetes" role="doc-biblioref">2022</a><a href="#ref-kubernetes" role="doc-biblioref">a</a>)</span> can back up the state of any cluster it manages and restore from a single component
to the complete cluster in case of disaster <span class="citation">(Velero Authors <a href="#ref-velero" role="doc-biblioref">2022</a><a href="#ref-velero" role="doc-biblioref">b</a>)</span>. If our infrastructure is not stored as-code, which may be the
case for legacy environments, we can only rely on taking regular snapshots of all systems and restoring them as needed.</p>
<p>
If part of our infrastructure is remote, we should keep in mind that cloud providers and third-party services can fail
and have downtimes of a day or two. Therefore, it is safer to have a set of geographically-distributed systems with a
mix of cloud and local deployments. In the case of inference modules, we can thus ensure that the users or the services
that consume the inference outputs can fall back to a functioning system in case of failures (hopefully handling retries
and fallbacks transparently).
</p>
</div>
<div id="nlp-architecture" class="section level2 hasAnchor" number="12.4">
<h2><span class="header-section-number">12.4</span> The Architecture of the Pipeline<a href="nlp.html#nlp-architecture" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>We aim to develop a pipeline that is as close as possible to production grade for the use case presented by
Lipizzi et al. <span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span>, while keeping it simple enough that it can serve as a useful illustration of the
practices we discussed in Parts <a href="design-code.html#"><strong>2</strong></a> and <a href="development-tools.html#"><strong>3</strong></a>. To make it completely reproducible, we use
only open-source components, installed and executed as standalone applications or from container images <span class="citation">(Docker <a href="#ref-docker" role="doc-biblioref">2022</a><a href="#ref-docker" role="doc-biblioref">a</a>)</span>.
Furthermore, we choose to manage the whole pipeline with a Git <span class="citation">(The Git Development Team <a href="#ref-git-git" role="doc-biblioref">2022</a>)</span> monorepo (Section <a href="writing-code.html#filesystem-structure">6.4</a>)
that encloses all the components to configure, provision, start and monitor its execution. Additional documentation on
the pipeline architecture and a list of the software prerequisites for our development environment can be found on the
<code>README.md</code> file in the repository’s root.</p>
<p>We will use as reference architecture the pipeline structure presented in Section <a href="design-code.html#processing-pipeline">5.3</a>, which
outlines at a high level a pipeline enclosed in five architectural modules:</p>
<ul>
<li>data ingestion and data preparation (Section <a href="nlp.html#nlp-ingestion-preparation">12.4.1</a>);</li>
<li>data tracking and versioning (Section <a href="nlp.html#nlp-tracking-with-dvc">12.4.2</a>);</li>
<li>model training, validation and experiment tracking (Section <a href="nlp.html#nlp-training-exp-tracking">12.4.3</a>);</li>
<li>model packaging (Section <a href="nlp.html#nlp-model-packaging">12.4.4</a>);</li>
<li>CI/CD deployment and inference (Section <a href="nlp.html#nlp-deployment-inference">12.4.5</a>).</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nlu-pipeline"/>
<img src="../Images/27108feb5f2e81c28088058d44055315.png" alt="The architecture of our NLU ML pipeline." width="100%" data-original-src="https://ppml.dev/chapter12/figures/nlu-pipe.svg"/>
<p class="caption">
Figure 12.1: The architecture of our NLU ML pipeline.
</p>
</div>
<p>We decided on this design, shown in Figure <a href="nlp.html#fig:nlu-pipeline">12.1</a>, for two reasons:</p>
<ul>
<li>Pipelines are rarely managed end-to-end by a single software solution in practice: in the vast majority of cases, they
comprise and integrate multiple platforms and components working together.</li>
<li>Different pieces of software have different strengths and weaknesses and each excels in a specific area: as we have
pointed out in Part <a href="development-tools.html#"><strong>3</strong></a>, there is no one-size-fits-all MLOps solution! Using separate solutions for data
engineering, model training and experimental tracking, we can illustrate different open-source tools and how to
interface them.</li>
</ul>
<div id="nlp-ingestion-preparation" class="section level3 hasAnchor" number="12.4.1">
<h3><span class="header-section-number">12.4.1</span> Data Ingestion and Data Preparation<a href="nlp.html#nlp-ingestion-preparation" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>
For reproducibility, we decided to use a set of freely-accessible documents instead of those originally used in Lipizzi
et al. <span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span>: a corpus of scientific articles belonging to a research topic that is fairly homogeneous but,
at the same time, has a large enough number of publications. The corpus we chose comprises the arXiv preprints whose
abstract contains the terms “causal inference”, “causal network”, “counterfactual” or “causal reasoning”, and that were
submitted between August 1, 2021 and August 31, 2022. The resulting query</p>
<pre><code>date_range:from 2021-08-01 to 2022-08-31;abs:"causal inference" OR
  "causal network" OR "counterfactual" OR "causal reasoning"</code></pre>
<p>submitted using the arXiv’s public APIs <span class="citation">(ArXiv <a href="#ref-arxiv-api" role="doc-biblioref">2022</a>)</span>, returns a corpus of 1044 articles with the associated metadata,
including the HTTP URL of the PDF file.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nlu-ingestion"/>
<img src="../Images/8e80f14f4d57f9a9a5ac7ada0ca07bc5.png" alt="Data ingestion and data preparation steps." width="90%" data-original-src="https://ppml.dev/chapter12/figures/ingestion.svg"/>
<p class="caption">
Figure 12.2: Data ingestion and data preparation steps.
</p>
</div>
<p>We implement this part of the pipeline using Apache Airflow <span class="citation">(The Apache Software Foundation <a href="#ref-airflow" role="doc-biblioref">2022</a><a href="#ref-airflow" role="doc-biblioref">a</a>)</span>, which we introduced in Section
<a href="development-tools.html#build-test-doc-tools">10.3</a>. The DAG that represents the data ingestion and data preparation steps is shown in Figure
<a href="nlp.html#fig:nlu-ingestion">12.2</a>: each step is implemented as a Python function and called by Airflow using the generic
<code>PythonOperator</code> and <code>pythonVirtualenvOperator</code> interfaces. More in detail:</p>
<p/>
<ol style="list-style-type: decimal">
<li><em>ArXiv Query</em>: we call the arXiv APIs and process the returned list to extract the PDF URLs.
</li>
<li><em>Article Download</em>: we download the PDFs returned by the query with a multi-threaded HTTP Python client, respecting
the rate limits imposed by arXiv, and we store them in a local filesystem or local object storage (implemented with
MinIO <span class="citation">(MinIO <a href="#ref-minio" role="doc-biblioref">2022</a>)</span>).</li>
<li><em>Text Conversion</em>: we extract the text in PDF into a plain-text file using one of the many available Python
libraries, such as PyPDF2 <span class="citation">(Fenniak <a href="#ref-pypdf2" role="doc-biblioref">2022</a>)</span>, PdfMiner <span class="citation">(Shinyama, Guglielmetti, and Marsman <a href="#ref-pdfminer.six" role="doc-biblioref">2022</a>)</span> or Spacy <span class="citation">(Explosion <a href="#ref-spacy" role="doc-biblioref">2021</a>)</span>. As before, we process multiple
documents in parallel using a thread pool.
</li>
<li><em>Basic Cleaning</em>, <em>n-Gramming</em>, <em>Stopwords Removal</em>: we preprocess the text files using NLP libraries such NLTK
<span class="citation">(NLTK Team <a href="#ref-nltk" role="doc-biblioref">2021</a>)</span>, Spacy <span class="citation">(Explosion <a href="#ref-spacy" role="doc-biblioref">2021</a>)</span> and Gensim <span class="citation">(Řehůřek and Sojka <a href="#ref-gensim" role="doc-biblioref">2022</a><a href="#ref-gensim" role="doc-biblioref">a</a>)</span>. In particular, we perform case conversion, punctuation and stopword
removal, stemming, lemmatisation and n-gramming.</li>
<li><em>Tokens</em>: what is left are tokens<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a>
suitable for modelling in NLP and NLU applications.</li>
</ol>
<p>The Python code for the Airflow DAG provides a programmatic view of how the blocks in Figure <a href="nlp.html#fig:nlu-pipeline">12.1</a> are
implemented and linked together.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="nlp.html#cb48-1" aria-hidden="true"/><span class="cf">with</span> DAG(<span class="st">'ingestion'</span>, ...) <span class="im">as</span> dag:</span>
<span id="cb48-2"><a href="nlp.html#cb48-2" aria-hidden="true"/>    [...]</span>
<span id="cb48-3"><a href="nlp.html#cb48-3" aria-hidden="true"/>    get_article_urls <span class="op">=</span> PythonOperator(</span>
<span id="cb48-4"><a href="nlp.html#cb48-4" aria-hidden="true"/>        task_id<span class="op">=</span><span class="st">'query_arxiv_archive'</span>,</span>
<span id="cb48-5"><a href="nlp.html#cb48-5" aria-hidden="true"/>        python_callable<span class="op">=</span>query_arxiv,</span>
<span id="cb48-6"><a href="nlp.html#cb48-6" aria-hidden="true"/>        op_kwargs<span class="op">=</span>{<span class="st">'query'</span>: query}</span>
<span id="cb48-7"><a href="nlp.html#cb48-7" aria-hidden="true"/>    )</span>
<span id="cb48-8"><a href="nlp.html#cb48-8" aria-hidden="true"/></span>
<span id="cb48-9"><a href="nlp.html#cb48-9" aria-hidden="true"/>    download_article <span class="op">=</span> PythonOperator(</span>
<span id="cb48-10"><a href="nlp.html#cb48-10" aria-hidden="true"/>        task_id<span class="op">=</span><span class="st">'download_from_arxiv_archive'</span>,</span>
<span id="cb48-11"><a href="nlp.html#cb48-11" aria-hidden="true"/>        python_callable<span class="op">=</span>download_arxiv,</span>
<span id="cb48-12"><a href="nlp.html#cb48-12" aria-hidden="true"/>        op_kwargs<span class="op">=</span>{}</span>
<span id="cb48-13"><a href="nlp.html#cb48-13" aria-hidden="true"/>    )</span>
<span id="cb48-14"><a href="nlp.html#cb48-14" aria-hidden="true"/></span>
<span id="cb48-15"><a href="nlp.html#cb48-15" aria-hidden="true"/>    extract_text_from_article <span class="op">=</span> PythonOperator(</span>
<span id="cb48-16"><a href="nlp.html#cb48-16" aria-hidden="true"/>        task_id<span class="op">=</span><span class="st">'extract_text'</span>,</span>
<span id="cb48-17"><a href="nlp.html#cb48-17" aria-hidden="true"/>        python_callable<span class="op">=</span>convert_pdf_to_text</span>
<span id="cb48-18"><a href="nlp.html#cb48-18" aria-hidden="true"/>        op_kwargs<span class="op">=</span>{},</span>
<span id="cb48-19"><a href="nlp.html#cb48-19" aria-hidden="true"/>    )</span>
<span id="cb48-20"><a href="nlp.html#cb48-20" aria-hidden="true"/>    [...]</span>
<span id="cb48-21"><a href="nlp.html#cb48-21" aria-hidden="true"/>    get_article_urls <span class="op">&gt;&gt;</span> download_article</span>
<span id="cb48-22"><a href="nlp.html#cb48-22" aria-hidden="true"/>    download_article <span class="op">&gt;&gt;</span> extract_text_from_article</span>
<span id="cb48-23"><a href="nlp.html#cb48-23" aria-hidden="true"/>    [...]</span></code></pre></div>
<p/>
<p>The two main challenges we tackle are the scalability of extracting the text from the PDF files and the robustness of
the software tests. We achieve scalability with multithreading in the Python code we call from Airflow; we could have
achieved similar results at the level of the Airflow DAG using Celery <span class="citation">(Apache Software Foundation <a href="#ref-celery-executor" role="doc-biblioref">2022</a><a href="#ref-celery-executor" role="doc-biblioref">a</a>)</span> or the Kubernetes <span class="citation">(The Kubernetes Authors <a href="#ref-kubernetes" role="doc-biblioref">2022</a><a href="#ref-kubernetes" role="doc-biblioref">a</a>)</span>
executor, or by completely replacing Airflow with Apache Spark <span class="citation">(The Apache Software Foundation <a href="#ref-spark" role="doc-biblioref">2022</a><a href="#ref-spark" role="doc-biblioref">f</a>)</span>. As for cleaning the extracted text, we develop
a set of custom methods to perform the basic NLP cleaning tasks, and a custom n-gramming method for detecting the
unigrams, bigrams and trigrams identified as the key terms by experts in the domain of causal inference. Both are
organised in dedicated submodules and complemented by unit tests. The n-grams list is a static resource file versioned
in Git and referenced via environment variables in the pipeline stages.</p>
<p>
The output of the DAG is a list of tokens that we will model with word2vec. The tokens, the list of the PDF URLs, the
list of n-grams and the metadata that define the arXiv query are stored inside a data tracking and versioning
repository backed by DVC <span class="citation">(Iterative <a href="#ref-dvc" role="doc-biblioref">2022</a><a href="#ref-dvc" role="doc-biblioref">b</a>)</span> to ensure reproducibility and to allow us to track data provenance, as discussed in
Section <a href="design-code.html#data-pipeline">5.3.3</a>. We can integrate Airflow and DVC with a custom Airflow operator or by calling the <code>dvc</code>
commandline client from the Airflow built-in operator <code>BashOperator</code>.


</p>
<p>
The Airflow DAG is configured to write task logs to <code>stdout</code>, where they are collected by a tool such as Fluentd
<span class="citation">(The Fluentd Project <a href="#ref-fluentd" role="doc-biblioref">2022</a>)</span> and forwarded to a logging database such as Elasticsearch <span class="citation">(Elasticsearch <a href="#ref-elastic" role="doc-biblioref">2022</a>)</span>. Airflow can also be configured to
export task execution metrics to dashboards built by tools such as Grafana <span class="citation">(GrafanaLabs <a href="#ref-grafana" role="doc-biblioref">2022</a>)</span>. The logs themselves take the form
of a JSON object representation of the <code>LogRecord</code> object in the Python Airflow code, which can be passed to the Python
logging module.
</p>
</div>
<div id="nlp-tracking-with-dvc" class="section level3 hasAnchor" number="12.4.2">
<h3><span class="header-section-number">12.4.2</span> Data Tracking and Versioning<a href="nlp.html#nlp-tracking-with-dvc" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>
In addition to ingesting and cleaning the data in a reproducible way, we also want to track all the data sets that are
produced by the steps described in Section <a href="nlp.html#nlp-ingestion-preparation">12.4.1</a>: the DAG may be scheduled to run daily
with different search queries to create additional knowledge domains as described in Lipizzi et al. <span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span>
or to retrain existing word2vec models. Therefore, we choose to version the machine learning code (Section
<a href="writing-code.html#versioning">6.5</a>) together with the text corpus. This allows us to evaluate different NLP frameworks, choices for the
parameters of word2vec and sets of n-grams from the domain experts.</p>
<p>As we mentioned in Section <a href="nlp.html#nlp-ingestion-preparation">12.4.1</a>, we choose DVC to implement data versioning. DVC can also
perform experiment tracking, but we will implement that in Section <a href="nlp.html#nlp-training-exp-tracking">12.4.3</a> with MLflow (which
we introduced in Section <a href="development-tools.html#exploration-experiment-tracking">10.1</a> along with DVC). We initialise the Git repository for
use by DVC, and we pull the tokens produced by the Airflow DAG from the remote object-storage we stored them in with the
command <em>dvc pull</em>. This also pulls the corresponding metadata, which are versioned and stored in a YAML <code>.dvc</code> file
like that below.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode default"><code class="sourceCode default"><span id="cb49-1"><a href="nlp.html#cb49-1" aria-hidden="true"/>outs:</span>
<span id="cb49-2"><a href="nlp.html#cb49-2" aria-hidden="true"/>- md5: 853c9693c5aac78162da1c3b46aec63e</span>
<span id="cb49-3"><a href="nlp.html#cb49-3" aria-hidden="true"/>  size: 2190841</span>
<span id="cb49-4"><a href="nlp.html#cb49-4" aria-hidden="true"/>  path: causal_inference.txt</span>
<span id="cb49-5"><a href="nlp.html#cb49-5" aria-hidden="true"/></span>
<span id="cb49-6"><a href="nlp.html#cb49-6" aria-hidden="true"/>meta:</span>
<span id="cb49-7"><a href="nlp.html#cb49-7" aria-hidden="true"/>  search_query: "causal inference"</span>
<span id="cb49-8"><a href="nlp.html#cb49-8" aria-hidden="true"/>  search_start: 1629410400</span>
<span id="cb49-9"><a href="nlp.html#cb49-9" aria-hidden="true"/>  search_end: 1672441200</span>
<span id="cb49-10"><a href="nlp.html#cb49-10" aria-hidden="true"/>  [...]</span></code></pre></div>
<p>The <code>md5</code> attribute represents the hash of content and the <code>path</code> attribute is the path of the file or directory
relative to the working directory, which defaults to the file’s location. We can then start experimenting using a
development flow like the following.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb50-1"><a href="nlp.html#cb50-1" aria-hidden="true"/>$ <span class="fu">git</span> log --oneline</span>
<span id="cb50-2"><a href="nlp.html#cb50-2" aria-hidden="true"/><span class="ex">669a39e</span> (HEAD -<span class="op">&gt;</span> master, tag: v0.0.1) <span class="ex">-</span> w2v baseline impl.</span>
<span id="cb50-3"><a href="nlp.html#cb50-3" aria-hidden="true"/>[<span class="ex">...</span>]</span>
<span id="cb50-4"><a href="nlp.html#cb50-4" aria-hidden="true"/>$ <span class="ex">dvc</span> remote list # list remote storage configured in DVC</span>
<span id="cb50-5"><a href="nlp.html#cb50-5" aria-hidden="true"/><span class="ex">exp_bucket</span>   s3://exp_bucket</span>
<span id="cb50-6"><a href="nlp.html#cb50-6" aria-hidden="true"/>$ <span class="ex">dvc</span> pull # fetch data from remote storage into the project</span>
<span id="cb50-7"><a href="nlp.html#cb50-7" aria-hidden="true"/><span class="ex">A</span>       datasets/causal_inference.txt</span>
<span id="cb50-8"><a href="nlp.html#cb50-8" aria-hidden="true"/><span class="ex">A</span>       datasets/causal_inference_small.txt</span>
<span id="cb50-9"><a href="nlp.html#cb50-9" aria-hidden="true"/><span class="ex">2</span> files added</span>
<span id="cb50-10"><a href="nlp.html#cb50-10" aria-hidden="true"/>$ <span class="ex">nvim</span> src/train-cli.py # tune the training code</span>
<span id="cb50-11"><a href="nlp.html#cb50-11" aria-hidden="true"/>$ <span class="ex">pipenv</span> run src/train-cli.py --dataset=datasets/causal_inference.txt</span>
<span id="cb50-12"><a href="nlp.html#cb50-12" aria-hidden="true"/>  <span class="ex">...</span></span>
<span id="cb50-13"><a href="nlp.html#cb50-13" aria-hidden="true"/>$ <span class="fu">git</span> status -s</span>
<span id="cb50-14"><a href="nlp.html#cb50-14" aria-hidden="true"/> <span class="ex">M</span> src/train.py</span>
<span id="cb50-15"><a href="nlp.html#cb50-15" aria-hidden="true"/>$ <span class="fu">git</span> add src/train-cli.py</span>
<span id="cb50-16"><a href="nlp.html#cb50-16" aria-hidden="true"/>$ <span class="fu">git</span> commit -m <span class="st">'Changed word2vec window size to 4'</span></span>
<span id="cb50-17"><a href="nlp.html#cb50-17" aria-hidden="true"/>$ <span class="fu">git</span> tag -a <span class="st">'v0.0.2'</span> -m <span class="st">'Changed word2vec window size to 4'</span></span></code></pre></div>
</div>
<div id="nlp-training-exp-tracking" class="section level3 hasAnchor" number="12.4.3">
<h3><span class="header-section-number">12.4.3</span> Training and Experiment Tracking<a href="nlp.html#nlp-training-exp-tracking" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>
The tokens we produced in Section <a href="nlp.html#nlp-ingestion-preparation">12.4.1</a> and tracked in Section <a href="nlp.html#nlp-tracking-with-dvc">12.4.2</a>
are the input for the word2vec implementation in Gensim, available from <code>models.word2vec</code>, together with the list of
n-grams provided by the domain experts (the <code>vocabulary</code> variable in the code below). word2vec returns a <code>wv</code> object
that stores each embedding (that is, a word vector) in a structure called <code>KeyedVectors</code> that maps the n-grams
(the “keys”) to vectors. The <code>KeyedVectors</code> can be used to perform operations on the vectors, such as computing their
distance or their similarity.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="nlp.html#cb51-1" aria-hidden="true"/>[...]</span>
<span id="cb51-2"><a href="nlp.html#cb51-2" aria-hidden="true"/>model <span class="op">=</span> Word2Vec(</span>
<span id="cb51-3"><a href="nlp.html#cb51-3" aria-hidden="true"/>    callbacks<span class="op">=</span>[Word2vecCallback()],</span>
<span id="cb51-4"><a href="nlp.html#cb51-4" aria-hidden="true"/>    compute_loss<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-5"><a href="nlp.html#cb51-5" aria-hidden="true"/>    vector_size<span class="op">=</span>vector_size,</span>
<span id="cb51-6"><a href="nlp.html#cb51-6" aria-hidden="true"/>    min_count<span class="op">=</span>min_count,</span>
<span id="cb51-7"><a href="nlp.html#cb51-7" aria-hidden="true"/>    window<span class="op">=</span>window,</span>
<span id="cb51-8"><a href="nlp.html#cb51-8" aria-hidden="true"/>    workers<span class="op">=</span>workers)</span>
<span id="cb51-9"><a href="nlp.html#cb51-9" aria-hidden="true"/>)</span>
<span id="cb51-10"><a href="nlp.html#cb51-10" aria-hidden="true"/></span>
<span id="cb51-11"><a href="nlp.html#cb51-11" aria-hidden="true"/>model.build_vocab(</span>
<span id="cb51-12"><a href="nlp.html#cb51-12" aria-hidden="true"/>    corpus_iterable<span class="op">=</span>vocabulary,</span>
<span id="cb51-13"><a href="nlp.html#cb51-13" aria-hidden="true"/>    progress_per<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-14"><a href="nlp.html#cb51-14" aria-hidden="true"/>    trim_rule<span class="op">=</span>_rule</span>
<span id="cb51-15"><a href="nlp.html#cb51-15" aria-hidden="true"/>)</span>
<span id="cb51-16"><a href="nlp.html#cb51-16" aria-hidden="true"/></span>
<span id="cb51-17"><a href="nlp.html#cb51-17" aria-hidden="true"/>model.train(</span>
<span id="cb51-18"><a href="nlp.html#cb51-18" aria-hidden="true"/>    sentences,</span>
<span id="cb51-19"><a href="nlp.html#cb51-19" aria-hidden="true"/>    total_examples<span class="op">=</span>model.corpus_count,</span>
<span id="cb51-20"><a href="nlp.html#cb51-20" aria-hidden="true"/>    epochs<span class="op">=</span>epochs,</span>
<span id="cb51-21"><a href="nlp.html#cb51-21" aria-hidden="true"/>    report_delay<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb51-22"><a href="nlp.html#cb51-22" aria-hidden="true"/>    compute_loss<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-23"><a href="nlp.html#cb51-23" aria-hidden="true"/>    callbacks<span class="op">=</span>[Word2vecCallback()],</span>
<span id="cb51-24"><a href="nlp.html#cb51-24" aria-hidden="true"/>)</span>
<span id="cb51-25"><a href="nlp.html#cb51-25" aria-hidden="true"/></span>
<span id="cb51-26"><a href="nlp.html#cb51-26" aria-hidden="true"/>word_vectors <span class="op">=</span> model.wv</span>
<span id="cb51-27"><a href="nlp.html#cb51-27" aria-hidden="true"/>[...]</span></code></pre></div>
<p/>
<p>We obtain the tokens by calling the <code>get_url()</code> method of the DVC Python API <span class="citation">(Iterative <a href="#ref-dvc-api" role="doc-biblioref">2022</a><a href="#ref-dvc-api" role="doc-biblioref">c</a>)</span>, which returns the URL of the
storage location of <code>corpus_path</code> for a specific revision defined in <code>revision</code> of the dataset present in the <code>path</code>.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="nlp.html#cb52-1" aria-hidden="true"/><span class="im">import</span> dvc.api</span>
<span id="cb52-2"><a href="nlp.html#cb52-2" aria-hidden="true"/>...</span>
<span id="cb52-3"><a href="nlp.html#cb52-3" aria-hidden="true"/>corpus_path <span class="op">=</span> dvc.api.get_url(</span>
<span id="cb52-4"><a href="nlp.html#cb52-4" aria-hidden="true"/>    path<span class="op">=</span>corpus_path,</span>
<span id="cb52-5"><a href="nlp.html#cb52-5" aria-hidden="true"/>    repo<span class="op">=</span>repo_path,</span>
<span id="cb52-6"><a href="nlp.html#cb52-6" aria-hidden="true"/>    rev<span class="op">=</span>revision,</span>
<span id="cb52-7"><a href="nlp.html#cb52-7" aria-hidden="true"/>    remote<span class="op">=</span>remote</span>
<span id="cb52-8"><a href="nlp.html#cb52-8" aria-hidden="true"/>)</span>
<span id="cb52-9"><a href="nlp.html#cb52-9" aria-hidden="true"/>...</span></code></pre></div>
<div style="page-break-after: always;"/>
<p>The corpus is sequentially read, tokenised and fed directly to the <code>train()</code> method of word2vec. We set the arguments
of the <code>train()</code> method <span class="citation">(Řehůřek and Sojka <a href="#ref-word2vec-api" role="doc-biblioref">2022</a><a href="#ref-word2vec-api" role="doc-biblioref">b</a>)</span> using environment variables, as suggested in Section <a href="design-code.html#data-as-code">5.1</a>, to
facilitate multiple experimentations with different combinations of:</p>
<ul>
<li><em>vector_size</em>: the number of dimensions of the word vectors (default: 100);</li>
<li><em>window</em>: the maximum distance between the current and predicted word within a sentence (default: 5).</li>
<li><em>min_count</em>: the minimum frequency for a word to be considered (default: 5).</li>
<li><em>workers</em>: the number of worker threads to train the model (default: 3).
</li>
</ul>
<p>As for experiment tracking, we implement it using the following MLflow tracking APIs:</p>
<ul>
<li><code>log_param()</code>: for tracking the word2vec parameters and the metadata associated with the input tokens, in particular
the arXiv query that produced them and the DVC file path and hash they were pulled from;</li>
<li><code>log_metric()</code>: for logging the dimensions of the embeddings produced by the model;</li>
<li><code>log_artifact()</code>: for logging the name of a local file or directory, such as those containing the n-grams from
the domain experts and the word vectors of the trained model, as an artefact of the experiment.</li>
</ul>
<p>Here is a short example of how we use these methods in our code.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="nlp.html#cb53-1" aria-hidden="true"/><span class="im">from</span> mlflow <span class="im">import</span> (log_metric,log_param,log_artifacts,</span>
<span id="cb53-2"><a href="nlp.html#cb53-2" aria-hidden="true"/>    create_experiment,start_run,end_run)</span>
<span id="cb53-3"><a href="nlp.html#cb53-3" aria-hidden="true"/>[...]</span>
<span id="cb53-4"><a href="nlp.html#cb53-4" aria-hidden="true"/>experiment_id <span class="op">=</span> create_experiment(</span>
<span id="cb53-5"><a href="nlp.html#cb53-5" aria-hidden="true"/>    <span class="st">"NLU experiments on causal inference corpus"</span>,</span>
<span id="cb53-6"><a href="nlp.html#cb53-6" aria-hidden="true"/>    artifact_location<span class="op">=</span>Path.cwd().joinpath(<span class="st">"mlruns"</span>).as_uri(),</span>
<span id="cb53-7"><a href="nlp.html#cb53-7" aria-hidden="true"/>    tags<span class="op">=</span>{},</span>
<span id="cb53-8"><a href="nlp.html#cb53-8" aria-hidden="true"/>)</span>
<span id="cb53-9"><a href="nlp.html#cb53-9" aria-hidden="true"/>start_run(experiment_id<span class="op">=</span>experiment_id)</span>
<span id="cb53-10"><a href="nlp.html#cb53-10" aria-hidden="true"/>log_param(<span class="st">"query"</span>, query)</span>
<span id="cb53-11"><a href="nlp.html#cb53-11" aria-hidden="true"/>[...]</span>
<span id="cb53-12"><a href="nlp.html#cb53-12" aria-hidden="true"/>log_param(<span class="st">"window"</span>, window)</span>
<span id="cb53-13"><a href="nlp.html#cb53-13" aria-hidden="true"/>log_param(<span class="st">"stop_date"</span>, stop_data)</span>
<span id="cb53-14"><a href="nlp.html#cb53-14" aria-hidden="true"/>[...]</span>
<span id="cb53-15"><a href="nlp.html#cb53-15" aria-hidden="true"/>log_metric(<span class="st">"wv_size"</span>, model.wv.vector_size)</span>
<span id="cb53-16"><a href="nlp.html#cb53-16" aria-hidden="true"/>[...]</span>
<span id="cb53-17"><a href="nlp.html#cb53-17" aria-hidden="true"/>log_artifact(<span class="st">"corpus.txt"</span>)</span>
<span id="cb53-18"><a href="nlp.html#cb53-18" aria-hidden="true"/>log_artifact(<span class="st">"keywords.txt"</span>)</span>
<span id="cb53-19"><a href="nlp.html#cb53-19" aria-hidden="true"/>log_artifact(<span class="st">"vectors.kv"</span>)</span>
<span id="cb53-20"><a href="nlp.html#cb53-20" aria-hidden="true"/>end_run()</span>
<span id="cb53-21"><a href="nlp.html#cb53-21" aria-hidden="true"/>[...]</span></code></pre></div>
<p>
We save the model in MLflow using its <code>python_function</code> interface, which supports custom models implemented as generic
Python functions. Specifically, we serialise the learned word vectors contained in <code>model.wv</code> with the Gensim function
<code>save()</code>, and we reload them later with the function <code>KeyedVectors.load()</code> when the serving model.
</p>
</div>
<div id="nlp-model-packaging" class="section level3 hasAnchor" number="12.4.4">
<h3><span class="header-section-number">12.4.4</span> Model Packaging<a href="nlp.html#nlp-model-packaging" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>
BentoML <span class="citation">(BentoML <a href="#ref-bentoml" role="doc-biblioref">2022</a>)</span>, which we introduced in Section <a href="production-tools.html#production-software">11.2</a>, can import a serialised Python model or
an MLflow model, and it can bind its API to a RESTful endpoint with a minimal use of glue code. Therefore, it is a
convenient choice to package and serve the word2vec model. In our case, the classification API that computes the degree
of similarity between the PDF document submitted by the user and those used to train the word2vec model (that is, what
we call the “room” in Section <a href="nlp.html#nlp-domain">12.1</a>) is exposed as a <code>/classify</code> endpoint.</p>
<p>The code snippet below shows the declaration of the service with the API and decorator provided by BentoML. Once the
service is running, the API will be available at <code>/classify</code>: it will accept a PDF file as input and return a scalar
between 0 and 1. As a future enhancement, we could build an additional <code>/rank</code> API endpoint that accepts a JSON-formatted
list of PDF URLs as input, runs calls <code>/classify</code> API for each of them and returns a sorted list of documents with the
associated ranking and similarities.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="nlp.html#cb54-1" aria-hidden="true"/><span class="im">from</span> __future__ <span class="im">import</span> annotations</span>
<span id="cb54-2"><a href="nlp.html#cb54-2" aria-hidden="true"/></span>
<span id="cb54-3"><a href="nlp.html#cb54-3" aria-hidden="true"/><span class="im">import</span> io</span>
<span id="cb54-4"><a href="nlp.html#cb54-4" aria-hidden="true"/><span class="im">from</span> typing <span class="im">import</span> Any</span>
<span id="cb54-5"><a href="nlp.html#cb54-5" aria-hidden="true"/><span class="im">import</span> typing</span>
<span id="cb54-6"><a href="nlp.html#cb54-6" aria-hidden="true"/></span>
<span id="cb54-7"><a href="nlp.html#cb54-7" aria-hidden="true"/><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb54-8"><a href="nlp.html#cb54-8" aria-hidden="true"/></span>
<span id="cb54-9"><a href="nlp.html#cb54-9" aria-hidden="true"/><span class="im">import</span> bentoml</span>
<span id="cb54-10"><a href="nlp.html#cb54-10" aria-hidden="true"/><span class="im">from</span> bentoml.io <span class="im">import</span> File</span>
<span id="cb54-11"><a href="nlp.html#cb54-11" aria-hidden="true"/><span class="im">from</span> bentoml.io <span class="im">import</span> JSON</span>
<span id="cb54-12"><a href="nlp.html#cb54-12" aria-hidden="true"/></span>
<span id="cb54-13"><a href="nlp.html#cb54-13" aria-hidden="true"/>nlu_runner <span class="op">=</span> bentoml.picklable_model.get(<span class="st">"nlu_exp:v0.0.2).to_runner()</span></span>
<span id="cb54-14"><a href="nlp.html#cb54-14" aria-hidden="true"/><span class="st">svc = bentoml.Service("</span>pdf_classifier<span class="st">", runners=[nlu_runner])</span></span>
<span id="cb54-15"><a href="nlp.html#cb54-15" aria-hidden="true"/></span>
<span id="cb54-16"><a href="nlp.html#cb54-16" aria-hidden="true"/><span class="st">@svc.api(input=File(), output=JSON())</span></span>
<span id="cb54-17"><a href="nlp.html#cb54-17" aria-hidden="true"/><span class="st">def classify(input_pdf: io.BytesIO[Any]) -&gt; typing.List[float]:</span></span>
<span id="cb54-18"><a href="nlp.html#cb54-18" aria-hidden="true"/><span class="st">    return nlu_runner.classify.run(input_pdf)</span></span></code></pre></div>
<p/>
</div>
<div id="nlp-deployment-inference" class="section level3 hasAnchor" number="12.4.5">
<h3><span class="header-section-number">12.4.5</span> Deployment and Inference<a href="nlp.html#nlp-deployment-inference" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>

One advantage of using containers to deploy and serve models is that they can be deployed locally using Docker or in a
target (possibly remote) environment using Kubernetes (Section <a href="deploying-code.html#container-packaging">7.1.4</a>). This is an important point
in our use case: as discussed in Section <a href="nlp.html#nlp-infra">12.3</a>, our pipeline runs on a combination of local and remote
systems. Therefore, we use the <code>bentoml containerize</code> command to build a container image with all the requirements
needed to run the inference API we defined in Section <a href="nlp.html#nlp-model-packaging">12.4.4</a>: the output is a Docker container with
a stateless RESTful API server implemented in Python. The commands for building the container are shown below.
</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb55-1"><a href="nlp.html#cb55-1" aria-hidden="true"/>$ <span class="ex">bentoml</span> containerise nlu_exp:v0.0.2</span>
<span id="cb55-2"><a href="nlp.html#cb55-2" aria-hidden="true"/>$ <span class="ex">docker</span> run -d --rm -p 3000:300 nlu_exp:v0.0.2</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:openapi"/>
<img src="../Images/a327ff14abdfde1b2e10ed90daf493a2.png" alt="The OpenAPI specification generated by BentoML." width="80%" data-original-src="https://ppml.dev/chapter12/figures/openapi.svg"/>
<p class="caption">
Figure 12.3: The OpenAPI specification generated by BentoML.
</p>
</div>
<p>After starting the container, the API server is reachable at <code>http://127.0.0.1:3000</code>. The URL
<code>http://127.0.0.1:3000/classify</code> serves the API from Section <a href="nlp.html#nlp-model-packaging">12.4.4</a> and <code>http://127.0.0.1:3000/</code>
displays a web page with the dynamically-generated OpenAPI documentation <span class="citation">(SmartBear Software <a href="#ref-swagger" role="doc-biblioref">2021</a>)</span> (Figure <a href="nlp.html#fig:openapi">12.3</a>). We also
make available additional liveness and readiness APIs to support deployment on Kubernetes, as well as a <code>/metrics</code>
endpoint that returns the service metrics in Prometheus format <span class="citation">(Prometheus Authors and The Linux Foundation <a href="#ref-prometheus" role="doc-biblioref">2022</a>)</span>.

</p>
<p>The RESTful interface is designed to be used programmatically: we can access it using tools like <code>curl</code> or API testing
tools like Postman <span class="citation">(Velero Authors <a href="#ref-postman" role="doc-biblioref">2022</a><a href="#ref-postman" role="doc-biblioref">a</a>)</span>. We can also query it in our continuous integration setup to run integration tests and
verify that the build process successfully created the container image. However, the RESTful interface can also serve as
a backend to build web applications that consume the API outputs and display them through dashboards (using the tools we
discussed in Section <a href="production-tools.html#production-dashboard">11.3</a>) or simple web interfaces (using libraries such as React <span class="citation">(Meta Platforms <a href="#ref-react" role="doc-biblioref">2022</a><a href="#ref-react" role="doc-biblioref">b</a>)</span> or
frameworks such as Vue.js <span class="citation">(You <a href="#ref-vue" role="doc-biblioref">2022</a>)</span>; or libraries for UI development in Python such as Gradio <span class="citation">(Abid et al. <a href="#ref-gradio" role="doc-biblioref">2022</a>)</span> and Streamlit
<span class="citation">(Streamlit <a href="#ref-streamlit" role="doc-biblioref">2022</a>)</span>). They are useful to domain experts to inspect the inference outputs and validate them and the model that
generates them as humans-in-the loop (Sections <a href="design-code.html#model-pipeline">5.3.4</a> and <a href="design-code.html#monitoring-pipeline">5.3.6</a>). In particular,
they make it possible for domain experts to iteratively refine the list of key terms we use as the vocabulary of
word2vec as envisaged by Lipizzi et al. <span class="citation">(Lipizzi et al. <a href="#ref-recommendations" role="doc-biblioref">2022</a>)</span>.

</p>
<p>
To validate the <code>/classify</code> API, we can upload (POST) the PDF of a scientific article on causal inference with the
command-line tool <code>curl</code>,</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb56-1"><a href="nlp.html#cb56-1" aria-hidden="true"/>$ <span class="ex">curl</span> -H <span class="st">"Content-Type: multipart/form-data"</span> <span class="kw">\</span></span>
<span id="cb56-2"><a href="nlp.html#cb56-2" aria-hidden="true"/>    <span class="ex">-F</span> <span class="st">'fileobj=@good-article.pdf;type=application/octet-stream'</span> <span class="kw">\</span></span>
<span id="cb56-3"><a href="nlp.html#cb56-3" aria-hidden="true"/>    <span class="ex">http</span>://remote:3000/classify</span>
<span id="cb56-4"><a href="nlp.html#cb56-4" aria-hidden="true"/><span class="dt">{"value":0.8203434225167339}</span><span class="ex">%</span></span></code></pre></div>
<p>and another on a completely different topic.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb57-1"><a href="nlp.html#cb57-1" aria-hidden="true"/>$ <span class="ex">curl</span> -H <span class="st">"Content-Type: multipart/form-data"</span> <span class="kw">\</span></span>
<span id="cb57-2"><a href="nlp.html#cb57-2" aria-hidden="true"/>    <span class="ex">-F</span> <span class="st">'fileobj=@bad-article.pdf;type=application/octet-stream'</span> <span class="kw">\</span></span>
<span id="cb57-3"><a href="nlp.html#cb57-3" aria-hidden="true"/>    <span class="ex">http</span>://remote:3000/classify</span>
<span id="cb57-4"><a href="nlp.html#cb57-4" aria-hidden="true"/><span class="dt">{"value":0.24675693999330117}</span><span class="ex">%</span></span></code></pre></div>
<p>As we can see from the relevance scores, the “/classify” API responds correctly for both relevant and unrelated
documents (Section <a href="troubleshooting-code.html#test-coverage">9.4.6</a>). The underlying <code>classify()</code> method computes the cosine distance between
the <code>KeyedVectors</code>, returns the degree of similarity as a float, and logs the PDF metadata and the relevance to a
remote logging database via Fluentd.
</p>
<div style="page-break-after: always;"/>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="nlp.html#cb58-1" aria-hidden="true"/>[...]</span>
<span id="cb58-2"><a href="nlp.html#cb58-2" aria-hidden="true"/><span class="co"># load serialised KeyedVectors from the `knowledge_ww_fp` path</span></span>
<span id="cb58-3"><a href="nlp.html#cb58-3" aria-hidden="true"/>knowledge_wv <span class="op">=</span> KeyedVectors.load(knowledge_ww_fp, mmap<span class="op">=</span><span class="st">"r"</span>)</span>
<span id="cb58-4"><a href="nlp.html#cb58-4" aria-hidden="true"/><span class="co"># get the KeyedVectors pairs that match the</span></span>
<span id="cb58-5"><a href="nlp.html#cb58-5" aria-hidden="true"/><span class="co"># vocabulary word (the keyword list from the expert)</span></span>
<span id="cb58-6"><a href="nlp.html#cb58-6" aria-hidden="true"/>knowledge_v <span class="op">=</span> get_word2vec_vectors(</span>
<span id="cb58-7"><a href="nlp.html#cb58-7" aria-hidden="true"/>        word_vectors<span class="op">=</span>knowledge_wv,</span>
<span id="cb58-8"><a href="nlp.html#cb58-8" aria-hidden="true"/>        vocabulary<span class="op">=</span>vocab</span>
<span id="cb58-9"><a href="nlp.html#cb58-9" aria-hidden="true"/>)</span>
<span id="cb58-10"><a href="nlp.html#cb58-10" aria-hidden="true"/><span class="co"># train on the fly a word2vec model on</span></span>
<span id="cb58-11"><a href="nlp.html#cb58-11" aria-hidden="true"/><span class="co"># the PDF converted into text</span></span>
<span id="cb58-12"><a href="nlp.html#cb58-12" aria-hidden="true"/>model <span class="op">=</span> word2vec(text, vocab)</span>
<span id="cb58-13"><a href="nlp.html#cb58-13" aria-hidden="true"/>document_v <span class="op">=</span> get_word2vec_vectors(</span>
<span id="cb58-14"><a href="nlp.html#cb58-14" aria-hidden="true"/>    word_vectors<span class="op">=</span>model.wv,</span>
<span id="cb58-15"><a href="nlp.html#cb58-15" aria-hidden="true"/>    vocabulary<span class="op">=</span>vocab</span>
<span id="cb58-16"><a href="nlp.html#cb58-16" aria-hidden="true"/>)</span>
<span id="cb58-17"><a href="nlp.html#cb58-17" aria-hidden="true"/>[...]</span>
<span id="cb58-18"><a href="nlp.html#cb58-18" aria-hidden="true"/>dist <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> distance.cosine(document_v.mean(<span class="dv">0</span>), knowledge_v.mean(<span class="dv">0</span>))</span>
<span id="cb58-19"><a href="nlp.html#cb58-19" aria-hidden="true"/>[...]</span>
<span id="cb58-20"><a href="nlp.html#cb58-20" aria-hidden="true"/>logger.warning(<span class="st">"Classify request with distance </span><span class="sc">%f</span><span class="st"> for </span><span class="sc">%s</span><span class="st">"</span>,</span>
<span id="cb58-21"><a href="nlp.html#cb58-21" aria-hidden="true"/>    dist, metadata)</span>
<span id="cb58-22"><a href="nlp.html#cb58-22" aria-hidden="true"/>[...]</span>
<span id="cb58-23"><a href="nlp.html#cb58-23" aria-hidden="true"/><span class="cf">return</span> dist</span></code></pre></div>
<p>


The Docker image that serves the APIs can be automatically rebuilt using tools like Jenkins <span class="citation">(Jenkins <a href="#ref-jenkins" role="doc-biblioref">2022</a><a href="#ref-jenkins" role="doc-biblioref">b</a>)</span>, GitLab CI or
GitHub Actions each time we release a new model. We can deploy it to a container service or to an orchestrator by
applying one of the techniques discussed in Section <a href="deploying-code.html#deployment-strategies">7.2</a>. Thanks to its stateless composition,
the container can scale horizontally if necessary (we just deploy more instances of it) so we can handle increasing
loads over time.</p>
<p/>
<!-- vim: set synmaxcol=600 textwidth=120 colorcolumn=120 spell wrap number: -->

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references hanging-indent">
<div id="ref-gradio">
<p>Abid, A., A. Abdalla, A. Ali, D. Khan, A. Alfozan, and J. Zou. 2022. <em>Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild</em>. <a href="https://www.gradio.app/docs/">https://www.gradio.app/docs/</a>.</p>
</div>
<div id="ref-celery-executor">
<p>Apache Software Foundation. 2022a. <em>Celery Executor</em>. <a href="https://airflow.apache.org/docs/apache-airflow/stable/executor/celery.html">https://airflow.apache.org/docs/apache-airflow/stable/executor/celery.html</a>.</p>
</div>
<div id="ref-arxiv-api">
<p>ArXiv. 2022. <em>arXiv API Access</em>. <a href="https://arxiv.org/help/api">https://arxiv.org/help/api</a>.</p>
</div>
<div id="ref-bentoml">
<p>BentoML. 2022. <em>Unified Model Serving Framework</em>. <a href="https://docs.bentoml.org/en/latest/">https://docs.bentoml.org/en/latest/</a>.</p>
</div>
<div id="ref-docker">
<p>Docker. 2022a. <em>Docker</em>. <a href="https://www.docker.com/">https://www.docker.com/</a>.</p>
</div>
<div id="ref-word2vec-ssize">
<p>Dusserre, E., and M. Padró. 2017. “Bigger Does Not Mean Better! We Prefer Specificity.” In <em>Proceedings of the 12th International Conference on Computational Semantics</em>, 1–6.</p>
</div>
<div id="ref-elastic">
<p>Elasticsearch. 2022. <em>Free and Open Search: The Creators of Elasticsearch, ELK &amp; Kibana</em>. <a href="https://www.elastic.co/">https://www.elastic.co/</a>.</p>
</div>
<div id="ref-spacy">
<p>Explosion. 2021. <em>Spacy: Industrial-Strength Natural Language Processing</em>. <a href="https://spacy.io/">https://spacy.io/</a>.</p>
</div>
<div id="ref-pypdf2">
<p>Fenniak, M. 2022. <em>PyPDF2 Documentation</em>. <a href="https://pypdf2.readthedocs.io/en/latest/">https://pypdf2.readthedocs.io/en/latest/</a>.</p>
</div>
<div id="ref-grafana">
<p>GrafanaLabs. 2022. <em>Grafana: The Open Observability Platform</em>. <a href="https://grafana.com/">https://grafana.com/</a>.</p>
</div>
<div id="ref-dvc">
<p>Iterative. 2022b. <em>DVC: Data Version Control. Git for Data &amp; Models</em>. <a href="https://github.com/iterative/dvc">https://github.com/iterative/dvc</a>.</p>
</div>
<div id="ref-dvc-api">
<p>Iterative. 2022c. <em>DVC Python API</em>. <a href="https://dvc.org/doc/api-reference">https://dvc.org/doc/api-reference</a>.</p>
</div>
<div id="ref-jenkins">
<p>Jenkins. 2022b. <em>Jenkins User Documentation</em>. <a href="https://www.jenkins.io/doc/">https://www.jenkins.io/doc/</a>.</p>
</div>
<div id="ref-kaji">
<p>Kaji, N., and H. Kobayashi. 2017. “Incremental Skip-gram Model with Negative Sampling.” In <em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>, 363–71.</p>
</div>
<div id="ref-recommendations">
<p>Lipizzi, C., H. Behrooz, M. Dressman, A. G. Vishwakumar, and K. Batra. 2022. “Acquisition Research: Creating Synergy for Informed Change.” In <em>Proceedings of the 19th Annual Acquisition Research Symposium</em>, 242–55.</p>
</div>
<div id="ref-room-theory">
<p>Lipizzi, C., D. Borrelli, and F. de Oliveira Capela. 2021. <em>A Computational Model Implementing Subjectivity with the “Room Theory”: The case of Detecting Emotion from Text</em>. <a href="https://arxiv.org/abs/2005.06059">https://arxiv.org/abs/2005.06059</a>.</p>
</div>
<div id="ref-react">
<p>Meta Platforms. 2022b. <em>React: A JavaScript Library for Building User Interfaces</em>. <a href="https://reactjs.org/">https://reactjs.org/</a>.</p>
</div>
<div id="ref-minio">
<p>MinIO. 2022. <em>MinIO Documentation</em>. <a href="https://docs.min.io/docs">https://docs.min.io/docs</a>.</p>
</div>
<div id="ref-nltk">
<p>NLTK Team. 2021. <em>NLTK: A Natural Language Toolkit</em>. <a href="https://www.nltk.org/">https://www.nltk.org/</a>.</p>
</div>
<div id="ref-prometheus">
<p>Prometheus Authors, and The Linux Foundation. 2022. <em>Prometheus: Monitoring System and Time Series Databases</em>. <a href="https://prometheus.io/">https://prometheus.io/</a>.</p>
</div>
<div id="ref-gensim">
<p>Řehůřek, R., and P. Sojka. 2022a. <em>Gensim Documentation</em>. <a href="https://radimrehurek.com/gensim/auto_examples/index.html">https://radimrehurek.com/gensim/auto_examples/index.html</a>.</p>
</div>
<div id="ref-word2vec-api">
<p>Řehůřek, R., and P. Sojka. 2022a. <em>Gensim Documentation</em>. <a href="https://radimrehurek.com/gensim/auto_examples/index.html">https://radimrehurek.com/gensim/auto_examples/index.html</a>.</p> 2022b. <em>Gensim Documentation</em>. <a href="https://radimrehurek.com/gensim/models/word2vec.html">https://radimrehurek.com/gensim/models/word2vec.html</a>.
</div>
<div id="ref-word2vec">
<p>Rong, X. 2014. “Word2vec Parameter Learning Explained.” <em>arXiv Preprint arXiv:1411.2738</em>.</p>
</div>
<div id="ref-first-page">
<p>Shelton, K. 2017. <em>The Value of Search Results Rankings</em>. <a href="https://www.forbes.com/sites/forbesagencycouncil/2017/10/30/the-value-of-search-results-rankings/">https://www.forbes.com/sites/forbesagencycouncil/2017/10/30/the-value-of-search-results-rankings/</a>.</p>
</div>
<div id="ref-pdfminer.six">
<p>Shinyama, Y., P. Guglielmetti, and P. Marsman. 2022. <em>Pdfminer.six’s Documentation</em>. <a href="https://pdfminersix.readthedocs.io/en/latest/">https://pdfminersix.readthedocs.io/en/latest/</a>.</p>
</div>
<div id="ref-swagger">
<p>SmartBear Software. 2021. <em>OpenAPI Specification</em>. <a href="https://swagger.io/specification/">https://swagger.io/specification/</a>.</p>
</div>
<div id="ref-streamlit">
<p>Streamlit. 2022. <em>Streamlit Documentation</em>. <a href="https://docs.streamlit.io/">https://docs.streamlit.io/</a>.</p>
</div>
<div id="ref-airflow">
<p>The Apache Software Foundation. 2022a. <em>Airflow Documentation</em>. <a href="https://airflow.apache.org/docs/">https://airflow.apache.org/docs/</a>.</p>
</div>
<div id="ref-spark">
<p>The Apache Software Foundation. 2022f. <em>Apache Spark Documentation</em>. <a href="https://spark.apache.org/docs/latest">https://spark.apache.org/docs/latest</a>.</p>
</div>
<div id="ref-fluentd">
<p>The Fluentd Project. 2022. <em>Fluentd: Open Source Data Collector</em>. <a href="https://www.fluentd.org/">https://www.fluentd.org/</a>.</p>
</div>
<div id="ref-git-git">
<p>The Git Development Team. 2022. <em>Git Source Code Mirror</em>. <a href="https://github.com/git/git">https://github.com/git/git</a>.</p>
</div>
<div id="ref-kubernetes">
<p>The Kubernetes Authors. 2022a. <em>Kubernetes</em>. <a href="https://kubernetes.io/">https://kubernetes.io/</a>.</p>
</div>
<div id="ref-postman">
<p>Velero Authors. 2022a. <em>Postman Documentation</em>. <a href="https://learning.postman.com/docs">https://learning.postman.com/docs</a>.</p>
</div>
<div id="ref-velero">
<p>Velero Authors. 2022b. <em>Velero Documentation</em>. <a href="https://velero.io/docs">https://velero.io/docs</a>.</p>
</div>
<div id="ref-vue">
<p>You, E. 2022. <em>Vue.js: The Progressive JavaScript Framework</em>. <a href="https://vuejs.org/">https://vuejs.org/</a>.</p>
</div>
</div>
<div class="footnotes">
<hr/>
<ol start="26">
<li id="fn26"><p>A sequence of characters grouped to provide a semantic unit for NLP processing.<a href="nlp.html#fnref26" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
                
</body>
</html>