["```py\nssh mjrovai@raspi-5.local\n```", "```py\nhostname -I\n```", "```py\nsudo apt update\nsudo apt upgrade -y\n```", "```py\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\n```", "```py\nsudo apt-get install libjpeg-dev libopenblas-dev libopenmpi-dev \\\n    libomp-dev\n```", "```py\npython3 -m venv ~/florence\nsource ~/florence/bin/activate\n```", "```py\npip3 install setuptools numpy Cython\npip3 install requests\npip3 install torch torchvision \\\n    --index-url https://download.pytorch.org/whl/cpu\npip3 install torchaudio \\\n    --index-url https://download.pytorch.org/whl/cpu\n```", "```py\npip3 install transformers\npip3 install timm einops\n```", "```py\npip3 install autodistill-florence-2\n```", "```py\npip3 install jupyter\npip3 install numpy Pillow matplotlib\njupyter notebook --generate-config\n```", "```py\njupyter notebook --ip=192.168.4.209 --no-browser\n```", "```py\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = (\n    torch.float16 if torch.cuda.is_available() else torch.float32\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Florence-2-base\",\n    torch_dtype=torch_dtype,\n    trust_remote_code=True,\n).to(device)\nprocessor = AutoProcessor.from_pretrained(\n    \"microsoft/Florence-2-base\", trust_remote_code=True\n)\n\nprompt = \"<OD>\"\n\nurl = (\n    \"https://huggingface.co/datasets/huggingface/\"\n    \"documentation-images/resolve/main/transformers/\"\n    \"tasks/car.jpg?download=true\"\n)\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\n    device, torch_dtype\n)\n\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\ngenerated_text = processor.batch_decode(\n    generated_ids, skip_special_tokens=False\n)[0]\n\nparsed_answer = processor.post_process_generation(\n    generated_text,\n    task=\"<OD>\",\n    image_size=(image.width, image.height),\n)\n\nprint(parsed_answer)\n```", "```py\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n```", "```py\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\ntorch_dtype = (\n    torch.float16 if torch.cuda.is_available() else torch.float32\n)\n```", "```py\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Florence-2-base\",\n    torch_dtype=torch_dtype,\n    trust_remote_code=True,\n).to(device)\n\nprocessor = AutoProcessor.from_pretrained(\n    \"microsoft/Florence-2-base\", trust_remote_code=True\n)\n```", "```py\nprompt = \"<OD>\"\n```", "```py\nurl = \"https://huggingface.co/datasets/huggingface/\"\n      \"documentation-images/resolve/main/transformers/\"\n      \"tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n```", "```py\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\n    device, torch_dtype\n)\n```", "```py\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\n```", "```py\ngenerated_text = processor.batch_decode(\n    generated_ids, skip_special_tokens=False\n)[0]\n```", "```py\nparsed_answer = processor.post_process_generation(\n    generated_text,\n    task=\"<OD>\",\n    image_size=(image.width, image.height),\n)\n```", "```py\nprint(parsed_answer)\n```", "```py\n[{'<OD>': {\n   'bboxes': [\n     [34.23999786376953, 160.0800018310547, 597.4400024414062],\n     [371.7599792480469, 272.32000732421875, 241.67999267578125],\n     [303.67999267578125, 247.4399871826172, 454.0799865722656],\n     [276.7200012207031, 553.9199829101562, 370.79998779296875],\n     [96.31999969482422, 280.55999755859375, 198.0800018310547],\n     [371.2799987792969]\n    ],\n    'labels': ['car', 'door handle', 'wheel', 'wheel']\n}}]\n```", "```py\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 8))\nplt.imshow(image)\nplt.axis(\"off\")\nplt.show()\n```", "```py\n'labels': ['car', 'door handle', 'wheel', 'wheel']\n```", "```py\ndef plot_bbox(image, data):\n    # Create a figure and axes\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(image)\n\n    # Plot each bounding box\n    for bbox, label in zip(data[\"bboxes\"], data[\"labels\"]):\n        # Unpack the bounding box coordinates\n        x1, y1, x2, y2 = bbox\n        # Create a Rectangle patch\n        rect = patches.Rectangle(\n            (x1, y1),\n            x2 - x1,\n            y2 - y1,\n            linewidth=1,\n            edgecolor=\"r\",\n            facecolor=\"none\",\n        )\n        # Add the rectangle to the Axes\n        ax.add_patch(rect)\n        # Annotate the label\n        plt.text(\n            x1,\n            y1,\n            label,\n            color=\"white\",\n            fontsize=8,\n            bbox=dict(facecolor=\"red\", alpha=0.5),\n        )\n\n    # Remove the axis ticks and labels\n    ax.axis(\"off\")\n\n    # Show the plot\n    plt.show()\n```", "```py\nplot_bbox(image, parsed_answer['<OD>'])\n```", "```py\ndogs_cats = Image.open(\"./images/dogs-cats.jpg\")\ntable = Image.open(\"./images/table.jpg\")\n```", "```py\ndef run_example(task_prompt, text_input=None, image=None):\n    start_time = time.perf_counter()  # Start timing\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(\n        text=prompt, images=image, return_tensors=\"pt\"\n    ).to(device)\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        early_stopping=False,\n        do_sample=False,\n        num_beams=3,\n    )\n    generated_text = processor.batch_decode(\n        generated_ids, skip_special_tokens=False\n    )[0]\n    parsed_answer = processor.post_process_generation(\n        generated_text,\n        task=task_prompt,\n        image_size=(image.width, image.height),\n    )\n\n    end_time = time.perf_counter()  # End timing\n    elapsed_time = end_time - start_time  # Calculate elapsed time\n    print(\n        f\" \\n[INFO] ==> Florence-2-base ({task_prompt}), \\\n took {elapsed_time:.1f} seconds to execute.\\n\"\n    )\n\n    return parsed_answer\n```", "```py\nrun_example(task_prompt=\"<CAPTION>\", image=dogs_cats)\n```", "```py\n[INFO] ==> Florence-2-base (<CAPTION>), \\\ntook 16.1 seconds to execute.\n\n{'<CAPTION>': 'A group of dogs and cats sitting in a garden.'}\n```", "```py\nrun_example(task_prompt=\"<CAPTION>\", image=table)\n```", "```py\n[INFO] ==> Florence-2-base (<CAPTION>), \\\ntook 16.5 seconds to execute.\n\n{'<CAPTION>': 'A wooden table topped with a plate of fruit \\\nand a glass of wine.'}\n```", "```py\nrun_example(task_prompt=\"<DETAILED_CAPTION>\", image=dogs_cats)\n```", "```py\n[INFO] ==> Florence-2-base (<DETAILED_CAPTION>), \\\ntook 25.5 seconds to execute.\n\n{'<DETAILED_CAPTION>': 'The image shows a group of cats and \\\ndogs sitting on top of a lush green field, surrounded by plants \\\nwith flowers, trees, and a house in the background. The sky is \\\nvisible above them, creating a peaceful atmosphere.'}\n```", "```py\nrun_example(task_prompt=\"<DETAILED_CAPTION>\", image=table)\n```", "```py\n[INFO] ==> Florence-2-base (<DETAILED_CAPTION>), \\\ntook 26.8 seconds to execute.\n\n{'<DETAILED_CAPTION>': 'The image shows a wooden table with \\\na bottle of wine and a glass of wine on it, surrounded by \\\na variety of fruits such as apples, oranges, and grapes. \\\nIn the background, there are chairs, plants, trees, and \\\na house, all slightly blurred.'}\n```", "```py\nrun_example(task_prompt=\"<MORE_DETAILED_CAPTION>\", image=dogs_cats)\n```", "```py\n[INFO] ==> Florence-2-base (<MORE_DETAILED_CAPTION>), \\\ntook 49.8 seconds to execute.\n\n{'<MORE_DETAILED_CAPTION>': 'The image shows a group of four \\\ncats and a dog in a garden. The garden is filled with colorful \\\nflowers and plants, and there is a pathway leading up to \\\na house in the background. The main focus of the image is \\\na large German Shepherd dog standing on the left side of \\\nthe garden, with its tongue hanging out and its mouth open, \\\nas if it is panting. On the right side, there are \\\ntwo smaller cats, one orange and one gray, sitting on the \\\ngrass. In the background, there is another golden retriever \\\ndog sitting and looking at the camera. The sky is blue and \\\nthe sun is shining, creating a warm and inviting atmosphere.'}\n```", "```py\nrun_example(task_prompt=\"< MORE_DETAILED_CAPTION>\", image=table)\n```", "```py\nINFO] ==> Florence-2-base (<MORE_DETAILED_CAPTION>), \\\ntook 32.4 seconds to execute.\n\n{'<MORE_DETAILED_CAPTION>': 'The image shows a wooden table \\\nwith a wooden tray on it. On the tray, there are various \\\nfruits such as grapes, oranges, apples, and grapes. There \\\nis also a bottle of red wine on the table. The background \\\nshows a garden with trees and a house. The overall mood \\\nof the image is peaceful and serene.'}\n```", "```py\ntask_prompt = \"<OD>\"\nresults = run_example(task_prompt, image=dogs_cats)\nprint(results)\n```", "```py\n[INFO] ==> Florence-2-base (<OD>), took 20.9 seconds to execute.\n\n{'<OD>': {'bboxes': [\n  [737.79, 571.90, 1022.46, 980.48],\n  [0.51, 593.40, 211.45, 991.74],\n  [445.95, 721.40, 680.44, 850.43],\n  [39.42, 91.64, 491.00, 933.37],\n  [570.88, 184.83, 974.33, 782.84]\n  ],\n  'labels': ['cat', 'cat', 'cat', 'dog', 'dog']\n}}\n```", "```py\nplot_bbox(dogs_cats, results[\"<OD>\"])\n```", "```py\ntask_prompt = \"<OD>\"\nresults = run_example(task_prompt, image=table)\nplot_bbox(table, results[\"<OD>\"])\n```", "```py\n[INFO] ==> Florence-2-base (<OD>), took 40.8 seconds to execute.\n```", "```py\ntask_prompt = \"<DENSE_REGION_CAPTION>\"\n\nresults = run_example(task_prompt, image=dogs_cats)\nplot_bbox(dogs_cats, results[\"<DENSE_REGION_CAPTION>\"])\n\nresults = run_example(task_prompt, image=table)\nplot_bbox(table, results[\"<DENSE_REGION_CAPTION>\"])\n```", "```py\ntask_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n\nresults = run_example(\n    task_prompt, text_input=\"a wine bottle\", image=table\n)\nplot_bbox(table, results[\"<CAPTION_TO_PHRASE_GROUNDING>\"])\n\nresults = run_example(\n    task_prompt, text_input=\"a wine glass\", image=table\n)\nplot_bbox(table, results[\"<CAPTION_TO_PHRASE_GROUNDING>\"])\n\nresults = run_example(\n    task_prompt, text_input=\"a half orange\", image=table\n)\nplot_bbox(table, results[\"<CAPTION_TO_PHRASE_GROUNDING>\"])\n```", "```py\n[INFO] ==> Florence-2-base (<CAPTION_TO_PHRASE_GROUNDING>), \\\ntook 15.7 seconds to execute\neach task.\n```", "```py\ntask_prompt = \"<CAPTION>\"\nresults = run_example(task_prompt, image=dogs_cats)\ntext_input = results[task_prompt]\ntask_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\nresults = run_example(task_prompt, text_input, image=dogs_cats)\nplot_bbox(dogs_cats, results[\"<CAPTION_TO_PHRASE_GROUNDING>\"])\n```", "```py\ntask_prompt = \"<OPEN_VOCABULARY_DETECTION>\"\n\ntext = [\n    \"a house\",\n    \"a tree\",\n    \"a standing cat at the left\",\n    \"a sleeping cat on the ground\",\n    \"a standing cat at the right\",\n    \"a yellow cat\",\n]\n\nfor txt in text:\n    results = run_example(\n        task_prompt, text_input=txt, image=dogs_cats\n    )\n\n    bbox_results = convert_to_od_format(\n        results[\"<OPEN_VOCABULARY_DETECTION>\"]\n    )\n\n    plot_bbox(dogs_cats, bbox_results)\n```", "```py\n[INFO] ==> Florence-2-base (<OPEN_VOCABULARY_DETECTION>), \\\ntook 15.1 seconds to execute\neach task.\n```", "```py\nfrom PIL import Image, ImageDraw, ImageFont\nimport copy\nimport random\nimport numpy as np\n\ncolormap = [\n    \"blue\",\n    \"orange\",\n    \"green\",\n    \"purple\",\n    \"brown\",\n    \"pink\",\n    \"gray\",\n    \"olive\",\n    \"cyan\",\n    \"red\",\n    \"lime\",\n    \"indigo\",\n    \"violet\",\n    \"aqua\",\n    \"magenta\",\n    \"coral\",\n    \"gold\",\n    \"tan\",\n    \"skyblue\",\n]\n\n\ndef draw_polygons(image, prediction, fill_mask=False):\n    \"\"\"\n Draws segmentation masks with polygons on an image.\n\n Parameters:\n - image_path: Path to the image file.\n - prediction: Dictionary containing 'polygons' and 'labels'\n keys. 'polygons' is a list of lists, each\n containing vertices of a polygon. 'labels' is\n a list of labels corresponding to each polygon.\n - fill_mask: Boolean indicating whether to fill the polygons\n with color.\n \"\"\"\n    # Load the image\n\n    draw = ImageDraw.Draw(image)\n\n    # Set up scale factor if needed (use 1 if not scaling)\n    scale = 1\n\n    # Iterate over polygons and labels\n    for polygons, label in zip(\n        prediction[\"polygons\"], prediction[\"labels\"]\n    ):\n        color = random.choice(colormap)\n        fill_color = random.choice(colormap) if fill_mask else None\n\n        for _polygon in polygons:\n            _polygon = np.array(_polygon).reshape(-1, 2)\n            if len(_polygon) < 3:\n                print(\"Invalid polygon:\", _polygon)\n                continue\n\n            _polygon = (_polygon * scale).reshape(-1).tolist()\n\n            # Draw the polygon\n            if fill_mask:\n                draw.polygon(_polygon, outline=color, fill=fill_color)\n            else:\n                draw.polygon(_polygon, outline=color)\n\n            # Draw the label text\n            draw.text(\n                (_polygon[0] + 8, _polygon[1] + 2), label, fill=color\n            )\n\n    # Save or display the image\n    # image.show()  # Display the image\n    display(image)\n```", "```py\ntask_prompt = \"<REFERRING_EXPRESSION_SEGMENTATION>\"\n\nresults = run_example(\n    task_prompt, text_input=\"a wine bottle\", image=table\n)\noutput_image = copy.deepcopy(table)\ndraw_polygons(\n    output_image,\n    results[\"<REFERRING_EXPRESSION_SEGMENTATION>\"],\n    fill_mask=True,\n)\n\nresults = run_example(\n    task_prompt, text_input=\"a german sheppard\", image=dogs_cats\n)\noutput_image = copy.deepcopy(dogs_cats)\ndraw_polygons(\n    output_image,\n    results[\"<REFERRING_EXPRESSION_SEGMENTATION>\"],\n    fill_mask=True,\n)\n```", "```py\n[INFO] ==> Florence-2-base\n(<REFERRING_EXPRESSION_SEGMENTATION>), took 207.0 seconds\nto execute each task.\n```", "```py\ntask_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\nresults = run_example(\n    task_prompt, text_input=\"a half orange\", image=table\n)\nresults\n```", "```py\n{'<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[343.552001953125,\n    689.6640625,\n    530.9440307617188,\n    873.9840698242188]],\n  'labels': ['a half']}}\n```", "```py\ntask_prompt = \"<REGION_TO_SEGMENTATION>\"\nresults = run_example(\n    task_prompt,\n    text_input=(\"<loc_343><loc_690>\" \"<loc_531><loc_874>\"),\n    image=table,\n)\noutput_image = copy.deepcopy(table)\ndraw_polygons(\n    output_image, results[\"<REGION_TO_SEGMENTATION>\"], fill_mask=True\n)\n```", "```py\ntask_prompt = \"<REGION_TO_CATEGORY>\"\nresults = run_example(\n    task_prompt,\n    text_input=(\"<loc_343><loc_690>\" \"<loc_531><loc_874>\"),\n    image=table,\n)\nresults\n```", "```py\n[INFO] ==> Florence-2-base (<REGION_TO_CATEGORY>), \\\ntook 14.3 seconds to execute.\n\n{{\n  '<REGION_TO_CATEGORY>':\n    'orange<loc_343><loc_690>'\n    '<loc_531><loc_874>'\n}\n```", "```py\ntask_prompt = \"<REGION_TO_DESCRIPTION>\"\nresults = run_example(\n    task_prompt,\n    text_input=(\"<loc_343><loc_690>\" \"<loc_531><loc_874>\"),\n    image=table,\n)\nresults\n```", "```py\n[INFO] ==> Florence-2-base (<REGION_TO_CATEGORY>), \\\ntook 14.6 seconds to execute.\n\n{\n  '<REGION_TO_CATEGORY>':\n    'orange<loc_343><loc_690>'\n    '<loc_531><loc_874>'\n}\n```", "```py\nflayer = Image.open(\"./images/embarcados.jpg\")\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(flayer)\nplt.axis(\"off\")\n# plt.title(\"Image\")\nplt.show()\n```", "```py\n[INFO] ==> Florence-2-base (<MORE_DETAILED_CAPTION>), \\\ntook 85.2 seconds to execute.\n\n{'<MORE_DETAILED_CAPTION>': 'The image is a promotional poster \\\nfor an event called \"Machine Learning Embarcados\" hosted by \\\nMarcelo Roval. The poster has a black background with white \\\ntext. On the left side of the poster, there is a logo of a \\\ncoffee cup with the text \"Café Com Embarcados\" above it. \\\nBelow the logo, it says \"25 de Setembro as 17th\" which \\\ntranslates to \"25th of September as 17\" in English. \\n\\nOn \\\nthe right side, there are two smaller text boxes with the names \\\nof the participants and their names. The first text box reads \\\n\"Democratizando a Inteligência Artificial para Paises em \\\nDesenvolvimento\" and the second text box says \"Toda \\\nquarta-feira\" which is Portuguese for \"Transmissão via in \\\nPortuguese\".\\n\\nIn the center of the image, there has a photo \\\nof Marcelo, a man with a beard and glasses, smiling at the \\\ncamera. He is wearing a white hard hat and a white shirt. \\\nThe text boxes are in orange and yellow colors.'}\n```", "```py\ntask_prompt = \"<OCR>\"\nrun_example(task_prompt, image=flayer)\n```", "```py\n[INFO] ==> Florence-2-base (<OCR>), took 37.7 seconds to execute.\n\n{'<OCR>':\n 'Machine Learning Café com Embarcado Embarcados '\n 'Democratizando a Inteligência Artificial para Paises em '\n '25 de Setembro às 17h Desenvolvimento Toda quarta-feira '\n 'Marcelo Roval Professor na UNIFIEI e Transmissão via in '\n 'Co-Director do TinyML4D'}\n```", "```py\ntask_prompt = \"<OCR_WITH_REGION>\"\nresults = run_example(task_prompt, image=flayer)\n```", "```py\ndef draw_ocr_bboxes(image, prediction):\n    scale = 1\n    draw = ImageDraw.Draw(image)\n    bboxes = prediction[\"quad_boxes\"]\n    labels = prediction[\"labels\"]\n    for box, label in zip(bboxes, labels):\n        color = random.choice(colormap)\n        new_box = (np.array(box) * scale).tolist()\n        draw.polygon(new_box, width=3, outline=color)\n        draw.text(\n            (new_box[0] + 8, new_box[1] + 2),\n            \"{}\".format(label),\n            align=\"right\",\n            fill=color,\n        )\n    display(image)\n```", "```py\noutput_image = copy.deepcopy(flayer)\ndraw_ocr_bboxes(output_image, results[\"<OCR_WITH_REGION>\"])\n```", "```py\nresults[\"<OCR_WITH_REGION>\"][\"labels\"]\n```", "```py\n'</s>Machine Learning',\n 'Café',\n 'com',\n 'Embarcado',\n 'Embarcados',\n 'Democratizando a Inteligência',\n 'Artificial para Paises em',\n '25 de Setembro ás 17h',\n 'Desenvolvimento',\n 'Toda quarta-feira',\n 'Marcelo Roval',\n 'Professor na UNIFIEI e',\n 'Transmissão via',\n 'in',\n 'Co-Director do TinyML4D']\n```"]