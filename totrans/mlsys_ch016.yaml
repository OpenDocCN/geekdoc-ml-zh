- en: Model Optimizations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型优化
- en: '*DALL·E 3 Prompt: Illustration of a neural network model represented as a busy
    construction site, with a diverse group of construction workers, both male and
    female, of various ethnicities, labeled as ‘pruning’, ‘quantization’, and ‘sparsity’.
    They are working together to make the neural network more efficient and smaller,
    while maintaining high accuracy. The ‘pruning’ worker, a Hispanic female, is cutting
    unnecessary connections from the middle of the network. The ‘quantization’ worker,
    a Caucasian male, is adjusting or tweaking the weights all over the place. The
    ‘sparsity’ worker, an African female, is removing unnecessary nodes to shrink
    the model. Construction trucks and cranes are in the background, assisting the
    workers in their tasks. The neural network is visually transforming from a complex
    and large structure to a more streamlined and smaller one.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3 提示：展示一个作为繁忙建筑工地表示的神经网络模型，有各种族、男女各种族的建设工人，被标记为‘剪枝’、‘量化’和‘稀疏性’。他们共同努力使神经网络更高效、更小，同时保持高精度。‘剪枝’工人，一位西班牙裔女性，正在从网络的中间剪除不必要的连接。‘量化’工人，一位白人男性，正在调整或微调所有地方的权重。‘稀疏性’工人，一位非洲裔女性，正在移除不必要的节点以缩小模型。背景中有建筑卡车和起重机，协助工人完成任务。神经网络在视觉上从复杂的大型结构转变为更简洁、更小的结构。*'
- en: '![](../media/file146.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file146.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*How does the mismatch between research-optimized models and production deployment
    constraints create critical engineering challenges in machine learning systems?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*研究优化模型与生产部署约束之间的不匹配如何在机器学习系统中创造关键工程挑战？*'
- en: 'Machine learning research prioritizes accuracy above all considerations, producing
    models with remarkable performance that cannot deploy where needed most: resource-constrained
    mobile devices, cost-sensitive cloud environments, or latency-critical edge applications.
    Model optimization bridges theoretical capability and practical deployment, transforming
    computationally intensive research models into efficient systems preserving performance
    while meeting stringent constraints on memory, energy, latency, and cost. Without
    systematic optimization techniques, advanced AI capabilities remain trapped in
    research laboratories. Understanding optimization principles enables engineers
    to democratize AI capabilities by making sophisticated models accessible across
    diverse deployment contexts, from billion-parameter language models running on
    mobile devices to embedded sensors.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习研究优先考虑精度，产生了在需要的地方无法部署的卓越性能模型：资源受限的移动设备、成本敏感的云环境或延迟关键的前端应用。模型优化连接了理论能力和实际部署，将计算密集型研究模型转化为高效系统，在满足对内存、能源、延迟和成本严格的约束的同时保持性能。没有系统化的优化技术，高级人工智能能力仍然被困在研究实验室中。理解优化原理使工程师能够通过使复杂模型在多样化的部署环境中可访问，从而民主化人工智能能力，从在移动设备上运行的数十亿参数语言模型到嵌入式传感器。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Compare model optimization techniques including pruning, quantization, knowledge
    distillation, and neural architecture search in terms of their mechanisms and
    applications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较包括剪枝、量化、知识蒸馏和神经架构搜索在内的模型优化技术，从其机制和应用方面进行比较
- en: Evaluate trade-offs between numerical precision levels and their effects on
    model accuracy, energy consumption, and hardware compatibility
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估数值精度级别与其对模型精度、能耗和硬件兼容性的影响之间的权衡
- en: Apply the tripartite optimization framework (model representation, numerical
    precision, architectural efficiency) to design deployment strategies for specific
    hardware constraints
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将三分优化框架（模型表示、数值精度、架构效率）应用于设计针对特定硬件约束的部署策略
- en: Analyze how hardware-aware design principles influence model architecture decisions
    and computational efficiency across different deployment platforms
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析硬件感知设计原则如何影响模型架构决策和不同部署平台上的计算效率
- en: Implement sparsity exploitation and dynamic computation techniques to improve
    inference performance while managing accuracy preservation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施稀疏性利用和动态计算技术，在管理精度保持的同时提高推理性能
- en: Design integrated optimization pipelines that combine multiple techniques to
    achieve specific deployment objectives within resource constraints
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计集成优化管道，结合多种技术，在资源约束内实现特定的部署目标
- en: Assess automated optimization approaches and their role in discovering novel
    optimization strategies beyond manual tuning
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估自动化优化方法及其在发现超出手动调整之外的新优化策略中的作用
- en: Model Optimization Fundamentals
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型优化基础
- en: Successful deployment of machine learning systems requires addressing the tension
    between model sophistication and computational feasibility. Contemporary research
    in machine learning has produced increasingly powerful models whose resource demands
    often exceed the practical constraints of real-world deployment environments.
    This represents the classic engineering challenge of translating theoretical advances
    into viable systems, affecting the accessibility and scalability of machine learning
    applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 成功部署机器学习系统需要解决模型复杂性与计算可行性之间的紧张关系。当代机器学习研究产生了越来越强大的模型，其资源需求往往超过了现实世界部署环境的实际限制。这代表了将理论进步转化为可行系统的经典工程挑战，影响了机器学习应用的可用性和可扩展性。
- en: The magnitude of this resource gap is substantial and multifaceted. State-of-the-art
    language models may require several hundred gigabytes of memory for full-precision
    parameter storage ([T. B. Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan,
    Shyam, Saxena, et al. 2020](ch058.xhtml#ref-brown2020gpt3); [Chowdhery et al.
    2022](ch058.xhtml#ref-chowdhery2022palm)), while target deployment platforms such
    as mobile devices typically provide only a few gigabytes of available memory.
    This disparity extends beyond memory constraints to encompass computational throughput,
    energy consumption, and latency requirements. The challenge is further compounded
    by the heterogeneous nature of deployment environments, each imposing distinct
    constraints and performance requirements.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种资源差距的幅度是巨大的，并且是多方面的。最先进的语言模型可能需要数百GB的内存来存储全精度参数（[T. B. Brown, Mann, Ryder,
    Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Saxena, 等人 2020](ch058.xhtml#ref-brown2020gpt3)；[Chowdhery
    等人 2022](ch058.xhtml#ref-chowdhery2022palm)），而目标部署平台如移动设备通常只提供几GB的可用内存。这种差异不仅限于内存限制，还包括计算吞吐量、能耗和延迟要求。部署环境的异构性质进一步加剧了这一挑战，每个环境都施加独特的约束和性能要求。
- en: Production machine learning systems operate within a complex optimization landscape
    characterized by multiple, often conflicting, performance objectives. Real-time
    applications impose strict latency bounds, mobile deployments require energy efficiency
    to preserve battery life, embedded systems must operate within thermal constraints,
    and cloud services demand cost-effective resource utilization at scale. These
    constraints collectively define a multi-objective optimization problem that requires
    systematic approaches to achieve satisfactory solutions across all relevant performance
    dimensions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 生产型机器学习系统运行在一个复杂的优化景观中，该景观由多个、通常是冲突的性能目标组成。实时应用施加严格的延迟限制，移动部署需要节能以保护电池寿命，嵌入式系统必须在热约束范围内运行，而云服务需要在大规模上实现成本效益的资源利用。这些限制共同定义了一个多目标优化问题，需要系统的方法来实现所有相关性能维度的满意解决方案。
- en: '***Model Optimization*** is the systematic transformation of machine learning
    models to maximize *computational efficiency* while preserving *task performance*,
    enabling deployment across *diverse hardware constraints*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型优化**是对机器学习模型进行系统性的转换，以最大化**计算效率**同时保持**任务性能**，使部署能够跨越**多样化的硬件约束**。'
- en: The engineering discipline of model optimization has evolved to address these
    challenges through systematic methodologies that integrate algorithmic innovation
    with hardware-aware design principles. Effective optimization strategies require
    deep understanding of the interactions between model architecture, numerical precision,
    computational patterns, and target hardware characteristics. This interdisciplinary
    approach transforms optimization from an ad hoc collection of techniques into
    a principled engineering discipline guided by theoretical foundations and empirical
    validation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 模型优化这一工程学科已经通过将算法创新与硬件感知设计原则相结合的系统方法来应对这些挑战。有效的优化策略需要深入理解模型架构、数值精度、计算模式和目标硬件特性之间的相互作用。这种跨学科的方法将优化从一种临时的技术集合转变为一个由理论基础和经验验证指导的原则性工程学科。
- en: 'This chapter establishes a comprehensive theoretical and practical framework
    for model optimization organized around three interconnected dimensions: structural
    efficiency in model representation, numerical efficiency through precision optimization,
    and computational efficiency via hardware-aware implementation. Through this framework,
    we examine how established techniques such as quantization achieve memory reduction
    and inference acceleration, how pruning methods eliminate parameter redundancy
    while preserving model accuracy, and how knowledge distillation enables capability
    transfer from complex models to efficient architectures. The overarching objective
    transcends simple performance metrics to enable the deployment of sophisticated
    machine learning capabilities across the complete spectrum of computational environments
    and application domains.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章建立了一个围绕三个相互关联维度构建的全面理论和实践框架，这三个维度是：模型表示中的结构效率、通过精度优化实现的数值效率以及通过硬件感知实现计算效率。通过这个框架，我们研究了量化等现有技术如何实现内存减少和推理加速，剪枝方法如何消除参数冗余同时保持模型精度，以及知识蒸馏如何使复杂模型到高效架构的能力迁移。总体目标超越了简单的性能指标，使复杂的机器学习能力能够在整个计算环境和应用领域得到部署。
- en: Optimization Framework
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化框架
- en: The optimization process operates through three interconnected dimensions that
    bridge software algorithms and hardware execution, as illustrated in [Figure 10.1](ch016.xhtml#fig-3-sections).
    Understanding these dimensions and their relationships provides the conceptual
    foundation for all techniques explored in this chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程通过三个相互关联的维度进行，这些维度将软件算法与硬件执行连接起来，如图10.1所示。[图10.1](ch016.xhtml#fig-3-sections)展示了这些维度及其关系，理解这些维度及其关系为本章探讨的所有技术提供了概念基础。
- en: '![](../media/file147.svg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![优化框架图](../media/file147.svg)'
- en: 'Figure 10.1: **Optimization Stack**: Model optimization progresses through
    three layers (efficient model representation, efficient numerics representation,
    and efficient hardware implementation), each addressing distinct aspects of system
    performance and resource utilization. These layers allow structured trade-offs
    between model accuracy, computational cost, and memory footprint to meet the demands
    of different deployment environments.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：**优化栈**：模型优化通过三个层次（高效模型表示、高效数值表示和高效硬件实现）进行，每个层次都针对系统性能和资源利用的不同方面。这些层次允许在模型精度、计算成本和内存占用之间进行结构化权衡，以满足不同部署环境的需求。
- en: Understanding these layer interactions reveals the systematic nature of optimization
    engineering. Model representation techniques (pruning, distillation, structured
    approximations) reduce computational complexity while creating opportunities for
    numerical precision optimization. Quantization and reduced-precision arithmetic
    exploit hardware capabilities for faster execution, while architectural efficiency
    techniques align computation patterns with processor designs. Software optimizations
    establish the foundation for hardware acceleration by creating structured, predictable
    workloads that specialized processors can execute efficiently.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些层之间的相互作用揭示了优化工程的系统性。模型表示技术（剪枝、蒸馏、结构化近似）降低计算复杂性，同时为数值精度优化创造机会。量化和低精度算术利用硬件能力实现更快执行，而架构效率技术使计算模式与处理器设计相匹配。软件优化通过创建结构化、可预测的工作负载，为硬件加速奠定基础，这些工作负载可以由专用处理器高效执行。
- en: This chapter examines each optimization layer through an engineering lens, providing
    specific algorithms for quantization (post-training and quantization-aware training),
    pruning strategies (magnitude-based, structured, and dynamic), and distillation
    procedures (temperature scaling, feature transfer). We explore how these techniques
    combine synergistically and how their effectiveness depends on target hardware
    characteristics. The framework guides systematic optimization decisions, ensuring
    that model transformations align with deployment constraints while preserving
    essential capabilities.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从工程角度审视每个优化层，提供了量化（训练后量化和量化感知训练）、剪枝策略（基于幅度、结构化和动态）以及蒸馏程序（温度缩放、特征迁移）的具体算法。我们探讨了这些技术如何协同作用，以及它们的有效性如何依赖于目标硬件特性。该框架指导系统性的优化决策，确保模型转换与部署约束相一致，同时保留基本功能。
- en: This chapter transforms the efficiency concepts from earlier foundations into
    actionable engineering practices through systematic application of optimization
    principles. Mastery of quantization, pruning, and distillation techniques provides
    practitioners with the essential tools for deploying sophisticated machine learning
    models across diverse computational environments. The optimization framework presented
    bridges the gap between theoretical model capabilities and practical deployment
    requirements, enabling machine learning systems that deliver both performance
    and efficiency in real-world applications.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过系统地应用优化原则，将早期基础中的效率概念转化为可操作的工程实践。掌握量化、剪枝和蒸馏技术为从业者提供了部署复杂机器学习模型到各种计算环境的基本工具。所提出的优化框架弥合了理论模型能力与实际部署需求之间的差距，使机器学习系统能在现实应用中提供性能和效率。
- en: Deployment Context
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署环境
- en: Machine learning models operate as part of larger systems with complex constraints,
    dependencies, and trade-offs. Model optimization cannot be treated as a purely
    algorithmic problem; it must be viewed as a systems-level challenge that considers
    computational efficiency, scalability, deployment feasibility, and overall system
    performance. Operational principles from [Chapter 13](ch019.xhtml#sec-ml-operations)
    provide the foundation for understanding the systems perspective on model optimization,
    highlighting why optimization is important, the key constraints that drive optimization
    efforts, and the principles that define an effective optimization strategy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型作为更大系统的一部分运行，具有复杂的约束、依赖和权衡。模型优化不能被视为一个纯粹算法问题；它必须被视为一个系统级挑战，需要考虑计算效率、可扩展性、部署可行性和整体系统性能。[第13章](ch019.xhtml#sec-ml-operations)中的操作原则为理解模型优化的系统视角提供了基础，强调了优化为何重要，驱动优化努力的约束关键，以及定义有效优化策略的原则。
- en: Practical Deployment
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际部署
- en: Modern machine learning models often achieve impressive accuracy on benchmark
    datasets, but making them practical for real-world use is far from trivial. Machine
    learning systems operate under computational, memory, latency, and energy constraints
    that significantly impact both training and inference ([Choudhary et al. 2020](ch058.xhtml#ref-choudhary2020comprehensive)).
    Models that perform well in research settings may prove impractical when integrated
    into broader systems, regardless of deployment context including cloud environments,
    smartphone integration, or microcontroller implementation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习模型在基准数据集上通常能实现令人印象深刻的准确性，但使它们适用于实际应用远非易事。机器学习系统在计算、内存、延迟和能源约束下运行，这些约束对训练和推理都有重大影响
    ([Choudhary et al. 2020](ch058.xhtml#ref-choudhary2020comprehensive))。在研究环境中表现良好的模型，在集成到更广泛的系统中时可能证明不切实际，无论部署环境包括云环境、智能手机集成还是微控制器实现。
- en: Beyond these deployment complexities, real-world feasibility encompasses efficiency
    in training, storage, and execution rather than accuracy alone.[1](#fn1)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些部署复杂性之外，实际可行性包括训练、存储和执行效率，而不仅仅是准确性。[1](#fn1)
- en: Efficiency requirements manifest differently across deployment contexts. In
    large-scale cloud ML settings, optimizing models helps minimize training time,
    computational cost, and power consumption, making large-scale AI workloads more
    efficient ([Jeff Dean, Patterson, and Young 2018](ch058.xhtml#ref-dean2018new)).
    In contrast, edge ML[2](#fn2) requires models to run with limited compute resources,
    necessitating optimizations that reduce memory footprint and computational complexity.
    Mobile ML introduces additional constraints, such as battery life and real-time
    responsiveness, while tiny ML[3](#fn3) pushes efficiency to the extreme, requiring
    models to fit within the memory and processing limits of ultra-low-power devices
    ([C. R. Banbury et al. 2020](ch058.xhtml#ref-banbury2020benchmarking)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 效率要求在不同部署环境中表现不同。在大规模云机器学习设置中，优化模型有助于最小化训练时间、计算成本和功耗，使大规模人工智能工作负载更加高效 ([Jeff
    Dean, Patterson, and Young 2018](ch058.xhtml#ref-dean2018new))。相比之下，边缘机器学习[2](#fn2)要求模型在有限的计算资源下运行，需要优化以减少内存占用和计算复杂性。移动机器学习引入了额外的约束，如电池寿命和实时响应性，而微型机器学习[3](#fn3)将效率推向极致，要求模型适应超低功耗设备的内存和处理限制
    ([C. R. Banbury et al. 2020](ch058.xhtml#ref-banbury2020benchmarking))。
- en: Optimization contributes to sustainable and accessible AI deployment, following
    sustainability principles established in [Chapter 18](ch024.xhtml#sec-sustainable-ai).
    Reducing a model’s energy footprint is important as AI workloads scale, helping
    mitigate the environmental impact of large-scale ML training and inference ([D.
    Patterson et al. 2021a](ch058.xhtml#ref-patterson2021carbon)). At the same time,
    optimized models can expand the reach of machine learning, supporting applications
    in low-resource environments, from rural healthcare to autonomous systems operating
    in the field.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 优化有助于可持续和可访问的人工智能部署，遵循在第18章[Chapter 18](ch024.xhtml#sec-sustainable-ai)中确立的可持续性原则。随着人工智能工作负载的扩展，减少模型的能耗足迹非常重要，有助于减轻大规模机器学习训练和推理的环境影响([D.
    Patterson et al. 2021a](ch058.xhtml#ref-patterson2021carbon))。同时，优化的模型可以扩大机器学习的应用范围，支持在资源匮乏的环境中的应用，从农村医疗保健到在野外运行的自主系统。
- en: Balancing Trade-offs
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平衡权衡
- en: The tension between accuracy and efficiency drives optimization decisions across
    all dimensions. Increasing model capacity generally enhances predictive performance
    while increasing computational cost, resulting in slower, more resource-intensive
    inference. These improvements introduce challenges related to memory footprint[4](#fn4),
    inference latency, power consumption, and training efficiency. As machine learning
    systems are deployed across a wide range of hardware platforms, balancing accuracy
    and efficiency becomes a key challenge in model optimization.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性与效率之间的紧张关系推动了所有维度的优化决策。增加模型容量通常可以增强预测性能，同时增加计算成本，导致推理速度变慢，资源消耗更多。这些改进引入了与内存占用[4](#fn4)、推理延迟、功耗和训练效率相关的挑战。随着机器学习系统部署在广泛的硬件平台上，平衡准确性和效率成为模型优化中的关键挑战。
- en: This tension manifests differently across deployment contexts. Training requires
    computational resources that scale with model size, while inference demands strict
    latency and power constraints in real-time applications.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这种紧张关系在不同的部署环境中表现不同。训练需要与模型大小成比例的计算资源，而推理则要求实时应用中具有严格的延迟和功耗限制。
- en: Framework Application and Navigation
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 框架应用与导航
- en: This section provides practical guidance for applying optimization techniques
    to real-world problems, examining how system constraints map to optimization dimensions
    and offering navigation strategies for technique selection.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了将优化技术应用于实际问题的实用指导，探讨系统约束如何映射到优化维度，并提供了技术选择导航策略。
- en: Mapping Constraints
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 约束映射
- en: Understanding how system constraints map to optimization dimensions provides
    a navigation framework before examining specific techniques. When facing deployment
    challenges, this mapping guides practitioners toward the most relevant approaches.
    Memory bandwidth limitations indicate focus areas in model representation and
    numerical precision optimizations, while latency bottlenecks suggest examination
    of model representation and architectural efficiency techniques.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 了解系统约束如何映射到优化维度，在检查具体技术之前提供了一个导航框架。面对部署挑战时，这种映射指导实践者走向最相关的方法。内存带宽限制表明了模型表示和数值精度优化的关注领域，而延迟瓶颈则建议检查模型表示和架构效率技术。
- en: '[Table 10.1](ch016.xhtml#tbl-constraint-opt-mapping) summarizes how different
    system constraints map to the three core dimensions of model optimization.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10.1](ch016.xhtml#tbl-constraint-opt-mapping)总结了不同的系统约束如何映射到模型优化的三个核心维度。'
- en: 'Table 10.1: **Optimization Dimensions**: System constraints drive optimization
    along three core dimensions—model representation, numerical precision, and architectural
    efficiency—each addressing different resource limitations and performance goals.
    The table maps computational cost to precision and efficiency, memory/storage
    to representation and precision, and latency/throughput to representation and
    efficiency, guiding the selection of appropriate optimization techniques.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.1：**优化维度**：系统约束推动优化沿着三个核心维度——模型表示、数值精度和架构效率——进行，每个维度解决不同的资源限制和性能目标。该表将计算成本映射到精度和效率，内存/存储映射到表示和精度，延迟/吞吐量映射到表示和效率，指导选择适当的优化技术。
- en: '| **System Constraint** | **Model Representation** | **Numerical Precision**
    | **Architectural Efficiency** |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **系统约束** | **模型表示** | **数值精度** | **架构效率** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Computational Cost** | ✗ | ✓ | ✓ |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **计算成本** | ✗ | ✓ | ✓ |'
- en: '| **Memory and Storage** | ✓ | ✓ | ✗ |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **内存和存储** | ✓ | ✓ | ✗ |'
- en: '| **Latency and Throughput** | ✓ | ✗ | ✓ |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| **延迟和吞吐量** | ✓ | ✗ | ✓ |'
- en: '| **Energy Efficiency** | ✗ | ✓ | ✓ |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **能源效率** | ✗ | ✓ | ✓ |'
- en: '| **Scalability** | ✓ | ✗ | ✓ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **可扩展性** | ✓ | ✗ | ✓ |'
- en: This systematic mapping builds on the efficiency principles established in [Chapter 9](ch015.xhtml#sec-efficient-ai).
    Here we focus specifically on model-level optimizations that implement these efficiency
    principles through concrete techniques. Although each system constraint primarily
    aligns with one or more optimization dimensions, the relationships are not strictly
    one-to-one. Many optimization techniques affect multiple constraints simultaneously.
    Structuring model optimization along these three dimensions and mapping techniques
    to specific system constraints allows practitioners to analyze trade-offs more
    effectively and select optimizations that best align with deployment requirements.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这种系统映射建立在第9章（ch015.xhtml#sec-efficient-ai）中确立的效率原则之上。在这里，我们特别关注通过具体技术实现这些效率原则的模型级优化。尽管每个系统约束主要与一个或多个优化维度相对应，但关系并非严格一对一。许多优化技术同时影响多个约束。沿着这三个维度构建模型优化并将技术映射到特定的系统约束，使从业者能够更有效地分析权衡并选择与部署要求最佳匹配的优化。
- en: Navigation Strategies
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导航策略
- en: This chapter presents a comprehensive toolkit of optimization techniques spanning
    model representation, numerical precision, and architectural efficiency. However,
    not all techniques apply to every problem, and the sheer variety can feel overwhelming.
    This navigation guide helps you determine where to start based on your specific
    constraints and objectives.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一套全面的优化技术工具包，涵盖了模型表示、数值精度和架构效率。然而，并非所有技术都适用于每个问题，而且种类繁多可能会让人感到不知所措。本导航指南帮助您根据具体的约束和目标确定从哪里开始。
- en: '[Table 10.1](ch016.xhtml#tbl-constraint-opt-mapping) identifies which optimization
    dimension addresses specific bottlenecks. Memory or model size limitations indicate
    focus on model representation and numerical precision techniques that reduce parameter
    count and bit-width. Inference latency requirements suggest examination of model
    representation and architectural efficiency approaches that reduce computational
    workload and improve hardware utilization. Training or inference cost constraints
    prioritize numerical precision and architectural efficiency methods that minimize
    computational cost per operation. Unacceptable accuracy degradation indicates
    training-aware optimization techniques integrated into the training process rather
    than post-hoc application.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10.1](ch016.xhtml#tbl-constraint-opt-mapping) 识别了哪些优化维度针对特定的瓶颈。内存或模型大小限制表明应关注模型表示和数值精度技术，这些技术可以减少参数数量和位宽。推理延迟需求表明应检查模型表示和架构效率方法，这些方法可以减少计算工作量并提高硬件利用率。训练或推理成本约束优先考虑数值精度和架构效率方法，这些方法可以最小化每操作的计算成本。不接受的精度下降表明应将训练感知优化技术集成到训练过程中，而不是事后应用。'
- en: Production systems typically follow established patterns rather than random
    technique exploration. Quick deployment approaches apply post-training modifications
    that require minimal code changes, achieving 4-8x compression with 1-2% accuracy
    loss in hours ([Gholami et al. 2021](ch058.xhtml#ref-gholami2021survey); [Nagel
    et al. 2021a](ch058.xhtml#ref-nagel2021white)). Production-grade optimization
    combines multiple techniques sequentially (reducing parameters, recovering accuracy
    through training refinement, then applying quantization), achieving 8-15x compression
    with <1% accuracy loss over weeks. Extreme constraint scenarios targeting sub-1MB
    models require architectural changes from the start, including automated architecture
    discovery and ultra-low precision, necessitating months of specialized engineering.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 生产系统通常遵循既定模式，而不是随机技术探索。快速部署方法应用后训练修改，需要最少的代码更改，在几小时内实现4-8倍的压缩，精度损失在1-2% ([Gholami等人2021](ch058.xhtml#ref-gholami2021survey)；[Nagel等人2021a](ch058.xhtml#ref-nagel2021white))。生产级优化按顺序结合多种技术（减少参数，通过训练细化恢复精度，然后应用量化），在几周内实现8-15倍的压缩，精度损失小于1%。针对小于1MB模型的极端约束场景需要从开始就进行架构更改，包括自动架构发现和超低精度，这需要数月的专门工程。
- en: Model optimization represents a systems engineering challenge rather than a
    universal solution. Optimization benefits depend heavily on target hardware, with
    identical quantization techniques achieving 4x speedup on specialized accelerators
    versus 1.5x on general-purpose processors ([Jacob et al. 2018b](ch058.xhtml#ref-jacob2018quantization);
    [Krishnamoorthi 2018](ch058.xhtml#ref-krishnamoorthi2018quantizing)). Accuracy
    preservation varies by model architecture and task, as vision models often tolerate
    aggressive optimization more effectively than language models. Optimization requires
    iterative measurement rather than single application. System-level bottlenecks
    may limit benefits when data preprocessing or network I/O dominate latency, rendering
    model optimization minimally effective. System-wide profiling before optimization
    investment remains essential (detailed in the Strategy and Implementation section).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 模型优化代表的是一个系统工程挑战，而不是一个通用的解决方案。优化的好处高度依赖于目标硬件，相同的量化技术在专用加速器上可以实现4倍的速度提升，而在通用处理器上只能实现1.5倍（[Jacob等人2018b](ch058.xhtml#ref-jacob2018quantization)；[Krishnamoorthi
    2018](ch058.xhtml#ref-krishnamoorthi2018quantizing)）。准确性的保持因模型架构和任务而异，因为视觉模型通常比语言模型更能有效地容忍激进优化。优化需要迭代测量而不是单一应用。当数据预处理或网络I/O主导延迟时，系统级瓶颈可能会限制好处，使得模型优化效果最小化。在优化投资之前进行系统级分析仍然至关重要（在策略和实施部分详细说明）。
- en: This comprehensive chapter supports non-linear reading approaches. ML engineers
    deploying existing models benefit from focusing on post-training techniques in
    the numerical precision section, which provide rapid improvements with minimal
    code changes. Researchers and advanced practitioners require thorough examination,
    with particular attention to mathematical formulations and integration principles.
    Students new to optimization benefit from following progressive complexity markers,
    advancing from foundational techniques to advanced methods and from basic concepts
    to specialized algorithms. Each major section builds systematically from accessible
    to sophisticated approaches.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这章全面支持非线性阅读方法。部署现有模型的ML工程师可以从数值精度部分的训练后技术中受益，这些技术通过最小的代码更改提供快速改进。研究人员和高级实践者需要进行彻底的审查，特别关注数学公式和集成原理。对优化新接触的学生来说，遵循渐进的复杂性标记是有益的，从基础技术到高级方法，从基本概念到专用算法。每个主要部分都是系统地从易到难的方法构建的。
- en: Optimization Dimensions
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化维度
- en: Each optimization dimension merits detailed examination. As shown in [Figure 10.1](ch016.xhtml#fig-3-sections),
    model representation optimization reduces what computations are performed, numerical
    precision optimization changes how computations are executed, and architectural
    efficiency optimization ensures operations run efficiently on target hardware.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 每个优化维度都值得详细审查。如图10.1[图](ch016.xhtml#fig-3-sections)所示，模型表示优化减少了需要执行的计算，数值精度优化改变了计算执行的方式，而架构效率优化确保操作在目标硬件上高效运行。
- en: Model Representation
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型表示
- en: The first dimension, model representation optimization, focuses on eliminating
    redundancy in the structure of machine learning models. Large models often contain
    excessive parameters[5](#fn5) that contribute little to overall performance but
    significantly increase memory footprint and computational cost. Optimizing model
    representation involves techniques that remove unnecessary components while maintaining
    predictive accuracy. These include pruning, knowledge distillation, and automated
    architecture search methods that refine model structures to balance efficiency
    and accuracy. These optimizations primarily impact how models are designed at
    an algorithmic level, ensuring that they remain effective while being computationally
    manageable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个维度，模型表示优化，专注于消除机器学习模型结构中的冗余。大型模型通常包含过多的参数[5](#fn5)，这些参数对整体性能贡献不大，但会显著增加内存占用和计算成本。优化模型表示涉及的技术包括去除不必要的组件同时保持预测准确性。这些技术包括剪枝、知识蒸馏和自动架构搜索方法，这些方法可以细化模型结构以平衡效率和准确性。这些优化主要影响模型在算法层面的设计，确保它们在计算上可管理的同时保持有效性。
- en: Numerical Precision
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数值精度
- en: While representation techniques modify what computations are performed, precision
    optimization changes how those computations are executed by reducing the numerical
    fidelity of weights, activations, and arithmetic operations. The second dimension,
    numerical precision optimization, addresses how numerical values are represented
    and processed within machine learning models. The precision optimization techniques
    detailed in this section address these efficiency challenges. Quantization techniques
    map high-precision weights and activations to lower-bit representations, enabling
    efficient execution on hardware accelerators such as GPUs, TPUs, and specialized
    AI chips ([Chapter 11](ch017.xhtml#sec-ai-acceleration)). Mixed-precision training[6](#fn6)
    dynamically adjusts precision levels during training to strike a balance between
    efficiency and accuracy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然表示技术修改了执行的计算，但精度优化通过降低权重、激活和算术操作的数值精度来改变这些计算的执行方式。第二个维度，数值精度优化，关注机器学习模型中数值的表示和处理方式。本节中详细介绍的精度优化技术解决了这些效率挑战。量化技术将高精度权重和激活映射到低比特表示，使得在GPU、TPU和专用AI芯片等硬件加速器上高效执行成为可能（[第11章](ch017.xhtml#sec-ai-acceleration)）。混合精度训练[6](#fn6)在训练过程中动态调整精度水平，在效率和精度之间取得平衡。
- en: Careful numerical precision optimization enables significant computational cost
    reductions while maintaining acceptable accuracy levels, providing sophisticated
    model access in resource-constrained environments.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细的数值精度优化可以在保持可接受的精度水平的同时，显著降低计算成本，在资源受限的环境中提供复杂的模型访问。
- en: Architectural Efficiency
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 架构效率
- en: The third dimension, architectural efficiency, addresses efficient computation
    performance during training and inference. Well-designed model structure proves
    insufficient when execution remains suboptimal. Many machine learning models contain
    redundancies in their computational graphs, leading to inefficiencies in how operations
    are scheduled and executed. Sparsity[7](#fn7) represents a key architectural efficiency
    technique where models exploit zero-valued parameters to reduce computation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第三维，架构效率，关注训练和推理期间的计算性能效率。当执行仍然不理想时，良好的模型结构证明是不够的。许多机器学习模型在其计算图中包含冗余，导致操作调度和执行的不效率。稀疏性[7](#fn7)代表了一种关键的架构效率技术，其中模型利用零值参数来减少计算。
- en: Architectural efficiency involves techniques that exploit sparsity in both model
    weights and activations, factorize large computational components into more efficient
    forms[8](#fn8), and dynamically adjust computation based on input complexity.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 架构效率涉及利用模型权重和激活中的稀疏性，将大型计算组件分解成更有效的形式[8](#fn8)，并根据输入复杂性动态调整计算的技术。
- en: These architectural optimization methods improve execution efficiency across
    different hardware platforms, reducing latency and power consumption. These efficiency
    principles extend naturally to training scenarios, where techniques such as gradient
    checkpointing and low-rank adaptation[9](#fn9) help reduce memory overhead and
    computational demands.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构优化方法提高了不同硬件平台上的执行效率，降低了延迟和功耗。这些效率原则自然地扩展到训练场景，其中梯度检查点和低秩适应[9](#fn9)等技术有助于减少内存开销和计算需求。
- en: Three-Dimensional Optimization Framework
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 三维优化框架
- en: The interconnected nature of this three-dimensional framework emerges when examining
    technique interactions. Pruning primarily addresses model representation but also
    affects architectural efficiency by reducing inference operations. Quantization
    focuses on numerical precision but impacts memory footprint and execution efficiency.
    Understanding these interdependencies enables optimal optimization combinations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当检查技术交互时，这个三维框架的相互关联性显现出来。剪枝主要解决模型表示问题，但通过减少推理操作也影响了架构效率。量化主要关注数值精度，但会影响内存占用和执行效率。理解这些相互依赖关系可以使得优化组合达到最佳。
- en: This interconnected nature means that the choice of optimizations is driven
    by system constraints, which define the practical limitations within which models
    must operate. A machine learning model deployed in a data center has different
    constraints from one running on a mobile device or an embedded system. Computational
    cost, memory usage, inference latency, and energy efficiency all influence which
    optimizations are most appropriate for a given scenario. A model that is too large
    for a resource-constrained device may require aggressive pruning and quantization,
    while a latency-sensitive application may benefit from operator fusion[10](#fn10)
    and hardware-aware scheduling.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种相互关联的特性意味着优化选择是由系统约束驱动的，这些约束定义了模型必须在其内运行的实用限制。在数据中心部署的机器学习模型与在移动设备或嵌入式系统上运行的模型有不同的约束。计算成本、内存使用、推理延迟和能效都会影响针对特定场景最合适的优化方案。对于资源受限的设备来说，如果模型过大，可能需要进行激进的剪枝和量化，而对于对延迟敏感的应用程序，可能从算子融合[10](#fn10)和硬件感知调度中受益。
- en: The constraint-dimension mapping established in [Table 10.1](ch016.xhtml#tbl-constraint-opt-mapping)
    demonstrates interdependence between optimization strategies and real-world constraints.
    These relationships extend beyond one-to-one correspondence, as many optimization
    techniques affect multiple constraints simultaneously.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表10.1](ch016.xhtml#tbl-constraint-opt-mapping)中建立的约束-维度映射展示了优化策略与现实世界约束之间的相互依赖性。这些关系超越了一对一的对应关系，因为许多优化技术同时影响多个约束。
- en: Systematic examination of each dimension begins with model representation optimization,
    encompassing techniques that modify neural network structure and parameters to
    eliminate redundancy while preserving accuracy.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个维度的系统审查从模型表示优化开始，包括修改神经网络结构和参数以消除冗余同时保持准确性的技术。
- en: Structural Model Optimization Methods
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化模型优化方法
- en: 'Model representation optimization modifies neural network structure and parameters
    to improve efficiency while preserving accuracy. Modern models often prioritize
    accuracy over efficiency, containing excessive parameters that increase costs
    and slow inference. This optimization addresses inefficiencies through two objectives:
    eliminating redundancy (exploiting overparameterization where models achieve similar
    performance with fewer parameters) and structuring computations for efficient
    hardware execution through techniques like gradient checkpointing[11](#fn11) and
    parallel processing patterns[12](#fn12).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 模型表示优化通过修改神经网络结构和参数来提高效率同时保持准确性。现代模型通常优先考虑准确性而不是效率，包含过多的参数，这增加了成本并减慢了推理速度。这种优化通过两个目标来解决低效：消除冗余（利用过度参数化，即模型在更少的参数下也能达到相似的性能）以及通过梯度检查点[11](#fn11)和并行处理模式[12](#fn12)等技术对计算进行结构化以实现高效的硬件执行。
- en: The optimization challenge lies in balancing competing constraints[13](#fn13).
    Aggressive compression risks accuracy degradation that renders models unreliable
    for production use, while insufficient optimization leaves models too large or
    slow for target deployment environments. Selecting appropriate techniques requires
    understanding trade-offs between model size, computational complexity, and generalization
    performance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 优化挑战在于平衡相互竞争的约束[13](#fn13)。激进的压缩可能会降低准确性，使得模型在生产使用中不可靠，而优化不足则可能导致模型过大或运行速度过慢，不适合目标部署环境。选择适当的技术需要理解模型大小、计算复杂性和泛化性能之间的权衡。
- en: 'Three key techniques address this challenge: pruning eliminates low-impact
    parameters, knowledge distillation transfers capabilities to smaller models, and
    NAS automates architecture design for specific constraints. Each technique offers
    distinct optimization pathways while maintaining model performance.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 三种关键技术应对这一挑战：剪枝消除低影响参数，知识蒸馏将能力转移到更小的模型，而NAS针对特定约束自动化架构设计。每种技术提供独特的优化路径，同时保持模型性能。
- en: These three techniques represent distinct but complementary approaches within
    our optimization framework. Pruning and knowledge distillation reduce redundancy
    in existing models, while NAS addresses building optimized architectures from
    the ground up. In many cases, they can be combined to achieve even greater optimization.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种技术代表了我们优化框架中独特但互补的方法。剪枝和知识蒸馏减少了现有模型中的冗余，而NAS从底层构建优化架构。在许多情况下，它们可以结合使用，以实现更大的优化。
- en: Pruning
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 剪枝
- en: 'The memory wall constrains system performance: as models grow larger, memory
    bandwidth becomes the bottleneck rather than computational capacity. Pruning directly
    addresses this constraint by lowering memory requirements through parameter elimination.
    State-of-the-art machine learning models often contain millions or billions of
    parameters, many of which contribute minimally to final predictions. While large
    models enhance representational power and generalization, they also introduce
    inefficiencies in memory footprint, computational cost, and scalability that impact
    both training and deployment across cloud, edge, and mobile environments.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 存储墙限制了系统性能：随着模型变大，内存带宽成为瓶颈，而不是计算能力。修剪通过参数消除来降低内存需求，直接解决了这一限制。最先进的机器学习模型通常包含数百万或数十亿个参数，其中许多对最终预测的贡献很小。虽然大型模型增强了表示能力和泛化能力，但它们也引入了内存占用、计算成本和可扩展性的低效，这影响了在云、边缘和移动环境中的训练和部署。
- en: Parameter necessity for accuracy maintenance varies considerably. Many weights
    contribute minimally to decision-making processes, enabling significant efficiency
    improvements through removal without substantial performance degradation. This
    redundancy exists because modern neural networks are heavily overparameterized,
    meaning they have far more weights than are strictly necessary to solve a task.
    This overparameterization serves important purposes during training by providing
    multiple optimization paths and helping avoid poor local minima, but it creates
    opportunities for compression during deployment. Model compression preserves performance
    through information-theoretic principles from [Chapter 3](ch009.xhtml#sec-dl-primer),
    where neural networks’ overparameterization creates compression opportunities.
    This observation motivates pruning, a class of optimization techniques that systematically
    removes redundant parameters while preserving model accuracy.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持准确度，参数的必要性差异很大。许多权重对决策过程的影响很小，通过去除这些权重可以实现显著的效率提升，而不会对性能造成实质性下降。这种冗余存在的原因是现代神经网络过度参数化严重，这意味着它们拥有的权重远多于解决任务所必需的。这种过度参数化在训练期间提供了多个优化路径，并有助于避免局部最小值，但在部署期间为压缩创造了机会。模型压缩通过来自[第3章](ch009.xhtml#sec-dl-primer)的信息论原理来保持性能，其中神经网络的过度参数化创造了压缩机会。这一观察结果促使人们进行修剪，这是一种优化技术，它系统地去除冗余参数，同时保持模型准确度。
- en: '***Pruning*** is a model optimization technique that removes *redundant parameters*
    from neural networks while preserving *performance*, reducing *model size* and
    *computational cost* for efficient deployment.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '***修剪***是一种模型优化技术，它从神经网络中去除*冗余参数*，同时保持*性能*，以减少*模型大小*和*计算成本*，从而实现高效的部署。'
- en: Pruning enables models to become smaller, faster, and more efficient without
    requiring architectural redesign. By eliminating redundancy, pruning directly
    addresses the memory, computation, and scalability constraints of machine learning
    systems, making it essential for deploying models across diverse hardware platforms.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪技术使得模型可以变得更小、更快、更高效，而无需重新设计架构。通过消除冗余，修剪直接解决了机器学习系统在内存、计算和可扩展性方面的限制，使其在跨不同硬件平台部署模型时变得至关重要。
- en: Modern frameworks provide built-in APIs that make these optimization techniques
    readily accessible. PyTorch offers `torch.nn.utils.prune` for pruning operations,
    while TensorFlow provides the Model Optimization Toolkit[14](#fn14) with functions
    like `tfmot.sparsity.keras.prune_low_magnitude()`. These tools transform complex
    research algorithms into practical function calls, making optimization achievable
    for practitioners at all levels.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现代框架提供了内置的API，使得这些优化技术易于访问。PyTorch提供了`torch.nn.utils.prune`用于修剪操作，而TensorFlow提供了模型优化工具包[14](#fn14)，其中包括`tfmot.sparsity.keras.prune_low_magnitude()`等函数。这些工具将复杂的研究算法转化为实用的函数调用，使得所有级别的从业者都能实现优化。
- en: Pruning Example
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 修剪示例
- en: Pruning can be illustrated through systematic example. Pruning identifies weights
    contributing minimally to model predictions and removes them while maintaining
    accuracy. The most intuitive approach examines weight magnitudes, as weights with
    small absolute values typically have minimal impact on outputs, making them candidates
    for removal.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪可以通过系统性的示例来展示。修剪识别对模型预测贡献最小的权重，并在保持准确度的同时去除它们。最直观的方法是检查权重的大小，因为绝对值较小的权重通常对输出影响很小，因此它们是移除的候选者。
- en: '[Listing 10.1](ch016.xhtml#lst-pruning_example) demonstrates magnitude-based
    pruning on a 3×3 weight matrix, showing how a simple threshold rule creates sparsity.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表10.1](ch016.xhtml#lst-pruning_example)演示了在3×3权重矩阵上基于幅度的剪枝，展示了如何通过简单的阈值规则创建稀疏性。'
- en: 'Listing 10.1: **Magnitude-Based Pruning**: Removes weights below a threshold
    to create sparse matrices, reducing the number of nonzero parameters from 9 to
    4.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 代码列表10.1：**基于幅度的剪枝**：移除低于阈值的权重以创建稀疏矩阵，将非零参数的数量从9减少到4。
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This example illustrates the core pruning objective: minimize the number of
    parameters while maintaining model performance. We reduced nonzero parameters
    from 9 to 4 (keeping only 4 weights, hence a budget of <semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation
    encoding="application/x-tex">k=4</annotation></semantics>). The weights with smallest
    magnitudes (0.4, 0.1, 0.05, 0.03, 0.02) were removed, while the four largest magnitude
    weights (0.8, -0.7, -0.9, -0.6) were preserved.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明了核心剪枝目标：在保持模型性能的同时最小化参数数量。我们将非零参数从9减少到4（仅保留4个权重，因此预算为<semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation
    encoding="application/x-tex">k=4</annotation></semantics>）。幅度最小的权重（0.4, 0.1, 0.05,
    0.03, 0.02）被移除，而幅度最大的四个权重（0.8, -0.7, -0.9, -0.6）被保留。
- en: Extending this intuition to full neural networks requires considering both how
    many parameters to remove (the sparsity level) and which parameters to remove
    (the selection criterion). The next visualization shows this applied to larger
    weight matrices.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 将这种直觉扩展到完整的神经网络需要考虑两个问题：要移除多少个参数（稀疏度水平）以及要移除哪些参数（选择标准）。接下来的可视化展示了这一过程在更大的权重矩阵上的应用。
- en: As illustrated in [Figure 10.2](ch016.xhtml#fig-sparse-matrix), pruning reduces
    the number of nonzero weights by eliminating small-magnitude values, transforming
    a dense weight matrix into a sparse representation. This explicit enforcement
    of sparsity aligns with the <semantics><msub><mo>ℓ</mo><mn>0</mn></msub><annotation
    encoding="application/x-tex">\ell_0</annotation></semantics>-norm constraint in
    our optimization formulation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图10.2](ch016.xhtml#fig-sparse-matrix)所示，剪枝通过消除小幅度值来减少非零权重数量，将密集的权重矩阵转换为稀疏表示。这种对稀疏性的显式强制与我们的优化公式中的<semantics><msub><mo>ℓ</mo><mn>0</mn></msub><annotation
    encoding="application/x-tex">\ell_0</annotation></semantics>-范数约束相一致。
- en: '![](../media/file148.svg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file148.svg)'
- en: 'Figure 10.2: **Sparse Matrix Transformation**: Pruning removes small-magnitude
    weights (shown as white/zero in the right matrix) while preserving large-magnitude
    weights (shown in color), creating a sparse representation that reduces both memory
    usage and computation while maintaining model accuracy.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：**稀疏矩阵转换**：剪枝移除小幅度权重（在右侧矩阵中显示为白色/零），同时保留大幅度权重（以颜色显示），创建一个既减少内存使用和计算，又保持模型精度的稀疏表示。
- en: Mathematical Formulation
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数学公式
- en: 'The goal of pruning can be stated simply: we want to find the version of our
    model that has the fewest non-zero weights (the smallest size) while causing the
    smallest possible increase in the prediction error (the loss). This intuitive
    goal translates into a mathematical optimization problem that guides practical
    pruning algorithms.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝的目标可以简单地陈述：我们希望找到我们的模型版本，它具有最少的非零权重（最小尺寸），同时引起预测误差（损失）的最小增加。这个直观的目标转化为一个数学优化问题，指导实际的剪枝算法。
- en: 'The pruning process can be formalized as an optimization problem. Given a trained
    model with parameters <semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics>,
    we seek a sparse version <semantics><mover><mi>W</mi><mo accent="true">̂</mo></mover><annotation
    encoding="application/x-tex">\hat{W}</annotation></semantics> that retains only
    the most important parameters. The objective is expressed as:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝过程可以形式化为一个优化问题。给定一个具有参数<semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics>的训练模型，我们寻求一个稀疏版本<semantics><mover><mi>W</mi><mo
    accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{W}</annotation></semantics>，它仅保留最重要的参数。目标被表达为：
- en: <semantics><mrow><munder><mo>min</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover></munder><mi>ℒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover><mo
    stretchy="true" form="postfix">)</mo></mrow><mtext mathvariant="normal">subject
    to</mtext><mo stretchy="false" form="postfix">∥</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover><msub><mo
    stretchy="false" form="postfix">∥</mo><mn>0</mn></msub><mo>≤</mo><mi>k</mi></mrow>
    <annotation encoding="application/x-tex">\min_{\hat{W}} \mathcal{L}(\hat{W}) \quad
    \text{subject to} \quad \|\hat{W}\|_0 \leq k</annotation></semantics>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><munder><mo>min</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover></munder><mi>ℒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover><mo
    stretchy="true" form="postfix">)</mo></mrow><mtext mathvariant="normal">subject
    to</mtext><mo stretchy="false" form="postfix">∥</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover><msub><mo
    stretchy="false" form="postfix">∥</mo><mn>0</mn></msub><mo>≤</mo><mi>k</mi></mrow>
    <annotation encoding="application/x-tex">\min_{\hat{W}} \mathcal{L}(\hat{W}) \quad
    \text{subject to} \quad \|\hat{W}\|_0 \leq k</annotation></semantics>
- en: where <semantics><mrow><mi>ℒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>W</mi><mo
    accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}(\hat{W})</annotation></semantics> represents
    the model’s loss function after pruning, <semantics><mover><mi>W</mi><mo accent="true">̂</mo></mover><annotation
    encoding="application/x-tex">\hat{W}</annotation></semantics> denotes the pruned
    model’s parameters, <semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mover><mi>W</mi><mo
    accent="true">̂</mo></mover><msub><mo stretchy="false" form="postfix">∥</mo><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">\|\hat{W}\|_0</annotation></semantics> is the L0-norm
    (number of nonzero parameters), and <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    is the parameter budget constraining maximum model size.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，<semantics><mrow><mi>ℒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>W</mi><mo
    accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}(\hat{W})</annotation></semantics> 表示剪枝后的模型损失函数，<semantics><mover><mi>W</mi><mo
    accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{W}</annotation></semantics>
    表示剪枝模型的参数，<semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mover><mi>W</mi><mo
    accent="true">̂</mo></mover><msub><mo stretchy="false" form="postfix">∥</mo><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">\|\hat{W}\|_0</annotation></semantics> 是L0范数（非零参数的数量），而<semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics> 是约束最大模型大小的参数预算。
- en: The L0-norm directly measures model size by counting nonzero parameters, which
    determines memory usage and computational cost. However, L0-norm minimization
    is NP-hard, making this optimization challenging. Practical pruning algorithms
    use heuristics like magnitude-based selection, gradient-based importance, or second-order
    sensitivity to approximate solutions efficiently.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: L0范数直接通过计数非零参数来衡量模型大小，这决定了内存使用和计算成本。然而，L0范数最小化是NP难的，这使得这种优化具有挑战性。实际的剪枝算法使用基于幅度的选择、基于梯度的重要性或二阶敏感性等启发式方法来有效地近似解。
- en: 'In [Listing 10.1](ch016.xhtml#lst-pruning_example), this constraint becomes
    concrete: we reduced <semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mover><mi>W</mi><mo
    accent="true">̂</mo></mover><msub><mo stretchy="false" form="postfix">∥</mo><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">\|\hat{W}\|_0</annotation></semantics> from 9 to
    4 (satisfying <semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation
    encoding="application/x-tex">k=4</annotation></semantics>), with the magnitude
    threshold acting as our selection heuristic. Alternative formulations using L1
    or L2 norms encourage small weights but don’t guarantee exact zeros, failing to
    reduce actual memory or computation without explicit thresholding.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表10.1](ch016.xhtml#lst-pruning_example)中，这个约束变得具体：我们将<semantics><mrow><mo
    stretchy="false" form="postfix">∥</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover><msub><mo
    stretchy="false" form="postfix">∥</mo><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\|\hat{W}\|_0</annotation></semantics>从9减少到4（满足<semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation
    encoding="application/x-tex">k=4</annotation></semantics>），使用幅度阈值作为我们的选择启发式方法。使用L1或L2范数的替代公式鼓励小的权重，但不能保证精确为零，没有明确的阈值就无法减少实际的内存或计算。
- en: 'To make pruning computationally feasible, practical methods replace the hard
    constraint with a soft regularization term: <semantics><mrow><munder><mo>min</mo><mi>W</mi></munder><mi>ℒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>λ</mi><mo
    stretchy="false" form="postfix">∥</mo><mi>W</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>1</mn></msub></mrow>
    <annotation encoding="application/x-tex">\min_W \mathcal{L}(W) + \lambda \| W
    \|_1</annotation></semantics> where <semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics>
    controls sparsity degree. The <semantics><msub><mo>ℓ</mo><mn>1</mn></msub><annotation
    encoding="application/x-tex">\ell_1</annotation></semantics>-norm encourages smaller
    weight values and promotes sparsity but does not strictly enforce zero values.
    Other methods use iterative heuristics, where parameters with smallest magnitudes
    are pruned in successive steps, followed by fine-tuning to recover lost accuracy
    ([Gale, Elsen, and Hooker 2019a](ch058.xhtml#ref-gale2020sparse); [Labarge, n.d.](ch058.xhtml#ref-blalock2020state)).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使剪枝在计算上可行，实际方法用软正则化项替换了硬约束：<semantics><mrow><munder><mo>min</mo><mi>W</mi></munder><mi>ℒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>λ</mi><mo
    stretchy="false" form="postfix">∥</mo><mi>W</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>1</mn></msub></mrow>
    <annotation encoding="application/x-tex">\min_W \mathcal{L}(W) + \lambda \| W
    \|_1</annotation></semantics> 其中 <semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics>
    控制稀疏度。<semantics><msub><mo>ℓ</mo><mn>1</mn></msub><annotation encoding="application/x-tex">\ell_1</annotation></semantics>-范数鼓励较小的权重值并促进稀疏性，但并不严格强制零值。其他方法使用迭代启发式算法，在连续步骤中剪除幅度最小的参数，随后进行微调以恢复丢失的精度
    ([Gale, Elsen, and Hooker 2019a](ch058.xhtml#ref-gale2020sparse); [Labarge, n.d.](ch058.xhtml#ref-blalock2020state))。
- en: Target Structures
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 目标结构
- en: Pruning methods vary based on which structures within a neural network are removed.
    The primary targets include neurons, channels, and layers, each with distinct
    implications for the model’s architecture and performance.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝方法根据从神经网络中移除的结构而有所不同。主要目标包括神经元、通道和层，每个都对模型的架构和性能有独特的含义。
- en: '**Neuron pruning** removes entire neurons along with their associated weights
    and biases, reducing the width of a layer. This technique is often applied to
    fully connected layers.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经元剪枝**移除整个神经元及其相关的权重和偏差，从而减少层的宽度。这种技术通常应用于全连接层。'
- en: '**Channel pruning** (or filter pruning), commonly used in convolutional neural
    networks, eliminates entire channels or filters. This reduces the depth of feature
    maps, which impacts the network’s ability to extract certain features. Channel
    pruning is particularly valuable in image-processing tasks where computational
    efficiency is a priority.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通道剪枝**（或称为滤波器剪枝），在卷积神经网络中常用，它消除了整个通道或滤波器。这减少了特征图的深度，影响了网络提取某些特征的能力。通道剪枝在图像处理任务中尤其有价值，在这些任务中，计算效率是首要考虑的因素。'
- en: '**Layer pruning** removes entire layers from the network, significantly reducing
    depth. While this approach can yield significant efficiency gains, it requires
    careful balance to ensure the model retains sufficient capacity to capture complex
    patterns.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层剪枝**从网络中移除整个层，显著减少深度。虽然这种方法可以带来显著的效率提升，但它需要仔细平衡，以确保模型保留足够的容量来捕捉复杂模式。'
- en: '[Figure 10.3](ch016.xhtml#fig-channel-layer-pruning) illustrates the differences
    between channel pruning and layer pruning. When a channel is pruned, the model’s
    architecture must be adjusted to accommodate the structural change. Specifically,
    the number of input channels in subsequent layers must be modified, requiring
    alterations to the depths of the filters applied to the layer with the removed
    channel. In contrast, layer pruning removes all channels within a layer, necessitating
    more significant architectural modifications. In this case, connections between
    remaining layers must be reconfigured to bypass the removed layer. Regardless
    of the pruning approach, fine-tuning is important to adapt the remaining network
    and restore performance.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.3](ch016.xhtml#fig-channel-layer-pruning)展示了通道剪枝和层剪枝之间的差异。当剪枝一个通道时，模型的架构必须调整以适应结构变化。具体来说，后续层的输入通道数量必须修改，需要改变应用于移除通道的层的滤波器深度。相比之下，层剪枝移除层中的所有通道，需要更大的架构修改。在这种情况下，必须重新配置剩余层之间的连接以绕过移除的层。无论采用哪种剪枝方法，微调都是重要的，以适应剩余网络并恢复性能。'
- en: '![](../media/file149.svg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file149.svg)'
- en: 'Figure 10.3: **Pruning Strategies**: Channel pruning adjusts filter sizes within
    layers, while layer pruning removes entire layers and necessitates reconnection
    of remaining network components. These approaches reduce model size and computational
    cost, but require fine-tuning to mitigate performance loss due to reduced model
    capacity.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：**剪枝策略**：通道剪枝调整层内的滤波器大小，而层剪枝则移除整个层，并需要重新连接剩余的网络组件。这些方法可以减少模型大小和计算成本，但需要微调以减轻由于模型容量减少而导致的性能损失。
- en: Unstructured Pruning
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无结构剪枝
- en: Unstructured pruning removes individual weights while preserving the overall
    network architecture. During training, some connections become redundant, contributing
    little to the final computation. Pruning these weak connections reduces memory
    requirements while preserving most of the model’s accuracy.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 无结构剪枝在保留整体网络架构的同时移除单个权重。在训练过程中，一些连接变得冗余，对最终计算贡献甚微。剪除这些弱连接可以减少内存需求，同时保留模型的大部分精度。
- en: 'The mathematical foundation for unstructured pruning helps understand how sparsity
    is systematically introduced. Mathematically, unstructured pruning introduces
    sparsity into the weight matrices of a neural network. Let <semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">W \in \mathbb{R}^{m \times n}</annotation></semantics>
    represent a weight matrix in a given layer of a network. Pruning removes a subset
    of weights by applying a binary mask <semantics><mrow><mi>M</mi><mo>∈</mo><mo
    stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><msup><mo stretchy="false"
    form="postfix">}</mo><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">M \in \{0,1\}^{m \times n}</annotation></semantics>,
    yielding a pruned weight matrix: <semantics><mrow><mover><mi>W</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>M</mi><mo>⊙</mo><mi>W</mi></mrow>
    <annotation encoding="application/x-tex">\hat{W} = M \odot W</annotation></semantics>
    where <semantics><mi>⊙</mi><annotation encoding="application/x-tex">\odot</annotation></semantics>
    represents the element-wise Hadamard product. The mask <semantics><mi>M</mi><annotation
    encoding="application/x-tex">M</annotation></semantics> is constructed based on
    a pruning criterion, typically weight magnitude. A common approach is magnitude-based
    pruning, which removes a fraction <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>
    of the lowest-magnitude weights. This is achieved by defining a threshold <semantics><mi>τ</mi><annotation
    encoding="application/x-tex">\tau</annotation></semantics> such that: <semantics><mrow><msub><mi>M</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo
    stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align:
    left"><mn>1</mn><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext
    mathvariant="normal">if</mtext></mrow> <mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>W</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">|</mo></mrow><mo>></mo><mi>τ</mi></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left"><mn>0</mn><mo>,</mo></mtd><mtd columnalign="left"
    style="text-align: left"><mtext mathvariant="normal">otherwise</mtext></mtd></mtr></mtable></mrow></mrow>
    <annotation encoding="application/x-tex">M_{i,j} = \begin{cases} 1, & \text{if
    } |W_{i,j}| > \tau \\ 0, & \text{otherwise} \end{cases}</annotation></semantics>
    where <semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics>
    is chosen to ensure that only the largest <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">(1 - s)</annotation></semantics> fraction of weights
    remain. This method assumes that larger-magnitude weights contribute more to the
    network’s function, making them preferable for retention.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '无结构剪枝的数学基础有助于理解稀疏性是如何系统地引入的。从数学上讲，无结构剪枝将稀疏性引入神经网络的权重矩阵中。用 <semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">W \in \mathbb{R}^{m \times n}</annotation></semantics>
    表示网络给定层的权重矩阵。剪枝通过应用二进制掩码 <semantics><mrow><mi>M</mi><mo>∈</mo><mo stretchy="false"
    form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><msup><mo stretchy="false" form="postfix">}</mo><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">M \in \{0,1\}^{m \times n}</annotation></semantics>
    来移除权重的一个子集，从而得到剪枝后的权重矩阵：<semantics><mrow><mover><mi>W</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>M</mi><mo>⊙</mo><mi>W</mi></mrow>
    <annotation encoding="application/x-tex">\hat{W} = M \odot W</annotation></semantics>
    其中 <semantics><mi>⊙</mi><annotation encoding="application/x-tex">\odot</annotation></semantics>
    表示逐元素Hadamard积。掩码 <semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics>
    是基于剪枝标准构建的，通常是权重幅度。一种常见的方法是基于幅度的剪枝，它移除幅度最低的权重的一部分。这是通过定义一个阈值 <semantics><mi>τ</mi><annotation
    encoding="application/x-tex">\tau</annotation></semantics> 来实现的，使得：<semantics><mrow><msub><mi>M</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo
    stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align:
    left"><mn>1</mn><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext
    mathvariant="normal">if</mtext></mrow> <mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>W</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">|</mo></mrow><mo>></mo><mi>τ</mi></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left"><mn>0</mn><mo>,</mo></mtd><mtd columnalign="left"
    style="text-align: left"><mtext mathvariant="normal">otherwise</mtext></mtd></mtr></mtable></mrow></mrow>
    <annotation encoding="application/x-tex">M_{i,j} = \begin{cases} 1, & \text{if
    } |W_{i,j}| > \tau \\ 0, & \text{otherwise} \end{cases}</annotation></semantics>
    其中 <semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics>
    被选择以确保只有最大的 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>s</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(1
    - s)</annotation></semantics> 权重分数保留。这种方法假设较大的幅度权重对网络的函数贡献更多，因此它们更适合保留。'
- en: The primary advantage of unstructured pruning is memory efficiency. By reducing
    the number of nonzero parameters, pruned models require less storage, which is
    particularly beneficial when deploying models to embedded or mobile devices with
    limited memory.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化剪枝的主要优势是内存效率。通过减少非零参数的数量，剪枝模型需要更少的存储空间，这在将模型部署到内存有限的嵌入式或移动设备时尤其有益。
- en: However, unstructured pruning does not necessarily improve computational efficiency
    on modern machine learning hardware. Standard GPUs and TPUs are optimized for
    dense matrix multiplications, and a sparse weight matrix often cannot fully utilize
    hardware acceleration unless specialized sparse computation kernels are available.
    Therefore, unstructured pruning primarily benefits model storage rather than inference
    acceleration. While unstructured pruning improves model efficiency at the parameter
    level, it does not alter the structural organization of the network.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，非结构化剪枝并不一定能在现代机器学习硬件上提高计算效率。标准GPU和TPU针对密集矩阵乘法进行了优化，而稀疏权重矩阵通常无法充分利用硬件加速，除非有专门的稀疏计算内核。因此，非结构化剪枝主要有利于模型存储，而不是推理加速。虽然非结构化剪枝在参数级别上提高了模型效率，但它并没有改变网络的架构组织。
- en: Structured Pruning
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结构化剪枝
- en: While unstructured pruning removes individual weights from a neural network,
    structured pruning[15](#fn15) eliminates entire computational units, such as neurons,
    filters, channels, or layers. This approach is particularly beneficial for hardware
    efficiency, as it produces smaller dense models that can be directly mapped to
    modern machine learning accelerators. Unlike unstructured pruning, which results
    in sparse weight matrices that require specialized execution kernels to exploit
    computational benefits, structured pruning leads to more efficient inference on
    general-purpose hardware by reducing the overall size of the network architecture.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与非结构化剪枝从神经网络中移除单个权重不同，结构化剪枝[15](#fn15)移除整个计算单元，例如神经元、滤波器、通道或层。这种方法特别有利于硬件效率，因为它产生了更小的密集模型，可以直接映射到现代机器学习加速器。与需要专用执行内核来利用计算优势的非结构化剪枝不同，结构化剪枝通过减少网络架构的整体大小，在通用硬件上实现更有效的推理。
- en: Structured pruning is motivated by the observation that not all neurons, filters,
    or layers contribute equally to a model’s predictions. Some units primarily carry
    redundant or low-impact information, and removing them does not significantly
    degrade model performance. The challenge lies in identifying which structures
    can be pruned while preserving accuracy.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化剪枝的动机源于观察到的现象：并非所有神经元、滤波器或层对模型预测的贡献程度相同。一些单元主要携带冗余或低影响的信息，移除它们不会显著降低模型性能。挑战在于识别哪些结构可以被剪枝同时保持准确性。
- en: '[Figure 10.4](ch016.xhtml#fig-structured-unstructured) illustrates the key
    differences between unstructured and structured pruning. On the left, unstructured
    pruning removes individual weights (depicted as dashed connections), creating
    a sparse weight matrix. This can disrupt the original network structure, as shown
    in the fully connected network where certain connections have been randomly pruned.
    While this reduces the number of active parameters, the resulting sparsity requires
    specialized execution kernels to fully utilize computational benefits.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.4](ch016.xhtml#fig-structured-unstructured)展示了非结构化剪枝和结构化剪枝之间的关键差异。在左侧，非结构化剪枝移除单个权重（表示为虚线连接），创建了一个稀疏权重矩阵。这可能会破坏原始网络结构，如图所示，在完全连接的网络中，某些连接已被随机剪枝。虽然这减少了活跃参数的数量，但结果稀疏性需要专用执行内核才能充分利用计算优势。'
- en: In contrast, structured pruning (depicted in the middle and right sections of
    the figure) removes entire neurons or filters while preserving the network’s overall
    structure. In the middle section, a pruned fully connected network retains its
    fully connected nature but with fewer neurons. On the right, structured pruning
    is applied to a CNN by removing convolutional kernels or entire channels (dashed
    squares). This method maintains the CNN’s core convolutional operations while
    reducing the computational load, making it more compatible with hardware accelerators.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，结构化剪枝（如图中中间和右侧部分所示）在保留网络整体结构的同时移除整个神经元或滤波器。在中间部分，剪枝后的全连接网络保持了其全连接的特性，但神经元数量减少。在右侧，结构化剪枝应用于卷积神经网络（CNN），通过移除卷积核或整个通道（虚线方块）来实现。这种方法在保持CNN核心卷积操作的同时减少了计算负担，使其更兼容硬件加速器。
- en: '![](../media/file150.svg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file150.svg)'
- en: 'Figure 10.4: **Pruning Strategies**: Unstructured pruning achieves sparsity
    by removing individual weights, requiring specialized hardware for efficient computation,
    while structured pruning removes entire neurons or filters, preserving network
    structure and enabling acceleration on standard hardware. This figure contrasts
    the resulting weight matrices and network architectures from both approaches,
    highlighting the trade-offs between sparsity level and computational efficiency.
    Source: ([C. Qi et al. 2021](ch058.xhtml#ref-qi2021efficient)).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：**剪枝策略**：非结构化剪枝通过移除单个权重来实现稀疏性，需要专用硬件进行高效计算，而结构化剪枝移除整个神经元或滤波器，保留网络结构，并允许在标准硬件上加速。此图对比了两种方法产生的权重矩阵和网络架构，突出了稀疏程度和计算效率之间的权衡。来源：([C.
    Qi等2021](ch058.xhtml#ref-qi2021efficient))。
- en: A common approach to structured pruning is magnitude-based pruning, where entire
    neurons or filters are removed based on the magnitude of their associated weights.
    The intuition behind this method is that parameters with smaller magnitudes contribute
    less to the model’s output, making them prime candidates for elimination. The
    importance of a neuron or filter is often measured using a norm function, such
    as the <semantics><msub><mo>ℓ</mo><mn>1</mn></msub><annotation encoding="application/x-tex">\ell_1</annotation></semantics>-norm
    or <semantics><msub><mo>ℓ</mo><mn>2</mn></msub><annotation encoding="application/x-tex">\ell_2</annotation></semantics>-norm,
    applied to the weights associated with that unit. If the norm falls below a predefined
    threshold, the corresponding neuron or filter is pruned. This method is straightforward
    to implement and does not require additional computational overhead beyond computing
    norms across layers.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化剪枝的常见方法是基于幅度的剪枝，其中根据相关权重的幅度移除整个神经元或滤波器。这种方法背后的直觉是，幅度较小的参数对模型输出的贡献较小，因此它们是消除的理想候选者。神经元或滤波器的重要性通常使用范数函数来衡量，例如对与该单元相关的权重应用<semantics><msub><mo>ℓ</mo><mn>1</mn></msub><annotation
    encoding="application/x-tex">\ell_1</annotation></semantics>-范数或<semantics><msub><mo>ℓ</mo><mn>2</mn></msub><annotation
    encoding="application/x-tex">\ell_2</annotation></semantics>-范数。如果范数低于预定义的阈值，则相应的神经元或滤波器将被剪枝。这种方法易于实现，并且除了在层间计算范数之外，不需要额外的计算开销。
- en: Another strategy is activation-based pruning, which evaluates the average activation
    values of neurons or filters over a dataset. Neurons that consistently produce
    low activations contribute less information to the network’s decision process
    and can be safely removed. This method captures the dynamic behavior of the network
    rather than relying solely on static weight values. Activation-based pruning requires
    profiling the model over a representative dataset to estimate the average activation
    magnitudes before making pruning decisions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种策略是激活基于的剪枝，它评估神经元或滤波器在数据集上的平均激活值。持续产生低激活的神经元对网络决策过程的贡献较小，可以安全地移除。这种方法捕捉了网络的动态行为，而不是仅仅依赖于静态的权重值。激活基于的剪枝需要在代表性数据集上对模型进行配置文件分析，以在剪枝决策之前估计平均激活幅度。
- en: Gradient-based pruning uses information from the model’s training process to
    identify less significant neurons or filters. The key idea is that units with
    smaller gradient magnitudes contribute less to reducing the loss function, making
    them less important for learning. By ranking neurons based on their gradient values,
    structured pruning can remove those with the least impact on model optimization.
    Unlike magnitude-based or activation-based pruning, which rely on static properties
    of the trained model, gradient-based pruning requires access to gradient computations
    and is typically applied during training rather than as a post-processing step.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的剪枝使用模型训练过程中的信息来识别不太重要的神经元或滤波器。关键思想是，梯度幅度较小的单元对减少损失函数的贡献较小，因此对学习的重要性较低。通过根据梯度值对神经元进行排名，结构化剪枝可以移除对模型优化影响最小的那些。与基于幅度或基于激活的剪枝不同，后者依赖于训练模型的静态属性，基于梯度的剪枝需要访问梯度计算，并且通常在训练期间而不是作为后处理步骤应用。
- en: Each of these methods presents trade-offs in terms of computational complexity
    and effectiveness. Magnitude-based pruning is computationally inexpensive and
    easy to implement but does not account for how neurons behave across different
    data distributions. Activation-based pruning provides a more data-driven pruning
    approach but requires additional computations to estimate neuron importance. Gradient-based
    pruning leverages training dynamics but may introduce additional complexity if
    applied to large-scale models. The choice of method depends on the specific constraints
    of the target deployment environment and the performance requirements of the pruned
    model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法在计算复杂度和有效性方面都存在权衡。基于幅度的剪枝计算成本低，易于实现，但未考虑神经元在不同数据分布中的行为。基于激活的剪枝提供了一种更数据驱动的剪枝方法，但需要额外的计算来估计神经元的重要性。基于梯度的剪枝利用训练动态，但如果应用于大规模模型可能会引入额外的复杂性。方法的选择取决于目标部署环境的特定约束和剪枝模型的性能要求。
- en: Dynamic Pruning
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态剪枝
- en: Traditional pruning methods, whether unstructured or structured, typically involve
    static pruning, where parameters are permanently removed after training or at
    fixed intervals during training. However, this approach assumes that the importance
    of parameters is fixed, which is not always the case. In contrast, dynamic pruning
    adapts pruning decisions based on the input data or training dynamics, allowing
    the model to adjust its structure in real time.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的剪枝方法，无论是无结构的还是结构的，通常涉及静态剪枝，即在训练后或训练期间固定间隔内永久移除参数。然而，这种方法假设参数的重要性是固定的，这并不总是如此。相比之下，动态剪枝根据输入数据或训练动态调整剪枝决策，允许模型实时调整其结构。
- en: Dynamic pruning can be implemented using runtime sparsity techniques, where
    the model actively determines which parameters to utilize based on input characteristics.
    Activation-conditioned pruning exemplifies this approach by selectively deactivating
    neurons or channels that exhibit low activation values for specific inputs ([J.
    Hu et al. 2023](ch058.xhtml#ref-dynamicpruning2023)). This method introduces input-dependent
    sparsity patterns, effectively reducing the computational workload during inference
    without permanently modifying the model architecture.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 动态剪枝可以通过运行时稀疏技术实现，其中模型根据输入特征主动确定要利用哪些参数。激活条件剪枝通过选择性地停用对特定输入表现出低激活值的神经元或通道来例证这种方法（[J.
    Hu等人 2023](ch058.xhtml#ref-dynamicpruning2023)）。这种方法引入了输入相关的稀疏模式，在推理期间有效地减少了计算工作量，而不会永久修改模型架构。
- en: For instance, consider a convolutional neural network processing images with
    varying complexity. During inference of a simple image containing mostly uniform
    regions, many convolutional filters may produce negligible activations. Dynamic
    pruning identifies these low-impact filters and temporarily excludes them from
    computation, improving efficiency while maintaining accuracy for the current input.
    This adaptive behavior is particularly advantageous in latency-sensitive applications,
    where computational resources must be allocated judiciously based on input complexity,
    connecting to performance measurement strategies ([Chapter 12](ch018.xhtml#sec-benchmarking-ai)).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个处理具有不同复杂度图像的卷积神经网络。在推理主要由均匀区域组成的简单图像时，许多卷积滤波器可能产生可忽略的激活。动态剪枝识别这些低影响滤波器，并暂时将它们排除在计算之外，从而提高效率，同时保持对当前输入的准确性。这种自适应行为在延迟敏感的应用中特别有利，在这些应用中，必须根据输入复杂度合理分配计算资源，这与性能测量策略（[第12章](ch018.xhtml#sec-benchmarking-ai)）相联系。
- en: Another class of dynamic pruning operates during training, where sparsity is
    gradually introduced and adjusted throughout the optimization process. Methods
    such as gradual magnitude pruning start with a dense network and progressively
    increase the fraction of pruned parameters as training progresses. Instead of
    permanently removing parameters, these approaches allow the network to recover
    from pruning-induced capacity loss by regrowing connections that prove to be important
    in later stages of training.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类动态剪枝发生在训练过程中，其中稀疏性逐渐引入并在整个优化过程中进行调整。例如，渐进幅度剪枝从密集网络开始，随着训练的进行逐渐增加剪枝参数的比例。这些方法不是永久移除参数，而是允许网络通过在训练后期证明重要的连接重新生长来从剪枝引起的容量损失中恢复。
- en: Dynamic pruning presents several advantages over static pruning. It allows models
    to adapt to different workloads, potentially improving efficiency while maintaining
    accuracy. Unlike static pruning, which risks over-pruning and degrading performance,
    dynamic pruning provides a mechanism for selectively reactivating parameters when
    necessary. However, implementing dynamic pruning requires additional computational
    overhead, as pruning decisions must be made in real-time, either during training
    or inference. This makes it more complex to integrate into standard machine learning
    pipelines compared to static pruning, requiring sophisticated production deployment
    strategies and monitoring frameworks covered in [Chapter 13](ch019.xhtml#sec-ml-operations).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 动态剪枝相较于静态剪枝具有多个优势。它允许模型适应不同的工作负载，在保持精度的同时可能提高效率。与静态剪枝不同，静态剪枝可能会过度剪枝并降低性能，动态剪枝提供了一个机制，在必要时可以选择性重新激活参数。然而，实现动态剪枝需要额外的计算开销，因为剪枝决策必须在实时进行，无论是在训练期间还是在推理期间。这使得它比静态剪枝更难集成到标准的机器学习流程中，需要更复杂的生产部署策略和监控框架，这些内容在[第13章](ch019.xhtml#sec-ml-operations)中有详细说明。
- en: Despite its challenges, dynamic pruning is particularly useful in edge computing
    and adaptive AI systems ([Chapter 14](ch020.xhtml#sec-ondevice-learning)), where
    resource constraints and real-time efficiency requirements vary across different
    inputs. The next section explores the practical considerations and trade-offs
    involved in choosing the right pruning method for a given machine learning system.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在挑战，动态剪枝在边缘计算和自适应人工智能系统中特别有用（[第14章](ch020.xhtml#sec-ondevice-learning)），在这些系统中，不同输入的资源约束和实时效率要求各不相同。下一节将探讨在选择适合特定机器学习系统的剪枝方法时涉及的实际考虑因素和权衡。
- en: Pruning Trade-offs
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 剪枝权衡
- en: Pruning techniques offer different trade-offs in terms of memory efficiency,
    computational efficiency, accuracy retention, hardware compatibility, and implementation
    complexity. The choice of pruning strategy depends on the specific constraints
    of the machine learning system and the deployment environment, integrating with
    operational considerations ([Chapter 13](ch019.xhtml#sec-ml-operations)).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝技术在内存效率、计算效率、精度保持、硬件兼容性和实现复杂性方面提供了不同的权衡。剪枝策略的选择取决于机器学习系统的具体约束和部署环境，需要与操作考虑因素相结合（[第13章](ch019.xhtml#sec-ml-operations)）。
- en: Unstructured pruning is particularly effective in reducing model size and memory
    footprint, as it removes individual weights while keeping the overall model architecture
    intact. However, since machine learning accelerators are optimized for dense matrix
    operations, unstructured pruning does not always translate to significant computational
    speed-ups unless specialized sparse execution kernels are used.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 无结构化剪枝在减少模型大小和内存占用方面特别有效，因为它在保持整体模型架构完整的同时移除了单个权重。然而，由于机器学习加速器针对密集矩阵操作进行了优化，无结构化剪枝并不总是能转化为显著的计算速度提升，除非使用了专门的稀疏执行内核。
- en: Structured pruning, in contrast, eliminates entire neurons, channels, or layers,
    leading to a more hardware-friendly model. This technique provides direct computational
    savings, as it reduces the number of floating-point operations (FLOPs)[16](#fn16)
    required during inference.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，结构化剪枝会消除整个神经元、通道或层，从而得到一个更符合硬件的模型。这种技术提供了直接的计算节省，因为它减少了推理过程中所需的浮点运算次数（FLOPs）[16](#fn16)。
- en: The downside is that modifying the network structure can lead to a greater accuracy
    drop, requiring careful fine-tuning to recover lost performance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 不足之处在于，修改网络结构可能会导致精度下降更大，需要仔细微调以恢复丢失的性能。
- en: Dynamic pruning introduces adaptability into the pruning process by adjusting
    which parameters are pruned at runtime based on input data or training dynamics.
    This allows for a better balance between accuracy and efficiency, as the model
    retains the flexibility to reintroduce previously pruned parameters if needed.
    However, dynamic pruning increases implementation complexity, as it requires additional
    computations to determine which parameters to prune on-the-fly.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 动态剪枝通过在运行时根据输入数据或训练动态调整要剪枝的参数，将适应性引入剪枝过程。这允许在精度和效率之间取得更好的平衡，因为模型保留了在需要时重新引入先前剪枝参数的灵活性。然而，动态剪枝增加了实现复杂性，因为它需要额外的计算来确定哪些参数需要即时剪枝。
- en: '[Table 10.2](ch016.xhtml#tbl-pruning) summarizes the key structural differences
    between these pruning approaches, outlining how each method modifies the model
    and impacts its execution.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10.2](ch016.xhtml#tbl-pruning)总结了这些剪枝方法之间的关键结构差异，概述了每种方法如何修改模型及其执行的影响。'
- en: 'Table 10.2: **Pruning Strategies**: Unstructured, structured, and dynamic pruning
    each modify model weights differently, impacting both model size and computational
    efficiency; unstructured pruning offers the greatest compression but requires
    specialized hardware, while dynamic pruning adapts to input data for a balance
    between accuracy and resource usage.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.2：**剪枝策略**：无结构、结构化和动态剪枝以不同的方式修改模型权重，影响模型大小和计算效率；无结构剪枝提供最大的压缩，但需要专用硬件，而动态剪枝则根据输入数据调整以在精度和资源使用之间取得平衡。
- en: '| **Aspect** | **Unstructured Pruning** | **Structured Pruning** | **Dynamic
    Pruning** |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **无结构剪枝** | **结构化剪枝** | **动态剪枝** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **What is removed?** | Individual weights in the model | Entire neurons,
    channels, filters, or layers | Adjusts pruning based on runtime conditions |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| **移除的内容** | 模型中的单个权重 | 整个神经元、通道、滤波器或层 | 根据运行时条件调整剪枝 |'
- en: '| **Model structure** | Sparse weight matrices; original architecture remains
    unchanged | Model architecture is modified; pruned layers are fully removed |
    Structure adapts dynamically |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| **模型结构** | 稀疏权重矩阵；原始架构保持不变 | 模型架构被修改；剪枝层被完全移除 | 结构动态适应 |'
- en: '| **Impact on memory** | Reduces model storage by eliminating nonzero weights
    | Reduces model storage by removing entire components | Varies based on real-time
    pruning |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| **对内存的影响** | 通过消除非零权重来减少模型存储 | 通过移除整个组件来减少模型存储 | 根据实时剪枝而变化 |'
- en: '| **Impact on computation** | Limited; dense matrix operations still required
    unless specialized sparse computation is used | Directly reduces FLOPs and speeds
    up inference | Balances accuracy and efficiency dynamically |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| **对计算的影响** | 有限；除非使用专门的稀疏计算，否则仍需要密集矩阵运算 | 直接减少FLOPs并加快推理速度 | 动态平衡精度和效率 |'
- en: '| **Hardware compatibility** | Sparse weight matrices require specialized execution
    support for efficiency | Works efficiently with standard deep learning hardware
    | Requires adaptive inference engines |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| **硬件兼容性** | 稀疏权重矩阵需要专门的执行支持以提高效率 | 与标准深度学习硬件高效工作 | 需要自适应推理引擎 |'
- en: '| **Fine-tuning required?** | Often necessary to recover accuracy after pruning
    | More likely to require fine-tuning due to larger structural modifications |
    Adjusts dynamically, reducing the need for retraining |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| **是否需要微调** | 剪枝后通常需要微调以恢复精度 | 由于较大的结构修改，更有可能需要微调 | 动态调整，减少重新训练的需求 |'
- en: '| **Use cases** | Memory-efficient model compression, particularly for cloud
    deployment | Real-time inference optimization, mobile/edge AI, and efficient training
    | Adaptive AI applications, real-time systems |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **用例** | 内存高效的模型压缩，尤其是针对云部署 | 实时推理优化、移动/边缘AI和高效训练 | 自适应AI应用、实时系统 |'
- en: Pruning Strategies
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 剪枝策略
- en: Beyond the broad categories of unstructured, structured, and dynamic pruning,
    different pruning workflows can impact model efficiency and accuracy retention.
    Two widely used pruning strategies are iterative pruning and one-shot pruning,
    each with its own benefits and trade-offs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在无结构、结构化和动态剪枝的广泛类别之外，不同的剪枝工作流程可以影响模型效率和精度保持。两种广泛使用的剪枝策略是迭代剪枝和一次性剪枝，每种策略都有其自身的优点和权衡。
- en: Iterative Pruning
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 迭代剪枝
- en: Iterative pruning implements a gradual approach to structure removal through
    multiple cycles of pruning followed by fine-tuning. During each cycle, the algorithm
    removes a small subset of structures based on predefined importance metrics. The
    model then undergoes fine-tuning to adapt to these structural modifications before
    proceeding to the next pruning iteration. This methodical approach helps prevent
    sudden drops in accuracy while allowing the network to progressively adjust to
    reduced complexity.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代剪枝通过多次剪枝循环和后续的微调实现结构移除的渐进式方法。在每一轮中，算法基于预定义的重要性指标移除一小部分结构。然后，模型进行微调以适应这些结构修改，然后再进行下一轮剪枝迭代。这种方法有助于防止精度突然下降，同时允许网络逐步调整到降低的复杂性。
- en: To illustrate this process, consider pruning six channels from a convolutional
    neural network as shown in [Figure 10.5](ch016.xhtml#fig-iterative-pruning). Rather
    than removing all channels simultaneously, iterative pruning eliminates two channels
    per iteration over three cycles. Following each pruning step, the model undergoes
    fine-tuning to recover performance. The first iteration, which removes two channels,
    results in an accuracy decrease from 0.995 to 0.971, but subsequent fine-tuning
    restores accuracy to 0.992\. After completing two additional pruning-tuning cycles,
    the final model achieves 0.991 accuracy, which represents only a 0.4% reduction
    from the original, while operating with 27% fewer channels. By distributing structural
    modifications across multiple iterations, the network maintains its performance
    capabilities while achieving improved computational efficiency.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个过程，考虑从卷积神经网络中剪除六个通道，如图[图10.5](ch016.xhtml#fig-iterative-pruning)所示。迭代剪枝不是一次性移除所有通道，而是在三个周期内每次迭代移除两个通道。在每个剪枝步骤之后，模型都会进行微调以恢复性能。第一次迭代移除两个通道导致准确率从0.995降至0.971，但随后的微调将准确率恢复到0.992。完成两个额外的剪枝-微调周期后，最终模型达到0.991的准确率，这仅比原始模型减少了0.4%，同时运行时通道数减少了27%。通过将结构修改分布在多个迭代中，网络在实现改进的计算效率的同时保持了其性能能力。
- en: '![](../media/file151.svg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file151.svg)'
- en: 'Figure 10.5: **Iterative Pruning Performance**: Gradual channel removal with
    interleaved fine-tuning maintains high accuracy while reducing model size; this
    figure provides a 0.4% accuracy drop with a 27% reduction in channels, showcasing
    the benefits of distributing structural modifications across multiple iterations.
    This approach contrasts with one-shot pruning, which often leads to significant
    performance degradation.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：**迭代剪枝性能**：逐步移除通道并穿插微调可以在减少模型尺寸的同时保持高准确率；此图显示通道减少了27%，准确率下降了0.4%，展示了将结构修改分布在多个迭代中的好处。这种方法与单步剪枝形成对比，后者通常会导致性能显著下降。
- en: One-shot Pruning
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 单步剪枝
- en: One-shot pruning removes multiple architectural components in a single step,
    followed by an extensive fine-tuning phase to recover model accuracy. This aggressive
    approach compresses the model quickly but risks greater accuracy degradation,
    as the network must adapt to significant structural changes simultaneously.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 单步剪枝在单步中移除多个架构组件，随后进行广泛的微调阶段以恢复模型准确率。这种激进的方法可以快速压缩模型，但风险是更大的准确率下降，因为网络必须同时适应重大的结构变化。
- en: Consider applying one-shot pruning to the same network from the iterative pruning
    example. Instead of removing two channels at a time over multiple iterations,
    one-shot pruning eliminates all six channels simultaneously, as illustrated in
    [Figure 10.6](ch016.xhtml#fig-oneshot-pruning). Removing 27% of the network’s
    channels simultaneously causes the accuracy to drop significantly, from 0.995
    to 0.914\. Even after fine-tuning, the network only recovers to an accuracy of
    0.943, which is a 5% degradation from the original unpruned network. While both
    iterative and one-shot pruning ultimately produce identical network structures,
    the gradual approach of iterative pruning better preserves model performance.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑将单步剪枝应用于迭代剪枝示例中的相同网络。与每次迭代移除两个通道不同，单步剪枝同时消除所有六个通道，如图[图10.6](ch016.xhtml#fig-oneshot-pruning)所示。同时移除网络27%的通道会导致准确率显著下降，从0.995降至0.914。即使经过微调，网络也只能恢复到0.943的准确率，这比原始未剪枝网络下降了5%。虽然迭代和单步剪枝最终产生相同的网络结构，但迭代剪枝的渐进方法更好地保留了模型性能。
- en: '![](../media/file152.svg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file152.svg)'
- en: 'Figure 10.6: **One-Shot Pruning Impact**: Aggressive removal of architectural
    components, like the 27% of channels shown, causes significant initial accuracy
    loss because the network struggles to adapt to significant structural changes
    simultaneously. Fine-tuning partially recovers performance, but establishes that
    iterative pruning preserves accuracy more effectively than single-step approaches.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：**单步剪枝影响**：像图中显示的27%的通道这样的架构组件的激进移除会导致显著的初始准确率损失，因为网络难以同时适应重大的结构变化。微调部分恢复了性能，但确立了迭代剪枝比单步方法更有效地保留准确率。
- en: The choice of pruning strategy requires careful consideration of several key
    factors that influence both model efficiency and performance. The desired level
    of parameter reduction, or sparsity target, directly impacts strategy selection.
    Higher reduction targets often necessitate iterative approaches to maintain accuracy,
    while moderate sparsity goals may be achievable through simpler one-shot methods.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 选择修剪策略需要仔细考虑几个关键因素，这些因素会影响模型效率和性能。期望的参数减少水平，或稀疏度目标，直接影响策略选择。较高的减少目标通常需要迭代方法来维持准确性，而适度的稀疏度目标可能通过更简单的单次方法实现。
- en: Available computational resources significantly influence strategy choice. Iterative
    pruning demands significant resources for multiple fine-tuning cycles, whereas
    one-shot approaches require fewer resources but may sacrifice accuracy. This resource
    consideration connects to performance requirements, where applications with strict
    accuracy requirements typically benefit from gradual, iterative pruning to carefully
    preserve model capabilities. Use cases with more flexible performance constraints
    may accommodate more aggressive one-shot approaches.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的计算资源对策略选择有重大影响。迭代修剪需要为多个微调周期提供大量资源，而单次方法需要的资源较少，但可能牺牲准确性。这种资源考虑与性能要求相联系，具有严格准确性要求的应程序通常从逐步、迭代的修剪中受益，以仔细保留模型能力。具有更灵活性能约束的用例可能更适合更激进的单一方法。
- en: Development timeline also impacts pruning decisions. One-shot methods enable
    faster deployment when time is limited, though iterative approaches generally
    achieve superior results given sufficient optimization periods. Finally, target
    platform capabilities significantly influence strategy selection, as certain hardware
    architectures may better support specific sparsity patterns, making particular
    pruning approaches more advantageous for deployment.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 开发时间表也会影响修剪决策。单次方法在时间有限时能实现更快的部署，尽管迭代方法在足够的优化周期内通常能实现更好的结果。最后，目标平台的能力显著影响策略选择，因为某些硬件架构可能更好地支持特定的稀疏模式，使得特定的修剪方法在部署上更有优势。
- en: The choice between pruning strategies requires careful evaluation of project
    requirements and constraints. One-shot pruning enables rapid model compression
    by removing multiple parameters simultaneously, making it suitable for scenarios
    where deployment speed is prioritized over accuracy. However, this aggressive
    approach often results in greater performance degradation compared to more gradual
    methods. Iterative pruning, on the other hand, while computationally intensive
    and time-consuming, typically achieves superior accuracy retention through structured
    parameter reduction across multiple cycles. This methodical approach enables the
    network to adapt progressively to structural modifications, preserving important
    connections that maintain model performance. The trade-off is increased optimization
    time and computational overhead. By evaluating these factors systematically, practitioners
    can select a pruning approach that optimally balances efficiency gains with model
    performance for their specific use case.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在修剪策略之间进行选择需要仔细评估项目需求和约束。单次修剪可以通过同时移除多个参数来实现快速模型压缩，这使得它在部署速度比准确性更受重视的场合适用。然而，这种激进的方法通常会导致与更渐进的方法相比性能下降更大。另一方面，迭代修剪虽然计算密集且耗时，但通常通过在多个周期中结构化参数减少，通常能实现更高的准确性保持。这种方法使网络能够逐步适应结构变化，保留维持模型性能的重要连接。这种权衡是增加了优化时间和计算开销。通过系统地评估这些因素，从业者可以选择一种修剪方法，以最佳方式平衡效率提升与特定用例的模型性能。
- en: Lottery Ticket Hypothesis
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 彩票假设
- en: Pruning is widely used to reduce the size and computational cost of neural networks,
    but the process of determining which parameters to remove is not always straightforward.
    While traditional pruning methods eliminate weights based on magnitude, structure,
    or dynamic conditions, recent research suggests that pruning is not just about
    reducing redundancy; it may also reveal inherently efficient subnetworks that
    exist within the original model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪技术被广泛用于减少神经网络的大小和计算成本，但在确定要移除哪些参数的过程中并不总是直接的。虽然传统的修剪方法基于权重的大小、结构或动态条件来消除权重，但最近的研究表明，修剪不仅仅是减少冗余；它还可能揭示存在于原始模型中的本质上高效的子网络。
- en: This perspective leads to the Lottery Ticket Hypothesis[17](#fn17) (LTH), which
    challenges conventional pruning workflows by proposing that within large neural
    networks, there exist small, well-initialized subnetworks, referred to as ‘winning
    tickets’, that can achieve comparable accuracy to the full model when trained
    in isolation. Rather than viewing pruning as just a post-training compression
    step, LTH suggests it can serve as a discovery mechanism to identify these efficient
    subnetworks early in training.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这种观点导致了彩票假设[17](#fn17)（LTH），它通过提出在大型神经网络中存在小型、良好初始化的子网络，称为“中奖彩票”，当独立训练时可以达到与完整模型相当准确度，从而挑战了传统的剪枝工作流程。LTH不是将剪枝仅仅视为训练后的压缩步骤，而是建议它可以作为一种发现机制，在训练早期就识别这些高效的子网络。
- en: LTH is validated through an iterative pruning process, illustrated in [Figure 10.7](ch016.xhtml#fig-winning-ticket).
    A large network is first trained to convergence. The lowest-magnitude weights
    are then pruned, and the remaining weights are reset to their original initialization
    rather than being re-randomized. This process is repeated iteratively, gradually
    reducing the network’s size while preserving performance. After multiple iterations,
    the remaining subnetwork, referred to as the ‘winning ticket’, proves capable
    of training to the same or higher accuracy as the original full model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: LTH通过迭代剪枝过程得到验证，如图10.7[图10.7](ch016.xhtml#fig-winning-ticket)所示。首先训练一个大型网络到收敛。然后剪除最低幅度的权重，并将剩余的权重重置到它们的原始初始化状态，而不是重新随机化。这个过程迭代进行，逐渐减小网络的大小同时保持性能。经过多次迭代后，剩余的子网络，被称为“中奖彩票”，证明能够训练到与原始完整模型相同或更高的准确度。
- en: '![](../media/file153.svg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file153.svg)'
- en: 'Figure 10.7: **Winning Ticket Discovery**: Iterative pruning and weight resetting
    identify subnetworks within larger models that, when trained in isolation, achieve
    comparable or superior accuracy, challenging the conventional view of pruning
    as solely a compression technique. This process establishes that well-initialized
    subnetworks exist and can be efficiently trained, suggesting that much of a large
    network’s capacity may be redundant.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7：**中奖彩票发现**：迭代剪枝和权重重置识别出在大模型内部的子网络，当独立训练时，可以达到相当或更高的准确度，挑战了将剪枝视为仅仅是压缩技术的传统观点。这个过程证明了良好初始化的子网络存在，并且可以高效地训练，暗示大型网络的大部分容量可能是冗余的。
- en: The implications of the Lottery Ticket Hypothesis extend beyond conventional
    pruning techniques. Instead of training large models and pruning them later, LTH
    suggests that compact, high-performing subnetworks could be trained directly from
    the start, eliminating the need for overparameterization. This insight challenges
    the traditional assumption that model size is necessary for effective learning.
    It also emphasizes the importance of initialization, as winning tickets only retain
    their performance when reset to their original weight values. This finding raises
    deeper questions about the role of initialization in shaping a network’s learning
    trajectory.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 彩票假设的启示超出了传统的剪枝技术。LTH建议，而不是训练大型模型然后在后面剪枝，可以直接从开始训练紧凑、高性能的子网络，从而消除对过度参数化的需求。这一见解挑战了传统的假设，即模型大小对于有效学习是必要的。它还强调了初始化的重要性，因为只有在将中奖彩票重置到其原始权重值时，它们才能保持其性能。这一发现引发了关于初始化在塑造网络学习轨迹中作用的更深层次问题。
- en: The hypothesis further reinforces the effectiveness of iterative pruning over
    one-shot pruning. Gradually refining the model structure allows the network to
    adapt at each stage, preserving accuracy more effectively than removing large
    portions of the model in a single step. This process aligns well with practical
    pruning strategies used in deployment, where preserving accuracy while reducing
    computation is important.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 该假设进一步强化了迭代剪枝相对于一次性剪枝的有效性。逐渐细化模型结构允许网络在每个阶段进行适应，比一次性移除模型的大部分部分更有效地保留准确度。这个过程与实际部署中使用的剪枝策略非常吻合，在部署中，在减少计算的同时保持准确度非常重要。
- en: Despite its promise, applying LTH in practice remains computationally expensive,
    as identifying winning tickets requires multiple cycles of pruning and retraining.
    Ongoing research explores whether winning subnetworks can be detected early without
    full training, potentially leading to more efficient sparse training techniques.
    If such methods become practical, LTH could corely reshape how machine learning
    models are trained, shifting the focus from pruning large networks after training
    to discovering and training only the important components from the beginning.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前景广阔，但在实践中应用 LTH 仍然计算成本高昂，因为识别获胜的子网络需要多次剪枝和重新训练的周期。正在进行的研究探索是否可以在不进行完整训练的情况下早期检测到获胜的子网络，这可能导致更有效的稀疏训练技术。如果此类方法变得实用，LTH
    将从根本上改变机器学习模型的训练方式，将重点从训练后剪枝大型网络转移到从一开始就发现和训练重要组件。
- en: While LTH presents a compelling theoretical perspective on pruning, practical
    implementations rely on established framework-level tools to integrate structured
    and unstructured pruning techniques.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 LTH 在剪枝方面提供了一个有说服力的理论视角，但实际实现依赖于已建立的框架级工具来集成结构和无结构剪枝技术。
- en: Pruning Practice
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 剪枝实践
- en: Several machine learning frameworks provide built-in tools to apply structured
    and unstructured pruning, fine-tune pruned models, and optimize deployment for
    cloud, edge, and mobile environments.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 几个机器学习框架提供了内置工具来应用结构和无结构剪枝，微调剪枝模型，并优化云、边缘和移动环境中的部署。
- en: Machine learning frameworks such as PyTorch, TensorFlow, and ONNX offer dedicated
    pruning utilities that allow practitioners to efficiently implement these techniques
    while ensuring compatibility with deployment hardware.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如 PyTorch、TensorFlow 和 ONNX 等机器学习框架提供专门的剪枝实用工具，允许从业者高效地实现这些技术，同时确保与部署硬件的兼容性。
- en: In PyTorch, pruning is available through the `torch.nn.utils.prune` module,
    which provides functions to apply magnitude-based pruning to individual layers
    or the entire model. Users can perform unstructured pruning by setting a fraction
    of the smallest-magnitude weights to zero or apply structured pruning to remove
    entire neurons or filters. PyTorch also allows for custom pruning strategies,
    where users define pruning criteria beyond weight magnitude, such as activation-based
    or gradient-based pruning. Once a model is pruned, it can be fine-tuned to recover
    lost accuracy before being exported for inference.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，剪枝可以通过 `torch.nn.utils.prune` 模块实现，该模块提供了将基于幅度的剪枝应用于单个层或整个模型的功能。用户可以通过将最小幅度的权重设置为零来执行无结构剪枝，或者应用结构化剪枝以移除整个神经元或滤波器。PyTorch
    还允许自定义剪枝策略，用户可以定义超出权重幅度的剪枝标准，例如基于激活或梯度的剪枝。一旦模型被剪枝，就可以进行微调以恢复丢失的精度，然后再导出用于推理。
- en: TensorFlow provides pruning support through the TensorFlow Model Optimization
    Toolkit (TF-MOT). This toolkit integrates pruning directly into the training process
    by applying sparsity-inducing regularization. TensorFlow’s pruning API supports
    global and layer-wise pruning, dynamically selecting parameters for removal based
    on weight magnitudes. Unlike PyTorch, TensorFlow’s pruning is typically applied
    during training, allowing models to learn sparse representations from the start
    rather than pruning them post-training. TF-MOT also provides export tools to convert
    pruned models into TFLite format, making them compatible with mobile and edge
    devices.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 通过 TensorFlow 模型优化工具包（TF-MOT）提供剪枝支持。此工具包通过应用稀疏性诱导正则化直接将剪枝集成到训练过程中。TensorFlow
    的剪枝 API 支持全局和层级剪枝，根据权重幅度动态选择要移除的参数。与 PyTorch 不同，TensorFlow 的剪枝通常在训练期间应用，允许模型从一开始就学习稀疏表示，而不是在训练后进行剪枝。TF-MOT
    还提供导出工具，将剪枝模型转换为 TFLite 格式，使其与移动和边缘设备兼容。
- en: ONNX[18](#fn18), an open standard for model representation, does not implement
    pruning directly but provides export and compatibility support for pruned models
    from PyTorch and TensorFlow. Since ONNX is designed to be hardware-agnostic, it
    allows models that have undergone pruning in different frameworks to be optimized
    for inference engines such as TensorRT[19](#fn19), OpenVINO, and EdgeTPU. These
    inference engines can further leverage structured and dynamic pruning for execution
    efficiency, particularly on specialized hardware accelerators.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX[18](#fn18)，一种用于模型表示的开源标准，不直接实现剪枝，但为从PyTorch和TensorFlow剪枝的模型提供导出和兼容性支持。由于ONNX旨在实现硬件无关性，它允许在不同框架中经过剪枝的模型针对TensorRT[19](#fn19)、OpenVINO和EdgeTPU等推理引擎进行优化。这些推理引擎可以进一步利用结构化和动态剪枝来提高执行效率，尤其是在专用硬件加速器上。
- en: Although framework-level support for pruning has advanced significantly, applying
    pruning in practice requires careful consideration of hardware compatibility and
    software optimizations. Standard CPUs and GPUs often do not natively accelerate
    sparse matrix operations, meaning that unstructured pruning may reduce memory
    usage without providing significant computational speed-ups. In contrast, structured
    pruning is more widely supported in inference engines, as it directly reduces
    the number of computations needed during execution. Dynamic pruning, when properly
    integrated with inference engines, can optimize execution based on workload variations
    and hardware constraints, making it particularly beneficial for adaptive AI applications.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在框架层面上的剪枝支持已经取得了显著进展，但在实际应用中实施剪枝需要仔细考虑硬件兼容性和软件优化。标准CPU和GPU通常不原生加速稀疏矩阵运算，这意味着无结构剪枝可能减少内存使用，但不会提供显著的计算速度提升。相比之下，结构化剪枝在推理引擎中支持更广泛，因为它直接减少了执行过程中所需的计算量。动态剪枝，当与推理引擎正确集成时，可以根据工作负载变化和硬件约束优化执行，对自适应人工智能应用尤其有益。
- en: At a practical level, choosing the right pruning strategy depends on several
    key trade-offs, including memory efficiency, computational performance, accuracy
    retention, and implementation complexity. These trade-offs impact how pruning
    methods are applied in real-world machine learning workflows, influencing deployment
    choices based on resource constraints and system requirements.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际层面，选择合适的剪枝策略取决于几个关键权衡，包括内存效率、计算性能、准确度保持和实现复杂性。这些权衡影响剪枝方法在实际机器学习工作流程中的应用，影响基于资源约束和系统要求的部署选择。
- en: To help guide these decisions, [Table 10.3](ch016.xhtml#tbl-pruning-tradeoffs)
    provides a high-level comparison of these trade-offs, summarizing the key efficiency
    and usability factors that practitioners must consider when selecting a pruning
    method.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助指导这些决策，[表10.3](ch016.xhtml#tbl-pruning-tradeoffs)提供了这些权衡的高级比较，总结了实践者在选择剪枝方法时必须考虑的关键效率和可用性因素。
- en: These trade-offs underscore the importance of aligning pruning methods with
    practical deployment needs. Frameworks such as PyTorch, TensorFlow, and ONNX enable
    developers to implement these strategies, but the effectiveness of a pruning approach
    depends on the underlying hardware and application requirements.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权衡突出了将剪枝方法与实际部署需求对齐的重要性。例如，PyTorch、TensorFlow 和 ONNX 等框架使开发者能够实施这些策略，但剪枝方法的有效性取决于底层硬件和应用需求。
- en: 'Table 10.3: **Pruning Trade-Offs**: Different pruning strategies balance memory
    efficiency, computational speed, accuracy retention, and hardware compatibility,
    impacting practical model deployment choices and system performance. Unstructured
    pruning offers high memory savings but requires specialized hardware, while structured
    pruning prioritizes computational efficiency at the cost of reduced accuracy.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.3：**剪枝权衡**：不同的剪枝策略在内存效率、计算速度、准确度保持和硬件兼容性之间取得平衡，影响实际模型部署选择和系统性能。无结构剪枝提供高内存节省，但需要专用硬件，而结构化剪枝以降低准确度为代价，优先考虑计算效率。
- en: '| **Criterion** | **Unstructured Pruning** | **Structured Pruning** | **Dynamic
    Pruning** |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| **标准** | **无结构剪枝** | **结构化剪枝** | **动态剪枝** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Memory Efficiency** | ↑↑ High | ↑ Moderate | ↑ Moderate |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **内存效率** | ↑↑ 高 | ↑ 中等 | ↑ 中等 |'
- en: '| **Computational Efficiency** | → Neutral | ↑↑ High | ↑ High |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| **计算效率** | → 中性 | ↑↑ 高 | ↑ 高 |'
- en: '| **Accuracy Retention** | ↑ Moderate | ↓↓ Low | ↑↑ High |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| **准确度保持** | ↑ 中等 | ↓↓ 低 | ↑↑ 高 |'
- en: '| **Hardware Compatibility** | ↓ Low | ↑↑ High | → Neutral |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| **硬件兼容性** | ↓ 低 | ↑↑ 高 | → 中立 |'
- en: '| **Implementation Complexity** | → Neutral | ↑ Moderate | ↓↓ High |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **实现复杂性** | → 中立 | ↑ 中等 | ↓↓ 高 |'
- en: For example, structured pruning is commonly used in mobile and edge applications
    because of its compatibility with standard inference engines, whereas dynamic
    pruning is better suited for adaptive AI workloads that need to adjust sparsity
    levels on the fly. Unstructured pruning, while useful for reducing memory footprints,
    requires specialized sparse execution kernels to fully realize computational savings.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，结构化剪枝由于其与标准推理引擎的兼容性，常用于移动和边缘应用，而动态剪枝则更适合需要动态调整稀疏度级别的自适应人工智能工作负载。虽然非结构化剪枝对于减少内存占用很有用，但需要专门的稀疏执行内核才能完全实现计算节省。
- en: Understanding these trade-offs is important when deploying pruned models in
    real-world settings. Several high-profile models have successfully integrated
    pruning to optimize performance. MobileNet, a lightweight convolutional neural
    network designed for mobile and embedded applications, has been pruned to reduce
    inference latency while preserving accuracy ([A. G. Howard et al. 2017](ch058.xhtml#ref-howard2017mobilenets)).
    BERT[20](#fn20), a widely used transformer model for natural language processing,
    has undergone structured pruning of attention heads and intermediate layers to
    create efficient versions such as DistilBERT[21](#fn21) and TinyBERT, which retain
    much of the original performance while reducing computational overhead ([Sanh
    et al. 2019](ch058.xhtml#ref-sanh2019distilbert)). In computer vision, EfficientNet[22](#fn22)
    has been pruned to remove unnecessary filters, optimizing it for deployment in
    resource-constrained environments ([Tan and Le 2019a](ch058.xhtml#ref-tan2019efficientnet)).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际环境中部署剪枝模型时，理解这些权衡很重要。一些知名模型已经成功地将剪枝集成到优化性能中。MobileNet，一种为移动和嵌入式应用设计的轻量级卷积神经网络，已被剪枝以减少推理延迟，同时保持准确性（[A.
    G. Howard 等人 2017](ch058.xhtml#ref-howard2017mobilenets)）。BERT[20](#fn20)，一种广泛使用的自然语言处理变压器模型，已经经历了注意力头和中间层的结构化剪枝，创建了如DistilBERT[21](#fn21)和TinyBERT等高效的版本，这些版本在减少计算开销的同时保留了大部分原始性能（[Sanh
    等人 2019](ch058.xhtml#ref-sanh2019distilbert)）。在计算机视觉领域，EfficientNet[22](#fn22)已被剪枝以去除不必要的滤波器，优化其在资源受限环境中的部署（[Tan
    和 Le 2019a](ch058.xhtml#ref-tan2019efficientnet)）。
- en: Knowledge Distillation
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: Imagine a world-class professor (the teacher model) who has read thousands of
    books and has a deep, nuanced understanding of a subject. Now, imagine a bright
    student (the student model) who needs to learn the subject quickly. Instead of
    just giving the student the textbook answers (the hard labels), the professor
    provides rich explanations, pointing out why one answer is better than another
    and how different concepts relate (the soft labels). The student learns much more
    effectively from this rich guidance than from the textbook alone. This is the
    essence of knowledge distillation.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一位世界级的教授（教师模型），他阅读了数千本书，对某一学科有着深刻而细腻的理解。现在，想象一位聪明的学生（学生模型），他需要快速学习这门学科。教师不是仅仅给学生提供教科书上的答案（硬标签），而是提供丰富的解释，指出为什么一个答案比另一个答案更好，以及不同概念之间的关系（软标签）。学生从这种丰富的指导中比从教科书本身学到的更多。这就是知识蒸馏的精髓。
- en: 'Knowledge distillation trains a smaller student model using guidance from a
    larger pre-trained teacher, learning from the teacher’s rich output distributions
    rather than simple correct/incorrect labels. This distinction matters because
    teacher models provide richer learning signals than ground-truth labels. Consider
    image classification: a ground-truth label might say “this is a dog” (one-hot
    encoding: [0, 1, 0, 0, …]). But a trained teacher model might output [0.02, 0.85,
    0.08, 0.05, …], revealing that while “dog” is most likely, the image shares some
    features with “wolf” (0.08) and “fox” (0.05). This inter-class similarity information
    helps the student learn feature relationships that hard labels cannot convey.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏通过从更大的预训练教师模型中获取指导来训练一个较小的学生模型，学习教师丰富的输出分布，而不是简单的正确/错误标签。这种区别很重要，因为教师模型提供的比真实标签更丰富的学习信号。以图像分类为例：一个真实标签可能说“这是一只狗”（独热编码：[0,
    1, 0, 0, …]）。但一个训练好的教师模型可能会输出[0.02, 0.85, 0.08, 0.05, …]，表明“狗”是最可能的，但图像与“狼”（0.08）和“狐狸”（0.05）有一些共同的特征。这种类间相似性信息有助于学生学习特征关系，这是硬标签无法传达的。
- en: Knowledge distillation differs from pruning. While pruning removes parameters
    from an existing model, distillation trains a separate, smaller architecture using
    guidance from a larger pre-trained teacher ([Gou et al. 2021](ch058.xhtml#ref-gou2021knowledge)).
    The student model optimizes to match the teacher’s soft predictions (probability
    distributions over classes) rather than simply learning from labeled data ([Jiong
    Lin et al. 2020](ch058.xhtml#ref-tang2020understanding)).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏与剪枝不同。虽然剪枝从现有模型中移除参数，但蒸馏使用来自较大预训练教师模型的指导来训练一个单独的、较小的架构 ([Gou等人 2021](ch058.xhtml#ref-gou2021knowledge))。学生模型优化以匹配教师的软预测（类别的概率分布）而不是简单地从标记数据中学习
    ([Jiong Lin等人 2020](ch058.xhtml#ref-tang2020understanding))。
- en: '[Figure 10.8](ch016.xhtml#fig-kd-overview) illustrates the distillation process.
    The teacher model produces probability distributions using a softened softmax
    function with temperature <semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>,
    and the student model trains using both these soft targets and ground truth labels.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.8](ch016.xhtml#fig-kd-overview)展示了蒸馏过程。教师模型使用带有温度<semantics><mi>T</mi><annotation
    encoding="application/x-tex">T</annotation></semantics>的软化的softmax函数生成概率分布，学生模型使用这两个软目标和真实标签进行训练。'
- en: '![](../media/file154.svg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file154.svg)'
- en: 'Figure 10.8: **Knowledge Distillation**: A student model learns from the softened
    probability distributions generated by a pre-trained teacher model, transferring
    knowledge beyond hard labels. This process enables the student to achieve comparable
    performance to the teacher with fewer parameters by using the teacher’s generalization
    capabilities and inter-class relationships.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8：**知识蒸馏**：学生模型从预训练教师模型生成的软概率分布中学习，将知识传递到硬标签之外。这个过程使学生模型能够通过使用教师模型的泛化能力和类间关系，以更少的参数达到与教师模型相当的性能。
- en: 'The training process for the student model incorporates two loss terms:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 学生模型的训练过程包含两个损失项：
- en: '**Distillation loss**: A loss function (often based on Kullback-Leibler (KL)
    divergence[23](#fn23)) that minimizes the difference between the student’s and
    teacher’s soft label distributions.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蒸馏损失**：一个损失函数（通常基于Kullback-Leibler (KL) 散度[23](#fn23)），它最小化学生模型和教师模型的软标签分布之间的差异。'
- en: '**Student loss**: A standard cross-entropy loss that ensures the student model
    correctly classifies the hard labels.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学生损失**：一个标准的交叉熵损失，确保学生模型正确分类硬标签。'
- en: The combination of these two loss functions enables the student model to absorb
    both structured knowledge from the teacher and label supervision from the dataset.
    This approach allows smaller models to reach accuracy levels close to their larger
    teacher models, making knowledge distillation a key technique for model compression
    and efficient deployment.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个损失函数的组合使学生模型能够吸收来自教师模型的有序知识以及来自数据集的标签监督。这种方法允许较小的模型达到接近其较大教师模型的准确度水平，使知识蒸馏成为模型压缩和高效部署的关键技术。
- en: Knowledge distillation allows smaller models to reach a level of accuracy that
    would be difficult to achieve through standard training alone. This makes it particularly
    useful in ML systems where inference efficiency is a priority, such as real-time
    applications, cloud-to-edge model compression, and low-power AI systems ([S. Sun
    et al. 2019](ch058.xhtml#ref-sun2019patient)).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏允许较小的模型达到通过标准训练单独实现难以达到的准确度水平。这使得它在以推理效率为优先的机器学习系统中特别有用，例如实时应用、云到边缘模型压缩和低功耗AI系统
    ([S. Sun等人 2019](ch058.xhtml#ref-sun2019patient))。
- en: Distillation Theory
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 蒸馏理论
- en: Knowledge distillation is based on the idea that a well-trained teacher model
    encodes more information about the data distribution than just the correct class
    labels. In conventional supervised learning, a model is trained to minimize the
    cross-entropy loss[24](#fn24) between its predictions and the ground truth labels.
    However, this approach only provides a hard decision boundary for each class,
    discarding potentially useful information about how the model relates different
    classes to one another ([G. Hinton, Vinyals, and Dean 2015](ch058.xhtml#ref-hinton2015distilling)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏基于这样一个观点：一个训练良好的教师模型编码了关于数据分布的信息，而不仅仅是正确的类标签。在传统的监督学习中，模型被训练以最小化其预测与真实标签之间的交叉熵损失[24](#fn24)。然而，这种方法只为每个类提供了一个硬决策边界，丢弃了关于模型如何将不同类相互关联的潜在有用信息
    ([G. Hinton, Vinyals, and Dean 2015](ch058.xhtml#ref-hinton2015distilling))。
- en: Knowledge distillation addresses this limitation by transferring additional
    information through the soft probability distributions produced by the teacher
    model. Instead of training the student model to match only the correct label,
    it is trained to match the teacher’s full probability distribution over all possible
    classes. This is achieved by introducing a temperature-scaled softmax function,
    which smooths the probability distribution, making it easier for the student model
    to learn from the teacher’s outputs ([Gou et al. 2021](ch058.xhtml#ref-gou2021knowledge)).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏通过通过教师模型产生的软概率分布来传递额外的信息，解决了这一限制。它不是训练学生模型仅匹配正确的标签，而是训练学生模型匹配教师对所有可能类别的完整概率分布。这是通过引入一个温度缩放的softmax函数来实现的，该函数平滑了概率分布，使得学生模型更容易从教师输出中学习([Gou
    et al. 2021](ch058.xhtml#ref-gou2021knowledge)))。
- en: Distillation Mathematics
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 知识蒸馏数学
- en: 'To formalize this temperature-based approach, let <semantics><msub><mi>z</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">z_i</annotation></semantics> be the logits (pre-softmax
    outputs) of the model for class <semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics>.
    The standard softmax function computes class probabilities as: <semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mo>exp</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><mo>exp</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow></mfrac></mrow> <annotation encoding="application/x-tex">p_i
    = \frac{\exp(z_i)}{\sum_j \exp(z_j)}</annotation></semantics> where higher logits
    correspond to higher confidence in a class prediction.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这种基于温度的方法形式化，设<semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics>为模型对类别<semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics>的logits（预softmax输出）。标准的softmax函数计算类别概率如下：<semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mo>exp</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><mo>exp</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow></mfrac></mrow> <annotation encoding="application/x-tex">p_i
    = \frac{\exp(z_i)}{\sum_j \exp(z_j)}</annotation></semantics>其中，更高的logits对应于对类别预测更高的置信度。
- en: 'In knowledge distillation, we introduce a temperature parameter[25](#fn25)
    <semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>
    that scales the logits before applying softmax: <semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo>exp</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mi>/</mi><mi>T</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><mo>exp</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>j</mi></msub><mi>/</mi><mi>T</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow> <annotation
    encoding="application/x-tex">p_i(T) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}</annotation></semantics>
    where a higher temperature produces a softer probability distribution, revealing
    more information about how the model distributes uncertainty across different
    classes.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在知识蒸馏中，我们引入一个温度参数[25](#fn25) <semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>，该参数在应用softmax之前对logits进行缩放：<semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo>exp</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mi>/</mi><mi>T</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><mo>exp</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>j</mi></msub><mi>/</mi><mi>T</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow> <annotation
    encoding="application/x-tex">p_i(T) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}</annotation></semantics>其中，更高的温度会产生更软的概率分布，揭示了模型如何在不同类别间分配不确定性的更多信息。
- en: 'The student model is then trained using a loss function that minimizes the
    difference between its output distribution and the teacher’s softened output distribution.
    The most common formulation combines two loss terms: <semantics><mrow><msub><mi>ℒ</mi><mtext
    mathvariant="normal">distill</mtext></msub><mo>=</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>ℒ</mi><mtext
    mathvariant="normal">CE</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>s</mi></msub><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>α</mi><msup><mi>T</mi><mn>2</mn></msup><munder><mo>∑</mo><mi>i</mi></munder><msubsup><mi>p</mi><mi>i</mi><mi>T</mi></msubsup><mo>log</mo><msubsup><mi>p</mi><mrow><mi>i</mi><mo>,</mo><mi>s</mi></mrow><mi>T</mi></msubsup></mrow>
    <annotation encoding="application/x-tex">\mathcal{L}_{\text{distill}} = (1 - \alpha)
    \mathcal{L}_{\text{CE}}(y_s, y) + \alpha T^2 \sum_i p_i^T \log p_{i, s}^T</annotation></semantics>
    where:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用一个损失函数来训练学生模型，该损失函数最小化其输出分布与教师软输出分布之间的差异。最常见的形式结合了两个损失项：<semantics><mrow><msub><mi>ℒ</mi><mtext
    mathvariant="normal">distill</mtext></msub><mo>=</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>ℒ</mi><mtext
    mathvariant="normal">CE</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>s</mi></msub><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>α</mi><msup><mi>T</mi><mn>2</mn></msup><munder><mo>∑</mo><mi>i</mi></munder><msubsup><mi>p</mi><mi>i</mi><mi>T</mi></msubsup><mo>log</mo><msubsup><mi>p</mi><mrow><mi>i</mi><mo>,</mo><mi>s</mi></mrow><mi>T</mi></msubsup></mrow>
    <annotation encoding="application/x-tex">\mathcal{L}_{\text{distill}} = (1 - \alpha)
    \mathcal{L}_{\text{CE}}(y_s, y) + \alpha T^2 \sum_i p_i^T \log p_{i, s}^T</annotation></semantics>
    其中：
- en: <semantics><mrow><msub><mi>ℒ</mi><mtext mathvariant="normal">CE</mtext></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>s</mi></msub><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{CE}}(y_s,
    y)</annotation></semantics> is the standard cross-entropy loss between the student’s
    predictions <semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding="application/x-tex">y_s</annotation></semantics>
    and the ground truth labels <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>ℒ</mi><mtext mathvariant="normal">CE</mtext></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>s</mi></msub><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{CE}}(y_s,
    y)</annotation></semantics> 是学生预测 <semantics><msub><mi>y</mi><mi>s</mi></msub><annotation
    encoding="application/x-tex">y_s</annotation></semantics> 和真实标签 <semantics><mi>y</mi><annotation
    encoding="application/x-tex">y</annotation></semantics> 之间的标准交叉熵损失。
- en: The second term minimizes the Kullback-Leibler (KL) divergence between the teacher’s
    softened predictions <semantics><msubsup><mi>p</mi><mi>i</mi><mi>T</mi></msubsup><annotation
    encoding="application/x-tex">p_i^T</annotation></semantics> and the student’s
    predictions <semantics><msubsup><mi>p</mi><mrow><mi>i</mi><mo>,</mo><mi>s</mi></mrow><mi>T</mi></msubsup><annotation
    encoding="application/x-tex">p_{i, s}^T</annotation></semantics>.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二项最小化了教师软预测 <semantics><msubsup><mi>p</mi><mi>i</mi><mi>T</mi></msubsup><annotation
    encoding="application/x-tex">p_i^T</annotation></semantics> 和学生预测 <semantics><msubsup><mi>p</mi><mrow><mi>i</mi><mo>,</mo><mi>s</mi></mrow><mi>T</mi></msubsup><annotation
    encoding="application/x-tex">p_{i, s}^T</annotation></semantics> 之间的 Kullback-Leibler
    (KL) 散度。
- en: The factor <semantics><msup><mi>T</mi><mn>2</mn></msup><annotation encoding="application/x-tex">T^2</annotation></semantics>
    ensures that gradients remain appropriately scaled when using high-temperature
    values.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子 <semantics><msup><mi>T</mi><mn>2</mn></msup><annotation encoding="application/x-tex">T^2</annotation></semantics>
    确保在采用高温值时梯度保持适当的缩放。
- en: The hyperparameter <semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>
    balances the importance of the standard training loss versus the distillation
    loss.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数 <semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>
    平衡了标准训练损失与蒸馏损失的重要性。
- en: By learning from both hard labels and soft teacher outputs, the student model
    benefits from the generalization power of the teacher, improving its ability to
    distinguish between similar classes even with fewer parameters.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从硬标签和软教师输出中学习，学生模型从教师的泛化能力中受益，即使参数较少，也能提高其区分相似类别的能力。
- en: Distillation Intuition
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 蒸馏直觉
- en: By learning from both hard labels and soft teacher outputs, the student model
    benefits from the generalization power of the teacher, improving its ability to
    distinguish between similar classes even with fewer parameters. Unlike conventional
    training, where a model learns only from binary correctness signals, knowledge
    distillation allows the student to absorb a richer understanding of the data distribution
    from the teacher’s predictions.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从硬标签和软教师输出中学习，学生模型从教师的泛化能力中受益，即使参数较少，也能提高其区分相似类别的能力。与传统的训练不同，传统的训练中模型只从二元正确性信号中学习，知识蒸馏允许学生从教师的预测中吸收对数据分布的更丰富理解。
- en: A key advantage of soft targets is that they provide relative confidence levels
    rather than just a single correct answer. Consider an image classification task
    where the goal is to distinguish between different animal species. A standard
    model trained with hard labels will only receive feedback on whether its prediction
    is right or wrong. If an image contains a cat, the correct label is “cat,” and
    all other categories, such as “dog” and “fox,” are treated as equally incorrect.
    However, a well-trained teacher model naturally understands that a cat is more
    visually similar to a dog than to a fox, and its soft output probabilities might
    look like [Figure 10.9](ch016.xhtml#fig-targets), where the relative confidence
    levels indicate that while “cat” is the most likely category, “dog” is still a
    plausible alternative, whereas “fox” is much less likely.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 软目标的一个关键优势是它们提供相对置信水平，而不仅仅是单个正确答案。考虑一个图像分类任务，其目标是区分不同的动物种类。使用硬标签训练的标准模型只会收到关于其预测是否正确的反馈。如果一个图像包含猫，正确的标签是“猫”，而所有其他类别，如“狗”和“狐狸”，都被视为同样不正确。然而，一个训练良好的教师模型自然理解猫在视觉上比狐狸更接近狗，其软输出概率可能看起来像[图10.9](ch016.xhtml#fig-targets)，其中相对置信水平表明虽然“猫”是最可能的类别，但“狗”仍然是一个合理的替代品，而“狐狸”则不太可能。
- en: '![](../media/file155.svg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file155.svg)'
- en: 'Figure 10.9: **Soft Target Distribution**: Relative confidence levels indicate
    which classes are more likely for a given input, showing that a model can express
    uncertainty and provide nuanced outputs beyond simple correct or incorrect labels.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：**软目标分布**：相对置信水平表明对于给定的输入，哪些类别更有可能，显示模型可以表达不确定性并提供比简单的正确或错误标签更细微的输出。
- en: Rather than simply forcing the student model to classify the image strictly
    as a cat, the teacher model provides a more nuanced learning signal, indicating
    that while “dog” is incorrect, it is a more reasonable mistake than “fox.” This
    subtle information helps the student model build better decision boundaries between
    similar classes, making it more robust to ambiguity in real-world data.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 与简单地将学生模型强制分类为猫不同，教师模型提供了一个更细微的学习信号，表明虽然“狗”是不正确的，但它比“狐狸”是一个更合理的错误。这种微妙的信息帮助学生模型在相似类别之间建立更好的决策边界，使其对现实世界数据中的模糊性更具鲁棒性。
- en: This effect is particularly useful in cases where training data is limited or
    noisy. A large teacher model trained on extensive data has already learned to
    generalize well, capturing patterns that might be difficult to discover with smaller
    datasets. The student benefits by inheriting this structured knowledge, acting
    as if it had access to a larger training signal than what is explicitly available.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这种效果在训练数据有限或噪声较大的情况下尤其有用。在大量数据上训练的大型教师模型已经学会了很好地泛化，捕捉到可能难以从较小数据集发现的模式。学生通过继承这种结构化知识，仿佛它能够访问比明确可用的更大的训练信号。
- en: Another key benefit of knowledge distillation is its regularization effect.
    Because soft targets distribute probability mass across multiple classes, they
    prevent the student model from overfitting to specific hard labels. This regularization
    improves model generalization and reduces sensitivity to adversarial inputs. Instead
    of confidently assigning a probability of 1.0 to the correct class and 0.0 to
    all others, the student learns to make more calibrated predictions, which improves
    its generalization performance. This is especially important when the student
    model has fewer parameters, as smaller networks are more prone to overfitting.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏的另一个关键好处是其正则化效果。由于软目标将概率质量分布在多个类别上，它们防止学生模型过度拟合到特定的硬标签。这种正则化提高了模型泛化能力并减少了对抗性输入的敏感性。学生模型不是自信地将正确类别的概率分配为1.0，而将所有其他类别的概率分配为0.0，而是学会做出更准确的预测，这提高了其泛化性能。这对于学生模型参数较少的情况尤为重要，因为较小的网络更容易过度拟合。
- en: Finally, distillation helps compress large models into smaller, more efficient
    versions without major performance loss. This compression capability directly
    enables the sustainable AI practices [Chapter 18](ch024.xhtml#sec-sustainable-ai)
    by reducing the environmental impact of model deployment while maintaining performance
    standards. Training a small model from scratch often results in lower accuracy
    because the model lacks the capacity to learn the complex representations that
    a larger network can capture. However, by using the knowledge of a well-trained
    teacher, the student can reach a higher accuracy than it would have on its own,
    making it a more practical choice for real-world ML deployments, particularly
    in edge computing, mobile applications, and other resource-constrained environments
    explored in [Chapter 14](ch020.xhtml#sec-ondevice-learning).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，蒸馏技术有助于将大型模型压缩成更小、更高效的版本，而不会造成主要性能损失。这种压缩能力直接促进了可持续的AI实践[第18章](ch024.xhtml#sec-sustainable-ai)，通过减少模型部署的环境影响同时保持性能标准。从头开始训练小型模型通常会导致较低的准确率，因为模型缺乏学习复杂表示的能力，而这些能力是大型网络能够捕捉到的。然而，通过使用训练有素的教师的知识，学生可以达到比自身更高的准确率，这使得它成为现实世界ML部署的更实际选择，尤其是在边缘计算、移动应用和其他资源受限的环境中，这些内容在第14章[第14章](ch020.xhtml#sec-ondevice-learning)中有所探讨。
- en: Efficiency Gains
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 效率提升
- en: 'Knowledge distillation’s efficiency benefits span three key areas: memory efficiency,
    computational efficiency, and deployment flexibility. Unlike pruning which modifies
    trained models, distillation trains compact models from the start using teacher
    guidance, enabling accuracy levels difficult to achieve through standard training
    alone ([Sanh et al. 2019](ch058.xhtml#ref-sanh2019distilbert)), supporting structured
    evaluation approaches in [Chapter 12](ch018.xhtml#sec-benchmarking-ai).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏的效率优势涵盖了三个关键领域：内存效率、计算效率和部署灵活性。与修改训练模型的剪枝技术不同，蒸馏从一开始就使用教师指导来训练紧凑的模型，这使得仅通过标准训练难以达到的准确率水平成为可能([Sanh等人，2019](ch058.xhtml#ref-sanh2019distilbert))，支持第12章[第12章](ch018.xhtml#sec-benchmarking-ai)中提到的结构化评估方法。
- en: Memory and Model Compression
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内存和模型压缩
- en: A key advantage of knowledge distillation is that it enables smaller models
    to retain much of the predictive power of larger models, significantly reducing
    memory footprint. This is particularly useful in resource-constrained environments
    such as mobile and embedded AI systems, where model size directly impacts storage
    requirements and load times.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏的一个关键优势是它使小型模型能够保留大型模型的大部分预测能力，从而显著减少内存占用。这在资源受限的环境中特别有用，如移动和嵌入式AI系统，因为模型大小直接影响存储需求和加载时间。
- en: For instance, models such as DistilBERT ([Sanh et al. 2019](ch058.xhtml#ref-sanh2019distilbert))
    in NLP and MobileNet distillation variants ([A. G. Howard et al. 2017](ch058.xhtml#ref-howard2017mobilenets))
    in computer vision have been shown to retain up to 97% of the accuracy of their
    larger teacher models while using only half the number of parameters. This level
    of compression is often superior to pruning, where aggressive parameter reduction
    can lead to deterioration in representational power.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，NLP中的DistilBERT([Sanh等人，2019](ch058.xhtml#ref-sanh2019distilbert))和计算机视觉中的MobileNet蒸馏变体([A.
    G. Howard等人，2017](ch058.xhtml#ref-howard2017mobilenets))等模型已被证明在仅使用一半参数的情况下，保留了其较大教师模型高达97%的准确率。这种压缩水平通常优于剪枝，因为激进的参数减少可能导致表示能力的下降。
- en: Another key benefit of knowledge distillation is its ability to transfer robustness
    and generalization from the teacher to the student. Large models are often trained
    with extensive datasets and develop strong generalization capabilities, meaning
    they are less sensitive to noise and data shifts. A well-trained student model
    inherits these properties, making it less prone to overfitting and more stable
    across diverse deployment conditions. This is particularly useful in low-data
    regimes, where training a small model from scratch may result in poor generalization
    due to insufficient training examples.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏的另一个关键好处是它能够将鲁棒性和泛化能力从教师模型转移到学生模型。大型模型通常使用大量数据集进行训练，并发展出强大的泛化能力，这意味着它们对噪声和数据变化不太敏感。一个训练良好的学生模型继承了这些属性，使其更不容易过拟合，并在不同的部署条件下更加稳定。这在低数据环境中尤其有用，因为在低数据环境中，从头开始训练一个小模型可能会由于训练样本不足而导致泛化能力差。
- en: Computation and Inference Speed
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算和推理速度
- en: By training the student model to approximate the teacher’s knowledge in a more
    compact representation, distillation results in models that require fewer FLOPs
    per inference, leading to faster execution times. Unlike unstructured pruning,
    which may require specialized hardware support for sparse computation, a distilled
    model remains densely structured, making it more compatible with existing machine
    learning accelerators such as GPUs, TPUs, and edge AI chips ([Jiao et al. 2020](ch058.xhtml#ref-jiao2020tinybert)).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 通过训练学生模型以更紧凑的表示近似教师的知识，蒸馏导致模型在推理时需要的FLOPs更少，从而实现更快的执行时间。与可能需要专门硬件支持进行稀疏计算的未结构化剪枝不同，蒸馏模型保持密集结构，使其与现有的机器学习加速器（如GPU、TPU和边缘AI芯片）更加兼容
    ([Jiao et al. 2020](ch058.xhtml#ref-jiao2020tinybert))。
- en: 'In real-world deployments, this translates to:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际部署中，这转化为：
- en: Reduced inference latency, which is important for real-time AI applications
    such as speech recognition, recommendation systems, and self-driving perception
    models.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少了推理延迟，这对于语音识别、推荐系统和自动驾驶感知模型等实时AI应用非常重要。
- en: Lower energy consumption, making distillation particularly relevant for low-power
    AI on mobile devices and IoT systems.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低能耗，使蒸馏对于移动设备和物联网系统上的低功耗AI特别相关。
- en: Higher throughput in cloud inference, where serving a distilled model allows
    large-scale AI applications to reduce computational cost while maintaining model
    quality.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云推理中的更高吞吐量，其中提供蒸馏模型允许大规模AI应用在保持模型质量的同时降低计算成本。
- en: For example, when deploying transformer models for NLP, organizations often
    use teacher-student distillation to create models that achieve similar accuracy
    at 2-4<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    lower latency, making it feasible to serve billions of requests per day with significantly
    lower computational overhead.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当部署用于NLP的transformer模型时，组织通常使用教师-学生蒸馏来创建在2-4倍更低延迟下达到相似精度的模型，这使得每天处理数十亿请求成为可能，同时显著降低了计算开销。
- en: Deployment and System Considerations
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 部署和系统考虑
- en: Knowledge distillation is also effective in multi-task learning scenarios, where
    a single teacher model can guide multiple student models for different tasks.
    For example, in multi-lingual NLP models, a large teacher trained on multiple
    languages can transfer language-specific knowledge to smaller, task-specific student
    models, enabling efficient deployment across different languages without retraining
    from scratch. Similarly, in computer vision, a teacher trained on diverse object
    categories can distill knowledge into specialized students optimized for tasks
    such as face recognition, medical imaging, or autonomous driving.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏在多任务学习场景中也非常有效，其中单个教师模型可以指导多个针对不同任务的学生模型。例如，在多语言NLP模型中，一个在多种语言上训练的大型教师模型可以将特定于语言的知识转移到较小、针对特定任务的模型中，从而实现跨不同语言的快速部署，无需从头开始重新训练。同样，在计算机视觉中，一个在多种对象类别上训练的教师模型可以将知识蒸馏到针对人脸识别、医学成像或自动驾驶等任务的专用学生模型中。
- en: Once a student model is distilled, it can be further optimized for hardware-specific
    acceleration using techniques such as pruning, quantization, and graph optimization.
    This ensures that compressed models remain inference-efficient across multiple
    hardware environments, particularly in edge AI and mobile deployments ([Gordon,
    Duh, and Andrews 2020](ch058.xhtml#ref-gordon2020compressing)).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦student模型被蒸馏，可以使用剪枝、量化和图优化等技术进一步优化以适应特定硬件的加速。这确保了压缩模型在多个硬件环境中保持推理效率，尤其是在边缘AI和移动部署中([Gordon,
    Duh, and Andrews 2020](ch058.xhtml#ref-gordon2020compressing))。
- en: Despite its advantages, knowledge distillation has some limitations. The effectiveness
    of distillation depends on the quality of the teacher model, a poorly trained
    teacher may transfer incorrect biases to the student. Distillation introduces
    an additional training phase, where both the teacher and student must be used
    together, increasing computational costs during training. In some cases, designing
    an appropriate student model architecture that can fully benefit from the teacher’s
    knowledge remains a challenge, as overly small student models may not have enough
    capacity to absorb all the relevant information.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管知识蒸馏有其优势，但也存在一些局限性。蒸馏的有效性取决于教师模型的质量，一个训练不当的教师模型可能会将错误的偏差传递给学生。蒸馏引入了一个额外的训练阶段，其中教师和学生模型必须一起使用，这增加了训练期间的计算成本。在某些情况下，设计一个能够充分利用教师知识的学生模型架构仍然是一个挑战，因为过小的学生模型可能没有足够的容量来吸收所有相关信息。
- en: Trade-offs
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权衡
- en: Compared to pruning, knowledge distillation preserves accuracy better but requires
    higher training complexity through training a new model rather than modifying
    an existing one. However, pruning provides a more direct computational efficiency
    gain, especially when structured pruning is used. In practice, combining pruning
    and distillation often yields the best trade-off, as seen in models like DistilBERT
    and MobileBERT, where pruning first reduces unnecessary parameters before distillation
    optimizes a final student model. [Table 10.4](ch016.xhtml#tbl-kd-pruning) summarizes
    the key trade-offs between knowledge distillation and pruning.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 与剪枝相比，知识蒸馏在保持准确率方面表现更好，但需要通过训练新模型而不是修改现有模型来提高训练复杂性。然而，剪枝提供了更直接的计算效率提升，尤其是在使用结构化剪枝时。在实践中，结合剪枝和蒸馏通常能获得最佳权衡，正如DistilBERT和MobileBERT等模型所示，其中剪枝首先减少不必要的参数，然后蒸馏优化最终的student模型。[表10.4](ch016.xhtml#tbl-kd-pruning)总结了知识蒸馏和剪枝之间的关键权衡。
- en: 'Table 10.4: **Model Compression Trade-Offs**: Knowledge distillation and pruning
    represent distinct approaches to reducing model size and improving efficiency,
    each with unique strengths and weaknesses regarding accuracy, computational cost,
    and implementation complexity. Distillation prioritizes preserving accuracy through
    knowledge transfer, while pruning directly reduces computational demands by eliminating
    redundant parameters, making their combined use a common strategy for optimal
    performance.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.4：**模型压缩权衡**：知识蒸馏和剪枝是减少模型大小和提高效率的不同方法，每种方法在准确率、计算成本和实现复杂性方面都有独特的优势和劣势。蒸馏优先考虑通过知识传递来保持准确率，而剪枝通过消除冗余参数直接减少计算需求，因此它们的结合使用是获得最佳性能的常见策略。
- en: '| **Criterion** | **Knowledge Distillation** | **Pruning** |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| **标准** | **知识蒸馏** | **剪枝** |'
- en: '| --- | --- | --- |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Accuracy retention** | High – Student learns from teacher, better generalization
    | Varies – Can degrade accuracy if over-pruned |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| **准确率保持** | 高 – 学生从教师那里学习，更好的泛化能力 | 变化 – 如果过度剪枝，可能会降低准确率 |'
- en: '| **Training cost** | Higher – Requires training both teacher and student |
    Lower – Only fine-tuning needed |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| **训练成本** | 较高 – 需要训练教师和学生模型 | 较低 – 只需微调 |'
- en: '| **Inference speed** | High – Produces dense, optimized models | Depends –
    Structured pruning is efficient, unstructured needs special support |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| **推理速度** | 高 – 生成密集、优化的模型 | 取决于 – 结构化剪枝效率高，非结构化需要特殊支持 |'
- en: '| **Hardware compatibility** | High – Works on standard accelerators | Limited
    – Sparse models may need specialized execution |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| **硬件兼容性** | 高 – 在标准加速器上工作 | 有限 – 稀疏模型可能需要专门的执行 |'
- en: '| **Ease of implementation** | Complex – Requires designing a teacher-student
    pipeline | Simple – Applied post-training |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| **实现难度** | 复杂 – 需要设计教师-学生流水线 | 简单 – 训练后应用 |'
- en: Knowledge distillation remains an important technique in ML systems optimization,
    often used alongside pruning and quantization for deployment-ready models. Understanding
    how distillation interacts with these complementary techniques is essential for
    building effective multi-stage optimization pipelines.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏仍然是机器学习系统优化的一个重要技术，通常与剪枝和量化一起用于部署就绪的模型。理解蒸馏如何与这些互补技术相互作用对于构建有效的多阶段优化管道至关重要。
- en: Structured Approximations
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化近似
- en: Approximation-based compression techniques restructure model representations
    to reduce complexity while maintaining expressive power, complementing the pruning
    and distillation methods discussed earlier.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 基于近似的压缩技术重构模型表示，以降低复杂性同时保持表达能力，补充了之前讨论的剪枝和蒸馏方法。
- en: Rather than eliminating individual parameters, approximation methods decompose
    large weight matrices and tensors into lower-dimensional components, allowing
    models to be stored and executed more efficiently. These techniques leverage the
    observation that many high-dimensional representations can be well-approximated
    by lower-rank structures, thereby reducing the number of parameters without a
    significant loss in performance. Unlike pruning, which selectively removes connections,
    or distillation, which transfers learned knowledge, factorization-based approaches
    optimize the internal representation of a model through structured approximations.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于消除单个参数，近似方法将大的权重矩阵和张量分解为低维组件，使得模型可以更高效地存储和执行。这些技术利用了这样一个观察：许多高维表示可以通过低秩结构很好地近似，从而在不显著损失性能的情况下减少参数数量。与选择性移除连接的剪枝或转移学习知识的蒸馏不同，基于分解的方法通过结构化近似优化了模型的内部表示。
- en: 'Among the most widely used approximation techniques are:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最广泛使用的近似技术包括：
- en: '**Low-Rank Matrix Factorization (LRMF)**: A method for decomposing weight matrices
    into products of lower-rank matrices, reducing storage and computational complexity.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低秩矩阵分解（LRMF）**：一种将权重矩阵分解为低秩矩阵乘积的方法，从而降低存储和计算复杂性。'
- en: '**Tensor Decomposition**: A generalization of LRMF to higher-dimensional tensors,
    enabling more efficient representations of multi-way interactions in neural networks.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量分解**：LRMF到高维张量的推广，使得神经网络中多向交互的更高效表示成为可能。'
- en: Both methods improve model efficiency in machine learning, particularly in resource-constrained
    environments such as edge ML and Tiny ML. Low-rank factorization and tensor decomposition
    accelerate model training and inference by reducing the number of required operations.
    The following sections will provide a detailed examination of low-rank matrix
    factorization and tensor decomposition, including their mathematical foundations,
    applications, and associated trade-offs.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都提高了机器学习中的模型效率，尤其是在资源受限的环境，如边缘机器学习和Tiny ML中。低秩分解和张量分解通过减少所需操作的数量来加速模型训练和推理。以下几节将详细探讨低秩矩阵分解和张量分解，包括它们的数学基础、应用和相关权衡。
- en: Low-Rank Factorization
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 低秩分解
- en: Many machine learning models contain a significant degree of redundancy in their
    weight matrices, leading to inefficiencies in computation, storage, and deployment.
    In the previous sections, pruning and knowledge distillation were introduced as
    methods to reduce model size, pruning by selectively removing parameters and distillation
    by transferring knowledge from a larger model to a smaller one. However, these
    techniques do not alter the structure of the model’s parameters. Instead, they
    focus on reducing redundant weights or optimizing training processes.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习模型的权重矩阵中包含大量的冗余，导致计算、存储和部署效率低下。在前几节中，介绍了剪枝和知识蒸馏作为减小模型尺寸的方法，剪枝是通过选择性移除参数，而蒸馏是通过从较大的模型向较小的模型转移知识。然而，这些技术并没有改变模型参数的结构。相反，它们专注于减少冗余权重或优化训练过程。
- en: Low-Rank Matrix Factorization (LRMF) provides an alternative approach by approximating
    a model’s weight matrices with lower-rank representations, rather than explicitly
    removing or transferring information. This technique restructures large parameter
    matrices into compact, lower-dimensional components, preserving most of the original
    information while significantly reducing storage and computational costs. Unlike
    pruning, which creates sparse representations, or distillation, which requires
    an additional training process, LRMF is a purely mathematical transformation that
    decomposes a weight matrix into two or more smaller matrices.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩矩阵分解（LRMF）通过用低秩表示来近似模型的权重矩阵，而不是显式地删除或转移信息，提供了一种替代方法。这项技术将大型参数矩阵重构为紧凑的、低维度的组件，在显著降低存储和计算成本的同时，保留了大部分原始信息。与创建稀疏表示的剪枝技术不同，或者需要额外训练过程的蒸馏技术不同，LRMF是一种纯粹的数学变换，它将权重矩阵分解为两个或更多较小的矩阵。
- en: This structured compression is particularly useful in machine learning systems
    where efficiency is a primary concern, such as edge computing, cloud inference,
    and hardware-accelerated ML execution. By using low-rank approximations, models
    can achieve significant reductions in parameter storage while maintaining predictive
    accuracy, making LRMF a valuable tool for optimizing machine learning architectures.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构化压缩在机器学习系统中特别有用，在这些系统中效率是一个主要关注点，例如边缘计算、云推理和硬件加速的机器学习执行。通过使用低秩近似，模型可以在保持预测准确性的同时实现参数存储的显著减少，使LRMF成为优化机器学习架构的有价值工具。
- en: Training Mathematics
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练数学
- en: LRMF is a mathematical technique used in linear algebra and machine learning
    systems to approximate a high-dimensional matrix by decomposing it into the product
    of lower-dimensional matrices. This factorization enables a more compact representation
    of model parameters, reducing both memory footprint and computational complexity
    while preserving important structural information. In the context of machine learning
    systems, LRMF plays a important role in optimizing model efficiency, particularly
    for resource-constrained environments such as edge AI and embedded deployments.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: LRMF是一种在线性代数和机器学习系统中使用的数学技术，通过将其分解为低维矩阵的乘积来近似高维矩阵。这种分解使得模型参数的表示更加紧凑，同时减少了内存占用和计算复杂度，并保留了重要的结构信息。在机器学习系统的背景下，LRMF在优化模型效率方面发挥着重要作用，尤其是在资源受限的环境，如边缘人工智能和嵌入式部署中。
- en: 'Formally, given a matrix <semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">A \in \mathbb{R}^{m \times n}</annotation></semantics>,
    LRMF seeks two matrices <semantics><mrow><mi>U</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">U \in \mathbb{R}^{m \times k}</annotation></semantics>
    and <semantics><mrow><mi>V</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>k</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">V \in \mathbb{R}^{k \times n}</annotation></semantics>
    such that: <semantics><mrow><mi>A</mi><mo>≈</mo><mi>U</mi><mi>V</mi></mrow> <annotation
    encoding="application/x-tex">A \approx UV</annotation></semantics> where <semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics> is the rank of the approximation,
    typically much smaller than both <semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics>
    and <semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>.
    This approximation is commonly obtained through singular value decomposition (SVD),
    where <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    is factorized as: <semantics><mrow><mi>A</mi><mo>=</mo><mi>U</mi><mi>Σ</mi><msup><mi>V</mi><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">A = U \Sigma V^T</annotation></semantics>
    where <semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics>
    is a diagonal matrix containing singular values, and <semantics><mi>U</mi><annotation
    encoding="application/x-tex">U</annotation></semantics> and <semantics><mi>V</mi><annotation
    encoding="application/x-tex">V</annotation></semantics> are orthogonal matrices.
    By retaining only the top <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    singular values, a low-rank approximation of <semantics><mi>A</mi><annotation
    encoding="application/x-tex">A</annotation></semantics> is obtained.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，给定一个矩阵 <semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">A \in \mathbb{R}^{m \times n}</annotation></semantics>，LRMF
    寻找两个矩阵 <semantics><mrow><mi>U</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">U \in \mathbb{R}^{m \times k}</annotation></semantics>
    和 <semantics><mrow><mi>V</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>k</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">V \in \mathbb{R}^{k \times n}</annotation></semantics>，使得：<semantics><mrow><mi>A</mi><mo>≈</mo><mi>U</mi><mi>V</mi></mrow>
    <annotation encoding="application/x-tex">A \approx UV</annotation></semantics>
    其中 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    是近似的秩，通常远小于 <semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics>
    和 <semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>。这种近似通常通过奇异值分解（SVD）获得，其中
    <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    被分解为：<semantics><mrow><mi>A</mi><mo>=</mo><mi>U</mi><mi>Σ</mi><msup><mi>V</mi><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">A = U \Sigma V^T</annotation></semantics>
    其中 <semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics>
    是包含奇异值的对角矩阵，而 <semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics>
    和 <semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics>
    是正交矩阵。通过仅保留前 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    个奇异值，获得 <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    的低秩近似。
- en: '[Figure 10.10](ch016.xhtml#fig-matrix-factorization) illustrates the decrease
    in parameterization enabled by low-rank matrix factorization. Observe how the
    matrix <semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics>
    can be approximated by the product of matrices <semantics><msub><mi>L</mi><mi>k</mi></msub><annotation
    encoding="application/x-tex">L_k</annotation></semantics> and <semantics><msubsup><mi>R</mi><mi>k</mi><mi>T</mi></msubsup><annotation
    encoding="application/x-tex">R_k^T</annotation></semantics>. For intuition, most
    fully connected layers in networks are stored as a projection matrix <semantics><mi>M</mi><annotation
    encoding="application/x-tex">M</annotation></semantics>, which requires <semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">m \times n</annotation></semantics> parameters to
    be loaded during computation. However, by decomposing and approximating it as
    the product of two lower-rank matrices, we only need to store <semantics><mrow><mi>m</mi><mo>×</mo><mi>k</mi><mo>+</mo><mi>k</mi><mo>×</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">m \times k + k \times n</annotation></semantics>
    parameters in terms of storage while incurring an additional compute cost of the
    matrix multiplication. So long as <semantics><mrow><mi>k</mi><mo><</mo><mi>n</mi><mi>/</mi><mn>2</mn></mrow><annotation
    encoding="application/x-tex">k < n/2</annotation></semantics>, this factorization
    has fewer total parameters to store while adding a computation of runtime <semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>m</mi><mi>k</mi><mi>n</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(mkn)</annotation></semantics>
    ([I. Gu 2023](ch058.xhtml#ref-gu2023deep)).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.10](ch016.xhtml#fig-matrix-factorization)展示了低秩矩阵分解带来的参数化减少。观察矩阵<semantics><mi>M</mi><annotation
    encoding="application/x-tex">M</annotation></semantics>如何通过矩阵<semantics><msub><mi>L</mi><mi>k</mi></msub><annotation
    encoding="application/x-tex">L_k</annotation></semantics>和<semantics><msubsup><mi>R</mi><mi>k</mi><mi>T</mi></msubsup><annotation
    encoding="application/x-tex">R_k^T</annotation></semantics>的乘积进行近似。为了直观理解，网络中的大多数全连接层都存储为投影矩阵<semantics><mi>M</mi><annotation
    encoding="application/x-tex">M</annotation></semantics>，这需要<semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">m \times n</annotation></semantics>参数在计算过程中加载。然而，通过将其分解并近似为两个低秩矩阵的乘积，我们只需要在存储方面存储<semantics><mrow><mi>m</mi><mo>×</mo><mi>k</mi><mo>+</mo><mi>k</mi><mo>×</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">m \times k + k \times n</annotation></semantics>参数，同时承担额外的矩阵乘法计算成本。只要<semantics><mrow><mi>k</mi><mo><</mo><mi>n</mi><mi>/</mi><mn>2</mn></mrow><annotation
    encoding="application/x-tex">k < n/2</annotation></semantics>，这种分解在存储总参数数量方面更少，同时增加了运行时的计算复杂度<semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>m</mi><mi>k</mi><mi>n</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(mkn)</annotation></semantics>
    ([I. Gu 2023](ch058.xhtml#ref-gu2023deep)).'
- en: '![](../media/file156.svg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file156.svg)'
- en: 'Figure 10.10: **Low-Rank Factorization**: Decomposing a matrix into lower-rank
    approximations reduces the number of parameters needed for storage and computation,
    enabling efficient model representation. By expressing a matrix <semantics><mi>a</mi><annotation
    encoding="application/x-tex">a</annotation></semantics> as the product of two
    smaller matrices, <semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics>
    and <semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics>,
    we transition from storing <semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">m \times n</annotation></semantics> parameters to
    <semantics><mrow><mi>m</mi><mo>×</mo><mi>k</mi><mo>+</mo><mi>k</mi><mo>×</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">m \times k + k \times n</annotation></semantics>
    parameters, with <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    representing the reduced rank. Source: The Clever Machine.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10：**低秩分解**：将矩阵分解为低秩近似可以减少存储和计算所需的参数数量，从而实现高效的模型表示。通过将矩阵<semantics><mi>a</mi><annotation
    encoding="application/x-tex">a</annotation></semantics>表示为两个较小矩阵<semantics><mi>u</mi><annotation
    encoding="application/x-tex">u</annotation></semantics>和<semantics><mi>v</mi><annotation
    encoding="application/x-tex">v</annotation></semantics>的乘积，我们由存储<semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">m \times n</annotation></semantics>参数转变为存储<semantics><mrow><mi>m</mi><mo>×</mo><mi>k</mi><mo>+</mo><mi>k</mi><mo>×</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">m \times k + k \times n</annotation></semantics>参数，其中<semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics>代表降低的秩。来源：The Clever Machine.
- en: LRMF is widely used to enhance the efficiency of machine learning models by
    reducing parameter redundancy, particularly in fully connected and convolutional
    layers. In the broader context of machine learning systems, factorization techniques
    contribute to optimizing model inference speed, storage efficiency, and adaptability
    to specialized hardware accelerators.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: LRMF通过减少参数冗余，广泛用于提高机器学习模型的效率，尤其是在完全连接和卷积层中。在更广泛的机器学习系统背景下，因式分解技术有助于优化模型推理速度、存储效率和适应专用硬件加速器的能力。
- en: Fully connected layers often contain large weight matrices, making them ideal
    candidates for factorization. Instead of storing a dense <semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">m \times n</annotation></semantics> weight matrix,
    LRMF allows for a more compact representation with two smaller matrices of dimensions
    <semantics><mrow><mi>m</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">m
    \times k</annotation></semantics> and <semantics><mrow><mi>k</mi><mo>×</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">k \times n</annotation></semantics>, significantly
    reducing storage and computational costs. This reduction is particularly valuable
    in cloud-to-edge ML pipelines, where minimizing model size can facilitate real-time
    execution on embedded devices.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接层通常包含大型权重矩阵，这使得它们成为因式分解的理想候选者。LRMF（低秩矩阵分解）不是存储一个密集的 <semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">m \times n</annotation></semantics> 权重矩阵，而是允许使用两个更小的矩阵来表示，这两个矩阵的维度分别是
    <semantics><mrow><mi>m</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">m
    \times k</annotation></semantics> 和 <semantics><mrow><mi>k</mi><mo>×</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">k \times n</annotation></semantics>，从而显著降低存储和计算成本。这种降低在云到边缘的机器学习管道中尤其有价值，因为最小化模型大小可以促进在嵌入式设备上的实时执行。
- en: Convolutional layers can also benefit from LRMF by decomposing convolutional
    filters into separable structures. Techniques such as depthwise-separable convolutions
    leverage factorization principles to achieve computational efficiency without
    significant loss in accuracy. These methods align well with hardware-aware optimizations
    used in modern AI acceleration frameworks.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层也可以通过将卷积滤波器分解为可分离结构来从LRMF中受益。深度可分离卷积等技术利用因式分解原理，在不显著损失精度的同时实现计算效率。这些方法与现代AI加速框架中使用的硬件感知优化技术相吻合。
- en: LRMF has been extensively used in collaborative filtering for recommendation
    systems. By factorizing user-item interaction matrices, latent factors corresponding
    to user preferences and item attributes can be extracted, enabling efficient and
    accurate recommendations. Within large-scale machine learning systems, such optimizations
    directly impact scalability and performance in production environments.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: LRMF在协同过滤推荐系统中得到了广泛的应用。通过因式分解用户-项目交互矩阵，可以提取出与用户偏好和项目属性相对应的潜在因子，从而实现高效且准确的推荐。在大型机器学习系统中，此类优化直接影响到生产环境中的可扩展性和性能。
- en: Factorization Efficiency and Challenges
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 因式分解效率和挑战
- en: By factorizing a weight matrix into lower-rank components, the number of parameters
    required for storage is reduced from <semantics><mrow><mi>O</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>m</mi><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">O(mn)</annotation></semantics> to <semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>m</mi><mi>k</mi><mo>+</mo><mi>k</mi><mi>n</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(mk
    + kn)</annotation></semantics>, where <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    is significantly smaller than <semantics><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">m, n</annotation></semantics>. However, this reduction
    comes at the cost of an additional matrix multiplication operation during inference,
    potentially increasing computational latency. In machine learning systems, this
    trade-off is carefully managed to balance storage efficiency and real-time inference
    speed.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将权重矩阵分解为低秩分量，存储所需的参数数量从<semantics><mrow><mi>O</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>m</mi><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">O(mn)</annotation></semantics>减少到<semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>m</mi><mi>k</mi><mo>+</mo><mi>k</mi><mi>n</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(mk
    + kn)</annotation></semantics>，其中<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>远小于<semantics><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">m, n</annotation></semantics>。然而，这种减少是以在推理期间增加额外的矩阵乘法操作为代价的，这可能会增加计算延迟。在机器学习系统中，这种权衡被仔细管理，以平衡存储效率和实时推理速度。
- en: Choosing an appropriate rank <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    is a key challenge in LRMF. A smaller <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    results in greater compression but may lead to significant information loss, while
    a larger <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    retains more information but offers limited efficiency gains. Methods such as
    cross-validation and heuristic approaches are often employed to determine the
    optimal rank, particularly in large-scale ML deployments where compute and storage
    constraints vary.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在LRMF（低秩矩阵分解）中选择一个合适的秩<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>是一个关键挑战。较小的<semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics>会导致更高的压缩率，但可能会造成显著的信息损失，而较大的<semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics>则能保留更多信息，但提供的效率提升有限。交叉验证和启发式方法等通常被用来确定最优的秩，尤其是在大规模机器学习部署中，计算和存储约束各不相同。
- en: In real-world machine learning applications, datasets may contain noise or missing
    values, which can affect the quality of factorization. Regularization techniques,
    such as adding an <semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding="application/x-tex">L_2</annotation></semantics>
    penalty, can help mitigate overfitting and improve the robustness of LRMF, ensuring
    stable performance across different ML system architectures.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的机器学习应用中，数据集可能包含噪声或缺失值，这可能会影响分解的质量。正则化技术，如添加一个<semantics><msub><mi>L</mi><mn>2</mn></msub><annotation
    encoding="application/x-tex">L_2</annotation></semantics>惩罚，可以帮助减轻过拟合并提高LRMF的鲁棒性，确保在不同机器学习系统架构中保持稳定的性能。
- en: Low-rank matrix factorization provides an effective approach for reducing the
    complexity of machine learning models while maintaining their expressive power.
    By approximating weight matrices with lower-rank representations, LRMF facilitates
    efficient inference and model deployment, particularly in resource-constrained
    environments such as edge computing. Within machine learning systems, factorization
    techniques contribute to scalable, hardware-aware optimizations that enhance real-world
    model performance. Despite challenges such as rank selection and computational
    overhead, LRMF remains a valuable tool for improving efficiency in ML system design
    and deployment.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩矩阵分解提供了一种有效的方法，在保持机器学习模型表达能力的同时降低其复杂性。通过用低秩表示来近似权重矩阵，LRMF促进了高效的推理和模型部署，尤其是在资源受限的环境，如边缘计算中。在机器学习系统中，分解技术有助于可扩展、硬件感知的优化，从而提升现实世界模型的性能。尽管存在诸如秩选择和计算开销等挑战，LRMF仍然是在机器学习系统设计和部署中提高效率的有价值工具。
- en: Tensor Decomposition
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量分解
- en: While low-rank matrix factorization provides an effective method for compressing
    large weight matrices in machine learning models, many modern architectures rely
    on multi-dimensional tensors rather than two-dimensional matrices. Convolutional
    layers, attention mechanisms, and embedding representations commonly involve multi-way
    interactions that cannot be efficiently captured using standard matrix factorization
    techniques. In such cases, tensor decomposition provides a more general approach
    to reducing model complexity while preserving structural relationships within
    the data.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然低秩矩阵分解为压缩机器学习模型中的大型权重矩阵提供了一种有效的方法，但许多现代架构依赖于多维张量而不是二维矩阵。卷积层、注意力机制和嵌入表示通常涉及多向交互，这些交互无法使用标准的矩阵分解技术有效地捕捉。在这种情况下，张量分解提供了一种更通用的方法来降低模型复杂性，同时保持数据中的结构关系。
- en: '![](../media/file157.svg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file157.svg)'
- en: 'Figure 10.11: **Tensor Decomposition**: Multi-dimensional tensors enable compact
    representations of high-dimensional data by factorizing them into lower-rank components,
    reducing computational costs and memory requirements compared to direct manipulation
    of the original tensor. This technique extends matrix factorization to handle
    the multi-way interactions common in modern machine learning models like convolutional
    neural networks. Source: ([Richter and Zhao 2021](ch058.xhtml#ref-xinyu)).'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11：**张量分解**：通过将多维张量分解为低秩分量，可以紧凑地表示高维数据，与直接操作原始张量相比，减少了计算成本和内存需求。这项技术将矩阵分解扩展到处理现代机器学习模型（如卷积神经网络）中常见的多向交互。来源：([Richter和Zhao
    2021](ch058.xhtml#ref-xinyu))。
- en: Tensor decomposition (TD) extends the principles of low-rank factorization to
    higher-order tensors, allowing large multi-dimensional arrays to be expressed
    in terms of lower-rank components (see [Figure 10.11](ch016.xhtml#fig-tensor-decomposition)).
    Given that tensors frequently appear in machine learning systems as representations
    of weight parameters, activations, and input features, their direct storage and
    computation often become impractical. By decomposing these tensors into a set
    of smaller factors, tensor decomposition significantly reduces memory requirements
    and computational overhead while maintaining the integrity of the original structure.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 张量分解（TD）将低秩分解的原理扩展到高阶张量，允许大型的多维数组通过低秩分量来表示（参见[图10.11](ch016.xhtml#fig-tensor-decomposition)）。鉴于张量经常出现在机器学习系统中作为权重参数、激活和输入特征的表示，它们的直接存储和计算通常变得不切实际。通过将这些张量分解成一组较小的因子，张量分解显著降低了内存需求和计算开销，同时保持了原始结构的完整性。
- en: Tensor decomposition improves efficiency across various machine learning architectures.
    In convolutional neural networks, it enables approximation of convolutional kernels
    with lower-dimensional factors, reducing parameters while preserving representational
    power. In natural language processing, high-dimensional embeddings can be factorized
    into more compact representations, leading to faster inference and reduced memory
    consumption. In hardware acceleration, tensor decomposition helps optimize tensor
    operations for execution on specialized processors, ensuring efficient utilization
    of computational resources.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 张量分解提高了各种机器学习架构的效率。在卷积神经网络中，它使得可以通过低维因子来近似卷积核，在保留表示能力的同时减少参数。在自然语言处理中，高维嵌入可以被分解成更紧凑的表示，从而实现更快的推理和降低内存消耗。在硬件加速中，张量分解有助于优化张量操作以在专用处理器上执行，确保计算资源的有效利用。
- en: Training Mathematics
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练数学
- en: A tensor is a multi-dimensional extension of a matrix, representing data across
    multiple axes rather than being confined to two-dimensional structures. In machine
    learning, tensors naturally arise in various contexts, including the representation
    of weight parameters, activations, and input features. Given the high dimensionality
    of these tensors, direct storage and computation often become impractical, necessitating
    efficient factorization techniques.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是矩阵的多维扩展，表示数据跨越多个轴而不是局限于二维结构。在机器学习中，张量自然出现在各种情境中，包括权重参数、激活和输入特征的表示。鉴于这些张量的高维性，直接存储和计算通常变得不切实际，需要高效的分解技术。
- en: Tensor decomposition generalizes the principles of low-rank matrix factorization
    by approximating a high-order tensor with a set of lower-rank components. Formally,
    for a given tensor <semantics><mrow><mi>𝒜</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathcal{A} \in \mathbb{R}^{m \times n \times p}</annotation></semantics>,
    the goal of decomposition is to express <semantics><mi>𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics>
    in terms of factorized components that require fewer parameters to store and manipulate.
    This decomposition reduces the memory footprint and computational requirements
    while retaining the structural relationships present in the original tensor.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 张量分解通过用一组低秩组件近似高阶张量，推广了低秩矩阵分解的原则。形式上，对于一个给定的张量 <semantics><mrow><mi>𝒜</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathcal{A} \in \mathbb{R}^{m \times n \times p}</annotation></semantics>，分解的目标是用需要较少参数存储和处理的分解组件来表示
    <semantics><mi>𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics>。这种分解减少了内存占用和计算需求，同时保留了原始张量中存在的结构关系。
- en: Several factorization methods have been developed for tensor decomposition,
    each suited to different applications in machine learning. One common approach
    is CANDECOMP/PARAFAC (CP) decomposition, which expresses a tensor as a sum of
    rank-one components. In CP decomposition, a tensor <semantics><mrow><mi>𝒜</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathcal{A} \in \mathbb{R}^{m \times n \times p}</annotation></semantics>
    is approximated as <semantics><mrow><mi>𝒜</mi><mo>≈</mo><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>u</mi><mi>r</mi></msub><mo>⊗</mo><msub><mi>v</mi><mi>r</mi></msub><mo>⊗</mo><msub><mi>w</mi><mi>r</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathcal{A} \approx \sum_{r=1}^{k} u_r
    \otimes v_r \otimes w_r</annotation></semantics> where <semantics><mrow><msub><mi>u</mi><mi>r</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation
    encoding="application/x-tex">u_r \in \mathbb{R}^{m}</annotation></semantics>,
    <semantics><mrow><msub><mi>v</mi><mi>r</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation
    encoding="application/x-tex">v_r \in \mathbb{R}^{n}</annotation></semantics>,
    and <semantics><mrow><msub><mi>w</mi><mi>r</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>p</mi></msup></mrow><annotation
    encoding="application/x-tex">w_r \in \mathbb{R}^{p}</annotation></semantics> are
    factor vectors and <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    is the rank of the approximation.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为张量分解开发了多种分解方法，每种方法都适用于机器学习中的不同应用。一种常见的方法是CANDECOMP/PARAFAC (CP) 分解，它将张量表示为秩一组件的和。在CP分解中，张量
    <semantics><mrow><mi>𝒜</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathcal{A} \in \mathbb{R}^{m \times n \times p}</annotation></semantics>
    被近似为 <semantics><mrow><mi>𝒜</mi><mo>≈</mo><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>u</mi><mi>r</mi></msub><mo>⊗</mo><msub><mi>v</mi><mi>r</mi></msub><mo>⊗</mo><msub><mi>w</mi><mi>r</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathcal{A} \approx \sum_{r=1}^{k} u_r
    \otimes v_r \otimes w_r</annotation></semantics> 其中 <semantics><mrow><msub><mi>u</mi><mi>r</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation
    encoding="application/x-tex">u_r \in \mathbb{R}^{m}</annotation></semantics>，<semantics><mrow><msub><mi>v</mi><mi>r</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation
    encoding="application/x-tex">v_r \in \mathbb{R}^{n}</annotation></semantics>，和
    <semantics><mrow><msub><mi>w</mi><mi>r</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>p</mi></msup></mrow><annotation
    encoding="application/x-tex">w_r \in \mathbb{R}^{p}</annotation></semantics> 是分解向量，而
    <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    是近似的秩。
- en: Another widely used approach is Tucker decomposition, which generalizes singular
    value decomposition to tensors by introducing a core tensor <semantics><mrow><mi>𝒢</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>k</mi><mn>1</mn></msub><mo>×</mo><msub><mi>k</mi><mn>2</mn></msub><mo>×</mo><msub><mi>k</mi><mn>3</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathcal{G} \in \mathbb{R}^{k_1 \times k_2 \times
    k_3}</annotation></semantics> and factor matrices <semantics><mrow><mi>U</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><msub><mi>k</mi><mn>1</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">U \in \mathbb{R}^{m \times k_1}</annotation></semantics>,
    <semantics><mrow><mi>V</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo>×</mo><msub><mi>k</mi><mn>2</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">V \in \mathbb{R}^{n \times k_2}</annotation></semantics>,
    and <semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>p</mi><mo>×</mo><msub><mi>k</mi><mn>3</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">W \in \mathbb{R}^{p \times k_3}</annotation></semantics>,
    such that <semantics><mrow><mi>𝒜</mi><mo>≈</mo><mi>𝒢</mi><msub><mo>×</mo><mn>1</mn></msub><mi>U</mi><msub><mo>×</mo><mn>2</mn></msub><mi>V</mi><msub><mo>×</mo><mn>3</mn></msub><mi>W</mi></mrow>
    <annotation encoding="application/x-tex">\mathcal{A} \approx \mathcal{G} \times_1
    U \times_2 V \times_3 W</annotation></semantics> where <semantics><msub><mo>×</mo><mi>i</mi></msub><annotation
    encoding="application/x-tex">\times_i</annotation></semantics> denotes the mode-<semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics> tensor-matrix multiplication.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种广泛使用的方法是 Tucker 分解，它通过引入核心张量 <semantics><mrow><mi>𝒢</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>k</mi><mn>1</mn></msub><mo>×</mo><msub><mi>k</mi><mn>2</mn></msub><mo>×</mo><msub><mi>k</mi><mn>3</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathcal{G} \in \mathbb{R}^{k_1 \times k_2 \times
    k_3}</annotation></semantics> 和因子矩阵 <semantics><mrow><mi>U</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><msub><mi>k</mi><mn>1</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">U \in \mathbb{R}^{m \times k_1}</annotation></semantics>，<semantics><mrow><mi>V</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo>×</mo><msub><mi>k</mi><mn>2</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">V \in \mathbb{R}^{n \times k_2}</annotation></semantics>，以及
    <semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>p</mi><mo>×</mo><msub><mi>k</mi><mn>3</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">W \in \mathbb{R}^{p \times k_3}</annotation></semantics>，使得
    <semantics><mrow><mi>𝒜</mi><mo>≈</mo><mi>𝒢</mi><msub><mo>×</mo><mn>1</mn></msub><mi>U</mi><msub><mo>×</mo><mn>2</mn></msub><mi>V</mi><msub><mo>×</mo><mn>3</mn></msub><mi>W</mi></mrow>
    <annotation encoding="application/x-tex">\mathcal{A} \approx \mathcal{G} \times_1
    U \times_2 V \times_3 W</annotation></semantics> 其中 <semantics><msub><mo>×</mo><mi>i</mi></msub><annotation
    encoding="application/x-tex">\times_i</annotation></semantics> 表示第 <semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics> 个模的张量矩阵乘法。
- en: Another method, Tensor-Train (TT) decomposition, factorizes high-order tensors
    into a sequence of lower-rank matrices, reducing both storage and computational
    complexity. Given a tensor <semantics><mrow><mi>𝒜</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>×</mo><msub><mi>m</mi><mn>2</mn></msub><mo>×</mo><mi>…</mi><mo>×</mo><msub><mi>m</mi><mi>d</mi></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathcal{A} \in \mathbb{R}^{m_1 \times m_2 \times
    \dots \times m_d}</annotation></semantics>, TT decomposition represents it as
    a product of lower-dimensional tensor cores <semantics><msup><mi>𝒢</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation
    encoding="application/x-tex">\mathcal{G}^{(i)}</annotation></semantics>, where
    each core <semantics><msup><mi>𝒢</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathcal{G}^{(i)}</annotation></semantics>
    has dimensions <semantics><msup><mi>ℝ</mi><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>×</mo><msub><mi>m</mi><mi>i</mi></msub><mo>×</mo><msub><mi>r</mi><mi>i</mi></msub></mrow></msup><annotation
    encoding="application/x-tex">\mathbb{R}^{r_{i-1} \times m_i \times r_i}</annotation></semantics>,
    and the full tensor is reconstructed as <semantics><mrow><mi>𝒜</mi><mo>≈</mo><msup><mi>𝒢</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>×</mo><msup><mi>𝒢</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>×</mo><mi>…</mi><mo>×</mo><msup><mi>𝒢</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathcal{A} \approx \mathcal{G}^{(1)}
    \times \mathcal{G}^{(2)} \times \dots \times \mathcal{G}^{(d)}</annotation></semantics>
    where <semantics><msub><mi>r</mi><mi>i</mi></msub><annotation encoding="application/x-tex">r_i</annotation></semantics>
    are the TT ranks.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法，张量-训练（TT）分解，将高阶张量分解为一系列低秩矩阵，从而降低存储和计算复杂度。给定一个张量 <semantics><mrow><mi>𝒜</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>×</mo><msub><mi>m</mi><mn>2</mn></msub><mo>×</mo><mi>…</mi><mo>×</mo><msub><mi>m</mi><mi>d</mi></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathcal{A} \in \mathbb{R}^{m_1 \times m_2 \times
    \dots \times m_d}</annotation></semantics>，TT分解将其表示为低维张量核的乘积 <semantics><msup><mi>𝒢</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation
    encoding="application/x-tex">\mathcal{G}^{(i)}</annotation></semantics>，其中每个核
    <semantics><msup><mi>𝒢</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathcal{G}^{(i)}</annotation></semantics>
    的维度为 <semantics><msup><mi>ℝ</mi><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>×</mo><msub><mi>m</mi><mi>i</mi></msub><mo>×</mo><msub><mi>r</mi><mi>i</mi></msub></mrow></msup><annotation
    encoding="application/x-tex">\mathbb{R}^{r_{i-1} \times m_i \times r_i}</annotation></semantics>，而完整的张量则被重建为
    <semantics><mrow><mi>𝒜</mi><mo>≈</mo><msup><mi>𝒢</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>×</mo><msup><mi>𝒢</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>×</mo><mi>…</mi><mo>×</mo><msup><mi>𝒢</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathcal{A} \approx \mathcal{G}^{(1)}
    \times \mathcal{G}^{(2)} \times \dots \times \mathcal{G}^{(d)}</annotation></semantics>，其中
    <semantics><msub><mi>r</mi><mi>i</mi></msub><annotation encoding="application/x-tex">r_i</annotation></semantics>
    是TT秩。
- en: These tensor decomposition methods play a important role in optimizing machine
    learning models by reducing parameter redundancy while maintaining expressive
    power. The next section will examine how these techniques are applied to machine
    learning architectures and discuss their computational trade-offs.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这些张量分解方法在优化机器学习模型中发挥着重要作用，通过减少参数冗余同时保持表达能力。下一节将探讨这些技术如何应用于机器学习架构，并讨论它们的计算权衡。
- en: Tensor Decomposition Applications
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 张量分解应用
- en: Tensor decomposition methods are widely applied in machine learning systems
    to improve efficiency and scalability. By factorizing high-dimensional tensors
    into lower-rank representations, these methods reduce memory usage and computational
    requirements while preserving the model’s expressive capacity. This section examines
    several key applications of tensor decomposition in machine learning, focusing
    on its impact on convolutional neural networks, natural language processing, and
    hardware acceleration.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 张量分解方法在机器学习系统中被广泛应用以提高效率和可扩展性。通过将高维张量分解为低秩表示，这些方法在保留模型表达能力的同时，减少了内存使用和计算需求。本节探讨了机器学习中张量分解的几个关键应用，重点关注其对卷积神经网络、自然语言处理和硬件加速的影响。
- en: In convolutional neural networks (CNNs), tensor decomposition is used to compress
    convolutional filters and reduce the number of required operations during inference.
    A standard convolutional layer contains a set of weight tensors that define how
    input features are transformed. These weight tensors often exhibit redundancy,
    meaning they can be decomposed into smaller components without significantly degrading
    performance. Techniques such as CP decomposition and Tucker decomposition enable
    convolutional filters to be approximated using lower-rank tensors, reducing the
    number of parameters and computational complexity of the convolution operation.
    This form of structured compression is particularly valuable in edge and mobile
    machine learning applications, where memory and compute resources are constrained.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络（CNNs）中，张量分解用于压缩卷积滤波器，并在推理过程中减少所需的操作数量。标准的卷积层包含一组权重张量，这些张量定义了输入特征如何被转换。这些权重张量通常存在冗余，这意味着它们可以被分解为较小的组件，而不会显著降低性能。CP分解和Tucker分解等技术使得卷积滤波器可以使用低秩张量进行近似，从而减少了参数数量和卷积操作的计算复杂度。这种结构化压缩形式在边缘和移动机器学习应用中尤其有价值，在这些应用中，内存和计算资源是受限的。
- en: In natural language processing (NLP), tensor decomposition is commonly applied
    to embedding layers and attention mechanisms. Many NLP models, including transformers,
    rely on high-dimensional embeddings to represent words, sentences, or entire documents.
    These embeddings can be factorized using tensor decomposition to reduce storage
    requirements without compromising their ability to capture semantic relationships.
    Similarly, in transformer-based architectures, the self-attention mechanism requires
    large tensor multiplications, which can be optimized using decomposition techniques
    to lower the computational burden and accelerate inference.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，张量分解通常应用于嵌入层和注意力机制。许多NLP模型，包括转换器，依赖于高维嵌入来表示单词、句子或整个文档。这些嵌入可以通过张量分解进行分解，以减少存储需求，同时不损害其捕捉语义关系的能力。同样，在基于转换器的架构中，自注意力机制需要大量的张量乘法，这些可以通过分解技术进行优化，以降低计算负担并加速推理。
- en: Hardware acceleration for machine learning also benefits from tensor decomposition
    by enabling more efficient execution on specialized processors such as GPUs, tensor
    processing units (TPUs), and field-programmable gate arrays (FPGAs). Many machine
    learning frameworks include optimizations that leverage tensor decomposition to
    improve model execution speed and reduce energy consumption. Decomposing tensors
    into structured low-rank components aligns well with the memory hierarchy of modern
    hardware accelerators, facilitating more efficient data movement and parallel
    computation.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的硬件加速也得益于张量分解，因为它使得在专门的处理器（如GPU、张量处理单元（TPUs）和现场可编程门阵列（FPGAs））上执行更加高效。许多机器学习框架包括利用张量分解来提高模型执行速度和降低能耗的优化。将张量分解为结构化的低秩组件与现代硬件加速器的内存层次结构相吻合，从而促进了更高效的数据移动和并行计算。
- en: Despite these advantages, tensor decomposition introduces certain trade-offs
    that must be carefully managed. The choice of decomposition method and rank significantly
    influences model accuracy and computational efficiency. Selecting an overly aggressive
    rank reduction may lead to excessive information loss, while retaining too many
    components diminishes the efficiency gains. The factorization process itself can
    introduce a computational overhead, requiring careful consideration when applying
    tensor decomposition to large-scale machine learning systems.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些优势，张量分解引入了一些必须谨慎管理的权衡。分解方法和秩的选择显著影响模型精度和计算效率。选择过于激进的秩减少可能导致信息损失过多，而保留过多的成分会减少效率提升。分解过程本身可能引入计算开销，当将张量分解应用于大规模机器学习系统时需要仔细考虑。
- en: TD Trade-offs and Challenges
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: TD权衡与挑战
- en: While tensor decomposition provides significant efficiency gains in machine
    learning systems, it introduces trade-offs that must be carefully managed to maintain
    model accuracy and computational feasibility. These trade-offs primarily involve
    the selection of decomposition rank, the computational complexity of factorization,
    and the stability of factorized representations.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然张量分解在机器学习系统中提供了显著的效率提升，但它引入了必须谨慎管理的权衡，以保持模型精度和计算可行性。这些权衡主要涉及分解秩的选择、分解的计算复杂性和分解表示的稳定性。
- en: One of the primary challenges in tensor decomposition is determining an appropriate
    rank for the factorized representation. In low-rank matrix factorization, the
    rank defines the dimensionality of the factorized matrices, directly influencing
    the balance between compression and information retention. In tensor decomposition,
    rank selection becomes even more complex, as different decomposition methods define
    rank in varying ways. For instance, in CANDECOMP/PARAFAC (CP) decomposition, the
    rank corresponds to the number of rank-one tensors used to approximate the original
    tensor. In Tucker decomposition, the rank is determined by the dimensions of the
    core tensor, while in Tensor-Train (TT) decomposition, the ranks of the factorized
    components dictate the level of compression. Selecting an insufficient rank can
    lead to excessive information loss, degrading the model’s predictive performance,
    whereas an overly conservative rank reduction results in limited compression benefits.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 张量分解的主要挑战之一是确定分解表示的适当秩。在低秩矩阵分解中，秩定义了分解矩阵的维度，直接影响压缩和信息保留之间的平衡。在张量分解中，秩的选择变得更加复杂，因为不同的分解方法以不同的方式定义秩。例如，在CANDECOMP/PARAFAC
    (CP) 分解中，秩对应于用于逼近原始张量的秩一张量的数量。在Tucker分解中，秩由核心张量的维度决定，而在Tensor-Train (TT) 分解中，分解成分的秩决定了压缩级别。选择一个不充分的秩可能导致信息损失过多，降低模型的预测性能，而过度的保守秩减少则导致压缩效益有限。
- en: Another key challenge is the computational overhead associated with performing
    tensor decomposition. The factorization process itself requires solving an optimization
    problem, often involving iterative procedures such as alternating least squares
    (ALS) or optimization algorithms such as stochastic gradient descent. These methods
    can be computationally expensive, particularly for large-scale tensors used in
    machine learning models. During inference, the need to reconstruct tensors from
    their factorized components introduces additional matrix and tensor multiplications,
    which may increase computational latency. The efficiency of tensor decomposition
    in practice depends on striking a balance between reducing parameter storage and
    minimizing the additional computational cost incurred by factorized representations.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键挑战是与执行张量分解相关的计算开销。分解过程本身需要解决一个优化问题，通常涉及交替最小二乘（ALS）或随机梯度下降等迭代过程。这些方法可能计算成本高昂，尤其是在机器学习模型中使用的大规模张量中。在推理过程中，需要从分解成分重建张量，这引入了额外的矩阵和标量乘法，可能会增加计算延迟。张量分解在实际中的效率取决于在减少参数存储和最小化分解表示引起的额外计算成本之间取得平衡。
- en: Numerical stability is another concern when applying tensor decomposition to
    machine learning models. Factorized representations can suffer from numerical
    instability, particularly when the original tensor contains highly correlated
    structures or when decomposition methods introduce ill-conditioned factors. Regularization
    techniques, such as adding constraints on factor matrices or applying low-rank
    approximations incrementally, can help mitigate these issues. The optimization
    process used for decomposition must be carefully tuned to avoid convergence to
    suboptimal solutions that fail to preserve the important properties of the original
    tensor.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在将张量分解应用于机器学习模型时，数值稳定性也是一个需要考虑的问题。分解表示可能存在数值不稳定性，尤其是在原始张量包含高度相关结构或分解方法引入病态因子时。正则化技术，如对因子矩阵添加约束或逐步应用低秩近似，可以帮助缓解这些问题。用于分解的优化过程必须仔细调整，以避免收敛到次优解，这些解未能保留原始张量的重要属性。
- en: Despite these challenges, tensor decomposition remains a valuable tool for optimizing
    machine learning models, particularly in applications where reducing memory footprint
    and computational complexity is a priority. Advances in adaptive decomposition
    methods, automated rank selection strategies, and hardware-aware factorization
    techniques continue to improve the practical utility of tensor decomposition in
    machine learning. The following section will summarize the key insights gained
    from low-rank matrix factorization and tensor decomposition, highlighting their
    role in designing efficient machine learning systems.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，张量分解仍然是优化机器学习模型的有价值工具，尤其是在减少内存占用和计算复杂性的应用中。自适应分解方法、自动秩选择策略和硬件感知分解技术的进步继续提高张量分解在机器学习中的实际效用。下一节将总结从低秩矩阵分解和张量分解中获得的关键见解，突出它们在设计高效机器学习系统中的作用。
- en: LRMF vs. TD
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LRMF vs. TD
- en: Both low-rank matrix factorization and tensor decomposition serve as core techniques
    for reducing the complexity of machine learning models by approximating large
    parameter structures with lower-rank representations. While they share the common
    goal of improving storage efficiency and computational performance, their applications,
    computational trade-offs, and structural assumptions differ significantly. This
    section provides a comparative analysis of these two techniques, highlighting
    their advantages, limitations, and practical use cases in machine learning systems.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩矩阵分解和张量分解都是通过用低秩表示来近似大型参数结构，从而降低机器学习模型复杂性的核心技术。虽然它们共享提高存储效率和计算性能的共同目标，但它们的应用、计算权衡和结构假设存在显著差异。本节对这些两种技术进行了比较分析，突出了它们的优点、局限性和在机器学习系统中的实际应用案例。
- en: One of the key distinctions between LRMF and tensor decomposition lies in the
    dimensionality of the data they operate on. LRMF applies to two-dimensional matrices,
    making it particularly useful for compressing weight matrices in fully connected
    layers or embeddings. Tensor decomposition, on the other hand, extends factorization
    to multi-dimensional tensors, which arise naturally in convolutional layers, attention
    mechanisms, and multi-modal learning. This generalization allows tensor decomposition
    to exploit additional structural properties of high-dimensional data that LRMF
    cannot capture.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: LRMF（低秩矩阵分解）与张量分解之间的一个关键区别在于它们操作数据的维度。LRMF适用于二维矩阵，这使得它在压缩全连接层或嵌入中的权重矩阵方面特别有用。另一方面，张量分解将分解扩展到多维张量，这些张量在卷积层、注意力机制和多模态学习中自然出现。这种推广使得张量分解能够利用LRMF无法捕捉的高维数据的额外结构属性。
- en: Computationally, both methods introduce trade-offs between storage savings and
    inference speed. LRMF reduces the number of parameters in a model by factorizing
    a weight matrix into two smaller matrices, thereby reducing memory footprint while
    incurring an additional matrix multiplication during inference. In contrast, tensor
    decomposition further reduces storage by decomposing tensors into multiple lower-rank
    components, but at the cost of more complex tensor contractions, which may introduce
    higher computational overhead. The choice between these methods depends on whether
    the primary constraint is memory storage or inference latency.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算角度来看，这两种方法在存储节省和推理速度之间引入了权衡。LRMF通过将权重矩阵分解为两个较小的矩阵来减少模型中的参数数量，从而减少内存占用，但在推理过程中会引入额外的矩阵乘法。相比之下，tensor
    decomposition通过将tensor分解为多个低秩分量来进一步减少存储，但代价是更复杂的tensor收缩，这可能会引入更高的计算开销。这两种方法之间的选择取决于主要约束是内存存储还是推理延迟。
- en: '[Table 10.5](ch016.xhtml#tbl-lrmf-tensor) summarizes the key differences between
    LRMF and tensor decomposition:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10.5](ch016.xhtml#tbl-lrmf-tensor) 总结了LRMF和tensor decomposition之间的关键区别：'
- en: 'Table 10.5: **Dimensionality & Factorization**: Low-rank matrix factorization
    (LRMF) and tensor decomposition reduce model storage requirements by representing
    data with fewer parameters, but introduce computational trade-offs during inference;
    LRMF applies to two-dimensional matrices, while tensor decomposition extends this
    approach to multi-dimensional tensors for greater compression potential.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.5：**维度与分解**：低秩矩阵分解（LRMF）和tensor分解通过使用更少的参数来表示数据，从而减少模型存储需求，但在推理过程中引入了计算权衡；LRMF适用于二维矩阵，而tensor
    decomposition将这种方法扩展到多维tensor以实现更大的压缩潜力。
- en: '| **Feature** | **Low-Rank Matrix Factorization (LRMF)** | **Tensor Decomposition**
    |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| **特征** | **低秩矩阵分解（LRMF）** | **tensor分解** |'
- en: '| --- | --- | --- |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Applicable Data Structure** | Two-dimensional matrices | Multi-dimensional
    tensors |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| **适用数据结构** | 二维矩阵 | 多维tensor |'
- en: '| **Compression Mechanism** | Factorizes a matrix into two or more lower-rank
    matrices | Decomposes a tensor into multiple lower-rank components |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| **压缩机制** | 将矩阵分解为两个或更多低秩矩阵 | 将tensor分解为多个低秩分量 |'
- en: '| **Common Methods** | Singular Value Decomposition (SVD), Alternating Least
    Squares (ALS) | CP Decomposition, Tucker Decomposition, Tensor-Train (TT) |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| **常用方法** |奇异值分解（SVD）、交替最小二乘法（ALS） | CP分解、Tucker分解、tensor-Train（TT） |'
- en: '| **Computational Complexity** | Generally lower, often $ O(mnk) $ for a rank-$
    k $ approximation | Higher, due to iterative optimization and tensor contractions
    |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| **计算复杂度** | 通常较低，对于秩为k的近似通常为$ O(mnk) $ | 较高，由于迭代优化和tensor收缩 |'
- en: '| **Storage Reduction** | Reduces storage from $ O(mn) $ to $ O(mk + kn) $
    | Achieves higher compression but requires more complex storage representations
    |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| **存储减少** | 将存储从$ O(mn) $减少到$ O(mk + kn) $ | 实现更高的压缩，但需要更复杂的存储表示 |'
- en: '| **Inference Overhead** | Requires additional matrix multiplication | Introduces
    additional tensor operations, potentially increasing inference latency |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| **推理开销** | 需要额外的矩阵乘法 | 引入额外的tensor操作，可能增加推理延迟 |'
- en: '| **Primary Use Cases** | Fully connected layers, embeddings, recommendation
    systems | Convolutional filters, attention mechanisms, multi-modal learning |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| **主要用例** | 全连接层、嵌入、推荐系统 | 卷积核、注意力机制、多模态学习 |'
- en: '| **Implementation Complexity** | Easier to implement, often involves direct
    factorization methods | More complex, requiring iterative optimization and rank
    selection |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| **实现复杂性** | 实现起来更容易，通常涉及直接分解方法 | 更复杂，需要迭代优化和秩选择 |'
- en: Despite these differences, LRMF and tensor decomposition are not mutually exclusive.
    In many machine learning models, both methods can be applied together to optimize
    different components of the architecture. For example, fully connected layers
    may be compressed using LRMF, while convolutional kernels and attention tensors
    undergo tensor decomposition. The choice of technique ultimately depends on the
    specific characteristics of the model and the trade-offs between storage efficiency
    and computational complexity.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些区别，LRMF和tensor decomposition并不是相互排斥的。在许多机器学习模型中，这两种方法可以同时应用以优化架构的不同组件。例如，全连接层可以使用LRMF进行压缩，而卷积核和注意力tensor则进行tensor
    decomposition。技术的选择最终取决于模型的具体特征以及存储效率与计算复杂度之间的权衡。
- en: Neural Architecture Search
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经架构搜索
- en: Pruning, knowledge distillation, and other techniques explored in previous sections
    rely on human expertise to determine optimal model configurations. While these
    manual approaches have led to significant advancements, selecting optimal architectures
    requires extensive experimentation, and even experienced practitioners may overlook
    more efficient designs ([Elsken, Metzen, and Hutter 2019a](ch058.xhtml#ref-elsken2019neural)).
    Neural Architecture Search (NAS) automates this process by systematically exploring
    large spaces of possible architectures to identify those that best balance accuracy,
    computational cost, memory efficiency, and inference latency.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中探讨的剪枝、知识蒸馏和其他技术依赖于人类专业知识来确定最佳模型配置。虽然这些手动方法已经导致了重大进步，但选择最佳架构需要大量的实验，即使是经验丰富的从业者也可能忽略更有效的设计([Elsken,
    Metzen, and Hutter 2019a](ch058.xhtml#ref-elsken2019neural))。神经架构搜索(NAS)通过系统地探索大量可能的架构来识别那些最佳平衡准确度、计算成本、内存效率和推理延迟的架构，从而自动化这个过程。
- en: '[Figure 10.12](ch016.xhtml#fig-nas-flow) illustrates the NAS process. NAS[26](#fn26)
    operates through three interconnected stages: defining the search space (architectural
    components and constraints), applying search strategies (reinforcement learning([Zoph
    and Le 2017a](ch058.xhtml#ref-zoph2017neural)), evolutionary algorithms, or gradient-based
    methods) to explore candidate architectures, and evaluating performance to ensure
    discovered designs satisfy accuracy and efficiency objectives. This automation
    enables the discovery of novel architectures that often match or surpass human-designed
    models while requiring substantially less expert effort.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.12](ch016.xhtml#fig-nas-flow)展示了NAS过程。NAS[26](#fn26)通过三个相互关联的阶段运作：定义搜索空间（架构组件和约束）、应用搜索策略（强化学习([Zoph
    and Le 2017a](ch058.xhtml#ref-zoph2017neural))、进化算法或基于梯度的方法）来探索候选架构，并评估性能以确保发现的设计满足准确性和效率目标。这种自动化使得发现的新架构通常与人类设计的模型相匹配或超过它们，同时需要显著减少专家的努力。'
- en: '![](../media/file158.svg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file158.svg)'
- en: 'Figure 10.12: **Neural Architecture Search Flow**: Automated NAS techniques
    iteratively refine model architectures and their weights, jointly optimizing for
    performance and efficiency, a departure from manual design approaches that rely
    on human expertise and extensive trial-and-error. This process enables the discovery
    of novel, high-performing architectures tailored to specific computational constraints.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12：**神经架构搜索流程**：自动化的NAS技术迭代优化模型架构及其权重，共同优化性能和效率，与依赖人类专业知识和大量试错的手动设计方法不同。这个过程能够发现针对特定计算约束的新颖、高性能架构。
- en: NAS search strategies employ diverse optimization techniques. Reinforcement
    learning[27](#fn27) treats architecture selection as a sequential decision problem,
    using accuracy as reward signal. Evolutionary algorithms[28](#fn28) evolve populations
    of architectures through mutation and crossover. Gradient-based methods enable
    differentiable architecture search, reducing computational cost.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: NAS搜索策略采用多种优化技术。强化学习[27](#fn27)将架构选择视为一个序列决策问题，使用准确度作为奖励信号。进化算法[28](#fn28)通过变异和交叉进化架构群体。基于梯度的方法使可微架构搜索成为可能，从而降低计算成本。
- en: Model Efficiency Encoding
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型效率编码
- en: 'NAS operates in three key stages: defining the search space, exploring candidate
    architectures, and evaluating their performance. The search space defines the
    architectural components and constraints that NAS can modify. The search strategy
    determines how NAS explores possible architectures, selecting promising candidates
    based on past observations. The evaluation process ensures that the discovered
    architectures satisfy multiple objectives, including accuracy, efficiency, and
    hardware suitability.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: NAS在三个关键阶段运作：定义搜索空间、探索候选架构和评估其性能。搜索空间定义了NAS可以修改的架构组件和约束。搜索策略确定NAS如何探索可能的架构，根据过去的观察选择有希望的候选者。评估过程确保发现架构满足多个目标，包括准确度、效率和硬件适应性。
- en: 'Search Space Definition: This stage establishes the architectural components
    and constraints NAS can modify, such as the number of layers, convolution types,
    activation functions, and hardware-specific optimizations. A well-defined search
    space balances innovation with computational feasibility.'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索空间定义：这个阶段建立了NAS可以修改的架构组件和约束，例如层数、卷积类型、激活函数和针对特定硬件的优化。一个定义良好的搜索空间在创新与计算可行性之间取得平衡。
- en: 'Search Strategy: NAS explores the search space using methods such as reinforcement
    learning, evolutionary algorithms, or gradient-based techniques. These approaches
    guide the search toward architectures that maximize performance while meeting
    resource constraints.'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索策略：NAS使用强化学习、进化算法或基于梯度的技术等方法来探索搜索空间。这些方法引导搜索向最大化性能的同时满足资源约束的架构。
- en: 'Evaluation Criteria: Candidate architectures are assessed based on multiple
    metrics, including accuracy, FLOPs, memory consumption, inference latency, and
    power efficiency. NAS ensures that the selected architectures align with deployment
    requirements.'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估标准：候选架构基于多个指标进行评估，包括准确性、FLOPs、内存消耗、推理延迟和能效。NAS确保所选架构与部署需求相一致。
- en: NAS unifies structural design and optimization into a singular, automated framework.
    The result is the discovery of architectures that are not only highly accurate
    but also computationally efficient and well-suited for target hardware platforms.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: NAS将结构设计和优化统一到一个单一的自动化框架中。结果是发现不仅高度准确，而且计算效率高且非常适合目标硬件平台的架构。
- en: Search Space Definition
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 搜索空间定义
- en: The first step in NAS is determining the set of architectures it is allowed
    to explore, known as the search space. The size and structure of this space directly
    affect how efficiently NAS can discover optimal models. A well-defined search
    space must be broad enough to allow innovation while remaining narrow enough to
    prevent unnecessary computation on impractical designs.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: NAS的第一步是确定它允许探索的架构集合，即搜索空间。这个空间的大小和结构直接影响NAS发现最优模型的有效性。一个定义良好的搜索空间必须足够广泛以允许创新，同时又要足够狭窄以避免对不切实际的设计进行不必要的计算。
- en: A typical NAS search space consists of modular building blocks that define the
    structure of the model. These include the types of layers available for selection,
    such as standard convolutions, depthwise separable convolutions, attention mechanisms,
    and residual blocks. The search space also defines constraints on network depth
    and width, specifying how many layers the model can have and how many channels
    each layer should include. NAS considers activation functions, such as ReLU, Swish,
    or GELU, which influence both model expressiveness and computational efficiency.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的NAS搜索空间由定义模型结构的模块化构建块组成。这些包括可供选择的层类型，如标准卷积、深度可分离卷积、注意力机制和残差块。搜索空间还定义了网络深度和宽度的约束，指定模型可以有多少层以及每层应包含多少通道。NAS还考虑了激活函数，如ReLU、Swish或GELU，这些函数会影响模型的表达能力和计算效率。
- en: Other architectural decisions within the search space include kernel sizes,
    receptive fields, and skip connections, which impact both feature extraction and
    model complexity. Some NAS implementations also incorporate hardware-aware optimizations,
    ensuring that the discovered architectures align with specific hardware, such
    as GPUs, TPUs, or mobile CPUs.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间内的其他架构决策包括内核大小、感受野和跳跃连接，这些都会影响特征提取和模型复杂度。一些NAS实现还结合了硬件感知优化，确保发现的架构与特定的硬件（如GPU、TPU或移动CPU）相匹配。
- en: The choice of search space determines the extent to which NAS can optimize a
    model. If the space is too constrained, the search algorithm may fail to discover
    novel and efficient architectures. If it is too large, the search becomes computationally
    expensive, requiring extensive resources to explore a vast number of possibilities.
    Striking the right balance ensures that NAS can efficiently identify architectures
    that improve upon human-designed models.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间的选择决定了NAS能够优化模型的程度。如果空间过于受限，搜索算法可能无法发现新颖且高效的架构。如果空间过大，搜索将变得计算成本高昂，需要大量资源来探索大量的可能性。找到合适的平衡点可以确保NAS能够高效地识别出优于人工设计的架构。
- en: Search Space Exploration
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 搜索空间探索
- en: Once the search space is defined, NAS must determine how to explore different
    architectures effectively. The search strategy guides this process by selecting
    which architectures to evaluate based on past observations. An effective search
    strategy must balance exploration (testing new architectures) with exploitation
    (refining promising designs).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了搜索空间，NAS必须确定如何有效地探索不同的架构。搜索策略通过根据过去的观察选择要评估的架构来指导这一过程。一个有效的搜索策略必须在探索（测试新的架构）与利用（完善有希望的设计）之间取得平衡。
- en: Several methods have been developed to explore the search space efficiently.
    Reinforcement learning-based NAS formulates the search process as a decision-making
    problem, where an agent sequentially selects architectural components and receives
    a reward signal based on the performance of the generated model. Over time, the
    agent learns to generate better architectures by maximizing this reward. While
    effective, reinforcement learning-based NAS can be computationally expensive because
    it requires training many candidate models before converging on an optimal design.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发出几种方法来有效地探索搜索空间。基于强化学习的NAS将搜索过程表述为一个决策问题，其中代理依次选择架构组件，并根据生成的模型性能接收奖励信号。随着时间的推移，代理通过最大化这一奖励来学习生成更好的架构。虽然有效，但基于强化学习的NAS可能计算成本较高，因为它需要在收敛到最佳设计之前训练许多候选模型。
- en: An alternative approach uses evolutionary algorithms, which maintain a population
    of candidate architectures and iteratively improve them through mutation and selection.
    Stronger architectures, which possess higher accuracy and efficiency, are retained,
    while modifications such as changing layer types or filter sizes introduce new
    variations. This approach has been shown to balance exploration and computational
    feasibility more effectively than reinforcement learning-based NAS.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用进化算法，它维护一个候选架构种群，并通过变异和选择迭代地改进它们。具有更高精度和效率的强大架构被保留，而诸如更改层类型或滤波器大小等修改则引入了新的变体。这种方法已被证明比基于强化学习的NAS更有效地平衡了探索和计算可行性。
- en: More recent methods, such as gradient-based NAS, introduce differentiable parameters
    that represent architectural choices. Instead of treating architectures as discrete
    entities, gradient-based methods optimize both model weights and architectural
    parameters simultaneously using standard gradient descent. This significantly
    reduces the computational cost of the search, making NAS more practical for real-world
    applications.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的方法，如基于梯度的NAS，引入了表示架构选择的可微分参数。与将架构视为离散实体不同，基于梯度的方 法同时使用标准梯度下降优化模型权重和架构参数。这显著降低了搜索的计算成本，使NAS在现实世界应用中更加实用。
- en: The choice of search strategy has a direct impact on the feasibility of NAS.
    Early NAS methods that relied on reinforcement learning required weeks of GPU
    computation to discover a single architecture. More recent methods, particularly
    those based on gradient-based search, have significantly reduced this cost, making
    NAS more efficient and accessible.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索策略的选择对NAS（神经架构搜索）的可行性有直接影响。早期依赖强化学习的NAS方法需要数周的GPU计算才能发现一个单一架构。最近的方法，尤其是基于梯度搜索的方法，显著降低了这一成本，使NAS更加高效和易于访问。
- en: Candidate Architecture Evaluation
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 候选架构评估
- en: Every architecture explored by NAS must be evaluated based on a set of predefined
    criteria. While accuracy is a core metric, NAS also optimizes for efficiency constraints
    to ensure that models are practical for deployment. The evaluation process determines
    whether an architecture should be retained for further refinement or discarded
    in favor of more promising designs.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: NAS探索的每一个架构都必须根据一组预定义的标准进行评估。虽然准确性是一个核心指标，但NAS还针对效率约束进行优化，以确保模型在实际部署中是实用的。评估过程决定了架构是否应该保留以进行进一步优化，或者为了更有希望的设计而被舍弃。
- en: The primary evaluation metrics include computational complexity, memory consumption,
    inference latency, and energy efficiency[29](#fn29). Computational complexity,
    often measured in FLOPs, determines the overall resource demands of a model. NAS
    favors architectures that achieve high accuracy while reducing unnecessary computations.
    Memory consumption, which includes both parameter count and activation storage,
    ensures that models fit within hardware constraints. For real-time applications,
    inference latency is a key factor, with NAS selecting architectures that minimize
    execution time on specific hardware platforms. Finally, some NAS implementations
    explicitly optimize for power consumption, ensuring that models are suitable for
    mobile and edge devices.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 主要评估指标包括计算复杂度、内存消耗、推理延迟和能效[29](#fn29)。计算复杂度，通常以FLOPs衡量，决定了模型的总体资源需求。NAS倾向于选择在提高准确率的同时减少不必要的计算的架构。内存消耗，包括参数数量和激活存储，确保模型符合硬件约束。对于实时应用，推理延迟是一个关键因素，NAS会选择在特定硬件平台上最小化执行时间的架构。最后，一些NAS实现明确优化功耗，确保模型适用于移动和边缘设备。
- en: For example, FBNet[30](#fn30), a NAS-generated architecture optimized for mobile
    inference, incorporated latency constraints into the search process.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，FBNet[30](#fn30)，这是一个为移动推理优化的NAS生成架构，将延迟约束纳入了搜索过程。
- en: By integrating these constraints into the search process, NAS systematically
    discovers architectures that balance accuracy, efficiency, and hardware adaptability.
    Instead of manually fine-tuning these trade-offs, NAS automates the selection
    of optimal architectures, ensuring that models are well-suited for real-world
    deployment scenarios.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这些约束整合到搜索过程中，NAS系统地发现能够平衡准确率、效率和硬件适应性的架构。而不是手动微调这些权衡，NAS自动化了最优架构的选择，确保模型适合现实世界的部署场景。
- en: The NAS Optimization Problem
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NAS优化问题
- en: Neural Architecture Search can be formulated as a bi-level optimization problem
    that simultaneously searches for the optimal architecture while evaluating its
    performance. The outer loop searches the architecture space, while the inner loop
    trains candidate architectures to measure their quality.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构搜索可以表述为一个双层优化问题，它同时搜索最优架构并评估其性能。外层循环搜索架构空间，而内层循环训练候选架构以衡量其质量。
- en: 'Formally, NAS seeks to find the optimal architecture <semantics><msup><mi>α</mi><mo>*</mo></msup><annotation
    encoding="application/x-tex">\alpha^*</annotation></semantics> from a search space
    <semantics><mi>𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics>
    that minimizes validation loss <semantics><msub><mi>ℒ</mi><mtext mathvariant="normal">val</mtext></msub><annotation
    encoding="application/x-tex">\mathcal{L}_{\text{val}}</annotation></semantics>
    while respecting deployment constraints <semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>
    (latency, memory, energy):'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，NAS旨在从搜索空间<semantics><mi>𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics>中找到最优架构<semantics><msup><mi>α</mi><mo>*</mo></msup><annotation
    encoding="application/x-tex">\alpha^*</annotation></semantics>，该搜索空间最小化验证损失<semantics><msub><mi>ℒ</mi><mtext
    mathvariant="normal">val</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{val}}</annotation></semantics>，同时遵守部署约束<semantics><mi>C</mi><annotation
    encoding="application/x-tex">C</annotation></semantics>（延迟、内存、能耗）：
- en: <semantics><mrow><msup><mi>α</mi><mo>*</mo></msup><mo>=</mo><mo>arg</mo><munder><mo>min</mo><mrow><mi>α</mi><mo>∈</mo><mi>𝒜</mi></mrow></munder><msub><mi>ℒ</mi><mtext
    mathvariant="normal">val</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>w</mi><mo>*</mo></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>α</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mtext mathvariant="normal">subject
    to</mtext><mi>C</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><msub><mi>C</mi><mtext mathvariant="normal">max</mtext></msub></mrow>
    <annotation encoding="application/x-tex">\alpha^* = \arg\min_{\alpha \in \mathcal{A}}
    \mathcal{L}_{\text{val}}(w^*(\alpha), \alpha) \quad \text{subject to} \quad C(\alpha)
    \leq C_{\text{max}}</annotation></semantics>
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msup><mi>α</mi><mo>*</mo></msup><mo>=</mo><mo>arg</mo><munder><mo>min</mo><mrow><mi>α</mi><mo>∈</mo><mi>𝒜</mi></mrow></munder><msub><mi>ℒ</mi><mtext
    mathvariant="normal">val</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>w</mi><mo>*</mo></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>α</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mtext mathvariant="normal">subject
    to</mtext><mi>C</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><msub><mi>C</mi><mtext mathvariant="normal">max</mtext></msub></mrow>
    <annotation encoding="application/x-tex">\alpha^* = \arg\min_{\alpha \in \mathcal{A}}
    \mathcal{L}_{\text{val}}(w^*(\alpha), \alpha) \quad \text{subject to} \quad C(\alpha)
    \leq C_{\text{max}}</annotation></semantics>
- en: 'where <semantics><mrow><msup><mi>w</mi><mo>*</mo></msup><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">w^*(\alpha)</annotation></semantics> represents the
    optimal weights for architecture <semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>,
    obtained by minimizing training loss:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <semantics><mrow><msup><mi>w</mi><mo>*</mo></msup><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">w^*(\alpha)</annotation></semantics> 代表了架构 <semantics><mi>α</mi><annotation
    encoding="application/x-tex">\alpha</annotation></semantics> 的最优权重，通过最小化训练损失获得：
- en: <semantics><mrow><msup><mi>w</mi><mo>*</mo></msup><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>arg</mo><munder><mo>min</mo><mi>w</mi></munder><msub><mi>ℒ</mi><mtext
    mathvariant="normal">train</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo>,</mo><mi>α</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">w^*(\alpha)
    = \arg\min_{w} \mathcal{L}_{\text{train}}(w, \alpha)</annotation></semantics>
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msup><mi>w</mi><mo>*</mo></msup><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>arg</mo><munder><mo>min</mo><mi>w</mi></munder><msub><mi>ℒ</mi><mtext
    mathvariant="normal">train</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo>,</mo><mi>α</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">w^*(\alpha)
    = \arg\min_{w} \mathcal{L}_{\text{train}}(w, \alpha)</annotation></semantics>
- en: 'This formulation reveals the core challenge of NAS: evaluating each candidate
    architecture requires expensive training to convergence, making exhaustive search
    infeasible. A search space with just 10 design choices per layer across 20 layers
    yields <semantics><msup><mn>10</mn><mn>20</mn></msup><annotation encoding="application/x-tex">10^{20}</annotation></semantics>
    possible architectures. Training each for 100 epochs would require millions of
    GPU-years. Efficient NAS methods address this challenge through three key design
    decisions: defining a tractable search space, employing efficient search strategies,
    and accelerating architecture evaluation.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这种公式揭示了NAS的核心挑战：评估每个候选架构都需要昂贵的训练以达到收敛，这使得穷举搜索变得不可行。一个每层有10个设计选择的搜索空间，在20层的情况下会产生
    <semantics><msup><mn>10</mn><mn>20</mn></msup><annotation encoding="application/x-tex">10^{20}</annotation></semantics>
    种可能的架构。对每种架构进行100个epoch的训练将需要数百万GPU年。高效的NAS方法通过三个关键的设计决策来应对这一挑战：定义可处理的搜索空间、采用高效的搜索策略和加速架构评估。
- en: Search Space Design
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 搜索空间设计
- en: The search space defines what architectures NAS can discover. Well-designed
    search spaces incorporate domain knowledge to focus search on promising regions
    while remaining flexible enough to discover novel patterns.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间定义了NAS可以发现的架构。设计良好的搜索空间结合领域知识，将搜索集中在有希望的区域内，同时保持足够的灵活性以发现新颖的模式。
- en: '**Cell-Based Search Spaces**'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于Cell的搜索空间**'
- en: Rather than searching entire network architectures, cell-based NAS searches
    for reusable computational blocks (cells) that can be stacked to form complete
    networks. For example, a convolutional cell might choose from operations like
    3×3 convolution, 5×5 convolution, depthwise separable convolution, max pooling,
    or identity connections. A simplified cell with 4 nodes and 2 operations per edge
    yields roughly 10,000 possible cell designs, far more tractable than searching
    full architectures. EfficientNet uses this approach to discover scalable cell
    designs that generalize across different model sizes.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 与搜索整个网络架构不同，基于单元的NAS搜索可重用计算块（单元），这些块可以堆叠形成完整的网络。例如，一个卷积单元可能从3×3卷积、5×5卷积、深度可分离卷积、最大池化或恒等连接等操作中选择。具有4个节点和每条边2个操作的简化单元大约有10,000种可能的设计，这比搜索完整架构要容易得多。EfficientNet使用这种方法来发现可扩展的单元设计，这些设计可以推广到不同的模型大小。
- en: '**Hardware-Aware Search Spaces**'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '**硬件感知搜索空间**'
- en: Hardware-aware NAS extends search spaces to include deployment constraints as
    first-class objectives. Rather than optimizing solely for accuracy and FLOPs,
    the search explicitly minimizes actual latency on target hardware (mobile CPUs,
    GPUs, edge accelerators). MobileNetV3’s search space includes a latency prediction
    model that estimates inference time for each candidate architecture on Pixel phones
    without actually deploying them. This hardware-in-the-loop approach ensures discovered
    architectures run efficiently on real devices rather than just achieving low theoretical
    FLOP counts.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件感知NAS将搜索空间扩展到包括部署约束作为首要目标。而不是仅仅为了优化准确性和FLOPs，搜索明确地最小化在目标硬件（移动CPU、GPU、边缘加速器）上的实际延迟。MobileNetV3的搜索空间包括一个延迟预测模型，该模型估计每个候选架构在Pixel手机上的推理时间，而无需实际部署它们。这种硬件在环方法确保发现的架构在真实设备上运行效率高，而不仅仅是实现低理论FLOP计数。
- en: Search Strategies
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 搜索策略
- en: Search strategies determine how to navigate the architecture space efficiently
    without exhaustive enumeration. Different strategies make different trade-offs
    between search cost, architectural diversity, and optimality guarantees, as summarized
    in [Table 10.6](ch016.xhtml#tbl-nas-strategies).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索策略决定了如何有效地在架构空间中导航，而不进行穷举枚举。不同的策略在搜索成本、架构多样性和最优性保证之间做出不同的权衡，如[表10.6](ch016.xhtml#tbl-nas-strategies)中总结。
- en: 'Table 10.6: **NAS Search Strategy Comparison**: Trade-offs between search efficiency,
    use cases, and limitations for different NAS approaches. Reinforcement learning
    offers unconstrained exploration at high cost, evolutionary methods leverage parallelism,
    and gradient-based approaches achieve dramatic speedups with potential optimality
    trade-offs.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.6：**NAS搜索策略比较**：不同NAS方法在搜索效率、用例和限制之间的权衡。强化学习以高成本提供无约束的探索，进化方法利用并行性，基于梯度的方法通过潜在的优化权衡实现显著加速。
- en: '| **Strategy** | **Search Efficiency** | **When to Use** | **Key Challenge**
    |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| **策略** | **搜索效率** | **何时使用** | **主要挑战** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Reinforcement Learning | 400-1000 GPU-days | Novel domains, unconstrained
    search | High computational cost |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 强化学习 | 400-1000 GPU天 | 新领域，无约束搜索 | 高计算成本 |'
- en: '| Evolutionary Algorithms | 200-500 GPU-days | Parallel infrastructure available
    | Requires large populations |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 进化算法 | 200-500 GPU天 | 可用并行基础设施 | 需要大量种群 |'
- en: '| Gradient-Based (DARTS) | 1-4 GPU-days | Limited compute budget | May converge
    to suboptimal local minima |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 基于梯度的（DARTS） | 1-4 GPU天 | 计算预算有限 | 可能收敛到次优局部最小值 |'
- en: Reinforcement learning based NAS treats architecture search as a sequential
    decision problem where a controller generates architectures and receives accuracy
    as reward. The controller (typically an LSTM) learns to propose better architectures
    over time through policy gradient optimization. While this approach discovered
    groundbreaking architectures like NASNet, the sequential nature limits parallelism
    and requires hundreds of GPU-days.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 基于强化学习的NAS将架构搜索视为一个序列决策问题，其中控制器生成架构并接收准确性作为奖励。控制器（通常是LSTM）通过策略梯度优化学习在时间上提出更好的架构。虽然这种方法发现了像NASNet这样的突破性架构，但其序列性质限制了并行性，需要数百个GPU天。
- en: Evolutionary algorithms maintain a population of candidate architectures and
    iteratively apply mutations (changing operations, adding connections) and crossover
    (combining parent architectures) to generate offspring. Fitness-based selection
    retains high-performing architectures for the next generation. AmoebaNet used
    evolution to achieve state-of-the-art results, with massive parallelism amortizing
    the cost across thousands of workers.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算法维护一个候选架构的种群，并通过迭代应用突变（改变操作，添加连接）和交叉（结合父架构）来生成后代。基于适应度的选择保留高性能架构用于下一代。AmoebaNet利用进化实现了最先进的成果，通过大规模并行化将成本分摊到数千个工作者身上。
- en: Gradient-based methods like DARTS (Differentiable Architecture Search) represent
    the search space as a continuous relaxation where all possible operations are
    weighted combinations. Rather than discrete sampling, DARTS optimizes architecture
    weights and model weights jointly using gradient descent. By making the search
    differentiable, DARTS reduces search cost from hundreds to just 1-4 GPU-days,
    though the continuous relaxation may miss discrete architectural patterns that
    discrete search methods discover.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的方法，如DARTS（可微分架构搜索），将搜索空间表示为一种连续松弛，其中所有可能的操作都是加权组合。DARTS不同于离散采样，它通过梯度下降联合优化架构权重和模型权重。通过使搜索可微分，DARTS将搜索成本从数百个GPU天减少到仅1-4个GPU天，尽管连续松弛可能会错过离散搜索方法发现的离散架构模式。
- en: NAS in Practice
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实践中的NAS
- en: 'Hardware-aware NAS moves beyond FLOPs as a proxy for efficiency, directly optimizing
    for actual deployment metrics. MnasNet’s search incorporates a latency prediction
    model trained on thousands of architecture-latency pairs measured on actual mobile
    phones. The search objective combines accuracy and latency through a weighted
    product:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件感知的NAS超越了FLOPs作为效率代理的角色，直接优化实际部署指标。MnasNet的搜索结合了一个在真实手机上测量的数千个架构-延迟对上训练的延迟预测模型。搜索目标通过加权乘积结合准确性和延迟：
- en: <semantics><mrow><mtext mathvariant="normal">Reward</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext
    mathvariant="normal">Accuracy</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><msup><mrow><mo stretchy="true"
    form="prefix">(</mo><mfrac><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><msub><mi>L</mi><mtext mathvariant="normal">target</mtext></msub></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>β</mi></msup></mrow> <annotation
    encoding="application/x-tex">\text{Reward}(\alpha) = \text{Accuracy}(\alpha) \times
    \left(\frac{L(\alpha)}{L_{\text{target}}}\right)^\beta</annotation></semantics>
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mtext mathvariant="normal">奖励</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext
    mathvariant="normal">准确度</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><msup><mrow><mo stretchy="true"
    form="prefix">(</mo><mfrac><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><msub><mi>L</mi><mtext mathvariant="normal">目标</mtext></msub></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>β</mi></msup></mrow> <annotation
    encoding="application/x-tex">\text{Reward}(\alpha) = \text{Accuracy}(\alpha) \times
    \left(\frac{L(\alpha)}{L_{\text{target}}}\right)^\beta</annotation></semantics>
- en: where <semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\alpha)</annotation></semantics>
    is measured latency, <semantics><msub><mi>L</mi><mtext mathvariant="normal">target</mtext></msub><annotation
    encoding="application/x-tex">L_{\text{target}}</annotation></semantics> is the
    latency constraint, and <semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>
    controls the accuracy-latency trade-off. This formulation penalizes architectures
    that exceed latency targets while rewarding those that achieve high accuracy within
    the budget. MnasNet discovered that inverted residuals with varying expansion
    ratios achieve better accuracy-latency trade-offs than uniform expansion, a design
    insight that manual exploration likely would have missed.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，<semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\alpha)</annotation></semantics>表示测量的延迟，<semantics><msub><mi>L</mi><mtext
    mathvariant="normal">target</mtext></msub><annotation encoding="application/x-tex">L_{\text{target}}</annotation></semantics>是延迟约束，而<semantics><mi>β</mi><annotation
    encoding="application/x-tex">\beta</annotation></semantics>控制精度-延迟权衡。这种公式惩罚超过延迟目标的架构，同时奖励在预算内实现高精度的架构。MnasNet发现，具有可变扩张比的倒残差比均匀扩张实现了更好的精度-延迟权衡，这是一个手动探索可能错过的设计洞察。
- en: When to Use NAS
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 何时使用NAS
- en: Neural Architecture Search is a powerful tool, but its significant computational
    cost demands careful consideration of when the investment is justified.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构搜索是一个强大的工具，但其显著的计算成本要求仔细考虑何时进行投资是合理的。
- en: NAS becomes worthwhile when dealing with novel hardware platforms with unique
    constraints (new accelerator architectures, extreme edge devices) where existing
    architectures are poorly optimized. It also makes sense for deployment at massive
    scale (billions of inferences) where even 1-2% efficiency improvements justify
    the upfront search cost, or when multiple deployment configurations require architecture
    families (cloud, edge, mobile) that can amortize one search across many variants.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理具有独特约束（如新的加速器架构、极端边缘设备）的新型硬件平台时，NAS变得有意义，在这些平台上，现有的架构优化不佳。在需要大规模部署（数十亿推理）的情况下，即使1-2%的效率提升也能证明前期搜索成本是合理的，或者当需要多个部署配置（云、边缘、移动）且这些配置可以分摊到许多变体上时，这也同样合理。
- en: Conversely, avoid NAS when working with standard deployment constraints (e.g.,
    ResNet-50 accuracy on NVIDIA GPUs) where well-optimized architectures already
    exist. Similarly, if the compute budget is limited (less than 100 GPU-days available),
    even efficient NAS methods like DARTS become infeasible. Rapidly changing requirements
    also make NAS impractical, as architecture selection may become obsolete before
    the search completes.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当与标准部署约束（例如，在NVIDIA GPU上实现ResNet-50精度）工作时，应避免使用NAS，因为已经存在经过良好优化的架构。同样，如果计算预算有限（可用的GPU天数少于100天），即使是高效的NAS方法如DARTS也变得不可行。快速变化的需求也使得NAS不切实际，因为架构选择可能在搜索完成之前就过时了。
- en: For most practitioners, starting with existing NAS-discovered architectures
    (EfficientNet, MobileNetV3, MnasNet) provides better ROI than running NAS from
    scratch. These architectures are highly tuned and generalize well across tasks.
    Reserve custom NAS for scenarios with truly novel constraints or deployment scales
    that justify the investment.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数从业者来说，从现有的NAS发现的架构（EfficientNet、MobileNetV3、MnasNet）开始，比从头开始运行NAS提供更好的投资回报率。这些架构经过高度调优，在多个任务上具有良好的泛化能力。将定制的NAS保留在具有真正新颖约束或部署规模可以证明投资合理性的场景中。
- en: Architecture Examples
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 架构示例
- en: NAS has been successfully used to design several state-of-the-art architectures
    that outperform manually designed models in terms of efficiency and accuracy.
    These architectures illustrate how NAS integrates scaling optimization, computation
    reduction, memory efficiency, and hardware-aware design into an automated process.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: NAS已被成功用于设计几个最先进的架构，这些架构在效率和精度方面优于手动设计的模型。这些架构展示了NAS如何将缩放优化、计算减少、内存效率和硬件感知设计整合到自动化流程中。
- en: One of the most well-known NAS-generated models is EfficientNet, which was discovered
    using a NAS framework that searched for the most effective combination of depth,
    width, and resolution scaling. Unlike traditional scaling strategies that independently
    adjust these factors, NAS optimized the model using compound scaling, which applies
    a fixed set of scaling coefficients to ensure that the network grows in a balanced
    way. EfficientNet achieves higher accuracy with fewer parameters and lower FLOPs
    than previous architectures, making it ideal for both cloud and mobile deployment.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的 NAS 生成的模型之一是 EfficientNet，它是通过一个 NAS 框架发现的，该框架搜索深度、宽度和分辨率缩放的最有效组合。与独立调整这些因素的常规缩放策略不同，NAS
    使用复合缩放优化模型，该缩放将一组固定的缩放系数应用于网络，以确保网络以平衡的方式增长。EfficientNet 与之前的架构相比，具有更高的准确性和更少的参数以及更低的
    FLOPs，使其非常适合云和移动部署。
- en: Another key example is MobileNetV3, which used NAS to optimize its network structure
    for mobile hardware. The search process led to the discovery of inverted residual
    blocks with squeeze-and-excitation layers, which improve accuracy while reducing
    computational cost. NAS also selected optimized activation functions and efficient
    depthwise separable convolutions, leading to a <semantics><mrow><mn>5</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">5\times</annotation></semantics> reduction in FLOPs
    compared to earlier MobileNet versions.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键例子是 MobileNetV3，它使用 NAS 优化了其网络结构以适应移动硬件。搜索过程导致了具有挤压和激励层的倒残差块被发现，这些块在降低计算成本的同时提高了准确性。NAS
    还选择了优化的激活函数和高效的深度可分离卷积，与早期的 MobileNet 版本相比，FLOPs 减少了 <semantics><mrow><mn>5</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">5\times</annotation></semantics>。
- en: FBNet, another NAS-generated model, was specifically optimized for real-time
    inference on mobile CPUs. Unlike architectures designed for general-purpose acceleration,
    FBNet’s search process explicitly considered latency constraints during training,
    ensuring that the final model runs efficiently on low-power hardware. Similar
    approaches have been used in TPU-optimized NAS models, where the search process
    is guided by hardware-aware cost functions to maximize parallel execution efficiency.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: FBNet 是另一个由 NAS 生成的模型，专门针对移动 CPU 上的实时推理进行了优化。与为通用加速设计的架构不同，FBNet 的搜索过程在训练期间明确考虑了延迟约束，确保最终模型在低功耗硬件上高效运行。类似的方法也用于
    TPU 优化的 NAS 模型，其中搜索过程由硬件感知的成本函数引导，以最大化并行执行效率。
- en: NAS has also been applied beyond convolutional networks. NAS-BERT explores transformer-based
    architectures, searching for efficient model structures that retain strong natural
    language understanding capabilities while reducing compute and memory overhead.
    NAS has been particularly useful in designing efficient vision transformers (ViTs)
    by automatically discovering lightweight attention mechanisms tailored for edge
    AI applications.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: NAS 也已被应用于卷积网络之外。NAS-BERT 探索基于 Transformer 的架构，寻找在减少计算和内存开销的同时保持强大自然语言理解能力的有效模型结构。NAS
    在设计高效的视觉 Transformer（ViTs）方面特别有用，通过自动发现针对边缘 AI 应用量身定制的轻量级注意力机制。
- en: Each of these NAS-generated models demonstrates how automated architecture search
    can uncover novel efficiency trade-offs that may not be immediately intuitive
    to human designers. Explicit encoding of efficiency constraints into the search
    process enables NAS to systematically produce architectures that are more computationally
    efficient, memory-friendly, and hardware-adapted than those designed manually
    ([Radosavovic et al. 2020](ch058.xhtml#ref-radosavovic2020designing)).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这些由 NAS 生成的每个模型都展示了自动化架构搜索如何揭示人类设计师可能不会立即直观的新的效率权衡。将效率约束显式编码到搜索过程中，使 NAS 能够系统地产生比手动设计的更计算高效、内存友好和硬件适应的架构
    ([Radosavovic 等人 2020](ch058.xhtml#ref-radosavovic2020designing))。
- en: 'Model representation optimization has delivered substantial improvements. Through
    structured pruning and knowledge distillation, we transformed a 440MB BERT-Base
    model ([Devlin et al. 2018b](ch058.xhtml#ref-devlin2018bert)) into a 110MB variant,
    decreasing memory footprint by 75% with only 0.8% accuracy loss. The pruned model
    eliminates 40% of attention heads and intermediate dimensions, significantly reducing
    parameter count. This success creates a natural question: with the model structurally
    optimized, why does mobile deployment still fail to meet our 50ms latency target,
    consistently running at 120ms?'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 模型表示优化已经带来了实质性的改进。通过结构剪枝和知识蒸馏，我们将440MB的BERT-Base模型（[Devlin等人2018b](ch058.xhtml#ref-devlin2018bert)）转换成了110MB的变体，内存占用减少了75%，只损失了0.8%的准确度。剪枝模型消除了40%的注意力头和中间维度，显著减少了参数数量。这一成功引发了一个自然的问题：在模型结构优化后，为什么移动部署仍然无法达到我们的50ms延迟目标，持续运行在120ms？
- en: 'Profiling reveals the answer. While we eliminated 75% of parameters, each remaining
    matrix multiplication still uses 32-bit floating-point operations (FP32). The
    27.5 million remaining parameters consume excessive memory bandwidth: loading
    weights from DRAM to compute units dominates execution time. The model structure
    is optimized, but numerical representation is not. Each parameter occupies 4 bytes,
    and limited mobile memory bandwidth (25-35 GB/s versus 900 GB/s on server GPUs)
    creates a bottleneck that structural optimization alone cannot resolve.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 分析揭示了答案。虽然我们消除了75%的参数，但每个剩余的矩阵乘法仍然使用32位浮点运算（FP32）。剩余的2750万个参数消耗了过多的内存带宽：从DRAM加载权重到计算单元主导了执行时间。模型结构已优化，但数值表示未优化。每个参数占用4字节，有限的移动内存带宽（25-35
    GB/s与服务器GPU上的900 GB/s相比）创建了一个瓶颈，仅结构优化无法解决。
- en: This illustrates why model representation optimization represents only the first
    dimension of a comprehensive efficiency strategy. Representation techniques modify
    what computations are performed (which operations, which parameters, which layers
    execute). Numerical precision optimization, the second dimension, changes how
    those computations are executed by reducing the numerical fidelity of weights,
    activations, and arithmetic operations. Moving from 32-bit to 8-bit representations
    reduces memory traffic by 4x and enables specialized integer arithmetic units
    that execute 4-8x faster than floating-point equivalents on mobile processors.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了为什么模型表示优化只代表全面效率策略的第一个维度。表示技术修改了执行的计算（哪些操作、哪些参数、哪些层执行）。数值精度优化，第二个维度，通过降低权重、激活和算术运算的数值保真度来改变这些计算的执行方式。从32位到8位表示的转换将内存流量减少了4倍，并使移动处理器上执行速度比浮点等效值快4-8倍的专用整数算术单元成为可能。
- en: 'These precision optimizations work synergistically with representation optimizations.
    The pruned 110MB BERT model, when further quantized to INT8 precision, shrinks
    to 28MB while inference latency drops to 45ms, finally meeting the deployment
    target. The quantization provides the missing piece: structural efficiency (fewer
    parameters) combined with numerical efficiency (lower precision per parameter)
    delivers compound benefits that neither technique achieves alone.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这些精度优化与表示优化协同工作。剪枝的110MB BERT模型，当进一步量化到INT8精度时，缩小到28MB，推理延迟降低到45ms，最终达到了部署目标。量化提供了缺失的部分：结构效率（更少的参数）与数值效率（每个参数的更低精度）相结合，带来了单一技术无法实现的复合效益。
- en: Quantization and Precision Optimization
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化与精度优化
- en: '***Quantization*** is a model compression technique that reduces *numerical
    precision* of weights and activations from floating-point to lower-bit representations,
    decreasing *model size* and *computational cost* with minimal accuracy loss.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '***量化***是一种模型压缩技术，通过将权重和激活的数值精度从浮点数降低到低比特表示，以最小的精度损失减少了*模型大小*和*计算成本*。'
- en: While model representation optimization determines what computations are performed,
    the efficiency of those computations depends critically on numerical precision—the
    second dimension of our optimization framework.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型表示优化确定执行的计算时，这些计算的效率关键取决于数值精度——我们优化框架的第二个维度。
- en: Numerical precision determines how weights and activations are represented during
    computation, directly affecting memory usage, computational efficiency, and power
    consumption. Many state-of-the-art models use high-precision floating-point formats
    like FP32 (32-bit floating point), which offer numerical stability and high accuracy
    ([S. Gupta et al. 2015](ch058.xhtml#ref-gupta2015deep)) but increase storage requirements,
    memory bandwidth usage, and power consumption. Modern AI accelerators include
    dedicated hardware for low-precision computation, allowing FP16 and INT8 operations
    to run at significantly higher throughput than FP32 ([Y. E. Wang, Wei, and Brooks
    2019](ch058.xhtml#ref-wang2019benchmarking)). Reducing precision introduces quantization
    error that can degrade accuracy, with tolerance depending on model architecture,
    dataset properties, and hardware support.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 数值精度决定了权重和激活在计算过程中的表示方式，直接影响内存使用、计算效率和功耗。许多最先进的模型使用高精度浮点格式，如FP32（32位浮点），这提供了数值稳定性和高精度（[S.
    Gupta等人 2015](ch058.xhtml#ref-gupta2015deep)），但增加了存储需求、内存带宽使用和功耗。现代AI加速器包括用于低精度计算的专用硬件，允许FP16和INT8操作以比FP32显著更高的吞吐量运行（[Y.
    E. Wang, Wei, 和 Brooks 2019](ch058.xhtml#ref-wang2019benchmarking)）。降低精度会引入量化误差，这可能会降低精度，其容忍度取决于模型架构、数据集属性和硬件支持。
- en: The relationship between precision reduction and system performance proves more
    complex than hardware specifications suggest. While aggressive precision reduction
    (e.g., INT8) can deliver impressive chip-level performance improvements (often
    4x higher TOPS compared to FP32), these micro-benchmarks may not translate to
    end-to-end system benefits. Ultra-low precision training often requires longer
    convergence times, complex mixed-precision orchestration, and sophisticated accuracy
    recovery techniques that can offset hardware speedups. Precision conversions between
    numerical formats introduce computational overhead and memory bandwidth pressure
    that chip-level benchmarks typically ignore. Balanced approaches, such as FP16
    mixed-precision training, often provide optimal compromise between hardware efficiency
    and training convergence, avoiding the systems-level complexity that accompanies
    more aggressive quantization strategies.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 精度降低与系统性能之间的关系比硬件规格所暗示的要复杂。虽然激进的精度降低（例如，INT8）可以带来令人印象深刻的芯片级性能提升（通常比FP32高4倍TOPS），但这些微基准测试可能不会转化为端到端系统的益处。超低精度训练通常需要更长的收敛时间、复杂的混合精度编排和复杂的精度恢复技术，这些技术可能会抵消硬件加速。在数值格式之间进行精度转换会引入计算开销和内存带宽压力，而芯片级基准测试通常忽略这些因素。平衡的方法，如FP16混合精度训练，通常在硬件效率和训练收敛之间提供最佳折衷，避免了更激进的量化策略带来的系统级复杂性。
- en: 'This section examines precision optimization techniques across three complexity
    tiers: post-training quantization for rapid deployment, quantization-aware training
    for production systems, and extreme quantization (binarization and ternarization)
    for resource-constrained environments. We explore trade-offs between precision
    formats, hardware-software co-design considerations, and methods for minimizing
    accuracy degradation while maximizing efficiency gains.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了跨越三个复杂级别的精度优化技术：用于快速部署的培训后量化、用于生产系统的量化感知训练，以及针对资源受限环境的极端量化（二值化和三值化）。我们探讨了精度格式之间的权衡、硬件-软件协同设计考虑因素，以及最小化精度下降同时最大化效率提升的方法。
- en: Precision and Energy
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精度和能源
- en: Efficient numerical representations enable significant reductions in storage
    requirements, computation latency, and power usage, making them particularly beneficial
    for mobile AI, embedded systems, and cloud inference. Precision levels can be
    tuned to specific hardware capabilities, maximizing throughput on AI accelerators
    such as GPUs, TPUs, NPUs, and edge AI chips.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的数值表示可以显著减少存储需求、计算延迟和功耗，对移动AI、嵌入式系统和云推理特别有益。精度级别可以根据特定硬件能力进行调整，以在AI加速器（如GPU、TPU、NPU和边缘AI芯片）上最大化吞吐量。
- en: Energy Costs
  id: totrans-406
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 能源成本
- en: Beyond computational and memory benefits, the energy costs associated with different
    numerical precisions further highlight the benefits of reducing precision. As
    shown in [Figure 10.13](ch016.xhtml#fig-quantized-energy), performing a 32-bit
    floating-point addition (FAdd) consumes approximately 0.9 pJ, whereas a 16-bit
    floating-point addition only requires 0.4 pJ. Similarly, a 32-bit integer addition
    costs 0.1 pJ, while an 8-bit integer addition is significantly lower at just 0.03
    pJ. These savings compound when considering large-scale models operating across
    billions of operations, supporting the sustainability goals outlined in [Chapter 18](ch024.xhtml#sec-sustainable-ai).
    The energy efficiency gained through quantization also enhances the security posture
    discussed in [Chapter 15](ch021.xhtml#sec-security-privacy) by reducing the computational
    resources available to potential attackers.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算和内存的好处之外，不同数值精度相关的能耗进一步突显了降低精度的益处。如图[图10.13](ch016.xhtml#fig-quantized-energy)所示，执行32位浮点数加法（FAdd）大约消耗0.9
    pJ，而16位浮点数加法只需要0.4 pJ。同样，32位整数加法需要0.1 pJ，而8位整数加法的能耗显著降低，仅为0.03 pJ。当考虑到在数十亿操作中运行的大型模型时，这些节省会累积起来，支持[第18章](ch024.xhtml#sec-sustainable-ai)中概述的可持续性目标。通过量化获得的能效也增强了[第15章](ch021.xhtml#sec-security-privacy)中讨论的安全态势，通过减少潜在攻击者可用的计算资源。
- en: '![](../media/file159.svg)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file159.svg)'
- en: 'Figure 10.13: **Energy Costs**: Lower precision reduces computational energy,
    illustrating trade-offs in model accuracy. Machine learning systems can optimize
    efficiency by reducing floating-point operations from 32-bit to 16-bit or even
    lower for significant savings. Source: IEEE spectrum.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13：**能耗**：降低精度减少了计算能耗，说明了模型精度之间的权衡。机器学习系统可以通过将浮点运算从32位降低到16位甚至更低位来优化效率，从而实现显著的节省。来源：IEEE
    spectrum.
- en: Beyond direct compute savings, reducing numerical precision has a significant
    impact on memory energy consumption, which often dominates total system power.
    Lower-precision representations reduce data storage requirements and memory bandwidth
    usage, leading to fewer and more efficient memory accesses. This is important
    because accessing memory, particularly off-chip DRAM, is far more energy-intensive
    than performing arithmetic operations. For instance, DRAM accesses require orders
    of magnitude more energy (1.3–2.6 nJ) compared to cache accesses (e.g., 10 pJ
    for an 8 KB L1 cache access). The breakdown of instruction energy underscores
    the cost of moving data within the memory hierarchy, where an instruction’s total
    energy can be significantly impacted by memory access patterns[31](#fn31).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直接的计算节省之外，降低数值精度对内存能耗的影响也很大，这通常占用了整个系统的功耗。低精度表示减少了数据存储需求和内存带宽使用，导致更少且更有效的内存访问。这很重要，因为访问内存，尤其是片外DRAM，比执行算术运算消耗的能量要大得多。例如，DRAM访问需要比缓存访问高几个数量级的能量（例如，8
    KB L1缓存访问需要10 pJ），而缓存访问只需要10 pJ。指令能耗的分解突显了在内存层次结构中移动数据所付出的代价，其中指令的总能耗可能会受到内存访问模式的影响[31](#fn31)。
- en: By reducing numerical precision, models can not only execute computations more
    efficiently but also reduce data movement, leading to lower overall energy consumption.
    This is particularly important for hardware accelerators and edge devices, where
    memory bandwidth and power efficiency are key constraints.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 通过降低数值精度，模型不仅可以更高效地执行计算，还可以减少数据移动，从而降低整体能耗。这对于硬件加速器和边缘设备尤为重要，在这些设备中，内存带宽和能效是关键约束。
- en: Performance Gains
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能提升
- en: '[Figure 10.14](ch016.xhtml#fig-quantization_impact) illustrates the impact
    of quantization on both inference time and model size using a stacked bar chart
    with a dual-axis representation. The left bars in each category show inference
    time improvements when moving from FP32 to INT8, while the right bars depict the
    corresponding reduction in model size. The results indicate that quantized models
    achieve up to <semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">4\times</annotation></semantics>
    faster inference while reducing storage requirements by a factor of <semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">4\times</annotation></semantics>, making them highly
    suitable for deployment in resource-constrained environments.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.14](ch016.xhtml#fig-quantization_impact) 使用双轴条形图展示了量化对推理时间和模型大小的影响。每个类别中的左侧条形显示了从FP32转换为INT8时的推理时间改进，而右侧条形则描述了相应的模型大小减少。结果表明，量化模型可以实现高达<semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">4\times</annotation></semantics>的更快推理速度，同时将存储需求减少一个<semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">4\times</annotation></semantics>的因子，这使得它们在资源受限的环境中高度适用。'
- en: '![](../media/file160.svg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file160.svg)'
- en: 'Figure 10.14: **Quantization Impact**: Moving from FP32 to INT8 reduces inference
    time by up to 4 times while decreasing model size by a factor of 4, making models
    more efficient for resource-constrained environments.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14：**量化影响**：从FP32转换为INT8可以将推理时间减少高达4倍，同时将模型大小减少一个4倍的因子，这使得模型在资源受限的环境中更加高效。
- en: However, reducing numerical precision introduces trade-offs. Lower-precision
    formats can lead to numerical instability and quantization noise, potentially
    affecting model accuracy. Some architectures, such as large transformer-based
    NLP models, tolerate quantization well, whereas others may experience significant
    degradation. Thus, selecting the appropriate numerical precision requires balancing
    accuracy constraints, hardware support, and efficiency gains.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，降低数值精度会带来权衡。较低的精度格式可能导致数值不稳定和量化噪声，可能影响模型精度。一些架构，如大型基于Transformer的自然语言处理模型，对量化有很好的容忍度，而其他架构可能会经历显著的退化。因此，选择适当的数值精度需要平衡精度约束、硬件支持和效率提升。
- en: '![](../media/file161.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file161.png)'
- en: 'Figure 10.15: Quantization error weighted by p(x).'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15：按p(x)加权的量化误差。
- en: '[Figure 10.15](ch016.xhtml#fig-quantization) illustrates the quantization error
    weighted by the probability distribution of values, comparing different numerical
    formats (FP8 variants and INT8). The error distribution highlights how different
    formats introduce varying levels of quantization noise across the range of values,
    which in turn influences model accuracy and stability.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.15](ch016.xhtml#fig-quantization) 展示了按值概率分布加权的量化误差，比较了不同的数值格式（FP8变体和INT8）。误差分布突出了不同格式如何在值范围内引入不同水平的量化噪声，这反过来又影响模型的精度和稳定性。'
- en: Numeric Encoding and Storage
  id: totrans-420
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数值编码和存储
- en: The representation of numerical data in machine learning systems extends beyond
    precision levels to encompass encoding formats and storage mechanisms, both of
    which significantly influence computational efficiency. The encoding of numerical
    values determines how floating-point and integer representations are stored in
    memory and processed by hardware, directly affecting performance in machine learning
    workloads. As machine learning models grow in size and complexity, optimizing
    numeric encoding becomes increasingly important for ensuring efficiency, particularly
    on specialized hardware accelerators ([Mellempudi et al. 2019](ch058.xhtml#ref-mellempudi2019mixed)).
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统中，数值数据的表示不仅限于精度水平，还包括编码格式和存储机制，这两者都显著影响计算效率。数值值的编码决定了浮点数和整数表示如何在内存中存储并由硬件处理，这直接影响到机器学习工作负载的性能。随着机器学习模型的大小和复杂性增加，优化数值编码对于确保效率变得越来越重要，尤其是在专用硬件加速器上（[Mellempudi等人2019](ch058.xhtml#ref-mellempudi2019mixed)）。
- en: Floating-point representations, which are widely used in machine learning, follow
    the [IEEE 754 standard](https://standards.ieee.org/standard/754-2019.html), defining
    how numbers are represented using a combination of sign, exponent, and mantissa
    (fraction) bits. Standard formats such as FP32 (single precision) and FP64 (double
    precision) provide high accuracy but demand significant memory and computational
    resources. To enhance efficiency, reduced-precision formats such as FP16, [bfloat16](https://cloud.google.com/tpu/docs/bfloat16),
    and [FP8](https://arxiv.org/abs/2209.05433) have been introduced, offering lower
    storage requirements while maintaining sufficient numerical range for machine
    learning computations. Unlike FP16, which allocates more bits to the mantissa,
    bfloat16 retains the same exponent size as FP32, allowing it to represent a wider
    dynamic range while reducing precision in the fraction. This characteristic makes
    bfloat16 particularly effective for machine learning training, where maintaining
    dynamic range is important for stable gradient updates.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中广泛使用的浮点数表示遵循[IEEE 754标准](https://standards.ieee.org/standard/754-2019.html)，该标准定义了如何使用符号、指数和尾数（分数）比特的组合来表示数字。标准格式如FP32（单精度）和FP64（双精度）提供了高精度，但需要大量的内存和计算资源。为了提高效率，引入了低精度格式如FP16、[bfloat16](https://cloud.google.com/tpu/docs/bfloat16)和[FP8](https://arxiv.org/abs/2209.05433)，它们提供了较低的存储需求，同时保持了机器学习计算所需的足够数值范围。与FP16不同，bfloat16保留了与FP32相同的指数大小，允许它表示更宽的动态范围，同时在分数上降低精度。这一特性使得bfloat16特别适用于机器学习训练，在保持动态范围对于稳定的梯度更新很重要的情况下。
- en: Integer-based representations, including INT8 and INT4, further reduce storage
    and computational overhead by eliminating the need for exponent and mantissa encoding.
    These formats are commonly used in quantized inference, where model weights and
    activations are converted to discrete integer values to accelerate computation
    and reduce power consumption. The deterministic nature of integer arithmetic simplifies
    execution on hardware, making it particularly well-suited for edge AI and mobile
    devices. At the extreme end, binary and ternary representations restrict values
    to just one or two bits, leading to significant reductions in memory footprint
    and power consumption. However, such aggressive quantization can degrade model
    accuracy unless complemented by specialized training techniques or architectural
    adaptations.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 基于整数的表示，包括INT8和INT4，通过消除指数和尾数编码的需要，进一步减少了存储和计算开销。这些格式在量化推理中常用，其中模型权重和激活被转换为离散的整数值，以加速计算并降低功耗。整数算术的确定性简化了硬件上的执行，使其特别适合边缘AI和移动设备。在极端情况下，二进制和三进制表示将值限制在仅一个或两个比特，从而显著减少了内存占用和功耗。然而，除非辅以专门的训练技术或架构调整，否则这种激进的量化可能会降低模型精度。
- en: Emerging numeric formats seek to balance the trade-off between efficiency and
    accuracy. [TF32](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/),
    introduced by NVIDIA for Ampere GPUs, modifies FP32 by reducing the mantissa size
    while maintaining the exponent width, allowing for faster computations with minimal
    precision loss. Similarly, FP8, which is gaining adoption in AI accelerators,
    provides an even lower-precision floating-point alternative while retaining a
    structure that aligns well with machine learning workloads ([Micikevicius et al.
    2022](ch058.xhtml#ref-micikevicius2022fp8)). Alternative formats such as [Posit](https://ieeexplore.ieee.org/document/9399648),
    [Flexpoint](https://arxiv.org/abs/1711.02213), and [BF16ALT](https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/BFMLALB--BFMLALT--vector---BFloat16-floating-point-widening-multiply-add-long--vector--)
    are also being explored for their potential advantages in numerical stability
    and hardware adaptability.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 新兴的数值格式试图在效率和精度之间取得平衡。[TF32](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)，由NVIDIA为Ampere
    GPU引入，通过减少尾数大小而保持指数宽度，允许以最小的精度损失进行更快的计算。类似地，FP8，在AI加速器中得到采用，提供了一个更低精度的浮点数替代方案，同时保留了一个与机器学习工作负载很好地对齐的结构([Micikevicius等人2022](ch058.xhtml#ref-micikevicius2022fp8))。其他如[Posit](https://ieeexplore.ieee.org/document/9399648)、[Flexpoint](https://arxiv.org/abs/1711.02213)和[BF16ALT](https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/BFMLALB--BFMLALT--vector---BFloat16-floating-point-widening-multiply-add-long--vector--)等替代格式也在探索其潜在的数值稳定性和硬件适应性优势。
- en: The efficiency of numeric encoding is further influenced by how data is stored
    and accessed in memory. AI accelerators optimize memory hierarchies to maximize
    the benefits of reduced-precision formats, using specialized hardware such as
    tensor cores, matrix multiply units (MMUs), and vector processing engines to accelerate
    lower-precision computations. On these platforms, data alignment, memory tiling,
    and compression techniques play a important role in ensuring that reduced-precision
    computations deliver tangible performance gains.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 数字编码的效率还受到数据在内存中存储和访问方式的影响。AI加速器通过优化内存层次结构来最大化减少精度格式的优势，使用专门的硬件，如张量核心、矩阵乘法单元（MMUs）和向量处理引擎来加速低精度计算。在这些平台上，数据对齐、内存分块和压缩技术在确保减少精度计算带来实际性能提升方面发挥着重要作用。
- en: As machine learning systems evolve, numeric encoding and storage strategies
    will continue to adapt to meet the demands of large-scale models and diverse hardware
    environments. The ongoing development of precision formats tailored for AI workloads
    highlights the importance of co-designing numerical representations with underlying
    hardware capabilities, ensuring that machine learning models achieve optimal performance
    while minimizing computational costs.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统的演变，数值编码和存储策略将继续适应以满足大规模模型和多样化硬件环境的需求。针对AI工作负载定制的精度格式的持续开发突出了与底层硬件能力协同设计数值表示的重要性，确保机器学习模型在最小化计算成本的同时实现最佳性能。
- en: Numerical Format Comparison
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数值格式比较
- en: '[Table 10.7](ch016.xhtml#tbl-numerics) compares commonly used numerical precision
    formats in machine learning, highlighting their trade-offs in storage efficiency,
    computational speed, and energy consumption. Emerging formats like FP8 and TF32
    have been introduced to further optimize performance, particularly on AI accelerators.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10.7](ch016.xhtml#tbl-numerics)比较了机器学习中常用的数值精度格式，突出了它们在存储效率、计算速度和能耗方面的权衡。FP8和TF32等新兴格式已被引入以进一步优化性能，尤其是在AI加速器上。'
- en: 'Table 10.7: Comparison of numerical precision formats.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.7：数值精度格式比较。
- en: '| **Precision Format** | **Bit-Width** | **Storage Reduction (vs FP32)** |
    **Compute Speed (vs FP32)** | **Power Consumption** | **Use Cases** |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| **精度格式** | **位宽** | **存储减少（与FP32相比）** | **计算速度（与FP32相比）** | **功耗** | **用例**'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | ---'
- en: '| **FP32 (Single-Precision Floating Point)** | 32-bit | Baseline (1×) | Baseline
    (1×) | High | Training & inference (general-purpose) |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| **FP32 (单精度浮点)** | 32位 | 基准（1倍） | 基准（1倍） | 较高 | 训练与推理（通用）'
- en: '| **FP16 (Half-Precision Floating Point)** | 16-bit | 2× smaller | 2× faster
    on FP16-optimized hardware | Lower | Accelerated training, inference (NVIDIA Tensor
    Cores, TPUs) |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| **FP16 (半精度浮点)** | 16位 | 小2倍 | 在FP16优化硬件上快2倍 | 较低 | 加速训练，推理（NVIDIA张量核心，TPUs）'
- en: '| **bfloat16 (Brain Floating Point)** | 16-bit | 2× smaller | Similar speed
    to FP16, better dynamic range | Lower | Training on TPUs, transformer-based models
    |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| **bfloat16 (脑浮点)** | 16位 | 小2倍 | 与FP16速度相似，动态范围更好 | 较低 | 在TPUs上训练，基于转换器的模型'
- en: '| **TF32 (TensorFloat-32)** | 19-bit | Similar to FP16 | Up to 8× faster on
    NVIDIA Ampere GPUs | Lower | Training on NVIDIA GPUs |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| **TF32 (TensorFloat-32)** | 19位 | 类似于FP16 | 在NVIDIA Ampere GPU上最高可快8倍 | 较低
    | 在NVIDIA GPU上训练'
- en: '| **FP8 (Floating-Point 8-bit)** | 8-bit | 4× smaller | Faster than INT8 in
    some cases | Significantly lower | Efficient training/inference (H100, AI accelerators)
    |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| **FP8 (浮点8位)** | 8位 | 小4倍 | 在某些情况下比INT8快 | 显著较低 | 高效训练/推理（H100，AI加速器）'
- en: '| **INT8 (8-bit Integer)** | 8-bit | 4× smaller | 4–8× faster than FP32 | Significantly
    lower | Quantized inference (Edge AI, mobile AI, NPUs) |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| **INT8 (8位整数)** | 8位 | 小4倍 | 比FP32快4-8倍 | 显著较低 | 量化推理（边缘AI，移动AI，NPUs）'
- en: '| **INT4 (4-bit Integer)** | 4-bit | 8× smaller | Hardware-dependent | Extremely
    low | Ultra-low-power AI, experimental quantization |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| **INT4 (4位整数)** | 4位 | 小8倍 | 依赖于硬件 | 极低 | 超低功耗AI，实验性量化'
- en: '| **Binary/Ternary (1-bit / 2-bit)** | 1–2-bit | 16–32× smaller | Highly hardware-dependent
    | Lowest | Extreme efficiency (binary/ternary neural networks) |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| **二进制/三进制（1位/2位）** | 1-2位 | 16-32倍更小 | 极度依赖于硬件 | 最低 | 极端效率（二进制/三进制神经网络）'
- en: FP16 and bfloat16 formats provide moderate efficiency gains while preserving
    model accuracy. Many AI accelerators, such as NVIDIA Tensor Cores and TPUs, include
    dedicated support for FP16 computations, enabling <semantics><mrow><mn>2</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">2\times</annotation></semantics> faster matrix operations
    compared to FP32\. BFloat16, in particular, retains the same 8-bit exponent as
    FP32 but with a reduced 7-bit mantissa, allowing it to maintain a similar dynamic
    range (~<semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>38</mn></mrow></msup><annotation
    encoding="application/x-tex">10^{-38}</annotation></semantics> to <semantics><msup><mn>10</mn><mn>38</mn></msup><annotation
    encoding="application/x-tex">10^{38}</annotation></semantics>) while sacrificing
    precision. In contrast, FP16, with its 5-bit exponent and 10-bit mantissa, has
    a significantly reduced dynamic range (~<semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>5</mn></mrow></msup><annotation
    encoding="application/x-tex">10^{-5}</annotation></semantics> to <semantics><msup><mn>10</mn><mn>5</mn></msup><annotation
    encoding="application/x-tex">10^5</annotation></semantics>), making it more suitable
    for inference rather than training. Since BFloat16 preserves the exponent size
    of FP32, it better handles extreme values encountered during training, whereas
    FP16 may struggle with underflow or overflow. This makes BFloat16 a more robust
    alternative for deep learning workloads that require a wide dynamic range.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: FP16和bfloat16格式在保持模型精度的同时提供了适度的效率提升。许多AI加速器，如NVIDIA Tensor Cores和TPUs，都包括对FP16计算的专用支持，这使得与FP32相比，矩阵运算速度提高了<semantics><mrow><mn>2</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">2\times</annotation></semantics>。特别是BFloat16，它保留了与FP32相同的8位指数，但具有减少的7位尾数，使其能够保持类似的动态范围(~<semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>38</mn></mrow></msup><annotation
    encoding="application/x-tex">10^{-38}</annotation></semantics>到<semantics><msup><mn>10</mn><mn>38</mn></msup><annotation
    encoding="application/x-tex">10^{38}</annotation></semantics>)，同时牺牲了精度。相比之下，FP16具有5位指数和10位尾数，其动态范围显著减小(~<semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>5</mn></mrow></msup><annotation
    encoding="application/x-tex">10^{-5}</annotation></semantics>到<semantics><msup><mn>10</mn><mn>5</mn></msup><annotation
    encoding="application/x-tex">10^5</annotation></semantics>)，使其更适合推理而不是训练。由于BFloat16保留了FP32的指数大小，它更好地处理训练过程中遇到的极端值，而FP16可能难以处理下溢或上溢。这使得BFloat16成为需要广泛动态范围的深度学习工作负载的更稳健的替代方案。
- en: '[Figure 10.16](ch016.xhtml#fig-3float) highlights these differences, showing
    how bit-width allocations impact the trade-offs between precision and numerical
    range.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.16](ch016.xhtml#fig-3float)突出了这些差异，展示了位宽分配如何影响精度和数值范围之间的权衡。'
- en: '![](../media/file162.svg)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file162.svg)'
- en: 'Figure 10.16: **Floating-Point Precision**: Reduced-precision formats like
    FP16 and bfloat16 trade off numerical range for computational efficiency and memory
    savings. Bfloat16 maintains the exponent size of FP32, preserving its dynamic
    range and suitability for training, while FP16’s smaller exponent limits its use
    to inference or carefully scaled training scenarios.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.16：**浮点精度**：FP16和bfloat16等低精度格式以计算效率和内存节省为代价，牺牲了数值范围。BFloat16保持了FP32的指数大小，保留了其动态范围和训练适用性，而FP16较小的指数限制了其在推理或仔细缩放的训练场景中的应用。
- en: INT8 precision offers more aggressive efficiency improvements, particularly
    for inference workloads. Many quantized models use INT8 for inference, reducing
    storage by <semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">4\times</annotation></semantics>
    while accelerating computation by 4–8<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    on optimized hardware. INT8 is widely used in mobile and embedded AI, where energy
    constraints are significant.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: INT8精度提供了更激进的效率提升，尤其是在推理工作负载中。许多量化模型使用INT8进行推理，通过<semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">4\times</annotation></semantics>减少存储，并在优化硬件上加速计算4–8<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>。INT8在移动和嵌入式AI中得到了广泛应用，在这些领域中能源限制是显著的。
- en: Binary and ternary networks represent the extreme end of quantization, where
    weights and activations are constrained to 1-bit (binary) or 2-bit (ternary) values.
    This results in massive storage and energy savings, but model accuracy often degrades
    significantly unless specialized architectures are used.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制和三进制网络代表了量化的极端，其中权重和激活被限制为1位（二进制）或2位（三进制）值。这导致了巨大的存储和能源节省，但除非使用专门的架构，否则模型精度通常会显著下降。
- en: Precision Reduction Trade-offs
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精度降低的权衡
- en: Reducing numerical precision in machine learning systems offers significant
    gains in efficiency, including lower memory requirements, reduced power consumption,
    and increased computational throughput. However, these benefits come with trade-offs,
    as lower-precision representations introduce numerical error and quantization
    noise, which can affect model accuracy. The extent of this impact depends on multiple
    factors, including the model architecture, the dataset, and the specific precision
    format used.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统中降低数值精度可以显著提高效率，包括降低内存需求、减少功耗和增加计算吞吐量。然而，这些好处伴随着权衡，因为低精度表示引入了数值误差和量化噪声，这可能会影响模型精度。这种影响程度取决于多个因素，包括模型架构、数据集和使用的特定精度格式。
- en: Models exhibit varying levels of tolerance to quantization. Large-scale architectures,
    such as convolutional neural networks and transformer-based models, often retain
    high accuracy even when using reduced-precision formats such as bfloat16 or INT8\.
    In contrast, smaller models or those trained on tasks requiring high numerical
    precision may experience greater degradation in performance. Not all layers within
    a neural network respond equally to precision reduction. Certain layers, such
    as batch normalization and attention mechanisms, may be more sensitive to numerical
    precision than standard feedforward layers. As a result, techniques such as mixed-precision
    training, where different layers operate at different levels of precision, can
    help maintain accuracy while optimizing computational efficiency.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对量化的容忍度各不相同。大型架构，如卷积神经网络和基于转换器的模型，即使在使用bfloat16或INT8等低精度格式的情况下，通常也能保持高精度。相比之下，小型模型或训练需要高数值精度的任务可能会经历性能的更大下降。神经网络中的所有层对精度降低的反应并不相同。某些层，如批归一化和注意力机制，可能比标准前馈层对数值精度更敏感。因此，混合精度训练等技术，其中不同层以不同精度级别运行，可以帮助在优化计算效率的同时保持精度。
- en: Hardware support is another important factor in determining the effectiveness
    of precision reduction. AI accelerators, including GPUs, TPUs, and NPUs, are designed
    with dedicated low-precision arithmetic units that enable efficient computation
    using FP16, bfloat16, INT8, and, more recently, FP8\. These architectures exploit
    reduced precision to perform high-throughput matrix operations, improving both
    speed and energy efficiency. In contrast, general-purpose CPUs often lack specialized
    hardware for low-precision computations, limiting the potential benefits of numerical
    quantization. The introduction of newer floating-point formats, such as TF32 for
    NVIDIA GPUs and FP8 for AI accelerators, seeks to optimize the trade-off between
    precision and efficiency, offering an alternative for hardware that is not explicitly
    designed for extreme quantization.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件支持是决定精度降低有效性的另一个重要因素。人工智能加速器，包括GPU、TPU和NPU，都配备了专门的低精度算术单元，这些单元能够使用FP16、bfloat16、INT8，以及最近出现的FP8进行高效计算。这些架构利用降低的精度来执行高吞吐量矩阵运算，提高速度和能源效率。相比之下，通用CPU通常缺乏用于低精度计算的专用硬件，限制了数值量化潜在效益的实现。引入新的浮点格式，如NVIDIA
    GPU的TF32和人工智能加速器的FP8，旨在优化精度和效率之间的权衡，为未明确设计用于极端量化的硬件提供替代方案。
- en: In addition to hardware constraints, reducing numerical precision impacts power
    consumption. Lower-precision arithmetic reduces the number of required memory
    accesses and simplifies computational operations, leading to lower overall energy
    use. This is particularly advantageous for energy-constrained environments such
    as mobile devices and edge AI systems. At the extreme end, ultra-low precision
    formats, including INT4 and binary/ternary representations, provide significant
    reductions in power and memory usage. However, these formats often require specialized
    architectures to compensate for the accuracy loss associated with such aggressive
    quantization.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 除了硬件限制之外，降低数值精度还会影响功耗。低精度算术减少了所需的内存访问次数并简化了计算操作，从而降低了整体能耗。这对于移动设备和边缘人工智能系统等能源受限的环境尤其有利。在极端情况下，包括INT4和二进制/三进制表示在内的超低精度格式，在功耗和内存使用方面提供了显著的降低。然而，这些格式通常需要专门的架构来补偿与这种激进量化相关的精度损失。
- en: To mitigate accuracy loss associated with reduced precision, various quantization
    strategies can be employed. Ultimately, selecting the appropriate numerical precision
    for a given machine learning model requires balancing efficiency gains against
    accuracy constraints. This selection depends on the model’s architecture, the
    computational requirements of the target application, and the underlying hardware’s
    support for low-precision operations. By using advancements in both hardware and
    software optimization techniques, practitioners can effectively integrate lower-precision
    numerics into machine learning pipelines, maximizing efficiency while maintaining
    performance.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻与降低精度相关的精度损失，可以采用各种量化策略。最终，为给定的机器学习模型选择适当的数值精度需要在效率提升和精度约束之间进行平衡。这个选择取决于模型的架构、目标应用的计算需求以及底层硬件对低精度操作的支持。通过利用硬件和软件优化技术的进步，从业者可以有效地将低精度数值集成到机器学习管道中，在保持性能的同时最大化效率。
- en: Precision Reduction Strategies
  id: totrans-452
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精度降低策略
- en: Reducing numerical precision is an important optimization technique for improving
    the efficiency of machine learning models. By lowering the bit-width of weights
    and activations, models can reduce memory footprint, improve computational throughput,
    and decrease power consumption. However, naive quantization can introduce quantization
    errors, leading to accuracy degradation. To address this, different precision
    reduction strategies have been developed, allowing models to balance efficiency
    gains while preserving predictive performance.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 降低数值精度是提高机器学习模型效率的重要优化技术。通过降低权重和激活的位宽，模型可以减少内存占用，提高计算吞吐量，并降低功耗。然而，简单的量化可能会引入量化误差，导致精度下降。为了解决这个问题，已经开发出不同的精度降低策略，允许模型在保持预测性能的同时平衡效率提升。
- en: Quantization techniques can be applied at different stages of a model’s lifecycle.
    Post-training quantization reduces precision after training, making it a simple
    and low-cost approach for optimizing inference. Quantization-aware training incorporates
    quantization effects into the training process, enabling models to adapt to lower
    precision and retain higher accuracy. Mixed-precision training leverages hardware
    support to dynamically assign precision levels to different computations, optimizing
    execution efficiency without sacrificing accuracy.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 量化技术可以在模型生命周期的不同阶段应用。训练后量化在训练后降低精度，使其成为优化推理的简单且低成本方法。量化感知训练将量化效果纳入训练过程，使模型能够适应较低的精度并保持较高的精度。混合精度训练利用硬件支持动态地为不同的计算分配精度级别，优化执行效率而不牺牲精度。
- en: To help navigate this increasing complexity, [Figure 10.17](ch016.xhtml#fig-quantization-roadmap)
    organizes quantization techniques into three progressive tiers based on implementation
    complexity, resource requirements, and target use cases.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助导航日益增加的复杂性，[图10.17](ch016.xhtml#fig-quantization-roadmap) 将量化技术根据实现复杂性、资源需求和目标用例分为三个渐进层次。
- en: '![](../media/file163.svg)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
  zh: '![图10.17](../media/file163.svg)'
- en: 'Figure 10.17: **Quantization Complexity Roadmap**: Three progressive tiers
    of quantization techniques, from foundational approaches suitable for quick deployment
    to research frontier methods for extreme resource constraints, reflecting increasing
    implementation effort, resource requirements, and potential accuracy trade-offs.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.17：**量化复杂性路线图**：量化技术的三个渐进层次，从适合快速部署的基础方法到针对极端资源限制的研究前沿方法，反映了不断增加的实现努力、资源需求和潜在的精度权衡。
- en: Post-Training Quantization
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练后量化
- en: Quantization is the specific algorithmic technique that enables significant
    memory bandwidth reduction when addressing the memory wall. These quantization
    methods provide standardized APIs across different platforms, showing exactly
    how to implement the efficiency principles established earlier.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是在解决内存墙问题时实现显著内存带宽降低的特定算法技术。这些量化方法在不同平台上提供标准化的API，明确展示了如何实现之前建立的效率原则。
- en: Post-training quantization (PTQ) reduces numerical precision after training,
    converting weights and activations from high-precision formats (FP32) to lower-precision
    representations (INT8 or FP16) without retraining ([Jacob et al. 2018b](ch058.xhtml#ref-jacob2018quantization)).
    This achieves smaller model sizes, faster computation, and reduced energy consumption,
    making it practical for resource-constrained environments such as mobile devices,
    edge AI systems, and cloud inference platforms ([H. Wu et al. 2020](ch058.xhtml#ref-wu2020integer)).
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后量化（PTQ）在训练后降低数值精度，将权重和激活从高精度格式（FP32）转换为低精度表示（INT8 或 FP16）而无需重新训练 ([Jacob
    et al. 2018b](ch058.xhtml#ref-jacob2018quantization))。这实现了更小的模型尺寸、更快的计算和更低的能耗，使其适用于资源受限的环境，如移动设备、边缘
    AI 系统和云推理平台 ([H. Wu et al. 2020](ch058.xhtml#ref-wu2020integer))。
- en: PTQ’s key advantage is low computational cost—it requires no retraining or access
    to training data. However, reducing precision introduces quantization error that
    can degrade accuracy, particularly for tasks requiring fine-grained numerical
    precision. Machine learning frameworks (TensorFlow Lite, ONNX Runtime, PyTorch)
    provide built-in PTQ support.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ 的主要优势是计算成本低——无需重新训练或访问训练数据。然而，降低精度会引入量化误差，这可能会降低准确性，尤其是对于需要精细数值精度的任务。机器学习框架（TensorFlow
    Lite、ONNX Runtime、PyTorch）提供了内置的 PTQ 支持。
- en: PTQ Functionality
  id: totrans-462
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PTQ 功能
- en: PTQ converts a trained model’s weights and activations from high-precision floating-point
    representations (e.g., FP32) to lower-precision formats (e.g., INT8 or FP16).
    This process reduces the memory footprint of the model, accelerates inference,
    and lowers power consumption. However, since lower-precision formats have a smaller
    numerical range, quantization introduces rounding errors, which can impact model
    accuracy.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ 将训练模型的权重和激活从高精度浮点表示（例如，FP32）转换为低精度格式（例如，INT8 或 FP16）。这个过程减少了模型的内存占用，加速了推理，并降低了功耗。然而，由于低精度格式具有较小的数值范围，量化会引入舍入误差，这可能会影响模型准确性。
- en: 'The core mechanism behind PTQ is scaling and mapping high-precision values
    into a reduced numerical range. A widely used approach is uniform quantization,
    which maps floating-point values to discrete integer levels using a consistent
    scaling factor. In uniform quantization, the interval between each quantized value
    is constant, simplifying implementation and ensuring efficient execution on hardware.
    The quantized value <semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics>
    is computed as: <semantics><mrow><mi>q</mi><mo>=</mo><mtext mathvariant="normal">round</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">q
    = \text{round} \left(\frac{x}{s} \right)</annotation></semantics> where:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ 的核心机制是将高精度值缩放并映射到较小的数值范围。一种广泛使用的方法是均匀量化，它使用一致的缩放因子将浮点值映射到离散整数级别。在均匀量化中，每个量化值之间的间隔是恒定的，这简化了实现并确保了在硬件上的高效执行。量化值
    <semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics>
    的计算如下：<semantics><mrow><mi>q</mi><mo>=</mo><mtext mathvariant="normal">round</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">q
    = \text{round} \left(\frac{x}{s} \right)</annotation></semantics> 其中：
- en: <semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics>
    is the quantized integer representation,
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics>
    是量化整数表示，
- en: <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    is the original floating-point value,
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    是原始浮点值，
- en: <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>
    is a scaling factor that maps the floating-point range to the available integer
    range.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>
    是一个缩放因子，它将浮点范围映射到可用的整数范围。
- en: '[Listing 10.2](ch016.xhtml#lst-quantization_example) demonstrates uniform quantization
    from FP32 to INT8.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.2](ch016.xhtml#lst-quantization_example) 展示了从 FP32 到 INT8 的均匀量化。'
- en: 'Listing 10.2: **Uniform Quantization**: Converts FP32 weights to INT8 format,
    achieving 4x memory reduction while measuring quantization error.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.2：**均匀量化**：将 FP32 权重转换为 INT8 格式，实现 4 倍内存减少的同时测量量化误差。
- en: '[PRE1]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This example demonstrates the compression from 32 bits to 8 bits per weight,
    with minimal quantization error.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例演示了从 32 位到每个权重 8 位的压缩，量化误差最小。
- en: For example, in INT8 quantization, the model’s floating-point values (typically
    ranging from <semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mi>r</mi><mo>,</mo><mi>r</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[-r,
    r]</annotation></semantics>) are mapped to an integer range of <semantics><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>−</mi><mn>128</mn><mo>,</mo><mn>127</mn><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[-128,
    127]</annotation></semantics>. The scaling factor ensures that the most significant
    information is retained while reducing precision loss. Once the model has been
    quantized, inference is performed using integer arithmetic, which is significantly
    more efficient than floating-point operations on many hardware platforms ([Gholami
    et al. 2021](ch058.xhtml#ref-gholami2021survey)). However, due to rounding errors
    and numerical approximation, quantized models may experience slight accuracy degradation
    compared to their full-precision counterparts.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在INT8量化中，模型的浮点值（通常范围在<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mi>r</mi><mo>,</mo><mi>r</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[-r,
    r]</annotation></semantics>之间）被映射到一个整数范围<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mn>128</mn><mo>,</mo><mn>127</mn><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[-128,
    127]</annotation></semantics>。缩放因子确保在减少精度损失的同时保留了最重要的信息。一旦模型被量化，推理就使用整数算术进行，这在许多硬件平台上比浮点运算效率高得多（[Gholami等人2021](ch058.xhtml#ref-gholami2021survey)）。然而，由于舍入误差和数值逼近，量化模型与全精度模型相比可能会出现轻微的精度下降。
- en: Once the model has been quantized, inference is performed using integer arithmetic,
    which is significantly more efficient than floating-point operations on many hardware
    platforms. However, due to rounding errors and numerical approximation, quantized
    models may experience slight accuracy degradation compared to their full-precision
    counterparts.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被量化，推理就使用整数算术进行，这在许多硬件平台上比浮点运算效率高得多。然而，由于舍入误差和数值逼近，量化模型与全精度模型相比可能会出现轻微的精度下降。
- en: In addition to uniform quantization, non-uniform quantization can be employed
    to preserve accuracy in certain scenarios. Unlike uniform quantization, which
    uses a consistent scaling factor, non-uniform quantization assigns finer-grained
    precision to numerical ranges that are more densely populated. This approach can
    be beneficial for models with weight distributions that concentrate around certain
    values, as it allows more details to be retained where it matters most. However,
    non-uniform quantization typically requires more complex calibration and may involve
    additional computational overhead. While it is not as commonly used as uniform
    quantization in production environments, non-uniform techniques can be effective
    for preserving accuracy in models that are particularly sensitive to precision
    changes.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 除了均匀量化之外，还可以采用非均匀量化来在特定场景中保留精度。与使用一致缩放因子的均匀量化不同，非均匀量化将更细粒度的精度分配给数值范围，这些范围更密集。这种方法对于权重分布集中在某些值的模型来说是有益的，因为它允许在最重要的地方保留更多细节。然而，非均匀量化通常需要更复杂的校准，并可能涉及额外的计算开销。尽管在生产环境中非均匀量化不如均匀量化常用，但对于特别敏感于精度变化的模型，非均匀技术可以有效地保留精度。
- en: PTQ is particularly effective for computer vision models, where CNNs often tolerate
    quantization well. However, models that rely on small numerical differences, such
    as NLP transformers or speech recognition models, may require additional tuning
    or alternative quantization techniques, including non-uniform strategies, to retain
    performance.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ对于计算机视觉模型特别有效，其中CNN通常可以很好地容忍量化。然而，依赖于微小数值差异的模型，如NLP转换器或语音识别模型，可能需要额外的调整或替代量化技术，包括非均匀策略，以保留性能。
- en: Calibration
  id: totrans-476
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 校准
- en: An important aspect of PTQ is the calibration step, which involves selecting
    the most effective clipping range [<semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>,
    <semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>]
    for quantizing model weights and activations. During PTQ, the model’s weights
    and activations are converted to lower-precision formats (e.g., INT8), but the
    effectiveness of this reduction depends heavily on the chosen quantization range.
    Without proper calibration, the quantization process may cause significant accuracy
    degradation, even if the overall precision is reduced. Calibration ensures that
    the chosen range minimizes loss of information and helps preserve the model’s
    performance after precision reduction.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ的一个重要方面是校准步骤，它涉及选择对量化模型权重和激活最有效的裁剪范围 [<semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>,
    <semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>]。在PTQ过程中，模型的权重和激活被转换为低精度格式（例如，INT8），但这种减少的有效性在很大程度上取决于所选的量化范围。如果没有适当的校准，量化过程可能会引起显著的精度下降，即使整体精度有所降低。校准确保所选范围最小化信息损失，并有助于在精度降低后保持模型性能。
- en: The overall workflow of post-training quantization is illustrated in [Figure 10.18](ch016.xhtml#fig-ptq-calibration).
    The process begins with a pre-trained model, which serves as the starting point
    for optimization. To determine an effective quantization range, a calibration
    dataset, which is a representative subset of training or validation data, is passed
    through the model. This step allows the calibration process to estimate the numerical
    distribution of activations and weights, which is then used to define the clipping
    range for quantization. Following calibration, the quantization step converts
    the model parameters to a lower-precision format, producing the final quantized
    model, which is more efficient in terms of memory and computation.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.18](ch016.xhtml#fig-ptq-calibration)展示了训练后量化的整体工作流程。该过程从预训练模型开始，作为优化的起点。为了确定有效的量化范围，一个校准数据集，即训练或验证数据的一个代表性子集，被传递到模型中。这一步骤允许校准过程估计激活和权重的数值分布，然后用于定义量化的裁剪范围。校准之后，量化步骤将模型参数转换为低精度格式，生成最终的量化模型，在内存和计算效率方面更为高效。'
- en: '![](../media/file164.svg)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file164.svg)'
- en: 'Figure 10.18: **Post-Training Quantization**: Calibration with a representative
    dataset determines optimal quantization ranges for model weights and activations,
    minimizing information loss during quantization to create efficient, lower-precision
    models. This process converts a pre-trained model into a quantized version suitable
    for deployment on resource-constrained devices.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.18：**训练后量化**：使用代表性数据集进行校准，确定模型权重和激活的最佳量化范围，最小化量化过程中的信息损失，以创建高效、低精度的模型。此过程将预训练模型转换为适合在资源受限设备上部署的量化版本。
- en: For example, consider quantizing activations that originally have a floating-point
    range between –6 and 6 to 8-bit integers. Simply using the full integer range
    of –128 to 127 for quantization might not be the most effective approach. Instead,
    calibration involves passing a representative dataset through the model and observing
    the actual range of the activations. The observed range can then be used to set
    a more effective quantization range, reducing information loss.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑将原本在-6到6之间的浮点范围激活量化为8位整数。简单地使用-128到127的完整整数范围进行量化可能不是最有效的方法。相反，校准涉及通过模型传递一个代表性数据集，并观察激活的实际范围。然后可以使用观察到的范围来设置一个更有效的量化范围，从而减少信息损失。
- en: Calibration Methods
  id: totrans-482
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 校准方法
- en: 'There are several commonly used calibration methods:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的校准方法有几种：
- en: '**Max**: This method uses the maximum absolute value seen during calibration
    as the clipping range. While simple, it is susceptible to outlier data. For example,
    in the activation distribution shown in [Figure 10.19](ch016.xhtml#fig-resnet-activations-histogram),
    we see an outlier cluster around 2.1, while the rest of the values are clustered
    around smaller values. The Max method could lead to an inefficient range if the
    outliers significantly influence the quantization.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大值**：此方法使用校准期间观察到的最大绝对值作为裁剪范围。虽然简单，但容易受到异常数据的影响。例如，在[图10.19](ch016.xhtml#fig-resnet-activations-histogram)所示的激活分布中，我们看到2.1附近的异常值簇，而其余的值则聚集在较小的值周围。如果异常值显著影响量化，最大值方法可能导致不高效的范围。'
- en: '**Entropy**: This method minimizes information loss between the original floating-point
    values and the values that could be represented by the quantized format, typically
    using KL divergence. This is the default calibration method used by TensorRT and
    works well when trying to preserve the distribution of the original values.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**熵**：此方法通过使用KL散度，最小化原始浮点值与量化格式所能表示的值之间的信息损失。这是TensorRT使用的默认校准方法，当试图保留原始值的分布时效果良好。'
- en: '**Percentile**: This method sets the clipping range to a percentile of the
    distribution of absolute values seen during calibration. For example, a 99% calibration
    would clip the top 1% of the largest magnitude values. This method helps avoid
    the impact of outliers, which are not representative of the general data distribution.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**百分位数**：此方法将裁剪范围设置为校准期间观察到的绝对值分布的百分位数。例如，99%的校准将裁剪最大值中最大的1%。此方法有助于避免异常值的影响，因为异常值不代表一般数据分布。'
- en: '![](../media/file165.png)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file165.png)'
- en: 'Figure 10.19: **Activation Distribution**: Resnet50 layer activations exhibit
    a long tail, with a small percentage of values significantly larger than the majority;
    this distribution impacts quantization range selection, as outlier values can
    lead to inefficient use of precision if not handled carefully. Source: ([H. Wu
    et al. 2020](ch058.xhtml#ref-wu2020integer)).'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.19：**激活分布**：Resnet50层的激活表现出长尾分布，其中一小部分值显著大于大多数值；这种分布影响量化范围的选择，因为如果不小心处理，异常值可能导致精度使用效率低下。来源：([H.
    Wu等人 2020](ch058.xhtml#ref-wu2020integer))。
- en: 'The quality of calibration directly affects the performance of the quantized
    model. A poor calibration could lead to a model that suffers from significant
    accuracy loss, while a well-calibrated model can retain much of its original performance
    after quantization. There are two types of calibration ranges to consider:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 校准的质量直接影响量化模型的性能。不良的校准可能导致模型精度损失严重，而良好的校准模型在量化后仍能保留大部分原始性能。需要考虑两种类型的校准范围：
- en: '**Symmetric Calibration**: The clipping range is symmetric around zero, meaning
    both the positive and negative ranges are equally scaled.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对称校准**：裁剪范围围绕零对称，意味着正负范围具有相同的缩放。'
- en: '**Asymmetric Calibration**: The clipping range is not symmetric, which means
    the positive and negative ranges may have different scaling factors. This can
    be useful when the data is not centered around zero.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非对称校准**：裁剪范围不是对称的，这意味着正负范围可能具有不同的缩放因子。当数据不是围绕零中心时，这可能很有用。'
- en: Choosing the right calibration method and range is important for maintaining
    model accuracy while benefiting from the efficiency gains of reduced precision.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的校准方法和范围对于在提高效率的同时保持模型精度非常重要。
- en: Calibration Ranges
  id: totrans-493
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 校准范围
- en: 'A key challenge in post-training quantization is selecting the appropriate
    calibration range <semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>α</mi><mo>,</mo><mi>β</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[\alpha,
    \beta]</annotation></semantics> to map floating-point values into a lower-precision
    representation. The choice of this range directly affects the quantization error
    and, consequently, the accuracy of the quantized model. As illustrated in [Figure 10.20](ch016.xhtml#fig-calibration-ranges),
    there are two primary calibration strategies: symmetric calibration and asymmetric
    calibration.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练后量化中的一个关键挑战是选择合适的校准范围<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>α</mi><mo>,</mo><mi>β</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[\alpha,
    \beta]</annotation></semantics>，以将浮点值映射到低精度表示。这个范围的选择直接影响量化误差，从而影响量化模型的精度。如图[图10.20](ch016.xhtml#fig-calibration-ranges)所示，有两种主要的校准策略：对称校准和非对称校准。
- en: '![](../media/file166.svg)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file166.svg)'
- en: 'Figure 10.20: **Calibration Range Selection**: Symmetric calibration uses a
    fixed range around zero, while asymmetric calibration adapts the range to the
    data distribution, potentially minimizing quantization error and preserving model
    accuracy. Choosing an appropriate calibration strategy balances precision with
    the risk of saturation for outlier values.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.20：**校准范围选择**：对称校准使用围绕零的固定范围，而非对称校准则根据数据分布调整范围，可能最小化量化误差并保持模型精度。选择合适的校准策略需要在精度与异常值饱和风险之间取得平衡。
- en: On the left side of [Figure 10.20](ch016.xhtml#fig-calibration-ranges), symmetric
    calibration is depicted, where the clipping range is centered around zero. The
    range extends from <semantics><mrow><mi>α</mi><mo>=</mo><mi>−</mi><mn>1</mn></mrow><annotation
    encoding="application/x-tex">\alpha = -1</annotation></semantics> to <semantics><mrow><mi>β</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">\beta = 1</annotation></semantics>, mapping these
    values to the integer range <semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mn>127</mn><mo>,</mo><mn>127</mn><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[-127,
    127]</annotation></semantics>. This method ensures that positive and negative
    values are treated equally, preserving zero-centered distributions. A key advantage
    of symmetric calibration is its simplified implementation, as the same scale factor
    is applied to both positive and negative values. However, this approach may not
    be optimal for datasets where the activation distributions are skewed, leading
    to poor representation of significant portions of the data.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图10.20](ch016.xhtml#fig-calibration-ranges)的左侧，展示了对称校准，其中裁剪范围以零为中心。范围从<semantics><mrow><mi>α</mi><mo>=</mo><mi>−</mi><mn>1</mn></mrow><annotation
    encoding="application/x-tex">\alpha = -1</annotation></semantics>扩展到<semantics><mrow><mi>β</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">\beta = 1</annotation></semantics>，将这些值映射到整数范围<semantics><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>−</mi><mn>127</mn><mo>,</mo><mn>127</mn><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[-127,
    127]</annotation></semantics>。这种方法确保了正负值被同等对待，保留了以零为中心的分布。对称校准的一个关键优势是其简化了实现，因为相同的比例因子应用于正负值。然而，这种方法可能不适合激活分布偏斜的数据集，导致数据的重要部分表示不佳。
- en: On the right side, asymmetric calibration is shown, where <semantics><mrow><mi>α</mi><mo>=</mo><mi>−</mi><mn>0.5</mn></mrow><annotation
    encoding="application/x-tex">\alpha = -0.5</annotation></semantics> and <semantics><mrow><mi>β</mi><mo>=</mo><mn>1.5</mn></mrow><annotation
    encoding="application/x-tex">\beta = 1.5</annotation></semantics>. Here, zero
    is mapped to a shifted quantized value <semantics><mrow><mi>−</mi><mi>Z</mi></mrow><annotation
    encoding="application/x-tex">-Z</annotation></semantics>, and the range extends
    asymmetrically. In this case, the quantization scale is adjusted to account for
    non-zero mean distributions. Asymmetric calibration is particularly useful when
    activations or weights exhibit skew, ensuring that the full quantized range is
    effectively utilized. However, it introduces additional computational complexity
    in determining the optimal offset and scaling factors.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧，展示了非对称校准，其中 <semantics><mrow><mi>α</mi><mo>=</mo><mi>−</mi><mn>0.5</mn></mrow><annotation
    encoding="application/x-tex">\alpha = -0.5</annotation></semantics> 和 <semantics><mrow><mi>β</mi><mo>=</mo><mn>1.5</mn></mrow><annotation
    encoding="application/x-tex">\beta = 1.5</annotation></semantics>。在这里，零被映射到一个偏移的量化值
    <semantics><mrow><mi>−</mi><mi>Z</mi></mrow><annotation encoding="application/x-tex">-Z</annotation></semantics>，范围非对称地扩展。在这种情况下，量化尺度被调整以考虑非零均值分布。非对称校准在激活或权重表现出偏斜时特别有用，确保了整个量化范围的充分利用。然而，它引入了确定最佳偏移和缩放因子的额外计算复杂性。
- en: 'The choice between these calibration strategies depends on the model and dataset
    characteristics:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 这些校准策略之间的选择取决于模型和数据集的特征：
- en: Symmetric calibration is commonly used when weight distributions are centered
    around zero, which is often the case for well-initialized machine learning models.
    It simplifies computation and hardware implementation but may not be optimal for
    all scenarios.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当权重分布围绕零中心时，对称校准通常被使用，这对于良好初始化的机器学习模型来说是常见情况。它简化了计算和硬件实现，但可能不是所有场景的最佳选择。
- en: Asymmetric calibration is useful when the data distribution is skewed, ensuring
    that the full quantized range is effectively utilized. It can improve accuracy
    retention but may introduce additional computational complexity in determining
    the optimal quantization parameters.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据分布偏斜时，非对称校准是有用的，确保了整个量化范围的充分利用。它可以提高准确性保留，但可能在确定最佳量化参数时引入额外的计算复杂性。
- en: Many machine learning frameworks, including TensorRT and PyTorch, support both
    calibration modes, enabling practitioners to empirically evaluate the best approach.
    Selecting an appropriate calibration range is important for PTQ, as it directly
    influences the trade-off between numerical precision and efficiency, ultimately
    affecting the performance of quantized models.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习框架，包括TensorRT和PyTorch，都支持这两种校准模式，使从业者能够经验性地评估最佳方法。选择合适的校准范围对于PTQ很重要，因为它直接影响数值精度和效率之间的权衡，最终影响量化模型的性能。
- en: Granularity
  id: totrans-503
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 粒度
- en: After determining the clipping range, the next step in optimizing quantization
    involves adjusting the granularity of the clipping range to ensure that the model
    retains as much accuracy as possible. In CNNs, for instance, the input activations
    of a layer undergo convolution with multiple convolutional filters, each of which
    may have a unique range of values. The quantization process, therefore, must account
    for these differences in range across filters to preserve the model’s performance.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定截断范围之后，优化量化的下一步涉及调整截断范围的粒度，以确保模型尽可能保留准确性。例如，在卷积神经网络（CNNs）中，某一层的输入激活会与多个卷积滤波器进行卷积，每个滤波器可能具有独特的值范围。因此，量化过程必须考虑到这些滤波器之间范围的不同，以保持模型性能。
- en: As illustrated in [Figure 10.21](ch016.xhtml#fig-quantization-granularity),
    the range for Filter 1 is significantly smaller than that for Filter 3, demonstrating
    the variation in the magnitude of values across different filters. The precision
    with which the clipping range [<semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>,
    <semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>]
    is determined for the weights becomes a important factor in effective quantization.
    This variability in ranges is a key reason why different quantization strategies,
    based on granularity, are employed.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图10.21](ch016.xhtml#fig-quantization-granularity)所示，滤波器1的范围显著小于滤波器3的范围，这展示了不同滤波器之间值的大小的变化。确定权重剪裁范围[<semantics><mi>α</mi><annotation
    encoding="application/x-tex">\alpha</annotation></semantics>, <semantics><mi>β</mi><annotation
    encoding="application/x-tex">\beta</annotation></semantics>]的精度成为有效量化的重要因素。这种范围的可变性是采用基于粒度的不同量化策略的关键原因。
- en: '![](../media/file167.svg)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file167.svg)'
- en: 'Figure 10.21: **Quantization Range Variation**: Different convolutional filters
    exhibit unique activation ranges, necessitating per-filter quantization to minimize
    accuracy loss during quantization. Adjusting the granularity of clipping ranges—as
    shown by the differing scales for each filter—optimizes the trade-off between
    model size and performance. Source: ([Gholami et al. 2021](ch058.xhtml#ref-gholami2021survey)).'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.21：**量化范围变化**：不同的卷积滤波器表现出独特的激活范围，需要针对每个滤波器进行量化以最小化量化过程中的精度损失。调整剪裁范围的粒度——如每个滤波器不同的刻度所示——优化了模型大小和性能之间的权衡。来源：([Gholami等人
    2021](ch058.xhtml#ref-gholami2021survey))。
- en: Several methods are commonly used to determine the granularity of quantization,
    each with its own trade-offs in terms of accuracy, efficiency, and computational
    cost.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 确定量化粒度的几种常用方法各有其权衡，包括精度、效率和计算成本。
- en: Layerwise Quantization
  id: totrans-509
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 层量化
- en: In this approach, the clipping range is determined by considering all weights
    in the convolutional filters of a layer. The same clipping range is applied to
    all filters within the layer. While this method is simple to implement, it often
    leads to suboptimal accuracy due to the wide range of values across different
    filters. For example, if one convolutional kernel has a narrower range of values
    than another in the same layer, the quantization resolution of the narrower range
    may be compromised, resulting in a loss of information.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，剪裁范围是通过考虑层中卷积滤波器的所有权重来确定的。相同的剪裁范围应用于层内的所有滤波器。虽然这种方法易于实现，但由于不同滤波器之间值范围的广泛性，它通常会导致次优精度。例如，如果同一层中的一个卷积核的值范围比另一个窄，则较窄范围的量化分辨率可能会受损，从而导致信息丢失。
- en: Groupwise Quantization
  id: totrans-511
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 分组量化
- en: Groupwise quantization divides the convolutional filters into groups and calculates
    a shared clipping range for each group. This method can be beneficial when the
    distribution of values within a layer is highly variable. For example, the Q-BERT
    model ([Shen et al. 2019](ch058.xhtml#ref-sheng2019qbert)) applied this technique
    when quantizing Transformer models ([Vaswani et al. 2017](ch058.xhtml#ref-vaswani2017attention)),
    particularly for the fully-connected attention layers. While groupwise quantization
    offers better accuracy than layerwise quantization, it incurs additional computational
    cost due to the need to account for multiple scaling factors.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 分组量化将卷积滤波器分为组，并为每个组计算一个共享的剪裁范围。当层内值的分布高度可变时，这种方法可能有益。例如，Q-BERT模型([Shen等人 2019](ch058.xhtml#ref-sheng2019qbert))在量化Transformer模型([Vaswani等人
    2017](ch058.xhtml#ref-vaswani2017attention))时应用了这项技术，特别是对于全连接注意力层。虽然分组量化比层量化提供了更高的精度，但它由于需要考虑多个缩放因子而增加了额外的计算成本。
- en: Channelwise Quantization
  id: totrans-513
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 通道量化
- en: Channelwise quantization assigns a dedicated clipping range and scaling factor
    to each convolutional filter. This approach ensures a higher resolution in quantization,
    as each channel is quantized independently. Channelwise quantization is widely
    used in practice, as it often yields better accuracy compared to the previous
    methods. By allowing each filter to have its own clipping range, this method ensures
    that the quantization process is tailored to the specific characteristics of each
    filter.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 通道量化为每个卷积滤波器分配一个专门的裁剪范围和缩放因子。这种方法确保了量化具有更高的分辨率，因为每个通道都是独立量化的。通道量化在实践中被广泛使用，因为它通常比以前的方法提供更好的精度。通过允许每个滤波器都有自己的裁剪范围，这种方法确保了量化过程针对每个滤波器的特定特征进行了定制。
- en: Sub-channelwise Quantization
  id: totrans-515
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 子通道量化
- en: Sub-channelwise quantization subdivides each convolutional filter into smaller
    groups, each with its own clipping range. Although it provides very fine-grained
    control over quantization, it introduces significant computational overhead as
    multiple scaling factors must be managed for each group within a filter. As a
    result, sub-channelwise quantization is generally only used in scenarios where
    maximum precision is required, despite the increased computational cost.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 子通道量化将每个卷积滤波器细分为更小的组，每组都有自己的裁剪范围。尽管它提供了对量化的非常精细的控制，但它引入了显著的计算开销，因为必须在滤波器内的每个组中管理多个缩放因子。因此，尽管计算成本增加，子通道量化通常仅在需要最大精度的场景中使用。
- en: Among these methods, channelwise quantization is the current standard for quantizing
    convolutional filters. It strikes a balance between the accuracy gains from finer
    granularity and the computational efficiency needed for practical deployment.
    Adjusting the clipping range for each individual kernel provides significant improvements
    in model accuracy with minimal overhead, making it the most widely adopted approach
    in machine learning applications.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方法中，通道量化是量化卷积滤波器的当前标准。它在更细粒度带来的精度提升和实际部署所需的计算效率之间取得了平衡。调整每个单独内核的裁剪范围可以显著提高模型精度，同时开销最小，这使得它成为机器学习应用中最广泛采用的方法。
- en: Weights vs. Activations
  id: totrans-518
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 权重与激活
- en: Weight Quantization involves converting the continuous, high-precision weights
    of a model into lower-precision values, such as converting 32-bit floating-point
    (Float32) weights to 8-bit integer (INT8) weights. As illustrated in [Figure 10.22](ch016.xhtml#fig-weight-activations-quantization),
    weight quantization occurs in the second step (red squares) during the multiplication
    of inputs. This process significantly reduces the model size, decreasing both
    the memory required to store the model and the computational resources needed
    for inference. For example, a weight matrix in a neural network layer with Float32
    weights like <semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0.215</mn><mo>,</mo><mi>−</mi><mn>1.432</mn><mo>,</mo><mn>0.902</mn><mo>,</mo><mi>…</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0.215,
    -1.432, 0.902,\ldots]</annotation></semantics> might be mapped to INT8 values
    such as <semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>27</mn><mo>,</mo><mi>−</mi><mn>183</mn><mo>,</mo><mn>115</mn><mo>,</mo><mi>…</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[27,
    -183, 115, \ldots]</annotation></semantics>, leading to a significant reduction
    in memory usage.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 权重量化涉及将模型的连续、高精度权重转换为低精度值，例如将32位浮点（Float32）权重转换为8位整数（INT8）权重。如图[图10.22](ch016.xhtml#fig-weight-activations-quantization)所示，权重量化发生在输入乘法的第二步（红色方块）中。这个过程显著减小了模型大小，减少了存储模型所需的内存和推理所需的计算资源。例如，一个具有Float32权重的神经网络层的权重矩阵，如<semantics><mrow><mo
    stretchy="true" form="prefix">[</mo><mn>0.215</mn><mo>,</mo><mi>−</mi><mn>1.432</mn><mo>,</mo><mn>0.902</mn><mo>,</mo><mi>…</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0.215,
    -1.432, 0.902,\ldots]</annotation></semantics>，可能映射到INT8值，如<semantics><mrow><mo
    stretchy="true" form="prefix">[</mo><mn>27</mn><mo>,</mo><mi>−</mi><mn>183</mn><mo>,</mo><mn>115</mn><mo>,</mo><mi>…</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[27,
    -183, 115, \ldots]</annotation></semantics>，从而显著减少内存使用。
- en: '![](../media/file168.svg)'
  id: totrans-520
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file168.svg)'
- en: 'Figure 10.22: **Quantization and Weight Precision**: Reducing weight and activation
    precision from float32 to INT8 significantly lowers model size and computational
    cost during inference by representing values with fewer bits, though it may introduce
    a trade-off with model accuracy. This process alters the numerical representation
    of model parameters and intermediate results, impacting both memory usage and
    processing speed. Source: HarvardX.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.22：**量化和权重精度**：将权重和激活精度从 float32 降低到 INT8，通过使用更少的位来表示值，可以显著降低推理过程中的模型大小和计算成本，尽管这可能会与模型精度产生权衡。这个过程改变了模型参数和中间结果的数值表示，影响内存使用和处理速度。来源：HarvardX。
- en: Activation Quantization refers to the process of quantizing the activation values,
    or outputs of the layers, during model inference. This quantization can reduce
    the computational resources required during inference, particularly when targeting
    hardware optimized for integer arithmetic. It introduces challenges related to
    maintaining model accuracy, as the precision of intermediate computations is reduced.
    For instance, in a CNN, the activation maps (or feature maps) produced by convolutional
    layers, originally represented in Float32, may be quantized to INT8 during inference.
    This can significantly accelerate computation on hardware capable of efficiently
    processing lower-precision integers.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 激活量化是指在模型推理过程中量化激活值或层输出的过程。这种量化可以减少推理过程中所需的计算资源，尤其是在针对优化整数运算的硬件时。它引入了与保持模型精度相关的问题，因为中间计算的精度降低了。例如，在
    CNN 中，由卷积层产生的激活图（或特征图），最初以 Float32 表示，在推理过程中可能被量化为 INT8。这可以在能够高效处理低精度整数的硬件上显著加速计算。
- en: Recent advancements have explored Activation-aware Weight Quantization (AWQ)
    for the compression and acceleration of large language models (LLMs). This approach
    focuses on protecting only a small fraction of the most salient weights, approximately
    1%, by observing the activations rather than the weights themselves. This method
    has been shown to improve model efficiency while preserving accuracy, as discussed
    in ([Ji Lin, Tang, et al. 2023](ch058.xhtml#ref-lin2023awq)).
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究探索了用于大型语言模型（LLMs）压缩和加速的激活感知权重量化（AWQ）。这种方法通过观察激活而不是权重本身，仅保护一小部分最显著的权重，大约
    1%。这种方法已被证明可以提高模型效率，同时保持精度，如 [Ji Lin, Tang, 等人 2023](ch058.xhtml#ref-lin2023awq)
    中所讨论。
- en: Static vs. Dynamic Quantization
  id: totrans-524
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 静态量化与动态量化
- en: 'After determining the type and granularity of the clipping range, practitioners
    must decide when the clipping ranges are calculated in their quantization algorithms.
    Two primary approaches exist for quantizing activations: static quantization and
    dynamic quantization.'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定截断范围的类型和粒度之后，从业者必须决定在量化算法中何时计算截断范围。量化激活值主要有两种方法：静态量化和动态量化。
- en: Static Quantization is the more commonly used approach. In static quantization,
    the clipping range is pre-calculated and remains fixed during inference. This
    method does not introduce any additional computational overhead during runtime,
    which makes it efficient in terms of computational resources. However, the fixed
    range can lead to lower accuracy compared to dynamic quantization. A typical implementation
    of static quantization involves running a series of calibration inputs to compute
    the typical range of activations ([Jacob et al. 2018b](ch058.xhtml#ref-jacob2018quantization);
    [Yao et al. 2021](ch058.xhtml#ref-yao2021hawq)).
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 静态量化是更常用的方法。在静态量化中，截断范围是预先计算的，并在推理过程中保持固定。这种方法在运行时不引入任何额外的计算开销，因此在计算资源方面效率很高。然而，固定的范围可能导致与动态量化相比的精度降低。静态量化的典型实现包括运行一系列校准输入来计算激活值的典型范围（[Jacob
    等人 2018b](ch058.xhtml#ref-jacob2018quantization)；[Yao 等人 2021](ch058.xhtml#ref-yao2021hawq)）。
- en: In contrast, Dynamic Quantization dynamically calculates the range for each
    activation map during runtime. This approach allows the quantization process to
    adjust in real time based on the input, potentially yielding higher accuracy since
    the range is specifically calculated for each input activation. However, dynamic
    quantization incurs higher computational overhead because the range must be recalculated
    at each step. Although this often results in higher accuracy, the real-time computations
    can be expensive, particularly when deployed at scale.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相反，动态量化在运行时动态计算每个激活图的范围。这种方法允许量化过程根据输入实时调整，可能由于范围是针对每个输入激活专门计算的，因此可能产生更高的准确度。然而，动态量化需要更高的计算开销，因为必须在每个步骤中重新计算范围。尽管这通常会导致更高的准确度，但实时计算可能非常昂贵，尤其是在大规模部署时。
- en: The following table, [Table 10.8](ch016.xhtml#tbl-quantization_methods), summarizes
    the characteristics of post-training quantization, quantization-aware training,
    and dynamic quantization, providing an overview of their respective strengths,
    limitations, and trade-offs. These methods are widely deployed across machine
    learning systems of varying scales, and understanding their pros and cons is important
    for selecting the appropriate approach for a given application.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 下表 [表 10.8](ch016.xhtml#tbl-quantization_methods) 总结了训练后量化、量化感知训练和动态量化的特征，概述了它们各自的优势、局限性和权衡。这些方法在各个规模的机器学习系统中广泛部署，理解它们的优缺点对于选择适合特定应用的方法非常重要。
- en: 'Table 10.8: **Quantization Trade-Offs**: Post-training quantization, quantization-aware
    training, and dynamic quantization represent distinct approaches to model compression,
    each balancing accuracy, computational cost, and implementation complexity for
    machine learning systems. Understanding these trade-offs is important for selecting
    the optimal quantization strategy based on application requirements and resource
    constraints.'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.8：**量化权衡**：训练后量化、量化感知训练和动态量化代表了模型压缩的不同方法，每种方法都在机器学习系统中平衡准确度、计算成本和实现复杂性。理解这些权衡对于根据应用需求和资源限制选择最佳量化策略非常重要。
- en: '| **Aspect** | **Post Training Quantization** | **Quantization-Aware Training**
    | **Dynamic Quantization** |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **训练后量化** | **量化感知训练** | **动态量化** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Pros** |  |  |  |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| **Pros** |  |  |  |'
- en: '| **Simplicity** | ✓ | ✗ | ✗ |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| **简单性** | ✓ | ✗ | ✗ |'
- en: '| **Accuracy Preservation** | ✗ | ✓ | ✓ |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| **准确度保持** | ✗ | ✓ | ✓ |'
- en: '| **Adaptability** | ✗ | ✗ | ✓ |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| **适应性** | ✗ | ✗ | ✓ |'
- en: '| **Optimized Performance** | ✗ | ✓ | Potentially |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| **优化性能** | ✗ | ✓ | 可能 |'
- en: '| **Cons** |  |  |  |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| **Cons** |  |  |  |'
- en: '| **Accuracy Degradation** | ✓ | ✗ | Potentially |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| **准确度下降** | ✓ | ✗ | 可能 |'
- en: '| **Computational Overhead** | ✗ | ✓ | ✓ |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| **计算开销** | ✗ | ✓ | ✓ |'
- en: '| **Implementation Complexity** | ✗ | ✓ | ✓ |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| **实现复杂性** | ✗ | ✓ | ✓ |'
- en: '| **Tradeoffs** |  |  |  |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| **权衡** |  |  |  |'
- en: '| **Speed vs. Accuracy** | ✓ | ✗ | ✗ |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| **速度 vs. 准确度** | ✓ | ✗ | ✗ |'
- en: '| **Accuracy vs. Cost** | ✗ | ✓ | ✗ |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| **准确度 vs. 成本** | ✗ | ✓ | ✗ |'
- en: '| **Adaptability vs. Overhead** | ✗ | ✗ | ✓ |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| **适应性 vs. 开销** | ✗ | ✗ | ✓ |'
- en: PTQ Advantages
  id: totrans-545
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PTQ 优点
- en: One of the key advantages of PTQ is its low computational cost, as it does not
    require retraining the model. This makes it an attractive option for the rapid
    deployment of trained models, particularly when retraining is computationally
    expensive or infeasible. Since PTQ only modifies the numerical representation
    of weights and activations, the underlying model architecture remains unchanged,
    allowing it to be applied to a wide range of pre-trained models without modification.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ 的一个关键优点是其低计算成本，因为它不需要重新训练模型。这使得它成为快速部署训练模型的吸引人选择，尤其是在重新训练计算成本高昂或不可行的情况下。由于
    PTQ 只修改权重和激活的数值表示，因此底层模型架构保持不变，允许它应用于广泛的预训练模型而无需修改。
- en: PTQ also provides significant memory and storage savings by reducing the bit-width
    of model parameters. For instance, converting a model from FP32 to INT8 results
    in a <semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">4\times</annotation></semantics>
    reduction in storage size, making it feasible to deploy larger models on resource-constrained
    devices such as mobile phones, edge AI hardware, and embedded systems. These reductions
    in memory footprint also lead to lower bandwidth requirements when transferring
    models across networked systems.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ通过减少模型参数的位宽提供了显著的内存和存储节省。例如，将模型从FP32转换为INT8会导致存储大小减少<semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">4\times</annotation></semantics>，这使得在资源受限的设备上部署更大的模型成为可能，例如手机、边缘AI硬件和嵌入式系统。这些内存占用减少也导致在通过网络系统传输模型时带宽需求降低。
- en: In terms of computational efficiency, PTQ allows inference to be performed using
    integer arithmetic, which is inherently faster than floating-point operations
    on many hardware platforms. AI accelerators such as TPUs and Neural Processing
    Units (NPUs) are optimized for lower-precision computations, enabling higher throughput
    and reduced power consumption when executing quantized models. This makes PTQ
    particularly useful for applications requiring real-time inference, such as object
    detection in autonomous systems or speech recognition on mobile devices.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算效率方面，PTQ允许使用整数算术进行推理，这在许多硬件平台上比浮点运算更快。AI加速器，如TPUs和神经网络单元（NPUs），针对低精度计算进行了优化，在执行量化模型时能够实现更高的吞吐量和降低功耗。这使得PTQ特别适用于需要实时推理的应用，例如自主系统中的目标检测或移动设备上的语音识别。
- en: PTQ Challenges and Limitations
  id: totrans-549
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PTQ的挑战和局限性
- en: Despite its advantages, PTQ introduces quantization errors due to rounding effects
    when mapping floating-point values to discrete lower-precision representations.
    While some models remain robust to these changes, others may experience notable
    accuracy degradation, especially in tasks that rely on small numerical differences.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PTQ具有优势，但在将浮点值映射到离散的低精度表示时，由于舍入效应，它引入了量化误差。虽然一些模型对这些变化具有鲁棒性，但其他模型可能会经历显著的准确度下降，尤其是在依赖于微小数值差异的任务中。
- en: The extent of accuracy loss depends on both the model architecture and the task
    domain. CNNs for image classification are generally tolerant to PTQ, often maintaining
    near-original accuracy even with aggressive INT8 quantization. Transformer-based
    models used in NLP and speech recognition tend to be more sensitive, as these
    architectures rely on the precision of numerical relationships in attention mechanisms.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度损失的程度取决于模型架构和任务领域。用于图像分类的CNN通常对PTQ具有容忍性，即使在激进的INT8量化下也能保持接近原始的准确度。在NLP和语音识别中使用的基于Transformer的模型则更敏感，因为这些架构依赖于注意力机制中数值关系的精度。
- en: To mitigate accuracy loss, calibration techniques such as KL divergence-based
    scaling or per-channel quantization are commonly applied to fine-tune the scaling
    factor and minimize information loss. Some frameworks, including TensorFlow Lite
    and PyTorch, provide automated quantization tools with built-in calibration methods
    to improve accuracy retention.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻准确度损失，通常会应用如基于KL散度的缩放或逐通道量化等校准技术来微调缩放因子并最小化信息损失。一些框架，包括TensorFlow Lite和PyTorch，提供了具有内置校准方法的自动化量化工具，以提高准确度保留。
- en: Another limitation of PTQ is that not all hardware supports efficient integer
    arithmetic. While GPUs, TPUs, and specialized edge AI chips often include dedicated
    support for INT8 inference, general-purpose CPUs may lack the optimized instructions
    for low-precision execution, resulting in suboptimal performance improvements.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ的另一个局限性是并非所有硬件都支持高效的整数算术。虽然GPU、TPUs和专门的边缘AI芯片通常包括针对INT8推理的专用支持，但通用CPU可能缺乏用于低精度执行的优化指令，导致性能提升不佳。
- en: PTQ is not always suitable for training purposes. Since PTQ applies quantization
    after training, models that require further fine-tuning or adaptation may benefit
    more from alternative approaches, such as quantization-aware training (discussed
    next), to ensure that precision constraints are adequately considered during the
    learning process.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ并不总是适合用于训练目的。由于PTQ在训练后应用量化，需要进一步微调或适应的模型可能从替代方法中受益更多，例如量化感知训练（下文将讨论），以确保在学习过程中充分考虑到精度约束。
- en: Post-training quantization remains one of the most practical and widely used
    techniques for improving inference efficiency. It provides significant memory
    and computational savings with minimal overhead, making it an ideal choice for
    deploying machine learning models in resource-constrained environments. However,
    the success of PTQ depends on model architecture, task sensitivity, and hardware
    compatibility. In scenarios where accuracy degradation is unacceptable, alternative
    quantization strategies, such as quantization-aware training, may be required.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 培训后量化仍然是提高推理效率最实用和最广泛使用的技术之一。它提供了显著的内存和计算节省，同时开销最小，使其成为在资源受限环境中部署机器学习模型的理想选择。然而，PTQ
    的成功取决于模型架构、任务敏感性和硬件兼容性。在精度下降不可接受的情况下，可能需要采用替代量化策略，例如量化感知训练。
- en: Post-training quantization provides the foundation for more advanced quantization
    methods. The core concepts—quantization workflows, numerical format trade-offs,
    and calibration methods—remain essential throughout all precision optimization
    techniques. For rapid deployment scenarios with production deadlines under two
    weeks and acceptable accuracy loss of 1-2%, PTQ with min-max calibration often
    provides a complete solution. Production systems requiring less than 1% accuracy
    loss should consider Quantization-Aware Training, which recovers accuracy through
    fine-tuning with quantization simulation at the cost of 20-50% additional training
    time. Extreme constraints like sub-1MB models or sub-10mW power budgets may require
    INT4 or binary quantization, accepting 5-20% accuracy degradation that necessitates
    architectural changes.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 培训后量化是更高级量化方法的基础。核心概念——量化工作流程、数值格式权衡和校准方法——在所有精度优化技术中始终至关重要。对于生产截止日期在两周以内且可接受的精度损失为
    1-2% 的快速部署场景，PTQ 与 min-max 校准通常提供完整的解决方案。需要低于 1% 精度损失的量产系统应考虑量化感知训练，它通过量化模拟的微调来恢复精度，但代价是额外
    20-50% 的训练时间。极端限制，如小于 1MB 的模型或小于 10mW 的功耗预算，可能需要 INT4 或二进制量化，接受 5-20% 的精度下降，这需要架构变更。
- en: Quantization-Aware Training
  id: totrans-557
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化感知训练
- en: 'QAT integrates quantization constraints directly into the training process,
    simulating low-precision arithmetic during forward passes to allow the model to
    adapt to quantization effects ([Jacob et al. 2018b](ch058.xhtml#ref-jacob2018quantization)).
    This approach proves particularly important for models requiring fine-grained
    numerical precision, such as transformers used in NLP and speech recognition systems
    ([Nagel et al. 2021b](ch058.xhtml#ref-nagel2021whitepaper)). [Figure 10.23](ch016.xhtml#fig-qat)
    illustrates the QAT process: quantization is applied to a pre-trained model, followed
    by fine-tuning to adapt weights to low-precision constraints.'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: QAT将量化约束直接集成到训练过程中，在正向传播过程中模拟低精度算术，以允许模型适应量化效应（[Jacob 等人 2018b](ch058.xhtml#ref-jacob2018quantization)）。这种方法对于需要精细数值精度的模型尤为重要，例如在自然语言处理和语音识别系统中使用的变压器（[Nagel
    等人 2021b](ch058.xhtml#ref-nagel2021whitepaper)）。[图 10.23](ch016.xhtml#fig-qat)
    展示了 QAT 的过程：量化应用于预训练模型，随后进行微调以适应低精度约束。
- en: '![](../media/file169.svg)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file169.svg)'
- en: 'Figure 10.23: **Quantization-Aware Training**: Retraining a pre-trained model
    with simulated low-precision arithmetic adapts weights to mitigate accuracy loss
    during deployment with reduced numerical precision, enabling efficient inference
    on resource-constrained devices. This process refines the model to become robust
    to the effects of quantization, maintaining performance despite lower precision
    representations.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.23：**量化感知训练**：使用模拟的低精度算术重新训练预训练模型，以适应降低数值精度的部署，从而减轻精度损失，在资源受限的设备上实现高效的推理。这个过程使模型对量化效应具有鲁棒性，即使在精度较低的情况下也能保持性能。
- en: In many cases, QAT can also build off PTQ (discussed in detail in the previous
    section), as shown in [Figure 10.24](ch016.xhtml#fig-ptq-qat). Instead of starting
    from a full-precision model, PTQ is first applied to produce an initial quantized
    model using calibration data. This quantized model then serves as the starting
    point for QAT, where additional fine-tuning with training data helps the model
    better adapt to low-precision constraints. This hybrid approach combines PTQ’s
    efficiency with QAT’s accuracy preservation, reducing the degradation typically
    associated with post-training approaches alone.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，QAT也可以建立在PTQ之上（如前节所述），如图10.24所示。不是从全精度模型开始，PTQ首先应用于使用校准数据生成一个初始量化模型。然后，这个量化模型作为QAT的起点，使用训练数据进行额外的微调，有助于模型更好地适应低精度约束。这种混合方法结合了PTQ的效率与QAT的精度保持，减少了仅使用训练后方法通常相关的退化。
- en: '![](../media/file170.svg)'
  id: totrans-562
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file170.svg)'
- en: 'Figure 10.24: **Hybrid Quantization Approach**: Post-training quantization
    (PTQ) generates an initial quantized model that serves as a warm start for quantization-aware
    training (QAT), accelerating convergence and mitigating accuracy loss compared
    to quantizing a randomly initialized network. This two-stage process leverages
    the efficiency of PTQ while refining the model with training data to optimize
    performance under low-precision constraints.'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.24：**混合量化方法**：训练后量化（PTQ）生成一个初始量化模型，该模型作为量化感知训练（QAT）的预热启动，与随机初始化网络的量化相比，加速收敛并减轻精度损失。这个两阶段过程利用了PTQ的效率，同时使用训练数据来优化低精度约束下的性能。
- en: Training Mathematics
  id: totrans-564
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练数学
- en: 'During forward propagation, weights and activations are quantized and dequantized
    to mimic reduced precision. This process is typically represented as: <semantics><mrow><mi>q</mi><mo>=</mo><mtext
    mathvariant="normal">round</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><mi>s</mi></mrow> <annotation
    encoding="application/x-tex">q = \text{round} \left(\frac{x}{s} \right) \times
    s</annotation></semantics> where <semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics>
    represents the simulated quantized value, <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    denotes the full-precision weight or activation, and <semantics><mi>s</mi><annotation
    encoding="application/x-tex">s</annotation></semantics> is the scaling factor
    mapping floating-point values to lower-precision integers.'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 在正向传播过程中，权重和激活被量化和解量化以模拟降低精度。这个过程通常表示为：<semantics><mrow><mi>q</mi><mo>=</mo><mtext
    mathvariant="normal">round</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><mi>s</mi></mrow> <annotation
    encoding="application/x-tex">q = \text{round} \left(\frac{x}{s} \right) \times
    s</annotation></semantics> 其中 <semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics>
    表示模拟的量化值，<semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    表示全精度权重或激活，而 <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>
    是缩放因子，将浮点值映射到低精度整数。
- en: Although the forward pass utilizes quantized values, gradient calculations during
    backpropagation remain in full precision. This is achieved using the Straight-Through
    Estimator (STE)[32](#fn32), which approximates the gradient of the quantized function
    by treating the rounding operation as if it had a derivative of one. This approach
    prevents the gradient from being obstructed due to the non-differentiable nature
    of the quantization operation, thereby allowing effective model training ([Y.
    Bengio, Léonard, and Courville 2013a](ch058.xhtml#ref-bengio2013estimating)).
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然正向传播使用量化值，但在反向传播中的梯度计算保持全精度。这是通过使用直通估计器（STE）[32](#fn32)实现的，该估计器通过将舍入操作视为具有导数为一的导数来近似量化函数的梯度。这种方法防止了由于量化操作的非可微性质而阻碍梯度，从而允许有效模型训练（[Y.
    Bengio, Léonard, and Courville 2013a](ch058.xhtml#ref-bengio2013estimating)）。
- en: Integrating quantization effects during training enables the model to learn
    an optimal distribution of weights and activations that minimizes the impact of
    numerical precision loss. The resulting model, when deployed using true low-precision
    arithmetic (e.g., INT8 inference), maintains significantly higher accuracy than
    one that is quantized post hoc ([Krishnamoorthi 2018](ch058.xhtml#ref-krishnamoorthi2018quantizing)).
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中整合量化效果使模型能够学习一个最优的权重和激活分布，以最小化数值精度损失的影响。当使用真正的低精度算术（例如，INT8 推理）部署时，所得到的模型与事后量化的模型相比，保持了显著更高的精度([Krishnamoorthi
    2018](ch058.xhtml#ref-krishnamoorthi2018quantizing))。
- en: QAT Advantages
  id: totrans-568
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: QAT 优势
- en: A primary advantage of QAT[33](#fn33) is its ability to maintain model accuracy,
    even under low-precision inference conditions. Incorporating quantization during
    training helps the model to compensate for precision loss, reducing the impact
    of rounding errors and numerical instability. This is important for quantization-sensitive
    models commonly used in NLP, speech recognition, and high-resolution computer
    vision ([Gholami et al. 2021](ch058.xhtml#ref-gholami2021survey)).
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: QAT[33](#fn33) 的一个主要优势是，即使在低精度推理条件下，也能保持模型精度。在训练过程中引入量化有助于模型补偿精度损失，减少舍入误差和数值不稳定性。这对于在
    NLP、语音识别和高分辨率计算机视觉中常用的量化敏感模型来说非常重要([Gholami 等人 2021](ch058.xhtml#ref-gholami2021survey))。
- en: Another major benefit is that QAT permits low-precision inference on hardware
    accelerators without significant accuracy degradation. AI processors such as TPUs,
    NPUs, and specialized edge devices include dedicated hardware for integer operations,
    permitting INT8 models to run much faster and with lower energy consumption compared
    to FP32 models. Training with quantization effects in mind ensures that the final
    model can fully leverage these hardware optimizations ([H. Wu et al. 2020](ch058.xhtml#ref-wu2020integer)).
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个主要好处是，QAT 允许在硬件加速器上进行低精度推理，而不会显著降低精度。AI 处理器，如 TPUs、NPUs 和专用边缘设备，包括用于整数操作的专用硬件，这使得
    INT8 模型比 FP32 模型运行得更快，能耗更低。考虑到量化效果进行训练确保最终模型可以充分利用这些硬件优化([H. Wu 等人 2020](ch058.xhtml#ref-wu2020integer))。
- en: QAT Challenges and Trade-offs
  id: totrans-571
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: QAT 挑战与权衡
- en: Despite its benefits, QAT introduces additional computational overhead during
    training. Simulated quantization at every forward pass slows down training relative
    to full-precision methods. The process adds complexity to the training schedule,
    making QAT less practical for very large-scale models where the additional training
    time might be prohibitive.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管QAT有其好处，但在训练过程中会引入额外的计算开销。每次前向传递时的模拟量化会减慢训练速度，相对于全精度方法。这个过程增加了训练计划的复杂性，使得QAT对于可能因额外训练时间而变得不可行的超大规模模型来说不太实用。
- en: QAT introduces extra hyperparameters and design considerations, such as choosing
    appropriate quantization schemes and scaling factors. Unlike PTQ, which applies
    quantization after training, QAT requires careful tuning of the training dynamics
    to ensure that the model suitably adapts to low-precision constraints ([Gong et
    al. 2019](ch058.xhtml#ref-choukroun2019low)).
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: QAT 引入了额外的超参数和设计考虑，例如选择合适的量化方案和缩放因子。与PTQ（在训练后应用量化）不同，QAT 需要仔细调整训练动态，以确保模型能够适当地适应低精度约束([Gong
    等人 2019](ch058.xhtml#ref-choukroun2019low))。
- en: '[Table 10.9](ch016.xhtml#tbl-qat) summarizes the key trade-offs of QAT compared
    to PTQ:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10.9](ch016.xhtml#tbl-qat) 总结了 QAT 与 PTQ 的关键权衡：'
- en: 'Table 10.9: **Quantization Trade-Offs**: Quantization-aware training (QAT)
    minimizes accuracy loss from reduced numerical precision by incorporating quantization
    into the training process, while post-training quantization (PTQ) offers faster
    deployment but may require calibration to mitigate accuracy degradation. QAT’s
    retraining requirement increases training complexity compared to the simplicity
    of applying PTQ to a pre-trained model.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.9：**量化权衡**：量化感知训练（QAT）通过将量化纳入训练过程，最小化由降低数值精度引起的精度损失，而训练后量化（PTQ）提供了更快的部署，但可能需要校准以减轻精度退化。与将
    PTQ 应用于预训练模型相比，QAT 的重新训练需求增加了训练复杂性。
- en: '| **Aspect** | **QAT (Quantization-Aware Training)** | **PTQ (Post-Training
    Quantization)** |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **QAT (量化感知训练)** | **PTQ (训练后量化)** |'
- en: '| --- | --- | --- |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Accuracy Retention** | Minimizes accuracy loss from quantization | May
    suffer from accuracy degradation |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| **精度保留** | 最小化量化带来的精度损失 | 可能会遭受精度退化 |'
- en: '| **Inference Efficiency** | Optimized for low-precision hardware (e.g., INT8
    on TPUs) | Optimized but may require calibration |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| **推理效率** | 优化用于低精度硬件（例如，TPU上的INT8） | 优化但可能需要校准 |'
- en: '| **Training Complexity** | Requires retraining with quantization constraints
    | No retraining required |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| **训练复杂度** | 需要使用量化约束重新训练 | 不需要重新训练 |'
- en: '| **Training Time** | Slower due to simulated quantization in forward pass
    | Faster, as quantization is applied post hoc |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| **训练时间** | 由于正向传播中模拟量化而变慢 | 由于量化是在事后应用而变快 |'
- en: '| **Deployment Readiness** | Best for models sensitive to quantization errors
    | Fastest way to optimize models for inference |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| **部署准备** | 适用于对量化误差敏感的模型 | 优化模型以用于推理的最快方式 |'
- en: Integrating quantization into the training process preserves model accuracy
    more effectively than post-training quantization, although it requires additional
    training resources and time.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 将量化集成到训练过程中比训练后量化更有效地保留了模型准确性，尽管这需要额外的训练资源和时间。
- en: PTQ vs. QAT
  id: totrans-584
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PTQ与QAT
- en: The choice between PTQ and QAT depends on trade-offs between accuracy, computational
    cost, and deployment constraints. PTQ provides computationally inexpensive optimization
    requiring only post-training conversion, making it ideal for rapid deployment.
    However, effectiveness varies by architecture—CNNs tolerate PTQ well while NLP
    and speech models may experience degradation due to reliance on precise numerical
    representations.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ和QAT之间的选择取决于精度、计算成本和部署约束之间的权衡。PTQ提供了一种计算成本低的优化方法，只需要训练后的转换，使其非常适合快速部署。然而，其有效性因架构而异——CNN对PTQ的容忍度较好，而NLP和语音模型可能由于依赖精确数值表示而经历退化。
- en: QAT proves necessary when high accuracy retention is critical. Integrating quantization
    effects during training allows models to adapt to lower-precision arithmetic,
    reducing quantization errors ([Jacob et al. 2018c](ch058.xhtml#ref-Jacob2018)).
    While achieving higher low-precision accuracy, QAT requires additional training
    time and computational resources. In practice, a hybrid approach starting with
    PTQ and selectively applying QAT for accuracy-critical models provides optimal
    balance between efficiency and performance.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 当高精度保留至关重要时，QAT是必要的。在训练过程中集成量化效果允许模型适应低精度算术，减少量化误差([Jacob et al. 2018c](ch058.xhtml#ref-Jacob2018))。虽然实现了更高的低精度准确性，但QAT需要额外的训练时间和计算资源。在实践中，一种混合方法从PTQ开始，并针对准确性至关重要的模型选择性地应用QAT，在效率和性能之间提供了最佳平衡。
- en: Extreme Quantization
  id: totrans-587
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 极端量化
- en: Beyond INT8 and INT4 quantization, extreme quantization techniques use 1-bit
    (binarization) or 2-bit (ternarization) representations to achieve dramatic reductions
    in memory usage and computational requirements ([Courbariaux, Bengio, and David
    2016](ch058.xhtml#ref-Courbariaux2016)). Binarization constrains weights and activations
    to two values (typically -1 and +1, or 0 and 1), drastically reducing model size
    and accelerating inference on specialized hardware like binary neural networks
    ([Rastegari et al. 2016](ch058.xhtml#ref-Rastegari2016)). However, this constraint
    severely limits model expressiveness, often degrading accuracy on tasks requiring
    high precision such as image recognition or natural language processing ([Hubara
    et al. 2018](ch058.xhtml#ref-Hubara2018)).
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，极端量化技术使用1位（二值化）或2位（三值化）表示来显著降低内存使用和计算需求([Courbariaux, Bengio, and David
    2016](ch058.xhtml#ref-Courbariaux2016))。二值化将权重和激活值限制为两个值（通常是-1和+1，或0和1），大幅减小模型大小并加速在二进制神经网络等专用硬件上的推理([Rastegari
    et al. 2016](ch058.xhtml#ref-Rastegari2016))。然而，这种限制严重限制了模型的表达能力，通常会导致在需要高精度的任务（如图像识别或自然语言处理）上的准确性下降([Hubara
    et al. 2018](ch058.xhtml#ref-Hubara2018))。
- en: Ternarization extends binarization by allowing three values (-1, 0, +1), providing
    additional flexibility that slightly improves accuracy over pure binarization
    ([Zhu et al. 2017](ch058.xhtml#ref-Zhu2017)). The zero value enables greater sparsity
    while maintaining more representational power. Both techniques require gradient
    approximation methods like Straight-Through Estimator (STE) to handle non-differentiable
    quantization operations during training ([Y. Bengio, Léonard, and Courville 2013b](ch058.xhtml#ref-Bengio2013)),
    with QAT integration helping mitigate accuracy loss ([J. Choi et al. 2018](ch058.xhtml#ref-Choi2019)).
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 三值化通过允许三个值（-1、0、+1）扩展了二值化，提供了额外的灵活性，在纯二值化之上略微提高了精度（[Zhu 等人 2017](ch058.xhtml#ref-Zhu2017)）。零值使得稀疏性更高，同时保持了更强的表示能力。这两种技术都需要梯度近似方法，如直通估计器（STE），以处理训练期间的非可微量化操作（[Y.
    Bengio、Léonard 和 Courville 2013b](ch058.xhtml#ref-Bengio2013)），QAT 集成有助于减轻精度损失（[J.
    Choi 等人 2018](ch058.xhtml#ref-Choi2019)）。
- en: Challenges and Limitations
  id: totrans-590
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 挑战和局限性
- en: Despite enabling ultra-low-power machine learning for embedded systems and mobile
    devices, binarization and ternarization face significant challenges. Performance
    maintenance proves difficult with such drastic quantization, requiring specialized
    hardware capable of efficiently handling binary or ternary operations ([Umuroglu
    et al. 2017](ch058.xhtml#ref-Umuroglu2017)). Traditional processors lack optimization
    for these computations, necessitating custom hardware accelerators.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管为嵌入式系统和移动设备实现了超低功耗的机器学习，但二值化和三值化面临着重大挑战。在这种极端量化下，性能维护变得困难，需要能够高效处理二进制或三值操作的专用硬件（[Umuroglu
    等人 2017](ch058.xhtml#ref-Umuroglu2017)）。传统处理器缺乏对这些计算的优化，需要定制硬件加速器。
- en: Accuracy loss remains a critical concern. These methods suit tasks where high
    precision is not critical or where QAT can compensate for precision constraints.
    Despite challenges, the ability to drastically reduce model size while maintaining
    acceptable accuracy makes them attractive for edge AI and resource-constrained
    environments ([Jacob et al. 2018c](ch058.xhtml#ref-Jacob2018)). Future advances
    in specialized hardware and training techniques will likely enhance their role
    in efficient, scalable AI.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 精度损失仍然是一个关键问题。这些方法适用于对高精度不是关键的任务，或者 QAT 可以补偿精度约束的任务。尽管存在挑战，但能够在保持可接受精度的同时大幅减少模型大小，这使得它们对边缘
    AI 和资源受限环境具有吸引力（[Jacob 等人 2018c](ch058.xhtml#ref-Jacob2018)）。未来在专用硬件和训练技术方面的进步可能会增强它们在高效、可扩展
    AI 中的作用。
- en: Multi-Technique Optimization Strategies
  id: totrans-593
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多技术优化策略
- en: Having explored quantization techniques (PTQ, QAT, binarization, and ternarization),
    pruning methods, and knowledge distillation, we now examine how these complementary
    approaches can be systematically combined to achieve superior optimization results.
    Rather than applying techniques in isolation, integrated strategies leverage the
    synergies between different optimization dimensions to maximize efficiency gains
    while preserving model accuracy.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了量化技术（PTQ、QAT、二值化和三值化）、剪枝方法和知识蒸馏之后，我们现在研究如何系统地结合这些互补方法以实现卓越的优化结果。而不是单独应用技术，集成策略利用不同优化维度之间的协同效应，在保持模型精度的同时最大限度地提高效率。
- en: 'Each optimization technique addresses distinct aspects of model efficiency:
    quantization reduces numerical precision, pruning eliminates redundant parameters,
    knowledge distillation transfers capabilities to compact architectures, and NAS
    optimizes structural design. These techniques exhibit complementary characteristics
    that enable powerful combinations.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 每种优化技术都针对模型效率的不同方面：量化减少了数值精度，剪枝消除了冗余参数，知识蒸馏将能力转移到紧凑架构，而 NAS 优化结构设计。这些技术表现出互补特性，使得强大的组合成为可能。
- en: Pruning and quantization create synergistic effects because pruning reduces
    parameter count while quantization reduces precision, creating multiplicative
    compression effects. Applying pruning first reduces the parameter set, making
    subsequent quantization more effective and reducing the search space for optimal
    quantization strategies. This sequential approach can achieve compression ratios
    exceeding either technique alone.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝和量化产生了协同效应，因为剪枝减少了参数数量，而量化减少了精度，从而产生了乘法压缩效果。首先应用剪枝可以减少参数集，使后续的量化更加有效，并减少最佳量化策略的搜索空间。这种顺序方法可以实现超过任何一种技术单独使用的压缩比率。
- en: Knowledge distillation integrates effectively with quantization by mitigating
    accuracy loss from aggressive quantization. This approach trains student models
    to match teacher behavior rather than just minimizing task loss, proving particularly
    effective for extreme quantization scenarios where direct quantization would cause
    unacceptable accuracy degradation.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏通过减轻激进量化带来的精度损失，有效地与量化结合。这种方法训练学生模型以匹配教师的行为，而不仅仅是最小化任务损失，这在直接量化会导致不可接受的精度退化的极端量化场景中特别有效。
- en: Neural architecture search enables co-design approaches that optimize model
    structures specifically for quantization constraints, identifying architectures
    that maintain accuracy under low-precision operations. This co-design approach
    produces models inherently suited for subsequent optimization, improving the effectiveness
    of both quantization and pruning techniques.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构搜索使协同设计方法成为可能，该方法针对量化约束优化模型结构，识别出在低精度操作下保持精度的架构。这种协同设计方法产生的模型天生适合后续优化，提高了量化和剪枝技术的有效性。
- en: As shown in [Figure 10.25](ch016.xhtml#fig-compression-methods), different compression
    strategies such as pruning, quantization, and singular value decomposition (SVD)
    exhibit varying trade-offs between model size and accuracy loss. While pruning
    combined with quantization (red circles) achieves high compression ratios with
    minimal accuracy loss, quantization alone (yellow squares) also provides a reasonable
    balance. In contrast, SVD (green diamonds) requires a larger model size to maintain
    accuracy, illustrating how different techniques can impact compression effectiveness.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 10.25](ch016.xhtml#fig-compression-methods) 所示，不同的压缩策略，如剪枝、量化和奇异值分解（SVD），在模型大小和精度损失之间表现出不同的权衡。虽然剪枝与量化（红色圆圈）结合实现了高压缩比和最小精度损失，但单独量化（黄色方块）也提供了合理的平衡。相比之下，SVD（绿色菱形）需要更大的模型大小来保持精度，说明了不同技术如何影响压缩的有效性。
- en: '![](../media/file171.svg)'
  id: totrans-600
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file171.svg)'
- en: 'Figure 10.25: **Compression Trade-Offs**: Combining pruning and quantization
    achieves superior compression ratios with minimal accuracy loss compared to quantization
    or singular value decomposition (SVD) alone, demonstrating the impact of different
    numerical precision optimization techniques on model size and performance. Architectural
    and numerical optimizations can complement each other to efficiently deploy machine
    learning models via this figure. Source: ([Han, Mao, and Dally 2015a](ch058.xhtml#ref-han2015deep)).'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.25：**压缩权衡**：结合剪枝和量化在保持最小精度损失的情况下实现了优于单独量化或奇异值分解（SVD）的压缩比率，展示了不同数值精度优化技术对模型大小和性能的影响。如图所示，架构和数值优化可以相互补充，以有效地通过此图部署机器学习模型。来源：([Han,
    Mao, and Dally 2015a](ch058.xhtml#ref-han2015deep))。
- en: Quantization differs from pruning, knowledge distillation, and NAS in that it
    specifically focuses on reducing the numerical precision of weights and activations.
    While quantization alone can provide significant computational benefits, its effectiveness
    can be amplified when combined with the complementary techniques of pruning, distillation,
    and NAS. These methods, each targeting a different aspect of model efficiency,
    work together to create more compact, faster, and energy-efficient models, enabling
    better performance in constrained environments.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 量化与剪枝、知识蒸馏和 NAS 不同，它专门关注减少权重和激活的数值精度。虽然量化本身可以提供显著的计算优势，但其有效性可以通过与剪枝、蒸馏和 NAS
    等互补技术相结合而得到增强。这些方法针对模型效率的不同方面，共同工作以创建更紧凑、更快、更节能的模型，在受限环境中实现更好的性能。
- en: 'Our optimization journey continues. We pruned BERT-Base from 440MB to 110MB
    through structured pruning and knowledge distillation, then quantized it to INT8,
    reducing the model to 28MB with inference latency dropping from 120ms to 45ms
    on mobile hardware. These optimizations transformed an unusable model into one
    approaching deployment viability. Yet profiling reveals a puzzling inefficiency:
    theoretical FLOP count suggests inference should complete in 25ms, yet actual
    execution takes 45ms. Where does the remaining 20ms disappear?'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最优化之旅仍在继续。我们通过结构化剪枝和知识蒸馏将 BERT-Base 从 440MB 剪枝到 110MB，然后将其量化为 INT8，将模型大小减少到
    28MB，在移动硬件上的推理延迟从 120ms 降低到 45ms。这些优化将一个不可用的模型转变为接近部署可行性的模型。然而，分析揭示了一个令人困惑的低效问题：理论上的
    FLOP 计数表明推理应在 25ms 内完成，但实际执行却需要 45ms。剩下的 20ms 去哪里了？
- en: Detailed profiling exposes the answer. While quantization reduced precision,
    the model still computes zeros unnecessarily. Structured pruning removed entire
    attention heads, but the remaining sparse weight matrices are stored in dense
    format, wasting both memory bandwidth and computation on zero-valued elements.
    Layer normalization operations run sequentially despite their inherent parallelism.
    The model processes all tokens identically, even though simple inputs could exit
    early from shallow layers. The GPU spends 40% of execution time idle, waiting
    for memory transfers rather than executing operations.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 详细分析揭示了答案。虽然量化降低了精度，但模型仍然不必要地计算零值。结构化剪枝移除了整个注意力头，但剩余的稀疏权重矩阵以密集格式存储，浪费了内存带宽和零值元素的计算。层归一化操作尽管具有内在的并行性，却按顺序运行。模型对所有标记进行相同的处理，即使简单输入可以从浅层退出。GPU有40%的执行时间处于空闲状态，等待内存传输而不是执行操作。
- en: These observations reveal why model representation and numerical precision optimizations,
    while necessary, are insufficient. Representation techniques determine what computations
    are performed. Precision techniques determine how individual operations execute.
    But neither addresses how computations are organized and scheduled to maximize
    hardware utilization. This is the domain of architectural efficiency optimization,
    the third dimension of our framework.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观察揭示了为什么模型表示和数值精度优化，虽然必要，但不足以解决问题。表示技术确定要执行的计算。精度技术确定单个操作的执行方式。但它们都没有解决如何组织计算和调度以最大化硬件利用率的问题。这是建筑效率优化的领域，也是我们框架的第三个维度。
- en: Architectural efficiency techniques transform the execution pattern itself.
    Exploiting sparsity through specialized kernels eliminates computation on pruned
    weights. Operator fusion combines sequential operations (layer norm, attention,
    feedforward) into single GPU kernels, reducing memory traffic by 40%. Dynamic
    computation enables simple inputs to exit after 6 layers rather than processing
    all 12 layers. Hardware-aware scheduling parallelizes operations to maintain high
    GPU utilization. Applying these techniques to our optimized BERT model reduces
    inference from 45ms to 22ms, finally achieving the 25ms theoretical target and
    making deployment truly viable.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 建筑效率技术本身改变了执行模式。通过专用内核利用稀疏性消除了对剪枝权重的计算。算子融合将顺序操作（层归一化、注意力、前馈）合并为单个GPU内核，通过减少内存流量40%来提高效率。动态计算使得简单输入在6层后即可退出，而不是处理所有12层。硬件感知调度并行化操作以保持高GPU利用率。将这些技术应用于我们的优化BERT模型，将推理时间从45ms减少到22ms，最终实现了25ms的理论目标，使部署真正可行。
- en: This progression illustrates why all three optimization dimensions must work
    in concert. Model representation provides structural efficiency (fewer parameters).
    Numerical precision provides computational efficiency (lower precision arithmetic).
    Architectural efficiency provides execution efficiency (optimized scheduling and
    hardware utilization). The compound effect, 440MB/120ms → 28MB/22ms (16x memory
    reduction, 5.5x latency improvement), emerges only when all dimensions are addressed
    systematically.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 这个进展说明了为什么所有三个优化维度必须协同工作。模型表示提供结构效率（更少的参数）。数值精度提供计算效率（更低的精度算术）。建筑效率提供执行效率（优化的调度和硬件利用率）。只有当所有维度都系统地解决时，才能出现复合效应，即440MB/120ms
    → 28MB/22ms（内存减少16倍，延迟提高5.5倍）。
- en: Architectural Efficiency Techniques
  id: totrans-608
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建筑效率技术
- en: Architectural efficiency optimization ensures that computations execute efficiently
    on target hardware by aligning model operations with processor capabilities and
    memory hierarchies. Unlike representation optimization (which determines what
    computations to perform) and precision optimization (which determines numerical
    fidelity), architectural efficiency addresses how operations are scheduled, how
    memory is accessed, and how workloads adapt to input characteristics and hardware
    constraints.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 建筑效率优化确保通过将模型操作与处理器能力和内存层次结构对齐，在目标硬件上高效执行计算。与表示优化（确定要执行的计算）和精度优化（确定数值精度）不同，建筑效率解决的是操作如何调度、内存如何访问以及工作负载如何适应输入特性和硬件约束的问题。
- en: This optimization dimension proves particularly important for resource-constrained
    scenarios ([Chapter 14](ch020.xhtml#sec-ondevice-learning)), where theoretical
    FLOP reductions from pruning and quantization may not translate to actual speedups
    without architectural modifications. Sparse weight matrices stored in dense format
    waste memory bandwidth. Sequential operations that could execute in parallel underutilize
    GPU cores. Fixed computation graphs process simple and complex inputs identically,
    wasting resources on unnecessary work.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优化维度在资源受限的场景（[第14章](ch020.xhtml#sec-ondevice-learning)）中尤其重要，在这些场景中，从剪枝和量化中得到的理论FLOP减少可能不会在没有架构修改的情况下转化为实际加速。以密集格式存储的稀疏权重矩阵浪费了内存带宽。本可以并行执行的顺序操作未能充分利用GPU核心。固定的计算图以相同的方式处理简单和复杂输入，浪费了在非必要工作上的资源。
- en: 'This section examines four complementary approaches to architectural efficiency:
    hardware-aware design principles that proactively integrate deployment constraints
    during model development, sparsity exploitation techniques that accelerate computation
    on pruned models, dynamic computation strategies that adapt workload to input
    complexity, and operator fusion methods that reduce memory traffic by combining
    operations. These techniques transform algorithmic optimizations into realized
    performance gains.'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了四种互补的架构效率方法：在模型开发期间积极整合部署约束的硬件感知设计原则、加速剪枝模型计算的稀疏性利用技术、适应输入复杂性的动态计算策略以及通过组合操作减少内存流量的算子融合方法。这些技术将算法优化转化为实际性能提升。
- en: Hardware-Aware Design
  id: totrans-612
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件感知设计
- en: Hardware-aware design incorporates target platform constraints—memory bandwidth,
    processing power, parallelism capabilities, and energy budgets—directly into model
    architecture decisions. Rather than optimizing models after training, this approach
    ensures computational patterns, memory access, and operation types match hardware
    capabilities from the outset, maximizing efficiency across diverse deployment
    platforms.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件感知设计将目标平台约束（内存带宽、处理能力、并行性能力和能量预算）直接纳入模型架构决策中。这种方法不是在训练后优化模型，而是确保计算模式、内存访问和操作类型从一开始就与硬件能力相匹配，从而在多种部署平台上最大化效率。
- en: Efficient Design Principles
  id: totrans-614
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高效设计原则
- en: Designing machine learning models for hardware efficiency requires structuring
    architectures to account for computational cost, memory usage, inference latency,
    and power consumption, all while maintaining strong predictive performance. Unlike
    post-training optimizations, which attempt to recover efficiency after training,
    hardware-aware model design proactively integrates hardware considerations from
    the outset. This ensures that models are computationally efficient and deployable
    across diverse hardware environments with minimal adaptation.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 为硬件效率设计机器学习模型需要构建架构以考虑计算成本、内存使用、推理延迟和功耗，同时保持强大的预测性能。与训练后优化不同，硬件感知模型设计从一开始就积极整合硬件考虑因素。这确保了模型在计算效率上高效，并且能够在各种硬件环境中以最小的适应性进行部署。
- en: Central to this proactive approach, a key aspect of hardware-aware design is
    using the strengths of specific hardware platforms (e.g., GPUs, TPUs, mobile or
    edge devices) to maximize parallelism, optimize memory hierarchies, and minimize
    latency through hardware-optimized operations. As summarized in [Table 10.10](ch016.xhtml#tbl-hardware-efficient-design),
    hardware-aware model design can be categorized into several principles, each addressing
    a core aspect of computational and system constraints.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种积极的方法中，硬件感知设计的一个关键方面是利用特定硬件平台（例如，GPU、TPU、移动或边缘设备）的优势，通过硬件优化的操作来最大化并行性、优化内存层次结构并最小化延迟。如[表10.10](ch016.xhtml#tbl-hardware-efficient-design)中总结的，硬件感知模型设计可以划分为几个原则，每个原则都针对计算和系统约束的核心方面。
- en: 'Table 10.10: **Hardware-Aware Design Principles**: Categorizing model design
    choices by their impact on computational cost, memory usage, and inference latency
    enables structured optimization for diverse hardware platforms and deployment
    scenarios. The table outlines key principles—such as minimizing data movement
    and exploiting parallelism—along with representative network architectures that
    embody these concepts.'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.10：**硬件感知设计原则**：通过按其对计算成本、内存使用和推理延迟的影响对模型设计选择进行分类，可以实现对不同硬件平台和部署场景的结构化优化。该表概述了关键原则——例如最小化数据移动和利用并行性——以及体现这些概念的代表性网络架构。
- en: '| **Principle** | **Goal** | **Example Networks** |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| **原则** | **目标** | **示例网络** |'
- en: '| --- | --- | --- |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Scaling Optimization** | Adjust model depth, width, and resolution to balance
    efficiency and hardware constraints. | EfficientNet, RegNet |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| **缩放优化** | 调整模型深度、宽度和分辨率以平衡效率和硬件约束。 | EfficientNet、RegNet |'
- en: '| **Computation Reduction** | Minimize redundant operations to reduce computational
    cost, utilizing hardware-specific optimizations (e.g., using depthwise separable
    convolutions on mobile chips). | MobileNet, ResNeXt |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| **计算减少** | 通过利用硬件特定的优化（例如，在移动芯片上使用深度可分离卷积）来最小化冗余操作，以降低计算成本。 | MobileNet、ResNeXt
    |'
- en: '| **Memory Optimization** | Ensure efficient memory usage by reducing activation
    and parameter storage requirements, using hardware-specific memory hierarchies
    (e.g., local and global memory in GPUs). | DenseNet, SqueezeNet |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| **内存优化** | 通过减少激活和参数存储需求，使用硬件特定的内存层次结构（例如，GPU 中的局部和全局内存）来确保高效的内存使用。 | DenseNet、SqueezeNet
    |'
- en: '| **Hardware-Aware Design** | Optimize architectures for specific hardware
    constraints (e.g., low power, parallelism, high throughput). | TPU-optimized models,
    MobileNet |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| **硬件感知设计** | 优化架构以适应特定的硬件约束（例如，低功耗、并行性、高吞吐量）。 | TPU 优化模型、MobileNet |'
- en: 'The principles in [Table 10.10](ch016.xhtml#tbl-hardware-efficient-design)
    work synergistically: scaling optimization sizes models appropriately for available
    resources, computation reduction eliminates redundant operations through techniques
    like depthwise separable convolutions[34](#fn34), memory optimization aligns access
    patterns with hardware hierarchies, and hardware-aware design ensures architectural
    decisions match platform capabilities. Together, these principles enable models
    that balance accuracy with efficiency while maintaining consistent behavior across
    deployment environments.'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10.10](ch016.xhtml#tbl-hardware-efficient-design) 中的原则协同工作：缩放优化根据可用资源适当地调整模型大小，计算减少通过深度可分离卷积等技术消除冗余操作，内存优化将访问模式与硬件层次结构对齐，而硬件感知设计确保架构决策与平台能力相匹配。这些原则共同作用，使模型在保持部署环境一致性行为的同时，在准确性和效率之间取得平衡。'
- en: Scaling Optimization
  id: totrans-625
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**缩放优化**'
- en: Scaling a model’s architecture involves balancing accuracy with computational
    cost, and optimizing it to align with the capabilities of the target hardware.
    Each component of a model, whether its depth, width, or input resolution, impacts
    resource consumption. In hardware-aware design, these dimensions should not only
    be optimized for accuracy but also for efficiency in memory usage, processing
    power, and energy consumption, especially when the model is deployed on specific
    hardware like GPUs, TPUs, or edge devices.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放模型架构涉及平衡准确性与计算成本，并优化以与目标硬件的能力相匹配。模型的每个组件，无论是深度、宽度还是输入分辨率，都会影响资源消耗。在硬件感知设计中，这些维度不仅应该优化以获得准确性，还应该优化内存使用、处理能力和能耗的效率，尤其是在模型部署在特定的硬件（如
    GPU、TPU 或边缘设备）上时。
- en: From a hardware-aware perspective, it is important to consider how different
    hardware platforms, such as GPUs, TPUs, or edge devices, interact with scaling
    dimensions. For instance, deeper models can capture more complex representations,
    but excessive depth can lead to increased inference latency, longer training times,
    and higher memory consumption, issues that are particularly problematic on resource-constrained
    platforms. Similarly, increasing the width of the model to process more parallel
    information may be beneficial for GPUs and TPUs with high parallelism, but it
    requires careful management of memory usage. In contrast, increasing the input
    resolution can provide finer details for tasks like image classification, but
    it exponentially increases computational costs, potentially overloading hardware
    memory or causing power inefficiencies on edge devices.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 从硬件感知的角度来看，考虑不同的硬件平台（如 GPU、TPU 或边缘设备）如何与缩放维度交互是很重要的。例如，更深的模型可以捕捉更复杂的表示，但过深的深度可能导致推理延迟增加、训练时间延长和内存消耗增加，这些问题在资源受限的平台上是尤其成问题的。同样，增加模型的宽度以处理更多的并行信息可能对具有高并行的
    GPU 和 TPU 有益，但它需要仔细管理内存使用。相比之下，增加输入分辨率可以为图像分类等任务提供更精细的细节，但它会指数级增加计算成本，可能超载硬件内存或在边缘设备上造成能源效率低下。
- en: 'Mathematically, the total FLOPs for a convolutional model can be approximated
    as: <semantics><mrow><mtext mathvariant="normal">FLOPs</mtext><mo>∝</mo><mi>d</mi><mo>⋅</mo><msup><mi>w</mi><mn>2</mn></msup><mo>⋅</mo><msup><mi>r</mi><mn>2</mn></msup><mo>,</mo></mrow>
    <annotation encoding="application/x-tex">\text{FLOPs} \propto d \cdot w^2 \cdot
    r^2,</annotation></semantics> where <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>
    is depth, <semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics>
    is width, and <semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics>
    is the input resolution. Increasing all three dimensions without considering the
    hardware limitations can result in suboptimal performance, especially on devices
    with limited computational power or memory bandwidth.'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，卷积模型的总体浮点运算次数（FLOPs）可以近似为：<semantics><mrow><mtext mathvariant="normal">FLOPs</mtext><mo>∝</mo><mi>d</mi><mo>⋅</mo><msup><mi>w</mi><mn>2</mn></msup><mo>⋅</mo><msup><mi>r</mi><mn>2</mn></msup><mo>,</mo></mrow>
    <annotation encoding="application/x-tex">\text{FLOPs} \propto d \cdot w^2 \cdot
    r^2,</annotation></semantics> 其中 <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>
    是深度，<semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics>
    是宽度，而 <semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics>
    是输入分辨率。不考虑硬件限制而增加所有三个维度可能会导致性能不佳，尤其是在计算能力或内存带宽有限的设备上。
- en: 'For efficient model scaling, managing these parameters in a balanced way becomes
    essential, ensuring that the model remains within the limits of the hardware while
    maximizing performance. This is where compound scaling comes into play. Instead
    of adjusting depth, width, and resolution independently, compound scaling balances
    all three dimensions together by applying fixed ratios <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>α</mi><mo>,</mo><mi>β</mi><mo>,</mo><mi>γ</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\alpha,
    \beta, \gamma)</annotation></semantics> relative to a base model: <semantics><mrow><mi>d</mi><mo>=</mo><msup><mi>α</mi><mi>ϕ</mi></msup><msub><mi>d</mi><mn>0</mn></msub><mo>,</mo><mi>w</mi><mo>=</mo><msup><mi>β</mi><mi>ϕ</mi></msup><msub><mi>w</mi><mn>0</mn></msub><mo>,</mo><mi>r</mi><mo>=</mo><msup><mi>γ</mi><mi>ϕ</mi></msup><msub><mi>r</mi><mn>0</mn></msub></mrow>
    <annotation encoding="application/x-tex">d = \alpha^\phi d_0, \quad w = \beta^\phi
    w_0, \quad r = \gamma^\phi r_0</annotation></semantics> Here, <semantics><mi>ϕ</mi><annotation
    encoding="application/x-tex">\phi</annotation></semantics> is a scaling coefficient,
    and <semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>,
    <semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>,
    and <semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics>
    are scaling factors determined based on hardware constraints and empirical data.
    This approach ensures that models grow in a way that optimizes hardware resource
    usage, keeping them efficient while improving accuracy.'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效地进行模型扩展，以平衡的方式管理这些参数变得至关重要，确保模型在硬件限制范围内运行，同时最大化性能。这正是复合扩展发挥作用的地方。复合扩展不是独立调整深度、宽度和分辨率，而是通过应用相对于基础模型的固定比率
    <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo>,</mo><mi>β</mi><mo>,</mo><mi>γ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\alpha,
    \beta, \gamma)</annotation></semantics> 来平衡这三个维度：<semantics><mrow><mi>d</mi><mo>=</mo><msup><mi>α</mi><mi>ϕ</mi></msup><msub><mi>d</mi><mn>0</mn></msub><mo>,</mo><mi>w</mi><mo>=</mo><msup><mi>β</mi><mi>ϕ</mi></msup><msub><mi>w</mi><mn>0</mn></msub><mo>,</mo><mi>r</mi><mo>=</mo><msup><mi>γ</mi><mi>ϕ</mi></msup><msub><mi>r</mi><mn>0</mn></msub></mrow>
    <annotation encoding="application/x-tex">d = \alpha^\phi d_0, \quad w = \beta^\phi
    w_0, \quad r = \gamma^\phi r_0</annotation></semantics> 在这里，<semantics><mi>ϕ</mi><annotation
    encoding="application/x-tex">\phi</annotation></semantics> 是一个缩放系数，而 <semantics><mi>α</mi><annotation
    encoding="application/x-tex">\alpha</annotation></semantics>、<semantics><mi>β</mi><annotation
    encoding="application/x-tex">\beta</annotation></semantics> 和 <semantics><mi>γ</mi><annotation
    encoding="application/x-tex">\gamma</annotation></semantics> 是基于硬件限制和经验数据确定的缩放因子。这种方法确保模型以优化硬件资源使用的方式增长，在提高准确性的同时保持效率。
- en: For example, EfficientNet, which employs compound scaling, demonstrates how
    carefully balancing depth, width, and resolution results in models that are both
    computationally efficient and high-performing. Compound scaling reduces computational
    cost while preserving accuracy, making it a key consideration for hardware-aware
    model design. This approach is particularly beneficial when deploying models on
    GPUs or TPUs, where parallelism can be fully leveraged, but memory and power usage
    need to be carefully managed, connecting to the performance evaluation methods
    in [Chapter 12](ch018.xhtml#sec-benchmarking-ai).
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，采用复合缩放的EfficientNet展示了如何精心平衡深度、宽度和分辨率，从而实现计算高效且性能高的模型。复合缩放降低了计算成本同时保持精度，使其成为硬件感知模型设计的关键考虑因素。这种方法在将模型部署到GPU或TPU时尤其有益，在这些设备上可以充分利用并行性，但需要仔细管理内存和功耗，这与第12章中提到的性能评估方法相联系。
- en: This principle extends beyond convolutional models to other architectures like
    transformers. Adjusting the number of layers, attention heads, or embedding dimensions
    impacts computational efficiency similarly. Hardware-aware scaling has become
    central to optimizing model performance across various computational constraints,
    particularly when working with large models or resource-constrained devices.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 这一原则不仅适用于卷积模型，还适用于其他架构，如Transformer。调整层数、注意力头或嵌入维度以类似的方式影响计算效率。硬件感知缩放已成为优化跨各种计算约束的模型性能的关键，尤其是在处理大型模型或资源受限的设备时。
- en: Computation Reduction
  id: totrans-632
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算减少
- en: Modern architectures leverage factorized computations to decompose complex operations
    into simpler components, reducing computational overhead while maintaining representational
    power. Standard convolutions apply filters uniformly across all spatial locations
    and channels, creating computational bottlenecks on resource-constrained hardware.
    Factorization techniques address this inefficiency by restructuring operations
    to minimize redundant computation.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 现代架构利用分解计算将复杂操作分解成更简单的组件，在保持表示能力的同时减少计算开销。标准卷积在所有空间位置和通道上均匀应用滤波器，在资源受限的硬件上造成计算瓶颈。分解技术通过重新结构操作以最小化冗余计算来解决这个问题。
- en: 'Depthwise separable convolutions, introduced in MobileNet, exemplify this approach
    by decomposing standard convolutions into two stages: depthwise convolution (applying
    separate filters to each input channel independently) and pointwise convolution
    (1×1 convolution mixing outputs across channels). The computational complexity
    of standard convolution with input size <semantics><mrow><mi>h</mi><mo>×</mo><mi>w</mi></mrow><annotation
    encoding="application/x-tex">h \times w</annotation></semantics>, <semantics><msub><mi>C</mi><mtext
    mathvariant="normal">in</mtext></msub><annotation encoding="application/x-tex">C_{\text{in}}</annotation></semantics>
    input channels, and <semantics><msub><mi>C</mi><mtext mathvariant="normal">out</mtext></msub><annotation
    encoding="application/x-tex">C_{\text{out}}</annotation></semantics> output channels
    is: <semantics><mrow><mi>𝒪</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>w</mi><msub><mi>C</mi><mtext
    mathvariant="normal">in</mtext></msub><msub><mi>C</mi><mtext mathvariant="normal">out</mtext></msub><msup><mi>k</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{O}(h
    w C_{\text{in}} C_{\text{out}} k^2)</annotation></semantics> where <semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics> is kernel size. Depthwise
    separable convolutions reduce this to: <semantics><mrow><mi>𝒪</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>h</mi><mi>w</mi><msub><mi>C</mi><mtext mathvariant="normal">in</mtext></msub><msup><mi>k</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>𝒪</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>h</mi><mi>w</mi><msub><mi>C</mi><mtext mathvariant="normal">in</mtext></msub><msub><mi>C</mi><mtext
    mathvariant="normal">out</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(h w C_{\text{in}} k^2) +
    \mathcal{O}(h w C_{\text{in}} C_{\text{out}})</annotation></semantics> eliminating
    the <semantics><msup><mi>k</mi><mn>2</mn></msup><annotation encoding="application/x-tex">k^2</annotation></semantics>
    factor from channel-mixing operations, achieving 5×-10× FLOP reduction. This directly
    translates to reduced memory bandwidth requirements and improved inference latency
    on mobile and edge devices.'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积，在MobileNet中引入，通过将标准卷积分解为两个阶段来体现这种方法：深度卷积（独立地对每个输入通道应用单独的滤波器）和点卷积（1×1卷积混合通道间的输出）。标准卷积的计算复杂度，对于输入大小为<semantics><mrow><mi>h</mi><mo>×</mo><mi>w</mi></mrow><annotation
    encoding="application/x-tex">h \times w</annotation></semantics>，<semantics><msub><mi>C</mi><mtext
    mathvariant="normal">in</mtext></msub><annotation encoding="application/x-tex">C_{\text{in}}</annotation></semantics>输入通道，和<semantics><msub><mi>C</mi><mtext
    mathvariant="normal">out</mtext></msub><annotation encoding="application/x-tex">C_{\text{out}}</annotation></semantics>输出通道是：<semantics><mrow><mi>𝒪</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>h</mi><mi>w</mi><msub><mi>C</mi><mtext
    mathvariant="normal">in</mtext></msub><msub><mi>C</mi><mtext mathvariant="normal">out</mtext></msub><msup><mi>k</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{O}(h
    w C_{\text{in}} C_{\text{out}} k^2)</annotation></semantics> 其中 <semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics> 是内核大小。深度可分离卷积将此降低到：<semantics><mrow><mi>𝒪</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>h</mi><mi>w</mi><msub><mi>C</mi><mtext
    mathvariant="normal">in</mtext></msub><msup><mi>k</mi><mn>2</mn></msup><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>+</mo><mi>𝒪</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>w</mi><msub><mi>C</mi><mtext
    mathvariant="normal">in</mtext></msub><msub><mi>C</mi><mtext mathvariant="normal">out</mtext></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{O}(h
    w C_{\text{in}} k^2) + \mathcal{O}(h w C_{\text{in}} C_{\text{out}})</annotation></semantics>
    消除了通道混合操作中的 <semantics><msup><mi>k</mi><mn>2</mn></msup><annotation encoding="application/x-tex">k^2</annotation></semantics>
    因子，实现了5×-10×的FLOP减少。这直接转化为降低移动和边缘设备上的内存带宽需求和改进推理延迟。
- en: Complementary factorization techniques extend these benefits. Grouped convolutions
    (ResNeXt) partition feature maps into independent groups processed separately
    before merging, maintaining accuracy while reducing redundant operations. Bottleneck
    layers (ResNet) apply 1×1 convolutions to reduce feature dimensionality before
    expensive operations, concentrating computation where it provides maximum value.
    Combined with sparsity and hardware-aware scheduling, these techniques maximize
    accelerator utilization across GPUs, TPUs, and specialized edge processors.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 补充的分解技术扩展了这些优势。分组卷积（ResNeXt）将特征图划分为独立的组，在合并之前分别处理，保持精度同时减少冗余操作。瓶颈层（ResNet）在昂贵操作之前应用1×1卷积以降低特征维度，将计算集中在提供最大价值的地方。结合稀疏性和硬件感知调度，这些技术在GPU、TPU和专用边缘处理器上最大化加速器的利用率。
- en: Memory Optimization
  id: totrans-636
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存优化
- en: Memory optimization[35](#fn35) addresses performance bottlenecks arising when
    memory demands for activations, feature maps, and parameters exceed hardware capacity
    on resource-constrained devices. Modern architectures employ memory-efficient
    strategies to reduce storage requirements while maintaining performance, ensuring
    computational tractability and energy efficiency on GPUs, TPUs, and edge AI platforms.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 内存优化[35](#fn35)解决了在资源受限设备上，当激活、特征图和参数的内存需求超过硬件容量时出现的性能瓶颈。现代架构采用内存高效策略来减少存储需求，同时保持性能，确保在GPU、TPU和边缘AI平台上计算的可处理性和能源效率。
- en: 'One effective technique for memory optimization is feature reuse, a strategy
    employed in DenseNet. In traditional convolutional networks, each layer typically
    computes a new set of feature maps, increasing the model’s memory footprint. However,
    DenseNet reduces the need for redundant activations by reusing feature maps from
    previous layers and selectively applying transformations. This method reduces
    the total number of feature maps that need to be stored, which in turn lowers
    the memory requirements without sacrificing accuracy. In a standard convolutional
    network with <semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>
    layers, if each layer generates <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    new feature maps, the total number of feature maps grows linearly: <semantics><mrow><mi>𝒪</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(L k)</annotation></semantics>'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有效的内存优化技术是特征重用，这是DenseNet采用的一种策略。在传统的卷积网络中，每一层通常计算一组新的特征图，增加了模型的内存占用。然而，DenseNet通过重复使用先前层的特征图并选择性地应用变换来减少冗余激活的需求。这种方法减少了需要存储的总特征图数量，从而在不牺牲准确性的情况下降低了内存需求。在一个具有<semantics><mi>L</mi><annotation
    encoding="application/x-tex">L</annotation></semantics>层的标准卷积网络中，如果每一层生成<semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics>个新的特征图，特征图的总数将线性增长：<semantics><mrow><mi>𝒪</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(L k)</annotation></semantics>
- en: In contrast, DenseNet reuses feature maps from earlier layers, reducing the
    number of feature maps stored. This leads to improved parameter efficiency and
    a reduced memory footprint, which is important for hardware with limited memory
    resources.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，DenseNet重复使用早期层的特征图，减少了存储的特征图数量。这提高了参数效率并减少了内存占用，这对于内存资源有限的硬件来说非常重要。
- en: Another useful technique is activation checkpointing[36](#fn36), which is especially
    beneficial during training. In a typical neural network, backpropagation requires
    storing all forward activations for the backward pass. This can lead to a significant
    memory overhead, especially for large models. Activation checkpointing reduces
    memory consumption by only storing a subset of activations and recomputing the
    remaining ones when needed.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有用的技术是激活检查点[36](#fn36)，这在训练期间特别有益。在典型的神经网络中，反向传播需要存储所有前向激活以进行反向传递。这可能导致显著的内存开销，特别是对于大型模型。激活检查点通过仅存储激活子集并在需要时重新计算剩余的激活来减少内存消耗。
- en: 'If an architecture requires storing <semantics><msub><mi>A</mi><mtext mathvariant="normal">total</mtext></msub><annotation
    encoding="application/x-tex">A_{\text{total}}</annotation></semantics> activations,
    the standard backpropagation method requires the full storage: <semantics><mrow><mi>𝒪</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mtext mathvariant="normal">total</mtext></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{O}(A_{\text{total}})</annotation></semantics>'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个架构需要存储<semantics><msub><mi>A</mi><mtext mathvariant="normal">total</mtext></msub><annotation
    encoding="application/x-tex">A_{\text{total}}</annotation></msub></semantics>激活，标准的反向传播方法需要完整存储：<semantics><mrow><mi>𝒪</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mtext mathvariant="normal">total</mtext></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{O}(A_{\text{total}})</annotation></semantics>
- en: 'With activation checkpointing, however, only a fraction of activations is stored,
    and the remaining ones are recomputed on-the-fly, reducing storage requirements
    to: <semantics><mrow><mi>𝒪</mi><mo minsize="1.8" maxsize="1.8" stretchy="false"
    form="prefix">(</mo><msqrt><msub><mi>A</mi><mtext mathvariant="normal">total</mtext></msub></msqrt><mo
    minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">)</mo></mrow> <annotation
    encoding="application/x-tex">\mathcal{O}\Big(\sqrt{A_{\text{total}}}\Big)</annotation></semantics>'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用激活检查点时，只有一部分激活被存储，其余的激活在运行时重新计算，从而将存储需求降低到：<semantics><mrow><mi>𝒪</mi><mo
    minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">(</mo><msqrt><msub><mi>A</mi><mtext
    mathvariant="normal">total</mtext></msub></msqrt><mo minsize="1.8" maxsize="1.8"
    stretchy="false" form="postfix">)</mo></mrow> <annotation encoding="application/x-tex">\mathcal{O}\Big(\sqrt{A_{\text{total}}}\Big)</annotation></semantics>
- en: Feature reuse can significantly reduce peak memory consumption, making it particularly
    useful for training large models on hardware with limited memory.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重用可以显著减少峰值内存消耗，这对于在内存有限的硬件上训练大型模型特别有用。
- en: 'Parameter reduction is another important technique, particularly for models
    that use large filters. For instance, SqueezeNet uses a novel architecture where
    it applies <semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times
    1</annotation></semantics> convolutions to reduce the number of input channels
    before applying standard convolutions. By first reducing the number of channels
    with <semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times
    1</annotation></semantics> convolutions, SqueezeNet reduces the model size significantly
    without compromising the model’s expressive power. The number of parameters in
    a standard convolutional layer is: <semantics><mrow><mi>𝒪</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>C</mi><mtext mathvariant="normal">in</mtext></msub><msub><mi>C</mi><mtext
    mathvariant="normal">out</mtext></msub><msup><mi>k</mi><mn>2</mn></msup><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{O}(C_{\text{in}}
    C_{\text{out}} k^2)</annotation></semantics>'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 参数减少是另一种重要的技术，尤其是对于使用大滤波器的模型。例如，SqueezeNet使用了一种新颖的架构，它在应用标准卷积之前，先使用<semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">1\times 1</annotation></semantics>卷积来减少输入通道的数量。通过首先使用<semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">1\times 1</annotation></semantics>卷积来减少通道数，SqueezeNet显著减小了模型大小，同时没有牺牲模型的表达能力。标准卷积层中的参数数量为：<semantics><mrow><mi>𝒪</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>C</mi><mtext mathvariant="normal">in</mtext></msub><msub><mi>C</mi><mtext
    mathvariant="normal">out</mtext></msub><msup><mi>k</mi><mn>2</mn></msup><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{O}(C_{\text{in}}
    C_{\text{out}} k^2)</annotation></semantics>
- en: By reducing <semantics><msub><mi>C</mi><mtext mathvariant="normal">in</mtext></msub><annotation
    encoding="application/x-tex">C_{\text{in}}</annotation></semantics> using <semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">1\times 1</annotation></semantics> convolutions,
    SqueezeNet[37](#fn37) reduces the number of parameters, achieving a 50x reduction
    in model size compared to AlexNet while maintaining similar performance. This
    method is particularly valuable for edge devices that have strict memory and storage
    constraints.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用<semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times
    1</annotation></semantics>卷积来减少<semantics><msub><mi>C</mi><mtext mathvariant="normal">in</mtext></msub><annotation
    encoding="application/x-tex">C_{\text{in}}</annotation></semantics>，SqueezeNet[37](#fn37)减少了参数数量，与AlexNet相比，模型大小减少了50倍，同时保持了相似的性能。这种方法对于具有严格内存和存储约束的边缘设备尤其有价值。
- en: Feature reuse, activation checkpointing, and parameter reduction form key components
    of hardware-aware model design, allowing models to fit within memory limits of
    modern accelerators while reducing power consumption through fewer memory accesses.
    Specialized accelerators like TPUs and GPUs leverage memory hierarchies, caching,
    and high bandwidth memory to efficiently handle sparse or reduced-memory representations,
    enabling faster inference with minimal overhead.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重用、激活检查点和参数减少是硬件感知模型设计的关键组成部分，允许模型适应现代加速器的内存限制，并通过减少内存访问来降低功耗。专门的加速器，如TPUs和GPU，利用内存层次结构、缓存和高带宽内存来有效地处理稀疏或减少内存表示，从而实现快速推理并最小化开销。
- en: Adaptive Computation Methods
  id: totrans-647
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自适应计算方法
- en: Dynamic computation enables models to adapt computational load based on input
    complexity, allocating resources more effectively than traditional fixed-architecture
    approaches. While conventional models apply uniform processing to all inputs regardless
    of complexity—wasting resources on simple cases and increasing power consumption—dynamic
    computation allows models to skip layers or operations for simple inputs while
    processing deeper networks for complex cases.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 动态计算使模型能够根据输入复杂度调整计算负载，比传统的固定架构方法更有效地分配资源。而传统模型对所有输入都应用统一的处理，无论其复杂度如何——在简单情况下浪费资源，并增加功耗——动态计算允许模型在简单输入时跳过层或操作，而在复杂情况下处理更深的网络。
- en: This adaptive approach optimizes computational efficiency, reduces energy consumption,
    minimizes latency, and preserves predictive performance. Dynamic adjustment based
    on input complexity proves essential for resource-constrained hardware in mobile
    devices, embedded systems, and autonomous vehicles where computational efficiency
    and real-time processing are critical.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自适应方法优化了计算效率，降低了能耗，最小化了延迟，并保持了预测性能。基于输入复杂度的动态调整对于资源受限的硬件至关重要，尤其是在移动设备、嵌入式系统和自动驾驶汽车中，这些设备对计算效率和实时处理能力要求极高。
- en: Dynamic Schemes
  id: totrans-650
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态方案
- en: Dynamic schemes enable models to selectively reduce computation when inputs
    are simple, preserving resources while maintaining predictive performance. The
    approaches discussed below, beginning with early exit architectures, illustrate
    how to implement this adaptive strategy effectively.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 动态方案使模型能够在输入简单时选择性减少计算，同时保持预测性能，从而节省资源。以下讨论的方法，从早期退出架构开始，说明了如何有效地实施这种自适应策略。
- en: Early Exit Architectures
  id: totrans-652
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 早期退出架构
- en: Early exit architectures allow a model to make predictions at intermediate points
    in the network rather than completing the full forward pass for every input. This
    approach is particularly effective for real-time applications and energy-efficient
    inference, as it enables selective computation based on the complexity of individual
    inputs ([Teerapittayanon, McDanel, and Kung 2017](ch058.xhtml#ref-teerapittayanon2016branchynet)).
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 早期退出架构允许模型在网络中间点进行预测，而不是对每个输入完成完整的正向传播。这种方法对于实时应用和节能推理特别有效，因为它可以根据单个输入的复杂度进行选择性计算（[Teerapittayanon,
    McDanel, 和 Kung 2017](ch058.xhtml#ref-teerapittayanon2016branchynet)）。
- en: The core mechanism in early exit architectures involves multiple exit points
    embedded within the network. Simpler inputs, which can be classified with high
    confidence early in the model, exit at an intermediate layer, reducing unnecessary
    computations. Conversely, more complex inputs continue processing through deeper
    layers to ensure accuracy.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 早期退出架构的核心机制涉及在网络中嵌入多个退出点。对于可以早期以高置信度分类的简单输入，它们在中间层退出，减少了不必要的计算。相反，更复杂的输入继续通过更深层的处理以确保准确性。
- en: A well-known example is BranchyNet[38](#fn38), which introduces multiple exit
    points throughout the network. For each input, the model evaluates intermediate
    predictions using confidence thresholds. If the prediction confidence exceeds
    a predefined threshold at an exit point, the model terminates further computations
    and outputs the result. Otherwise, it continues processing until the final layer
    ([Teerapittayanon, McDanel, and Kung 2017](ch058.xhtml#ref-teerapittayanon2016branchynet)).
    This approach minimizes inference time without compromising performance on challenging
    inputs.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 一个著名的例子是BranchyNet[38](#fn38)，它在整个网络中引入了多个退出点。对于每个输入，模型使用置信度阈值评估中间预测。如果在退出点预测的置信度超过预定义的阈值，模型将终止进一步的计算并输出结果。否则，它将继续处理直到最终层（[Teerapittayanon,
    McDanel, 和 Kung 2017](ch058.xhtml#ref-teerapittayanon2016branchynet)）。这种方法在不影响对挑战性输入性能的情况下最小化了推理时间。
- en: Another example is multi-exit vision transformers, which extend early exits
    to transformer-based architectures. These models use lightweight classifiers at
    various transformer layers, allowing predictions to be generated early when possible
    ([Scardapane, Wang, and Panella 2020](ch058.xhtml#ref-scardapane2020should)).
    This technique significantly reduces inference time while maintaining robust performance
    for complex samples.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是多退出视觉Transformer，它将早期退出扩展到基于Transformer的架构。这些模型在各个Transformer层使用轻量级分类器，使得在可能的情况下可以早期生成预测（[Scardapane,
    Wang, 和 Panella 2020](ch058.xhtml#ref-scardapane2020should)）。这项技术显著减少了推理时间，同时保持了复杂样本的稳健性能。
- en: Early exit models are particularly advantageous for resource-constrained devices,
    such as mobile processors and edge accelerators. By dynamically adjusting computational
    effort, these architectures reduce power consumption and processing latency, making
    them ideal for real-time decision-making ([B. Hu, Zhang, and Fu 2021](ch058.xhtml#ref-hu2021triple)).
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 早期退出模型对于资源受限的设备，如移动处理器和边缘加速器特别有利。通过动态调整计算工作量，这些架构可以降低功耗和处理延迟，使它们非常适合实时决策 ([B.
    Hu, Zhang, and Fu 2021](ch058.xhtml#ref-hu2021triple))。
- en: When deployed on hardware accelerators such as GPUs and TPUs, early exit architectures
    can be further optimized by exploiting parallelism. For instance, different exit
    paths can be evaluated concurrently, thereby improving throughput while preserving
    the benefits of adaptive computation ([Yu, Li, and Wang 2023](ch058.xhtml#ref-yu2023efficient)).
    This approach is illustrated in [Figure 10.26](ch016.xhtml#fig-early-exit-transformers),
    where each transformer layer is followed by a classifier and an optional early
    exit mechanism based on confidence estimation or latency-to-accuracy trade-offs
    (LTE). At each stage, the system may choose to exit early if sufficient confidence
    is achieved, or continue processing through deeper layers, enabling dynamic allocation
    of computational resources.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 当部署在硬件加速器如 GPU 和 TPUs 上时，早期退出架构可以通过利用并行性进一步优化。例如，可以同时评估不同的退出路径，从而提高吞吐量同时保持自适应计算的好处
    ([Yu, Li, and Wang 2023](ch058.xhtml#ref-yu2023efficient))。这种方法在 [图 10.26](ch016.xhtml#fig-early-exit-transformers)
    中得到说明，其中每个 Transformer 层后面跟着一个分类器和一个基于置信度估计或延迟-精度权衡（LTE）的可选早期退出机制。在每一个阶段，系统可以选择在达到足够置信度时提前退出，或者继续通过更深的层进行处理，从而实现计算资源的动态分配。
- en: '![](../media/file172.svg)'
  id: totrans-659
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file172.svg)'
- en: 'Figure 10.26: **Early Exit Architecture**: Transformer layers dynamically adjust
    computation by classifying each layer’s output and enabling early termination
    if sufficient confidence is reached, reducing latency and power consumption for
    resource-constrained devices. This approach allows for parallel evaluation of
    different exit paths, improving throughput on hardware accelerators like gpus
    and tpus. Source: ([Xin et al. 2021](ch058.xhtml#ref-xin-etal-2021-berxit)).'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.26：**早期退出架构**：Transformer 层通过分类每一层的输出并在达到足够置信度时启用早期终止来动态调整计算，从而降低资源受限设备的延迟和功耗。这种方法允许并行评估不同的退出路径，提高硬件加速器如
    gpus 和 tpus 的吞吐量。来源：([Xin 等人 2021](ch058.xhtml#ref-xin-etal-2021-berxit))。
- en: Conditional Computation
  id: totrans-661
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 条件计算
- en: Conditional computation refers to the ability of a neural network to decide
    which parts of the model to activate based on the input, thereby reducing unnecessary
    computation. This approach can be highly beneficial in resource-constrained environments,
    such as mobile devices or real-time systems, where reducing the number of operations
    directly translates to lower computational cost, power consumption, and inference
    latency ([E. Bengio et al. 2015](ch058.xhtml#ref-bengio2015conditional)).
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 条件计算指的是神经网络根据输入决定激活模型哪些部分的能力，从而减少不必要的计算。这种方法在资源受限的环境中非常有用，例如移动设备或实时系统，在这里减少操作数量可以直接转化为降低计算成本、功耗和推理延迟
    ([E. Bengio 等人 2015](ch058.xhtml#ref-bengio2015conditional))。
- en: In contrast to Early Exit Architectures, where the decision to exit early is
    typically made once a threshold confidence level is met, conditional computation
    works by dynamically selecting which layers, units, or paths in the network should
    be computed based on the characteristics of the input. This can be achieved through
    mechanisms such as gating functions or dynamic routing, which “turn off” parts
    of the network that are not needed for a particular input, allowing the model
    to focus computational resources where they are most required.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期退出架构不同，其中通常在达到阈值置信度水平时做出提前退出的决定，条件计算通过根据输入的特征动态选择网络中哪些层、单元或路径应该进行计算来工作。这可以通过门控函数或动态路由等机制实现，这些机制“关闭”对于特定输入不需要的网络部分，使模型能够将计算资源集中在最需要的地方。
- en: One example of conditional computation is SkipNet, which uses a gating mechanism
    to skip layers in a CNN when the input is deemed simple enough. The gating mechanism
    uses a lightweight classifier to predict if the layer should be skipped. This
    prediction is made based on the input, and the model adjusts the number of layers
    used during inference accordingly ([X. Wang et al. 2018](ch058.xhtml#ref-wang2018skipnet)).
    If the gating function determines that the input is simple, certain layers are
    bypassed, resulting in faster inference. However, for more complex inputs, the
    model uses the full depth of the network to achieve the necessary accuracy.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 条件计算的一个例子是SkipNet，它使用门控机制在输入被认为足够简单时跳过CNN中的层。门控机制使用轻量级分类器来预测是否应该跳过该层。这个预测基于输入，模型根据推理过程中使用的层数进行调整（[X.
    Wang 等人 2018](ch058.xhtml#ref-wang2018skipnet))。如果门控函数确定输入简单，某些层将被绕过，从而实现更快的推理。然而，对于更复杂的输入，模型使用网络的全部深度以达到必要的准确性。
- en: Another example is Dynamic Routing Networks, such as in the Capsule Networks
    (CapsNets), where routing mechanisms dynamically choose the path that activations
    take through the network. In these networks, the decision-making process involves
    selecting specific pathways for information flow based on the input’s complexity,
    which can significantly reduce the number of operations and computations required
    ([Sabour, Frosst, and Hinton 2017](ch058.xhtml#ref-sabour2017dynamic)). This mechanism
    introduces adaptability by using different routing strategies, providing computational
    efficiency while preserving the quality of predictions.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是动态路由网络，例如在胶囊网络（CapsNets）中，路由机制会动态选择激活信号在网络中传递的路径。在这些网络中，决策过程涉及根据输入的复杂性选择特定的信息流路径，这可以显著减少所需操作和计算的数量（[Sabour,
    Frosst, 和 Hinton 2017](ch058.xhtml#ref-sabour2017dynamic)）。这种机制通过使用不同的路由策略引入了适应性，同时提供计算效率并保持预测质量。
- en: These conditional computation strategies have significant advantages in real-world
    applications where computational resources are limited. For example, in autonomous
    driving, the system must process a variety of inputs (e.g., pedestrians, traffic
    signs, road lanes) with varying complexity. In cases where the input is straightforward,
    a simpler, less computationally demanding path can be taken, whereas more complex
    scenarios (such as detecting obstacles or performing detailed scene understanding)
    will require full use of the model’s capacity. Conditional computation ensures
    that the system adapts its computation based on the real-time complexity of the
    input, leading to improved speed and efficiency ([W. Huang, Chen, and Zhang 2023](ch058.xhtml#ref-huang2023adaptive)).
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 这些条件计算策略在计算资源有限的现实世界应用中具有显著优势。例如，在自动驾驶中，系统必须处理各种不同复杂性的输入（例如行人、交通标志、道路车道）。在输入简单的情况下，可以采取更简单、计算需求更低的路径，而在更复杂的情况（如检测障碍物或执行详细场景理解）下，则需要充分利用模型的能力。条件计算确保系统根据输入的实时复杂性调整其计算，从而提高速度和效率（[W.
    Huang, Chen 和 Zhang 2023](ch058.xhtml#ref-huang2023adaptive))。
- en: Gate-Based Computation
  id: totrans-667
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于门控的计算
- en: Gate-based conditional computation introduces learned gating mechanisms that
    dynamically control which parts of a neural network are activated based on input
    complexity. Unlike static architectures that process all inputs with the same
    computational effort, this approach enables dynamic activation of sub-networks
    or layers by learning decision boundaries during training ([Shazeer, Mirhoseini,
    Maziarz, and others 2017](ch058.xhtml#ref-shazeer2017outrageously)).
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 基于门控的条件计算引入了学习到的门控机制，这些机制根据输入复杂性动态控制神经网络哪些部分被激活。与处理所有输入都使用相同计算努力的静态架构不同，这种方法通过在训练期间学习决策边界，使子网络或层的动态激活成为可能（[Shazeer,
    Mirhoseini, Maziarz 和其他人 2017](ch058.xhtml#ref-shazeer2017outrageously))。
- en: Gating mechanisms are typically implemented using binary or continuous gating
    functions, wherein a lightweight control module (often called a router or gating
    network) predicts whether a particular layer or path should be executed. This
    decision-making occurs dynamically at inference time, allowing the model to allocate
    computational resources adaptively.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 门控机制通常使用二进制或连续的门控函数来实现，其中轻量级控制模块（通常称为路由器或门控网络）预测特定层或路径是否应该执行。这种决策在推理时动态发生，允许模型自适应地分配计算资源。
- en: A well-known example of this paradigm is the Dynamic Filter Network (DFN), which
    applies input-dependent filtering by selecting different convolutional kernels
    at runtime. DFN reduces unnecessary computation by avoiding uniform filter application
    across inputs, tailoring its computations based on input complexity ([Xu Jia et
    al. 2016](ch058.xhtml#ref-jia2016dynamic)).
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 这种范例的一个著名例子是动态滤波网络（DFN），它在运行时通过选择不同的卷积核来应用输入相关的滤波。DFN通过避免在输入上应用均匀滤波器来减少不必要的计算，根据输入复杂性定制其计算
    ([Xu Jia 等人 2016](ch058.xhtml#ref-jia2016dynamic))。
- en: Another widely adopted strategy is the Mixture of Experts (MoE) framework. In
    this architecture, a gating network selects a subset of specialized expert subnetworks
    to process each input ([Shazeer, Mirhoseini, Maziarz, and others 2017](ch058.xhtml#ref-shazeer2017outrageously)).
    This allows only a small portion of the total model to be active for any given
    input, significantly improving computational efficiency without sacrificing model
    capacity. A notable instantiation of this idea is Google’s Switch Transformer,
    which extends the transformer architecture with expert-based conditional computation
    ([Fedus, Zoph, and Shazeer 2021a](ch058.xhtml#ref-fedus2021switch)).
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种广泛采用的策略是专家混合（MoE）框架。在这个架构中，门控网络选择一组专门的专家子网络来处理每个输入 ([Shazeer, Mirhoseini,
    Maziarz, 以及其他人 2017](ch058.xhtml#ref-shazeer2017outrageously))。这仅允许模型的一小部分在给定输入时处于活动状态，显著提高了计算效率，同时不牺牲模型容量。这一想法的一个显著实例是谷歌的Switch
    Transformer，它通过基于专家的条件计算扩展了Transformer架构 ([Fedus, Zoph, 和 Shazeer 2021a](ch058.xhtml#ref-fedus2021switch))。
- en: '![](../media/file173.svg)'
  id: totrans-672
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file173.svg)'
- en: 'Figure 10.27: **Conditional Computation**: Switch transformers enhance efficiency
    by dynamically routing tokens to specialized expert subnetworks, enabling parallel
    processing and reducing the computational load per input. this architecture implements
    a form of mixture of experts where a gating network selects which experts process
    each token, allowing for increased model capacity without a proportional increase
    in computation. *source ([Fedus, Zoph, and Shazeer 2021a](ch058.xhtml#ref-fedus2021switch))*.'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.27：**条件计算**：开关变压器通过动态路由标记到专门的专家子网络，从而提高效率，实现并行处理并减少每个输入的计算负载。这种架构实现了一种专家混合形式，其中门控网络选择处理每个标记的专家，从而在不增加计算量的情况下提高模型容量。*来源
    ([Fedus, Zoph, 和 Shazeer 2021a](ch058.xhtml#ref-fedus2021switch))*。
- en: As shown in [Figure 10.27](ch016.xhtml#fig-switch-transformer), the Switch Transformer
    replaces the traditional feedforward layer with a Switching FFN Layer. For each
    token, a lightweight router selects a single expert from a pool of feedforward
    networks. The router outputs a probability distribution over available experts,
    and the highest-probability expert is activated per token. This design enables
    large models to scale parameter count without proportionally increasing inference
    cost.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图10.27](ch016.xhtml#fig-switch-transformer)所示，Switch Transformer用开关FFN层替换了传统的前馈层。对于每个标记，一个轻量级路由器从一组前馈网络中选择一个专家。路由器输出一个关于可用专家的概率分布，每个标记激活最高概率的专家。这种设计使得大型模型能够在不按比例增加推理成本的情况下扩展参数数量。
- en: Gate-based conditional computation is particularly effective for multi-task
    and transfer learning settings, where inputs may benefit from specialized processing
    pathways. By enabling fine-grained control over model execution, such mechanisms
    allow for adaptive specialization across tasks while maintaining efficiency.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 基于门控的条件计算对于多任务和迁移学习设置特别有效，在这些设置中，输入可能从专门的加工路径中受益。通过允许对模型执行进行细粒度控制，这些机制允许在保持效率的同时，在任务之间进行自适应专业化。
- en: However, these benefits come at the cost of increased architectural complexity.
    The routing and gating operations themselves introduce additional overhead, both
    in terms of latency and memory access. Efficient deployment, particularly on hardware
    accelerators such as GPUs, TPUs, or edge devices, requires careful attention to
    the scheduling and batching of expert activations ([Lepikhin et al. 2020](ch058.xhtml#ref-lepikhin2020gshard)).
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些好处是以增加架构复杂性为代价的。路由和门控操作本身引入了额外的开销，包括延迟和内存访问。在硬件加速器（如GPU、TPU或边缘设备）上的高效部署，需要仔细关注专家激活的调度和批处理
    ([Lepikhin 等人 2020](ch058.xhtml#ref-lepikhin2020gshard))。
- en: Adaptive Inference
  id: totrans-677
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自适应推理
- en: Adaptive inference refers to a model’s ability to dynamically adjust its computational
    effort during inference based on input complexity. Unlike earlier approaches that
    rely on predefined exit points or discrete layer skipping, adaptive inference
    continuously modulates computational depth and resource allocation based on real-time
    confidence and task complexity ([Yang et al. 2020](ch058.xhtml#ref-yang2020resolution)).
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应推理指的是模型在推理过程中根据输入复杂性动态调整其计算工作量的能力。与依赖于预定义退出点或离散层跳过的早期方法不同，自适应推理根据实时置信度和任务复杂性连续调节计算深度和资源分配（[Yang等人2020](ch058.xhtml#ref-yang2020resolution)）。
- en: This flexibility allows models to make on-the-fly decisions about how much computation
    is required, balancing efficiency and accuracy without rigid thresholds. Instead
    of committing to a fixed computational path, adaptive inference enables models
    to dynamically allocate layers, operations, or specialized computations based
    on intermediate assessments of the input ([Yang et al. 2020](ch058.xhtml#ref-yang2020resolution)).
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 这种灵活性允许模型即时决定所需的计算量，平衡效率和精度，而不需要固定的阈值。与承诺固定的计算路径不同，自适应推理使模型能够根据对输入的中间评估动态分配层、操作或专用计算（[Yang等人2020](ch058.xhtml#ref-yang2020resolution)）。
- en: One example of adaptive inference is Fast Neural Networks (FNNs), which adjust
    the number of active layers based on real-time complexity estimation. If an input
    is deemed straightforward, only a subset of layers is activated, reducing inference
    time. However, if early layers produce low-confidence outputs, additional layers
    are engaged to refine the prediction ([Jian Wu, Cheng, and Zhang 2019](ch058.xhtml#ref-wu2019fast)).
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应推理的一个例子是快速神经网络（FNNs），它根据实时复杂性估计调整活动层的数量。如果一个输入被认为是简单的，则只激活部分层，从而减少推理时间。然而，如果早期层产生低置信度的输出，则会激活额外的层来细化预测（[Jian
    Wu, Cheng, 和 Zhang 2019](ch058.xhtml#ref-wu2019fast)）。
- en: A related approach is dynamic layer scaling, where models progressively increase
    computational depth based on uncertainty estimates. This technique is particularly
    useful for fine-grained classification tasks, where some inputs require only coarse-grained
    processing while others need deeper feature extraction ([Contro et al. 2021](ch058.xhtml#ref-wang2021glam)).
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 一种相关的方法是动态层缩放，模型根据不确定性估计逐步增加计算深度。这项技术在需要细粒度分类任务中特别有用，其中一些输入只需要粗粒度处理，而其他输入则需要更深入的特征提取（[Contro等人2021](ch058.xhtml#ref-wang2021glam)）。
- en: Adaptive inference is particularly effective in latency-sensitive applications
    where resource constraints fluctuate dynamically. For instance, in autonomous
    systems, tasks such as lane detection may require minimal computation, while multi-object
    tracking in dense environments demands additional processing power. By adjusting
    computational effort in real-time, adaptive inference ensures that models operate
    within strict timing constraints without unnecessary resource consumption.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应推理在资源约束动态变化的延迟敏感应用中特别有效。例如，在自主系统中，如车道检测这样的任务可能只需要最小的计算量，而在密集环境中的多目标跟踪则可能需要额外的处理能力。通过实时调整计算工作量，自适应推理确保模型在严格的时序约束下运行，同时避免不必要的资源消耗。
- en: On hardware accelerators such as GPUs and TPUs, adaptive inference leverages
    parallel processing capabilities by distributing workloads dynamically. This adaptability
    maximizes throughput while minimizing energy expenditure, making it ideal for
    real-time, power-sensitive applications.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 在如GPU和TPU这样的硬件加速器上，自适应推理通过动态分配工作负载来利用并行处理能力。这种适应性最大化吞吐量同时最小化能耗，使其非常适合实时、对功耗敏感的应用。
- en: Implementation Challenges
  id: totrans-684
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实施挑战
- en: Dynamic computation introduces flexibility and efficiency by allowing models
    to adjust their computational workload based on input complexity. However, this
    adaptability comes with several challenges that must be addressed to make dynamic
    computation practical and scalable. These challenges arise in training, inference
    efficiency, hardware execution, generalization, and evaluation, each presenting
    unique difficulties that impact model design and deployment.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 动态计算通过允许模型根据输入复杂性调整其计算工作量，引入了灵活性和效率。然而，这种适应性伴随着几个挑战，必须解决这些挑战才能使动态计算实用且可扩展。这些挑战出现在训练、推理效率、硬件执行、泛化和评估中，每个都带来了独特的困难，影响了模型的设计和部署。
- en: Training and Optimization Difficulties
  id: totrans-686
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练和优化困难
- en: Unlike standard neural networks, which follow a fixed computational path for
    every input, dynamic computation requires additional control mechanisms, such
    as gating networks, confidence estimators, or expert selection strategies. These
    mechanisms determine which parts of the model should be activated or skipped,
    adding complexity to the training process. One major difficulty is that many of
    these decisions are discrete, meaning they cannot be optimized using standard
    backpropagation. Instead, models often rely on techniques like reinforcement learning
    or continuous approximations, but these approaches introduce additional computational
    costs and can slow down convergence.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 与遵循每个输入固定计算路径的标准神经网络不同，动态计算需要额外的控制机制，如门控网络、置信度估计器或专家选择策略。这些机制确定模型应该激活或跳过的部分，增加了训练过程的复杂性。一个主要困难是许多这些决策是离散的，这意味着它们不能使用标准反向传播进行优化。相反，模型通常依赖于强化学习或连续近似等技术，但这些方法引入了额外的计算成本，并可能减慢收敛速度。
- en: Training dynamic models also presents instability because different inputs follow
    different paths, leading to inconsistent gradient updates across training examples.
    This variability can make optimization less efficient, requiring careful regularization
    strategies to maintain smooth learning dynamics. Dynamic models introduce new
    hyperparameters, such as gating thresholds or confidence scores for early exits.
    Selecting appropriate values for these parameters is important to ensuring the
    model effectively balances accuracy and efficiency, but it significantly increases
    the complexity of the training process.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 训练动态模型也带来了不稳定性，因为不同的输入遵循不同的路径，导致训练示例中梯度更新的不一致。这种可变性可能会降低优化效率，需要仔细的正则化策略来维持平滑的学习动态。动态模型引入了新的超参数，例如门控阈值或早期退出的置信度分数。为这些参数选择适当的值对于确保模型有效地平衡准确性和效率至关重要，但它显著增加了训练过程的复杂性。
- en: Overhead and Latency Variability
  id: totrans-689
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 负载和延迟变化
- en: Although dynamic computation reduces unnecessary operations, the process of
    determining which computations to perform introduces additional overhead. Before
    executing inference, the model must first decide which layers, paths, or subnetworks
    to activate. This decision-making process, often implemented through lightweight
    gating networks, adds computational cost and can partially offset the savings
    gained by skipping computations. While these overheads are usually small, they
    become significant in resource-constrained environments where every operation
    matters.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管动态计算减少了不必要的操作，但确定要执行的计算的过程引入了额外的开销。在执行推理之前，模型必须首先决定要激活哪些层、路径或子网络。这个决策过程，通常通过轻量级门控网络实现，增加了计算成本，并可能部分抵消了跳过计算所获得的节省。虽然这些开销通常很小，但在每个操作都很重要的资源受限环境中，它们变得很重要。
- en: An even greater challenge is the variability in inference time. In static models,
    inference follows a fixed sequence of operations, leading to predictable execution
    times. In contrast, dynamic models exhibit variable processing times depending
    on input complexity. For applications with strict real-time constraints, such
    as autonomous driving or robotics, this unpredictability can be problematic. A
    model that processes some inputs in milliseconds but others in significantly longer
    time frames may fail to meet strict latency requirements, limiting its practical
    deployment.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更大的挑战是推理时间的可变性。在静态模型中，推理遵循一系列固定的操作，导致可预测的执行时间。相比之下，动态模型根据输入复杂度表现出可变的处理时间。对于具有严格实时约束的应用，如自动驾驶或机器人技术，这种不可预测性可能成为问题。一个模型可能以毫秒处理某些输入，但在显著更长的时间框架内处理其他输入，可能无法满足严格的延迟要求，限制其实际部署。
- en: Hardware Execution Inefficiencies
  id: totrans-692
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 硬件执行效率低下
- en: Modern hardware accelerators, such as GPUs and TPUs, are optimized for [uniform,
    parallel computation patterns](https://pytorch.org/xla/master/perf/recompilation.html).
    These accelerators achieve maximum efficiency by executing identical operations
    across large batches of data simultaneously. However, dynamic computation introduces
    conditional branching, which can disrupt this parallel execution model. When different
    inputs follow different computational paths, some processing units may remain
    idle while others are active, leading to suboptimal hardware utilization.
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 现代硬件加速器，如GPU和TPU，针对[统一、并行计算模式](https://pytorch.org/xla/master/perf/recompilation.html)进行了优化。这些加速器通过在大批量数据上同时执行相同的操作来实现最大效率。然而，动态计算引入了条件分支，这可能会破坏这种并行执行模型。当不同的输入遵循不同的计算路径时，一些处理单元可能会闲置，而其他处理单元处于活动状态，导致硬件利用率不佳。
- en: This divergent execution pattern creates significant challenges for hardware
    efficiency. For example, in a GPU where multiple threads process data in parallel,
    conditional branches cause thread divergence, where some threads must wait while
    others complete their operations. Similarly, TPUs are designed for large matrix
    operations and achieve peak performance when all processing units are fully utilized.
    Dynamic computation can prevent these accelerators from maintaining high throughput,
    potentially reducing the cost-effectiveness of deployment at scale.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 这种发散的执行模式对硬件效率提出了重大挑战。例如，在一个GPU中，多个线程并行处理数据时，条件分支会导致线程发散，其中一些线程必须等待，而其他线程完成其操作。同样，TPU是为大型矩阵运算设计的，当所有处理单元都得到充分利用时，可以达到峰值性能。动态计算可能会阻止这些加速器保持高吞吐量，从而降低大规模部署的成本效益。
- en: The impact is particularly pronounced in scenarios requiring real-time processing
    or high-throughput inference. When hardware resources are not fully utilized,
    the theoretical computational benefits of dynamic computation may not translate
    into practical performance gains. This inefficiency becomes more significant in
    large-scale deployments where maximizing hardware utilization is important for
    managing operational costs and maintaining service-level agreements.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要实时处理或高吞吐量推理的场景中，这种影响尤为明显。当硬件资源没有得到充分利用时，动态计算的理论计算优势可能无法转化为实际性能提升。在大规模部署中，这种低效变得更加显著，因为最大化硬件利用率对于管理运营成本和维持服务水平协议至关重要。
- en: Memory access patterns also become less predictable in dynamic models. Standard
    machine learning models process data in a structured manner, optimizing for efficient
    memory access. In contrast, dynamic models require frequent branching, leading
    to irregular memory access and increased latency. Optimizing these models for
    hardware execution requires specialized scheduling strategies and compiler optimizations
    to mitigate these inefficiencies, but such solutions add complexity to deployment.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 在动态模型中，内存访问模式也变得不太可预测。标准的机器学习模型以结构化的方式处理数据，优化高效的内存访问。相比之下，动态模型需要频繁的分支，导致不规则的内存访问和增加的延迟。为了优化这些模型在硬件上的执行，需要专门的调度策略和编译器优化来减轻这些低效，但这些解决方案增加了部署的复杂性。
- en: Generalization and Robustness
  id: totrans-697
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**泛化**和**鲁棒性**'
- en: Because dynamic computation allows different inputs to take different paths
    through the model, there is a risk that certain data distributions receive less
    computation than necessary. If the gating functions are not carefully designed,
    the model may learn to consistently allocate fewer resources to specific types
    of inputs, leading to biased predictions. This issue is particularly concerning
    in safety-important applications, where failing to allocate enough computation
    to rare but important inputs can result in catastrophic failures.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 由于动态计算允许不同的输入通过模型的不同路径，存在某些数据分布接收的计算量少于必要的风险。如果门控函数没有精心设计，模型可能会学会持续为特定类型的输入分配较少的资源，导致预测偏差。这个问题在安全至关重要的应用中尤其令人担忧，未能为罕见但重要的输入分配足够的计算可能导致灾难性故障。
- en: Another concern is overfitting to training-time computational paths. If a model
    is trained with a certain distribution of computational choices, it may struggle
    to generalize to new inputs where different paths should be taken. Ensuring that
    a dynamic model remains adaptable to unseen data requires additional robustness
    mechanisms, such as entropy-based regularization or uncertainty-driven gating,
    but these introduce additional training complexities.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个担忧是训练时间计算路径的过拟合。如果一个模型在特定的计算选择分布下进行训练，它可能难以泛化到需要采取不同路径的新输入。确保动态模型能够适应未见数据需要额外的鲁棒性机制，例如基于熵的正则化或不确定性驱动的门控，但这些机制引入了额外的训练复杂性。
- en: Dynamic computation also creates new vulnerabilities to adversarial attacks.
    In standard models, an attacker might attempt to modify an input in a way that
    alters the final prediction. In dynamic models, an attacker could manipulate the
    gating mechanisms themselves, forcing the model to choose an incorrect or suboptimal
    computational path. Defending against such attacks requires additional security
    measures that further complicate model design and deployment.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 动态计算也带来了新的对抗性攻击漏洞。在标准模型中，攻击者可能会尝试以改变最终预测的方式修改输入。在动态模型中，攻击者可以操纵门控机制本身，迫使模型选择错误或次优的计算路径。防御此类攻击需要额外的安全措施，这进一步复杂了模型的设计和部署。
- en: Evaluation and Benchmarking
  id: totrans-701
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估和基准测试
- en: Most machine learning benchmarks assume a fixed computational budget, making
    it difficult to evaluate the performance of dynamic models. Traditional metrics
    such as FLOPs or latency do not fully capture the adaptive nature of these models,
    where computation varies based on input complexity. As a result, standard benchmarks
    fail to reflect the true trade-offs between accuracy and efficiency in dynamic
    architectures.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习基准假设固定的计算预算，这使得评估动态模型的性能变得困难。传统的指标，如FLOPs或延迟，并不能完全捕捉这些模型的适应性，因为计算量会根据输入的复杂性而变化。因此，标准基准未能反映动态架构中准确性和效率之间的真实权衡。
- en: Another issue is reproducibility. Because dynamic models make input-dependent
    decisions, running the same model on different hardware or under slightly different
    conditions can lead to variations in execution paths. This variability complicates
    fair comparisons between models and requires new evaluation methodologies to accurately
    assess the benefits of dynamic computation. Without standardized benchmarks that
    account for adaptive scaling, it remains challenging to measure and compare dynamic
    models against their static counterparts in a meaningful way.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是对可重复性的担忧。由于动态模型会根据输入做出决策，在不同的硬件或略微不同的条件下运行相同的模型可能会导致执行路径的变化。这种可变性使得模型之间的公平比较变得复杂，并需要新的评估方法来准确评估动态计算的好处。没有考虑到自适应缩放的标准化基准，测量和比较动态模型与其静态对应物仍然具有挑战性。
- en: Despite these challenges, dynamic computation remains a promising direction
    for optimizing efficiency in machine learning. Addressing these limitations requires
    more robust training techniques, hardware-aware execution strategies, and improved
    evaluation frameworks that properly account for dynamic scaling. As machine learning
    continues to scale and computational constraints become more pressing, solving
    these challenges will be key to unlocking the full potential of dynamic computation.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，动态计算仍然是优化机器学习效率的有希望的方向。解决这些限制需要更鲁棒的训练技术、硬件感知的执行策略和改进的评估框架，这些框架能够正确考虑动态缩放。随着机器学习继续扩展，计算约束变得更加紧迫，解决这些挑战将是解锁动态计算全部潜力的关键。
- en: Sparsity Exploitation
  id: totrans-705
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稀疏性利用
- en: 'Sparsity in machine learning refers to the condition where a significant portion
    of the elements within a tensor, such as weight matrices or activation tensors,
    are zero or nearly zero. More formally, for a tensor <semantics><mrow><mi>T</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">T \in \mathbb{R}^{m \times n}</annotation></semantics>
    (or higher dimensions), the sparsity <semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics>
    can be expressed as: <semantics><mrow><mi>S</mi><mo>=</mo><mfrac><mrow><mo stretchy="false"
    form="postfix">‖</mo><msub><mn>𝟏</mn><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>T</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>0</mn><mo
    stretchy="false" form="postfix">}</mo></mrow></msub><msub><mo stretchy="false"
    form="postfix">‖</mo><mn>0</mn></msub></mrow><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">S = \frac{\Vert \mathbf{1}_{\{T_{ij}
    = 0\}} \Vert_0}{m \times n}</annotation></semantics> where <semantics><msub><mn>𝟏</mn><mrow><mo
    stretchy="false" form="prefix">{</mo><msub><mi>T</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>0</mn><mo
    stretchy="false" form="postfix">}</mo></mrow></msub><annotation encoding="application/x-tex">\mathbf{1}_{\{T_{ij}
    = 0\}}</annotation></semantics> is an indicator function that yields 1 if <semantics><mrow><msub><mi>T</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">T_{ij} = 0</annotation></semantics> and 0 otherwise,
    and <semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mo>⋅</mo><msub><mo
    stretchy="false" form="postfix">‖</mo><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\Vert
    \cdot \Vert_0</annotation></semantics> represents the L0 norm, which counts the
    number of non-zero elements.'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的稀疏性指的是张量（如权重矩阵或激活张量）中很大一部分元素为零或接近零的状态。更正式地说，对于一个张量 <semantics><mrow><mi>T</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">T \in \mathbb{R}^{m \times n}</annotation></semantics>（或更高维度），其稀疏性
    <semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics>
    可以表示为：<semantics><mrow><mi>S</mi><mo>=</mo><mfrac><mrow><mo stretchy="false" form="postfix">‖</mo><msub><mn>𝟏</mn><mrow><mo
    stretchy="false" form="prefix">{</mo><msub><mi>T</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>0</mn><mo
    stretchy="false" form="postfix">}</mo></mrow></msub><msub><mo stretchy="false"
    form="postfix">‖</mo><mn>0</mn></msub></mrow><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">S = \frac{\Vert \mathbf{1}_{\{T_{ij}
    = 0\}} \Vert_0}{m \times n}</annotation></semantics> 其中 <semantics><msub><mn>𝟏</mn><mrow><mo
    stretchy="false" form="prefix">{</mo><msub><mi>T</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>0</mn><mo
    stretchy="false" form="postfix">}</mo></mrow></msub><annotation encoding="application/x-tex">\mathbf{1}_{\{T_{ij}
    = 0\}}</annotation></semantics> 是一个指示函数，当 <semantics><mrow><msub><mi>T</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">T_{ij} = 0</annotation></semantics> 时返回 1，否则返回 0，而
    <semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mo>⋅</mo><msub><mo
    stretchy="false" form="postfix">‖</mo><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\Vert
    \cdot \Vert_0</annotation></semantics> 表示 L0 范数，它计算非零元素的数量。
- en: 'Due to the nature of floating-point representations, we often extend this definition
    to include elements that are close to zero. This leads to: <semantics><mrow><msub><mi>S</mi><mi>ϵ</mi></msub><mo>=</mo><mfrac><mrow><mo
    stretchy="false" form="postfix">‖</mo><msub><mn>𝟏</mn><mrow><mo stretchy="false"
    form="prefix">{</mo><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>T</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">|</mo></mrow><mo><</mo><mi>ϵ</mi><mo stretchy="false"
    form="postfix">}</mo></mrow></msub><msub><mo stretchy="false" form="postfix">‖</mo><mn>0</mn></msub></mrow><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">S_{\epsilon} = \frac{\Vert \mathbf{1}_{\{|T_{ij}|
    < \epsilon\}} \Vert_0}{m \times n}</annotation></semantics> where <semantics><mi>ϵ</mi><annotation
    encoding="application/x-tex">\epsilon</annotation></semantics> is a small threshold
    value.'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 由于浮点表示的性质，我们通常将此定义扩展到包括接近零的元素。这导致：<semantics><mrow><msub><mi>S</mi><mi>ϵ</mi></msub><mo>=</mo><mfrac><mrow><mo
    stretchy="false" form="postfix">‖</mo><msub><mn>𝟏</mn><mrow><mo stretchy="false"
    form="prefix">{</mo><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>T</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">|</mo></mrow><mo><</mo><mi>ϵ</mi><mo stretchy="false"
    form="postfix">}</mo></mrow></msub><msub><mo stretchy="false" form="postfix">‖</mo><mn>0</mn></msub></mrow><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">S_{\epsilon} = \frac{\Vert \mathbf{1}_{\{|T_{ij}|
    < \epsilon\}} \Vert_0}{m \times n}</annotation></semantics> 其中 <semantics><mi>ϵ</mi><annotation
    encoding="application/x-tex">\epsilon</annotation></semantics> 是一个小的阈值值。
- en: Sparsity can emerge naturally during training, often as a result of regularization
    techniques, or be deliberately introduced through methods like pruning, where
    elements below a specific threshold are forced to zero. Effectively exploiting
    sparsity leads to significant computational efficiency, memory savings, and reduced
    power consumption, which are particularly valuable when deploying models on devices
    with limited resources, such as mobile phones, embedded systems, and edge devices.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性可以在训练过程中自然出现，通常是由于正则化技术的结果，或者通过剪枝等方法故意引入，其中低于特定阈值的元素被强制设为零。有效地利用稀疏性可以带来显著的计算效率、内存节省和降低功耗，这在将模型部署在资源有限的设备上，如手机、嵌入式系统和边缘设备时尤其有价值。
- en: Sparsity Types
  id: totrans-709
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稀疏性类型
- en: 'Sparsity in neural networks can be broadly classified into two types: unstructured
    sparsity and structured sparsity.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的稀疏性可以大致分为两种类型：非结构化稀疏性和结构化稀疏性。
- en: Unstructured sparsity occurs when individual weights are set to zero without
    any specific pattern. This type of sparsity can be achieved through techniques
    like pruning, where weights that are considered less important (often based on
    magnitude or other criteria) are removed. While unstructured sparsity is highly
    flexible and can be applied to any part of the network, it can be less efficient
    on hardware since it lacks a predictable structure. In practice, exploiting unstructured
    sparsity requires specialized hardware or software optimizations to make the most
    of it.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化稀疏性发生在单个权重被设置为零而没有任何特定模式的情况下。这种类型的稀疏性可以通过剪枝等技术实现，其中被认为不那么重要的权重（通常基于幅度或其他标准）被移除。虽然非结构化稀疏性非常灵活，可以应用于网络的任何部分，但在硬件上可能效率较低，因为它缺乏可预测的结构。在实践中，利用非结构化稀疏性需要专门的硬件或软件优化，以最大限度地发挥其作用。
- en: In contrast, structured sparsity involves removing entire components of the
    network, such as filters, neurons, or channels, in a more structured manner. By
    eliminating entire parts of the network, structured sparsity is more efficient
    on hardware accelerators like GPUs or TPUs, which can leverage this structure
    for faster computations. Structured sparsity is often used when there is a need
    for predictability and efficiency in computational resources, as it enables the
    hardware to fully exploit regular patterns in the network.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，结构化稀疏性涉及以更结构化的方式移除网络中的整个组件，如过滤器、神经元或通道。通过消除网络的整个部分，结构化稀疏性在硬件加速器（如GPU或TPU）上更为高效，这些硬件可以利用这种结构进行更快地计算。结构化稀疏性通常在需要计算资源中的可预测性和效率时使用，因为它使硬件能够充分利用网络中的常规模式。
- en: Sparsity Utilization Methods
  id: totrans-713
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稀疏性利用方法
- en: Exploiting sparsity effectively requires specialized techniques and hardware
    support to translate theoretical parameter reduction into actual performance gains
    ([Hoefler, Alistarh, Ben-Nun, Dryden, and Peste 2021](ch058.xhtml#ref-Hoefler2021)).
    Pruning introduces sparsity by removing less important weights (unstructured)
    or entire components like filters, channels, or layers (structured) ([Han et al.
    2015](ch058.xhtml#ref-Han2015)). Structured pruning proves more hardware-efficient,
    enabling accelerators like GPUs and TPUs to fully exploit regular patterns.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 有效地利用稀疏性需要专门的技术和硬件支持，以将理论参数减少转化为实际性能提升（[Hoefler, Alistarh, Ben-Nun, Dryden,
    和 Peste 2021](ch058.xhtml#ref-Hoefler2021)）。剪枝通过移除不那么重要的权重（非结构化）或整个组件，如过滤器、通道或层（结构化）来引入稀疏性（[Han
    等人 2015](ch058.xhtml#ref-Han2015)）。结构化剪枝在硬件效率上更为优越，使得GPU和TPU等加速器能够充分利用常规模式。
- en: 'Sparse matrix operations skip zero elements during computation, significantly
    reducing arithmetic operations. For example, multiplying a dense <semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation
    encoding="application/x-tex">4\times 4</annotation></semantics> matrix with a
    vector typically requires 16 multiplications, while a sparse-aware implementation
    computes only the 6 nonzero operations: <semantics><mrow><mrow><mo stretchy="true"
    form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align:
    center"><mn>2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd
    columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>3</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd
    columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>4</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>5</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>6</mn></mtd></mtr></mtable><mo
    stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>4</mn></msub></mtd></mtr></mtable><mo
    stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mrow><mo stretchy="true"
    form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align:
    center"><mn>2</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>4</mn></msub></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>3</mn><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>4</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>5</mn><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>6</mn><msub><mi>x</mi><mn>4</mn></msub></mtd></mtr></mtable><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow> <annotation encoding="application/x-tex">\begin{bmatrix}
    2 & 0 & 0 & 1 \\ 0 & 3 & 0 & 0 \\ 4 & 0 & 5 & 0 \\ 0 & 0 & 0 & 6 \end{bmatrix}
    \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix} = \begin{bmatrix} 2x_1
    + x_4 \\ 3x_2 \\ 4x_1 + 5x_3 \\ 6x_4 \end{bmatrix}</annotation></semantics>'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
- en: A third important technique for exploiting sparsity is low-rank approximation.
    In this approach, large, dense weight matrices are approximated by smaller, lower-rank
    matrices that capture the most important information while discarding redundant
    components. This reduces both the storage requirements and computational cost.
    For instance, a weight matrix of size <semantics><mrow><mn>1000</mn><mo>×</mo><mn>1000</mn></mrow><annotation
    encoding="application/x-tex">1000 \times 1000</annotation></semantics> with one
    million parameters can be factorized into two smaller matrices, say <semantics><mi>U</mi><annotation
    encoding="application/x-tex">U</annotation></semantics> (size <semantics><mrow><mn>1000</mn><mo>×</mo><mn>50</mn></mrow><annotation
    encoding="application/x-tex">1000 \times 50</annotation></semantics>) and <semantics><mi>V</mi><annotation
    encoding="application/x-tex">V</annotation></semantics> (size <semantics><mrow><mn>50</mn><mo>×</mo><mn>1000</mn></mrow><annotation
    encoding="application/x-tex">50 \times 1000</annotation></semantics>), which results
    in only 100,000 parameters, much fewer than the original one million. This smaller
    representation retains the key features of the original matrix while significantly
    reducing the computational burden ([Denton, Chintala, and Fergus 2014](ch058.xhtml#ref-Denton2014)).
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 利用稀疏性的第三个重要技术是低秩近似。这种方法中，大型密集权重矩阵被更小、秩更低的矩阵近似，这些矩阵捕捉最重要的信息同时丢弃冗余组件。这降低了存储需求和计算成本。例如，一个大小为<semantics><mrow><mn>1000</mn><mo>×</mo><mn>1000</mn></mrow><annotation
    encoding="application/x-tex">1000 \times 1000</annotation></semantics>的权重矩阵，包含一百万个参数，可以被分解成两个较小的矩阵，比如<semantics><mi>U</mi><annotation
    encoding="application/x-tex">U</annotation></semantics>（大小<semantics><mrow><mn>1000</mn><mo>×</mo><mn>50</mn></mrow><annotation
    encoding="application/x-tex">1000 \times 50</annotation></semantics>）和<semantics><mi>V</mi><annotation
    encoding="application/x-tex">V</annotation></semantics>（大小<semantics><mrow><mn>50</mn><mo>×</mo><mn>1000</mn></mrow><annotation
    encoding="application/x-tex">50 \times 1000</annotation></semantics>），这仅包含10万个参数，远少于原始的一百万个。这种较小的表示保留了原始矩阵的关键特征，同时显著降低了计算负担（[Denton,
    Chintala, 和 Fergus 2014](ch058.xhtml#ref-Denton2014)）。
- en: Low-rank approximations, such as Singular Value Decomposition, are commonly
    used to compress weight matrices in neural networks. These approximations are
    widely applied in recommendation systems and natural language processing models
    to reduce computational complexity and memory usage without a significant loss
    in performance ([Joulin et al. 2017](ch058.xhtml#ref-Joulin2017)).
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩近似，如奇异值分解，常用于压缩神经网络中的权重矩阵。这些近似在推荐系统和自然语言处理模型中得到广泛应用，以降低计算复杂性和内存使用，同时不会显著损失性能（[Joulin
    等人 2017](ch058.xhtml#ref-Joulin2017)）。
- en: In addition to these core methods, other techniques like sparsity-aware training
    can also help models to learn sparse representations during training. For instance,
    using sparse gradient descent, where the training algorithm updates only non-zero
    elements, can help the model operate with fewer active parameters. While pruning
    and low-rank approximations directly reduce parameters or factorize weight matrices,
    sparsity-aware training helps maintain efficient models throughout the training
    process ([C. Liu et al. 2018](ch058.xhtml#ref-Bellec2018)).
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些核心方法之外，其他技术如稀疏感知训练也可以帮助模型在训练过程中学习稀疏表示。例如，使用稀疏梯度下降，其中训练算法只更新非零元素，可以帮助模型以更少的活跃参数运行。虽然剪枝和低秩近似直接减少参数或分解权重矩阵，但稀疏感知训练有助于在整个训练过程中保持模型的效率（[C.
    Liu 等人 2018](ch058.xhtml#ref-Bellec2018)）。
- en: Sparsity Hardware Support
  id: totrans-719
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稀疏性硬件支持
- en: While sparsity theoretically reduces computational cost, memory usage, and power
    consumption, achieving actual speedups requires overcoming hardware-software mismatches.
    General-purpose processors like CPUs lack optimization for sparse matrix operations
    ([Han, Mao, and Dally 2016](ch058.xhtml#ref-Han2016)), while modern accelerators
    (GPUs, TPUs, FPGAs) face architectural challenges in efficiently processing irregular
    sparse data patterns. Hardware support proves integral to model optimization—specialized
    accelerators must efficiently process sparse data to translate theoretical compression
    into actual performance gains during training and inference.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然稀疏性在理论上可以降低计算成本、内存使用和功耗，但要实现实际的速度提升，需要克服硬件和软件不匹配的问题。通用处理器如CPU缺乏对稀疏矩阵操作的优化（[Han,
    Mao, 和 Dally 2016](ch058.xhtml#ref-Han2016)），而现代加速器（GPU、TPU、FPGA）在高效处理不规则稀疏数据模式方面面临架构挑战。硬件支持对于模型优化至关重要——专门的加速器必须高效地处理稀疏数据，以便在训练和推理过程中将理论压缩转化为实际性能提升。
- en: Sparse operations can also be well mapped onto hardware via software. For example,
    MegaBlocks ([Gale et al. 2022](ch058.xhtml#ref-gale2022megablocksefficientsparsetraining))
    reformulates sparse Mixture of Experts training into block-sparse operations and
    develops GPU specific kernels to efficiently handle the sparsity of these computations
    on hardware and maintain high accelerator utilization.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏操作也可以通过软件很好地映射到硬件上。例如，MegaBlocks ([Gale et al. 2022](ch058.xhtml#ref-gale2022megablocksefficientsparsetraining))
    将稀疏混合专家训练重新表述为块稀疏操作，并开发了针对GPU的特定内核，以高效地处理这些计算在硬件上的稀疏性，并保持高加速器利用率。
- en: Structured Patterns
  id: totrans-722
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结构化模式
- en: Various sparsity formats have been developed, each with unique structural characteristics
    and implications. Two of the most prominent are block sparse matrices and N:M
    sparsity patterns. Block sparse matrices generally have isolated blocks of zero
    and non-zero dense submatrices such that a matrix operation on the large sparse
    matrix can be easily re-expressed as a smaller (overall arithmetic-wise) number
    of dense operations on submatrices. This sparsity allows more efficient storage
    of the dense submatricies while maintaining shape compatibility for operations
    like matrix or vector products. For example, [Figure 10.28](ch016.xhtml#fig-block-sparse-gemm)
    shows how NVIDIA’s [cuSPARSE](https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/)
    library supports sparse block matrix operations and storage. Several other works,
    such as Monarch matrices ([Dao et al. 2022](ch058.xhtml#ref-dao2022monarchexpressivestructuredmatrices)),
    have extended on this block-sparsity to strike an improved balance between matrix
    expressivity and compute/memory efficiency.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发了各种稀疏格式，每种格式都有独特的结构特性和影响。其中最突出的是块稀疏矩阵和N:M稀疏模式。块稀疏矩阵通常具有孤立的零和非零密集子矩阵块，使得对大型稀疏矩阵的操作可以很容易地重新表述为对子矩阵的较小（从算术角度看）数量的密集操作。这种稀疏性允许更有效地存储密集子矩阵，同时保持与矩阵或向量乘法等操作形状兼容。例如，[图10.28](ch016.xhtml#fig-block-sparse-gemm)
    展示了NVIDIA的[cuSPARSE](https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/)库如何支持稀疏块矩阵操作和存储。其他一些工作，如Monarch矩阵([Dao
    et al. 2022](ch058.xhtml#ref-dao2022monarchexpressivestructuredmatrices))，在此基础上扩展了块稀疏性，以在矩阵表达性和计算/内存效率之间取得更好的平衡。
- en: '![](../media/file174.svg)'
  id: totrans-724
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file174.svg)'
- en: 'Figure 10.28: **Block Sparse Representation**: NVIDIA’s cusparse library efficiently
    stores block sparse matrices by exploiting dense submatrix structures, enabling
    accelerated matrix operations while maintaining compatibility with dense matrix
    computations through block indexing. this approach reduces memory footprint and
    arithmetic complexity for sparse linear algebra, important for scaling machine
    learning models. *source: NVIDIA.*.'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.28：**块稀疏表示**：NVIDIA的cusparse库通过利用密集子矩阵结构有效地存储块稀疏矩阵，从而实现加速矩阵操作，同时通过块索引与密集矩阵计算保持兼容。这种方法减少了稀疏线性代数的内存占用和算术复杂性，这对于扩展机器学习模型非常重要。*来源：NVIDIA.*。
- en: Similarly, the <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>:<semantics><mi>M</mi><annotation
    encoding="application/x-tex">M</annotation></semantics> sparsity pattern is a
    structured sparsity format where, in every set of <semantics><mi>M</mi><annotation
    encoding="application/x-tex">M</annotation></semantics> consecutive elements (e.g.,
    weights or activations), exactly <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    are non-zero, and the other two are zero ([Zhou et al. 2021](ch058.xhtml#ref-zhou2021learningnmfinegrainedstructured)).
    This deterministic pattern facilitates efficient hardware acceleration, as it
    allows for predictable memory access patterns and optimized computations. By enforcing
    this structure, models can achieve a balance between sparsity-induced efficiency
    gains and maintaining sufficient capacity for learning complex representations.
    [Figure 10.29](ch016.xhtml#fig-2-4-gemm) below shows a comparison between accelerating
    dense versus 2:4 sparsity matrix multiplication, a common sparsity pattern used
    in model training. Later works like STEP ([Lu et al. 2023](ch058.xhtml#ref-lu2023steplearningnmstructured))
    have examined learning more general <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>:<semantics><mi>M</mi><annotation
    encoding="application/x-tex">M</annotation></semantics> sparsity masks for accelerating
    deep learning inference under the same principles.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>:<semantics><mi>M</mi><annotation
    encoding="application/x-tex">M</annotation></semantics>的稀疏模式是一种结构化稀疏格式，其中在每组<semantics><mi>M</mi><annotation
    encoding="application/x-tex">M</annotation></semantics>个连续元素（例如，权重或激活）中，恰好有<semantics><mi>N</mi><annotation
    encoding="application/x-tex">N</annotation></semantics>个非零，其余两个为零（[周等人2021](ch058.xhtml#ref-zhou2021learningnmfinegrainedstructured)）。这种确定性模式促进了高效的硬件加速，因为它允许预测内存访问模式和优化的计算。通过强制执行这种结构，模型可以在稀疏引起的效率提升和保持足够的学习复杂表示能力之间取得平衡。[图10.29](ch016.xhtml#fig-2-4-gemm)下面展示了加速密集矩阵乘法和2:4稀疏矩阵乘法之间的比较，这是模型训练中常用的稀疏模式。后续工作如STEP([Lu等人2023](ch058.xhtml#ref-lu2023steplearningnmstructured))考察了学习更通用的<semantics><mi>N</mi><annotation
    encoding="application/x-tex">N</annotation></semantics>:<semantics><mi>M</mi><annotation
    encoding="application/x-tex">M</annotation></semantics>稀疏掩码，以在相同的原则下加速深度学习推理。
- en: '![](../media/file175.svg)'
  id: totrans-727
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file175.svg)'
- en: 'Figure 10.29: **Sparse Matrix Multiplication**: Block sparsity optimizes matrix
    operations by storing only non-zero elements and using structured indexing, enabling
    efficient GPU acceleration for neural network computations. this technique maintains
    compatibility with dense matrix operations while reducing memory access and computational
    cost, particularly beneficial for large-scale models. source: [PyTorch blog](HTTPS://PyTorch.org/blog/accelerating-neural-network-training/).'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.29：**稀疏矩阵乘法**：块稀疏通过仅存储非零元素和使用结构化索引来优化矩阵运算，从而为神经网络计算提供高效的GPU加速。这种技术保持了与密集矩阵运算的兼容性，同时减少了内存访问和计算成本，对于大规模模型特别有益。来源：[PyTorch博客](HTTPS://PyTorch.org/blog/accelerating-neural-network-training/)。
- en: GPUs and Sparse Operations
  id: totrans-729
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPU和稀疏运算
- en: Graphics Processing Units (GPUs) are widely recognized for their ability to
    perform highly parallel computations, making them ideal for handling the large-scale
    matrix operations that are common in machine learning. Modern GPUs, such as NVIDIA’s
    Ampere architecture, include specialized Sparse Tensor Cores that accelerate sparse
    matrix multiplications. These tensor cores are designed to recognize and skip
    over zero elements in sparse matrices, thereby reducing the number of operations
    required ([Abdelkhalik et al. 2022](ch058.xhtml#ref-NVIDIA2020)). This is particularly
    advantageous for structured pruning techniques, where entire filters, channels,
    or layers are pruned, resulting in a significant reduction in the amount of computation.
    By skipping over the zero values, GPUs can speed up matrix multiplications by
    a factor of two or more, resulting in lower processing times and reduced power
    consumption for sparse networks.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 图形处理单元（GPUs）因其执行高度并行计算的能力而广为人知，这使得它们非常适合处理机器学习中常见的大规模矩阵运算。现代GPU，如NVIDIA的Ampere架构，包括专门用于加速稀疏矩阵乘法的稀疏张量核心。这些张量核心被设计用来识别并跳过稀疏矩阵中的零元素，从而减少所需的操作次数（[Abdelkhalik等人2022](ch058.xhtml#ref-NVIDIA2020)）。这对于结构化剪枝技术特别有利，其中整个滤波器、通道或层被剪枝，从而显著减少了计算量。通过跳过零值，GPU可以将矩阵乘法的速度提高两倍或更多，从而降低稀疏网络的处理时间和功耗。
- en: GPUs leverage their parallel architecture to handle multiple operations simultaneously.
    This parallelism is especially beneficial for sparse operations, as it allows
    the hardware to exploit the inherent sparsity in the data more efficiently. However,
    the full benefit of sparse operations on GPUs requires that the sparsity is structured
    in a way that aligns with the underlying hardware architecture, making structured
    pruning more advantageous for optimization ([Hoefler, Alistarh, Ben-Nun, Dryden,
    and Peste 2021](ch058.xhtml#ref-Hoefler2021)).
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 利用其并行架构来同时处理多个操作。这种并行性对于稀疏操作特别有益，因为它允许硬件更有效地利用数据中的固有稀疏性。然而，GPU 上稀疏操作的全部好处需要稀疏性以与底层硬件架构相匹配的方式组织，这使得结构化剪枝在优化方面更有优势
    ([Hoefler, Alistarh, Ben-Nun, Dryden, 和 Peste 2021](ch058.xhtml#ref-Hoefler2021))。
- en: TPUs and Sparse Optimization
  id: totrans-732
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: TPUs 和稀疏优化
- en: TPUs, developed by Google, are custom-built hardware accelerators specifically
    designed to handle tensor computations at a much higher efficiency than traditional
    processors. TPUs, such as TPU v4, have built-in support for sparse weight matrices,
    which is particularly beneficial for models like transformers, including BERT
    and GPT, that rely on large-scale matrix multiplications ([Norman P. Jouppi et
    al. 2021a](ch058.xhtml#ref-Jouppi2021)). TPUs optimize sparse weight matrices
    by reducing the computational load associated with zero elements, enabling faster
    processing and improved energy efficiency.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: TPUs 是由谷歌开发的定制硬件加速器，专门设计用于以比传统处理器更高的效率处理张量计算。TPUs，例如 TPU v4，内置了对稀疏权重矩阵的支持，这对于像
    BERT 和 GPT 这样的模型特别有益，因为这些模型依赖于大规模矩阵乘法 ([Norman P. Jouppi 等人 2021a](ch058.xhtml#ref-Jouppi2021))。TPUs
    通过减少与零元素相关的计算负载来优化稀疏权重矩阵，从而实现更快的处理和更高的能源效率。
- en: The efficiency of TPUs comes from their ability to perform operations at high
    throughput and low latency, thanks to their custom-designed matrix multiply units.
    These units are able to accelerate sparse matrix operations by directly processing
    the non-zero elements, making them well-suited for models that incorporate significant
    sparsity, whether through pruning or low-rank approximations. As the demand for
    larger models increases, TPUs continue to play a important role in maintaining
    performance while minimizing the energy and computational cost associated with
    dense computations.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: TPUs 的效率得益于它们执行操作时的高吞吐量和低延迟，这得益于它们定制的矩阵乘法单元。这些单元能够通过直接处理非零元素来加速稀疏矩阵操作，这使得它们非常适合包含大量稀疏性的模型，无论是通过剪枝还是低秩近似。随着对更大模型的需求数量增加，TPUs
    继续在保持性能的同时，最小化与密集计算相关的能源和计算成本发挥重要作用。
- en: FPGAs and Sparse Computations
  id: totrans-735
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: FPGAs 和稀疏计算
- en: Field-Programmable Gate Arrays (FPGAs) are another important class of hardware
    accelerators for sparse networks. Unlike GPUs and TPUs, FPGAs are highly customizable,
    offering flexibility in their design to optimize specific computational tasks.
    This makes them particularly suitable for sparse operations that require fine-grained
    control over hardware execution. FPGAs can be programmed to perform sparse matrix-vector
    multiplications and other sparse matrix operations with minimal overhead, delivering
    high performance for models that use unstructured pruning or require custom sparse
    patterns.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 场可编程门阵列 (FPGA) 是稀疏网络的另一类重要硬件加速器。与 GPU 和 TPUs 不同，FPGA 具有高度的定制性，提供了在设计上的灵活性，以优化特定的计算任务。这使得它们特别适合需要精细控制硬件执行的稀疏操作。FPGA
    可以编程以执行稀疏矩阵-向量乘法和其他稀疏矩阵操作，最小化开销，为使用非结构化剪枝或需要自定义稀疏模式的模型提供高性能。
- en: One of the main advantages of FPGAs in sparse networks is their ability to be
    tailored for specific applications, which allows for optimizations that general-purpose
    hardware cannot achieve. For instance, an FPGA can be designed to skip over zero
    elements in a matrix by customizing the data path and memory management, providing
    significant savings in both computation and memory usage. FPGAs also allow for
    low-latency execution, making them well-suited for real-time applications that
    require efficient processing of sparse data streams.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: FPGA 在稀疏网络中的一个主要优势是它们能够针对特定应用进行定制，这允许进行通用硬件无法实现的优化。例如，可以通过定制数据路径和内存管理来设计 FPGA
    跳过矩阵中的零元素，从而在计算和内存使用上提供显著的节省。FPGA 还允许低延迟执行，这使得它们非常适合需要高效处理稀疏数据流的实时应用。
- en: Memory and Energy Optimization
  id: totrans-738
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内存和能源优化
- en: One of the key challenges in sparse networks is managing memory bandwidth, as
    matrix operations often require significant memory access. Sparse networks offer
    a solution by reducing the number of elements that need to be accessed, thus minimizing
    memory traffic. Hardware accelerators detailed in [Chapter 11](ch017.xhtml#sec-ai-acceleration)
    are optimized for these sparse matrices, utilizing specialized memory access patterns
    that skip zero values, reducing the total amount of memory bandwidth used ([Baraglia
    and Konno 2019](ch058.xhtml#ref-Gale2020)).
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏网络中的一个关键挑战是管理内存带宽，因为矩阵操作通常需要大量的内存访问。稀疏网络通过减少需要访问的元素数量来提供解决方案，从而最小化内存流量。第11章[第11章](ch017.xhtml#sec-ai-acceleration)中详细介绍的硬件加速器针对这些稀疏矩阵进行了优化，利用专门的内存访问模式跳过零值，从而减少使用的总内存带宽([Baraglia和Konno
    2019](ch058.xhtml#ref-Gale2020))。
- en: For example, GPUs and TPUs are designed to minimize memory access latency by
    taking advantage of their high memory bandwidth. By accessing only non-zero elements,
    these accelerators ensure that memory is used more efficiently. The memory hierarchies
    in these devices are also optimized for sparse computations, allowing for faster
    data retrieval and reduced power consumption.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，GPU和TPU通过利用其高内存带宽来最小化内存访问延迟。通过仅访问非零元素，这些加速器确保内存使用更加高效。这些设备中的内存层次结构也针对稀疏计算进行了优化，允许更快的数据检索和降低功耗。
- en: The reduction in the number of computations and memory accesses directly translates
    into energy savings[39](#fn39). Sparse operations require fewer arithmetic operations
    and fewer memory fetches, leading to a decrease in the energy consumption required
    for both training and inference. This energy efficiency is particularly important
    for applications that run on edge devices, where power constraints are important,
    as explored in [Chapter 14](ch020.xhtml#sec-ondevice-learning).
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 计算和内存访问次数的减少直接转化为能源节约[39](#fn39)。稀疏操作需要更少的算术运算和更少的内存读取，导致训练和推理所需的能耗降低。这种能源效率对于在边缘设备上运行的应用尤其重要，因为这些设备对电力限制很敏感，如第14章[第14章](ch020.xhtml#sec-ondevice-learning)所述。
- en: 'Future: Hardware and Sparse Networks'
  id: totrans-742
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 未来：硬件和稀疏网络
- en: As hardware continues to evolve, we can expect more innovations tailored specifically
    for sparse networks. Future hardware accelerators may offer deeper integration
    with sparsity-aware training and optimization algorithms, allowing even greater
    reductions in computational and memory costs. Emerging fields like neuromorphic
    computing, inspired by the brain’s structure, may provide new avenues for processing
    sparse networks in energy-efficient ways ([Mike Davies et al. 2021](ch058.xhtml#ref-Davies2021)).
    These advancements promise to further enhance the efficiency and scalability of
    machine learning models, particularly in applications that require real-time processing
    and run on power-constrained devices, connecting to the sustainable AI principles
    in [Chapter 18](ch024.xhtml#sec-sustainable-ai).
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 随着硬件的不断进化，我们可以期待更多专门针对稀疏网络的创新。未来的硬件加速器可能会提供与稀疏感知训练和优化算法的更深入集成，从而实现计算和内存成本的进一步降低。受大脑结构启发的神经形态计算等新兴领域可能为以节能方式处理稀疏网络提供新的途径([Mike
    Davies等人 2021](ch058.xhtml#ref-Davies2021))。这些进步有望进一步提高机器学习模型的效率和可扩展性，尤其是在需要实时处理并在电力受限设备上运行的应用中，这与第18章[第18章](ch024.xhtml#sec-sustainable-ai)中提到的可持续AI原则相连接。
- en: Challenges and Limitations
  id: totrans-744
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 挑战和限制
- en: While exploiting sparsity offers significant advantages in reducing computational
    cost and memory usage, several challenges and limitations must be considered for
    the effective implementation of sparse networks. [Table 10.11](ch016.xhtml#tbl-sparsity-optimization)
    summarizes some of the challenges and limitations associated with sparsity optimizations.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然利用稀疏性在降低计算成本和内存使用方面提供了显著优势，但在有效实施稀疏网络时，必须考虑几个挑战和限制。[表10.11](ch016.xhtml#tbl-sparsity-optimization)总结了与稀疏优化相关的一些挑战和限制。
- en: 'Table 10.11: **Sparsity Optimization Challenges**: Unstructured sparsity, while
    reducing model size, hinders hardware acceleration due to irregular memory access
    patterns, limiting potential computational savings and requiring specialized hardware
    or software to realize efficiency gains. This table summarizes key challenges
    in effectively deploying sparse neural networks.'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.11：**稀疏优化挑战**：非结构化稀疏性，虽然减少了模型大小，但由于不规则的内存访问模式，阻碍了硬件加速，限制了潜在的节省计算量，并需要专门的硬件或软件来实现效率提升。本表总结了有效部署稀疏神经网络的要点挑战。
- en: '| **Challenge** | **Description** | **Impact** |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
  zh: '| **挑战** | **描述** | **影响** |'
- en: '| --- | --- | --- |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Unstructured Sparsity Optimization** | Irregular sparse patterns make it
    difficult to exploit sparsity on hardware. | Limited hardware acceleration and
    reduced computational savings. |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '| **非结构化稀疏优化** | 不规则的稀疏模式使得在硬件上利用稀疏性变得困难。 | 有限的硬件加速和减少的计算节省。 |'
- en: '| **Algorithmic Complexity** | Sophisticated pruning and sparse matrix operations
    require complex algorithms. | High computational overhead and algorithmic complexity
    for large models. |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
  zh: '| **算法复杂性** | 复杂的修剪和稀疏矩阵操作需要复杂的算法。 | 大型模型的高计算开销和算法复杂性。 |'
- en: '| **Hardware Support** | Hardware accelerators are optimized for structured
    sparsity, making unstructured sparsity harder to optimize. | Suboptimal hardware
    utilization and lower performance for unstructured sparsity. |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
  zh: '| **硬件支持** | 硬件加速器针对结构化稀疏性进行了优化，这使得非结构化稀疏性更难优化。 | 非结构化稀疏性的次优硬件利用和较低性能。 |'
- en: '| **Accuracy Trade-off** | Aggressive sparsity may degrade model accuracy if
    not carefully balanced. | Potential loss in performance, requiring careful tuning
    and validation. |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
  zh: '| **准确性与权衡** | 如果没有仔细平衡，激进的稀疏性可能会降低模型准确性。 | 可能的性能损失，需要仔细的调整和验证。 |'
- en: '| **Energy Efficiency** | Overhead from sparse matrix storage and management
    can offset the energy savings from reduced computation. | Power consumption may
    not improve if the overhead surpasses savings from sparse computations. |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '| **能源效率** | 稀疏矩阵存储和管理带来的开销可能会抵消减少计算带来的能源节省。 | 如果开销超过了稀疏计算带来的节省，功耗可能不会改善。 |'
- en: '| **Limited Applicability** | Sparsity may not benefit all models or tasks,
    especially in domains requiring dense representations. | Not all models or hardware
    benefit equally from sparsity. |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
  zh: '| **适用性有限** | 稀疏性可能不会对所有模型或任务有益，尤其是在需要密集表示的领域。 | 并非所有模型或硬件都能从稀疏性中同等受益。 |'
- en: One of the main challenges of sparsity is the optimization of unstructured sparsity.
    In unstructured pruning, individual weights are removed based on their importance,
    leading to an irregular sparse pattern. This irregularity makes it difficult to
    fully exploit the sparsity on hardware, as most hardware accelerators (like GPUs
    and TPUs) are designed to work more efficiently with structured data. Without
    a regular structure, these accelerators may not be able to skip zero elements
    as effectively, which can limit the computational savings.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性的主要挑战之一是非结构化稀疏性的优化。在非结构化修剪中，根据其重要性移除单个权重，导致不规则的稀疏模式。这种不规则性使得在硬件上完全利用稀疏性变得困难，因为大多数硬件加速器（如GPU和TPU）设计时是为了更有效地处理结构化数据。没有规则的结构，这些加速器可能无法有效地跳过零元素，这可能会限制计算节省。
- en: Another challenge is the algorithmic complexity involved in pruning and sparse
    matrix operations. The process of deciding which weights to prune, particularly
    in an unstructured manner, requires sophisticated algorithms that must balance
    model accuracy with computational efficiency. These pruning algorithms can be
    computationally expensive themselves, and applying them across large models can
    result in significant overhead. The optimization of sparse matrices also requires
    specialized techniques that may not always be easy to implement or generalize
    across different architectures.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是修剪和稀疏矩阵操作中涉及的算法复杂性。决定哪些权重需要修剪的过程，尤其是在非结构化的方式下，需要复杂的算法，这些算法必须在模型准确性和计算效率之间取得平衡。这些修剪算法本身可能计算成本高昂，并且在大型模型上应用它们可能导致显著的开销。稀疏矩阵的优化也需要专门的技巧，这些技巧可能并不总是容易实现或在不同架构之间推广。
- en: Hardware support is another important limitation. Although modern GPUs, TPUs,
    and FPGAs have specialized features designed to accelerate sparse operations,
    fully optimizing sparse networks on hardware requires careful alignment between
    the hardware architecture and the sparsity format. While structured sparsity is
    easier to leverage on these accelerators, unstructured sparsity remains a challenge,
    as hardware accelerators may struggle to efficiently handle irregular sparse patterns.
    Even when hardware is optimized for sparse operations, the overhead associated
    with sparse matrix storage formats and the need for specialized memory management
    can still result in suboptimal performance.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件支持是另一个重要的限制。尽管现代GPU、TPU和FPGA具有专门为加速稀疏操作而设计的功能，但要在硬件上完全优化稀疏网络，需要仔细地对硬件架构和稀疏格式进行对齐。虽然结构化稀疏在这些加速器上更容易利用，但非结构化稀疏仍然是一个挑战，因为硬件加速器可能难以高效地处理不规则稀疏模式。即使硬件针对稀疏操作进行了优化，与稀疏矩阵存储格式相关的开销以及需要专门的内存管理，仍然可能导致性能不佳。
- en: There is always a trade-off between sparsity and accuracy. Aggressive pruning
    or low-rank approximation techniques that aggressively reduce the number of parameters
    can lead to accuracy degradation. Finding the right balance between reducing parameters
    and maintaining high model performance is a delicate process that requires extensive
    experimentation. In some cases, introducing too much sparsity can result in a
    model that is too small or too underfit to achieve high performance.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性和精度之间总是存在权衡。激进地剪枝或低秩近似技术，这些技术会大幅度减少参数数量，可能会导致精度下降。在减少参数和保持高模型性能之间找到正确的平衡是一个微妙的过程，需要大量的实验。在某些情况下，引入过多的稀疏性可能会导致模型过小或欠拟合，无法实现高性能。
- en: While sparsity can lead to energy savings, energy efficiency is not always guaranteed.
    Although sparse operations require fewer floating-point operations, the overhead
    of managing sparse data and ensuring that hardware optimally skips over zero values
    can introduce additional power consumption. In edge devices or mobile environments
    with tight power budgets, the benefits of sparsity may be less clear if the overhead
    associated with sparse data structures and hardware utilization outweighs the
    energy savings.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然稀疏性可能导致节能，但能源效率并不总是有保证。尽管稀疏操作需要的浮点运算较少，但管理稀疏数据并确保硬件最优地跳过零值所带来的开销，可能会引入额外的功耗。在边缘设备或功率预算紧张的移动环境中，如果与稀疏数据结构和硬件利用率相关的开销超过了节能效果，稀疏性的好处可能就不那么明显了。
- en: There is a limited applicability of sparsity to certain types of models or tasks.
    Not all models benefit equally from sparsity, especially those where dense representations
    are important for performance. For example, models in domains such as image segmentation
    or some types of reinforcement learning may not show significant gains when sparsity
    is introduced. Sparsity may not be effective for all hardware platforms, particularly
    for older or lower-end devices that lack the computational power or specialized
    features required to take advantage of sparse matrix operations.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性在特定类型的模型或任务上的适用性有限。并非所有模型都能从稀疏性中同等受益，尤其是那些密集表示对性能至关重要的模型。例如，图像分割或某些类型的强化学习中的模型，在引入稀疏性时可能不会显示出显著的增益。稀疏性可能对所有硬件平台都无效，尤其是对于缺乏计算能力或专门功能以利用稀疏矩阵操作的较老或低端设备。
- en: Combined Optimizations
  id: totrans-761
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结合优化
- en: While sparsity in neural networks is a powerful technique for improving computational
    efficiency and reducing memory usage, its full potential is often realized when
    it is used alongside other optimization strategies. These optimizations include
    techniques like pruning, quantization, and efficient model design. Understanding
    how sparsity interacts with these methods is important for effectively combining
    them to achieve optimal performance ([Hoefler, Alistarh, Ben-Nun, Dryden, and
    Ziogas 2021](ch058.xhtml#ref-hoefler2021sparsity)).
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然神经网络中的稀疏性是一种提高计算效率和减少内存使用的强大技术，但它的全部潜力通常是在与其他优化策略结合使用时实现的。这些优化包括剪枝、量化和高效模型设计等技术。了解稀疏性与这些方法如何相互作用对于有效地结合它们以实现最佳性能至关重要
    ([Hoefler, Alistarh, Ben-Nun, Dryden, and Ziogas 2021](ch058.xhtml#ref-hoefler2021sparsity))。
- en: Sparsity and Pruning
  id: totrans-763
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 稀疏性和剪枝
- en: Pruning and sparsity are closely related techniques. When pruning is applied,
    the resulting model may become sparse, but the sparsity pattern, such as whether
    it is structured or unstructured, affects how effectively the model can be optimized
    for hardware. For example, structured pruning (e.g., pruning entire filters or
    layers) typically results in more efficient sparsity, as hardware accelerators
    like GPUs and TPUs are better equipped to handle regular patterns in sparse matrices
    ([Elsen et al. 2020](ch058.xhtml#ref-elsen2020fast)). Unstructured pruning, on
    the other hand, can introduce irregular sparsity patterns, which may not be as
    efficiently processed by hardware, especially when combined with other techniques
    like quantization.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝和稀疏性是密切相关的技术。当应用剪枝时，生成的模型可能变得稀疏，但稀疏模式，例如它是有结构的还是非结构的，会影响模型如何有效地针对硬件进行优化。例如，结构化剪枝（例如，剪枝整个过滤器或层）通常会产生更有效的稀疏性，因为像GPU和TPU这样的硬件加速器更适合处理稀疏矩阵中的常规模式（[Elsen
    et al. 2020](ch058.xhtml#ref-elsen2020fast)）。另一方面，非结构化剪枝可能会引入不规则稀疏模式，这些模式可能无法被硬件有效地处理，尤其是在与其他技术如量化结合使用时。
- en: Pruning-generated sparse patterns must align with underlying hardware architecture
    to achieve computational savings ([Gale, Elsen, and Hooker 2019b](ch058.xhtml#ref-gale2019state)).
    Structured pruning proves particularly effective for hardware optimization.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 由剪枝生成的稀疏模式必须与底层硬件架构相匹配，以实现计算节省（[Gale, Elsen, and Hooker 2019b](ch058.xhtml#ref-gale2019state)）。结构化剪枝在硬件优化方面特别有效。
- en: Sparsity and Quantization
  id: totrans-766
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 稀疏性和量化
- en: Combining sparsity and quantization yields significant reductions in memory
    usage and computation, but presents unique challenges ([Nagel et al. 2021a](ch058.xhtml#ref-nagel2021white)).
    Unstructured sparsity exacerbates low-precision weight processing challenges,
    particularly on hardware lacking efficient support for irregular sparse matrices.
    GPUs and TPUs amplify sparse matrix acceleration when combined with low-precision
    arithmetic, while CPUs struggle with combined overhead ([Yi Zhang et al. 2021](ch058.xhtml#ref-zhang2021learning)).
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 将稀疏性和量化结合起来，可以显著减少内存使用和计算，但也带来了独特的挑战（[Nagel et al. 2021a](ch058.xhtml#ref-nagel2021white)）。非结构化稀疏性加剧了低精度权重处理挑战，尤其是在缺乏对不规则稀疏矩阵有效支持的硬件上。当与低精度算术结合使用时，GPU和TPU可以放大稀疏矩阵加速，而CPU则难以处理组合开销（[Yi
    Zhang et al. 2021](ch058.xhtml#ref-zhang2021learning)）。
- en: Sparsity and Model Design
  id: totrans-768
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 稀疏性和模型设计
- en: Efficient model design creates inherently efficient architectures through techniques
    like depthwise separable convolutions, low-rank approximation, and dynamic computation.
    Sparsity amplifies these benefits by further reducing memory and computation requirements
    ([Dettmers and Zettlemoyer 2019](ch058.xhtml#ref-dettmers2019sparse)). However,
    efficient sparse models require hardware support for sparse operations to avoid
    suboptimal performance. Hardware alignment ensures both computational cost and
    memory usage minimization ([Elsen et al. 2020](ch058.xhtml#ref-elsen2020fast)).
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的模型设计通过深度可分离卷积、低秩近似和动态计算等技术，天生地创造出高效的架构。稀疏性通过进一步减少内存和计算需求来放大这些好处（[Dettmers
    and Zettlemoyer 2019](ch058.xhtml#ref-dettmers2019sparse)）。然而，高效的稀疏模型需要硬件对稀疏操作的支持，以避免次优性能。硬件对齐确保了计算成本和内存使用的最小化（[Elsen
    et al. 2020](ch058.xhtml#ref-elsen2020fast)）。
- en: Sparsity and Optimization Challenges
  id: totrans-770
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 稀疏性和优化挑战
- en: Coordinating sparsity with pruning, quantization, and efficient design involves
    managing accuracy trade-offs ([Labarge, n.d.](ch058.xhtml#ref-blalock2020state)).
    Hardware accelerators like GPUs and TPUs optimize for structured sparsity but
    struggle with unstructured patterns or sparsity-quantization combinations. Optimal
    performance requires selecting appropriate technique combinations aligned with
    hardware capabilities ([Gale, Elsen, and Hooker 2019b](ch058.xhtml#ref-gale2019state)),
    carefully balancing model accuracy, computational cost, memory usage, and hardware
    efficiency.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 通过协调稀疏性、剪枝、量化和高效设计，涉及管理精度权衡（[Labarge, n.d.](ch058.xhtml#ref-blalock2020state)）。硬件加速器如GPU和TPU优化结构化稀疏性，但在处理非结构化模式或稀疏性-量化组合时遇到困难。最佳性能需要选择与硬件能力相匹配的技术组合（[Gale,
    Elsen, and Hooker 2019b](ch058.xhtml#ref-gale2019state)），仔细平衡模型精度、计算成本、内存使用和硬件效率。
- en: Implementation Strategy and Evaluation
  id: totrans-772
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施策略和评估
- en: We now examine systematic application strategies. The individual techniques
    we have studied rarely succeed in isolation; production systems typically employ
    coordinated optimization strategies that balance multiple constraints simultaneously.
    Effective deployment requires structured approaches for profiling systems, measuring
    optimization impact, and combining techniques to achieve deployment goals.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在来探讨系统化应用策略。我们研究过的个别技术很少能单独成功；生产系统通常采用协调的优化策略，同时平衡多个约束。有效的部署需要结构化的方法来分析系统、衡量优化影响以及结合技术以实现部署目标。
- en: 'This section provides methodological guidance for moving from theoretical understanding
    to practical implementation, addressing three critical questions: Where should
    optimization efforts focus? How do we measure whether optimizations achieve their
    intended goals? How do we combine multiple techniques without introducing conflicts
    or diminishing returns?'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了从理论理解到实际实施的方法论指导，针对三个关键问题：优化努力应该集中在哪里？我们如何衡量优化是否达到了预期的目标？我们如何结合多种技术而不引入冲突或减少回报？
- en: Profiling and Opportunity Analysis
  id: totrans-775
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能分析及机会分析
- en: The foundation of optimization lies in thorough profiling to identify where
    computational resources are being consumed and which components offer the greatest
    optimization potential. However, a critical first step is determining whether
    model optimization will actually improve system performance, as model computation
    often represents only a fraction of total system overhead in production environments.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 优化的基础在于彻底的性能分析，以确定计算资源消耗在哪里以及哪些组件具有最大的优化潜力。然而，一个关键的第一步是确定模型优化是否真的能提高系统性能，因为在生产环境中，模型计算通常只代表总系统开销的一小部分。
- en: Modern machine learning models exhibit heterogeneous resource consumption patterns,
    where specific layers, operations, or data paths contribute disproportionately
    to memory usage, computational cost, or latency. Understanding these patterns
    is important for prioritizing optimization efforts and achieving maximum impact
    with minimal accuracy degradation.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习模型表现出异构的资源消耗模式，其中特定的层、操作或数据路径不成比例地贡献于内存使用、计算成本或延迟。理解这些模式对于优先考虑优化努力和以最小的精度损失实现最大影响至关重要。
- en: Effective profiling begins with establishing baseline measurements across all
    relevant performance dimensions. Memory profiling reveals both static memory consumption
    (model parameters and buffers) and dynamic memory allocation patterns during training
    and inference. Computational profiling identifies bottleneck operations, typically
    measured in FLOPS and actual wall-clock execution time. Energy profiling becomes
    important for battery-powered and edge deployment scenarios, where power consumption
    directly impacts operational feasibility. Latency profiling measures end-to-end
    response times and identifies which operations contribute most to inference delay.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的性能分析始于在所有相关性能维度上建立基线测量。内存分析揭示了静态内存消耗（模型参数和缓冲区）以及训练和推理过程中的动态内存分配模式。计算分析确定了瓶颈操作，通常以FLOPS和实际墙钟执行时间来衡量。对于电池供电和边缘部署场景，能耗分析变得尤为重要，因为功耗直接影响到操作的可行性。延迟分析测量端到端响应时间，并确定哪些操作对推理延迟贡献最大。
- en: Consider profiling a Vision Transformer (ViT) for edge deployment. Using PyTorch
    Profiler reveals attention layers consuming 65% of total FLOPs (highly amenable
    to structured pruning), layer normalization consuming 8% of latency despite only
    2% of FLOPs (memory-bound operation), and the final classification head consuming
    1% of computation but 15% of parameter memory. This profile suggests applying
    magnitude-based pruning to attention layers as priority one (high FLOP reduction
    potential), quantizing the classification head to INT8 as priority two (large
    memory savings, minimal accuracy impact), and fusing layer normalization operations
    as priority three (reduces memory bandwidth bottleneck).
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑对用于边缘部署的视觉Transformer（ViT）进行性能分析。使用PyTorch Profiler可以发现，注意力层消耗了65%的总FLOPS（非常适合结构化剪枝），层归一化消耗了8%的延迟，尽管只消耗了2%的FLOPS（内存密集型操作），而最终的分类头消耗了1%的计算但15%的参数内存。此分析表明，将基于幅度的剪枝应用于注意力层作为首要任务（具有高FLOP减少潜力），将分类头量化为INT8作为第二优先级（大量节省内存，最小化精度影响），以及将层归一化操作融合作为第三优先级（减少内存带宽瓶颈）。
- en: Extending beyond these baseline measurements, modern optimization requires understanding
    model sensitivity to different types of modifications. Not all parameters contribute
    equally to model accuracy, and structured sensitivity analysis helps identify
    which components can be optimized aggressively versus those that require careful
    preservation. Layer-wise sensitivity analysis reveals which network components
    are most important for maintaining accuracy, guiding decisions about where to
    apply aggressive pruning or quantization versus where to use conservative approaches.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 超出这些基线测量范围，现代优化需要理解模型对不同类型修改的敏感性。并非所有参数对模型准确性的贡献都相同，结构化敏感性分析有助于识别哪些组件可以大胆优化，而哪些组件则需要谨慎保留。层级敏感性分析揭示了哪些网络组件对保持准确性最为重要，指导决策在何处应用大胆的剪枝或量化，以及在何处使用保守的方法。
- en: Measuring Optimization Effectiveness
  id: totrans-781
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量优化有效性
- en: Optimization requires rigorous measurement frameworks that go beyond simple
    accuracy metrics to capture the full impact of optimization decisions. Effective
    measurement considers multiple objectives simultaneously, including accuracy preservation,
    computational efficiency gains, memory reduction, latency improvement, and energy
    savings. The challenge lies in balancing these often-competing objectives while
    maintaining structured decision-making processes.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 优化需要严格的测量框架，它不仅超越了简单的准确率指标，还捕捉了优化决策的全面影响。有效的测量同时考虑多个目标，包括准确率保持、计算效率提升、内存减少、延迟改进和节能。挑战在于在保持结构化决策过程的同时，平衡这些经常相互竞争的目标。
- en: The measurement framework should establish clear baselines before applying any
    optimizations, capturing thorough performance profiles across all relevant metrics.
    Accuracy baselines include not only top-line metrics like classification accuracy
    but also more nuanced measures such as calibration, fairness across demographic
    groups, and robustness to input variations. Efficiency baselines capture computational
    cost (FLOPS, memory bandwidth), actual execution time across different hardware
    platforms, peak memory consumption during training and inference, and energy consumption
    profiles.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 测量框架在应用任何优化之前应建立清晰的基线，全面捕捉所有相关指标的性能概况。准确性基线不仅包括分类准确率等主要指标，还包括校准、不同人口群体间的公平性以及对抗输入变化的鲁棒性等更细致的度量。效率基线包括计算成本（FLOPS、内存带宽）、不同硬件平台上的实际执行时间、训练和推理期间的峰值内存消耗以及能耗概况。
- en: When quantizing ResNet-50 from FP32 to INT8, baseline metrics show Top-1 accuracy
    of 76.1%, inference latency on V100 of 4.2ms, model size of 98MB, and energy per
    inference of 0.31J. Post-quantization metrics reveal Top-1 accuracy of 75.8% (0.3%
    degradation), inference latency of 1.3ms (3.2x speedup), model size of 25MB (3.9x
    reduction), and energy per inference of 0.08J (3.9x improvement). Additional analysis
    shows per-class accuracy degradation ranging from 0.1% to 1.2% with highest impact
    on fine-grained categories, calibration error increasing from 2.1% to 3.4%, and
    INT8 quantization providing 3.2x speedup on GPU but only 1.8x on CPU, demonstrating
    hardware-dependent gains.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 当将ResNet-50从FP32量化到INT8时，基线指标显示Top-1准确率为76.1%，V100上的推理延迟为4.2ms，模型大小为98MB，每次推理能耗为0.31J。量化后的指标揭示Top-1准确率为75.8%（下降0.3%），推理延迟为1.3ms（加速3.2倍），模型大小为25MB（减少3.9倍），每次推理能耗为0.08J（提高3.9倍）。额外的分析显示每类准确率下降从0.1%到1.2%，对细粒度类别影响最大，校准误差从2.1%增加到3.4%，INT8量化在GPU上提供3.2倍的速度提升，但在CPU上仅提供1.8倍，这表明硬件依赖的收益。
- en: With these comprehensive baselines in place, the measurement framework must
    track optimization impact systematically. Rather than evaluating techniques in
    isolation, applying our three-dimensional framework requires understanding how
    different approaches interact when combined. Sequential application can lead to
    compounding benefits or unexpected interactions that diminish overall effectiveness.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些全面的基线建立之后，测量框架必须系统地跟踪优化影响。而不是单独评估技术，应用我们的三维框架需要理解不同方法结合时如何相互作用。顺序应用可能导致累积效益或意外的相互作用，从而降低整体有效性。
- en: Multi-Technique Integration Strategies
  id: totrans-786
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多技术集成策略
- en: The most significant optimization gains emerge from combining multiple techniques
    across our three-dimensional framework. Model representation techniques (pruning)
    reduce parameter count, numerical precision techniques (quantization) reduce computational
    cost per operation, and architectural efficiency techniques (operator fusion,
    dynamic computation) reduce execution overhead. These techniques operate at different
    optimization dimensions, providing multiplicative benefits when sequenced appropriately.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 最显著的优化收益来自在我们三维框架内结合多种技术。模型表示技术（剪枝）减少参数数量，数值精度技术（量化）减少每操作的计算成本，而架构效率技术（算子融合、动态计算）减少执行开销。这些技术在不同的优化维度上操作，当适当排序时提供乘法效益。
- en: Sequencing critically impacts results. Consider deploying BERT-Base on mobile
    devices through three stages. Stage one applies structured pruning, removing 30%
    of attention heads and 40% of intermediate FFN dimensions, resulting in 75% parameter
    reduction with accuracy dropping from 76.2% to 75.1%. Stage two uses knowledge
    distillation to recover accuracy to 75.9%. Stage three applies quantization-aware
    training with INT8 quantization, achieving 4x additional memory reduction with
    final accuracy of 75.6%. The combined impact shows 16x memory reduction (440MB
    to 28MB), 12x inference speedup on mobile CPU, and 0.6% final accuracy loss versus
    2.1% if quantization had been applied before pruning.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 序列化对结果有重大影响。考虑通过三个阶段在移动设备上部署BERT-Base。第一阶段应用结构化剪枝，移除30%的注意力头和40%的中间FFN维度，参数减少75%，准确率从76.2%下降到75.1%。第二阶段使用知识蒸馏将准确率恢复到75.9%。第三阶段应用量化感知训练，使用INT8量化，实现额外的4倍内存减少，最终准确率为75.6%。综合影响显示内存减少16倍（从440MB到28MB），在移动CPU上推理速度提高12倍，与在剪枝前应用量化相比，最终准确率损失为0.6%，而如果量化在剪枝前应用，损失将达2.1%。
- en: 'This example illustrates why sequencing matters: pruning first concentrates
    important weights into smaller ranges, making subsequent quantization more effective.
    Applying quantization before pruning reduces numerical precision available for
    importance-based pruning decisions, degrading final accuracy. Effective combination
    requires understanding these dependencies and developing application sequences
    that maximize cumulative benefits. Modern automated approaches, explored in the
    following AutoML section, leverage our dimensional framework to discover effective
    technique combinations systematically.'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明了为什么排序很重要：剪枝首先将重要权重集中到更小的范围内，使后续的量化更有效。在剪枝前应用量化会减少基于重要性的剪枝决策可用的数值精度，降低最终准确度。有效的组合需要理解这些依赖关系，并开发出最大化累积效益的应用序列。在接下来的AutoML部分中，现代自动化方法利用我们的维度框架系统地发现有效的技术组合。
- en: AutoML and Automated Optimization Strategies
  id: totrans-790
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动机器学习与自动化优化策略
- en: As machine learning models grow in complexity, optimizing them for real-world
    deployment requires balancing multiple factors, including accuracy, efficiency,
    and hardware constraints. We have explored various optimization techniques, including
    pruning, quantization, and neural architecture search, each of which targets specific
    aspects of model efficiency. However, applying these optimizations effectively
    often requires extensive manual effort, domain expertise, and iterative experimentation.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型复杂性的增加，为了在现实世界中部署它们，需要平衡多个因素，包括准确度、效率和硬件限制。我们已经探索了各种优化技术，包括剪枝、量化和神经架构搜索，每种技术都针对模型效率的特定方面。然而，有效地应用这些优化通常需要大量的手动工作、领域专业知识和迭代实验。
- en: Automated Machine Learning (AutoML) aims to streamline this process by automating
    the search for optimal model configurations, building on the training methodologies
    from [Chapter 8](ch014.xhtml#sec-ai-training). AutoML frameworks leverage machine
    learning algorithms to optimize architectures, hyperparameters, model compression
    techniques, and other important parameters, reducing the need for human intervention
    ([F. Hutter, Kotthoff, and Vanschoren 2019](ch058.xhtml#ref-Hutter2019)). By systematically
    exploring the vast design space of possible models, AutoML can improve efficiency
    while maintaining competitive accuracy, often discovering novel solutions that
    may be overlooked through manual tuning ([Zoph and Le 2017b](ch058.xhtml#ref-Zoph2017)).
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 自动机器学习（AutoML）旨在通过自动化搜索最优模型配置的过程来简化这一过程，基于[第8章](ch014.xhtml#sec-ai-training)中的训练方法。AutoML框架利用机器学习算法来优化架构、超参数、模型压缩技术和其他重要参数，减少了对人类干预的需求（[F.
    Hutter, Kotthoff, and Vanschoren 2019](ch058.xhtml#ref-Hutter2019)）。通过系统地探索可能的模型设计空间，AutoML可以在保持竞争性准确度的同时提高效率，通常可以发现通过手动调整可能被忽视的新颖解决方案（[Zoph和Le
    2017b](ch058.xhtml#ref-Zoph2017)）。
- en: AutoML does not replace the need for human expertise but rather enhances it
    by providing a structured and scalable approach to model optimization. As illustrated
    in [Figure 10.30](ch016.xhtml#fig-automl-comparison), the key difference between
    traditional workflows and AutoML is that preprocessing, training and evaluation
    are automated in the latter. Instead of manually adjusting pruning thresholds,
    quantization strategies, or architecture designs, practitioners can define high-level
    objectives, including latency constraints, memory limits, and accuracy targets,
    and allow AutoML systems to explore configurations that best satisfy these constraints
    ([Feurer et al. 2019](ch058.xhtml#ref-Feurer2015)), enabling the robust deployment
    strategies detailed in [Chapter 16](ch022.xhtml#sec-robust-ai).
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 自动机器学习（AutoML）并不取代人类专业知识的需求，而是通过提供一种结构化和可扩展的模型优化方法来增强它。如图[图10.30](ch016.xhtml#fig-automl-comparison)所示，传统工作流程与AutoML之间的关键区别在于，后者自动化了预处理、训练和评估。而不是手动调整剪枝阈值、量化策略或架构设计，从业者可以定义高级目标，包括延迟约束、内存限制和准确度目标，并允许AutoML系统探索最能满足这些约束的配置（[Feurer等人2019](ch058.xhtml#ref-Feurer2015)），从而实现[第16章](ch022.xhtml#sec-robust-ai)中详细描述的稳健部署策略。
- en: '![](../media/file176.svg)'
  id: totrans-794
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file176.svg)'
- en: 'Figure 10.30: **AutoML Workflow**: Automated machine learning (automl) streamlines
    model development by structurally automating data preprocessing, model selection,
    and hyperparameter tuning, contrasting with traditional workflows requiring extensive
    manual effort for each stage. This automation enables practitioners to define
    high-level objectives and constraints, allowing automl systems to efficiently
    explore a vast design space and identify optimal model configurations.'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.30：**AutoML工作流程**：自动机器学习（automl）通过结构化自动化数据预处理、模型选择和超参数调整来简化模型开发，与传统工作流程形成对比，后者在每个阶段都需要大量的手动工作。这种自动化使从业者能够定义高级目标和约束，允许automl系统高效地探索广阔的设计空间并识别最优模型配置。
- en: This section explores the core aspects of AutoML, starting with the key dimensions
    of optimization, followed by the methodologies used in AutoML systems, and concluding
    with challenges and limitations. This examination reveals how AutoML serves as
    an integrative framework that unifies many of the optimization strategies discussed
    earlier.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了AutoML的核心方面，从优化的关键维度开始，然后是AutoML系统中使用的方法，最后是挑战和限制。这种审查揭示了AutoML如何作为一个综合框架，统一了之前讨论的许多优化策略。
- en: AutoML Optimizations
  id: totrans-797
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AutoML优化
- en: AutoML is designed to optimize multiple aspects of a machine learning model,
    ensuring efficiency, accuracy, and deployability. Unlike traditional approaches
    that focus on individual techniques, such as quantization for reducing numerical
    precision or pruning for compressing models, AutoML takes a holistic approach
    by jointly considering these factors. This enables a more thorough search for
    optimal model configurations, balancing performance with real-world constraints
    ([Yihui He et al. 2018](ch058.xhtml#ref-He2018)).
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML旨在优化机器学习模型的多个方面，确保效率、准确性和可部署性。与专注于单个技术（如量化以降低数值精度或剪枝以压缩模型）的传统方法不同，AutoML通过综合考虑这些因素采取了一种整体方法。这使搜索最优模型配置更加彻底，在性能与实际约束之间取得平衡（[Yihui
    He等人2018](ch058.xhtml#ref-He2018)）。
- en: One of the primary optimization targets of AutoML is neural network architecture
    search. Designing an efficient model architecture is a complex process that requires
    balancing layer configurations, connectivity patterns, and computational costs.
    NAS automates this by structuredally exploring different network structures, evaluating
    their efficiency, and selecting the most optimal design ([Elsken, Metzen, and
    Hutter 2019b](ch058.xhtml#ref-Elsken2019)). This process has led to the discovery
    of architectures such as MobileNetV3 and EfficientNet, which outperform manually
    designed models on key efficiency metrics ([Tan and Le 2019b](ch058.xhtml#ref-Tan2019)).
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML的主要优化目标之一是神经网络架构搜索。设计一个高效的模型架构是一个复杂的过程，需要平衡层配置、连接模式和计算成本。NAS通过结构性地探索不同的网络结构、评估其效率并选择最优设计来实现自动化
    ([Elsken, Metzen, and Hutter 2019b](ch058.xhtml#ref-Elsken2019))。这个过程导致了诸如MobileNetV3和EfficientNet等架构的发现，这些架构在关键效率指标上优于手动设计的模型
    ([Tan and Le 2019b](ch058.xhtml#ref-Tan2019))。
- en: Beyond architecture design, AutoML also focuses on hyperparameter optimization[40](#fn40),
    which plays a important role in determining a model’s performance. Parameters
    such as learning rate, batch size[41](#fn41), weight decay, and activation functions
    must be carefully tuned for stability and efficiency.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 除了架构设计之外，AutoML还专注于超参数优化[40](#fn40)，这在确定模型性能方面起着重要作用。学习率、批量大小[41](#fn41)、权重衰减和激活函数等参数必须仔细调整以确保稳定性和效率。
- en: Instead of relying on trial and error, AutoML frameworks employ structured search
    strategies, including Bayesian optimization[42](#fn42), evolutionary algorithms,
    and adaptive heuristics, to efficiently identify the best hyperparameter settings
    for a given model and dataset ([Bardenet et al. 2015](ch058.xhtml#ref-Bergstra2011)).
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 与依赖试错不同，AutoML框架采用结构化搜索策略，包括贝叶斯优化[42](#fn42)、进化算法和自适应启发式方法，以有效地识别给定模型和数据集的最佳超参数设置
    ([Bardenet等人 2015](ch058.xhtml#ref-Bergstra2011))。
- en: Another important aspect of AutoML is model compression. Techniques such as
    pruning and quantization help reduce the memory footprint and computational requirements
    of a model, making it more suitable for deployment on resource-constrained hardware.
    AutoML frameworks automate the selection of pruning thresholds, sparsity patterns,
    and quantization levels, optimizing models for both speed and energy efficiency
    ([Jiaxiang Wu et al. 2016](ch058.xhtml#ref-Wu2016)). This is particularly important
    for edge AI applications, where models need to operate with minimal latency and
    power consumption ([Chowdhery et al. 2021](ch058.xhtml#ref-Chowdhery2021)).
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML的另一个重要方面是模型压缩。例如，剪枝和量化等技术有助于减少模型的内存占用和计算需求，使其更适合在资源受限的硬件上部署。AutoML框架自动化选择剪枝阈值、稀疏模式和量化级别，优化模型以实现速度和能效
    ([Jiaxiang Wu等人 2016](ch058.xhtml#ref-Wu2016))。这对于边缘人工智能应用尤为重要，在这些应用中，模型需要以最小的延迟和功耗运行
    ([Chowdhery等人 2021](ch058.xhtml#ref-Chowdhery2021))。
- en: Finally, AutoML considers deployment-aware optimization, ensuring that the final
    model is suited for real-world execution. Different hardware platforms impose
    varying constraints on model execution, such as memory bandwidth limitations,
    computational throughput, and energy efficiency requirements. AutoML frameworks
    incorporate hardware-aware optimization techniques, tailoring models to specific
    devices by adjusting computational workloads, memory access patterns, and execution
    strategies ([H. Cai, Gan, and Han 2020](ch058.xhtml#ref-Cai2020)).
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，AutoML考虑了部署感知优化，确保最终模型适合实际执行。不同的硬件平台对模型执行施加不同的约束，例如内存带宽限制、计算吞吐量和能效要求。AutoML框架结合了硬件感知优化技术，通过调整计算工作量、内存访问模式和执行策略来针对特定设备定制模型
    ([H. Cai, Gan, and Han 2020](ch058.xhtml#ref-Cai2020))。
- en: Optimization across these dimensions enables AutoML to provide a unified framework
    for enhancing machine learning models, streamlining the process to achieve efficiency
    without sacrificing accuracy. This holistic approach ensures that models are not
    only theoretically optimal but also practical for real-world deployment across
    diverse applications and hardware platforms.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些维度上的优化使得AutoML能够为增强机器学习模型提供一个统一的框架，简化流程以实现效率，同时不牺牲准确性。这种整体方法确保模型不仅在理论上最优，而且在实际部署到各种应用和硬件平台上也是实用的。
- en: Optimization Strategies
  id: totrans-805
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化策略
- en: AutoML systems systematically explore different configurations to identify optimal
    combinations of architectures, hyperparameters, and compression strategies. Unlike
    manual tuning requiring extensive domain expertise, AutoML leverages algorithmic
    search methods to navigate the vast design space while balancing accuracy, efficiency,
    and deployment constraints.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML系统系统地探索不同的配置，以识别架构、超参数和压缩策略的最佳组合。与需要广泛领域专业知识的手动调整不同，AutoML利用算法搜索方法在平衡精度、效率和部署约束的同时，在广阔的设计空间中导航。
- en: NAS forms the foundation of AutoML by automating architecture design through
    reinforcement learning, evolutionary algorithms, and gradient-based optimization
    ([Zoph and Le 2017b](ch058.xhtml#ref-Zoph2017)). By systematically evaluating
    candidate architectures, NAS identifies structures that outperform manually designed
    models ([Real et al. 2019](ch058.xhtml#ref-Real2019)). Hyperparameter optimization
    (HPO) complements this by fine-tuning training parameters—learning rate, batch
    size, weight decay—using Bayesian optimization and adaptive heuristics that converge
    faster than grid search ([Feurer et al. 2019](ch058.xhtml#ref-Feurer2015)).
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: NAS通过强化学习、进化算法和基于梯度的优化自动化架构设计，构成了AutoML的基础([Zoph和Le 2017b](ch058.xhtml#ref-Zoph2017))。通过系统地评估候选架构，NAS识别出优于人工设计的模型的结构([Real等人
    2019](ch058.xhtml#ref-Real2019))。超参数优化(HPO)通过使用贝叶斯优化和自适应启发式方法来微调训练参数——学习率、批量大小、权重衰减——以实现比网格搜索更快的收敛速度([Feurer等人
    2019](ch058.xhtml#ref-Feurer2015))来补充这一点。
- en: Model compression optimization automatically selects pruning and quantization
    strategies based on deployment requirements, evaluating trade-offs between model
    size, latency, and accuracy. This enables efficient deployment on resource-constrained
    devices ([Chapter 14](ch020.xhtml#sec-ondevice-learning)) without manual tuning.
    Data processing strategies further enhance performance through automated feature
    selection, adaptive augmentation policies, and dataset balancing that improve
    robustness ([Chapter 16](ch022.xhtml#sec-robust-ai)) without computational overhead.
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩优化根据部署需求自动选择剪枝和量化策略，评估模型大小、延迟和精度之间的权衡。这使得在资源受限的设备上高效部署成为可能([第14章](ch020.xhtml#sec-ondevice-learning))，无需手动调整。数据处理策略通过自动特征选择、自适应增强策略和数据集平衡来进一步提高性能，这些方法在不增加计算开销的情况下提高了鲁棒性([第16章](ch022.xhtml#sec-robust-ai))。
- en: Meta-learning approaches represent recent advances where knowledge from previous
    optimization tasks accelerates searches for new models ([Vanschoren 2018](ch058.xhtml#ref-Vanschoren2019)).
    By learning from prior experiments, AutoML systems intelligently explore the optimization
    space, reducing training and evaluation costs while enabling faster adaptation
    to new tasks and datasets.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习方法是最近的一项进展，其中先前优化任务的知识加速了新模型搜索的过程([Vanschoren 2018](ch058.xhtml#ref-Vanschoren2019))。通过从先前实验中学习，AutoML系统智能地探索优化空间，减少训练和评估成本，同时使系统能够更快地适应新任务和数据集。
- en: Finally, many modern AutoML frameworks offer end-to-end automation, integrating
    architecture search, hyperparameter tuning, and model compression into a single
    pipeline. Platforms such as Google AutoML, Amazon SageMaker Autopilot, and Microsoft
    Azure AutoML provide fully automated workflows that streamline the entire model
    optimization process ([L. Li et al. 2017](ch058.xhtml#ref-Li2021)).
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，许多现代AutoML框架提供端到端自动化，将架构搜索、超参数调整和模型压缩集成到一个单一的流程中。例如，Google AutoML、Amazon
    SageMaker Autopilot和Microsoft Azure AutoML等平台提供完全自动化的工作流程，简化了整个模型优化过程([L. Li等人
    2017](ch058.xhtml#ref-Li2021))。
- en: The integration of these strategies enables AutoML systems to provide a scalable
    and efficient approach to model optimization, reducing the reliance on manual
    experimentation. This automation not only accelerates model development but also
    enables the discovery of novel architectures and configurations that might otherwise
    be overlooked, supporting the structured evaluation methods in [Chapter 12](ch018.xhtml#sec-benchmarking-ai).
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 这些策略的集成使得AutoML系统能够提供一种可扩展且高效的模型优化方法，减少了对手动实验的依赖。这种自动化不仅加速了模型开发，还使得发现可能被忽视的新型架构和配置成为可能，支持第12章中提到的结构化评估方法([第12章](ch018.xhtml#sec-benchmarking-ai))。
- en: AutoML Optimization Challenges
  id: totrans-812
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AutoML优化挑战
- en: While AutoML offers a powerful framework for optimizing machine learning models,
    it also introduces several challenges and trade-offs that must be carefully considered.
    Despite its ability to automate model design and hyperparameter tuning, AutoML
    is not a one-size-fits-all solution. The effectiveness of AutoML depends on computational
    resources, dataset characteristics, and the specific constraints of a given application.
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管AutoML为优化机器学习模型提供了一个强大的框架，但它也引入了几个挑战和权衡，必须仔细考虑。尽管AutoML能够自动化模型设计和超参数调整，但它并不是万能的解决方案。AutoML的有效性取决于计算资源、数据集特征以及特定应用的特定约束。
- en: One of the most significant challenges in AutoML is computational cost. The
    process of searching for optimal architectures, hyperparameters, and compression
    strategies requires evaluating numerous candidate models, each of which must be
    trained and validated. Methods like NAS can be particularly expensive, often requiring
    thousands of GPU hours to explore a large search space. While techniques such
    as early stopping, weight sharing, and surrogate models help reduce search costs,
    the computational overhead remains a major limitation, especially for organizations
    with limited access to high-performance computing resources.
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML中最显著的挑战之一是计算成本。搜索最优架构、超参数和压缩策略的过程需要评估大量候选模型，每个模型都必须进行训练和验证。像NAS这样的方法可能特别昂贵，通常需要成千上万的GPU小时来探索大型的搜索空间。虽然像早期停止、权重共享和代理模型这样的技术有助于降低搜索成本，但计算开销仍然是一个主要的限制，特别是对于有限访问高性能计算资源的组织。
- en: Another challenge is bias in search strategies, which can influence the final
    model selection. The optimization process in AutoML is guided by heuristics and
    predefined objectives, which may lead to biased results depending on how the search
    space is defined. If the search algorithm prioritizes certain architectures or
    hyperparameters over others, it may fail to discover alternative configurations
    that could be more effective for specific tasks. Biases in training data can propagate
    through the AutoML process, reinforcing unwanted patterns in the final model.
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是搜索策略中的偏差，这可能会影响最终的模型选择。AutoML中的优化过程由启发式方法和预定义的目标指导，这可能导致根据搜索空间定义的方式产生偏差的结果。如果搜索算法优先考虑某些架构或超参数而不是其他，它可能无法发现对特定任务可能更有效的替代配置。训练数据中的偏差可能会通过AutoML过程传播，加强最终模型中的不希望出现的模式。
- en: Generalization and transferability present additional concerns. AutoML-generated
    models are optimized for specific datasets and deployment conditions, but their
    performance may degrade when applied to new tasks or environments. Unlike manually
    designed models, where human intuition can guide the selection of architectures
    that generalize well, AutoML relies on empirical evaluation within a constrained
    search space. This limitation raises questions about the robustness of AutoML-optimized
    models when faced with real-world variability.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化和迁移性提出了额外的担忧。AutoML生成的模型针对特定数据集和部署条件进行了优化，但将它们应用于新任务或环境时，其性能可能会下降。与手动设计的模型不同，手动设计的模型中人类的直觉可以指导选择泛化能力良好的架构，而AutoML依赖于在约束搜索空间内的经验评估。这种限制引发了关于AutoML优化模型在面对现实世界变化时的鲁棒性的疑问。
- en: Interpretability is another key consideration. Many AutoML-generated architectures
    and configurations are optimized for efficiency but lack transparency in their
    design choices. Understanding why a particular AutoML-discovered model performs
    well can be challenging, making it difficult for practitioners to debug issues
    or adapt models for specific needs. The black-box nature of some AutoML techniques
    limits human insight into the underlying optimization process.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性是另一个关键考虑因素。许多AutoML生成的架构和配置在效率上进行了优化，但在设计选择上缺乏透明度。理解为什么特定的AutoML发现模型表现良好可能具有挑战性，这使得从业者难以调试问题或针对特定需求调整模型。某些AutoML技术的黑盒性质限制了人类对底层优化过程的洞察。
- en: Beyond technical challenges, there is also a trade-off between automation and
    control. While AutoML reduces the need for manual intervention, it also abstracts
    away many decision-making processes that experts might otherwise fine-tune for
    specific applications. In some cases, domain knowledge is important for guiding
    model optimization, and fully automated systems may not always account for subtle
    but important constraints imposed by the problem domain.
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 除了技术挑战之外，自动化与控制之间也存在权衡。虽然AutoML减少了手动干预的需求，但它也抽象了许多专家可能对特定应用进行微调的决策过程。在某些情况下，领域知识对于指导模型优化很重要，而完全自动化的系统可能并不总是考虑到由问题域施加的微妙但重要的约束。
- en: Despite these challenges, AutoML continues to evolve, with ongoing research
    focused on reducing computational costs, improving generalization, and enhancing
    interpretability. As these improvements emerge, AutoML is expected to play an
    increasingly prominent role in the development of optimized machine learning models,
    making AI systems more accessible and efficient for a wide range of applications.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，AutoML仍在不断发展，持续的研究集中在降低计算成本、提高泛化能力和增强可解释性。随着这些改进的出现，AutoML预计将在优化机器学习模型的发展中扮演越来越突出的角色，使AI系统对广泛的适用性更加易于访问和高效。
- en: The optimization techniques explored—spanning model representation, numerical
    precision, architectural efficiency, and automated selection—provide a comprehensive
    toolkit for efficient machine learning systems. However, practical implementation
    requires robust software infrastructure bridging the gap between optimization
    research and deployment through easy-to-use APIs, efficient implementations, and
    seamless workflow integration.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 探索的优化技术——包括模型表示、数值精度、架构效率和自动化选择——为高效机器学习系统提供了一个全面的工具包。然而，实际实施需要稳健的软件基础设施，通过易于使用的API、高效的实现和无缝的工作流程集成，弥合优化研究与部署之间的差距。
- en: Implementation Tools and Software Frameworks
  id: totrans-821
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现工具和软件框架
- en: The theoretical understanding of model optimization techniques like pruning,
    quantization, and efficient numerics is important, but their practical implementation
    relies heavily on robust software support. Without extensive framework development
    and tooling, these optimization methods would remain largely inaccessible to practitioners.
    Implementing quantization would require manual modification of model definitions
    and careful insertion of quantization operations throughout the network. Pruning
    would involve direct manipulation of weight tensors, tasks that become prohibitively
    complex as models scale.
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型优化技术（如剪枝、量化和高效数值计算）的理论理解很重要，但它们的实际实施高度依赖于稳健的软件支持。没有广泛的框架开发和工具，这些优化方法将主要对实践者不可及。实现量化需要手动修改模型定义并在整个网络中仔细插入量化操作。剪枝将涉及直接操作权重张量，随着模型规模的扩大，这些任务变得极其复杂。
- en: 'Modern machine learning frameworks provide high-level APIs and automated workflows
    that abstract away implementation complexity, making sophisticated optimization
    techniques accessible to practitioners. Frameworks address key challenges: providing
    pre-built modules for common optimization techniques, assisting with hyperparameter
    tuning (pruning schedules, quantization bit-widths), managing accuracy-compression
    trade-offs through automated evaluation, and ensuring hardware compatibility through
    device-specific code generation.'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习框架提供了高级API和自动化工作流程，抽象了实现复杂性，使复杂的优化技术对实践者可及。框架解决了关键挑战：提供常见优化技术的预构建模块，协助超参数调整（剪枝计划、量化位宽），通过自动化评估管理精度-压缩权衡，并通过特定于设备的代码生成确保硬件兼容性。
- en: This software infrastructure transforms theoretical optimization techniques
    into practical tools readily applied in production environments ([Chapter 13](ch019.xhtml#sec-ml-operations)).
    Production optimization workflows involve additional considerations including
    model versioning strategies, monitoring optimization impact on data pipelines,
    managing optimization artifacts across development and deployment environments,
    and establishing rollback procedures when optimizations fail. This accessibility
    bridges the gap between academic research and industrial applications, enabling
    widespread deployment of efficient machine learning models.
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 该软件基础设施将理论优化技术转化为可直接在生产环境中应用的实用工具（[第13章](ch019.xhtml#sec-ml-operations)）。生产优化工作流程涉及额外的考虑因素，包括模型版本控制策略、监控优化对数据管道的影响、管理开发与部署环境中的优化工件，以及在优化失败时建立回滚程序。这种可访问性弥合了学术研究与工业应用之间的差距，使得高效的机器学习模型得以广泛部署。
- en: Model Optimization APIs and Tools
  id: totrans-825
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型优化API和工具
- en: Leading frameworks such as TensorFlow, PyTorch, and MXNet provide comprehensive
    APIs enabling practitioners to apply optimization techniques without implementing
    complex algorithms from scratch ([Chapter 7](ch013.xhtml#sec-ai-frameworks)).
    These built-in optimizations enhance model efficiency while ensuring adherence
    to established best practices.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 领先的框架，如TensorFlow、PyTorch和MXNet，提供了全面的API，使从业者能够应用优化技术，而无需从头实现复杂的算法（[第7章](ch013.xhtml#sec-ai-frameworks)）。这些内置优化增强了模型效率，同时确保遵循既定的最佳实践。
- en: TensorFlow’s Model Optimization Toolkit facilitates quantization, pruning, and
    clustering. QAT converts floating-point models to lower-precision formats (INT8)
    while preserving accuracy, systematically managing both weight and activation
    quantization across diverse architectures. Pruning algorithms introduce sparsity
    by removing redundant connections at varying granularity levels—individual weights
    to entire layers—allowing practitioners to tailor strategies to specific requirements.
    Weight clustering groups similar weights for compression while preserving functionality,
    providing multiple pathways for improving model efficiency.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的模型优化工具包支持量化、剪枝和聚类。QAT将浮点模型转换为低精度格式（INT8），同时保持精度，系统性地管理跨不同架构的权重和激活量化。剪枝算法通过在不同粒度级别上移除冗余连接来引入稀疏性——从单个权重到整个层——允许从业者根据特定需求定制策略。权重聚类将相似的权重分组以进行压缩，同时保持功能，提供多条途径来提高模型效率。
- en: Similarly, PyTorch offers thorough optimization support through built-in modules
    for quantization and pruning. The `torch.quantization` package provides tools
    for converting models to lower-precision representations, supporting both post-training
    quantization and quantization-aware training, as shown in [Listing 10.3](ch016.xhtml#lst-qat_example).
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，PyTorch通过内置的量化和剪枝模块提供全面的优化支持。`torch.quantization`包提供了将模型转换为低精度表示的工具，支持训练后量化和量化感知训练，如[列表10.3](ch016.xhtml#lst-qat_example)所示。
- en: 'Listing 10.3: **Quantization-Aware Training**: Prepares a model to be trained
    in lower-precision formats, ensuring that quantization errors are accounted for
    during training.'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.3：**量化感知训练**：准备模型以在低精度格式下进行训练，确保在训练过程中考虑到量化误差。
- en: '[PRE2]'
  id: totrans-830
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For pruning, PyTorch provides the `torch.nn.utils.prune` module, which supports
    both unstructured and structured pruning. An example of both pruning strategies
    is given in [Listing 10.4](ch016.xhtml#lst-pytorch_pruning).
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 对于剪枝，PyTorch提供了`torch.nn.utils.prune`模块，它支持无结构和有结构的剪枝。这两种剪枝策略的示例在[列表10.4](ch016.xhtml#lst-pytorch_pruning)中给出。
- en: 'Listing 10.4: **PyTorch Pruning APIs**: Applies unstructured and structured
    pruning techniques to reduce model complexity while maintaining performance. *Source:
    PyTorch Documentation*'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.4：**PyTorch剪枝API**：应用无结构和有结构的剪枝技术以降低模型复杂度，同时保持性能。*来源：PyTorch文档*
- en: '[PRE3]'
  id: totrans-833
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These tools integrate seamlessly into PyTorch’s training pipelines, enabling
    efficient experimentation with different optimization strategies.
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具无缝集成到PyTorch的训练管道中，使得对不同的优化策略进行高效实验成为可能。
- en: Built-in optimization APIs offer significant benefits that make model optimization
    more accessible and reliable. By providing pre-tested, production-ready tools,
    these APIs dramatically reduce the implementation complexity that practitioners
    face when optimizing their models. Rather than having to implement complex optimization
    algorithms from scratch, developers can leverage standardized interfaces that
    have been thoroughly vetted.
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的优化API提供了显著的好处，使得模型优化更加易于访问和可靠。通过提供预测试、生产就绪的工具，这些API大大降低了从业者优化模型时所面临的实现复杂性。开发者无需从头开始实现复杂的优化算法，而是可以利用经过彻底审查的标准接口。
- en: The consistency provided by these built-in APIs is particularly valuable when
    working across different model architectures. The standardized interfaces ensure
    that optimization techniques are applied uniformly, reducing the risk of implementation
    errors or inconsistencies that could arise from custom solutions. This standardization
    helps maintain reliable and reproducible results across different projects and
    teams.
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内置API提供的一致性在跨不同模型架构工作时尤其有价值。标准化的接口确保了优化技术被统一应用，减少了由于定制解决方案可能出现的实现错误或不一致的风险。这种标准化有助于在不同项目和团队之间保持可靠和可重复的结果。
- en: These frameworks also serve as a bridge between cutting-edge research and practical
    applications. As new optimization techniques emerge from the research community,
    framework maintainers incorporate these advances into their APIs, making state-of-the-art
    methods readily available to practitioners. This continuous integration of research
    advances ensures that developers have access to the latest optimization strategies
    without needing to implement them independently.
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 这些框架也充当了前沿研究与实际应用之间的桥梁。随着新的优化技术从研究社区中涌现，框架维护者将这些进步整合到他们的API中，使得最先进的方法对从业者来说易于获取。这种持续集成研究进展确保了开发者能够访问最新的优化策略，而无需独立实现。
- en: The comprehensive nature of built-in APIs enables rapid experimentation with
    different optimization approaches. Developers can easily test various strategies,
    compare their effectiveness, and iterate quickly to find the optimal configuration
    for their specific use case. This ability to experiment efficiently is important
    for finding the right balance between model performance and resource constraints.
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 内置API的全面性使得对不同的优化方法进行快速实验成为可能。开发者可以轻松测试各种策略，比较其有效性，并快速迭代以找到特定用例的最佳配置。这种高效实验的能力对于在模型性能和资源限制之间找到合适的平衡至关重要。
- en: As model optimization continues to evolve, major frameworks maintain and expand
    their built-in support, further reducing barriers to efficient model deployment.
    The standardization of these APIs has played a important role in democratizing
    access to model efficiency techniques while ensuring high-quality implementations
    remain consistent and reliable.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型优化不断演进，主要框架维护并扩展了其内置支持，进一步降低了高效模型部署的障碍。这些API的标准化在民主化访问模型效率技术的同时，确保了高质量实现的一致性和可靠性。
- en: Hardware-Specific Optimization Libraries
  id: totrans-840
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件特定优化库
- en: Hardware optimization libraries in modern machine learning frameworks covered
    in [Chapter 7](ch013.xhtml#sec-ai-frameworks) enable efficient deployment of optimized
    models across different hardware platforms. These libraries integrate directly
    with training and deployment pipelines to provide hardware-specific acceleration
    for various optimization techniques across model representation, numerical precision,
    and architectural efficiency dimensions.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch013.xhtml#sec-ai-frameworks)中涵盖的现代机器学习框架中的硬件优化库，使得在不同硬件平台上高效部署优化模型成为可能。这些库直接与训练和部署管道集成，为模型表示、数值精度和架构效率等维度上的各种优化技术提供硬件特定的加速。
- en: For model representation optimizations like pruning, libraries such as TensorRT,
    XLA[43](#fn43), and OpenVINO provide sparsity-aware acceleration through optimized
    kernels that efficiently handle sparse computations. TensorRT specifically supports
    structured sparsity patterns, allowing models trained with techniques like two-out-of-four
    structured pruning to run efficiently on NVIDIA GPUs. Similarly, TPUs leverage
    XLA’s sparse matrix optimizations, while FPGAs enable custom sparse execution
    through frameworks like Vitis AI.
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型表示优化，如剪枝，TensorRT、XLA[43](#fn43)和OpenVINO等库通过优化的内核提供稀疏感知加速，这些内核能够高效地处理稀疏计算。TensorRT特别支持结构化稀疏模式，允许使用如两出四结构剪枝等技术训练的模型在NVIDIA
    GPU上高效运行。类似地，TPU利用XLA的稀疏矩阵优化，而FPGA通过如Vitis AI等框架实现定制的稀疏执行。
- en: Knowledge distillation benefits from hardware-aware optimizations that help
    compact student models achieve high inference efficiency. Libraries like TensorRT,
    OpenVINO, and SNPE optimize distilled models for low-power execution, often combining
    distillation with quantization or architectural restructuring to meet hardware
    constraints. For models discovered through neural architecture search (NAS), frameworks
    such as TVM[44](#fn44) and TIMM provide compiler support to tune the architectures
    for various hardware backends.
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏受益于硬件感知优化，有助于紧凑的学生模型实现高推理效率。TensorRT、OpenVINO和SNPE等库优化蒸馏模型以实现低功耗执行，通常将蒸馏与量化或架构重构相结合，以满足硬件约束。对于通过神经架构搜索（NAS）发现的模型，TVM[44](#fn44)和TIMM等框架提供编译器支持，以调整适用于各种硬件后端的架构。
- en: In terms of numerical precision optimization, these libraries offer extensive
    support for both PTQ and QAT. TensorRT and TensorFlow Lite implement INT8 and
    INT4 quantization during model conversion, reducing computational complexity while
    using specialized hardware acceleration on mobile SoCs and edge AI chips. NVIDIA
    TensorRT incorporates calibration-based quantization using representative datasets
    to optimize weight and activation scaling.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 在数值精度优化的方面，这些库为PTQ和QAT提供了广泛的支持。TensorRT和TensorFlow Lite在模型转换期间实现INT8和INT4量化，降低计算复杂度，同时在移动SoC和边缘AI芯片上使用专用硬件加速。
- en: More granular quantization approaches like channelwise and groupwise quantization
    are supported in frameworks such as SNPE and OpenVINO. Dynamic quantization capabilities
    in PyTorch and ONNX Runtime enable runtime activation quantization, making models
    adaptable to varying hardware conditions. For extreme quantization, techniques
    like binarization and ternarization are optimized through libraries such as CMSIS-NN,
    enabling efficient execution of binary-weight models on ARM Cortex-M microcontrollers.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 在SNPE和OpenVINO等框架中支持更细粒度的量化方法，如通道和组量化。PyTorch和ONNX Runtime中的动态量化功能使模型能够进行运行时激活量化，使模型能够适应不同的硬件条件。对于极端量化，通过CMSIS-NN等库优化了二值化和三值化技术，使得在ARM
    Cortex-M微控制器上高效执行二权重模型成为可能。
- en: Architectural efficiency techniques integrate tightly with hardware-specific
    execution frameworks. TensorFlow XLA and TVM provide operator-level tuning through
    aggressive fusion and kernel reordering, improving efficiency across GPUs, TPUs,
    and edge devices.
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 架构效率技术紧密集成于特定硬件的执行框架中。TensorFlow XLA和TVM通过积极的融合和内核重排提供操作级别的调整，从而提高GPU、TPU和边缘设备的效率。
- en: The widespread support for sparsity-aware execution spans multiple hardware
    platforms. NVIDIA GPUs utilize specialized sparse tensor cores for accelerating
    structured sparse models, while TPUs implement hardware-level sparse matrix optimizations.
    On FPGAs, vendor-specific compilers like Vitis AI enable custom sparse computations
    to be highly optimized.
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 对稀疏感知执行的广泛支持跨越多个硬件平台。NVIDIA GPU使用专门的稀疏张量核心来加速结构化稀疏模型，而TPU在硬件级别实现稀疏矩阵优化。在FPGA上，如Vitis
    AI等供应商特定的编译器使定制的稀疏计算得到高度优化。
- en: This thorough integration of hardware optimization libraries with machine learning
    frameworks enables developers to effectively implement pruning, quantization,
    NAS, dynamic computation, and sparsity-aware execution while ensuring optimal
    adaptation to target hardware, supporting the deployment strategies detailed in
    [Chapter 13](ch019.xhtml#sec-ml-operations). The ability to optimize across multiple
    dimensions, including model representation, numerical precision, and architectural
    efficiency, is important for deploying machine learning models efficiently across
    diverse platforms.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 这种将硬件优化库与机器学习框架的彻底集成，使得开发者能够有效地实现剪枝、量化、NAS、动态计算和稀疏感知执行，同时确保针对目标硬件的最佳适应，支持第13章中详细说明的部署策略。能够在多个维度上优化，包括模型表示、数值精度和架构效率，这对于在多样化的平台上高效部署机器学习模型非常重要。
- en: Optimization Process Visualization
  id: totrans-849
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化过程可视化
- en: Model optimization techniques alter model structure and numerical representations,
    but their impact can be difficult to interpret without visualization tools. Dedicated
    frameworks help practitioners understand how pruning, quantization, and other
    optimizations affect model behavior through graphical representations of sparsity
    patterns, quantization error distributions, and activation changes.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 模型优化技术会改变模型结构和数值表示，但没有可视化工具，其影响可能难以解释。专门的框架有助于从业者通过稀疏模式、量化误差分布和激活变化的图形表示来理解剪枝、量化和其他优化如何影响模型行为。
- en: Visualizing Quantization Effects
  id: totrans-851
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可视化量化效应
- en: Quantization reduces numerical precision, introducing rounding errors that can
    impact model accuracy. Visualization tools provide direct insight into how these
    errors are distributed, helping diagnose and mitigate precision-related performance
    degradation.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 量化降低了数值精度，引入了舍入误差，这些误差可能会影响模型精度。可视化工具可以直接揭示这些误差的分布情况，有助于诊断和减轻与精度相关的性能下降。
- en: One commonly used technique is quantization error histograms, which depict the
    distribution of errors across weights and activations. These histograms reveal
    whether quantization errors follow a Gaussian distribution or contain outliers,
    which could indicate problematic layers. TensorFlow’s Quantization Debugger and
    PyTorch’s FX Graph Mode Quantization tools allow users to analyze such histograms
    and compare error patterns between different quantization methods.
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常用的技术是量化误差直方图，它描绘了权重和激活之间的误差分布。这些直方图揭示了量化误差是否遵循高斯分布或包含异常值，这可能会指示有问题的层。TensorFlow的量化调试器和PyTorch的FX图模式量化工具允许用户分析此类直方图，并比较不同量化方法之间的错误模式。
- en: Activation visualizations also help detect overflow issues caused by reduced
    numerical precision. Tools such as ONNX Runtime’s quantization visualization utilities
    and NVIDIA’s TensorRT Inspector allow practitioners to color-map activations before
    and after quantization, making saturation and truncation issues visible. This
    enables calibration adjustments to prevent excessive information loss, preserving
    numerical stability. For example, [Figure 10.31](ch016.xhtml#fig-color-mapping)
    is a color mapping of the AlexNet convolutional kernels.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 激活可视化也有助于检测由降低数值精度引起的溢出问题。例如，ONNX运行时的量化可视化工具和NVIDIA的TensorRT检查器允许从业者对量化前后的激活进行颜色映射，使饱和和截断问题可见。这使校准调整能够防止信息损失过多，保持数值稳定性。例如，[图10.31](ch016.xhtml#fig-color-mapping)是AlexNet卷积核的颜色映射。
- en: '![](../media/file177.jpg)'
  id: totrans-855
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file177.jpg)'
- en: 'Figure 10.31: **Convolutional Kernel Weights**: Color mapping exposes patterns
    within learned convolutional filters, indicating feature detectors for edges,
    textures, or specific shapes within input images. Analyzing these weight distributions
    helps practitioners understand what features a neural network prioritizes and
    diagnose potential issues like dead or saturated filters—important for model calibration
    and performance optimization. Source: ([Krizhevsky, Sutskever, and Hinton 2017c](ch058.xhtml#ref-alexnet2012)).'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.31：**卷积核权重**：颜色映射揭示了学习到的卷积滤波器中的模式，指示输入图像中的边缘、纹理或特定形状的特征检测器。分析这些权重分布有助于从业者了解神经网络优先考虑哪些特征，并诊断潜在的死或饱和滤波器等问题——这对于模型校准和性能优化非常重要。来源：([Krizhevsky,
    Sutskever, and Hinton 2017c](ch058.xhtml#ref-alexnet2012))。
- en: Beyond static visualizations, tracking quantization error over the training
    process is important. Monitoring mean squared quantization error (MSQE) during
    quantization-aware training (QAT) helps identify divergence points where numerical
    precision significantly impacts learning. TensorBoard and PyTorch’s quantization
    debugging APIs provide real-time tracking, highlighting instability during training.
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 除了静态可视化之外，在训练过程中跟踪量化误差也很重要。在量化感知训练（QAT）期间监控均方量化误差（MSQE）有助于识别数值精度对学习产生重大影响的发散点。TensorBoard和PyTorch的量化调试API提供实时跟踪，突出显示训练过程中的不稳定性。
- en: By integrating these visualization tools into optimization workflows, practitioners
    can identify and correct issues early, ensuring optimized models maintain both
    accuracy and efficiency. These empirical insights provide a deeper understanding
    of how sparsity, quantization, and architectural optimizations affect models,
    guiding effective model compression and deployment strategies.
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这些可视化工具集成到优化工作流程中，从业者可以早期识别和纠正问题，确保优化模型既保持准确性又保持效率。这些经验见解提供了对稀疏性、量化和架构优化如何影响模型的更深入理解，指导有效的模型压缩和部署策略。
- en: Visualizing Sparsity Patterns
  id: totrans-859
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可视化稀疏模式
- en: Sparsity visualization tools provide detailed insight into pruned models by
    mapping out which weights have been removed and how sparsity is distributed across
    different layers. Frameworks such as TensorBoard (for TensorFlow) and Netron (for
    ONNX) allow users to inspect pruned networks at both the layer and weight levels.
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性可视化工具通过映射出哪些权重已被移除以及稀疏性如何分布在不同层之间，为修剪模型提供详细的洞察。例如，TensorBoard（用于TensorFlow）和Netron（用于ONNX）等框架允许用户在层和权重级别检查修剪的网络。
- en: One common visualization technique is sparsity heat maps, where color gradients
    indicate the proportion of weights removed from each layer. Layers with higher
    sparsity appear darker, revealing the model regions most impacted by pruning,
    as shown in [Figure 10.32](ch016.xhtml#fig-sprase-heat-map). This type of visualization
    transforms pruning from a black-box operation into an interpretable process, enabling
    practitioners to better understand and control sparsity-aware optimizations.
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的可视化技术是稀疏性热图，其中颜色渐变表示从每个层移除的权重比例。稀疏性较高的层看起来较暗，揭示了受修剪影响最大的模型区域，如[图10.32](ch016.xhtml#fig-sprase-heat-map)所示。此类可视化将修剪从黑盒操作转变为可解释的过程，使从业者能够更好地理解和控制稀疏感知优化。
- en: '![](../media/file178.png)'
  id: totrans-862
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file178.png)'
- en: 'Figure 10.32: **Sparsity Distribution**: Pruned neural networks exhibit varying
    degrees of weight removal across layers; darker shades indicate higher sparsity,
    revealing which parts of the model were most affected by the pruning process.
    Analyzing this distribution helps practitioners understand and refine sparsity-aware
    optimization strategies for model compression and efficiency. Source: [numenta](https://www.numenta.com/blog/)'
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.32：**稀疏分布**：修剪后的神经网络在层之间表现出不同程度的权重移除；较深的阴影表示更高的稀疏性，揭示了模型中哪些部分最受修剪过程的影响。分析这种分布有助于从业者理解和细化针对模型压缩和效率的稀疏感知优化策略。来源：[numenta](https://www.numenta.com/blog/)
- en: Beyond static snapshots, trend plots track sparsity progression across multiple
    pruning iterations. These visualizations illustrate how global model sparsity
    evolves, often showing an initial rapid increase followed by more gradual refinements.
    Tools like TensorFlow’s Model Optimization Toolkit and SparseML’s monitoring utilities
    provide such tracking capabilities, displaying per-layer pruning levels over time.
    These insights allow practitioners to fine-tune pruning strategies by adjusting
    sparsity constraints for individual layers.
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: 除了静态快照之外，趋势图跟踪多个修剪迭代中的稀疏性进展。这些可视化展示了全局模型稀疏性的演变，通常显示初始快速增加后是更渐进的细化。TensorFlow的模型优化工具包和SparseML的监控实用程序提供此类跟踪功能，显示随时间变化的每层修剪级别。这些见解允许从业者通过调整单个层的稀疏性约束来微调修剪策略。
- en: Libraries such as DeepSparse’s visualization suite and PyTorch’s pruning utilities
    enable the generation of these visualization tools, helping analyze how pruning
    decisions affect different model components. By making sparsity data visually
    accessible, these tools help practitioners optimize their models more effectively.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 如DeepSparse的可视化套件和PyTorch的修剪实用程序之类的库能够生成这些可视化工具，帮助分析修剪决策如何影响不同的模型组件。通过使稀疏数据可视化，这些工具帮助从业者更有效地优化他们的模型。
- en: Technique Comparison
  id: totrans-866
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术比较
- en: Having explored the three major optimization approaches in depth, a comparative
    analysis reveals how different techniques address distinct aspects of the efficiency-accuracy
    trade-off. This comparison guides technique selection based on deployment constraints
    and available resources.
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 深入研究了三种主要优化方法后，比较分析揭示了不同技术如何解决效率-精度权衡的不同方面。这种比较有助于根据部署约束和可用资源选择技术。
- en: 'Table 10.12: **Optimization Technique Trade-offs**: Comparison of the three
    major optimization approaches across key performance dimensions, highlighting
    how each technique addresses different constraints and deployment scenarios. Pruning
    excels for computational reduction but requires sparse hardware support, quantization
    provides balanced size and speed improvements with wide hardware compatibility,
    while distillation produces high-quality compressed models at higher training
    cost.'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.12：**优化技术权衡**：比较三种主要优化方法在关键性能维度上的差异，突出每种技术如何解决不同的约束和部署场景。剪枝在计算减少方面表现优异，但需要稀疏硬件支持；量化提供了平衡的大小和速度改进，具有广泛的硬件兼容性；而蒸馏在更高的训练成本下产生高质量的压缩模型。
- en: '| **Technique** | **Primary Goal** | **Accuracy Impact** | **Training Cost**
    | **Hardware Dependency** | **Best For** |'
  id: totrans-869
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **主要目标** | **精度影响** | **训练成本** | **硬件依赖** | **最佳适用** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-870
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **Pruning** | Reduce FLOPs/Size | Moderate | Low (fine-tuning) | High (for
    sparse ops) | Latency-critical apps |'
  id: totrans-871
  prefs: []
  type: TYPE_TB
  zh: '| **剪枝** | 减少FLOPs/大小 | 中等 | 低（微调） | 高（对于稀疏操作） | 延迟关键应用 |'
- en: '| **Quantization** | Reduce Size/Latency | Low | Low (PTQ) / High (QAT) | High
    (INT8 support) | Edge/Mobile deployment |'
  id: totrans-872
  prefs: []
  type: TYPE_TB
  zh: '| **量化** | 减少大小/延迟 | 低 | 低（PTQ）/高（QAT） | 高（INT8支持） | 边缘/移动部署 |'
- en: '| **Distillation** | Reduce Size | Low-Moderate | High (retraining) | Low |
    Creating smaller, high-quality models |'
  id: totrans-873
  prefs: []
  type: TYPE_TB
  zh: '| **蒸馏** | 减少大小 | 低-中 | 高（重新训练） | 低 | 创建更小、高质量的模型 |'
- en: Understanding these trade-offs enables systematic technique selection ([Table 10.12](ch016.xhtml#tbl-optimization-comparison)).
    Pruning works best when sparse computation hardware is available and when reducing
    floating-point operations is critical. Quantization provides the most versatile
    approach with broad hardware support, making it ideal for diverse deployment scenarios.
    Knowledge distillation requires significant computational investment but produces
    consistently high-quality compressed models, making it valuable when accuracy
    preservation is paramount.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些权衡有助于系统地选择技术（[表10.12](ch016.xhtml#tbl-optimization-comparison)）。剪枝在稀疏计算硬件可用且减少浮点运算至关重要时效果最佳。量化提供了最灵活的方法，具有广泛的硬件支持，使其成为各种部署场景的理想选择。知识蒸馏需要大量的计算投入，但能产生一致的高质量压缩模型，因此在精度保护至关重要的场合非常有价值。
- en: 'These techniques combine synergistically, with quantization often applied after
    pruning or distillation to achieve compound compression benefits. Production systems
    frequently employ sequential application: initial pruning reduces parameter count,
    quantization optimizes numerical representation, and fine-tuning through distillation
    principles recovers any accuracy loss. Sequential application enables compression
    ratios of 10-50x while maintaining competitive accuracy across diverse deployment
    scenarios.'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术协同作用，量化通常在剪枝或蒸馏之后应用，以实现复合压缩效益。生产系统通常采用顺序应用：初始剪枝减少参数数量，量化优化数值表示，通过蒸馏原则进行微调以恢复任何精度损失。顺序应用可以实现10-50倍的压缩比率，同时在各种部署场景中保持有竞争力的精度。
- en: Fallacies and Pitfalls
  id: totrans-876
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谬误和陷阱
- en: Model optimization represents one of the most technically complex areas in machine
    learning systems, where multiple techniques must be coordinated to achieve efficiency
    gains without sacrificing accuracy. The sophisticated nature of pruning, quantization,
    and distillation techniques—combined with their complex interdependencies—creates
    numerous opportunities for misapplication and suboptimal results that can undermine
    deployment success.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 模型优化是机器学习系统中技术最复杂的领域之一，其中必须协调多种技术以实现效率提升而不牺牲精度。剪枝、量化和蒸馏技术的复杂性和它们之间复杂的相互依赖性创造了大量误用和次优结果的机会，这些结果可能会损害部署的成功。
- en: '**Fallacy:** *Optimization techniques can be applied independently without
    considering their interactions.*'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误**：*优化技术可以独立应用，无需考虑它们之间的相互作用。*'
- en: This misconception leads teams to apply multiple optimization techniques simultaneously
    without understanding how they interact. Combining pruning with aggressive quantization
    might compound accuracy losses beyond acceptable levels, while knowledge distillation
    from heavily pruned models may transfer suboptimal behaviors to student networks.
    Different optimization approaches can interfere with each other’s effectiveness,
    creating complex trade-offs that require careful orchestration. Successful optimization
    requires understanding technique interactions and applying them in coordinated
    strategies rather than as independent modifications.
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 这种误解导致团队在没有理解它们如何相互作用的情况下同时应用多种优化技术。将剪枝与激进的量化相结合可能会使精度损失超过可接受水平，而从大量剪枝的模型中提取的知识蒸馏可能会将次优行为转移到学生网络上。不同的优化方法可能会相互干扰，产生复杂的权衡，需要仔细协调。成功的优化需要理解技术之间的相互作用，并协调一致地应用它们，而不是作为独立的修改。
- en: '**Pitfall:** *Optimizing for theoretical metrics rather than actual deployment
    performance.*'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *优化理论指标而不是实际部署性能。*'
- en: Many practitioners focus on reducing parameter counts, FLOPs, or model size
    without measuring actual deployment performance improvements. A model with fewer
    parameters might still have poor cache locality, irregular memory access patterns,
    or inefficient hardware utilization that negates theoretical efficiency gains.
    Quantization that reduces model size might increase inference latency on certain
    hardware platforms due to format conversion overhead. Effective optimization requires
    measuring and optimizing for actual deployment metrics rather than relying on
    theoretical complexity reductions.
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 许多从业者专注于减少参数数量、FLOPs或模型大小，而没有衡量实际部署性能的提升。一个参数更少的模型可能仍然具有较差的缓存局部性、不规则的内存访问模式或低效的硬件利用率，从而抵消了理论上的效率提升。量化减少模型大小可能会由于格式转换开销而在某些硬件平台上增加推理延迟。有效的优化需要衡量和优化实际的部署指标，而不是依赖于理论上的复杂性降低。
- en: '**Fallacy:** *Aggressive quantization maintains model performance with minimal
    accuracy loss.*'
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *激进的量化可以在最小精度损失的情况下保持模型性能。*'
- en: This belief drives teams to apply extreme quantization levels without understanding
    the relationship between numerical precision and model expressiveness. While many
    models tolerate moderate quantization well, extreme quantization can cause catastrophic
    accuracy degradation, numerical instability, or training divergence. Different
    model architectures and tasks have varying sensitivity to quantization, requiring
    careful analysis rather than assuming universal applicability. Some operations
    like attention mechanisms or normalization layers may require higher precision
    to maintain functionality.
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信念驱使团队在不理解数值精度与模型表达能力之间关系的情况下应用极端的量化级别。虽然许多模型可以很好地容忍适度的量化，但极端的量化可能会导致灾难性的精度下降、数值不稳定或训练发散。不同的模型架构和任务对量化的敏感性不同，需要仔细分析，而不是假设其普遍适用性。一些操作，如注意力机制或归一化层，可能需要更高的精度以保持其功能。
- en: '**Pitfall:** *Using post-training optimization without considering training-aware
    alternatives.*'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *使用后训练优化而不考虑训练感知替代方案。*'
- en: Teams often apply optimization techniques after training completion to avoid
    modifying existing training pipelines. Post-training optimization is convenient
    but typically achieves inferior results compared to optimization-aware training
    approaches. Quantization-aware training, gradual pruning during training, and
    distillation-integrated training can achieve better accuracy-efficiency trade-offs
    than applying these techniques post-hoc. The convenience of post-training optimization
    comes at the cost of suboptimal results that may not meet deployment requirements.
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 团队通常在训练完成后应用优化技术，以避免修改现有的训练流程。后训练优化虽然方便，但通常比优化感知训练方法的结果差。量化感知训练、训练过程中的逐步剪枝和蒸馏集成训练可以在应用这些技术后获得更好的精度-效率权衡。后训练优化的便利性是以次优结果为代价的，这些结果可能不符合部署要求。
- en: '**Pitfall:** *Focusing on individual model optimization without considering
    system-level performance bottlenecks.*'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *专注于单个模型优化，而不考虑系统级性能瓶颈。*'
- en: Many optimization efforts concentrate solely on reducing model complexity without
    analyzing the broader system context where models operate, requiring the structured
    profiling approaches detailed in [Chapter 12](ch018.xhtml#sec-benchmarking-ai).
    A highly optimized model may provide minimal benefit if data preprocessing pipelines,
    I/O operations, or network communication dominate overall system latency. Memory
    bandwidth limitations, cache misses, or inefficient batch processing can negate
    the advantages of aggressive model optimization. Similarly, optimizing for single-model
    inference may miss opportunities for throughput improvements through batch processing,
    model parallelism, or request pipelining. Effective optimization requires profiling
    the entire system to identify actual bottlenecks and ensuring that model-level
    improvements translate to measurable system-level performance gains. This systems
    perspective is particularly important in multi-model ensembles, real-time serving
    systems, or edge deployments where resource constraints extend beyond individual
    model efficiency. The holistic optimization approach connects directly to the
    operational excellence principles [Chapter 13](ch019.xhtml#sec-ml-operations)
    by ensuring that optimizations contribute to overall system reliability and maintainability.
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 许多优化工作仅专注于降低模型复杂性，而没有分析模型运行的整体系统环境，需要详细说明的[第12章](ch018.xhtml#sec-benchmarking-ai)中的结构化分析方法。如果数据预处理管道、I/O操作或网络通信主导了整体系统延迟，那么高度优化的模型可能只提供微小的益处。内存带宽限制、缓存未命中或低效的批量处理可能会抵消激进模型优化的优势。同样，针对单个模型推理的优化可能会错过通过批量处理、模型并行或请求流水线提高吞吐量的机会。有效的优化需要分析整个系统以确定实际瓶颈，并确保模型级别的改进转化为可衡量的系统级别性能提升。这种系统视角在多模型集成、实时服务系统或边缘部署中尤为重要，在这些场景中，资源限制超出了单个模型效率的范围。整体优化方法通过确保优化有助于整体系统可靠性和可维护性，直接与操作卓越原则[第13章](ch019.xhtml#sec-ml-operations)相联系。
- en: Summary
  id: totrans-888
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Model optimization represents the important bridge between theoretical machine
    learning advances and practical deployment realities, where computational constraints,
    memory limitations, and energy efficiency requirements demand sophisticated engineering
    solutions. This chapter demonstrated how the core tension between model accuracy
    and resource efficiency drives a rich ecosystem of optimization techniques that
    operate across multiple dimensions simultaneously. Rather than simply reducing
    model size or complexity, modern optimization approaches strategically reorganize
    model representations, numerical precision, and computational patterns to preserve
    important capabilities while dramatically improving efficiency characteristics.
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 模型优化代表了理论机器学习进步与实际部署现实之间的重要桥梁，在计算约束、内存限制和能源效率要求下，需要复杂的工程解决方案。本章展示了模型精度与资源效率之间的核心紧张关系如何驱动一个丰富的优化技术生态系统，这些技术可以在多个维度上同时操作。现代优化方法不是简单地减少模型大小或复杂性，而是战略性地重新组织模型表示、数值精度和计算模式，以保留重要能力的同时，显著提高效率特性。
- en: 'Our optimization framework demonstrates how different aspects of model design
    can be systematically refined to meet deployment constraints. The journey from
    a 440MB BERT-Base model ([Devlin et al. 2018b](ch058.xhtml#ref-devlin2018bert))
    to a 28MB deployment-ready version exemplifies the power of combining complementary
    techniques: structural pruning shrinks the model to 110MB, knowledge distillation
    with DistilBERT ([Sanh et al. 2019](ch058.xhtml#ref-sanh2019distilbert)) maintains
    performance while further reducing size, and INT8 quantization achieves the final
    28MB target. The integration of hardware-aware design principles ensures that
    optimization strategies align with underlying computational architectures, maximizing
    practical benefits across different deployment environments.'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的优化框架展示了如何系统地细化模型设计的各个方面，以满足部署限制。从440MB的BERT-Base模型([Devlin等人2018b](ch058.xhtml#ref-devlin2018bert))到28MB的部署版本的过程，展示了结合互补技术的力量：结构化剪枝将模型缩小到110MB，使用DistilBERT([Sanh等人2019](ch058.xhtml#ref-sanh2019distilbert))的知识蒸馏在保持性能的同时进一步减小了大小，INT8量化实现了最终的28MB目标。硬件感知设计原则的集成确保了优化策略与底层计算架构相一致，最大化了不同部署环境中的实际效益。
- en: '**Key Takeaways**'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: Model optimization requires coordinated approaches across representation, precision,
    and architectural efficiency—as demonstrated by BERT’s 16x compression through
    combined pruning, distillation, and quantization
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型优化需要在表示、精度和架构效率方面采取协调一致的方法——正如BERT通过联合剪枝、蒸馏和量化实现16倍压缩所证明的那样
- en: Hardware-aware optimization aligns model characteristics with computational
    architectures to maximize practical performance benefits
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件感知优化将模型特性与计算架构对齐，以最大化实际性能效益
- en: Automated optimization through AutoML can discover novel combinations of techniques
    that outperform manual optimization strategies
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过AutoML进行自动化优化可以发现优于手动优化策略的技术组合
- en: Optimization techniques must balance accuracy preservation with deployment constraints—DistilBERT
    retains 97% of BERT’s performance with 40% fewer parameters
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化技术必须在保持准确性的同时平衡部署限制——DistilBERT在参数减少40%的情况下保留了BERT的97%的性能
- en: Success requires understanding that no single technique provides a universal
    solution; the optimal strategy depends on specific deployment constraints, hardware
    characteristics, and application requirements
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功需要理解没有单一技术能提供通用的解决方案；最佳策略取决于特定的部署限制、硬件特性和应用需求
- en: The emergence of AutoML frameworks for optimization represents a paradigm shift
    toward automated discovery of optimization strategies that can adapt to specific
    deployment contexts and performance requirements. These automated approaches build
    on training methodologies while pointing toward the emerging frontiers of self-optimizing
    systems. Such systems enable practitioners to explore vast optimization spaces
    more systematically than manual approaches, often uncovering novel combinations
    of techniques that achieve superior efficiency-accuracy trade-offs. As models
    grow larger and deployment contexts become more diverse, mastering these optimization
    techniques becomes increasingly critical for bridging the gap between research
    accuracy and production efficiency.
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 优化自动化框架的出现代表了向自动化发现适应特定部署环境和性能要求的优化策略的范式转变。这些自动化方法建立在训练方法之上，同时指向自我优化系统的新兴前沿。这样的系统使从业者能够比手动方法更系统地探索广泛的优化空间，通常揭示出实现更优效率-准确性权衡的新技术组合。随着模型变得越来越大，部署环境变得更加多样化，掌握这些优化技术对于弥合研究准确性与生产效率之间的差距变得越来越关键。
- en: '* * *'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
