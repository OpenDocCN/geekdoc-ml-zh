["```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy                                                  # Hermite polynomials\nfrom scipy import stats                                       # statistical methods\nimport pandas as pd                                           # DataFrames\nimport pandas.plotting as pd_plot\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn.linear_model import LinearRegression             # linear regression with scikit learn\nfrom sklearn.preprocessing import PolynomialFeatures          # polynomial basis expansion\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.preprocessing import (StandardScaler,PolynomialFeatures) # standardize the features, polynomial basis expansion\nfrom sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning\nfrom sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # supress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5]) \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\ndf = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/1D_Porosity.csv\") # data from Dr. Pyrcz's github repository \n```", "```py\ndf.head(n=13)                                                 # preview the data \n```", "```py\ndf.describe(percentiles=[0.1,0.9]).transpose()                # summary statistics \n```", "```py\nXname = ['Depth']; yname = ['Nporosity']                      # select the predictor and response feature\n\nXlabel = ['Depth']; ylabel = ['Gaussian Transformed Porosity'] # specify the feature labels for plotting\nXunit = ['m']; yunit = ['N[%]']\nXlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')']\nylabelunit = ylabel[0] + ' (' + yunit[0] + ')'\n\nX = df[Xname[0]]                                              # extract the 1D ndarrays from the DataFrame\ny = df[yname[0]]\n\nXmin = 0.0; Xmax = 10.0                                       # limits for plotting\nymin = -3.0; ymax = 3.0\n\nX_values = np.linspace(Xmin,Xmax,100)                         # X intervals to visualize the model \n```", "```py\nlin = LinearRegression()                                      # instantiate linear regression object, note no hyperparameters \nlin.fit(X.values.reshape(-1, 1), y)                           # train linear regression model\n\nslope = lin.coef_[0]                                          # get the model parameters\nintercept = lin.intercept_\n\nplt.subplot(111)                                              # plot the data and the model\nplt.scatter(X,y,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.plot(X_values,intercept + slope*X_values,label='model',color = 'black')\nplt.title('Linear Regression Model, Regression of ' + yname[0] + ' on ' + Xname[0])\nplt.xlabel(Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel(yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])\nplt.annotate('Linear Regression Model',[4.5,-1.8])\nplt.annotate(r'    $\\beta_1$ :' + str(round(slope,2)),[6.8,-2.3])\nplt.annotate(r'    $\\beta_0$ :' + str(round(intercept,2)),[6.8,-2.7])\nplt.annotate(r'$N[\\phi] = \\beta_1 \\times z + \\beta_0$',[4.0,-2.3])\nplt.annotate(r'$N[\\phi] = $' + str(round(slope,2)) + r' $\\times$ $z$ + (' + str(round(intercept,2)) + ')',[4.0,-2.7])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nfrom sklearn import tree                                      # tree program from scikit learn \n\nmy_tree = tree.DecisionTreeRegressor(min_samples_leaf=5, max_depth = 20) # instantiate the decision tree model with hyperparameters\nmy_tree = my_tree.fit(X.values.reshape(-1, 1),y)              # fit the decision tree to the training data (all the data in this case)\nDT_y = my_tree.predict(X_values.reshape(-1,1))                # predict at high resolution over the range of depths\n\nplt.subplot(111)                                              # plot the model and data\nplt.scatter(X,y,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.plot(X_values, DT_y, label='model', color = 'black')\nplt.title('Decision Tree Model, ' + yname[0] + ' as a Function of ' + Xname[0])\nplt.xlabel(Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel(yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor            # random forest method\n\nmax_depth = 5                                                 # set the random forest hyperparameters\nnum_tree = 1000\nmax_features = 1\n\nmy_forest = RandomForestRegressor(max_depth=max_depth,random_state=seed,n_estimators=num_tree,max_features=max_features)\nmy_forest.fit(X = X.values.reshape(-1, 1), y = y)  \nRF_y = my_forest.predict(X_values.reshape(-1,1))\nplt.subplot(111)\nplt.scatter(X,y,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.plot(X_values, RF_y, label='model', color = 'black')\nplt.title('Random Forest Tree Model, ' + yname[0] + ' as a Function of ' + Xname[0])\nplt.xlabel(Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel(yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2)\nplt.show() \n```", "```py\nimport geostatspy.geostats as geostats                        # for Gaussian transform from GSLIB\n\ndf_ns = pd.DataFrame()   \ndf_ns[Xname[0]], tvPor, tnsPor = geostats.nscore(df, Xname[0]) # nscore transform for all facies porosity \ndf_ns[yname[0]], tvdepth, tnsdepth = geostats.nscore(df, yname[0]) # nscore transform for all facies permeability\nX_ns = df_ns[Xname[0]]; y_ns = df_ns[yname[0]]\nX_ns_values = np.linspace(-3.0,3.0,1000)                      # values to predict at in standard normal space \n```", "```py\nplt.subplot(221)                                              # plot original sand and shale porosity histograms\nplt.hist(df[Xname[0]], facecolor='red',bins=np.linspace(Xmin,Xmax,1000),histtype=\"stepfilled\",alpha=0.2,density=True,\n         cumulative=True,edgecolor='black',label='Original')\nplt.xlim([0.0,10.0]); plt.ylim([0,1.0])\nplt.xlabel(Xname[0] + ' (' + Xunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Original Depth')\nplt.legend(loc='upper left')\nplt.grid(True)\n\nplt.subplot(222)  \nplt.hist(df_ns[Xname[0]], facecolor='blue',bins=np.linspace(-3.0,3.0,1000),histtype=\"stepfilled\",alpha=0.2,density=True,\n         cumulative=True,edgecolor='black',label = 'NS')\nplt.xlim([-3.0,3.0]); plt.ylim([0,1.0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Nscore ' + Xname[0])\nplt.legend(loc='upper left')\nplt.grid(True)\n\nplt.subplot(223)                                        # plot nscore transformed sand and shale histograms\nplt.hist(df[yname[0]], facecolor='red',bins=np.linspace(ymin,ymax,1000),histtype=\"stepfilled\",alpha=0.2,density=True,\n         cumulative=True,edgecolor='black',label='Original')\nplt.xlim([-3.0,3.0]); plt.ylim([0,1.0])\nplt.xlabel(yname[0] + ' (' + yunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Original Porosity')\nplt.legend(loc='upper left')\nplt.grid(True)\n\nplt.subplot(224)                                        # plot nscore transformed sand and shale histograms\nplt.hist(df_ns[yname[0]], facecolor='blue',bins=np.linspace(-3.0,3.0,1000),histtype=\"stepfilled\",alpha=0.2,density=True,\n         cumulative=True,edgecolor='black',label = 'NS')\nplt.xlim([-3.0,3.0]); plt.ylim([0,1.0])\nplt.xlabel('NS: ' + yname[0] + ' (' + yunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Nscore ' + yname[0])\nplt.legend(loc='upper left')\nplt.grid(True)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=2.0, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nlin_ns = LinearRegression()                                   # instantiate linear regression object, note no hyperparameters \nlin_ns.fit(X_ns.values.reshape(-1, 1), y_ns)                  # train linear regression model\nslope_ns = lin_ns.coef_[0]                                    # get the model parameters\nintercept_ns = lin_ns.intercept_\n\nplt.subplot(111)                                              # plot the data and the model\nplt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.plot(X_ns_values,intercept_ns + slope_ns*X_ns_values,label='model',color = 'black')\nplt.title('Linear Regression Model, Regression of NS ' + yname[0] + ' on ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])\nplt.annotate('Linear Regression Model',[0.8,-1.8])\nplt.annotate(r'    $\\beta_1$ :' + str(round(slope_ns,2)),[1.8,-2.3])\nplt.annotate(r'    $\\beta_0$ :' + str(round(intercept_ns,2)),[1.8,-2.7])\nplt.annotate(r'$N[\\phi] = \\beta_1 \\times z + \\beta_0$',[0.5,-2.3])\nplt.annotate(r'$N[\\phi] = $' + str(round(slope_ns,2)) + r' $\\times$ $z$ + (' + str(round(intercept_ns,2)) + ')',[0.5,-2.7])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\npoly4 = PolynomialFeatures(degree = 4)                        # instantiate polynomial expansion \nX_ns_poly4 = poly4.fit_transform(X_ns.values.reshape(-1, 1))  # calculate the basis expansion for our dataset\ndf_X_ns_poly4 = pd.DataFrame({'Values':X_ns,'0th':X_ns_poly4[:,0],'1st':X_ns_poly4[:,1],'2nd':X_ns_poly4[:,2], \n                              '3rd':X_ns_poly4[:,3],'4th':X_ns_poly4[:,4]}) # make a new DataFrame from the vectors\ndf_X_ns_poly4 = pd.DataFrame({'Values':X_ns,'1st':X_ns_poly4[:,1],'2nd':X_ns_poly4[:,2], \n                              '3rd':X_ns_poly4[:,3],'4th':X_ns_poly4[:,4]}) # make a new DataFrame from the vectors\ndf_X_ns_poly4.head()                                          # preview the polynomial basis expansion with the original predictor feature \n```", "```py\ncorr_matrix = df_X_ns_poly4.iloc[:,1:].corr()                 # calculate the correlation matrix\n\nplt.subplot(111)\nplot_corr(corr_matrix,'Polynomial Expansion Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nsns.pairplot(df_X_ns_poly4.iloc[:,1:],vars=['1st','2nd','3rd','4th'],markers='o', kind='reg',diag_kind='kde')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nplt.subplot(111)                                              # plot the polynomial basis expansion\nplt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,0],label='0th',color = 'black')\nplt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,1],label='1th',color = 'blue')\nplt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,2],label='2th',color = 'green')\nplt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,3],label='3th',color = 'red')\nplt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,4],label='4th',color = 'orange') \nplt.title('Polynomial Basis Expansion of ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('h[ NS: ' + Xname[0] + ' (' + Xunit[0] + ') ]')\nplt.legend(); plt.xlim(-3,3); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nprint('The averages of each basis expansion, 0 - 4th order = ' + str(stats.describe(X_ns_poly4)[2]) + '.') \n```", "```py\nThe averages of each basis expansion, 0 - 4th order = [1\\.         0.00536486 0.9458762  0.07336308 2.31077802]. \n```", "```py\nlin_poly4 = LinearRegression()                                # instantiate new linear model \nlin_poly4.fit(df_X_ns_poly4.iloc[:,1:], y_ns)                 # train linear model with polynomial expansion, polynomial regression\nb1,b2,b3,b4 = np.round(lin_poly4.coef_,3)                     # retrieve the model parameters\nb0 = lin_poly4.intercept_\n\nplt.subplot(111)\nplt.plot(X_ns_values,lin_poly4.predict(poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,1:]),label='polynomial',color = 'red') \nplt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.title('Polynomial Regression Model, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])\nplt.annotate('Polynomial Regression Model',[-2.8,2.6])\nplt.annotate(r'    $\\beta_4$ :' + str(round(b4,3)),[-2.8,2.1])\nplt.annotate(r'    $\\beta_3$ :' + str(round(b3,3)),[-2.8,1.7])\nplt.annotate(r'    $\\beta_2$ :' + str(round(b2,3)),[-2.8,1.3])\nplt.annotate(r'    $\\beta_1$ :' + str(round(b1,3)),[-2.8,0.9])\nplt.annotate(r'    $\\beta_0$ :' + str(round(b0,2)),[-2.8,0.5])\nplt.annotate(r'$N[\\phi] = \\beta_4 \\times N[z]^4 + \\beta_3 \\times N[z]^3 + \\beta_2 \\times N[z]^2 + \\beta_1 \\times N[z] + \\beta_0$',[-1.0,-2.0])\nplt.annotate(r'$N[\\phi] = $' + str(b4) + r' $\\times N[z]^4 +$ ' + str(b3) + r' $\\times N[z]^3 +$ ' + str(b2) + r' $\\times N[z]^2 +$ ' + \n             str(b1) + r' $\\times N[z]$ + ' + str(round(b0,2)),[-1.0,-2.5])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\norders4 = [1,2,3,4]                                           # specify the orders for Hermite basis expansion\nX_ns_hermite4 = scipy.special.eval_hermitenorm(orders4,X_ns.values.reshape(-1, 1), out=None) # Hermite polynomials for X \ndf_X_ns_hermite4 = pd.DataFrame({'value':X_ns.values,'1st':X_ns_hermite4[:,0],'2nd':X_ns_hermite4[:,1], \n                                     '3rd':X_ns_hermite4[:,2],'4th':X_ns_hermite4[:,3]}) # make a new DataFrame from the vectors\ndf_X_ns_hermite4.head() \n```", "```py\nhermite_corr_matrix = df_X_ns_hermite4.iloc[:,1:].corr()      # calculate correlation matrix of Hermite basis expansion of X\n\nplt.subplot(111)\nplot_corr(hermite_corr_matrix,'Hermite Polynomial Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nsns.pairplot(df_X_ns_hermite4.iloc[:,1:],vars=['1st','2nd','3rd','4th'],markers='o', kind='reg',diag_kind='kde')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nprint('The means of each basis expansion, 1 - 4th order = ' + str(stats.describe(X_ns_hermite4)[2]) + '.') \n```", "```py\nThe means of each basis expansion, 1 - 4th order = [ 0.00536486 -0.0541238   0.05726848 -0.36447919]. \n```", "```py\nplt.subplot(111)                                              # plot Hermite polynomials\nplt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,0],label='1st',color = 'blue')\nplt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,1],label='2nd',color = 'green')\nplt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,2],label='3rd',color = 'red')\nplt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,3],label='4th',color = 'orange')\nplt.title('Hermite Polynomial Basis Expansion of ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('h[ NS: ' + Xname[0] + ' (' + Xunit[0] + ') ]')\nplt.legend(); plt.xlim(-3,3); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlin_herm4 = LinearRegression()                                # instantiate model\nlin_herm4.fit(df_X_ns_hermite4.iloc[:,1:], y_ns)              # fit Hermite polynomials \nhb1,hb2,hb3,hb4 = np.round(lin_herm4.coef_,3)                 # retrieve the model parameters\nhb0 = lin_herm4.intercept_\nplt.subplot(111)                                              # plot data and model\nplt.plot(X_ns_values, lin_herm4.predict(scipy.special.eval_hermitenorm(orders4,X_ns_values.reshape(-1, 1), out=None)), \n         label='4th order',color = 'red') \nplt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.title('Hermite Polynomial Regression Model, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])\nplt.annotate('Hermite Polynomial Regression Model',[-2.8,2.6])\nplt.annotate(r'    $\\beta_4$ :' + str(round(hb4,3)),[-2.8,2.1])\nplt.annotate(r'    $\\beta_3$ :' + str(round(hb3,3)),[-2.8,1.7])\nplt.annotate(r'    $\\beta_2$ :' + str(round(hb2,3)),[-2.8,1.3])\nplt.annotate(r'    $\\beta_1$ :' + str(round(hb1,3)),[-2.8,0.9])\nplt.annotate(r'    $\\beta_0$ :' + str(round(hb0,2)),[-2.8,0.5])\nplt.annotate(r'$N[\\phi] = \\beta_4 \\times N[z]^4 + \\beta_3 \\times N[z]^3 + \\beta_2 \\times N[z]^2 + \\beta_1 \\times N[z] + \\beta_0$',[-1.0,-2.0])\nplt.annotate(r'$N[\\phi] = $' + str(hb4) + r' $\\times N[z]^4 +$ ' + str(hb3) + r' $\\times N[z]^3 +$ ' + str(hb2) + r' $\\times N[z]^2 +$ ' + \n             str(hb1) + r' $\\times N[z]$ + ' + str(round(hb0,2)),[-1.0,-2.5])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\n# functions taken (without modification) from http://davmre.github.io/blog/python/2013/12/15/orthogonal_poly\n# appreciation to Dave Moore for the great blog post on titled 'Orthogonal polynomial regression in Python'\n# functions are Dave's reimplementation of poly() from R\n\ndef ortho_poly_fit(x, degree = 1):\n    n = degree + 1\n    x = np.asarray(x).flatten()\n    if(degree >= len(np.unique(x))):\n            stop(\"'degree' must be less than number of unique points\")\n    xbar = np.mean(x)\n    x = x - xbar\n    X = np.fliplr(np.vander(x, n))\n    q,r = np.linalg.qr(X)\n\n    z = np.diag(np.diag(r))\n    raw = np.dot(q, z)\n\n    norm2 = np.sum(raw**2, axis=0)\n    alpha = (np.sum((raw**2)*np.reshape(x,(-1,1)), axis=0)/norm2 + xbar)[:degree]\n    Z = raw / np.sqrt(norm2)\n    return Z, norm2, alpha\n\ndef ortho_poly_predict(x, alpha, norm2, degree = 1):\n    x = np.asarray(x).flatten()\n    n = degree + 1\n    Z = np.empty((len(x), n))\n    Z[:,0] = 1\n    if degree > 0:\n        Z[:, 1] = x - alpha[0]\n    if degree > 1:\n        for i in np.arange(1,degree):\n             Z[:, i+1] = (x - alpha[i]) * Z[:, i] - (norm2[i] / norm2[i-1]) * Z[:, i-1]\n    Z /= np.sqrt(norm2)\n    return Z \n```", "```py\nX_ns_ortho4, norm2, alpha = ortho_poly_fit(X_ns.values.reshape(-1, 1), degree = 4) # orthogonal polynomial expansion\ndf_X_ns_ortho4 = pd.DataFrame({'value':X_ns.values,'1st':X_ns_ortho4[:,1],'2nd':X_ns_ortho4[:,2],'3rd':X_ns_ortho4[:,3],\n                               '4th':X_ns_ortho4[:,4]})       # make a new DataFrame from the vectors\ndf_X_ns_ortho4.head() \n```", "```py\northo_corr_matrix = df_X_ns_ortho4.iloc[:,1:].corr()          # calculate the correlation matrix\n\nplt.subplot(111)\nplot_corr(ortho_corr_matrix,'Orthogonal Polynomial Expansion Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nsns.pairplot(df_X_ns_ortho4.iloc[:,1:],vars=['1st','2nd','3rd','4th'],markers='o',kind='reg',diag_kind='kde') \n```", "```py\n<seaborn.axisgrid.PairGrid at 0x1ed608d8370> \n```", "```py\northo_poly_ns_values = ortho_poly_predict(X_ns_values.reshape(-1, 1), alpha, norm2, degree = 4)\n\nplt.subplot(111)\nplt.plot(X_ns_values, ortho_poly_ns_values[:,0], label='0th', color = 'black')\nplt.plot(X_ns_values, ortho_poly_ns_values[:,1], label='1st', color = 'blue')\nplt.plot(X_ns_values, ortho_poly_ns_values[:,2], label='2nd', color = 'green')\nplt.plot(X_ns_values, ortho_poly_ns_values[:,3], label='3rd', color = 'red')\nplt.plot(X_ns_values, ortho_poly_ns_values[:,4], label='4th', color = 'orange')\nplt.title('Orthogonal Polynomial Basis Expansion of ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('h[ NS: ' + Xname[0] + ' (' + Xunit[0] + ') ]')\nplt.legend(); plt.xlim(-3,3); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlin_ortho4 = LinearRegression()                               # instantiate model\nlin_ortho4.fit(df_X_ns_ortho4.iloc[:,1:], y_ns)               # fit Hermite polynomials \nob1,ob2,ob3,ob4 = np.round(lin_ortho4.coef_,3)                # retrieve the model parameters\nob0 = lin_ortho4.intercept_\n\nplt.subplot(111)\nplt.plot(X_ns_values,lin_ortho4.predict(ortho_poly_ns_values[:,1:]),label='orthogonal polynomial',color = 'red') \nplt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.title('Orthogonal Polynomial Regression Model, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])\nplt.annotate('Orthogonal Polynomial Regression Model',[-2.8,2.6])\nplt.annotate(r'    $\\beta_4$ :' + str(round(ob4,3)),[-2.8,2.1])\nplt.annotate(r'    $\\beta_3$ :' + str(round(ob3,3)),[-2.8,1.7])\nplt.annotate(r'    $\\beta_2$ :' + str(round(ob2,3)),[-2.8,1.3])\nplt.annotate(r'    $\\beta_1$ :' + str(round(ob1,3)),[-2.8,0.9])\nplt.annotate(r'    $\\beta_0$ :' + str(round(ob0,2)),[-2.8,0.5])\nplt.annotate(r'$N[\\phi] = \\beta_4 \\times N[z]^4 + \\beta_3 \\times N[z]^3 + \\beta_2 \\times N[z]^2 + \\beta_1 \\times N[z] + \\beta_0$',[-1.0,-2.0])\nplt.annotate(r'$N[\\phi] = $' + str(ob4) + r' $\\times N[z]^4 +$ ' + str(ob3) + r' $\\times N[z]^3 +$ ' + str(ob2) + r' $\\times N[z]^2 +$ ' + \n             str(ob1) + r' $\\times N[z]$ + ' + str(round(ob0,2)),[-1.0,-2.5])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\norder=4                                                       # set the polynomial order\n\npolyreg_pipe=make_pipeline(PolynomialFeatures(order),LinearRegression()) # make the modeling pipeline\npolyreg_pipe.fit(X_ns.values.reshape(-1, 1), y_ns)            # fit the model to the data\ny_hat = polyreg_pipe.predict(X_ns_values.reshape(-1, 1))      # predict with the modeling pipeline\npoly_reg_model = polyreg_pipe.named_steps['linearregression'] # retrieve the model from the pipeline\npb0a,pb1,pb2,pb3,pb4 = np.round(poly_reg_model.coef_,3)       # retrieve the model parameters\npb0b = poly_reg_model.intercept_\npb0 = pb0a + pb0b\n\nplt.subplot(111)                                              # plot the data and model\nplt.plot(X_ns_values,y_hat, label='4th order',color = 'red') \nplt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.title(str(order) + r'$^{th}$ Polynomial Regression Model with Pipelines, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])\nplt.annotate('Orthogonal Polynomial Regression Model',[-2.8,2.6])\nplt.annotate(r'    $\\beta_4$ :' + str(round(pb4,3)),[-2.8,2.1])\nplt.annotate(r'    $\\beta_3$ :' + str(round(pb3,3)),[-2.8,1.7])\nplt.annotate(r'    $\\beta_2$ :' + str(round(pb2,3)),[-2.8,1.3])\nplt.annotate(r'    $\\beta_1$ :' + str(round(pb1,3)),[-2.8,0.9])\nplt.annotate(r'    $\\beta_0$ :' + str(round(pb0,2)),[-2.8,0.5])\nplt.annotate(r'$N[\\phi] = \\beta_4 \\times N[z]^4 + \\beta_3 \\times N[z]^3 + \\beta_2 \\times N[z]^2 + \\beta_1 \\times N[z] + \\beta_0$',[-1.0,-2.0])\nplt.annotate(r'$N[\\phi] = $' + str(pb4) + r' $\\times N[z]^4 +$ ' + str(pb3) + r' $\\times N[z]^3 +$ ' + str(pb2) + r' $\\times N[z]^2 +$ ' + \n             str(pb1) + r' $\\times N[z]$ + ' + str(round(pb0,2)),[-1.0,-2.5])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy                                                  # Hermite polynomials\nfrom scipy import stats                                       # statistical methods\nimport pandas as pd                                           # DataFrames\nimport pandas.plotting as pd_plot\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn.linear_model import LinearRegression             # linear regression with scikit learn\nfrom sklearn.preprocessing import PolynomialFeatures          # polynomial basis expansion\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.preprocessing import (StandardScaler,PolynomialFeatures) # standardize the features, polynomial basis expansion\nfrom sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning\nfrom sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # supress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5]) \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\ndf = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/1D_Porosity.csv\") # data from Dr. Pyrcz's github repository \n```", "```py\ndf.head(n=13)                                                 # preview the data \n```", "```py\ndf.describe(percentiles=[0.1,0.9]).transpose()                # summary statistics \n```", "```py\nXname = ['Depth']; yname = ['Nporosity']                      # select the predictor and response feature\n\nXlabel = ['Depth']; ylabel = ['Gaussian Transformed Porosity'] # specify the feature labels for plotting\nXunit = ['m']; yunit = ['N[%]']\nXlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')']\nylabelunit = ylabel[0] + ' (' + yunit[0] + ')'\n\nX = df[Xname[0]]                                              # extract the 1D ndarrays from the DataFrame\ny = df[yname[0]]\n\nXmin = 0.0; Xmax = 10.0                                       # limits for plotting\nymin = -3.0; ymax = 3.0\n\nX_values = np.linspace(Xmin,Xmax,100)                         # X intervals to visualize the model \n```", "```py\nlin = LinearRegression()                                      # instantiate linear regression object, note no hyperparameters \nlin.fit(X.values.reshape(-1, 1), y)                           # train linear regression model\n\nslope = lin.coef_[0]                                          # get the model parameters\nintercept = lin.intercept_\n\nplt.subplot(111)                                              # plot the data and the model\nplt.scatter(X,y,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.plot(X_values,intercept + slope*X_values,label='model',color = 'black')\nplt.title('Linear Regression Model, Regression of ' + yname[0] + ' on ' + Xname[0])\nplt.xlabel(Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel(yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])\nplt.annotate('Linear Regression Model',[4.5,-1.8])\nplt.annotate(r'    $\\beta_1$ :' + str(round(slope,2)),[6.8,-2.3])\nplt.annotate(r'    $\\beta_0$ :' + str(round(intercept,2)),[6.8,-2.7])\nplt.annotate(r'$N[\\phi] = \\beta_1 \\times z + \\beta_0$',[4.0,-2.3])\nplt.annotate(r'$N[\\phi] = $' + str(round(slope,2)) + r' $\\times$ $z$ + (' + str(round(intercept,2)) + ')',[4.0,-2.7])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nfrom sklearn import tree                                      # tree program from scikit learn \n\nmy_tree = tree.DecisionTreeRegressor(min_samples_leaf=5, max_depth = 20) # instantiate the decision tree model with hyperparameters\nmy_tree = my_tree.fit(X.values.reshape(-1, 1),y)              # fit the decision tree to the training data (all the data in this case)\nDT_y = my_tree.predict(X_values.reshape(-1,1))                # predict at high resolution over the range of depths\n\nplt.subplot(111)                                              # plot the model and data\nplt.scatter(X,y,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.plot(X_values, DT_y, label='model', color = 'black')\nplt.title('Decision Tree Model, ' + yname[0] + ' as a Function of ' + Xname[0])\nplt.xlabel(Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel(yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor            # random forest method\n\nmax_depth = 5                                                 # set the random forest hyperparameters\nnum_tree = 1000\nmax_features = 1\n\nmy_forest = RandomForestRegressor(max_depth=max_depth,random_state=seed,n_estimators=num_tree,max_features=max_features)\nmy_forest.fit(X = X.values.reshape(-1, 1), y = y)  \nRF_y = my_forest.predict(X_values.reshape(-1,1))\nplt.subplot(111)\nplt.scatter(X,y,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.plot(X_values, RF_y, label='model', color = 'black')\nplt.title('Random Forest Tree Model, ' + yname[0] + ' as a Function of ' + Xname[0])\nplt.xlabel(Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel(yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2)\nplt.show() \n```", "```py\nimport geostatspy.geostats as geostats                        # for Gaussian transform from GSLIB\n\ndf_ns = pd.DataFrame()   \ndf_ns[Xname[0]], tvPor, tnsPor = geostats.nscore(df, Xname[0]) # nscore transform for all facies porosity \ndf_ns[yname[0]], tvdepth, tnsdepth = geostats.nscore(df, yname[0]) # nscore transform for all facies permeability\nX_ns = df_ns[Xname[0]]; y_ns = df_ns[yname[0]]\nX_ns_values = np.linspace(-3.0,3.0,1000)                      # values to predict at in standard normal space \n```", "```py\nplt.subplot(221)                                              # plot original sand and shale porosity histograms\nplt.hist(df[Xname[0]], facecolor='red',bins=np.linspace(Xmin,Xmax,1000),histtype=\"stepfilled\",alpha=0.2,density=True,\n         cumulative=True,edgecolor='black',label='Original')\nplt.xlim([0.0,10.0]); plt.ylim([0,1.0])\nplt.xlabel(Xname[0] + ' (' + Xunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Original Depth')\nplt.legend(loc='upper left')\nplt.grid(True)\n\nplt.subplot(222)  \nplt.hist(df_ns[Xname[0]], facecolor='blue',bins=np.linspace(-3.0,3.0,1000),histtype=\"stepfilled\",alpha=0.2,density=True,\n         cumulative=True,edgecolor='black',label = 'NS')\nplt.xlim([-3.0,3.0]); plt.ylim([0,1.0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Nscore ' + Xname[0])\nplt.legend(loc='upper left')\nplt.grid(True)\n\nplt.subplot(223)                                        # plot nscore transformed sand and shale histograms\nplt.hist(df[yname[0]], facecolor='red',bins=np.linspace(ymin,ymax,1000),histtype=\"stepfilled\",alpha=0.2,density=True,\n         cumulative=True,edgecolor='black',label='Original')\nplt.xlim([-3.0,3.0]); plt.ylim([0,1.0])\nplt.xlabel(yname[0] + ' (' + yunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Original Porosity')\nplt.legend(loc='upper left')\nplt.grid(True)\n\nplt.subplot(224)                                        # plot nscore transformed sand and shale histograms\nplt.hist(df_ns[yname[0]], facecolor='blue',bins=np.linspace(-3.0,3.0,1000),histtype=\"stepfilled\",alpha=0.2,density=True,\n         cumulative=True,edgecolor='black',label = 'NS')\nplt.xlim([-3.0,3.0]); plt.ylim([0,1.0])\nplt.xlabel('NS: ' + yname[0] + ' (' + yunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Nscore ' + yname[0])\nplt.legend(loc='upper left')\nplt.grid(True)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=2.0, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nlin_ns = LinearRegression()                                   # instantiate linear regression object, note no hyperparameters \nlin_ns.fit(X_ns.values.reshape(-1, 1), y_ns)                  # train linear regression model\nslope_ns = lin_ns.coef_[0]                                    # get the model parameters\nintercept_ns = lin_ns.intercept_\n\nplt.subplot(111)                                              # plot the data and the model\nplt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.plot(X_ns_values,intercept_ns + slope_ns*X_ns_values,label='model',color = 'black')\nplt.title('Linear Regression Model, Regression of NS ' + yname[0] + ' on ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])\nplt.annotate('Linear Regression Model',[0.8,-1.8])\nplt.annotate(r'    $\\beta_1$ :' + str(round(slope_ns,2)),[1.8,-2.3])\nplt.annotate(r'    $\\beta_0$ :' + str(round(intercept_ns,2)),[1.8,-2.7])\nplt.annotate(r'$N[\\phi] = \\beta_1 \\times z + \\beta_0$',[0.5,-2.3])\nplt.annotate(r'$N[\\phi] = $' + str(round(slope_ns,2)) + r' $\\times$ $z$ + (' + str(round(intercept_ns,2)) + ')',[0.5,-2.7])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\npoly4 = PolynomialFeatures(degree = 4)                        # instantiate polynomial expansion \nX_ns_poly4 = poly4.fit_transform(X_ns.values.reshape(-1, 1))  # calculate the basis expansion for our dataset\ndf_X_ns_poly4 = pd.DataFrame({'Values':X_ns,'0th':X_ns_poly4[:,0],'1st':X_ns_poly4[:,1],'2nd':X_ns_poly4[:,2], \n                              '3rd':X_ns_poly4[:,3],'4th':X_ns_poly4[:,4]}) # make a new DataFrame from the vectors\ndf_X_ns_poly4 = pd.DataFrame({'Values':X_ns,'1st':X_ns_poly4[:,1],'2nd':X_ns_poly4[:,2], \n                              '3rd':X_ns_poly4[:,3],'4th':X_ns_poly4[:,4]}) # make a new DataFrame from the vectors\ndf_X_ns_poly4.head()                                          # preview the polynomial basis expansion with the original predictor feature \n```", "```py\ncorr_matrix = df_X_ns_poly4.iloc[:,1:].corr()                 # calculate the correlation matrix\n\nplt.subplot(111)\nplot_corr(corr_matrix,'Polynomial Expansion Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npoly4 = PolynomialFeatures(degree = 4)                        # instantiate polynomial expansion \nX_ns_poly4 = poly4.fit_transform(X_ns.values.reshape(-1, 1))  # calculate the basis expansion for our dataset\ndf_X_ns_poly4 = pd.DataFrame({'Values':X_ns,'0th':X_ns_poly4[:,0],'1st':X_ns_poly4[:,1],'2nd':X_ns_poly4[:,2], \n                              '3rd':X_ns_poly4[:,3],'4th':X_ns_poly4[:,4]}) # make a new DataFrame from the vectors\ndf_X_ns_poly4 = pd.DataFrame({'Values':X_ns,'1st':X_ns_poly4[:,1],'2nd':X_ns_poly4[:,2], \n                              '3rd':X_ns_poly4[:,3],'4th':X_ns_poly4[:,4]}) # make a new DataFrame from the vectors\ndf_X_ns_poly4.head()                                          # preview the polynomial basis expansion with the original predictor feature \n```", "```py\ncorr_matrix = df_X_ns_poly4.iloc[:,1:].corr()                 # calculate the correlation matrix\n\nplt.subplot(111)\nplot_corr(corr_matrix,'Polynomial Expansion Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nsns.pairplot(df_X_ns_poly4.iloc[:,1:],vars=['1st','2nd','3rd','4th'],markers='o', kind='reg',diag_kind='kde')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nplt.subplot(111)                                              # plot the polynomial basis expansion\nplt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,0],label='0th',color = 'black')\nplt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,1],label='1th',color = 'blue')\nplt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,2],label='2th',color = 'green')\nplt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,3],label='3th',color = 'red')\nplt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,4],label='4th',color = 'orange') \nplt.title('Polynomial Basis Expansion of ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('h[ NS: ' + Xname[0] + ' (' + Xunit[0] + ') ]')\nplt.legend(); plt.xlim(-3,3); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nprint('The averages of each basis expansion, 0 - 4th order = ' + str(stats.describe(X_ns_poly4)[2]) + '.') \n```", "```py\nThe averages of each basis expansion, 0 - 4th order = [1\\.         0.00536486 0.9458762  0.07336308 2.31077802]. \n```", "```py\nlin_poly4 = LinearRegression()                                # instantiate new linear model \nlin_poly4.fit(df_X_ns_poly4.iloc[:,1:], y_ns)                 # train linear model with polynomial expansion, polynomial regression\nb1,b2,b3,b4 = np.round(lin_poly4.coef_,3)                     # retrieve the model parameters\nb0 = lin_poly4.intercept_\n\nplt.subplot(111)\nplt.plot(X_ns_values,lin_poly4.predict(poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,1:]),label='polynomial',color = 'red') \nplt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.title('Polynomial Regression Model, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])\nplt.annotate('Polynomial Regression Model',[-2.8,2.6])\nplt.annotate(r'    $\\beta_4$ :' + str(round(b4,3)),[-2.8,2.1])\nplt.annotate(r'    $\\beta_3$ :' + str(round(b3,3)),[-2.8,1.7])\nplt.annotate(r'    $\\beta_2$ :' + str(round(b2,3)),[-2.8,1.3])\nplt.annotate(r'    $\\beta_1$ :' + str(round(b1,3)),[-2.8,0.9])\nplt.annotate(r'    $\\beta_0$ :' + str(round(b0,2)),[-2.8,0.5])\nplt.annotate(r'$N[\\phi] = \\beta_4 \\times N[z]^4 + \\beta_3 \\times N[z]^3 + \\beta_2 \\times N[z]^2 + \\beta_1 \\times N[z] + \\beta_0$',[-1.0,-2.0])\nplt.annotate(r'$N[\\phi] = $' + str(b4) + r' $\\times N[z]^4 +$ ' + str(b3) + r' $\\times N[z]^3 +$ ' + str(b2) + r' $\\times N[z]^2 +$ ' + \n             str(b1) + r' $\\times N[z]$ + ' + str(round(b0,2)),[-1.0,-2.5])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\norders4 = [1,2,3,4]                                           # specify the orders for Hermite basis expansion\nX_ns_hermite4 = scipy.special.eval_hermitenorm(orders4,X_ns.values.reshape(-1, 1), out=None) # Hermite polynomials for X \ndf_X_ns_hermite4 = pd.DataFrame({'value':X_ns.values,'1st':X_ns_hermite4[:,0],'2nd':X_ns_hermite4[:,1], \n                                     '3rd':X_ns_hermite4[:,2],'4th':X_ns_hermite4[:,3]}) # make a new DataFrame from the vectors\ndf_X_ns_hermite4.head() \n```", "```py\nhermite_corr_matrix = df_X_ns_hermite4.iloc[:,1:].corr()      # calculate correlation matrix of Hermite basis expansion of X\n\nplt.subplot(111)\nplot_corr(hermite_corr_matrix,'Hermite Polynomial Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nsns.pairplot(df_X_ns_hermite4.iloc[:,1:],vars=['1st','2nd','3rd','4th'],markers='o', kind='reg',diag_kind='kde')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nprint('The means of each basis expansion, 1 - 4th order = ' + str(stats.describe(X_ns_hermite4)[2]) + '.') \n```", "```py\nThe means of each basis expansion, 1 - 4th order = [ 0.00536486 -0.0541238   0.05726848 -0.36447919]. \n```", "```py\nplt.subplot(111)                                              # plot Hermite polynomials\nplt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,0],label='1st',color = 'blue')\nplt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,1],label='2nd',color = 'green')\nplt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,2],label='3rd',color = 'red')\nplt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,3],label='4th',color = 'orange')\nplt.title('Hermite Polynomial Basis Expansion of ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('h[ NS: ' + Xname[0] + ' (' + Xunit[0] + ') ]')\nplt.legend(); plt.xlim(-3,3); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlin_herm4 = LinearRegression()                                # instantiate model\nlin_herm4.fit(df_X_ns_hermite4.iloc[:,1:], y_ns)              # fit Hermite polynomials \nhb1,hb2,hb3,hb4 = np.round(lin_herm4.coef_,3)                 # retrieve the model parameters\nhb0 = lin_herm4.intercept_\nplt.subplot(111)                                              # plot data and model\nplt.plot(X_ns_values, lin_herm4.predict(scipy.special.eval_hermitenorm(orders4,X_ns_values.reshape(-1, 1), out=None)), \n         label='4th order',color = 'red') \nplt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.title('Hermite Polynomial Regression Model, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])\nplt.annotate('Hermite Polynomial Regression Model',[-2.8,2.6])\nplt.annotate(r'    $\\beta_4$ :' + str(round(hb4,3)),[-2.8,2.1])\nplt.annotate(r'    $\\beta_3$ :' + str(round(hb3,3)),[-2.8,1.7])\nplt.annotate(r'    $\\beta_2$ :' + str(round(hb2,3)),[-2.8,1.3])\nplt.annotate(r'    $\\beta_1$ :' + str(round(hb1,3)),[-2.8,0.9])\nplt.annotate(r'    $\\beta_0$ :' + str(round(hb0,2)),[-2.8,0.5])\nplt.annotate(r'$N[\\phi] = \\beta_4 \\times N[z]^4 + \\beta_3 \\times N[z]^3 + \\beta_2 \\times N[z]^2 + \\beta_1 \\times N[z] + \\beta_0$',[-1.0,-2.0])\nplt.annotate(r'$N[\\phi] = $' + str(hb4) + r' $\\times N[z]^4 +$ ' + str(hb3) + r' $\\times N[z]^3 +$ ' + str(hb2) + r' $\\times N[z]^2 +$ ' + \n             str(hb1) + r' $\\times N[z]$ + ' + str(round(hb0,2)),[-1.0,-2.5])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\n# functions taken (without modification) from http://davmre.github.io/blog/python/2013/12/15/orthogonal_poly\n# appreciation to Dave Moore for the great blog post on titled 'Orthogonal polynomial regression in Python'\n# functions are Dave's reimplementation of poly() from R\n\ndef ortho_poly_fit(x, degree = 1):\n    n = degree + 1\n    x = np.asarray(x).flatten()\n    if(degree >= len(np.unique(x))):\n            stop(\"'degree' must be less than number of unique points\")\n    xbar = np.mean(x)\n    x = x - xbar\n    X = np.fliplr(np.vander(x, n))\n    q,r = np.linalg.qr(X)\n\n    z = np.diag(np.diag(r))\n    raw = np.dot(q, z)\n\n    norm2 = np.sum(raw**2, axis=0)\n    alpha = (np.sum((raw**2)*np.reshape(x,(-1,1)), axis=0)/norm2 + xbar)[:degree]\n    Z = raw / np.sqrt(norm2)\n    return Z, norm2, alpha\n\ndef ortho_poly_predict(x, alpha, norm2, degree = 1):\n    x = np.asarray(x).flatten()\n    n = degree + 1\n    Z = np.empty((len(x), n))\n    Z[:,0] = 1\n    if degree > 0:\n        Z[:, 1] = x - alpha[0]\n    if degree > 1:\n        for i in np.arange(1,degree):\n             Z[:, i+1] = (x - alpha[i]) * Z[:, i] - (norm2[i] / norm2[i-1]) * Z[:, i-1]\n    Z /= np.sqrt(norm2)\n    return Z \n```", "```py\nX_ns_ortho4, norm2, alpha = ortho_poly_fit(X_ns.values.reshape(-1, 1), degree = 4) # orthogonal polynomial expansion\ndf_X_ns_ortho4 = pd.DataFrame({'value':X_ns.values,'1st':X_ns_ortho4[:,1],'2nd':X_ns_ortho4[:,2],'3rd':X_ns_ortho4[:,3],\n                               '4th':X_ns_ortho4[:,4]})       # make a new DataFrame from the vectors\ndf_X_ns_ortho4.head() \n```", "```py\northo_corr_matrix = df_X_ns_ortho4.iloc[:,1:].corr()          # calculate the correlation matrix\n\nplt.subplot(111)\nplot_corr(ortho_corr_matrix,'Orthogonal Polynomial Expansion Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nsns.pairplot(df_X_ns_ortho4.iloc[:,1:],vars=['1st','2nd','3rd','4th'],markers='o',kind='reg',diag_kind='kde') \n```", "```py\n<seaborn.axisgrid.PairGrid at 0x1ed608d8370> \n```", "```py\northo_poly_ns_values = ortho_poly_predict(X_ns_values.reshape(-1, 1), alpha, norm2, degree = 4)\n\nplt.subplot(111)\nplt.plot(X_ns_values, ortho_poly_ns_values[:,0], label='0th', color = 'black')\nplt.plot(X_ns_values, ortho_poly_ns_values[:,1], label='1st', color = 'blue')\nplt.plot(X_ns_values, ortho_poly_ns_values[:,2], label='2nd', color = 'green')\nplt.plot(X_ns_values, ortho_poly_ns_values[:,3], label='3rd', color = 'red')\nplt.plot(X_ns_values, ortho_poly_ns_values[:,4], label='4th', color = 'orange')\nplt.title('Orthogonal Polynomial Basis Expansion of ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('h[ NS: ' + Xname[0] + ' (' + Xunit[0] + ') ]')\nplt.legend(); plt.xlim(-3,3); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlin_ortho4 = LinearRegression()                               # instantiate model\nlin_ortho4.fit(df_X_ns_ortho4.iloc[:,1:], y_ns)               # fit Hermite polynomials \nob1,ob2,ob3,ob4 = np.round(lin_ortho4.coef_,3)                # retrieve the model parameters\nob0 = lin_ortho4.intercept_\n\nplt.subplot(111)\nplt.plot(X_ns_values,lin_ortho4.predict(ortho_poly_ns_values[:,1:]),label='orthogonal polynomial',color = 'red') \nplt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.title('Orthogonal Polynomial Regression Model, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])\nplt.annotate('Orthogonal Polynomial Regression Model',[-2.8,2.6])\nplt.annotate(r'    $\\beta_4$ :' + str(round(ob4,3)),[-2.8,2.1])\nplt.annotate(r'    $\\beta_3$ :' + str(round(ob3,3)),[-2.8,1.7])\nplt.annotate(r'    $\\beta_2$ :' + str(round(ob2,3)),[-2.8,1.3])\nplt.annotate(r'    $\\beta_1$ :' + str(round(ob1,3)),[-2.8,0.9])\nplt.annotate(r'    $\\beta_0$ :' + str(round(ob0,2)),[-2.8,0.5])\nplt.annotate(r'$N[\\phi] = \\beta_4 \\times N[z]^4 + \\beta_3 \\times N[z]^3 + \\beta_2 \\times N[z]^2 + \\beta_1 \\times N[z] + \\beta_0$',[-1.0,-2.0])\nplt.annotate(r'$N[\\phi] = $' + str(ob4) + r' $\\times N[z]^4 +$ ' + str(ob3) + r' $\\times N[z]^3 +$ ' + str(ob2) + r' $\\times N[z]^2 +$ ' + \n             str(ob1) + r' $\\times N[z]$ + ' + str(round(ob0,2)),[-1.0,-2.5])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\norder=4                                                       # set the polynomial order\n\npolyreg_pipe=make_pipeline(PolynomialFeatures(order),LinearRegression()) # make the modeling pipeline\npolyreg_pipe.fit(X_ns.values.reshape(-1, 1), y_ns)            # fit the model to the data\ny_hat = polyreg_pipe.predict(X_ns_values.reshape(-1, 1))      # predict with the modeling pipeline\npoly_reg_model = polyreg_pipe.named_steps['linearregression'] # retrieve the model from the pipeline\npb0a,pb1,pb2,pb3,pb4 = np.round(poly_reg_model.coef_,3)       # retrieve the model parameters\npb0b = poly_reg_model.intercept_\npb0 = pb0a + pb0b\n\nplt.subplot(111)                                              # plot the data and model\nplt.plot(X_ns_values,y_hat, label='4th order',color = 'red') \nplt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')\nplt.title(str(order) + r'$^{th}$ Polynomial Regression Model with Pipelines, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])\nplt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')\nplt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')\nplt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])\nplt.annotate('Orthogonal Polynomial Regression Model',[-2.8,2.6])\nplt.annotate(r'    $\\beta_4$ :' + str(round(pb4,3)),[-2.8,2.1])\nplt.annotate(r'    $\\beta_3$ :' + str(round(pb3,3)),[-2.8,1.7])\nplt.annotate(r'    $\\beta_2$ :' + str(round(pb2,3)),[-2.8,1.3])\nplt.annotate(r'    $\\beta_1$ :' + str(round(pb1,3)),[-2.8,0.9])\nplt.annotate(r'    $\\beta_0$ :' + str(round(pb0,2)),[-2.8,0.5])\nplt.annotate(r'$N[\\phi] = \\beta_4 \\times N[z]^4 + \\beta_3 \\times N[z]^3 + \\beta_2 \\times N[z]^2 + \\beta_1 \\times N[z] + \\beta_0$',[-1.0,-2.0])\nplt.annotate(r'$N[\\phi] = $' + str(pb4) + r' $\\times N[z]^4 +$ ' + str(pb3) + r' $\\times N[z]^3 +$ ' + str(pb2) + r' $\\times N[z]^2 +$ ' + \n             str(pb1) + r' $\\times N[z]$ + ' + str(round(pb0,2)),[-1.0,-2.5])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```"]