<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>14  Reporting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>14  Reporting</h1>
<blockquote>原文：<a href="https://ml-science-book.com/reporting.html">https://ml-science-book.com/reporting.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part-two.html">Integrating Machine Learning Into Science</a></li><li class="breadcrumb-item"><a href="./reporting.html"><span class="chapter-number">14</span>  <span class="chapter-title">Reporting</span></a></li></ol></nav>
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>If you don’t publish the results of your experiments, can it still be considered science? If you don’t share your results, others can’t verify or build on your work. That’s why publishing is so important in science. When you use machine learning in your research, there are additional needs for reporting. You may publish a paper but also report on the model or dataset you created. To make reporting easier, you can use checklists. Rather than just copying these checklists here – which would make for a terrible read – this chapter points you to the best checklists, their scope, and the ideas behind them.</p>
<div class="raven-box">
<p>After some years, other birds became interested in Rattle and Krarah’s tornado prediction model. The pigeons, the magpies, and, of course, the parrots wanted to repeat the Ravens’ success. But they had so many questions, such as what data was used in training? Krarah’s disciples were tired of answering the same questions over and over again. They decided to compile all the information about the model into one document and publish it.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../Images/31474697f6cd66f0bbc2e63e5587e077.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%" data-original-src="https://ml-science-book.com/images/raven-reporting.jpg"/></p>
</figure>
</div>
</div>
<section id="checklist-for-papers" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="checklist-for-papers"><span class="header-section-number">14.1</span> Checklist for papers</h2>
<p>If you use machine learning in your research project, what details should you include in your paper? One checklist you might want to use is REFORMS, which stands for “Reporting standards for machine learning-based science” <span class="citation" data-cites="kapoor2024reforms"><a href="references.html#ref-kapoor2024reforms" role="doc-biblioref">[1]</a></span>. The authors developed REFORMS based on literature reviews and a consensus of 19 researchers from different fields. REFORMS also includes guidelines on completing the checklist and identifying potential problems in your modeling approach. REFORMS is <em>field agnostic</em> – this means the checklist items are broadly applicable, but some may be missing depending on your needs.</p>
<p>The REFORMS checklist consists of 8 modules (categories) and a total of 32 items:</p>
<ul>
<li><strong>Study goals.</strong> Example: <em>Motivation for the use of machine learning methods in the study</em></li>
<li><strong>Computational reproducibility.</strong> Example: <em>README file which contains instructions for generating the results using the provided dataset and code</em></li>
<li><strong>Data quality.</strong> Example: <em>Sample size and outcome frequencies</em></li>
<li><strong>Data preprocessing.</strong> Example: <em>Identification of whether any samples are excluded with a rationale for why they are excluded</em></li>
<li><strong>Modeling.</strong> Example: <em>Method for selecting the model(s) reported in the paper</em></li>
<li><strong>Data leakage.</strong> Example: <em>Justification that each feature or input used in the model is legitimate for the task at hand and does not lead to leakage</em></li>
<li><strong>Metrics and uncertainty.</strong> Example: <em>Justification for the choice of statistical tests (if used) and a check for the assumptions of the statistical test</em></li>
<li><strong>Generalizability and limitations.</strong> Example: <em>Evidence of external validity</em></li>
</ul>
<p>Further resources:</p>
<ul>
<li><a href="https://reforms.cs.princeton.edu/">REFORMS Website</a></li>
<li><a href="https://reforms.cs.princeton.edu/checklist.docx">REFORMS Checklist (Word Document)</a></li>
<li><a href="https://reforms.cs.princeton.edu/guidelines.docx">REFORMS Guidelines (Word Document)</a></li>
<li><a href="https://www.science.org/doi/epdf/10.1126/sciadv.adk3452">REFORMS Paper</a></li>
</ul>
<p>Some papers, especially in the medical field may require additional attention:</p>
<ul>
<li>TRIPOD+AI <span class="citation" data-cites="Collinse078378"><a href="references.html#ref-Collinse078378" role="doc-biblioref">[2]</a></span> is a publication approach for clinical prediction models. It comes with a <a href="https://www.tripod-statement.org/wp-content/uploads/2019/12/TRIPODAI_checklist.pdf">checklist</a>.</li>
<li><a href="https://pubs.rsna.org/page/ai/claim">CLAIM for AI</a> provides a checklist for AI papers about medical imaging <span class="citation" data-cites="tejani2024checklist"><a href="references.html#ref-tejani2024checklist" role="doc-biblioref">[3]</a></span>.</li>
</ul>
</section>
<section id="model-cards" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="model-cards"><span class="header-section-number">14.2</span> Model Cards</h2>
<!-- motivation for model cards -->
<p>Besides publishing a paper, you may want to share the model. You could just upload the model somewhere (like Github) and call it a day. The problem: Other researchers and interested parties will have trouble working with your model. A standard README file might be too short to describe the model, and a research paper might be too long and in the wrong format. The solution: Create a special document with your model such as a Model Card. “Model Cards” by Google Research <span class="citation" data-cites="mitchell2019model"><a href="references.html#ref-mitchell2019model" role="doc-biblioref">[4]</a></span> refers to both the approach and the resulting documents. Model Cards is a checklist approach used by many institutions, including <a href="https://huggingface.co">HuggingFace</a> a large model hub and community for machine learning.</p>
<!-- describing model cards -->
<p>The original Model Cards paper introduces 10 categories each with multiple details prompts:</p>
<ul>
<li><strong>Model Details.</strong> Example: <em>Model date</em></li>
<li><strong>Intended Use.</strong> Example: <em>Primary intended use</em></li>
<li><strong>Factors.</strong> Example: <em>Evaluation Factors</em></li>
<li><strong>Metrics.</strong> Example: <em>Decision Thresholds</em></li>
<li><strong>Evaluation Data.</strong> Example: <em>Datasets</em></li>
<li><strong>Training Data</strong></li>
<li><strong>Quantitative Analyses</strong></li>
<li><strong>Ethical Considerations</strong></li>
<li><strong>Caveats and Recommendations</strong></li>
</ul>
<p>The Model Cards approach is constantly evolving. For example, the Model Cards Guidebook <span class="citation" data-cites="ozoani2022model"><a href="references.html#ref-ozoani2022model" role="doc-biblioref">[5]</a></span> and template (see below) contain new and renamed categories. Model Cards is a great starting point for model documentation, but you should always customize it to your needs.</p>
<p>Further resources:</p>
<ul>
<li><a href="https://arxiv.org/abs/1810.03993">Model Card paper</a> <span class="citation" data-cites="mitchell2019model"><a href="references.html#ref-mitchell2019model" role="doc-biblioref">[4]</a></span></li>
<li><a href="https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md">Model Card template</a></li>
<li><a href="https://huggingface.co/docs/hub/en/model-card-guidebook">Model Cards Guidebook</a></li>
</ul>
<p>Again, you may have more specific documentation requirements. For example, medical devices using machine learning have special reporting requirements, see <a href="https://github.com/johner-institut/ai-guideline/blob/master/Guideline-AI-Medical-Devices_EN.md">this guideline</a> (focus on Germany).</p>
</section>
<section id="datasheets-for-datasets" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="datasheets-for-datasets"><span class="header-section-number">14.3</span> Datasheets for Datasets</h2>
<p>Carefully built and curated datasets are at the heart of machine learning. A lot of time and effort can go into collecting and cleaning data. Sometimes years. To maximize your return on investment, you might publish the dataset so that others can use it and cite your work. But just like the model, the dataset will raise a lot of questions if it doesn’t come with the right information. When it comes to documentation, it is not wise to reinvent the wheel but to stand on the shoulders of giants. In this case, “Datasheets for Datasets” <span class="citation" data-cites="gebru2021datasheets"><a href="references.html#ref-gebru2021datasheets" role="doc-biblioref">[6]</a></span> is a popular approach to “facilitate communication between datasets creators and consumers.”</p>
<p>Datasheets for Datasets lists 57 questions in 7 categories:</p>
<ul>
<li><strong>Motivation.</strong> Example: <em>For what purpose was the dataset created?</em></li>
<li><strong>Composition.</strong> Example: <em>What data does each instance consist of?</em></li>
<li><strong>Collection Process.</strong> Example: <em>How was the data associated with each instance acquired?</em></li>
<li><strong>Preprocessing/cleaning/labeling.</strong> Example: <em>Is the software that was used to preprocess/clean/label the data available?</em></li>
<li><strong>Uses.</strong> Example: <em>Has the dataset been used for any tasks already?</em></li>
<li><strong>Distribution.</strong> Example: <em>How will the dataset be distributed (e.g., tarball on website, API, GitHub)?</em></li>
<li><strong>Maintenance.</strong> Example: <em>How can the owner/curator/manager of the dataset be contacted (e.g., email address)?</em></li>
</ul>
<p>Resources:</p>
<ul>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3458723">Datasheets paper (PDF)</a> <span class="citation" data-cites="gebru2021datasheets"><a href="references.html#ref-gebru2021datasheets" role="doc-biblioref">[6]</a></span></li>
<li><a href="https://github.com/fau-masters-collected-works-cgarbin/datasheet-for-dataset-template/tree/master">Datasheets template (Markdown)</a></li>
<li><a href="https://www.overleaf.com/latex/templates/datasheet-for-dataset-template/jgqyyzyprxth">Datasheets template (Latex/Overleaf)</a></li>
</ul>
</section>
<section id="reporting-relies-on-good-practice" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="reporting-relies-on-good-practice"><span class="header-section-number">14.4</span> Reporting relies on good practice</h2>
<p>If you want to document your research project well, you need to follow good research practices that go hand in hand with other topics in this book:</p>
<ul>
<li>Documenting the research project, model, and dataset forces you to think about use cases and limitations, which is closely related to how well your model generalizes to different contexts (<a href="generalization.html" class="quarto-xref"><span>Chapter 7</span></a>).</li>
<li>Publishing results from your model may require model insights, for which you may need interpretability (<a href="interpretability.html" class="quarto-xref"><span>Chapter 9</span></a>).</li>
<li>Sharing models and data is also an integral part of reproducibility (<a href="reproducibility.html" class="quarto-xref"><span>Chapter 13</span></a>).</li>
</ul>


<div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-kapoor2024reforms" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">S. Kapoor <em>et al.</em>, <span>“<span>REFORMS</span>: <span>Consensus</span>-based <span>Recommendations</span> for <span>Machine</span>-learning-based <span>Science</span>,”</span> <em>Science Advances</em>, vol. 10, no. 18, p. eadk3452, May 2024, doi: <a href="https://doi.org/10.1126/sciadv.adk3452">10.1126/sciadv.adk3452</a>.</div>
</div>
<div id="ref-Collinse078378" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">G. S. Collins <em>et al.</em>, <span>“<span>TRIPOD</span>+<span>AI</span> statement: Updated guidance for reporting clinical prediction models that use regression or machine learning methods,”</span> <em>BMJ (Clinical research ed.)</em>, vol. 385, 2024, doi: <a href="https://doi.org/10.1136/bmj-2023-078378">10.1136/bmj-2023-078378</a>.</div>
</div>
<div id="ref-tejani2024checklist" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">A. S. Tejani <em>et al.</em>, <span>“Checklist for <span>Artificial</span> <span>Intelligence</span> in <span>Medical</span> <span>Imaging</span> (<span>CLAIM</span>): 2024 <span>Update</span>,”</span> <em>Radiology: Artificial Intelligence</em>, vol. 6, no. 4, p. e240300, Jul. 2024, doi: <a href="https://doi.org/10.1148/ryai.240300">10.1148/ryai.240300</a>.</div>
</div>
<div id="ref-mitchell2019model" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">M. Mitchell <em>et al.</em>, <span>“Model <span>Cards</span> for <span>Model</span> <span>Reporting</span>,”</span> in <em>Proceedings of the <span>Conference</span> on <span>Fairness</span>, <span>Accountability</span>, and <span>Transparency</span></em>, in <span>FAT</span>* ’19. New York, NY, USA: Association for Computing Machinery, Jan. 2019, pp. 220–229. doi: <a href="https://doi.org/10.1145/3287560.3287596">10.1145/3287560.3287596</a>.</div>
</div>
<div id="ref-ozoani2022model" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">E. Ozoani, M. Gerchick, and M. Mitchell, <em>Model card guidebook</em>. Hugging Face, 2022. Available: <a href="https://huggingface.co/docs/hub/en/model-card-guidebook">https://huggingface.co/docs/hub/en/model-card-guidebook</a></div>
</div>
<div id="ref-gebru2021datasheets" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">T. Gebru <em>et al.</em>, <span>“Datasheets for datasets,”</span> <em>Communications of the ACM</em>, vol. 64, no. 12, pp. 86–92, 2021, doi: <a href="https://doi.org/10.1145/3458723">10.1145/3458723</a>.</div>
</div>
</div>
</section>

    
</body>
</html>