["```py\ndef f(x):\n    a = x * x  # Square\n    b = sin(x)  # Sine\n    return a * b  # Product\n```", "```py\ndef f(x):  # Computing both value and derivative\n    # Step 1: x -> x²\n    a = x * x  # Value: x²\n    da = 2 * x  # Derivative: 2x\n\n    # Step 2: x -> sin(x)\n    b = sin(x)  # Value: sin(x)\n    db = cos(x)  # Derivative: cos(x)\n\n    # Step 3: Combine using product rule\n    result = a * b  # Value: x² * sin(x)\n    dresult = a * db + b * da  # Derivative: x²*cos(x) + sin(x)*2x\n\n    return result, dresult\n```", "```py\nx = 2.0  # Initial value\ndx = 1.0  # We're tracking derivative with respect to x\n\n# Step 1: x²\na = 4.0  # (2.0)²\nda = 4.0  # 2 * 2.0\n\n# Step 2: sin(x)\nb = 0.909  # sin(2.0)\ndb = -0.416  # cos(2.0)\n\n# Final result\nresult = 3.637  # 4.0 * 0.909\ndresult = 2.805  # 4.0 * (-0.416) + 0.909 * 4.0\n```", "```py\ndef f(x):\n    a = x * x\n    b = sin(x)\n    return a * b\n```", "```py\n# Conceptually, each computation tracks (value, derivative)\nx = (2.0, 1.0)  # Input value and its derivative\na = (4.0, 4.0)  # x² and its derivative 2x\nb = (0.909, -0.416)  # sin(x) and its derivative cos(x)\nresult = (3.637, 2.805)  # Final value and derivative\n```", "```py\ndef analyze_image_sensitivity(model, image):\n    # Forward mode tracks how changing one pixel\n    # affects the final classification\n    layer1 = relu(W1 @ image + b1)\n    layer2 = relu(W2 @ layer1 + b2)\n    predictions = softmax(W3 @ layer2 + b3)\n    return predictions\n```", "```py\ndef compute_feature_importance(model, input_features):\n    # Track influence of each input feature\n    # through the network's computation\n    hidden = tanh(W1 @ input_features + b1)\n    logits = W2 @ hidden + b2\n    # Forward mode efficiently computes d(logits)/d(input)\n    return logits\n```", "```py\ndef f(x):\n    a = x * x  # First operation: square x\n    b = sin(x)  # Second operation: sine of x\n    c = a * b  # Third operation: multiply results\n    return c\n```", "```py\n x = 2.0             # Our input value\n a = 4.0             # x * x = 2.0 * 2.0 = 4.0\n b = 0.909           # sin(2.0) ≈ 0.909\n c = 3.637           # a * b = 4.0 * 0.909 ≈ 3.637\n```", "```py\n#| eval: false\ndc/dc = 1.0    # Derivative of output with respect to itself is 1\n\n# Moving backward through multiplication c = a * b\ndc/da = b      # ∂(a*b)/∂a = b = 0.909\ndc/db = a      # ∂(a*b)/∂b = a = 4.0\n\n# Finally, combining derivatives for x through both paths\n# Path 1: x -> x² -> c    contribution: 2x * dc/da\n# Path 2: x -> sin(x) -> c contribution: cos(x) * dc/db\ndc/dx = (2 * x * dc/da) + (cos(x) * dc/db)\n      = (2 * 2.0 * 0.909) + (cos(2.0) * 4.0)\n      = 3.636 + (-0.416 * 4.0)\n      = 2.805\n```", "```py\ndef simple_network(x, w1, w2):\n    # Forward pass\n    hidden = x * w1  # First layer multiplication\n    activated = max(0, hidden)  # ReLU activation\n    output = activated * w2  # Second layer multiplication\n    return output  # Final output (before loss)\n```", "```py\nx = 1.0\nw1 = 2.0\nw2 = 3.0\n\nhidden = 2.0  # x * w1 = 1.0 * 2.0\nactivated = 2.0  # max(0, 2.0) = 2.0\noutput = 6.0  # activated * w2 = 2.0 * 3.0\n```", "```py\nd_output = 1.0  # Start with derivative of output\n\nd_w2 = activated  # d_output * d(output)/d_w2\n# = 1.0 * 2.0 = 2.0\nd_activated = w2  # d_output * d(output)/d_activated\n# = 1.0 * 3.0 = 3.0\n\n# ReLU gradient: 1 if input was > 0, 0 otherwise\nd_hidden = d_activated * (1 if hidden > 0 else 0)\n# 3.0 * 1 = 3.0\n\nd_w1 = x * d_hidden  # 1.0 * 3.0 = 3.0\nd_x = w1 * d_hidden  # 2.0 * 3.0 = 6.0\n```", "```py\ndef deep_network(x, w1, w2, w3):\n    # Forward pass - must store intermediates\n    hidden1 = x * w1\n    activated1 = max(0, hidden1)  # Store for backward\n    hidden2 = activated1 * w2\n    activated2 = max(0, hidden2)  # Store for backward\n    output = activated2 * w3\n    return output\n```", "```py\ndef training_step(model, input_batch):\n    # Strategy 1: Checkpointing\n    with checkpoint_scope():\n        hidden1 = activation(layer1(input_batch))\n        # Framework might free some memory here\n        hidden2 = activation(layer2(hidden1))\n        # More selective memory management\n        output = layer3(hidden2)\n\n    # Strategy 2: Gradient accumulation\n    loss = compute_loss(output)\n    # Backward pass with managed memory\n    loss.backward()\n```", "```py\ndef deep_network(input_tensor):\n    # A typical deep network computation\n    layer1 = large_dense_layer(input_tensor)\n    activation1 = relu(layer1)\n    layer2 = large_dense_layer(activation1)\n    activation2 = relu(layer2)\n    # ... many more layers\n    output = final_layer(activation_n)\n    return output\n```", "```py\n# Conceptual representation of checkpointing\ncheckpoint1 = save_for_backward(activation1)\n# Intermediate activations can be recomputed\ncheckpoint2 = save_for_backward(activation4)\n# Framework balances storage vs recomputation\n```", "```py\n# PyTorch-style automatic differentiation\ndef neural_network(x):\n    # Framework transparently tracks operations\n    layer1 = nn.Linear(784, 256)\n    layer2 = nn.Linear(256, 10)\n\n    # Each operation is automatically tracked\n    hidden = torch.relu(layer1(x))\n    output = layer2(hidden)\n    return output\n\n\n# Training loop showing AD integration\nfor batch_x, batch_y in data_loader:\n    optimizer.zero_grad()  # Clear previous gradients\n    output = neural_network(batch_x)\n    loss = loss_function(output, batch_y)\n\n    # Framework handles all AD machinery\n    loss.backward()  # Automatic backward pass\n    optimizer.step()  # Parameter updates\n```", "```py\n# Computing higher-order gradients\nwith torch.set_grad_enabled(True):\n    # First-order gradient computation\n    output = model(input)\n    grad_output = torch.autograd.grad(output, model.parameters())\n\n    # Second-order gradient computation\n    grad2_output = torch.autograd.grad(\n        grad_output, model.parameters()\n    )\n```", "```py\ndef neural_network(x):\n    # Each operation creates values that must be remembered\n    a = layer1(x)  # Must store for backward pass\n    b = relu(a)  # Must store input to relu\n    c = layer2(b)  # Must store for backward pass\n    return c\n```", "```py\n# A deeper network shows the accumulating memory needs\nhidden1 = large_matrix_multiply(input, weights1)\nactivated1 = relu(hidden1)\nhidden2 = large_matrix_multiply(activated1, weights2)\nactivated2 = relu(hidden2)\noutput = large_matrix_multiply(activated2, weights3)\n```", "```py\ndef train_epoch(model, data_loader):\n    for batch_x, batch_y in data_loader:\n        # Moving data between CPU and accelerator\n        batch_x = batch_x.to(device)\n        batch_y = batch_y.to(device)\n\n        # Forward pass builds computational graph\n        outputs = model(batch_x)\n        loss = criterion(outputs, batch_y)\n\n        # Backward pass computes gradients\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n```", "```py\ndef parallel_network(x):\n    # These operations could run concurrently\n    branch1 = conv_layer1(x)\n    branch2 = conv_layer2(x)\n\n    # Must synchronize for combination\n    combined = branch1 + branch2\n    return final_layer(combined)\n```", "```py\nimport torch\n\n# PyTorch builds computational graph during execution\nx = torch.tensor(2.0, requires_grad=True)\ny = torch.tensor(3.0, requires_grad=True)\n\n# Each operation adds to the dynamic tape\nz = x * y  # Creates MulBackward node\nw = z + x  # Creates AddBackward node\nloss = w**2  # Creates PowBackward node\n\n# Graph exists only after forward pass completes\nprint(f\"Computation graph: {loss.grad_fn}\")\n# Output: <PowBackward0 object>\n\n# Backward pass traverses the dynamically built graph\nloss.backward()\nprint(f\"dx/dloss = {x.grad}\")  # Immediate access to gradients\nprint(f\"dy/dloss = {y.grad}\")\n```", "```py\ndef dynamic_model(x, condition):\n    # Computation graph varies based on runtime conditions\n    hidden = torch.relu(torch.mm(x, weights1))\n\n    if condition > 0.5:  # Runtime decision affects graph structure\n        # More complex computation path\n        hidden = torch.relu(torch.mm(hidden, weights2))\n        hidden = torch.relu(torch.mm(hidden, weights3))\n\n    output = torch.mm(hidden, final_weights)\n    return output\n\n\n# Different calls create different computational graphs\nresult1 = dynamic_model(input_data, 0.3)  # Shorter graph\nresult2 = dynamic_model(input_data, 0.7)  # Longer graph\n\n# Both handle backpropagation correctly despite different structures\n```", "```py\nimport tensorflow.compat.v1 as tf\n\ntf.disable_v2_behavior()\n\n# Graph definition phase - no actual computation\nx = tf.placeholder(tf.float32, shape=())\ny = tf.placeholder(tf.float32, shape=())\n\n# Define computation symbolically\nz = x * y\nw = z + x\nloss = w**2\n\n# Symbolic gradient computation during graph construction\ngradients = tf.gradients(loss, [x, y])\n\n# Execution phase - actual computation occurs\nwith tf.Session() as sess:\n    # Same graph can be executed multiple times efficiently\n    for step in range(1000):\n        grad_vals, loss_val = sess.run(\n            [gradients, loss], feed_dict={x: 2.0, y: 3.0}\n        )\n        # Optimized execution with compiled kernels\n```", "```py\nimport jax\nimport jax.numpy as jnp\n\n\n# Pure function definition\ndef compute_loss(params, x, y):\n    z = x * params[\"w1\"] + y * params[\"w2\"]\n    return z**2\n\n\n# JAX transforms functions rather than tracking operations\ngrad_fn = jax.grad(compute_loss)  # Returns gradient function\nvalue_and_grad_fn = jax.value_and_grad(compute_loss)\n\n# Multiple gradient modes available\nforward_grad_fn = jax.jacfwd(compute_loss)  # Forward mode\nreverse_grad_fn = jax.jacrev(compute_loss)  # Reverse mode\n\n# Function transformations compose naturally\nbatched_grad_fn = jax.vmap(grad_fn)  # Vectorized gradients\njit_grad_fn = jax.jit(grad_fn)  # Compiled gradients\n\n# Execution with immutable parameters\nparams = {\"w1\": 2.0, \"w2\": 3.0}\ngradients = grad_fn(params, 1.0, 2.0)\nprint(f\"Gradients: {gradients}\")\n```", "```py\n# Compose multiple transformations\ndef model_step(params, batch_x, batch_y):\n    predictions = model_forward(params, batch_x)\n    return compute_loss(predictions, batch_y)\n\n\n# Build complex training function through composition\nbatch_grad_fn = jax.vmap(jax.grad(model_step), in_axes=(None, 0, 0))\ncompiled_batch_grad_fn = jax.jit(batch_grad_fn)\nparallel_batch_grad_fn = jax.pmap(compiled_batch_grad_fn)\n\n# Result: vectorized, compiled, parallelized gradient function\n# Created through simple function transformations\n```", "```py\ndef computation(x, w):\n    # Framework tracks operations\n    hidden = x * w  # Stored for backward pass\n    output = relu(hidden)  # Tracks activation pattern\n    return output\n```", "```py\nloss = model(input)  # Forward pass tracks computation\nloss.backward()  # Triggers efficient reverse mode AD\noptimizer.step()  # Uses computed gradients\n```", "```py\ndef neural_network(x):\n    hidden = w1 * x  # What exactly is x?\n    activated = relu(hidden)  # How is hidden stored?\n    output = w2 * activated  # What type of multiplication?\n    return output\n```", "```py\nimport tensorflow.compat.v1 as tf\n\ntf.disable_v2_behavior()\n\n# Expressions are constructed but not evaluated\nweights = tf.Variable(tf.random.normal([784, 10]))\ninput_data = tf.placeholder(tf.float32, [None, 784])\noutput = tf.matmul(input_data, weights)\n\n# Separate evaluation phase\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    result = sess.run(output, feed_dict={input_data: data})\n```", "```py\n# Each expression evaluates immediately\nweights = torch.randn(784, 10)\ninput = torch.randn(32, 784)\noutput = input @ weights  # Computation occurs now\n```", "```py\nimport tensorflow as tf\n\nx = tf.constant([[1.0, 2.0], [3.0, 4.0]])\ny = tf.constant([[1, 2], [3, 4]])\nz = tf.matmul(x, y)\nprint(z)\n```", "```py\nimport tensorflow.compat.v1 as tf\n\ntf.disable_eager_execution()\n\n# Define the graph\nx = tf.placeholder(tf.float32, shape=(2, 2))\ny = tf.placeholder(tf.float32, shape=(2, 2))\nz = tf.matmul(x, y)\n\n# Execute the graph\nwith tf.Session() as sess:\n    result = sess.run(\n        z,\n        feed_dict={x: [[1.0, 2.0], [3.0, 4.0]], y: [[1, 2], [3, 4]]},\n    )\n    print(result)\n```", "```py\nimport torch\n\n\n@torch.jit.script\ndef compute(x, y):\n    return torch.matmul(x, y)\n\n\nx = torch.randn(2, 2)\ny = torch.randn(2, 2)\n\n# First call compiles the function\nresult = compute(x, y)\nprint(result)\n\n# Subsequent calls use the optimized version\nresult = compute(x, y)\nprint(result)\n```", "```py\nimport torch\n\n# Manual tensor operations\nx = torch.randn(2, 3)\nw = torch.randn(3, 4)\nb = torch.randn(4)\ny = torch.matmul(x, w) + b\n\n# Manual gradient computation\ny.backward(torch.ones_like(y))\n```", "```py\nimport torch.nn as nn\n\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(3, 64, kernel_size=3)\n        self.fc = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = self.fc(x)\n        return x\n```", "```py\nfrom tensorflow import keras\n\nmodel = keras.Sequential(\n    [\n        keras.layers.Conv2D(\n            64, 3, activation=\"relu\", input_shape=(32, 32, 3)\n        ),\n        keras.layers.Flatten(),\n        keras.layers.Dense(10),\n    ]\n)\n\n# Automated training workflow\nmodel.compile(\n    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\"\n)\nmodel.fit(train_data, train_labels, epochs=10)\n```", "```py\nimport torch\nimport torch.nn as nn\n\n# Create a simple neural network\nmodel = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 1))\n\n# Define loss function and optimizer\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# Forward pass, compute loss, and backward pass\nx = torch.randn(32, 10)\ny = torch.randn(32, 1)\ny_pred = model(x)\nloss = loss_fn(y_pred, y)\nloss.backward()\noptimizer.step()\n```", "```py\n# PyTorch - Dynamic, Pythonic\nimport torch.nn as nn\n\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(10, 1)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# TensorFlow/Keras - High-level API\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential(\n    [tf.keras.layers.Dense(1, input_shape=(10,))]\n)\n\n# JAX - Functional approach\nimport jax.numpy as jnp\nfrom jax import random\n\n\ndef simple_net(params, x):\n    return jnp.dot(x, params[\"w\"]) + params[\"b\"]\n\n\nkey = random.PRNGKey(0)\nparams = {\n    \"w\": random.normal(key, (10, 1)),\n    \"b\": random.normal(key, (1,)),\n}\n```"]