["```py\nsudo apt update\nsudo apt upgrade -y\n```", "```py\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\n```", "```py\npython3 -m venv ~/tflite\nsource ~/tflite/bin/activate\n```", "```py\npip install tflite_runtime --no-deps\n```", "```py\npip3 uninstall numpy\n```", "```py\n pip3 install numpy==1.23.2\n```", "```py\npip3 install Pillow matplotlib\n```", "```py\nmkdir Documents\ncd Documents/\nmkdir TFLITE\ncd TFLITE/\nmkdir IMG_CLASS\ncd IMG_CLASS\nmkdir models\ncd models\n```", "```py\n# One long line, split with backslash \\\nwget https://storage.googleapis.com/download.tensorflow.org/\\\nmodels/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz\n\ntar xzf mobilenet_v2_1.0_224_quant.tgz\n```", "```py\nwget https://raw.githubusercontent.com/Mjrovai/EdgeML-with-Raspberry-Pi/refs/heads/\\\nmain/IMG_CLASS/models/labels.txt\n```", "```py\npip3 install jupyter\njupyter notebook --generate-config\n```", "```py\njupyter notebook --ip=192.168.4.210 --no-browser\n```", "```py\nimport tflite_runtime.interpreter as tflite\nimport numpy as np\nfrom PIL import Image\n\nprint(\"NumPy:\", np.__version__)\nprint(\"Pillow:\", Image.__version__)\n\n# Try to create a TFLite Interpreter\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nprint(\"TFLite Interpreter created successfully!\")\n```", "```py\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\n```", "```py\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n```", "```py\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n```", "```py\ninput_dtype = input_details[0][\"dtype\"]\ninput_dtype\n```", "```py\ndtype('uint8')\n```", "```py\nmkdir images\ncd images\nwget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\n```", "```py\n# Load he image\nimg_path = \"./images/Cat03.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\n```", "```py\nwidth, height = img.size\n```", "```py\nimg = img.resize(\n    (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n)\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape\n```", "```py\ninput_data.dtype\n```", "```py\ndtype('uint8')\n```", "```py\ninterpreter.set_tensor(input_details[0][\"index\"], input_data)\ninterpreter.invoke()\npredictions = interpreter.get_tensor(output_details[0][\"index\"])[0]\n```", "```py\ntop_k_results = 5\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\ntop_k_indices\n```", "```py\ndef load_labels(filename):\n    with open(filename, \"r\") as f:\n        return [line.strip() for line in f.readlines()]\n```", "```py\nlabels_path = \"./models/labels.txt\"\nlabels = load_labels(labels_path)\n\nprint(labels[286])\nprint(labels[283])\nprint(labels[282])\nprint(labels[288])\nprint(labels[479])\n```", "```py\nEgyptian cat\ntiger cat\ntabby\nlynx\ncarton\n```", "```py\nscale, zero_point = output_details[0][\"quantization\"]\ndequantized_output = (\n    predictions.astype(np.float32) - zero_point\n) * scale\nexp_output = np.exp(dequantized_output - np.max(dequantized_output))\nprobabilities = exp_output / np.sum(exp_output)\n```", "```py\nprint(probabilities[286])\nprint(probabilities[283])\nprint(probabilities[282])\nprint(probabilities[288])\nprint(probabilities[479])\n```", "```py\n0.27741462\n0.3732285\n0.16919471\n0.10319158\n0.023410844\n```", "```py\nfor i in range(top_k_results):\n    print(\n        \"\\t{:20}: {}%\".format(\n            labels[top_k_indices[i]],\n            (int(probabilities[top_k_indices[i]] * 100)),\n        )\n    )\n```", "```py\ntiger cat           : 37%\nEgyptian cat        : 27%\ntabby               : 16%\nlynx                : 10%\ncarton              : 2%\n```", "```py\ndef image_classification(\n    img_path, model_path, labels, top_k_results=5\n):\n    # load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis(\"off\")\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Preprocess\n    img = img.resize(\n        (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n    )\n    input_data = np.expand_dims(img, axis=0)\n\n    # Inference on Raspi-Zero\n    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n    interpreter.invoke()\n\n    # Obtain results and map them to the classes\n    predictions = interpreter.get_tensor(output_details[0][\"index\"])[\n        0\n    ]\n\n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n    # Get quantization parameters\n    scale, zero_point = output_details[0][\"quantization\"]\n\n    # Dequantize the output and apply softmax\n    dequantized_output = (\n        predictions.astype(np.float32) - zero_point\n    ) * scale\n    exp_output = np.exp(\n        dequantized_output - np.max(dequantized_output)\n    )\n    probabilities = exp_output / np.sum(exp_output)\n\n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\n            \"\\t{:20}: {}%\".format(\n                labels[top_k_indices[i]],\n                (int(probabilities[top_k_indices[i]] * 100)),\n            )\n        )\n```", "```py\n    source ~/tflite/bin/activate\n    ```", "```py\n    echo \"/usr/lib/python3/dist-packages\" > \\\n     $VIRTUAL_ENV/lib/python3.11/\n    site-packages/system_site_packages.pth\n    ```", "```py\n    python3\n    >>> import picamera2\n    >>> print(picamera2.__file__)\n    ```", "```py\n/home/mjrovai/tflite/lib/python3.11/site-packages/\\\npicamera2/__init__.py\n```", "```py\n>>> print(Picamera2.global_camera_info())\n```", "```py\nfrom picamera2 import Picamera2\nimport time\n\n# Initialize the camera\npicam2 = Picamera2()  # default is index 0\n\n# Configure the camera\nconfig = picam2.create_still_configuration(main={\"size\": (640, 480)})\npicam2.configure(config)\n\n# Start the camera\npicam2.start()\n\n# Wait for the camera to warm up\ntime.sleep(2)\n\n# Capture an image\npicam2.capture_file(\"usb_camera_image.jpg\")\nprint(\"Image captured and saved as 'usb_camera_image.jpg'\")\n\n# Stop the camera\npicam2.stop()\n```", "```py\n    pip3 install flask\n    ```", "```py\nfrom flask import Flask, Response, render_template_string,\n                  request, redirect, url_for\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport os\nimport signal\n\napp = Flask(__name__)\n\n# Global variables\nbase_dir = \"dataset\"\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\ncapture_counts = {}\ncurrent_label = None\nshutdown_event = threading.Event()\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(\n             main={\"size\": (320, 240)}\n    )\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while not shutdown_event.is_set():\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Adjust as needed for smooth preview\n\ndef generate_frames():\n    while not shutdown_event.is_set():\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' +\n                                       frame + b'\\r\\n')\n        time.sleep(0.1)  # Adjust as needed for smooth streaming\n\ndef shutdown_server():\n    shutdown_event.set()\n    if picam2:\n        picam2.stop()\n    # Give some time for other threads to finish\n    time.sleep(2)\n    # Send SIGINT to the main process\n    os.kill(os.getpid(), signal.SIGINT)\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global current_label\n    if request.method == 'POST':\n        current_label = request.form['label']\n        if current_label not in capture_counts:\n            capture_counts[current_label] = 0\n        os.makedirs(os.path.join(base_dir, current_label),\n                                 exist_ok=True)\n        return redirect(url_for('capture_page'))\n    return render_template_string('''\n <!DOCTYPE html>\n <html>\n <head>\n <title>Dataset Capture - Label Entry</title>\n </head>\n <body>\n <h1>Enter Label for Dataset</h1>\n <form method=\"post\">\n <input type=\"text\" name=\"label\" required>\n <input type=\"submit\" value=\"Start Capture\">\n </form>\n </body>\n </html>\n ''')\n\n@app.route('/capture')\ndef capture_page():\n    return render_template_string('''\n <!DOCTYPE html>\n <html>\n <head>\n <title>Dataset Capture</title>\n <script>\n var shutdownInitiated = false;\n function checkShutdown() {\n if (!shutdownInitiated) {\n fetch('/check_shutdown')\n .then(response => response.json())\n .then(data => {\n if (data.shutdown) {\n shutdownInitiated = true;\n document.getElementById(\n 'video-feed').src = '';\n document.getElementById(\n 'shutdown-message')\n .style.display = 'block';\n }\n });\n }\n }\n setInterval(checkShutdown, 1000); // Check\n every second\n </script>\n </head>\n <body>\n <h1>Dataset Capture</h1>\n <p>Current Label: {{ label }}</p>\n <p>Images captured for this label: {{ capture_count\n  }}</p>\n <img id=\"video-feed\" src=\"{{ url_for('video_feed')\n  }}\" width=\"640\"\n height=\"480\" />\n <div id=\"shutdown-message\" style=\"display: none;\n color: red;\">\n Capture process has been stopped.\n You can close this window.\n </div>\n <form action=\"/capture_image\" method=\"post\">\n <input type=\"submit\" value=\"Capture Image\">\n </form>\n <form action=\"/stop\" method=\"post\">\n <input type=\"submit\" value=\"Stop Capture\"\n style=\"background-color: #ff6666;\">\n </form>\n <form action=\"/\" method=\"get\">\n <input type=\"submit\" value=\"Change Label\"\n style=\"background-color: #ffff66;\">\n </form>\n </body>\n </html>\n ''', label=current_label, capture_count=capture_counts.get(\n                                            current_label, 0))\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace;\n boundary=frame')\n\n@app.route('/capture_image', methods=['POST'])\ndef capture_image():\n    global capture_counts\n    if current_label and not shutdown_event.is_set():\n        capture_counts[current_label] += 1\n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        filename = f\"image_{timestamp}.jpg\"\n        full_path = os.path.join(base_dir, current_label,\n                                 filename)\n\n        picam2.capture_file(full_path)\n\n    return redirect(url_for('capture_page'))\n\n@app.route('/stop', methods=['POST'])\ndef stop():\n    summary = render_template_string('''\n <!DOCTYPE html>\n <html>\n <head>\n <title>Dataset Capture - Stopped</title>\n </head>\n <body>\n <h1>Dataset Capture Stopped</h1>\n <p>The capture process has been stopped.\n You can close this window.</p>\n <p>Summary of captures:</p>\n <ul>\n {% for label, count in capture_counts.items() %}\n <li>{{ label }}: {{ count }} images</li>\n {% endfor %}\n </ul>\n </body>\n </html>\n ''', capture_counts=capture_counts)\n\n    # Start a new thread to shutdown the server\n    threading.Thread(target=shutdown_server).start()\n\n    return summary\n\n@app.route('/check_shutdown')\ndef check_shutdown():\n    return {'shutdown': shutdown_event.is_set()}\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n```", "```py\n    python3 get_img_data.py\n```", "```py\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(\n        image, new_height, new_width\n    )\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\n```", "```py\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\n```", "```py\nimg_path = \"./images/robot.jpg\"\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-\\\n model.tflite\"\nlabels = [\"background\", \"periquito\", \"robot\"]\n```", "```py\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n```", "```py\ninput_dtype = input_details[0][\"dtype\"]\ninput_dtype\n```", "```py\nnumpy.int8\n```", "```py\nimg = Image.open(img_path)\nplt.figure(figsize=(4, 4))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()\n```", "```py\nscale, zero_point = input_details[0][\"quantization\"]\nimg = img.resize(\n    (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n)\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (\n    (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\n)\ninput_data = np.expand_dims(img_array, axis=0)\n```", "```py\ninput_data.shape, input_data.dtype\n```", "```py\n((1, 160, 160, 3), dtype('int8'))\n```", "```py\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0][\"index\"], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert\n# to milliseconds\nprint(\"Inference time: {:.1f}ms\".format(inference_time))\n```", "```py\n# Obtain results and map them to the classes\npredictions = interpreter.get_tensor(output_details[0][\"index\"])[0]\n\n# Get indices of the top k results\ntop_k_results = 3\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n# Get quantization parameters\nscale, zero_point = output_details[0][\"quantization\"]\n\n# Dequantize the output\ndequantized_output = (\n    predictions.astype(np.float32) - zero_point\n) * scale\nprobabilities = dequantized_output\n\nprint(\"\\n\\t[PREDICTION]        [Prob]\\n\")\nfor i in range(top_k_results):\n    print(\n        \"\\t{:20}: {:.2f}%\".format(\n            labels[top_k_indices[i]],\n            probabilities[top_k_indices[i]] * 100,\n        )\n    )\n```", "```py\ndef image_classification(\n    img_path, model_path, labels, top_k_results=3, apply_softmax=False\n):\n    # Load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis(\"off\")\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Preprocess\n    img = img.resize(\n        (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n    )\n\n    input_dtype = input_details[0][\"dtype\"]\n\n    if input_dtype == np.uint8:\n        input_data = np.expand_dims(np.array(img), axis=0)\n    elif input_dtype == np.int8:\n        scale, zero_point = input_details[0][\"quantization\"]\n        img_array = np.array(img, dtype=np.float32) / 255.0\n        img_array = (\n            (img_array / scale + zero_point)\n            .clip(-128, 127)\n            .astype(np.int8)\n        )\n        input_data = np.expand_dims(img_array, axis=0)\n    else:  # float32\n        input_data = (\n            np.expand_dims(np.array(img, dtype=np.float32), axis=0)\n            / 255.0\n        )\n\n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (\n        end_time - start_time\n    ) * 1000  # Convert to milliseconds\n\n    # Obtain results\n    predictions = interpreter.get_tensor(output_details[0][\"index\"])[\n        0\n    ]\n\n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n    # Handle output based on type\n    output_dtype = output_details[0][\"dtype\"]\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0][\"quantization\"]\n        predictions = (\n            predictions.astype(np.float32) - zero_point\n        ) * scale\n\n    if apply_softmax:\n        # Apply softmax\n        exp_preds = np.exp(predictions - np.max(predictions))\n        probabilities = exp_preds / np.sum(exp_preds)\n    else:\n        probabilities = predictions\n\n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\n            \"\\t{:20}: {:.1f}%\".format(\n                labels[top_k_indices[i]],\n                probabilities[top_k_indices[i]] * 100,\n            )\n        )\n    print(\"\\n\\tInference time: {:.1f}ms\".format(inference_time))\n```", "```py\nfrom flask import Flask, Response, render_template_string,\n                  request, jsonify\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom queue import Queue\n\napp = Flask(__name__)\n\n# Global variables\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\nis_classifying = False\nconfidence_threshold = 0.8\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-\\\n model.tflite\"\nlabels = ['background', 'periquito', 'robot']\ninterpreter = None\nclassification_queue = Queue(maxsize=1)\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(\n        main={\"size\": (320, 240)}\n    )\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while True:\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Capture frames more frequently\n\ndef generate_frames():\n    while True:\n        with frame_lock:\n            if frame is not None:\n                yield (\n                   b'--frame\\r\\n'\n                   b'Content-Type: image/jpeg\\r\\n\\r\\n'\n                   + frame + b'\\r\\n'\n                )\n        time.sleep(0.1)\n\ndef load_model():\n    global interpreter\n    if interpreter is None:\n        interpreter = tflite.Interpreter(model_path=model_path)\n        interpreter.allocate_tensors()\n    return interpreter\n\ndef classify_image(img, interpreter):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    img = img.resize((input_details[0]['shape'][1],\n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(np.array(img), axis=0)\\\n                             .astype(input_details[0]['dtype'])\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    predictions = interpreter.get_tensor(output_details[0]\n                                         ['index'])[0]\n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) -\n                       zero_point) * scale\n    return predictions\n\ndef classification_worker():\n    interpreter = load_model()\n    while True:\n        if is_classifying:\n            with frame_lock:\n                if frame is not None:\n                    img = Image.open(io.BytesIO(frame))\n            predictions = classify_image(img, interpreter)\n            max_prob = np.max(predictions)\n            if max_prob >= confidence_threshold:\n                label = labels[np.argmax(predictions)]\n            else:\n                label = 'Uncertain'\n            classification_queue.put({\n                 'label': label,\n                 'probability': float(max_prob)\n            })\n        time.sleep(0.1)  # Adjust based on your needs\n\n@app.route('/')\ndef index():\n   return render_template_string('''\n <!DOCTYPE html>\n <html>\n <head>\n <title>Image Classification</title>\n <script\n src=\"https://code.jquery.com/jquery-3.6.0.min.js\">\n </script>\n <script>\n function startClassification() {\n $.post('/start');\n $('#startBtn').prop('disabled', true);\n $('#stopBtn').prop('disabled', false);\n }\n function stopClassification() {\n $.post('/stop');\n $('#startBtn').prop('disabled', false);\n $('#stopBtn').prop('disabled', true);\n }\n function updateConfidence() {\n var confidence = $('#confidence').val();\n $.post('/update_confidence',\n {confidence: confidence}\n );\n }\n function updateClassification() {\n $.get('/get_classification', function(data) {\n $('#classification').text(data.label + ': '\n + data.probability.toFixed(2));\n });\n }\n $(document).ready(function() {\n setInterval(updateClassification, 100);\n // Update every 100ms\n });\n </script>\n </head>\n <body>\n <h1>Image Classification</h1>\n <img src=\"{{ url_for('video_feed') }}\"\n width=\"640\"\n height=\"480\" />\n\n <br>\n <button id=\"startBtn\"\n onclick=\"startClassification()\">\n Start Classification\n </button>\n\n <button id=\"stopBtn\"\n onclick=\"stopClassification()\"\n disabled>\n Stop Classification\n </button>\n\n <br>\n <label for=\"confidence\">Confidence Threshold:</label>\n <input type=\"number\"\n id=\"confidence\"\n name=\"confidence\"\n min=\"0\" max=\"1\"\n step=\"0.1\"\n value=\"0.8\"\n onchange=\"updateConfidence()\" />\n\n <br>\n <div id=\"classification\">\n Waiting for classification...\n </div>\n\n </body>\n </html>\n ''')\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(\n       generate_frames(),\n       mimetype='multipart/x-mixed-replace; boundary=frame'\n    )\n\n@app.route('/start', methods=['POST'])\ndef start_classification():\n    global is_classifying\n    is_classifying = True\n    return '', 204\n\n@app.route('/stop', methods=['POST'])\ndef stop_classification():\n    global is_classifying\n    is_classifying = False\n    return '', 204\n\n@app.route('/update_confidence', methods=['POST'])\ndef update_confidence():\n    global confidence_threshold\n    confidence_threshold = float(request.form['confidence'])\n    return '', 204\n\n@app.route('/get_classification')\ndef get_classification():\n    if not is_classifying:\n        return jsonify({'label': 'Not classifying',\n                       'probability': 0})\n    try:\n        result = classification_queue.get_nowait()\n    except Queue.Empty:\n        result = {'label': 'Processing', 'probability': 0}\n    return jsonify(result)\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    threading.Thread(target=classification_worker,\n                     daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n```", "```py\npython3 img_class_live_infer.py\n```"]