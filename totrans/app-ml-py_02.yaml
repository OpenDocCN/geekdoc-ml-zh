- en: Machine Learning Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_concepts.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_concepts.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with
    Code‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite this e-Book as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with
    Code* [e-book]. Zenodo. doi:10.5281/zenodo.15169138 [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflows in this book and more are available here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite the MachineLearningDemos GitHub Repository as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration
    Workflows Repository* (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312\.
    GitHub repository: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
  prefs: []
  type: TYPE_NORMAL
- en: By Michael J. Pyrcz
  prefs: []
  type: TYPE_NORMAL
- en: ¬© Copyright 2024.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is a summary of **Machine Learning Concepts** including essential
    concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Statistics and Data Analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inferential Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictive Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning Model Training and Tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning Model Overfit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YouTube Lecture**: check out my lecture on [Introduction to Machine Learning](https://youtu.be/zOUM_AnI1DQ?si=Gi2KZTfPa5xQ2Qb6).
    For your convenience here‚Äôs a summary of salient points.'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for Machine Learning Concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You could just open up a Jupyter notebook in Python and start building machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: the [scikit-learn docs](https://scikit-learn.org/stable/) are quite good and
    for every machine learning function there is a short code example that you could
    copy and paste to repeat their work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, you could Google a question about using a specific machine learning algorithm
    in Python and the top results will include [StackOverflow](https://stackoverflow.com/)
    questions and responses, it is truly amazing how much experienced coders are willing
    to give back and share their knowledge. We truly have an amazing scientific community
    with the spirit of knowledge sharing and open-source development. Respect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: of course, you could learn a lot about machine learning from a machine learning
    large language model (LLM) like [ChatGPT](https://chatgpt.com/). Not only will
    ChatGPT answer your questions, but it will also provide codes and help you debug
    them when you tell it what went wrong.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One way of the other and you received and added this code to you data science
    workflow,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: et voil√† (note I‚Äôm Canadian, so I use some French phrases), you have a trained
    predictive machine learning model that could be applied to make predictions for
    new cases.
  prefs: []
  type: TYPE_NORMAL
- en: But, is this a good model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How good is it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Could it be better?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Without knowledge about basic machine learning concepts we can‚Äôt answer these
    questions and build the best possible models. In general, I‚Äôm not an advocate
    for black box modeling, because it is:'
  prefs: []
  type: TYPE_NORMAL
- en: likely to lead to mistakes that may be difficult to detect and correct
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: incompatible with the expectations for competent practice for professional engineers.
    I gave a talk on [Applying Machine Learning as a Compentent Engineer or Geoscientist](https://youtu.be/W_ZDg1Wb2vM?si=DF-n1E4-ik2ukLfF)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To help out this chapter provides you with the basic knowledge to answer these
    questions and to make better, more reliable machine learning models. Let‚Äôs start
    building this essential foundation with some definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Big Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Everyone hears that machine learning needs a lot of data. In fact, so much data
    that it is called ‚Äúbig data‚Äù, but how do you know if you are working with big
    data?
  prefs: []
  type: TYPE_NORMAL
- en: the criteria for big data are these ‚ÄòV‚Äôs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you answer ‚Äúyes‚Äù for at least some of these criteria, then you are working
    with big data,
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume**: many data samples, difficult to handle and visualize'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity**: high rate collection, continuous relative to decision making
    cycles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**: data form various sources, with various types and scales'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variability**: data acquisition changes during the project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Veracity**: data has various levels of accuracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In my experience, most subsurface engineers and geoscientists answer ‚Äúyes‚Äù to
    all of these ‚Äòv‚Äô criteria.
  prefs: []
  type: TYPE_NORMAL
- en: so I proudly say that we in the subsurface have been big data long before the
    tech sector learned about big data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In fact, I often state that we in the subsurface resource industries are the
    original data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôm getting ahead of myself, more on this in a bit. Don‚Äôt worry if I get carried
    away in hubris, rest assured this e-book is written for anyone interested to learn
    about machine learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you can skip the short sections on subsurface data science or read along if
    interested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know big data, let‚Äôs talk about the big data related topics, statistics,
    geostatistics and data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics, Geostatistics and Data Analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Statistics is the practice of,
  prefs: []
  type: TYPE_NORMAL
- en: collecting data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: organizing data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: interpreting data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: drawing conclusions from data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: making decisions with data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is all about moving from data to decision.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of decision making in statistics.
  prefs: []
  type: TYPE_NORMAL
- en: If your work does not impact the decision, you do not add value!
  prefs: []
  type: TYPE_NORMAL
- en: If you look up the definition of data analytics, you will find criteria that
    include, statistical analysis, and data visualization to support decision making.
  prefs: []
  type: TYPE_NORMAL
- en: I call it, data analytics and statistics are the same thing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we can append, geostatistics as a branch of applied statistics that accounts
    for,
  prefs: []
  type: TYPE_NORMAL
- en: the spatial (geological) context
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the spatial relationships
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: volumetric support
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: uncertainty
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remember all those statistics classes with the assumption of i.i.d., independent,
    identically distributed.
  prefs: []
  type: TYPE_NORMAL
- en: spatial phenomenon are not i.i.d., so we developed a unique branch of statistics
    to address this
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: by our assumption above (statistics is data analytics), we can state that geostatistics
    is the same as spatial data analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let‚Äôs use a Venn diagram to visualize statistics / data analytics and geostatistics
    / spatial data analytics,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d590b072c7838bf9671b2ee7b8bc2de2.png)'
  prefs: []
  type: TYPE_IMG
- en: Venn diagram for statistics and geostatistics.
  prefs: []
  type: TYPE_NORMAL
- en: and we can add our previously discussed big data to our Venn diagram resulting
    in big data analytics and spatial big data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c885d770c796b22e4c9022c27037a97c.png)'
  prefs: []
  type: TYPE_IMG
- en: Venn diagram for statistics and geostatistics with big data added.
  prefs: []
  type: TYPE_NORMAL
- en: Scientific Paradigms and the Fourth Paradigm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If no one else has said this to you, let me have the honor of saying,
  prefs: []
  type: TYPE_NORMAL
- en: Welcome
  prefs: []
  type: TYPE_NORMAL
- en: to the fourth paradigm of scientific discovery, data-driven scientific discovery
    or we can just call it data science.
  prefs: []
  type: TYPE_NORMAL
- en: The paradigms of scientific discovery are distinct approaches for humanity to
    apply science to expand human knowledge, discover and develop new technologies
    that impact society. These paradigms include,
  prefs: []
  type: TYPE_NORMAL
- en: '**First Paradigm** - logical reasoning with physical experiment and observation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Second Paradigm** - theory-driven, where mathematical models and laws are
    used to explain and predict natural phenomena'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Third Paradigm** - computer-based numerical simulations and models to study
    complex systems that are too difficult or impossible to solve analytically or
    test experimentally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fourth Paradigm** - analysis of massive datasets with tools such as data
    analytics, machine learning, cloud computing and AI, patterns in the data guide
    insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs all of the four paradigms with dates for important developments with
    each,
  prefs: []
  type: TYPE_NORMAL
- en: '| 1st Paradigm Empirical Science | 2nd Paradigm Theoretical Science | 3rd Paradigm
    Computational Science | 4th Paradigm Data Science |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Logic and Experiments | Theory and Models | Numerical Simulation | Learning
    from Data |'
  prefs: []
  type: TYPE_TB
- en: '| \(\approx\) 600 BC Thales of Miletus predicted a solar ecplipse | \(\approx\)
    300 BC Euclid - Elements | 1946 von Neumann weather simulation | 1990s Human Genome
    Project |'
  prefs: []
  type: TYPE_TB
- en: '| \(\approx\) 400 BC Hippocrates natural causes for diseases | \(\approx\)
    150 AD Ptolemy planetary motion model | 1952 Fermi simulated nonlinear systems
    | 2008 Large Hadron Collider |'
  prefs: []
  type: TYPE_TB
- en: '| 430 BC Empedocles proved air has substance | 1011 AD al-Haytham Book of Optics
    | 1957 Lorenz demonstrates chaos | 2009 Hey et al. Data-Intensive Book |'
  prefs: []
  type: TYPE_TB
- en: '| 230 BC Eratosthenes measures Earth‚Äôs diameter | 1687 AD Newton Principia
    | 1980s Mandelbrot simulating fractals | 2015 AlphaGo beats a professional Go
    player |'
  prefs: []
  type: TYPE_TB
- en: Of course, we can argue about the boundaries between the paradigms for scientific
    discovery, i.e., when did a specific paradigm begin? For example, consider these
    examples of the application of the first paradigm that push the start of the first
    paradigm deep into antiquity!
  prefs: []
  type: TYPE_NORMAL
- en: '| Field | Example | Approx. Date | Why It Fits 1st Paradigm |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Agriculture | Irrigation systems using canals, gates, and timing | \(\approx\)
    3000‚ÄØBC | Empirical understanding of water flow, soil, and seasonal timing |'
  prefs: []
  type: TYPE_TB
- en: '| Astronomy | Babylonian star charts; lunar calendars | \(\approx\) 1800‚ÄØBC
    | Systematic observations for predicting celestial events |'
  prefs: []
  type: TYPE_TB
- en: '| Engineering | The wheel (transport, pottery) | \(\approx\) 3500‚ÄØBC | Developed
    through trial-and-error and practical refinement |'
  prefs: []
  type: TYPE_TB
- en: '| Medicine | Herbal remedies, surgical procedures | \(\approx\) 2000‚ÄØBC | Based
    on observation and accumulated case knowledge |'
  prefs: []
  type: TYPE_TB
- en: '| Mathematics | Base-60 system; geometric calculation tablets | \(\approx\)
    2000‚ÄØBC | Used for land division, architecture, and astronomy; empirically tested
    |'
  prefs: []
  type: TYPE_TB
- en: certainly the Mesopotamians (4000 BC - 3500 BC) applied the first paradigm with
    their experiments that supported the development of the wheel, plow, chariot,
    weaving loom, and irrigation. Aside, I wanted to give a shout out to the ancient
    peoples, because I‚Äôm quite fascinated by the development of human societies and
    their technologies over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other side, we can trace the development of fourth paradigm approaches,
    for example the artificial neural networks to McColloch and Pitts in 1943 and
    consider these early examples of learning by distilling patterns from large datasets,
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Field | Example / Person | Why It Fits 4th Paradigm Traits |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| \(\approx\)100‚ÄØAD | Astronomy | Ptolemy‚Äôs star catalog | Massive empirical
    data used to predict celestial motion |'
  prefs: []
  type: TYPE_TB
- en: '| 1086 | Economics | Domesday Book (England) | Large-scale data census for
    administrative, economic decisions |'
  prefs: []
  type: TYPE_TB
- en: '| 1700s | Natural history | Carl Linnaeus ‚Äì taxonomy | Classification of species
    based on observable traits; data-first approach |'
  prefs: []
  type: TYPE_TB
- en: '| 1801 | Statistics | Adolphe Quetelet ‚Äì social physics | Used statistics to
    analyze human behavior from large datasets |'
  prefs: []
  type: TYPE_TB
- en: '| 1830s | Astronomy | John Herschel‚Äôs sky surveys | Cataloged thousands of
    stars and nebulae; data-driven sky classification |'
  prefs: []
  type: TYPE_TB
- en: '| 1859 | Biology | Darwin ‚Äì *Origin of Species* | Synthesized global species
    data to uncover evolutionary patterns |'
  prefs: []
  type: TYPE_TB
- en: '| 1890 | Public health | Florence Nightingale | Used mortality data visualization
    to influence hospital reform |'
  prefs: []
  type: TYPE_TB
- en: '| 1890 | Census, Computing | Hollerith punch cards (U.S. Census) | First automated
    analysis of large datasets; mechanical data processing |'
  prefs: []
  type: TYPE_TB
- en: '| 1910s‚Äì30s | Genetics | Morgan lab ‚Äì Drosophila studies | Mapped gene traits
    from thousands of fruit fly crosses |'
  prefs: []
  type: TYPE_TB
- en: '| 1920s | Economics | Kondratiev, Kuznets | Identified economic patterns using
    historical data series |'
  prefs: []
  type: TYPE_TB
- en: '| 1937 | Ecology | Arthur Tansley ‚Äì ecosystem concept | Integrated large-scale
    field data into systemic ecological frameworks |'
  prefs: []
  type: TYPE_TB
- en: Finally, the adoption of a new scientific paradigm represents a significant
    societal shift that unfolds unevenly across the globe. While I aim to summarize
    the chronology, it‚Äôs important to acknowledge its complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Foundations and Enablers of the Fourth Scientific Paradigm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What caused the fourth paradigm to start some time between 1943 and 2009? Is
    it the development of new math?
  prefs: []
  type: TYPE_NORMAL
- en: the foundational mathematics behind data-driven models has been available for
    decades and even centuries. Consider the following examples of mathematical and
    statistical developments that underpin modern data science,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Technique | Key Contributor(s) | Date / Publication |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Calculus** | Isaac Newton, Gottfried Wilhelm Leibniz | Newton: *Methods
    of Fluxions* (1671, pub. 1736); Leibniz: *Nova Methodus* (1684) |'
  prefs: []
  type: TYPE_TB
- en: '| **Bayesian Probability** | Reverend Thomas Bayes | *An Essay Towards Solving
    a Problem in the Doctrine of Chances* (posthumous, 1763) |'
  prefs: []
  type: TYPE_TB
- en: '| **Linear Regression** | Marie Legendre | Formalized in 1805 |'
  prefs: []
  type: TYPE_TB
- en: '| **Discriminant Analysis** | Ronald Fisher | *The Use of Multiple Measurements
    in Taxonomic Problems* (1939) |'
  prefs: []
  type: TYPE_TB
- en: '| **Monte Carlo Simulation** | Stanislaw Ulam, John von Neumann | Developed
    in early 1940s during the Manhattan Project |'
  prefs: []
  type: TYPE_TB
- en: 'Upon reflection, one might ask: why didn‚Äôt the fourth scientific paradigm emerge
    earlier‚Äîsay, in the 1800s or even before the 1940s? What changed? The answer lies
    in the fact that several key developments were still needed to create the fertile
    ground for data science to take root.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: the availability of inexpensive, accessible computing power
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the emergence of massive, high-volume datasets (big data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs explore each of these in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Cheap and Available Compute
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning from big data is not feasible without affordable and widely available
    computing and storage resources. Consider the following computational advancements
    that paved the way for the emergence of the fourth scientific paradigm,
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Milestone | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1837** | Babbage‚Äôs Analytical Engine | First design of a mechanical computer
    with concepts like memory and control flow (not built). |'
  prefs: []
  type: TYPE_TB
- en: '| **1941‚Äì45** | Zuse‚Äôs Z3, ENIAC | First programmable digital computers; rewired
    plugboards, limited memory, slow and unreliable. |'
  prefs: []
  type: TYPE_TB
- en: '| **1947** | Transistor Invented | Replaced vacuum tubes; enabled faster, smaller,
    more reliable second-gen computers (e.g., IBM 7090). |'
  prefs: []
  type: TYPE_TB
- en: '| **1960s** | Integrated Circuits | Multiple transistors on a single chip;
    led to third-generation computers. |'
  prefs: []
  type: TYPE_TB
- en: '| **1971** | Microprocessor Developed | Integrated CPU on a chip; enabled personal
    computers like Apple II (1977) and IBM PC (1981). |'
  prefs: []
  type: TYPE_TB
- en: Yes, when I was in grade 1 of my elementary school education we had the first
    computers in classrooms (Apple IIe), and it amazed all of us with the monochrome
    (orange and black pixels) monitor, 5 1/4 inch floppy disk loaded programs (there
    was no hard drive) and the beeps and clicks from the computer (no dedicated sound
    chip)!
  prefs: []
  type: TYPE_NORMAL
- en: We live in a society with more compute power in our pockets, i.e., our cell
    phones, than used to send the first astronauts to the moon and a SETI screen saver
    that uses home computers‚Äô idle time to search for extraterrestrial life!
  prefs: []
  type: TYPE_NORMAL
- en: '[SETI@home](https://setiathome.berkeley.edu/) - search for extraterrestrial
    intelligence. Note the @home component is now in hibernation while the team continuous
    the analysis the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also check out these ways to put your computer‚Äôs idle time to good use,
  prefs: []
  type: TYPE_NORMAL
- en: '[Folding@home](https://foldingathome.org/) - simulate protein dynamics, including
    the process of protein folding and the movements of proteins to develop therapeutics
    for treating disease.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Einstein@home](https://einsteinathome.org/) - search for weak astrophysical
    signals from spinning neutron stars, with big data from the LIGO gravitational-wave
    detectors, the MeerKAT radio telescope, the Fermi gamma-ray satellite, and the
    archival data from the Arecibo radio telescope.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we are surrounded by cheap and reliable compute that our grandparents (and
    perhaps even our parents) could scarcely have imagined.
  prefs: []
  type: TYPE_NORMAL
- en: As you will learn in this e-book, machine learning methods requires a lot of
    compute, because training most of these methods relies on,
  prefs: []
  type: TYPE_NORMAL
- en: numerous iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: matrix or parallel computations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bootstrapping techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stochastic gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: extensive model tuning, which may involve training many versions in nested loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affordable and accessible computing is a fundamental prerequisite for the fourth
    scientific paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: the development of data science has been largely driven by crowd-sourced and
    open-source contributions‚Äîefforts that rely on widespread access to computing
    power
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the era of Babbage‚Äôs Analytical Engine or the ENIAC, data science at scale
    would have been simply impossible.
  prefs: []
  type: TYPE_NORMAL
- en: Availability of Big Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With small data, we often rely on external sources of knowledge such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Physics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Engineering and geoscience principles
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These models are then calibrated using the limited available observations‚Äîa
    common approach in the second and third scientific paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, big data allows us to learn the full range of behaviors of natural
    systems directly from the data itself. In fact, big data often reveals the limitations
    of second and third paradigm models, exposing missing complexities in our traditional
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, big data is essential to:'
  prefs: []
  type: TYPE_NORMAL
- en: provide sufficient sampling to support the data-driven fourth paradigm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may even challenge the exclusive reliance on second and third paradigm models,
    as the complexity of large datasets uncovers the boundaries of our existing theoretical
    frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Today, big data is everywhere! Consider all the open-source and publically available
    data online. Here‚Äôs some great examples,
  prefs: []
  type: TYPE_NORMAL
- en: '| Resource | Description | Link |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Google Earth (Timelapse)** | Satellite imagery (not highest res, but useful
    for land use, geomorphology, and surface evolution studies). Used by research
    teams. | [Google Earth](https://www.google.com/maps) |'
  prefs: []
  type: TYPE_TB
- en: '| **USGS River Gage Data** | Public streamflow and hydrology data; useful for
    analysis and recreational planning (e.g., paddling). | [USGS NWIS](https://waterdata.usgs.gov/nwis)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Government Databases** | Includes U.S. Census and oil & gas production
    statistics. Open for public and research use. | [U.S. Census](https://data.census.gov/),
    [Texas Oil & Gas Data](https://www.rrc.texas.gov/oil-and-gas/research-and-statistics/production-data/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **NASA Moon Trek** | High-res lunar surface imagery from LRO, SELENE, and
    Clementine; interactive visualization and downloads. | [Moon Trek](https://trek.nasa.gov/moon/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Landsat Satellite Archive** | 50+ years of medium-resolution Earth observation
    data (since 1972); invaluable for land cover change. | [USGS EarthExplorer](https://earthexplorer.usgs.gov/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Copernicus Open Access Hub** | European Space Agency‚Äôs Sentinel satellite
    data (e.g., radar, multispectral imagery). | [Copernicus Hub](https://scihub.copernicus.eu/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Global Biodiversity Information Facility (GBIF)** | Open biodiversity data:
    species distributions, observations, and ecological records from around the world.
    | [GBIF](https://www.gbif.org/) |'
  prefs: []
  type: TYPE_TB
- en: '| **National Oceanic and Atmospheric Administration (NOAA)** | Huge archive
    of weather, ocean, and climate data, including radar, reanalysis, and forecasts.
    | [NOAA Data](https://www.ncei.noaa.gov/) |'
  prefs: []
  type: TYPE_TB
- en: '| **World Bank Open Data** | Economic, development, demographic, and environmental
    indicators for all countries. | [World Bank Data](https://data.worldbank.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Our World in Data** | Curated global datasets on health, population, energy,
    CO‚ÇÇ, education, and more. | [Our World in Data](https://ourworldindata.org/) |'
  prefs: []
  type: TYPE_TB
- en: '| **OpenStreetMap (OSM)** | Crowdsourced global geospatial data including roads,
    buildings, and land use. | [OpenStreetMap](https://www.openstreetmap.org/) |'
  prefs: []
  type: TYPE_TB
- en: '| **Google Dataset Search** | Meta-search engine indexing datasets across disciplines
    and repositories. | [Dataset Search](https://datasetsearch.research.google.com/)
    |'
  prefs: []
  type: TYPE_TB
- en: While open data is rapidly expanding, proprietary data is also growing quickly
    across many industries, driven by the adoption of smart, connected systems with
    advanced monitoring and control.
  prefs: []
  type: TYPE_NORMAL
- en: The result of all of this is a massive explosion of data, supported by faster,
    more affordable computing, processing, and storage. We are now immersed in a sea
    of data that previous generations could never have imagined.
  prefs: []
  type: TYPE_NORMAL
- en: Note, we could also talk about improved algorithms and hardware architectures
    optimized for data science, but I‚Äôll leave that out of scope for this e-book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs a local example of freely available big data. I downloaded the daily
    water level of our local Lake Travis from [Water Data for Texas](https://waterdatafortexas.org/reservoirs/individual/travis).
    Note the data is collected and provided by Lower Colorado Water Authority [LRCA](https://www.lcra.org/).
  prefs: []
  type: TYPE_NORMAL
- en: I made a plot of the daily plot and added useful information such as,
  prefs: []
  type: TYPE_NORMAL
- en: water height - in feet above mean sea level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: percentage full - the ratio of the current volume of water stored in a reservoir
    to its total storage capacity, expressed as a percentage, accounting for the shape
    of the reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: max pool - maximum conservation pool, the top of the reservoir used for water
    supply, irrigation, hydropower and recreation and not including flood storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8f21a478ebe4ba93df9f50b7ca4b5e31.png)'
  prefs: []
  type: TYPE_IMG
- en: Historical Lake Austin daily water level and percentage full.
  prefs: []
  type: TYPE_NORMAL
- en: And before you even ask - yes! I did calculate the semivariogram of water level
    and I was surprised that there was no significant nuggest effect.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dae27b907b4dada88f39fe0a7b6950d.png)'
  prefs: []
  type: TYPE_IMG
- en: Historical Lake Austin daily water level variogram.
  prefs: []
  type: TYPE_NORMAL
- en: To become a good data scientists!
  prefs: []
  type: TYPE_NORMAL
- en: Find and play with data!
  prefs: []
  type: TYPE_NORMAL
- en: All of these developments have provided the fertile ground for the seeds of
    data science to impact all sectors or our economy and society.
  prefs: []
  type: TYPE_NORMAL
- en: Data Science and Subsurface Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spoiler alert, I‚Äôm going to boast a bit in the section. I often hear students
    say,
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúI can‚Äôt believe this data science course is in the [Hildebrand Department of
    Petroleum and Geosystems Engineering](https://pge.utexas.edu/)!‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚ÄúWhy are you teaching machine learning in [Department of Earth and Planetary
    Sciences](https://eps.jsg.utexas.edu/)?‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once again, my response is,
  prefs: []
  type: TYPE_NORMAL
- en: We in the subsurface are the original data scientists!
  prefs: []
  type: TYPE_NORMAL
- en: We have been big data long before tech learned about big data!
  prefs: []
  type: TYPE_NORMAL
- en: This may sound a bit arrogant, but let me back this statement up with this timeline,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6046f01d90d6e62557dcf8b78e9bde99.png)'
  prefs: []
  type: TYPE_IMG
- en: Timeline of data science development from the perspective of subsurface engineering
    and geoscience.
  prefs: []
  type: TYPE_NORMAL
- en: Shortly after Kolmogorov developed the fundamental probability axioms, Danie
    Krige developed a set of statistical, spatial, i.e., data-driven tools for making
    estimates in space while accounting for spatial continuity and scale. These tools
    were formalized with theory developed by Professor Matheron during the 1960s in
    a new science called geostatistics. Over the 1970s - 1990s the geostatistical
    methods and applications expanded from mining to address oil and gas, environmental,
    agriculture, fisheries, etc. with many important open source developments.
  prefs: []
  type: TYPE_NORMAL
- en: Why was subsurface engineering and geoscience earlier in the development of
    data science?
  prefs: []
  type: TYPE_NORMAL
- en: because, necessity is the mother of invention! Complicated, heterogeneous, sparsely
    sampled, vast systems with complicated physics and high value decisions drove
    us to data-driven methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many other engineering fields that had little motivation to develop
    and apply data science due to,
  prefs: []
  type: TYPE_NORMAL
- en: homogeneous phenomenon that does not have significant spatial heterogeneity,
    continuity, nor uncertainty - with few samples the phenomenon is sufficiently
    understood, for example, sheet aluminum used for aircraft or cement used in structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: exhaustive sampling of the population relative to the modeling purpose and do
    not need an estimation model with uncertainty - the phenomenon‚Äôs properties are
    known and the performance can be simulated in a finite element model, for example,
    radiographic testing for cracks in a machine housing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: have well understood physics and can model the entire system with second and
    third paradigm phenomenon - the (coupled) physics is known and can be modeled,
    for example, the Boeing 777 was the first commerical airliner designed entirely
    using computed-aided design (CAD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compare this to the typical context of subsurface engineering and geoscience,
    where data challenges are both unique and substantial:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparsely sampled datasets** ‚Äì we often sample only a minuscule fraction‚Äîsometimes
    as little as one hundred-billionth‚Äîof the subsurface volume of interest. This
    leads to significant uncertainty and a heavy reliance on statistical and data-driven
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Massively multivariate datasets** ‚Äì integrating diverse data types (e.g.,
    seismic, well logs, core data) poses a major challenge, requiring robust multivariate
    statistical and machine learning techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex, heterogeneous earth systems** ‚Äì subsurface geology is highly variable
    and location-dependent. These open systems often behave unpredictably, demanding
    flexible and adaptive modeling approaches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-stakes, high-cost decisions** ‚Äì The need to support critical, high-value
    decisions drives innovation in methods, workflows, and the adoption of advanced
    analytics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, many of us in subsurface engineering and geoscience have a lot
    of experience learning, applying, and teaching data science to better understand
    and manage these complexities.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we are ready to define machine learning, a [Wikipedia artical on Machine
    Learning](https://en.wikipedia.org/wiki/Machine_learning) can be summarized and
    broken down as follows, machine learning has these aspects,
  prefs: []
  type: TYPE_NORMAL
- en: '**toolbox** - a collections of algorithms and mathematical models used by computer
    systems'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**learning** - that improve their performance progressively on a specific task'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**training data** - by learning patterns from sample data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**general** - to make predictions or decisions without being explicitly programmed
    for the specific task'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let‚Äôs highlight some key ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: machine learning is a numerical toolbox, a broad set of algorithms designed
    to adapt to different problems (toolbox)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: these algorithms improve by learning from or fitting to training data (learning
    from training data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a single algorithm can often be trained and applied across many different domains
    (generalization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An especially important point, often overlooked is found near the end of the
    article:'
  prefs: []
  type: TYPE_NORMAL
- en: This underscores that machine learning is most valuable when traditional rule-based
    programming becomes too complex, impractical, or infeasible, often due to the
    scale or variability of the task.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if the underlying theory is well understood, be it engineering
    principles, geoscience fundamentals, physics, chemical reactions, or mechanical
    systems, then use that knowledge first.
  prefs: []
  type: TYPE_NORMAL
- en: don‚Äôt rely on data science as a replacement for foundational understanding of
    science and engineering (Paraphrased from personal communication with Professor
    Carlos Torres-Verdin, 2024)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning Prerequisite Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand machine learning and in general, data science we need to first
    differentiate between the population and the sample.
  prefs: []
  type: TYPE_NORMAL
- en: Population
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The population is all possible information of our spatial phenomenon of interest.
    Exhaustive, finite list of feature of interest over area of interest at the resolution
    of the data. For example,
  prefs: []
  type: TYPE_NORMAL
- en: exhaustive set of porosity at every location within a gas reservoir at the scale
    of a core sample, i.e., imagine the entire reservoir drilled, extracted and analyzed
    core by core!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for every tree in a forest the species, diameter at breast height (DBH), crown
    diameter, age, tree health status, wood volume and location, i.e., perfectly knowledge
    of our forest!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally the entire population is not accessible due to,
  prefs: []
  type: TYPE_NORMAL
- en: technology limits - sampling methods that provide improved coverage generally
    have reduced resolution and accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: practicallity - removing too much reservoir will impact geomechanical stability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cost - wells cost 10s - 100s of millions of dollars each
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The limited set of values, at specific locations that have been measured. For
    example,
  prefs: []
  type: TYPE_NORMAL
- en: limited set of porosity values measured from extracted core sample and callibrated
    from well-logs within a reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1,000 randomly selected trees over an management unit, with species, diameter
    at breast height (DBH), crown diameter, age, tree health status, wood volume and
    location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is so much that I could mention about sampling! For example,
  prefs: []
  type: TYPE_NORMAL
- en: methods for representative sample selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: impact of sample frequency or coverage on uncertainty and required levels of
    sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: methods to debias biased spatial samples (see [Declustering to Debias Data](https://geostatsguy.github.io/GeostatsPyDemos_Book/GeostatsPy_declustering.html)
    from my [Applied Geostatistics in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For brevity I will not cover these topics here, but let‚Äôs now dive into,
  prefs: []
  type: TYPE_NORMAL
- en: What are we measuring from our population in our samples?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each distinct type of measure is called a variable or feature.
  prefs: []
  type: TYPE_NORMAL
- en: Variable or Feature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In any scientific or engineering study, a property that is measured or observed
    is typically referred to as a variable.
  prefs: []
  type: TYPE_NORMAL
- en: however, in data science and machine learning, we use the term feature almost
    exclusively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While the terminology differs, the concept is the same: both refer to quantitative
    or categorical information about an object or system.'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate what constitutes a feature, consider the following examples drawn
    from real-world geoscience, oil adn gas, and mining contexts,
  prefs: []
  type: TYPE_NORMAL
- en: porosity measured from 1.5 inch diameter, 2 inch long core plugs extracted from
    the Miocene-aged Tahiti field in the Gulf of Mexico
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability modeled from porosity (neutron density well log) and rock facies
    (interpreted fraction of shale logs) at 0.5 foot resolution along the well bore
    in the Late Devonian Leduc formation in the Western Canadian Sedimentary Basin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: blast hole cuttings nickel grade aggregated over 8 inch diameter 10 meter blast
    holes at Voisey‚Äôs Bay Mine, Proterozoic gneissic complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Did you see what I did?
  prefs: []
  type: TYPE_NORMAL
- en: I specified what was measured, how it was measured, and over what scale was
    it measured.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is important because,
  prefs: []
  type: TYPE_NORMAL
- en: '**how the measurement is made?** - changes the feature‚Äôs veracity (level of
    certainty in the measure) and different methods actually may change the shift
    the feature‚Äôs values so we may need to reconcile multiple measurement methods
    of the same feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**what is the scale of the measurement?** - is very important due to volume-variance
    effect, with increasing support volume, sample scale, the variance reduces due
    to volumetric averaging resulting in regression to the mean. I have a [chapter
    on volume-variance](https://geostatsguy.github.io/GeostatsPyDemos_Book/GeostatsPy_volume_variance.html)
    in my Applied Geostatistics in Python e-book for those interested in more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, our subsurface measurements often requires significant analysis,
    interpretation, etc.
  prefs: []
  type: TYPE_NORMAL
- en: we don‚Äôt just hold a tool up to the rock and get the number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we have a ‚Äúthick layer‚Äù of engineering and geoscience interpretation to map
    from measurement to a useable feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider this carbonate thin section from Bureau of Economic Geology, The University
    of Texas at Austin from [geoscience course](http://www.beg.utexas.edu/lmod/_IOL-CM07/old-4.29.03/cm07-step05.htm)
    by F. Jerry Lucia.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/236b7707dd6a04bc2f29f0b99cda1271.png)'
  prefs: []
  type: TYPE_IMG
- en: Carbonate thin section image (from [this link](http://www.beg.utexas.edu/lmod/_IOL-CM07/old-4.29.03/cm07-step05.htm)
    by F. Jerry Lucia).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: the blue dye in core samples visually indicates void space‚Äîthe pores
    in the rock. But is the porosity feature simply the blue area divided by the total
    area of the sample?'
  prefs: []
  type: TYPE_NORMAL
- en: that would give us total porosity, which is the ratio of total void volume to
    bulk volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: however, not all pore space contributes to fluid flow. Some pores are isolated
    or poorly connected, especially in low-permeability rocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To estimate the porosity that actually matters for flow, we need to interpret
    connectivity. This gives us effective porosity, a more useful feature for modeling
    fluid transport, permeability, and reservoir performance.
  prefs: []
  type: TYPE_NORMAL
- en: so even porosity, often seen as a ‚Äúsimple‚Äù feature that‚Äôs observable in well
    logs and suitable for linear averaging, doesn‚Äôt escape an essential interpretation
    step. Estimating effective porosity involves assumptions, thresholds, or models
    about pore geometry and connectivity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a critical reminder, our features generally require interpretation,
    even when they appear directly measurable. Measurement scale, observation method,
    and intended use all influence how we define and derive a useful feature for our
    data science models.
  prefs: []
  type: TYPE_NORMAL
- en: Predictor and Response Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand the difference between predictor and response features let‚Äôs look
    at the most concise expression of a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0064bedeb876a7d613fc0ceddedd18cd.png)'
  prefs: []
  type: TYPE_IMG
- en: The fundamental predictive machine learning model that maps from predictor to
    response features.
  prefs: []
  type: TYPE_NORMAL
- en: the predictors (or independent) features (or variables) the model inputs, i.e.,
    the \(X_1,\ldots,X_m\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: response (or dependent) feature(s) (or variable(s)) are the model output, \(y\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and there is an error term, \(\epsilon\).
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning is all about estimating such models, \(\hat{ùëì}\), for two purposes,
    inference or prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can make reliable predictions, we must first perform inference‚Äîthat
    is, we must learn about the underlying system from a limited sample in order to
    model the broader population. Inference is the process of building an understanding
    of the system, denoted as \(\hat{f}\), based on observed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through inference, we seek to answer questions like:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the relationships between features?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which features are most important?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there complicated or nonlinear interactions between features?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These insights are prerequisites for making any predictions, because they form
    the foundation of the model that will eventually be used for forecasting, classification,
    or decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine I walk into a room, pull a coin from my pocket, and flip
    it 10 times, observing 3 heads and 7 tails. Then I ask, ‚ÄúIs this a fair coin?‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: This is a classic inferential problem.
  prefs: []
  type: TYPE_NORMAL
- en: you‚Äôre using a limited sample (the 10 flips) to make an inference about the
    population‚Äîin this case, the probability distribution governing the coin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'and here‚Äôs the key twist: in this example, the coin itself is the population!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You‚Äôre not just summarizing the sample, you‚Äôre trying to learn something general
    about the system that generated the sample.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we‚Äôve completed inference‚Äîlearning from a sample to build a model of the
    underlying population‚Äîwe‚Äôre ready to use that model for prediction. Prediction
    is the process of applying what we‚Äôve learned to estimate the outcomes of new
    or future observations.
  prefs: []
  type: TYPE_NORMAL
- en: in short, inference goes from sample to the population, while prediction goes
    from population model to the new sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the goal of prediction is to produce the most accurate estimates of unknown
    or future outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs return to our earlier coin example,
  prefs: []
  type: TYPE_NORMAL
- en: you observe me flip a coin 10 times and see 3 heads and 7 tails. Based on this,
    you infer that the coin is likely biased toward tails.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, before I flip it again, you predict that the next 10 tosses will likely
    result in more tails than heads. That‚Äôs prediction, using your inferred model
    of the coin (i.e., the population) to estimate future data.
  prefs: []
  type: TYPE_NORMAL
- en: Inference vs. Prediction in Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, how do you know whether you‚Äôre doing inference or prediction in the machine
    learning context?
  prefs: []
  type: TYPE_NORMAL
- en: it depends on whether you‚Äôre modeling structure or estimating outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inferential Machine Learning**, also called unsupervised learning, involves
    only predictor features (no known outcomes or labels). The aim is to learn structure
    in the data and improve understanding of the system.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of machine learning inferential methods include,
  prefs: []
  type: TYPE_NORMAL
- en: Cluster analysis ‚Äì groups observations into distinct clusters (potential sub-populations),
    often as a preprocessing step for modeling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction ‚Äì reduces the number of features by finding combinations
    that best preserve information while minimizing noise, for example, princpal components
    analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE) or multidimensional
    scaling (MDS).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These techniques help reveal relationships, patterns, or latent variables in
    the data‚Äîcrucial for good modeling, but are not designed to make direct predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Some may push back on my definition of inferential machine learning and the
    inclusion of cluster analysis, because they include in statistical inference quantification
    of uncertainty and testing hypotheses that is not available in cluster analysis.
  prefs: []
  type: TYPE_NORMAL
- en: I defend my choice by the difficulty and importance of identifying groups in
    subsurface and spatial datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, depofacies for reservoir models commonly describe 80% of heterogeneity,
    i.e., the deposfacies are the most important inference for many reservoirs!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, I have a very broad view of uncertainty modeling and model checking that
    includes various types of resampling, for example, bootstrap and spatial boostrap
    and in the subsurface we can only rarely use analytical confidence intervals and
    hypothesis testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictive Machine Learning**, also called supervised learning, this involves
    both predictor features and response features (labels). The model is trained to
    predict the response for new samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of machine learning predictive methods include,
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression - estimating the response feature as a linear combination
    of the predictor features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes classifier - estimating the probability of each possible categorical
    result for a response feature from the product sum of the prior probability and
    each likelihood conditional probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is optimized to minimize prediction error, often evaluated through
    cross-validation or test data. Let‚Äôs talk about how to make predictive machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Tuning Predictive Machine Learning Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In predictive machine learning, we follow a standard model training and testing
    workflow. This process ensures that our model generalizes well to new data, rather
    than just fitting the training data perfectly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31abfecc21c246ba7144dfb847ab4378.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard predictive machine learning modeling workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs walk through the key steps,
  prefs: []
  type: TYPE_NORMAL
- en: '**Train and Test Split** - divide the available data into mutually exclusive,
    exhaustive subsets: a training set and a testing set.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: typically, 15%‚Äì30% of the data is held out for testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the remaining 70%‚Äì85% is used for training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define a range of hyperparameter(s)** values to explore, ranging from,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: simple models with low flexibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to complex models with high flexibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\quad\) This step may involve tuning multiple hyperparameters, in which case
    efficient sampling methods (e.g., grid search, random search, or Bayesian optimization)
    are often used.
  prefs: []
  type: TYPE_NORMAL
- en: '**Train Model Parameters for Each Hyperparameter Setting** - for each set of
    hyperparameters, train a model on the training data. This yields:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a suite of trained models, each with different complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: each model has parameters optimized to minimize error on the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluate Each Model on the Withheld Testing Data** - using the testing data,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: evaluate how each trained model performs on unseen data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: summarize prediction error (for example, root mean square error (RMSE), mean
    absolute error (MAE), classification accuracy) for each model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Select the Hyperparameters That Minimize Test Error** - this is the hyperparameter
    tuning step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: choose the model hyperparaemter(s) that performs best on the test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: these are your tuned hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrain the Final Model on All Data Using Tuned Hyperparameters** - now that
    the best model complexity has been identified,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: retrain the model using both the training and test sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this maximizes the amount of data used for final model parameter estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the resulting model is the one you deploy in real-world applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common Questions About the Model Training and Tuning Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a professor, I often hear these questions when I introduce the above machine
    learning model training and tuning workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the main outcome of steps 1‚Äì5?** - the only reliable outcome is the
    tuned hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\quad\) we do not use the model trained in step 3 or 4 directly, because it
    was trained without access to all available data. Instead, we retrain the final
    model using all data with the selected hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why not train the model on all the data from the beginning?** - because if
    we do that, we have no independent way to evaluate the model‚Äôs generalization.
    A very complex model can easily overfit‚Äîfitting the training data perfectly, but
    performing poorly on new, unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\quad\) overfitting happens when model flexibility is too high‚Äîit captures
    noise instead of the underlying pattern. Without a withheld test set, we can‚Äôt
    detect this.
  prefs: []
  type: TYPE_NORMAL
- en: This workflow for training and tuning predictive machine learning models is,
  prefs: []
  type: TYPE_NORMAL
- en: an empirical, cross-validation-based process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a practical simulation of real-world model use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a method to identify the model complexity that best balances fit and generalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôve said model parameters and model hyperparameters a bunch of times, so I
    owe you their definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Model Parameters and Model Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Model parameters** are fit during training phase to minimize error at the
    training data, i.e., model parameters are trained with training data and control
    model fit to the data. For example,'
  prefs: []
  type: TYPE_NORMAL
- en: for the polynomial predictive machine learning model from the machine learning
    workflow example above, the model parameters are the polynomial coefficients,
    e.g., \(b_3\), \(b_2\), \(b_1\) and \(c\) (often called \(b_0\)) for the third
    order polynomial model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/85cce1589249c12a11ff52ea10e6c893.png)'
  prefs: []
  type: TYPE_IMG
- en: Model parameters are adjusted to fit of the model to the data, i.e., model parameters
    are trained to minimize error over the training data (x markers).
  prefs: []
  type: TYPE_NORMAL
- en: '**Model hyperparameters** are very different. They do not constrain the model
    fit to the data directly, instead they constrain the model complexity. The model
    hyperparameters are selected (call tuning) to minimize error at the withheld testing
    data. Going back to our polynomial predictive machine learning example, the choice
    of polynomial order is the model hyperparameter.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aef7a81bd36cd7f2b25e1574784b4932.png)'
  prefs: []
  type: TYPE_IMG
- en: Model hyperparameters are adjusted to change the model complexity / flexibility,
    i.e., model hyperparameters are tuned to minimize error over the withheld testing
    data (solid circles).
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters vs. model hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters control the model fit and are trained with training data. Model
    hyperparameters control the model complexity and are tuned with testing data.
  prefs: []
  type: TYPE_NORMAL
- en: Regression and Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we proceed, we need to define regression and classification.
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression** - a predictive machine learning model where the response feature(s)
    is continuous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification** - a predictive machine learning model where the response
    feature(s) is categorical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It turns out that for each of these we need to build different models and use
    different methods to score these models.
  prefs: []
  type: TYPE_NORMAL
- en: for the remainder of this discussion we will focus on regression, but in later
    chapters we introduce classification models as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, to better understand predictive machine learning model tuning, i.e., the
    empirical approach to tune model complexity to minimize testing error, we need
    to understand the sources of testing error.
  prefs: []
  type: TYPE_NORMAL
- en: the causes of the thing that we are attempting to minimize!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sources of Predictive Machine Learning Testing Error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mean square error (MSE) is a standard way to express error. Mean squared error
    is known as a norm because we at taking a vector of error (over all of the data)
    and summarizing with a single, non-negative value.
  prefs: []
  type: TYPE_NORMAL
- en: specifically MSER is the L2 norm because the errors are squared before they
    are summed,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the L2 norm has a lot of nice properties including a continuous error function
    that can be differentiated over all values of error, but it is sensitive to data
    outliers that may have a disproportionate impact on the sum of the squares. More
    about this later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ MSE = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual observation of the response feature and \(\hat{y}_i\)
    is the model estimate over data indexed, \(i = 1,\ldots,n\). The \(\hat{y}_i\)
    is estimated with our predictive machine learning model with a general form,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{y}_i = \hat{f}(x^1_i, \ldots , x^m_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\hat{f}\) is our predictive machine learning model and \(x^1_i, \ldots
    , x^m_i\) are the predictor feature values for the \(i^{th}\) data.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, MSE can be calculated over training data and this is often the loss
    function that we minimize for training the model parameters for regression models.
  prefs: []
  type: TYPE_NORMAL
- en: \[ MSE_{train} = \frac{1}{n_{train}} \sum_{i=1}^{n_{train}} \left( y_i - \hat{y}_i
    \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and MSE can be calculated over the withheld testing data for hyperparameter
    tuning of regression models.
  prefs: []
  type: TYPE_NORMAL
- en: If we take the \(MSE_{test}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ MSE_{test} = \frac{1}{n_{test}} \sum_{i=1}^{n_{test}} \left( y_i - \hat{f}(x^1_i,
    \ldots , x^m_i) \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: but we pose it as the expected test square error we get this expectation form,
  prefs: []
  type: TYPE_NORMAL
- en: \[ E \left[ \left( y_0 - \hat{f}(x^1_0, \ldots , x^m_0) \right)^2 \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: where we use the \(_0\) notation to indicate data samples not in the training
    dataset split, but in the withheld testing split. We can expand the quadratic
    and group the terms to get this convenient decomposition of expect test square
    error into three additive sources (derivation is available in [Hastie et al.,
    2009](https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/ref=sr_1_1?crid=32MDGH9EIGR9T&dib=eyJ2IjoiMSJ9.p0bVgWPuChRIt10algkzSRLwDHzW4bCihIh1RA6GGZDYFtIQN37sOnIqDS5rCJ4fF5dgqsleBiGbmgJUSXISlcmayLc6C0aOcXVX8iCtyZElt9qVbd-Dvq9P3x4KTBlzHCFHDtjz0ImJUAd3LhT6D6KhOtiHOAveaz8xiE4jU1ah6LlDo0xzGoQVDXzNE6ODFzysbfBvzJMRGXhLbQBD292ixuH_tTBTPZwOzNGzpIw.TpRqJ1llQqMdsDXdXhWr0WQIhzTn6rpjDKSAzA10E_I&dib_tag=se&keywords=hastie+machine+learning&qid=1728683359&s=books&sprefix=hastie+machine+learn%2Cstripbooks%2C187&sr=1-1),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ae706c217c5b116fd94a810b1740d35.png)'
  prefs: []
  type: TYPE_IMG
- en: Model error in testing with three additive components, model variance, model
    bias and irreducible error.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs explain these three additive sources of error over test data, i.e., our
    best representation of the error we expect in the real-world use of our model
    to predict for cases not used to train the model,
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Variance** - is error due to sensitivity to the dataset, for example
    a simple model like linear regression does not change much if we change the training
    data, but a more complicated model like a fifth order polynomial model will jump
    around a lot as we change the training data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(\quad\) More model complexity tends to increase model variance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Bias** - is error due to using an approximate model, i.e., the model
    is too simple to fit the natural phenomenon. A very simple model is inflexible
    and will generally have higher model bias while a more complicated model is flexible
    enough to fit the data and will have lower model bias.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(\quad\) More model complexity tends to descrease model bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Irreducible Error** - is due to missing and incomplete data. There may be
    important features that were not sampled, or ranges of feature values that were
    not sampled. This is the error due to data limitations that cannot be address
    by the machine learning model hyperparameter tuning for optimum complexity; therefore,
    irreducible error is constant over the range of model complexity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(\quad\) More model complexity should not change irreducible error.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can take these three additive sources of error and produce an instructive,
    schematic plot,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e8b432c2838bbeaa8f6807e2658cc53.png)'
  prefs: []
  type: TYPE_IMG
- en: Model error in testing vs. model complexity with three additive error components,
    model variance, model bias and irreducible error.
  prefs: []
  type: TYPE_NORMAL
- en: From this plot we can observe, the **model bias-variance trade off**,
  prefs: []
  type: TYPE_NORMAL
- en: testing error is high for low complexity models due to high model bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: testing error is high for high complexity models due to high model variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So hyperparameter tuning is an optimization of the model bias-variance trade
    off. Wwe select the model complexity via hyperparameters that results in a model
    that is not,
  prefs: []
  type: TYPE_NORMAL
- en: '**underfit** - too simple, too inflexible to fit the natural phenomenon'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**overfit** - too complicated, too flexible and is too sensitive to the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let‚Äôs get into more details about under and overfit machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Underfit and Overfit Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First visualize underfit and overfit first with a simple prediction problem
    with one predictor feature and one response feature, here‚Äôs a model that is too
    simple (left) and a model that is too complicated (right).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb749f54d2627552ab6b8dbb87763120.png)'
  prefs: []
  type: TYPE_IMG
- en: Example underfit model (left), a model that is too simple / inflexible and overfit
    model (right), a model that is too complicated / flexible.
  prefs: []
  type: TYPE_NORMAL
- en: Some observations,
  prefs: []
  type: TYPE_NORMAL
- en: the simple model is not sufficiently flexible to fit the data, this is likely
    an underfit model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the overfit model perfectly fits all of the training data but is too noisy away
    from the training data and is likely an overfit model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To better understand the difference, we can now plot the training error alongside
    the previously shown testing error curve, illustrating how both change with model
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc91c3c39111509132db7e06602eb769.png)'
  prefs: []
  type: TYPE_IMG
- en: Model error in training and testing vs. model complexity with illustration of
    under- and overfit model regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the plot, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Error** ‚Äì as model complexity increases, training error consistently
    decreases. With sufficient flexibility, the model can eventually fit the training
    data perfectly, driving training error to zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing Error** ‚Äì testing error is composed of bias, variance, and irreducible
    error. The interplay between bias and variance leads to a trade-off, resulting
    in an optimal model complexity that minimizes testing error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on these observations, we can characterize underfitting and overfitting
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Underfitting** - occurs when a model is too simple to capture the underlying
    patterns in the data. Such models fail to generalize well and tend to show high
    training and testing errors. As complexity increases, testing error decreases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting** - occurs when a model is too complex and captures not only
    the underlying data patterns but also the noise. While training error continues
    to decrease, testing error starts to increase due to poor generalization. These
    models can give a false impression of strong performance, misleading us into thinking
    we understand the system better than we do.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More About Training and Testing Splits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A critical part of the machine learning training and tuning workflow is the
    training and testing data split. Here is some more considerations about training
    and testing data splits,
  prefs: []
  type: TYPE_NORMAL
- en: Proportion Withheld for Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Meta studies have identified that between 15% and 30% withheld for testing is
    typically optimum. To understand the underlying trade-off imagine these end members,
  prefs: []
  type: TYPE_NORMAL
- en: withhold 2% of the data for testing, then we will have a lot of training data
    to train the model very well, but too few testing data to test our model over
    a range of prediction cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: withhold 98% of the data for test, then with only 2% of the data available for
    training we will do a very good job testing our very poor model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proportion to withhold for testing is a trade off between building a good
    model and testing this model well with a wide range of prediction cases.
  prefs: []
  type: TYPE_NORMAL
- en: Other Cross Validation Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The train and test workflow above known as a cross validation or hold out approach,
    but there are many other methods for model training and testing,
  prefs: []
  type: TYPE_NORMAL
- en: '**k-fold Cross Validation (K-fold CV)** - we divide the data into k mutually
    exclusive, exhaustive equal size sets (called folds) for repeat the model train
    and test k times with each fold having a turn being withheld testing data while
    the remainder is training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leave-One-Out Cross Validation (LOO CV)** - we loop over all data, each time
    we assign the one datum as the testing data and train on the remainder n-1 data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leave-p-Out Cross Validation (LpO-CV)** - we assign a integer p < n and then
    loop over the combinatorial of possible cases with p withheld testing data. Since
    we consider all possible cases, this is an exhaustive cross validation approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each of the above folds or in general, training and testing dataset combinations,
    we calculate the error norm
  prefs: []
  type: TYPE_NORMAL
- en: then we average the error norm over the folds to provide a single error for
    hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this method removes the sensitivity to the exact train and test split so it
    is often seen as more robust than regular hold out methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of proportion of the data to withhold for testing varies by the the
    cross validation method,
  prefs: []
  type: TYPE_NORMAL
- en: for k-fold cross validation the proportion of testing is implicit to the choice
    of k, for k = 3, 33% of data are withheld for each fold and for k=5, 20% of data
    are withheld for each fold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for leave-one-out cross validation the proportion of testing is \(\frac{1}{n}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for leave-p-out cross validation the proportion of testing is \(\frac{p}{n}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train, Validate and Test Split
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is an alternative approach with three exhaustive, mutually exclusive subsets
    of the data
  prefs: []
  type: TYPE_NORMAL
- en: '**training data** - subset is the same as train above and is used to train
    the model parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**validate data** - subset is like testing subset for the train and test split
    method above and is used to tune the model hyperparameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**testing data** - subset is applied to check the final model trained on all
    the data with the tuned hyperparameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The train, validate and test split philosophy is that this final model check
    is performed with data that was not involved with model construction, including
    training model parameters nor tuning model hyperparameters. There are two reasons
    that I push back on this method,
  prefs: []
  type: TYPE_NORMAL
- en: '**circular quest of perfect model validation** - what do we do if we don‚Äôt
    quite like the performance in this testing phase, do we have a fourth subset for
    another test? and a fifth subset? and a sixth subset? ad infinitum?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**we must train the deployed model with all data** - we can never deploy a
    model that is not trained with all available data; therefore, we will still have
    to train with the test subset to get our final model?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**reducing modeling training and tuning performance** - the third data split
    for the testing phase reduces the data available for model training and tuning
    reducing the perforamnce of these critical steps.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spatial Fair Train and Test Splits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dr. Julian Salazar suggested that for spatial prediction problems that random
    train and test split may not fair. He proposed a [fair train and test split method](https://www.sciencedirect.com/science/article/pii/S0920410521015023)
    for spatial prediction models that splits the data based on the difficulty of
    the planned use of the model. Prediction difficulty is related to kriging variance
    that accounts for spatial continuity and distance offset. For example,
  prefs: []
  type: TYPE_NORMAL
- en: if the model will be used to impute data with small offsets from available data
    then construct a train and test split with train data close to test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if the model will be used to predict a large distance offsets then perform splits
    the result is large offsets between train and test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this method the tuned model may vary based on the planned real-world use
    for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical and Professional Practice Concerns with Data Science
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To demonstrate concerns with data science consider this example, [Rideiro et
    al. (2016)](https://arxiv.org/pdf/1602.04938.pdf) trained a logistic regression
    classifier with 20 wolf and dog images to detect the difference between wolves
    and dogs.
  prefs: []
  type: TYPE_NORMAL
- en: the input is a photo of a dog or wolf and the output is the probability of dog
    and the compliment, the probability of wolf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the model worked well until this example here (see the left side below),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/38b5db20dcd3b75de5c598c4ae89a459.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dog misclassified as a wolf (left) and pixels that resulted in this
    misclassification (right), image taken from Rideiro et al. (2016).
  prefs: []
  type: TYPE_NORMAL
- en: where this results in a high probability of wolf. Fortunately the authors were
    able to interrogate the model and determine the pixels that had the greatest influence
    of the model‚Äôs determination of wolf (see the right side above). What happened?
  prefs: []
  type: TYPE_NORMAL
- en: they trained a model to check for snow in the background. As a Canadian, I can
    assure you that many photos of wolves are in our snow filled northern regions
    of our country, while many dogs are photographed in grassy yards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problems with machine learning are,
  prefs: []
  type: TYPE_NORMAL
- en: interpretability may be low with complicated models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: application of machine learning may become routine and trusted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a dangerous combination, as the machine may become a trusted unquestioned
    authority. Rideiro and others state that they developed the problem to demonstrate
    this, but does it actually happen? Yes. I advised a team of students that attempted
    to automatically segment urban vs. rural environments from time lapse satellite
    photographs to build models of urban development. The model looked great, but
    I asked for an additional check with a plot of the by-pixel classification vs.
    pixel color. The results was a 100% correspondence, i.e., the model with all of
    it‚Äôs complicated convolution, activation, pooling was only looking for grey and
    tan pixels often associated with roads and buildings.
  prefs: []
  type: TYPE_NORMAL
- en: 'I‚Äôm not going to say, ‚ÄòSkynet‚Äô, oops I just did, but just consider these thoughts:'
  prefs: []
  type: TYPE_NORMAL
- en: New power and distribution of wealth by concentrating rapid inference with big
    data as more data is being shared
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trade-offs that matter to society while maximizing a machine learning objective
    function may be ignored, resulting in low interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Societal changes, disruptive technologies, post-labor society
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-scientific results such as [Clever Hans](https://en.wikipedia.org/wiki/Clever_Hans)
    effect with models that learn from tells in the data rather than really learning
    to perform the task resulting catastrophic failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I don‚Äôt want to be too negative and to take this too far. Full disclosure, I‚Äôm
    the old-fashioned professor that thinks we should put of phones in our pockets,
    walk around with our heads up so we can greet each other and observe our amazing
    environments and societies. One thing that is certain, data science is changing
    society in so many ways and as Neil Postman in [Technopoly](https://www.amazon.com/Technopoly-Surrender-Technology-Neil-Postman-ebook/dp/B004ZZJBW4/ref=sr_1_1?crid=E9SP0DFNP9JO&dib=eyJ2IjoiMSJ9.ELnF0aIjkOw11vdTEQ3Tpg.NSaPUOIOz6m6i7XxpEMUJvZobANzU5baE6DlEa40Uzs&dib_tag=se&keywords=technolopoly&qid=1728667409&sprefix=technolopo%2Caps%2C127&sr=8-1))
  prefs: []
  type: TYPE_NORMAL
- en: Neil Postman‚Äôs Quote for Technopoly
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúOnce a technology is admitted, it plays out its hand.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic description of machine learning concepts. Much more could be
    done and discussed, I have many more resources. Check out my [shared resource
    inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture links
    at the start of this chapter with resource links in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this was helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster,
    Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling
    and machine learning theory with practice to develop novel methods and workflows
    to add value. We are solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for Machine Learning Concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You could just open up a Jupyter notebook in Python and start building machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: the [scikit-learn docs](https://scikit-learn.org/stable/) are quite good and
    for every machine learning function there is a short code example that you could
    copy and paste to repeat their work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, you could Google a question about using a specific machine learning algorithm
    in Python and the top results will include [StackOverflow](https://stackoverflow.com/)
    questions and responses, it is truly amazing how much experienced coders are willing
    to give back and share their knowledge. We truly have an amazing scientific community
    with the spirit of knowledge sharing and open-source development. Respect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: of course, you could learn a lot about machine learning from a machine learning
    large language model (LLM) like [ChatGPT](https://chatgpt.com/). Not only will
    ChatGPT answer your questions, but it will also provide codes and help you debug
    them when you tell it what went wrong.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One way of the other and you received and added this code to you data science
    workflow,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: et voil√† (note I‚Äôm Canadian, so I use some French phrases), you have a trained
    predictive machine learning model that could be applied to make predictions for
    new cases.
  prefs: []
  type: TYPE_NORMAL
- en: But, is this a good model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How good is it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Could it be better?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Without knowledge about basic machine learning concepts we can‚Äôt answer these
    questions and build the best possible models. In general, I‚Äôm not an advocate
    for black box modeling, because it is:'
  prefs: []
  type: TYPE_NORMAL
- en: likely to lead to mistakes that may be difficult to detect and correct
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: incompatible with the expectations for competent practice for professional engineers.
    I gave a talk on [Applying Machine Learning as a Compentent Engineer or Geoscientist](https://youtu.be/W_ZDg1Wb2vM?si=DF-n1E4-ik2ukLfF)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To help out this chapter provides you with the basic knowledge to answer these
    questions and to make better, more reliable machine learning models. Let‚Äôs start
    building this essential foundation with some definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Big Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Everyone hears that machine learning needs a lot of data. In fact, so much data
    that it is called ‚Äúbig data‚Äù, but how do you know if you are working with big
    data?
  prefs: []
  type: TYPE_NORMAL
- en: the criteria for big data are these ‚ÄòV‚Äôs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you answer ‚Äúyes‚Äù for at least some of these criteria, then you are working
    with big data,
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume**: many data samples, difficult to handle and visualize'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity**: high rate collection, continuous relative to decision making
    cycles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**: data form various sources, with various types and scales'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variability**: data acquisition changes during the project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Veracity**: data has various levels of accuracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In my experience, most subsurface engineers and geoscientists answer ‚Äúyes‚Äù to
    all of these ‚Äòv‚Äô criteria.
  prefs: []
  type: TYPE_NORMAL
- en: so I proudly say that we in the subsurface have been big data long before the
    tech sector learned about big data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In fact, I often state that we in the subsurface resource industries are the
    original data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôm getting ahead of myself, more on this in a bit. Don‚Äôt worry if I get carried
    away in hubris, rest assured this e-book is written for anyone interested to learn
    about machine learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you can skip the short sections on subsurface data science or read along if
    interested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know big data, let‚Äôs talk about the big data related topics, statistics,
    geostatistics and data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics, Geostatistics and Data Analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Statistics is the practice of,
  prefs: []
  type: TYPE_NORMAL
- en: collecting data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: organizing data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: interpreting data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: drawing conclusions from data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: making decisions with data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is all about moving from data to decision.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of decision making in statistics.
  prefs: []
  type: TYPE_NORMAL
- en: If your work does not impact the decision, you do not add value!
  prefs: []
  type: TYPE_NORMAL
- en: If you look up the definition of data analytics, you will find criteria that
    include, statistical analysis, and data visualization to support decision making.
  prefs: []
  type: TYPE_NORMAL
- en: I call it, data analytics and statistics are the same thing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we can append, geostatistics as a branch of applied statistics that accounts
    for,
  prefs: []
  type: TYPE_NORMAL
- en: the spatial (geological) context
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the spatial relationships
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: volumetric support
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: uncertainty
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remember all those statistics classes with the assumption of i.i.d., independent,
    identically distributed.
  prefs: []
  type: TYPE_NORMAL
- en: spatial phenomenon are not i.i.d., so we developed a unique branch of statistics
    to address this
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: by our assumption above (statistics is data analytics), we can state that geostatistics
    is the same as spatial data analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let‚Äôs use a Venn diagram to visualize statistics / data analytics and geostatistics
    / spatial data analytics,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d590b072c7838bf9671b2ee7b8bc2de2.png)'
  prefs: []
  type: TYPE_IMG
- en: Venn diagram for statistics and geostatistics.
  prefs: []
  type: TYPE_NORMAL
- en: and we can add our previously discussed big data to our Venn diagram resulting
    in big data analytics and spatial big data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c885d770c796b22e4c9022c27037a97c.png)'
  prefs: []
  type: TYPE_IMG
- en: Venn diagram for statistics and geostatistics with big data added.
  prefs: []
  type: TYPE_NORMAL
- en: Scientific Paradigms and the Fourth Paradigm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If no one else has said this to you, let me have the honor of saying,
  prefs: []
  type: TYPE_NORMAL
- en: Welcome
  prefs: []
  type: TYPE_NORMAL
- en: to the fourth paradigm of scientific discovery, data-driven scientific discovery
    or we can just call it data science.
  prefs: []
  type: TYPE_NORMAL
- en: The paradigms of scientific discovery are distinct approaches for humanity to
    apply science to expand human knowledge, discover and develop new technologies
    that impact society. These paradigms include,
  prefs: []
  type: TYPE_NORMAL
- en: '**First Paradigm** - logical reasoning with physical experiment and observation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Second Paradigm** - theory-driven, where mathematical models and laws are
    used to explain and predict natural phenomena'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Third Paradigm** - computer-based numerical simulations and models to study
    complex systems that are too difficult or impossible to solve analytically or
    test experimentally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fourth Paradigm** - analysis of massive datasets with tools such as data
    analytics, machine learning, cloud computing and AI, patterns in the data guide
    insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs all of the four paradigms with dates for important developments with
    each,
  prefs: []
  type: TYPE_NORMAL
- en: '| 1st Paradigm Empirical Science | 2nd Paradigm Theoretical Science | 3rd Paradigm
    Computational Science | 4th Paradigm Data Science |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Logic and Experiments | Theory and Models | Numerical Simulation | Learning
    from Data |'
  prefs: []
  type: TYPE_TB
- en: '| \(\approx\) 600 BC Thales of Miletus predicted a solar ecplipse | \(\approx\)
    300 BC Euclid - Elements | 1946 von Neumann weather simulation | 1990s Human Genome
    Project |'
  prefs: []
  type: TYPE_TB
- en: '| \(\approx\) 400 BC Hippocrates natural causes for diseases | \(\approx\)
    150 AD Ptolemy planetary motion model | 1952 Fermi simulated nonlinear systems
    | 2008 Large Hadron Collider |'
  prefs: []
  type: TYPE_TB
- en: '| 430 BC Empedocles proved air has substance | 1011 AD al-Haytham Book of Optics
    | 1957 Lorenz demonstrates chaos | 2009 Hey et al. Data-Intensive Book |'
  prefs: []
  type: TYPE_TB
- en: '| 230 BC Eratosthenes measures Earth‚Äôs diameter | 1687 AD Newton Principia
    | 1980s Mandelbrot simulating fractals | 2015 AlphaGo beats a professional Go
    player |'
  prefs: []
  type: TYPE_TB
- en: Of course, we can argue about the boundaries between the paradigms for scientific
    discovery, i.e., when did a specific paradigm begin? For example, consider these
    examples of the application of the first paradigm that push the start of the first
    paradigm deep into antiquity!
  prefs: []
  type: TYPE_NORMAL
- en: '| Field | Example | Approx. Date | Why It Fits 1st Paradigm |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Agriculture | Irrigation systems using canals, gates, and timing | \(\approx\)
    3000‚ÄØBC | Empirical understanding of water flow, soil, and seasonal timing |'
  prefs: []
  type: TYPE_TB
- en: '| Astronomy | Babylonian star charts; lunar calendars | \(\approx\) 1800‚ÄØBC
    | Systematic observations for predicting celestial events |'
  prefs: []
  type: TYPE_TB
- en: '| Engineering | The wheel (transport, pottery) | \(\approx\) 3500‚ÄØBC | Developed
    through trial-and-error and practical refinement |'
  prefs: []
  type: TYPE_TB
- en: '| Medicine | Herbal remedies, surgical procedures | \(\approx\) 2000‚ÄØBC | Based
    on observation and accumulated case knowledge |'
  prefs: []
  type: TYPE_TB
- en: '| Mathematics | Base-60 system; geometric calculation tablets | \(\approx\)
    2000‚ÄØBC | Used for land division, architecture, and astronomy; empirically tested
    |'
  prefs: []
  type: TYPE_TB
- en: certainly the Mesopotamians (4000 BC - 3500 BC) applied the first paradigm with
    their experiments that supported the development of the wheel, plow, chariot,
    weaving loom, and irrigation. Aside, I wanted to give a shout out to the ancient
    peoples, because I‚Äôm quite fascinated by the development of human societies and
    their technologies over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other side, we can trace the development of fourth paradigm approaches,
    for example the artificial neural networks to McColloch and Pitts in 1943 and
    consider these early examples of learning by distilling patterns from large datasets,
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Field | Example / Person | Why It Fits 4th Paradigm Traits |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| \(\approx\)100‚ÄØAD | Astronomy | Ptolemy‚Äôs star catalog | Massive empirical
    data used to predict celestial motion |'
  prefs: []
  type: TYPE_TB
- en: '| 1086 | Economics | Domesday Book (England) | Large-scale data census for
    administrative, economic decisions |'
  prefs: []
  type: TYPE_TB
- en: '| 1700s | Natural history | Carl Linnaeus ‚Äì taxonomy | Classification of species
    based on observable traits; data-first approach |'
  prefs: []
  type: TYPE_TB
- en: '| 1801 | Statistics | Adolphe Quetelet ‚Äì social physics | Used statistics to
    analyze human behavior from large datasets |'
  prefs: []
  type: TYPE_TB
- en: '| 1830s | Astronomy | John Herschel‚Äôs sky surveys | Cataloged thousands of
    stars and nebulae; data-driven sky classification |'
  prefs: []
  type: TYPE_TB
- en: '| 1859 | Biology | Darwin ‚Äì *Origin of Species* | Synthesized global species
    data to uncover evolutionary patterns |'
  prefs: []
  type: TYPE_TB
- en: '| 1890 | Public health | Florence Nightingale | Used mortality data visualization
    to influence hospital reform |'
  prefs: []
  type: TYPE_TB
- en: '| 1890 | Census, Computing | Hollerith punch cards (U.S. Census) | First automated
    analysis of large datasets; mechanical data processing |'
  prefs: []
  type: TYPE_TB
- en: '| 1910s‚Äì30s | Genetics | Morgan lab ‚Äì Drosophila studies | Mapped gene traits
    from thousands of fruit fly crosses |'
  prefs: []
  type: TYPE_TB
- en: '| 1920s | Economics | Kondratiev, Kuznets | Identified economic patterns using
    historical data series |'
  prefs: []
  type: TYPE_TB
- en: '| 1937 | Ecology | Arthur Tansley ‚Äì ecosystem concept | Integrated large-scale
    field data into systemic ecological frameworks |'
  prefs: []
  type: TYPE_TB
- en: Finally, the adoption of a new scientific paradigm represents a significant
    societal shift that unfolds unevenly across the globe. While I aim to summarize
    the chronology, it‚Äôs important to acknowledge its complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Foundations and Enablers of the Fourth Scientific Paradigm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What caused the fourth paradigm to start some time between 1943 and 2009? Is
    it the development of new math?
  prefs: []
  type: TYPE_NORMAL
- en: the foundational mathematics behind data-driven models has been available for
    decades and even centuries. Consider the following examples of mathematical and
    statistical developments that underpin modern data science,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Technique | Key Contributor(s) | Date / Publication |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Calculus** | Isaac Newton, Gottfried Wilhelm Leibniz | Newton: *Methods
    of Fluxions* (1671, pub. 1736); Leibniz: *Nova Methodus* (1684) |'
  prefs: []
  type: TYPE_TB
- en: '| **Bayesian Probability** | Reverend Thomas Bayes | *An Essay Towards Solving
    a Problem in the Doctrine of Chances* (posthumous, 1763) |'
  prefs: []
  type: TYPE_TB
- en: '| **Linear Regression** | Marie Legendre | Formalized in 1805 |'
  prefs: []
  type: TYPE_TB
- en: '| **Discriminant Analysis** | Ronald Fisher | *The Use of Multiple Measurements
    in Taxonomic Problems* (1939) |'
  prefs: []
  type: TYPE_TB
- en: '| **Monte Carlo Simulation** | Stanislaw Ulam, John von Neumann | Developed
    in early 1940s during the Manhattan Project |'
  prefs: []
  type: TYPE_TB
- en: 'Upon reflection, one might ask: why didn‚Äôt the fourth scientific paradigm emerge
    earlier‚Äîsay, in the 1800s or even before the 1940s? What changed? The answer lies
    in the fact that several key developments were still needed to create the fertile
    ground for data science to take root.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: the availability of inexpensive, accessible computing power
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the emergence of massive, high-volume datasets (big data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs explore each of these in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Cheap and Available Compute
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning from big data is not feasible without affordable and widely available
    computing and storage resources. Consider the following computational advancements
    that paved the way for the emergence of the fourth scientific paradigm,
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Milestone | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1837** | Babbage‚Äôs Analytical Engine | First design of a mechanical computer
    with concepts like memory and control flow (not built). |'
  prefs: []
  type: TYPE_TB
- en: '| **1941‚Äì45** | Zuse‚Äôs Z3, ENIAC | First programmable digital computers; rewired
    plugboards, limited memory, slow and unreliable. |'
  prefs: []
  type: TYPE_TB
- en: '| **1947** | Transistor Invented | Replaced vacuum tubes; enabled faster, smaller,
    more reliable second-gen computers (e.g., IBM 7090). |'
  prefs: []
  type: TYPE_TB
- en: '| **1960s** | Integrated Circuits | Multiple transistors on a single chip;
    led to third-generation computers. |'
  prefs: []
  type: TYPE_TB
- en: '| **1971** | Microprocessor Developed | Integrated CPU on a chip; enabled personal
    computers like Apple II (1977) and IBM PC (1981). |'
  prefs: []
  type: TYPE_TB
- en: Yes, when I was in grade 1 of my elementary school education we had the first
    computers in classrooms (Apple IIe), and it amazed all of us with the monochrome
    (orange and black pixels) monitor, 5 1/4 inch floppy disk loaded programs (there
    was no hard drive) and the beeps and clicks from the computer (no dedicated sound
    chip)!
  prefs: []
  type: TYPE_NORMAL
- en: We live in a society with more compute power in our pockets, i.e., our cell
    phones, than used to send the first astronauts to the moon and a SETI screen saver
    that uses home computers‚Äô idle time to search for extraterrestrial life!
  prefs: []
  type: TYPE_NORMAL
- en: '[SETI@home](https://setiathome.berkeley.edu/) - search for extraterrestrial
    intelligence. Note the @home component is now in hibernation while the team continuous
    the analysis the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also check out these ways to put your computer‚Äôs idle time to good use,
  prefs: []
  type: TYPE_NORMAL
- en: '[Folding@home](https://foldingathome.org/) - simulate protein dynamics, including
    the process of protein folding and the movements of proteins to develop therapeutics
    for treating disease.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Einstein@home](https://einsteinathome.org/) - search for weak astrophysical
    signals from spinning neutron stars, with big data from the LIGO gravitational-wave
    detectors, the MeerKAT radio telescope, the Fermi gamma-ray satellite, and the
    archival data from the Arecibo radio telescope.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we are surrounded by cheap and reliable compute that our grandparents (and
    perhaps even our parents) could scarcely have imagined.
  prefs: []
  type: TYPE_NORMAL
- en: As you will learn in this e-book, machine learning methods requires a lot of
    compute, because training most of these methods relies on,
  prefs: []
  type: TYPE_NORMAL
- en: numerous iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: matrix or parallel computations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bootstrapping techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stochastic gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: extensive model tuning, which may involve training many versions in nested loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affordable and accessible computing is a fundamental prerequisite for the fourth
    scientific paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: the development of data science has been largely driven by crowd-sourced and
    open-source contributions‚Äîefforts that rely on widespread access to computing
    power
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the era of Babbage‚Äôs Analytical Engine or the ENIAC, data science at scale
    would have been simply impossible.
  prefs: []
  type: TYPE_NORMAL
- en: Availability of Big Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With small data, we often rely on external sources of knowledge such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Physics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Engineering and geoscience principles
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These models are then calibrated using the limited available observations‚Äîa
    common approach in the second and third scientific paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, big data allows us to learn the full range of behaviors of natural
    systems directly from the data itself. In fact, big data often reveals the limitations
    of second and third paradigm models, exposing missing complexities in our traditional
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, big data is essential to:'
  prefs: []
  type: TYPE_NORMAL
- en: provide sufficient sampling to support the data-driven fourth paradigm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may even challenge the exclusive reliance on second and third paradigm models,
    as the complexity of large datasets uncovers the boundaries of our existing theoretical
    frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Today, big data is everywhere! Consider all the open-source and publically available
    data online. Here‚Äôs some great examples,
  prefs: []
  type: TYPE_NORMAL
- en: '| Resource | Description | Link |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Google Earth (Timelapse)** | Satellite imagery (not highest res, but useful
    for land use, geomorphology, and surface evolution studies). Used by research
    teams. | [Google Earth](https://www.google.com/maps) |'
  prefs: []
  type: TYPE_TB
- en: '| **USGS River Gage Data** | Public streamflow and hydrology data; useful for
    analysis and recreational planning (e.g., paddling). | [USGS NWIS](https://waterdata.usgs.gov/nwis)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Government Databases** | Includes U.S. Census and oil & gas production
    statistics. Open for public and research use. | [U.S. Census](https://data.census.gov/),
    [Texas Oil & Gas Data](https://www.rrc.texas.gov/oil-and-gas/research-and-statistics/production-data/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **NASA Moon Trek** | High-res lunar surface imagery from LRO, SELENE, and
    Clementine; interactive visualization and downloads. | [Moon Trek](https://trek.nasa.gov/moon/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Landsat Satellite Archive** | 50+ years of medium-resolution Earth observation
    data (since 1972); invaluable for land cover change. | [USGS EarthExplorer](https://earthexplorer.usgs.gov/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Copernicus Open Access Hub** | European Space Agency‚Äôs Sentinel satellite
    data (e.g., radar, multispectral imagery). | [Copernicus Hub](https://scihub.copernicus.eu/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Global Biodiversity Information Facility (GBIF)** | Open biodiversity data:
    species distributions, observations, and ecological records from around the world.
    | [GBIF](https://www.gbif.org/) |'
  prefs: []
  type: TYPE_TB
- en: '| **National Oceanic and Atmospheric Administration (NOAA)** | Huge archive
    of weather, ocean, and climate data, including radar, reanalysis, and forecasts.
    | [NOAA Data](https://www.ncei.noaa.gov/) |'
  prefs: []
  type: TYPE_TB
- en: '| **World Bank Open Data** | Economic, development, demographic, and environmental
    indicators for all countries. | [World Bank Data](https://data.worldbank.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Our World in Data** | Curated global datasets on health, population, energy,
    CO‚ÇÇ, education, and more. | [Our World in Data](https://ourworldindata.org/) |'
  prefs: []
  type: TYPE_TB
- en: '| **OpenStreetMap (OSM)** | Crowdsourced global geospatial data including roads,
    buildings, and land use. | [OpenStreetMap](https://www.openstreetmap.org/) |'
  prefs: []
  type: TYPE_TB
- en: '| **Google Dataset Search** | Meta-search engine indexing datasets across disciplines
    and repositories. | [Dataset Search](https://datasetsearch.research.google.com/)
    |'
  prefs: []
  type: TYPE_TB
- en: While open data is rapidly expanding, proprietary data is also growing quickly
    across many industries, driven by the adoption of smart, connected systems with
    advanced monitoring and control.
  prefs: []
  type: TYPE_NORMAL
- en: The result of all of this is a massive explosion of data, supported by faster,
    more affordable computing, processing, and storage. We are now immersed in a sea
    of data that previous generations could never have imagined.
  prefs: []
  type: TYPE_NORMAL
- en: Note, we could also talk about improved algorithms and hardware architectures
    optimized for data science, but I‚Äôll leave that out of scope for this e-book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs a local example of freely available big data. I downloaded the daily
    water level of our local Lake Travis from [Water Data for Texas](https://waterdatafortexas.org/reservoirs/individual/travis).
    Note the data is collected and provided by Lower Colorado Water Authority [LRCA](https://www.lcra.org/).
  prefs: []
  type: TYPE_NORMAL
- en: I made a plot of the daily plot and added useful information such as,
  prefs: []
  type: TYPE_NORMAL
- en: water height - in feet above mean sea level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: percentage full - the ratio of the current volume of water stored in a reservoir
    to its total storage capacity, expressed as a percentage, accounting for the shape
    of the reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: max pool - maximum conservation pool, the top of the reservoir used for water
    supply, irrigation, hydropower and recreation and not including flood storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8f21a478ebe4ba93df9f50b7ca4b5e31.png)'
  prefs: []
  type: TYPE_IMG
- en: Historical Lake Austin daily water level and percentage full.
  prefs: []
  type: TYPE_NORMAL
- en: And before you even ask - yes! I did calculate the semivariogram of water level
    and I was surprised that there was no significant nuggest effect.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dae27b907b4dada88f39fe0a7b6950d.png)'
  prefs: []
  type: TYPE_IMG
- en: Historical Lake Austin daily water level variogram.
  prefs: []
  type: TYPE_NORMAL
- en: To become a good data scientists!
  prefs: []
  type: TYPE_NORMAL
- en: Find and play with data!
  prefs: []
  type: TYPE_NORMAL
- en: All of these developments have provided the fertile ground for the seeds of
    data science to impact all sectors or our economy and society.
  prefs: []
  type: TYPE_NORMAL
- en: Cheap and Available Compute
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning from big data is not feasible without affordable and widely available
    computing and storage resources. Consider the following computational advancements
    that paved the way for the emergence of the fourth scientific paradigm,
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Milestone | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1837** | Babbage‚Äôs Analytical Engine | First design of a mechanical computer
    with concepts like memory and control flow (not built). |'
  prefs: []
  type: TYPE_TB
- en: '| **1941‚Äì45** | Zuse‚Äôs Z3, ENIAC | First programmable digital computers; rewired
    plugboards, limited memory, slow and unreliable. |'
  prefs: []
  type: TYPE_TB
- en: '| **1947** | Transistor Invented | Replaced vacuum tubes; enabled faster, smaller,
    more reliable second-gen computers (e.g., IBM 7090). |'
  prefs: []
  type: TYPE_TB
- en: '| **1960s** | Integrated Circuits | Multiple transistors on a single chip;
    led to third-generation computers. |'
  prefs: []
  type: TYPE_TB
- en: '| **1971** | Microprocessor Developed | Integrated CPU on a chip; enabled personal
    computers like Apple II (1977) and IBM PC (1981). |'
  prefs: []
  type: TYPE_TB
- en: Yes, when I was in grade 1 of my elementary school education we had the first
    computers in classrooms (Apple IIe), and it amazed all of us with the monochrome
    (orange and black pixels) monitor, 5 1/4 inch floppy disk loaded programs (there
    was no hard drive) and the beeps and clicks from the computer (no dedicated sound
    chip)!
  prefs: []
  type: TYPE_NORMAL
- en: We live in a society with more compute power in our pockets, i.e., our cell
    phones, than used to send the first astronauts to the moon and a SETI screen saver
    that uses home computers‚Äô idle time to search for extraterrestrial life!
  prefs: []
  type: TYPE_NORMAL
- en: '[SETI@home](https://setiathome.berkeley.edu/) - search for extraterrestrial
    intelligence. Note the @home component is now in hibernation while the team continuous
    the analysis the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also check out these ways to put your computer‚Äôs idle time to good use,
  prefs: []
  type: TYPE_NORMAL
- en: '[Folding@home](https://foldingathome.org/) - simulate protein dynamics, including
    the process of protein folding and the movements of proteins to develop therapeutics
    for treating disease.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Einstein@home](https://einsteinathome.org/) - search for weak astrophysical
    signals from spinning neutron stars, with big data from the LIGO gravitational-wave
    detectors, the MeerKAT radio telescope, the Fermi gamma-ray satellite, and the
    archival data from the Arecibo radio telescope.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we are surrounded by cheap and reliable compute that our grandparents (and
    perhaps even our parents) could scarcely have imagined.
  prefs: []
  type: TYPE_NORMAL
- en: As you will learn in this e-book, machine learning methods requires a lot of
    compute, because training most of these methods relies on,
  prefs: []
  type: TYPE_NORMAL
- en: numerous iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: matrix or parallel computations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bootstrapping techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stochastic gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: extensive model tuning, which may involve training many versions in nested loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affordable and accessible computing is a fundamental prerequisite for the fourth
    scientific paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: the development of data science has been largely driven by crowd-sourced and
    open-source contributions‚Äîefforts that rely on widespread access to computing
    power
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the era of Babbage‚Äôs Analytical Engine or the ENIAC, data science at scale
    would have been simply impossible.
  prefs: []
  type: TYPE_NORMAL
- en: Availability of Big Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With small data, we often rely on external sources of knowledge such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Physics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Engineering and geoscience principles
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These models are then calibrated using the limited available observations‚Äîa
    common approach in the second and third scientific paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, big data allows us to learn the full range of behaviors of natural
    systems directly from the data itself. In fact, big data often reveals the limitations
    of second and third paradigm models, exposing missing complexities in our traditional
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, big data is essential to:'
  prefs: []
  type: TYPE_NORMAL
- en: provide sufficient sampling to support the data-driven fourth paradigm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may even challenge the exclusive reliance on second and third paradigm models,
    as the complexity of large datasets uncovers the boundaries of our existing theoretical
    frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Today, big data is everywhere! Consider all the open-source and publically available
    data online. Here‚Äôs some great examples,
  prefs: []
  type: TYPE_NORMAL
- en: '| Resource | Description | Link |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Google Earth (Timelapse)** | Satellite imagery (not highest res, but useful
    for land use, geomorphology, and surface evolution studies). Used by research
    teams. | [Google Earth](https://www.google.com/maps) |'
  prefs: []
  type: TYPE_TB
- en: '| **USGS River Gage Data** | Public streamflow and hydrology data; useful for
    analysis and recreational planning (e.g., paddling). | [USGS NWIS](https://waterdata.usgs.gov/nwis)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Government Databases** | Includes U.S. Census and oil & gas production
    statistics. Open for public and research use. | [U.S. Census](https://data.census.gov/),
    [Texas Oil & Gas Data](https://www.rrc.texas.gov/oil-and-gas/research-and-statistics/production-data/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **NASA Moon Trek** | High-res lunar surface imagery from LRO, SELENE, and
    Clementine; interactive visualization and downloads. | [Moon Trek](https://trek.nasa.gov/moon/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Landsat Satellite Archive** | 50+ years of medium-resolution Earth observation
    data (since 1972); invaluable for land cover change. | [USGS EarthExplorer](https://earthexplorer.usgs.gov/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Copernicus Open Access Hub** | European Space Agency‚Äôs Sentinel satellite
    data (e.g., radar, multispectral imagery). | [Copernicus Hub](https://scihub.copernicus.eu/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Global Biodiversity Information Facility (GBIF)** | Open biodiversity data:
    species distributions, observations, and ecological records from around the world.
    | [GBIF](https://www.gbif.org/) |'
  prefs: []
  type: TYPE_TB
- en: '| **National Oceanic and Atmospheric Administration (NOAA)** | Huge archive
    of weather, ocean, and climate data, including radar, reanalysis, and forecasts.
    | [NOAA Data](https://www.ncei.noaa.gov/) |'
  prefs: []
  type: TYPE_TB
- en: '| **World Bank Open Data** | Economic, development, demographic, and environmental
    indicators for all countries. | [World Bank Data](https://data.worldbank.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Our World in Data** | Curated global datasets on health, population, energy,
    CO‚ÇÇ, education, and more. | [Our World in Data](https://ourworldindata.org/) |'
  prefs: []
  type: TYPE_TB
- en: '| **OpenStreetMap (OSM)** | Crowdsourced global geospatial data including roads,
    buildings, and land use. | [OpenStreetMap](https://www.openstreetmap.org/) |'
  prefs: []
  type: TYPE_TB
- en: '| **Google Dataset Search** | Meta-search engine indexing datasets across disciplines
    and repositories. | [Dataset Search](https://datasetsearch.research.google.com/)
    |'
  prefs: []
  type: TYPE_TB
- en: While open data is rapidly expanding, proprietary data is also growing quickly
    across many industries, driven by the adoption of smart, connected systems with
    advanced monitoring and control.
  prefs: []
  type: TYPE_NORMAL
- en: The result of all of this is a massive explosion of data, supported by faster,
    more affordable computing, processing, and storage. We are now immersed in a sea
    of data that previous generations could never have imagined.
  prefs: []
  type: TYPE_NORMAL
- en: Note, we could also talk about improved algorithms and hardware architectures
    optimized for data science, but I‚Äôll leave that out of scope for this e-book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs a local example of freely available big data. I downloaded the daily
    water level of our local Lake Travis from [Water Data for Texas](https://waterdatafortexas.org/reservoirs/individual/travis).
    Note the data is collected and provided by Lower Colorado Water Authority [LRCA](https://www.lcra.org/).
  prefs: []
  type: TYPE_NORMAL
- en: I made a plot of the daily plot and added useful information such as,
  prefs: []
  type: TYPE_NORMAL
- en: water height - in feet above mean sea level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: percentage full - the ratio of the current volume of water stored in a reservoir
    to its total storage capacity, expressed as a percentage, accounting for the shape
    of the reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: max pool - maximum conservation pool, the top of the reservoir used for water
    supply, irrigation, hydropower and recreation and not including flood storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8f21a478ebe4ba93df9f50b7ca4b5e31.png)'
  prefs: []
  type: TYPE_IMG
- en: Historical Lake Austin daily water level and percentage full.
  prefs: []
  type: TYPE_NORMAL
- en: And before you even ask - yes! I did calculate the semivariogram of water level
    and I was surprised that there was no significant nuggest effect.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dae27b907b4dada88f39fe0a7b6950d.png)'
  prefs: []
  type: TYPE_IMG
- en: Historical Lake Austin daily water level variogram.
  prefs: []
  type: TYPE_NORMAL
- en: To become a good data scientists!
  prefs: []
  type: TYPE_NORMAL
- en: Find and play with data!
  prefs: []
  type: TYPE_NORMAL
- en: All of these developments have provided the fertile ground for the seeds of
    data science to impact all sectors or our economy and society.
  prefs: []
  type: TYPE_NORMAL
- en: Data Science and Subsurface Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spoiler alert, I‚Äôm going to boast a bit in the section. I often hear students
    say,
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúI can‚Äôt believe this data science course is in the [Hildebrand Department of
    Petroleum and Geosystems Engineering](https://pge.utexas.edu/)!‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚ÄúWhy are you teaching machine learning in [Department of Earth and Planetary
    Sciences](https://eps.jsg.utexas.edu/)?‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once again, my response is,
  prefs: []
  type: TYPE_NORMAL
- en: We in the subsurface are the original data scientists!
  prefs: []
  type: TYPE_NORMAL
- en: We have been big data long before tech learned about big data!
  prefs: []
  type: TYPE_NORMAL
- en: This may sound a bit arrogant, but let me back this statement up with this timeline,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6046f01d90d6e62557dcf8b78e9bde99.png)'
  prefs: []
  type: TYPE_IMG
- en: Timeline of data science development from the perspective of subsurface engineering
    and geoscience.
  prefs: []
  type: TYPE_NORMAL
- en: Shortly after Kolmogorov developed the fundamental probability axioms, Danie
    Krige developed a set of statistical, spatial, i.e., data-driven tools for making
    estimates in space while accounting for spatial continuity and scale. These tools
    were formalized with theory developed by Professor Matheron during the 1960s in
    a new science called geostatistics. Over the 1970s - 1990s the geostatistical
    methods and applications expanded from mining to address oil and gas, environmental,
    agriculture, fisheries, etc. with many important open source developments.
  prefs: []
  type: TYPE_NORMAL
- en: Why was subsurface engineering and geoscience earlier in the development of
    data science?
  prefs: []
  type: TYPE_NORMAL
- en: because, necessity is the mother of invention! Complicated, heterogeneous, sparsely
    sampled, vast systems with complicated physics and high value decisions drove
    us to data-driven methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many other engineering fields that had little motivation to develop
    and apply data science due to,
  prefs: []
  type: TYPE_NORMAL
- en: homogeneous phenomenon that does not have significant spatial heterogeneity,
    continuity, nor uncertainty - with few samples the phenomenon is sufficiently
    understood, for example, sheet aluminum used for aircraft or cement used in structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: exhaustive sampling of the population relative to the modeling purpose and do
    not need an estimation model with uncertainty - the phenomenon‚Äôs properties are
    known and the performance can be simulated in a finite element model, for example,
    radiographic testing for cracks in a machine housing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: have well understood physics and can model the entire system with second and
    third paradigm phenomenon - the (coupled) physics is known and can be modeled,
    for example, the Boeing 777 was the first commerical airliner designed entirely
    using computed-aided design (CAD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compare this to the typical context of subsurface engineering and geoscience,
    where data challenges are both unique and substantial:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparsely sampled datasets** ‚Äì we often sample only a minuscule fraction‚Äîsometimes
    as little as one hundred-billionth‚Äîof the subsurface volume of interest. This
    leads to significant uncertainty and a heavy reliance on statistical and data-driven
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Massively multivariate datasets** ‚Äì integrating diverse data types (e.g.,
    seismic, well logs, core data) poses a major challenge, requiring robust multivariate
    statistical and machine learning techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex, heterogeneous earth systems** ‚Äì subsurface geology is highly variable
    and location-dependent. These open systems often behave unpredictably, demanding
    flexible and adaptive modeling approaches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-stakes, high-cost decisions** ‚Äì The need to support critical, high-value
    decisions drives innovation in methods, workflows, and the adoption of advanced
    analytics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, many of us in subsurface engineering and geoscience have a lot
    of experience learning, applying, and teaching data science to better understand
    and manage these complexities.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we are ready to define machine learning, a [Wikipedia artical on Machine
    Learning](https://en.wikipedia.org/wiki/Machine_learning) can be summarized and
    broken down as follows, machine learning has these aspects,
  prefs: []
  type: TYPE_NORMAL
- en: '**toolbox** - a collections of algorithms and mathematical models used by computer
    systems'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**learning** - that improve their performance progressively on a specific task'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**training data** - by learning patterns from sample data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**general** - to make predictions or decisions without being explicitly programmed
    for the specific task'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let‚Äôs highlight some key ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: machine learning is a numerical toolbox, a broad set of algorithms designed
    to adapt to different problems (toolbox)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: these algorithms improve by learning from or fitting to training data (learning
    from training data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a single algorithm can often be trained and applied across many different domains
    (generalization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An especially important point, often overlooked is found near the end of the
    article:'
  prefs: []
  type: TYPE_NORMAL
- en: This underscores that machine learning is most valuable when traditional rule-based
    programming becomes too complex, impractical, or infeasible, often due to the
    scale or variability of the task.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if the underlying theory is well understood, be it engineering
    principles, geoscience fundamentals, physics, chemical reactions, or mechanical
    systems, then use that knowledge first.
  prefs: []
  type: TYPE_NORMAL
- en: don‚Äôt rely on data science as a replacement for foundational understanding of
    science and engineering (Paraphrased from personal communication with Professor
    Carlos Torres-Verdin, 2024)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning Prerequisite Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand machine learning and in general, data science we need to first
    differentiate between the population and the sample.
  prefs: []
  type: TYPE_NORMAL
- en: Population
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The population is all possible information of our spatial phenomenon of interest.
    Exhaustive, finite list of feature of interest over area of interest at the resolution
    of the data. For example,
  prefs: []
  type: TYPE_NORMAL
- en: exhaustive set of porosity at every location within a gas reservoir at the scale
    of a core sample, i.e., imagine the entire reservoir drilled, extracted and analyzed
    core by core!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for every tree in a forest the species, diameter at breast height (DBH), crown
    diameter, age, tree health status, wood volume and location, i.e., perfectly knowledge
    of our forest!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally the entire population is not accessible due to,
  prefs: []
  type: TYPE_NORMAL
- en: technology limits - sampling methods that provide improved coverage generally
    have reduced resolution and accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: practicallity - removing too much reservoir will impact geomechanical stability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cost - wells cost 10s - 100s of millions of dollars each
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The limited set of values, at specific locations that have been measured. For
    example,
  prefs: []
  type: TYPE_NORMAL
- en: limited set of porosity values measured from extracted core sample and callibrated
    from well-logs within a reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1,000 randomly selected trees over an management unit, with species, diameter
    at breast height (DBH), crown diameter, age, tree health status, wood volume and
    location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is so much that I could mention about sampling! For example,
  prefs: []
  type: TYPE_NORMAL
- en: methods for representative sample selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: impact of sample frequency or coverage on uncertainty and required levels of
    sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: methods to debias biased spatial samples (see [Declustering to Debias Data](https://geostatsguy.github.io/GeostatsPyDemos_Book/GeostatsPy_declustering.html)
    from my [Applied Geostatistics in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For brevity I will not cover these topics here, but let‚Äôs now dive into,
  prefs: []
  type: TYPE_NORMAL
- en: What are we measuring from our population in our samples?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each distinct type of measure is called a variable or feature.
  prefs: []
  type: TYPE_NORMAL
- en: Variable or Feature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In any scientific or engineering study, a property that is measured or observed
    is typically referred to as a variable.
  prefs: []
  type: TYPE_NORMAL
- en: however, in data science and machine learning, we use the term feature almost
    exclusively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While the terminology differs, the concept is the same: both refer to quantitative
    or categorical information about an object or system.'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate what constitutes a feature, consider the following examples drawn
    from real-world geoscience, oil adn gas, and mining contexts,
  prefs: []
  type: TYPE_NORMAL
- en: porosity measured from 1.5 inch diameter, 2 inch long core plugs extracted from
    the Miocene-aged Tahiti field in the Gulf of Mexico
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability modeled from porosity (neutron density well log) and rock facies
    (interpreted fraction of shale logs) at 0.5 foot resolution along the well bore
    in the Late Devonian Leduc formation in the Western Canadian Sedimentary Basin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: blast hole cuttings nickel grade aggregated over 8 inch diameter 10 meter blast
    holes at Voisey‚Äôs Bay Mine, Proterozoic gneissic complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Did you see what I did?
  prefs: []
  type: TYPE_NORMAL
- en: I specified what was measured, how it was measured, and over what scale was
    it measured.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is important because,
  prefs: []
  type: TYPE_NORMAL
- en: '**how the measurement is made?** - changes the feature‚Äôs veracity (level of
    certainty in the measure) and different methods actually may change the shift
    the feature‚Äôs values so we may need to reconcile multiple measurement methods
    of the same feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**what is the scale of the measurement?** - is very important due to volume-variance
    effect, with increasing support volume, sample scale, the variance reduces due
    to volumetric averaging resulting in regression to the mean. I have a [chapter
    on volume-variance](https://geostatsguy.github.io/GeostatsPyDemos_Book/GeostatsPy_volume_variance.html)
    in my Applied Geostatistics in Python e-book for those interested in more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, our subsurface measurements often requires significant analysis,
    interpretation, etc.
  prefs: []
  type: TYPE_NORMAL
- en: we don‚Äôt just hold a tool up to the rock and get the number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we have a ‚Äúthick layer‚Äù of engineering and geoscience interpretation to map
    from measurement to a useable feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider this carbonate thin section from Bureau of Economic Geology, The University
    of Texas at Austin from [geoscience course](http://www.beg.utexas.edu/lmod/_IOL-CM07/old-4.29.03/cm07-step05.htm)
    by F. Jerry Lucia.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/236b7707dd6a04bc2f29f0b99cda1271.png)'
  prefs: []
  type: TYPE_IMG
- en: Carbonate thin section image (from [this link](http://www.beg.utexas.edu/lmod/_IOL-CM07/old-4.29.03/cm07-step05.htm)
    by F. Jerry Lucia).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: the blue dye in core samples visually indicates void space‚Äîthe pores
    in the rock. But is the porosity feature simply the blue area divided by the total
    area of the sample?'
  prefs: []
  type: TYPE_NORMAL
- en: that would give us total porosity, which is the ratio of total void volume to
    bulk volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: however, not all pore space contributes to fluid flow. Some pores are isolated
    or poorly connected, especially in low-permeability rocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To estimate the porosity that actually matters for flow, we need to interpret
    connectivity. This gives us effective porosity, a more useful feature for modeling
    fluid transport, permeability, and reservoir performance.
  prefs: []
  type: TYPE_NORMAL
- en: so even porosity, often seen as a ‚Äúsimple‚Äù feature that‚Äôs observable in well
    logs and suitable for linear averaging, doesn‚Äôt escape an essential interpretation
    step. Estimating effective porosity involves assumptions, thresholds, or models
    about pore geometry and connectivity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a critical reminder, our features generally require interpretation,
    even when they appear directly measurable. Measurement scale, observation method,
    and intended use all influence how we define and derive a useful feature for our
    data science models.
  prefs: []
  type: TYPE_NORMAL
- en: Predictor and Response Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand the difference between predictor and response features let‚Äôs look
    at the most concise expression of a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0064bedeb876a7d613fc0ceddedd18cd.png)'
  prefs: []
  type: TYPE_IMG
- en: The fundamental predictive machine learning model that maps from predictor to
    response features.
  prefs: []
  type: TYPE_NORMAL
- en: the predictors (or independent) features (or variables) the model inputs, i.e.,
    the \(X_1,\ldots,X_m\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: response (or dependent) feature(s) (or variable(s)) are the model output, \(y\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and there is an error term, \(\epsilon\).
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning is all about estimating such models, \(\hat{ùëì}\), for two purposes,
    inference or prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can make reliable predictions, we must first perform inference‚Äîthat
    is, we must learn about the underlying system from a limited sample in order to
    model the broader population. Inference is the process of building an understanding
    of the system, denoted as \(\hat{f}\), based on observed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through inference, we seek to answer questions like:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the relationships between features?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which features are most important?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there complicated or nonlinear interactions between features?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These insights are prerequisites for making any predictions, because they form
    the foundation of the model that will eventually be used for forecasting, classification,
    or decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine I walk into a room, pull a coin from my pocket, and flip
    it 10 times, observing 3 heads and 7 tails. Then I ask, ‚ÄúIs this a fair coin?‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: This is a classic inferential problem.
  prefs: []
  type: TYPE_NORMAL
- en: you‚Äôre using a limited sample (the 10 flips) to make an inference about the
    population‚Äîin this case, the probability distribution governing the coin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'and here‚Äôs the key twist: in this example, the coin itself is the population!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You‚Äôre not just summarizing the sample, you‚Äôre trying to learn something general
    about the system that generated the sample.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we‚Äôve completed inference‚Äîlearning from a sample to build a model of the
    underlying population‚Äîwe‚Äôre ready to use that model for prediction. Prediction
    is the process of applying what we‚Äôve learned to estimate the outcomes of new
    or future observations.
  prefs: []
  type: TYPE_NORMAL
- en: in short, inference goes from sample to the population, while prediction goes
    from population model to the new sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the goal of prediction is to produce the most accurate estimates of unknown
    or future outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs return to our earlier coin example,
  prefs: []
  type: TYPE_NORMAL
- en: you observe me flip a coin 10 times and see 3 heads and 7 tails. Based on this,
    you infer that the coin is likely biased toward tails.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, before I flip it again, you predict that the next 10 tosses will likely
    result in more tails than heads. That‚Äôs prediction, using your inferred model
    of the coin (i.e., the population) to estimate future data.
  prefs: []
  type: TYPE_NORMAL
- en: Inference vs. Prediction in Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, how do you know whether you‚Äôre doing inference or prediction in the machine
    learning context?
  prefs: []
  type: TYPE_NORMAL
- en: it depends on whether you‚Äôre modeling structure or estimating outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inferential Machine Learning**, also called unsupervised learning, involves
    only predictor features (no known outcomes or labels). The aim is to learn structure
    in the data and improve understanding of the system.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of machine learning inferential methods include,
  prefs: []
  type: TYPE_NORMAL
- en: Cluster analysis ‚Äì groups observations into distinct clusters (potential sub-populations),
    often as a preprocessing step for modeling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction ‚Äì reduces the number of features by finding combinations
    that best preserve information while minimizing noise, for example, princpal components
    analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE) or multidimensional
    scaling (MDS).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These techniques help reveal relationships, patterns, or latent variables in
    the data‚Äîcrucial for good modeling, but are not designed to make direct predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Some may push back on my definition of inferential machine learning and the
    inclusion of cluster analysis, because they include in statistical inference quantification
    of uncertainty and testing hypotheses that is not available in cluster analysis.
  prefs: []
  type: TYPE_NORMAL
- en: I defend my choice by the difficulty and importance of identifying groups in
    subsurface and spatial datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, depofacies for reservoir models commonly describe 80% of heterogeneity,
    i.e., the deposfacies are the most important inference for many reservoirs!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, I have a very broad view of uncertainty modeling and model checking that
    includes various types of resampling, for example, bootstrap and spatial boostrap
    and in the subsurface we can only rarely use analytical confidence intervals and
    hypothesis testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictive Machine Learning**, also called supervised learning, this involves
    both predictor features and response features (labels). The model is trained to
    predict the response for new samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of machine learning predictive methods include,
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression - estimating the response feature as a linear combination
    of the predictor features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes classifier - estimating the probability of each possible categorical
    result for a response feature from the product sum of the prior probability and
    each likelihood conditional probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is optimized to minimize prediction error, often evaluated through
    cross-validation or test data. Let‚Äôs talk about how to make predictive machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Population
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The population is all possible information of our spatial phenomenon of interest.
    Exhaustive, finite list of feature of interest over area of interest at the resolution
    of the data. For example,
  prefs: []
  type: TYPE_NORMAL
- en: exhaustive set of porosity at every location within a gas reservoir at the scale
    of a core sample, i.e., imagine the entire reservoir drilled, extracted and analyzed
    core by core!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for every tree in a forest the species, diameter at breast height (DBH), crown
    diameter, age, tree health status, wood volume and location, i.e., perfectly knowledge
    of our forest!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally the entire population is not accessible due to,
  prefs: []
  type: TYPE_NORMAL
- en: technology limits - sampling methods that provide improved coverage generally
    have reduced resolution and accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: practicallity - removing too much reservoir will impact geomechanical stability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cost - wells cost 10s - 100s of millions of dollars each
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The limited set of values, at specific locations that have been measured. For
    example,
  prefs: []
  type: TYPE_NORMAL
- en: limited set of porosity values measured from extracted core sample and callibrated
    from well-logs within a reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1,000 randomly selected trees over an management unit, with species, diameter
    at breast height (DBH), crown diameter, age, tree health status, wood volume and
    location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is so much that I could mention about sampling! For example,
  prefs: []
  type: TYPE_NORMAL
- en: methods for representative sample selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: impact of sample frequency or coverage on uncertainty and required levels of
    sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: methods to debias biased spatial samples (see [Declustering to Debias Data](https://geostatsguy.github.io/GeostatsPyDemos_Book/GeostatsPy_declustering.html)
    from my [Applied Geostatistics in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For brevity I will not cover these topics here, but let‚Äôs now dive into,
  prefs: []
  type: TYPE_NORMAL
- en: What are we measuring from our population in our samples?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each distinct type of measure is called a variable or feature.
  prefs: []
  type: TYPE_NORMAL
- en: Variable or Feature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In any scientific or engineering study, a property that is measured or observed
    is typically referred to as a variable.
  prefs: []
  type: TYPE_NORMAL
- en: however, in data science and machine learning, we use the term feature almost
    exclusively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While the terminology differs, the concept is the same: both refer to quantitative
    or categorical information about an object or system.'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate what constitutes a feature, consider the following examples drawn
    from real-world geoscience, oil adn gas, and mining contexts,
  prefs: []
  type: TYPE_NORMAL
- en: porosity measured from 1.5 inch diameter, 2 inch long core plugs extracted from
    the Miocene-aged Tahiti field in the Gulf of Mexico
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability modeled from porosity (neutron density well log) and rock facies
    (interpreted fraction of shale logs) at 0.5 foot resolution along the well bore
    in the Late Devonian Leduc formation in the Western Canadian Sedimentary Basin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: blast hole cuttings nickel grade aggregated over 8 inch diameter 10 meter blast
    holes at Voisey‚Äôs Bay Mine, Proterozoic gneissic complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Did you see what I did?
  prefs: []
  type: TYPE_NORMAL
- en: I specified what was measured, how it was measured, and over what scale was
    it measured.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is important because,
  prefs: []
  type: TYPE_NORMAL
- en: '**how the measurement is made?** - changes the feature‚Äôs veracity (level of
    certainty in the measure) and different methods actually may change the shift
    the feature‚Äôs values so we may need to reconcile multiple measurement methods
    of the same feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**what is the scale of the measurement?** - is very important due to volume-variance
    effect, with increasing support volume, sample scale, the variance reduces due
    to volumetric averaging resulting in regression to the mean. I have a [chapter
    on volume-variance](https://geostatsguy.github.io/GeostatsPyDemos_Book/GeostatsPy_volume_variance.html)
    in my Applied Geostatistics in Python e-book for those interested in more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, our subsurface measurements often requires significant analysis,
    interpretation, etc.
  prefs: []
  type: TYPE_NORMAL
- en: we don‚Äôt just hold a tool up to the rock and get the number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we have a ‚Äúthick layer‚Äù of engineering and geoscience interpretation to map
    from measurement to a useable feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider this carbonate thin section from Bureau of Economic Geology, The University
    of Texas at Austin from [geoscience course](http://www.beg.utexas.edu/lmod/_IOL-CM07/old-4.29.03/cm07-step05.htm)
    by F. Jerry Lucia.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/236b7707dd6a04bc2f29f0b99cda1271.png)'
  prefs: []
  type: TYPE_IMG
- en: Carbonate thin section image (from [this link](http://www.beg.utexas.edu/lmod/_IOL-CM07/old-4.29.03/cm07-step05.htm)
    by F. Jerry Lucia).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: the blue dye in core samples visually indicates void space‚Äîthe pores
    in the rock. But is the porosity feature simply the blue area divided by the total
    area of the sample?'
  prefs: []
  type: TYPE_NORMAL
- en: that would give us total porosity, which is the ratio of total void volume to
    bulk volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: however, not all pore space contributes to fluid flow. Some pores are isolated
    or poorly connected, especially in low-permeability rocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To estimate the porosity that actually matters for flow, we need to interpret
    connectivity. This gives us effective porosity, a more useful feature for modeling
    fluid transport, permeability, and reservoir performance.
  prefs: []
  type: TYPE_NORMAL
- en: so even porosity, often seen as a ‚Äúsimple‚Äù feature that‚Äôs observable in well
    logs and suitable for linear averaging, doesn‚Äôt escape an essential interpretation
    step. Estimating effective porosity involves assumptions, thresholds, or models
    about pore geometry and connectivity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a critical reminder, our features generally require interpretation,
    even when they appear directly measurable. Measurement scale, observation method,
    and intended use all influence how we define and derive a useful feature for our
    data science models.
  prefs: []
  type: TYPE_NORMAL
- en: Predictor and Response Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand the difference between predictor and response features let‚Äôs look
    at the most concise expression of a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0064bedeb876a7d613fc0ceddedd18cd.png)'
  prefs: []
  type: TYPE_IMG
- en: The fundamental predictive machine learning model that maps from predictor to
    response features.
  prefs: []
  type: TYPE_NORMAL
- en: the predictors (or independent) features (or variables) the model inputs, i.e.,
    the \(X_1,\ldots,X_m\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: response (or dependent) feature(s) (or variable(s)) are the model output, \(y\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and there is an error term, \(\epsilon\).
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning is all about estimating such models, \(\hat{ùëì}\), for two purposes,
    inference or prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can make reliable predictions, we must first perform inference‚Äîthat
    is, we must learn about the underlying system from a limited sample in order to
    model the broader population. Inference is the process of building an understanding
    of the system, denoted as \(\hat{f}\), based on observed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through inference, we seek to answer questions like:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the relationships between features?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which features are most important?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there complicated or nonlinear interactions between features?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These insights are prerequisites for making any predictions, because they form
    the foundation of the model that will eventually be used for forecasting, classification,
    or decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine I walk into a room, pull a coin from my pocket, and flip
    it 10 times, observing 3 heads and 7 tails. Then I ask, ‚ÄúIs this a fair coin?‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: This is a classic inferential problem.
  prefs: []
  type: TYPE_NORMAL
- en: you‚Äôre using a limited sample (the 10 flips) to make an inference about the
    population‚Äîin this case, the probability distribution governing the coin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'and here‚Äôs the key twist: in this example, the coin itself is the population!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You‚Äôre not just summarizing the sample, you‚Äôre trying to learn something general
    about the system that generated the sample.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we‚Äôve completed inference‚Äîlearning from a sample to build a model of the
    underlying population‚Äîwe‚Äôre ready to use that model for prediction. Prediction
    is the process of applying what we‚Äôve learned to estimate the outcomes of new
    or future observations.
  prefs: []
  type: TYPE_NORMAL
- en: in short, inference goes from sample to the population, while prediction goes
    from population model to the new sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the goal of prediction is to produce the most accurate estimates of unknown
    or future outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs return to our earlier coin example,
  prefs: []
  type: TYPE_NORMAL
- en: you observe me flip a coin 10 times and see 3 heads and 7 tails. Based on this,
    you infer that the coin is likely biased toward tails.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, before I flip it again, you predict that the next 10 tosses will likely
    result in more tails than heads. That‚Äôs prediction, using your inferred model
    of the coin (i.e., the population) to estimate future data.
  prefs: []
  type: TYPE_NORMAL
- en: Inference vs. Prediction in Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, how do you know whether you‚Äôre doing inference or prediction in the machine
    learning context?
  prefs: []
  type: TYPE_NORMAL
- en: it depends on whether you‚Äôre modeling structure or estimating outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inferential Machine Learning**, also called unsupervised learning, involves
    only predictor features (no known outcomes or labels). The aim is to learn structure
    in the data and improve understanding of the system.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of machine learning inferential methods include,
  prefs: []
  type: TYPE_NORMAL
- en: Cluster analysis ‚Äì groups observations into distinct clusters (potential sub-populations),
    often as a preprocessing step for modeling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction ‚Äì reduces the number of features by finding combinations
    that best preserve information while minimizing noise, for example, princpal components
    analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE) or multidimensional
    scaling (MDS).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These techniques help reveal relationships, patterns, or latent variables in
    the data‚Äîcrucial for good modeling, but are not designed to make direct predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Some may push back on my definition of inferential machine learning and the
    inclusion of cluster analysis, because they include in statistical inference quantification
    of uncertainty and testing hypotheses that is not available in cluster analysis.
  prefs: []
  type: TYPE_NORMAL
- en: I defend my choice by the difficulty and importance of identifying groups in
    subsurface and spatial datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, depofacies for reservoir models commonly describe 80% of heterogeneity,
    i.e., the deposfacies are the most important inference for many reservoirs!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, I have a very broad view of uncertainty modeling and model checking that
    includes various types of resampling, for example, bootstrap and spatial boostrap
    and in the subsurface we can only rarely use analytical confidence intervals and
    hypothesis testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictive Machine Learning**, also called supervised learning, this involves
    both predictor features and response features (labels). The model is trained to
    predict the response for new samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of machine learning predictive methods include,
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression - estimating the response feature as a linear combination
    of the predictor features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes classifier - estimating the probability of each possible categorical
    result for a response feature from the product sum of the prior probability and
    each likelihood conditional probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is optimized to minimize prediction error, often evaluated through
    cross-validation or test data. Let‚Äôs talk about how to make predictive machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Tuning Predictive Machine Learning Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In predictive machine learning, we follow a standard model training and testing
    workflow. This process ensures that our model generalizes well to new data, rather
    than just fitting the training data perfectly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31abfecc21c246ba7144dfb847ab4378.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard predictive machine learning modeling workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs walk through the key steps,
  prefs: []
  type: TYPE_NORMAL
- en: '**Train and Test Split** - divide the available data into mutually exclusive,
    exhaustive subsets: a training set and a testing set.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: typically, 15%‚Äì30% of the data is held out for testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the remaining 70%‚Äì85% is used for training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define a range of hyperparameter(s)** values to explore, ranging from,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: simple models with low flexibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to complex models with high flexibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\quad\) This step may involve tuning multiple hyperparameters, in which case
    efficient sampling methods (e.g., grid search, random search, or Bayesian optimization)
    are often used.
  prefs: []
  type: TYPE_NORMAL
- en: '**Train Model Parameters for Each Hyperparameter Setting** - for each set of
    hyperparameters, train a model on the training data. This yields:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a suite of trained models, each with different complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: each model has parameters optimized to minimize error on the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluate Each Model on the Withheld Testing Data** - using the testing data,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: evaluate how each trained model performs on unseen data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: summarize prediction error (for example, root mean square error (RMSE), mean
    absolute error (MAE), classification accuracy) for each model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Select the Hyperparameters That Minimize Test Error** - this is the hyperparameter
    tuning step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: choose the model hyperparaemter(s) that performs best on the test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: these are your tuned hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrain the Final Model on All Data Using Tuned Hyperparameters** - now that
    the best model complexity has been identified,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: retrain the model using both the training and test sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this maximizes the amount of data used for final model parameter estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the resulting model is the one you deploy in real-world applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common Questions About the Model Training and Tuning Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a professor, I often hear these questions when I introduce the above machine
    learning model training and tuning workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the main outcome of steps 1‚Äì5?** - the only reliable outcome is the
    tuned hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\quad\) we do not use the model trained in step 3 or 4 directly, because it
    was trained without access to all available data. Instead, we retrain the final
    model using all data with the selected hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why not train the model on all the data from the beginning?** - because if
    we do that, we have no independent way to evaluate the model‚Äôs generalization.
    A very complex model can easily overfit‚Äîfitting the training data perfectly, but
    performing poorly on new, unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\quad\) overfitting happens when model flexibility is too high‚Äîit captures
    noise instead of the underlying pattern. Without a withheld test set, we can‚Äôt
    detect this.
  prefs: []
  type: TYPE_NORMAL
- en: This workflow for training and tuning predictive machine learning models is,
  prefs: []
  type: TYPE_NORMAL
- en: an empirical, cross-validation-based process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a practical simulation of real-world model use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a method to identify the model complexity that best balances fit and generalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôve said model parameters and model hyperparameters a bunch of times, so I
    owe you their definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Model Parameters and Model Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Model parameters** are fit during training phase to minimize error at the
    training data, i.e., model parameters are trained with training data and control
    model fit to the data. For example,'
  prefs: []
  type: TYPE_NORMAL
- en: for the polynomial predictive machine learning model from the machine learning
    workflow example above, the model parameters are the polynomial coefficients,
    e.g., \(b_3\), \(b_2\), \(b_1\) and \(c\) (often called \(b_0\)) for the third
    order polynomial model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/85cce1589249c12a11ff52ea10e6c893.png)'
  prefs: []
  type: TYPE_IMG
- en: Model parameters are adjusted to fit of the model to the data, i.e., model parameters
    are trained to minimize error over the training data (x markers).
  prefs: []
  type: TYPE_NORMAL
- en: '**Model hyperparameters** are very different. They do not constrain the model
    fit to the data directly, instead they constrain the model complexity. The model
    hyperparameters are selected (call tuning) to minimize error at the withheld testing
    data. Going back to our polynomial predictive machine learning example, the choice
    of polynomial order is the model hyperparameter.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aef7a81bd36cd7f2b25e1574784b4932.png)'
  prefs: []
  type: TYPE_IMG
- en: Model hyperparameters are adjusted to change the model complexity / flexibility,
    i.e., model hyperparameters are tuned to minimize error over the withheld testing
    data (solid circles).
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters vs. model hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters control the model fit and are trained with training data. Model
    hyperparameters control the model complexity and are tuned with testing data.
  prefs: []
  type: TYPE_NORMAL
- en: Regression and Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we proceed, we need to define regression and classification.
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression** - a predictive machine learning model where the response feature(s)
    is continuous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification** - a predictive machine learning model where the response
    feature(s) is categorical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It turns out that for each of these we need to build different models and use
    different methods to score these models.
  prefs: []
  type: TYPE_NORMAL
- en: for the remainder of this discussion we will focus on regression, but in later
    chapters we introduce classification models as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, to better understand predictive machine learning model tuning, i.e., the
    empirical approach to tune model complexity to minimize testing error, we need
    to understand the sources of testing error.
  prefs: []
  type: TYPE_NORMAL
- en: the causes of the thing that we are attempting to minimize!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sources of Predictive Machine Learning Testing Error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mean square error (MSE) is a standard way to express error. Mean squared error
    is known as a norm because we at taking a vector of error (over all of the data)
    and summarizing with a single, non-negative value.
  prefs: []
  type: TYPE_NORMAL
- en: specifically MSER is the L2 norm because the errors are squared before they
    are summed,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the L2 norm has a lot of nice properties including a continuous error function
    that can be differentiated over all values of error, but it is sensitive to data
    outliers that may have a disproportionate impact on the sum of the squares. More
    about this later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ MSE = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual observation of the response feature and \(\hat{y}_i\)
    is the model estimate over data indexed, \(i = 1,\ldots,n\). The \(\hat{y}_i\)
    is estimated with our predictive machine learning model with a general form,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{y}_i = \hat{f}(x^1_i, \ldots , x^m_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\hat{f}\) is our predictive machine learning model and \(x^1_i, \ldots
    , x^m_i\) are the predictor feature values for the \(i^{th}\) data.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, MSE can be calculated over training data and this is often the loss
    function that we minimize for training the model parameters for regression models.
  prefs: []
  type: TYPE_NORMAL
- en: \[ MSE_{train} = \frac{1}{n_{train}} \sum_{i=1}^{n_{train}} \left( y_i - \hat{y}_i
    \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and MSE can be calculated over the withheld testing data for hyperparameter
    tuning of regression models.
  prefs: []
  type: TYPE_NORMAL
- en: If we take the \(MSE_{test}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ MSE_{test} = \frac{1}{n_{test}} \sum_{i=1}^{n_{test}} \left( y_i - \hat{f}(x^1_i,
    \ldots , x^m_i) \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: but we pose it as the expected test square error we get this expectation form,
  prefs: []
  type: TYPE_NORMAL
- en: \[ E \left[ \left( y_0 - \hat{f}(x^1_0, \ldots , x^m_0) \right)^2 \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: where we use the \(_0\) notation to indicate data samples not in the training
    dataset split, but in the withheld testing split. We can expand the quadratic
    and group the terms to get this convenient decomposition of expect test square
    error into three additive sources (derivation is available in [Hastie et al.,
    2009](https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/ref=sr_1_1?crid=32MDGH9EIGR9T&dib=eyJ2IjoiMSJ9.p0bVgWPuChRIt10algkzSRLwDHzW4bCihIh1RA6GGZDYFtIQN37sOnIqDS5rCJ4fF5dgqsleBiGbmgJUSXISlcmayLc6C0aOcXVX8iCtyZElt9qVbd-Dvq9P3x4KTBlzHCFHDtjz0ImJUAd3LhT6D6KhOtiHOAveaz8xiE4jU1ah6LlDo0xzGoQVDXzNE6ODFzysbfBvzJMRGXhLbQBD292ixuH_tTBTPZwOzNGzpIw.TpRqJ1llQqMdsDXdXhWr0WQIhzTn6rpjDKSAzA10E_I&dib_tag=se&keywords=hastie+machine+learning&qid=1728683359&s=books&sprefix=hastie+machine+learn%2Cstripbooks%2C187&sr=1-1),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ae706c217c5b116fd94a810b1740d35.png)'
  prefs: []
  type: TYPE_IMG
- en: Model error in testing with three additive components, model variance, model
    bias and irreducible error.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs explain these three additive sources of error over test data, i.e., our
    best representation of the error we expect in the real-world use of our model
    to predict for cases not used to train the model,
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Variance** - is error due to sensitivity to the dataset, for example
    a simple model like linear regression does not change much if we change the training
    data, but a more complicated model like a fifth order polynomial model will jump
    around a lot as we change the training data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(\quad\) More model complexity tends to increase model variance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Bias** - is error due to using an approximate model, i.e., the model
    is too simple to fit the natural phenomenon. A very simple model is inflexible
    and will generally have higher model bias while a more complicated model is flexible
    enough to fit the data and will have lower model bias.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(\quad\) More model complexity tends to descrease model bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Irreducible Error** - is due to missing and incomplete data. There may be
    important features that were not sampled, or ranges of feature values that were
    not sampled. This is the error due to data limitations that cannot be address
    by the machine learning model hyperparameter tuning for optimum complexity; therefore,
    irreducible error is constant over the range of model complexity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(\quad\) More model complexity should not change irreducible error.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can take these three additive sources of error and produce an instructive,
    schematic plot,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e8b432c2838bbeaa8f6807e2658cc53.png)'
  prefs: []
  type: TYPE_IMG
- en: Model error in testing vs. model complexity with three additive error components,
    model variance, model bias and irreducible error.
  prefs: []
  type: TYPE_NORMAL
- en: From this plot we can observe, the **model bias-variance trade off**,
  prefs: []
  type: TYPE_NORMAL
- en: testing error is high for low complexity models due to high model bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: testing error is high for high complexity models due to high model variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So hyperparameter tuning is an optimization of the model bias-variance trade
    off. Wwe select the model complexity via hyperparameters that results in a model
    that is not,
  prefs: []
  type: TYPE_NORMAL
- en: '**underfit** - too simple, too inflexible to fit the natural phenomenon'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**overfit** - too complicated, too flexible and is too sensitive to the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let‚Äôs get into more details about under and overfit machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Underfit and Overfit Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First visualize underfit and overfit first with a simple prediction problem
    with one predictor feature and one response feature, here‚Äôs a model that is too
    simple (left) and a model that is too complicated (right).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb749f54d2627552ab6b8dbb87763120.png)'
  prefs: []
  type: TYPE_IMG
- en: Example underfit model (left), a model that is too simple / inflexible and overfit
    model (right), a model that is too complicated / flexible.
  prefs: []
  type: TYPE_NORMAL
- en: Some observations,
  prefs: []
  type: TYPE_NORMAL
- en: the simple model is not sufficiently flexible to fit the data, this is likely
    an underfit model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the overfit model perfectly fits all of the training data but is too noisy away
    from the training data and is likely an overfit model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To better understand the difference, we can now plot the training error alongside
    the previously shown testing error curve, illustrating how both change with model
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc91c3c39111509132db7e06602eb769.png)'
  prefs: []
  type: TYPE_IMG
- en: Model error in training and testing vs. model complexity with illustration of
    under- and overfit model regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the plot, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Error** ‚Äì as model complexity increases, training error consistently
    decreases. With sufficient flexibility, the model can eventually fit the training
    data perfectly, driving training error to zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing Error** ‚Äì testing error is composed of bias, variance, and irreducible
    error. The interplay between bias and variance leads to a trade-off, resulting
    in an optimal model complexity that minimizes testing error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on these observations, we can characterize underfitting and overfitting
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Underfitting** - occurs when a model is too simple to capture the underlying
    patterns in the data. Such models fail to generalize well and tend to show high
    training and testing errors. As complexity increases, testing error decreases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting** - occurs when a model is too complex and captures not only
    the underlying data patterns but also the noise. While training error continues
    to decrease, testing error starts to increase due to poor generalization. These
    models can give a false impression of strong performance, misleading us into thinking
    we understand the system better than we do.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More About Training and Testing Splits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A critical part of the machine learning training and tuning workflow is the
    training and testing data split. Here is some more considerations about training
    and testing data splits,
  prefs: []
  type: TYPE_NORMAL
- en: Proportion Withheld for Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Meta studies have identified that between 15% and 30% withheld for testing is
    typically optimum. To understand the underlying trade-off imagine these end members,
  prefs: []
  type: TYPE_NORMAL
- en: withhold 2% of the data for testing, then we will have a lot of training data
    to train the model very well, but too few testing data to test our model over
    a range of prediction cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: withhold 98% of the data for test, then with only 2% of the data available for
    training we will do a very good job testing our very poor model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proportion to withhold for testing is a trade off between building a good
    model and testing this model well with a wide range of prediction cases.
  prefs: []
  type: TYPE_NORMAL
- en: Other Cross Validation Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The train and test workflow above known as a cross validation or hold out approach,
    but there are many other methods for model training and testing,
  prefs: []
  type: TYPE_NORMAL
- en: '**k-fold Cross Validation (K-fold CV)** - we divide the data into k mutually
    exclusive, exhaustive equal size sets (called folds) for repeat the model train
    and test k times with each fold having a turn being withheld testing data while
    the remainder is training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leave-One-Out Cross Validation (LOO CV)** - we loop over all data, each time
    we assign the one datum as the testing data and train on the remainder n-1 data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leave-p-Out Cross Validation (LpO-CV)** - we assign a integer p < n and then
    loop over the combinatorial of possible cases with p withheld testing data. Since
    we consider all possible cases, this is an exhaustive cross validation approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each of the above folds or in general, training and testing dataset combinations,
    we calculate the error norm
  prefs: []
  type: TYPE_NORMAL
- en: then we average the error norm over the folds to provide a single error for
    hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this method removes the sensitivity to the exact train and test split so it
    is often seen as more robust than regular hold out methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of proportion of the data to withhold for testing varies by the the
    cross validation method,
  prefs: []
  type: TYPE_NORMAL
- en: for k-fold cross validation the proportion of testing is implicit to the choice
    of k, for k = 3, 33% of data are withheld for each fold and for k=5, 20% of data
    are withheld for each fold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for leave-one-out cross validation the proportion of testing is \(\frac{1}{n}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for leave-p-out cross validation the proportion of testing is \(\frac{p}{n}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train, Validate and Test Split
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is an alternative approach with three exhaustive, mutually exclusive subsets
    of the data
  prefs: []
  type: TYPE_NORMAL
- en: '**training data** - subset is the same as train above and is used to train
    the model parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**validate data** - subset is like testing subset for the train and test split
    method above and is used to tune the model hyperparameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**testing data** - subset is applied to check the final model trained on all
    the data with the tuned hyperparameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The train, validate and test split philosophy is that this final model check
    is performed with data that was not involved with model construction, including
    training model parameters nor tuning model hyperparameters. There are two reasons
    that I push back on this method,
  prefs: []
  type: TYPE_NORMAL
- en: '**circular quest of perfect model validation** - what do we do if we don‚Äôt
    quite like the performance in this testing phase, do we have a fourth subset for
    another test? and a fifth subset? and a sixth subset? ad infinitum?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**we must train the deployed model with all data** - we can never deploy a
    model that is not trained with all available data; therefore, we will still have
    to train with the test subset to get our final model?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**reducing modeling training and tuning performance** - the third data split
    for the testing phase reduces the data available for model training and tuning
    reducing the perforamnce of these critical steps.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spatial Fair Train and Test Splits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dr. Julian Salazar suggested that for spatial prediction problems that random
    train and test split may not fair. He proposed a [fair train and test split method](https://www.sciencedirect.com/science/article/pii/S0920410521015023)
    for spatial prediction models that splits the data based on the difficulty of
    the planned use of the model. Prediction difficulty is related to kriging variance
    that accounts for spatial continuity and distance offset. For example,
  prefs: []
  type: TYPE_NORMAL
- en: if the model will be used to impute data with small offsets from available data
    then construct a train and test split with train data close to test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if the model will be used to predict a large distance offsets then perform splits
    the result is large offsets between train and test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this method the tuned model may vary based on the planned real-world use
    for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Proportion Withheld for Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Meta studies have identified that between 15% and 30% withheld for testing is
    typically optimum. To understand the underlying trade-off imagine these end members,
  prefs: []
  type: TYPE_NORMAL
- en: withhold 2% of the data for testing, then we will have a lot of training data
    to train the model very well, but too few testing data to test our model over
    a range of prediction cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: withhold 98% of the data for test, then with only 2% of the data available for
    training we will do a very good job testing our very poor model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proportion to withhold for testing is a trade off between building a good
    model and testing this model well with a wide range of prediction cases.
  prefs: []
  type: TYPE_NORMAL
- en: Other Cross Validation Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The train and test workflow above known as a cross validation or hold out approach,
    but there are many other methods for model training and testing,
  prefs: []
  type: TYPE_NORMAL
- en: '**k-fold Cross Validation (K-fold CV)** - we divide the data into k mutually
    exclusive, exhaustive equal size sets (called folds) for repeat the model train
    and test k times with each fold having a turn being withheld testing data while
    the remainder is training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leave-One-Out Cross Validation (LOO CV)** - we loop over all data, each time
    we assign the one datum as the testing data and train on the remainder n-1 data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leave-p-Out Cross Validation (LpO-CV)** - we assign a integer p < n and then
    loop over the combinatorial of possible cases with p withheld testing data. Since
    we consider all possible cases, this is an exhaustive cross validation approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each of the above folds or in general, training and testing dataset combinations,
    we calculate the error norm
  prefs: []
  type: TYPE_NORMAL
- en: then we average the error norm over the folds to provide a single error for
    hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this method removes the sensitivity to the exact train and test split so it
    is often seen as more robust than regular hold out methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of proportion of the data to withhold for testing varies by the the
    cross validation method,
  prefs: []
  type: TYPE_NORMAL
- en: for k-fold cross validation the proportion of testing is implicit to the choice
    of k, for k = 3, 33% of data are withheld for each fold and for k=5, 20% of data
    are withheld for each fold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for leave-one-out cross validation the proportion of testing is \(\frac{1}{n}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for leave-p-out cross validation the proportion of testing is \(\frac{p}{n}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train, Validate and Test Split
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is an alternative approach with three exhaustive, mutually exclusive subsets
    of the data
  prefs: []
  type: TYPE_NORMAL
- en: '**training data** - subset is the same as train above and is used to train
    the model parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**validate data** - subset is like testing subset for the train and test split
    method above and is used to tune the model hyperparameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**testing data** - subset is applied to check the final model trained on all
    the data with the tuned hyperparameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The train, validate and test split philosophy is that this final model check
    is performed with data that was not involved with model construction, including
    training model parameters nor tuning model hyperparameters. There are two reasons
    that I push back on this method,
  prefs: []
  type: TYPE_NORMAL
- en: '**circular quest of perfect model validation** - what do we do if we don‚Äôt
    quite like the performance in this testing phase, do we have a fourth subset for
    another test? and a fifth subset? and a sixth subset? ad infinitum?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**we must train the deployed model with all data** - we can never deploy a
    model that is not trained with all available data; therefore, we will still have
    to train with the test subset to get our final model?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**reducing modeling training and tuning performance** - the third data split
    for the testing phase reduces the data available for model training and tuning
    reducing the perforamnce of these critical steps.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spatial Fair Train and Test Splits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dr. Julian Salazar suggested that for spatial prediction problems that random
    train and test split may not fair. He proposed a [fair train and test split method](https://www.sciencedirect.com/science/article/pii/S0920410521015023)
    for spatial prediction models that splits the data based on the difficulty of
    the planned use of the model. Prediction difficulty is related to kriging variance
    that accounts for spatial continuity and distance offset. For example,
  prefs: []
  type: TYPE_NORMAL
- en: if the model will be used to impute data with small offsets from available data
    then construct a train and test split with train data close to test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if the model will be used to predict a large distance offsets then perform splits
    the result is large offsets between train and test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this method the tuned model may vary based on the planned real-world use
    for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical and Professional Practice Concerns with Data Science
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To demonstrate concerns with data science consider this example, [Rideiro et
    al. (2016)](https://arxiv.org/pdf/1602.04938.pdf) trained a logistic regression
    classifier with 20 wolf and dog images to detect the difference between wolves
    and dogs.
  prefs: []
  type: TYPE_NORMAL
- en: the input is a photo of a dog or wolf and the output is the probability of dog
    and the compliment, the probability of wolf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the model worked well until this example here (see the left side below),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/38b5db20dcd3b75de5c598c4ae89a459.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dog misclassified as a wolf (left) and pixels that resulted in this
    misclassification (right), image taken from Rideiro et al. (2016).
  prefs: []
  type: TYPE_NORMAL
- en: where this results in a high probability of wolf. Fortunately the authors were
    able to interrogate the model and determine the pixels that had the greatest influence
    of the model‚Äôs determination of wolf (see the right side above). What happened?
  prefs: []
  type: TYPE_NORMAL
- en: they trained a model to check for snow in the background. As a Canadian, I can
    assure you that many photos of wolves are in our snow filled northern regions
    of our country, while many dogs are photographed in grassy yards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problems with machine learning are,
  prefs: []
  type: TYPE_NORMAL
- en: interpretability may be low with complicated models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: application of machine learning may become routine and trusted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a dangerous combination, as the machine may become a trusted unquestioned
    authority. Rideiro and others state that they developed the problem to demonstrate
    this, but does it actually happen? Yes. I advised a team of students that attempted
    to automatically segment urban vs. rural environments from time lapse satellite
    photographs to build models of urban development. The model looked great, but
    I asked for an additional check with a plot of the by-pixel classification vs.
    pixel color. The results was a 100% correspondence, i.e., the model with all of
    it‚Äôs complicated convolution, activation, pooling was only looking for grey and
    tan pixels often associated with roads and buildings.
  prefs: []
  type: TYPE_NORMAL
- en: 'I‚Äôm not going to say, ‚ÄòSkynet‚Äô, oops I just did, but just consider these thoughts:'
  prefs: []
  type: TYPE_NORMAL
- en: New power and distribution of wealth by concentrating rapid inference with big
    data as more data is being shared
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trade-offs that matter to society while maximizing a machine learning objective
    function may be ignored, resulting in low interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Societal changes, disruptive technologies, post-labor society
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-scientific results such as [Clever Hans](https://en.wikipedia.org/wiki/Clever_Hans)
    effect with models that learn from tells in the data rather than really learning
    to perform the task resulting catastrophic failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I don‚Äôt want to be too negative and to take this too far. Full disclosure, I‚Äôm
    the old-fashioned professor that thinks we should put of phones in our pockets,
    walk around with our heads up so we can greet each other and observe our amazing
    environments and societies. One thing that is certain, data science is changing
    society in so many ways and as Neil Postman in [Technopoly](https://www.amazon.com/Technopoly-Surrender-Technology-Neil-Postman-ebook/dp/B004ZZJBW4/ref=sr_1_1?crid=E9SP0DFNP9JO&dib=eyJ2IjoiMSJ9.ELnF0aIjkOw11vdTEQ3Tpg.NSaPUOIOz6m6i7XxpEMUJvZobANzU5baE6DlEa40Uzs&dib_tag=se&keywords=technolopoly&qid=1728667409&sprefix=technolopo%2Caps%2C127&sr=8-1))
  prefs: []
  type: TYPE_NORMAL
- en: Neil Postman‚Äôs Quote for Technopoly
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúOnce a technology is admitted, it plays out its hand.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic description of machine learning concepts. Much more could be
    done and discussed, I have many more resources. Check out my [shared resource
    inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture links
    at the start of this chapter with resource links in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this was helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster,
    Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling
    and machine learning theory with practice to develop novel methods and workflows
    to add value. We are solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
