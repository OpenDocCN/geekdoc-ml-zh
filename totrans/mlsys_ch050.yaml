- en: Image Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分类
- en: '![](../media/file812.jpg)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file812.jpg)'
- en: '*DALL·E prompt - A cover image for an ‘Image Classification’ chapter in a Raspberry
    Pi tutorial, designed in the same vintage 1950s electronics lab style as previous
    covers. The scene should feature a Raspberry Pi connected to a camera module,
    with the camera capturing a photo of the small blue robot provided by the user.
    The robot should be placed on a workbench, surrounded by classic lab tools like
    soldering irons, resistors, and wires. The lab background should include vintage
    equipment like oscilloscopes and tube radios, maintaining the detailed and nostalgic
    feel of the era. No text or logos should be included.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E提示 - 为树莓派教程中的“图像分类”章节设计的封面图像，采用与之前封面相同的1950年代电子实验室风格。场景应包括一个连接到摄像头模块的树莓派，摄像头捕捉用户提供的蓝色小机器人的照片。机器人应放置在工作台上，周围是经典的实验室工具，如烙铁、电阻和电线。实验室背景应包括示波器和管式收音机等复古设备，保持时代的详细和怀旧感。不应包含任何文本或标志。*'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Image classification is a fundamental task in computer vision that involves
    categorizing an image into one of several predefined classes. It’s a cornerstone
    of artificial intelligence, enabling machines to interpret and understand visual
    information in a way that mimics human perception.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类是计算机视觉中的一个基本任务，涉及将图像分类到几个预定义类别之一。它是人工智能的基石，使机器能够以模仿人类感知的方式解释和理解视觉信息。
- en: Image classification refers to assigning a label or category to an entire image
    based on its visual content. This task is crucial in computer vision and has numerous
    applications across various industries. Image classification’s importance lies
    in its ability to automate visual understanding tasks that would otherwise require
    human intervention.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类是指根据图像的视觉内容将其分配一个标签或类别。这项任务在计算机视觉中至关重要，并在各个行业中具有广泛的应用。图像分类的重要性在于其能够自动化原本需要人类干预的视觉理解任务。
- en: Applications in Real-World Scenarios
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际场景中的应用
- en: 'Image classification has found its way into numerous real-world applications,
    revolutionizing various sectors:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类已广泛应用于众多实际应用中，彻底改变了各个行业：
- en: 'Healthcare: Assisting in medical image analysis, such as identifying abnormalities
    in X-rays or MRIs.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗保健：协助进行医学图像分析，例如识别X光片或MRI中的异常。
- en: 'Agriculture: Monitoring crop health and detecting plant diseases through aerial
    imagery.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 农业：通过航空图像监测作物健康和检测植物疾病。
- en: 'Automotive: Enabling advanced driver assistance systems and autonomous vehicles
    to recognize road signs, pedestrians, and other vehicles.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汽车行业：使高级驾驶辅助系统和自动驾驶汽车能够识别路标、行人和其他车辆。
- en: 'Retail: Powering visual search capabilities and automated inventory management
    systems.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零售：提供视觉搜索功能和自动化库存管理系统。
- en: 'Security and Surveillance: Enhancing threat detection and facial recognition
    systems.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全和监控：增强威胁检测和面部识别系统。
- en: 'Environmental Monitoring: Analyzing satellite imagery for deforestation, urban
    planning, and climate change studies.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境监测：分析卫星图像进行森林砍伐、城市规划和气候变化研究。
- en: Advantages of Running Classification on Edge Devices like Raspberry Pi
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在树莓派等边缘设备上运行分类的优势
- en: 'Implementing image classification on edge devices such as the Raspberry Pi
    offers several compelling advantages:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘设备（如树莓派）上实现图像分类具有几个显著优势：
- en: 'Low Latency: Processing images locally eliminates the need to send data to
    cloud servers, significantly reducing response times.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 低延迟：本地处理图像消除了将数据发送到云服务器的需求，显著减少了响应时间。
- en: 'Offline Functionality: Classification can be performed without an internet
    connection, making it suitable for remote or connectivity-challenged environments.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 离线功能：分类可以在没有互联网连接的情况下进行，使其适用于偏远或连接困难的地区。
- en: 'Privacy and Security: Sensitive image data remains on the local device, addressing
    data privacy concerns and compliance requirements.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐私和安全：敏感的图像数据保留在本地设备上，解决数据隐私问题和合规性要求。
- en: 'Cost-Effectiveness: Eliminates the need for expensive cloud computing resources,
    especially for continuous or high-volume classification tasks.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成本效益：消除了对昂贵云计算资源的需求，特别是对于持续或高量级的分类任务。
- en: 'Scalability: Enables distributed computing architectures where multiple devices
    can work independently or in a network.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可扩展性：支持分布式计算架构，其中多个设备可以独立工作或在网络中协同工作。
- en: 'Energy Efficiency: Optimized models on dedicated hardware can be more energy-efficient
    than cloud-based solutions, which is crucial for battery-powered or remote applications.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 能效：在专用硬件上优化的模型可能比基于云的解决方案更节能，这对于电池供电或远程应用至关重要。
- en: 'Customization: Deploying specialized or frequently updated models tailored
    to specific use cases is more manageable.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定制化：部署针对特定用例专门或经常更新的模型更为方便。
- en: We can create more responsive, secure, and efficient computer vision solutions
    by leveraging the power of edge devices like Raspberry Pi for image classification.
    This approach opens up new possibilities for integrating intelligent visual processing
    into various applications and environments.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过利用边缘设备如树莓派在图像分类中的强大功能，创建更响应、更安全和更高效的计算机视觉解决方案。这种方法为将智能视觉处理集成到各种应用和环境开辟了新的可能性。
- en: In the following sections, we’ll explore how to implement and optimize image
    classification on the Raspberry Pi, harnessing these advantages to create powerful
    and efficient computer vision systems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨如何在树莓派上实现和优化图像分类，利用这些优势创建强大且高效的计算机视觉系统。
- en: Setting Up the Environment
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置环境
- en: Updating the Raspberry Pi
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新树莓派
- en: 'First, ensure your Raspberry Pi is up to date:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，确保你的树莓派是最新的：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Installing Required Libraries
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装所需的库
- en: 'Install the necessary libraries for image processing and machine learning:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 安装图像处理和机器学习所需的库：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Setting up a Virtual Environment (Optional but Recommended)
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置虚拟环境（可选但推荐）
- en: 'Create a virtual environment to manage dependencies:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个虚拟环境来管理依赖项：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Installing TensorFlow Lite
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装TensorFlow Lite
- en: We are interested in performing **inference**, which refers to executing a TensorFlow
    Lite model on a device to make predictions based on input data. To perform an
    inference with a TensorFlow Lite model, we must run it through an **interpreter**.
    The TensorFlow Lite interpreter is designed to be lean and fast. The interpreter
    uses a static graph ordering and a custom (less-dynamic) memory allocator to ensure
    minimal load, initialization, and execution latency.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对执行**推理**感兴趣，这指的是在设备上执行TensorFlow Lite模型以根据输入数据进行预测。要使用TensorFlow Lite模型进行推理，我们必须通过**解释器**运行它。TensorFlow
    Lite解释器旨在轻量且快速。解释器使用静态图排序和自定义（较少动态）内存分配器，以确保最小化负载、初始化和执行延迟。
- en: We’ll use the [TensorFlow Lite runtime](https://pypi.org/project/tflite-runtime/)
    for Raspberry Pi, a simplified library for running machine learning models on
    mobile and embedded devices, without including all TensorFlow packages.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用针对树莓派的[TensorFlow Lite运行时](https://pypi.org/project/tflite-runtime/)，这是一个简化库，用于在移动和嵌入式设备上运行机器学习模型，而不包括所有TensorFlow包。
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The wheel installed: `tflite_runtime-2.14.0-cp311-cp311-manylinux_2_34_aarch64.whl`'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 安装的轮子：`tflite_runtime-2.14.0-cp311-cp311-manylinux_2_34_aarch64.whl`
- en: Installing Additional Python Libraries
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装额外的Python库
- en: 'Install required Python libraries for use with Image Classification:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 安装用于图像分类所需的Python库：
- en: If you have another version of Numpy installed, first uninstall it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已安装了另一个版本的Numpy，请先卸载它。
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Install `version 1.23.2`, which is compatible with the tflite_runtime.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`版本1.23.2`，它与tflite_runtime兼容。
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Creating a working directory:'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建工作目录：
- en: 'If you are working on the Raspi-Zero with the minimum OS (No Desktop), you
    may not have a user-pre-defined directory tree (you can check it with `ls`. So,
    let’s create one:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是带有最小OS（无桌面）的Raspi-Zero，你可能没有用户预定义的目录树（你可以使用`ls`来检查。因此，让我们创建一个：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: On the Raspi-5, the /Documents should be there.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在Raspi-5上，/Documents应该存在。
- en: '**Get a pre-trained Image Classification model**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取预训练的图像分类模型**：'
- en: 'An appropriate pre-trained model is crucial for successful image classification
    on resource-constrained devices like the Raspberry Pi. **MobileNet** is designed
    for mobile and embedded vision applications with a good balance between accuracy
    and speed. Versions: MobileNetV1, MobileNetV2, MobileNetV3\. Let’s download the
    V2:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于资源受限的设备如树莓派，一个合适的预训练模型对于成功的图像分类至关重要。**MobileNet**是为移动和嵌入式视觉应用设计的，在准确性和速度之间取得了良好的平衡。版本：MobileNetV1、MobileNetV2、MobileNetV3。让我们下载V2版本：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Get its [labels](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/models/labels.txt):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 获取其[标签](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/models/labels.txt)：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the end, you should have the models in its directory:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你应该在其目录中拥有模型：
- en: '![](../media/file813.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file813.png)'
- en: We will only need the `mobilenet_v2_1.0_224_quant.tflite` model and the `labels.txt`.
    You can delete the other files.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们只需要`mobilenet_v2_1.0_224_quant.tflite`模型和`labels.txt`文件。你可以删除其他文件。
- en: Setting up Jupyter Notebook (Optional)
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置Jupyter Notebook（可选）
- en: 'If you prefer using Jupyter Notebook for development:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更喜欢使用Jupyter Notebook进行开发：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To run Jupyter Notebook, run the command (change the IP address for yours):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行Jupyter Notebook，请运行以下命令（根据你的IP地址进行更改）：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'On the terminal, you can see the local URL address to open the notebook:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端上，你可以看到打开笔记本的本地URL地址：
- en: '![](../media/file814.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file814.png)'
- en: You can access it from another device by entering the Raspberry Pi’s IP address
    and the provided token in a web browser (you can copy the token from the terminal).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在网页浏览器中输入Raspberry Pi的IP地址和提供的令牌来从另一台设备访问它（你可以从终端复制令牌）。
- en: '![](../media/file815.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file815.png)'
- en: Define your working directory in the Raspi and create a new Python 3 notebook.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在Raspi中定义你的工作目录并创建一个新的Python 3笔记本。
- en: Verifying the Setup
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证设置
- en: 'Test your setup by running a simple Python script:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行一个简单的Python脚本来测试你的设置：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can create the Python script using nano on the terminal, saving it with
    `CTRL+0` + `ENTER` + `CTRL+X`
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用终端上的nano创建Python脚本，使用`CTRL+0` + `ENTER` + `CTRL+X`保存它。
- en: '![](../media/file816.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file816.png)'
- en: 'And run it with the command:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用以下命令运行它：
- en: '![](../media/file817.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file817.png)'
- en: 'Or you can run it directly on the [Notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/setup_test.ipynb):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 或者你可以直接在[Notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/setup_test.ipynb)上运行：
- en: '![](../media/file818.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file818.png)'
- en: Making inferences with Mobilenet V2
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Mobilenet V2进行推理
- en: In the last section, we set up the environment, including downloading a popular
    pre-trained model, Mobilenet V2, trained on ImageNet’s <semantics><mrow><mn>224</mn><mo>×</mo><mn>224</mn></mrow><annotation
    encoding="application/x-tex">224\times 224</annotation></semantics> images (1.2
    million) for 1,001 classes (1,000 object categories plus 1 background). The model
    was converted to a compact 3.5 MB TensorFlow Lite format, making it suitable for
    the limited storage and memory of a Raspberry Pi.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们设置了环境，包括下载一个在ImageNet的<semantics><mrow><mn>224</mn><mo>×</mo><mn>224</mn></mrow><annotation
    encoding="application/x-tex">224\times 224</annotation></semantics>图像（1.2百万）上训练的流行预训练模型Mobilenet
    V2，用于1,001个类别（1,000个对象类别加上1个背景）。该模型被转换为紧凑的3.5 MB TensorFlow Lite格式，使其适合Raspberry
    Pi有限的存储和内存。
- en: '![](../media/file819.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file819.png)'
- en: 'Let’s start a new [notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/10_Image_Classification.ipynb)
    to follow all the steps to classify one image:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始一个新的[notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/10_Image_Classification.ipynb)，以遵循所有步骤来对一张图像进行分类：
- en: 'Import the needed libraries:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Load the TFLite model and allocate tensors:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 加载TFLite模型并分配张量：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Get input and output tensors.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 获取输入和输出张量。
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Input details** will give us information about how the model should be fed
    with an image. The shape of (1, 224, 224, 3) informs us that an image with dimensions
    <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>224</mn><mo>×</mo><mn>224</mn><mo>×</mo><mn>3</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(224\times
    224\times 3)</annotation></semantics> should be input one by one (Batch Dimension:
    1).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入细节**将告诉我们模型应该如何用图像进行喂养。形状为（1，224，224，3）告诉我们应该逐个输入具有维度<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>224</mn><mo>×</mo><mn>224</mn><mo>×</mo><mn>3</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(224\times
    224\times 3)</annotation></semantics>的图像（批处理维度：1）。'
- en: '![](../media/file820.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file820.png)'
- en: The **output details** show that the inference will result in an array of 1,001
    integer values. Those values result from the image classification, where each
    value is the probability of that specific label being related to the image.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出细节**显示推理将产生一个包含1,001个整数值的数组。这些值来自图像分类，其中每个值是该特定标签与图像相关联的概率。'
- en: '![](../media/file821.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file821.png)'
- en: Let’s also inspect the dtype of input details of the model
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也检查一下模型输入细节的数据类型。
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This shows that the input image should be raw pixels (0 - 255).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明输入图像应该是原始像素（0 - 255）。
- en: 'Let’s get a test image. You can transfer it from your computer or download
    one for testing. Let’s first create a folder under our working directory:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取一个测试图像。你可以从你的电脑传输它或下载一个用于测试。让我们首先在我们的工作目录下创建一个文件夹：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s load and display the image:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载并显示图像：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../media/file822.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file822.png)'
- en: 'We can see the image size running the command:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行命令查看图像大小：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'That shows us that the image is an RGB image with a width of 1600 and a height
    of 1600 pixels. So, to use our model, we should reshape it to (224, 224, 3) and
    add a batch dimension of 1, as defined in input details: (1, 224, 224, 3). The
    inference result, as shown in output details, will be an array with a 1001 size,
    as shown below:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明图像是一个宽度为1600像素、高度为1600像素的RGB图像。因此，为了使用我们的模型，我们应该将其重塑为(224, 224, 3)，并添加一个批处理维度1，如输入细节中定义：(1,
    224, 224, 3)。推理结果，如输出细节所示，将是一个大小为1001的数组，如下所示：
- en: '![](../media/file823.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file823.png)'
- en: 'So, let’s reshape the image, add the batch dimension, and see the result:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们重塑图像，添加批处理维度，并查看结果：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The input_data shape is as expected: (1, 224, 224, 3)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据的形状符合预期：(1, 224, 224, 3)
- en: 'Let’s confirm the dtype of the input data:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认输入数据的dtype：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The input data dtype is ‘uint8’, which is compatible with the dtype expected
    for the model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据的dtype是‘uint8’，这与模型期望的dtype兼容。
- en: 'Using the input_data, let’s run the interpreter and get the predictions (output):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用input_data运行解释器并获取预测（输出）：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The prediction is an array with 1001 elements. Let’s get the Top-5 indices
    where their elements have high values:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 预测是一个包含1001个元素的数组。让我们获取具有高值的Top-5索引：
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The top_k_indices is an array with 5 elements: `array([283, 286, 282])`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: top_k_indices是一个包含5个元素的数组：`array([283, 286, 282])`
- en: 'So, 283, 286, 282, 288, and 479 are the image’s most probable classes. Having
    the index, we must find to what class it appoints (such as car, cat, or dog).
    The text file downloaded with the model has a label associated with each index
    from 0 to 1,000\. Let’s use a function to load the .txt file as a list:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，283、286、282、288和479是图像最可能的类别。有了索引，我们必须找到它指定的类别（例如汽车、猫或狗）。与模型一起下载的文本文件与从0到1,000的每个索引关联一个标签。让我们使用一个函数来加载.txt文件作为列表：
- en: '[PRE26]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'And get the list, printing the labels associated with the indexes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然后获取列表，打印与索引关联的标签：
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As a result, we have:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: At least the four top indices are related to felines. The **prediction** content
    is the probability associated with each one of the labels. As we saw on output
    details, those values are quantized and should be dequantized and apply softmax.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 至少前四个索引与猫科动物相关。**预测**内容是每个标签的概率。正如我们在输出细节中看到的，这些值是量化的，应该进行反量化并应用softmax。
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s print the top-5 probabilities:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印前5个概率：
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'For clarity, let’s create a function to relate the labels with the probabilities:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，让我们创建一个函数来关联标签与概率：
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Define a general Image Classification function
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义一个通用的图像分类函数
- en: 'Let’s create a general function to give an image as input, and we get the Top-5
    possible classes:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个通用函数，将图像作为输入，并获取前5个可能的类别：
- en: '[PRE34]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'And loading some images for testing, we have:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后加载一些图像进行测试，我们有：
- en: '![](../media/file824.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file824.jpg)'
- en: Testing with a model trained from scratch
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用从头开始训练的模型进行测试
- en: 'Let’s get a TFLite model trained from scratch. For that, you can follow the
    Notebook:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头开始获取一个TFLite模型。为此，你可以参考以下笔记本：
- en: '[CNN to classify Cifar-10 dataset](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/2_Applications_Deploy/Class_16/cifar_10/CNN_Cifar_10_TFLite.ipynb#scrollTo=iiVBUpuHXEtw)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[CNN用于分类Cifar-10数据集](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/2_Applications_Deploy/Class_16/cifar_10/CNN_Cifar_10_TFLite.ipynb#scrollTo=iiVBUpuHXEtw)'
- en: In the notebook, we trained a model using the CIFAR10 dataset, which contains
    60,000 images from 10 classes of CIFAR (*airplane, automobile, bird, cat, deer,
    dog, frog, horse, ship, and truck*). CIFAR has <semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation
    encoding="application/x-tex">32\times 32</annotation></semantics> color images
    (3 color channels) where the objects are not centered and can have the object
    with a background, such as airplanes that might have a cloudy sky behind them!
    In short, small but real images.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中，我们使用CIFAR10数据集训练了一个模型，该数据集包含来自10个类别的60,000张图像（CIFAR的*飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车*）。CIFAR有<semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation
    encoding="application/x-tex">32\times 32</annotation></semantics>彩色图像（3个颜色通道），其中对象不是居中的，并且可以有背景中的对象，例如可能有云的飞机！简而言之，小但真实的图像。
- en: The CNN trained model (*cifar10_model.keras*) had a size of 2.0MB. Using the
    *TFLite Converter*, the model *cifar10.tflite* became with 674MB (around 1/3 of
    the original size).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的CNN模型(*cifar10_model.keras*)的大小为2.0MB。使用*TFLite Converter*，模型*cifar10.tflite*变成了674MB（大约是原始大小的1/3）。
- en: '![](../media/file825.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file825.png)'
- en: On the notebook [Cifar 10 - Image Classification on a Raspi with TFLite](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/20_Cifar_10_Image_Classification.ipynb)
    (which can be run over the Raspi), we can follow the same steps we did with the
    `mobilenet_v2_1.0_224_quant.tflite`. Below are examples of images using the *General
    Function for Image Classification* on a Raspi-Zero, as shown in the last section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在[notebook Cifar 10 - 在Raspi上使用TFLite进行图像分类](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/20_Cifar_10_Image_Classification.ipynb)（可以在Raspi上运行），我们可以遵循与`mobilenet_v2_1.0_224_quant.tflite`相同的步骤。以下是在最后一节中展示的Raspi-Zero上使用*通用图像分类功能*的图像示例。
- en: '![](../media/file826.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file826.png)'
- en: Installing Picamera2
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装Picamera2
- en: '[Picamera2](https://github.com/raspberrypi/picamera2), a Python library for
    interacting with Raspberry Pi’s camera, is based on the *libcamera* camera stack,
    and the Raspberry Pi foundation maintains it. The Picamera2 library is supported
    on all Raspberry Pi models, from the Pi Zero to the RPi 5\. It is already installed
    system-wide on the Raspi, but we should make it accessible within the virtual
    environment.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[Picamera2](https://github.com/raspberrypi/picamera2)，一个用于与Raspberry Pi摄像头交互的Python库，基于*libcamera*摄像头堆栈，并由Raspberry
    Pi基金会维护。Picamera2库支持所有Raspberry Pi型号，从Pi Zero到RPi 5。它已经在Raspi上全局安装，但我们应确保它在虚拟环境中可访问。'
- en: 'First, activate the virtual environment if it’s not already activated:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，如果尚未激活，请激活虚拟环境：
- en: '[PRE35]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, let’s create a .pth file in your virtual environment to add the system
    site-packages path:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们在您的虚拟环境中创建一个.pth文件，以添加系统site-packages路径：
- en: '[PRE36]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Note: If your Python version differs, replace `python3.11` with the appropriate
    version.'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：如果您的Python版本不同，请将`python3.11`替换为适当的版本。
- en: 'After creating this file, try importing picamera2 in Python:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建此文件后，尝试在Python中导入picamera2：
- en: '[PRE37]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The above code will show the file location of the `picamera2` module itself,
    proving that the library can be accessed from the environment.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将显示`picamera2`模块本身的文件位置，证明可以从环境中访问库。
- en: '[PRE38]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can also list the available cameras in the system:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以列出系统中的可用摄像头：
- en: '[PRE39]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In my case, with a USB installed, I got:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，安装了USB后，我得到了：
- en: '![](../media/file827.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file827.png)'
- en: 'Now that we’ve confirmed picamera2 is working in the environment with an `index
    0`, let’s try a simple Python script to capture an image from your USB camera:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确认picamera2在具有`index 0`的环境下工作，让我们尝试一个简单的Python脚本来从您的USB摄像头捕获一张图片：
- en: '[PRE40]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Use the Nano text editor, the Jupyter Notebook, or any other editor. Save this
    as a Python script (e.g., `capture_image.py`) and run it. This should capture
    an image from your camera and save it as “usb_camera_image.jpg” in the same directory
    as your script.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Nano文本编辑器、Jupyter Notebook或任何其他编辑器。将其保存为Python脚本（例如，`capture_image.py`）并运行。这应该会从您的摄像头捕获一张图片，并将其保存为与脚本相同的目录中的“usb_camera_image.jpg”。
- en: '![](../media/file828.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file828.png)'
- en: If the Jupyter is open, you can see the captured image on your computer. Otherwise,
    transfer the file from the Raspi to your computer.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Jupyter已经打开，您可以在您的电脑上看到捕获到的图片。否则，将文件从Raspi传输到您的电脑。
- en: '![](../media/file829.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file829.png)'
- en: If you are working with a Raspi-5 with a whole desktop, you can open the file
    directly on the device.
  id: totrans-165
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您在使用带有完整桌面的Raspi-5，可以直接在设备上打开文件。
- en: Image Classification Project
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像分类项目
- en: Now, we will develop a complete Image Classification project using the Edge
    Impulse Studio. As we did with the Movilinet V2, the trained and converted TFLite
    model will be used for inference.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用Edge Impulse Studio开发一个完整的图像分类项目。正如我们在Movilinet V2中所做的那样，我们将使用训练和转换后的TFLite模型进行推理。
- en: The Goal
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标
- en: 'The first step in any ML project is to define its goal. In this case, it is
    to detect and classify two specific objects present in one image. For this project,
    we will use two small toys: a robot and a small Brazilian parrot (named Periquito).
    We will also collect images of a *background* where those two objects are absent.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 任何ML项目的第一步是定义其目标。在这种情况下，目标是检测和分类一张图片中存在的两个特定对象。对于这个项目，我们将使用两个小玩具：一个机器人和一只小型巴西鹦鹉（名为Periquito）。我们还将收集两个对象不存在的*背景*图片。
- en: '![](../media/file830.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file830.jpg)'
- en: Data Collection
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集
- en: Once we have defined our Machine Learning project goal, the next and most crucial
    step is collecting the dataset. We can use a phone for the image capture, but
    we will use the Raspi here. Let’s set up a simple web server on our Raspberry
    Pi to view the `QVGA (320 x 240)` captured images in a browser.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了我们的机器学习项目目标，下一步也是最重要的一步是收集数据集。我们可以使用手机进行图像捕获，但在这里我们将使用 Raspi。让我们在我们的
    Raspberry Pi 上设置一个简单的网络服务器，以便在浏览器中查看捕获的 `QVGA (320 x 240)` 图像。
- en: 'First, let’s install Flask, a lightweight web framework for Python:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们安装 Flask，一个用于 Python 的轻量级网络框架：
- en: '[PRE41]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let’s create a new Python script combining image capture with a web server.
    We’ll call it `get_img_data.py`:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个新的 Python 脚本，结合图像捕获和 Web 服务器。我们将它命名为 `get_img_data.py`：
- en: '[PRE42]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Run this script:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行此脚本：
- en: '[PRE43]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Access the web interface:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问网络界面：
- en: 'On the Raspberry Pi itself (if you have a GUI): Open a web browser and go to
    `http://localhost:5000`'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Raspberry Pi 本身（如果您有 GUI）：打开网页浏览器并访问 `http://localhost:5000`
- en: 'From another device on the same network: Open a web browser and go to `http://<raspberry_pi_ip>:5000`
    (Replace `<raspberry_pi_ip>` with your Raspberry Pi’s IP address). For example:
    `http://192.168.4.210:5000/`'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从同一网络上的另一台设备：打开网页浏览器并访问 `http://<raspberry_pi_ip>:5000`（将 `<raspberry_pi_ip>`
    替换为您的 Raspberry Pi 的 IP 地址）。例如：`http://192.168.4.210:5000/`
- en: This Python script creates a web-based interface for capturing and organizing
    image datasets using a Raspberry Pi and its camera. It’s handy for machine learning
    projects that require labeled image data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 此 Python 脚本使用 Raspberry Pi 和其摄像头创建了一个基于网络的界面，用于捕获和组织图像数据集。这对于需要标记图像数据的机器学习项目来说非常方便。
- en: 'Key Features:'
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键特性：
- en: '**Web Interface**: Accessible from any device on the same network as the Raspberry
    Pi.'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Web 界面**：可通过与 Raspberry Pi 相同网络上的任何设备访问。'
- en: '**Live Camera Preview**: This shows a real-time feed from the camera.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实时摄像头预览**：显示来自摄像头的实时流。'
- en: '**Labeling System**: Allows users to input labels for different categories
    of images.'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标签系统**：允许用户为不同类别的图像输入标签。'
- en: '**Organized Storage**: Automatically saves images in label-specific subdirectories.'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**组织存储**：自动将图像保存到特定标签的子目录中。'
- en: '**Per-Label Counters**: Keeps track of how many images are captured for each
    label.'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**按标签计数器**：跟踪每个标签捕获了多少图像。'
- en: '**Summary Statistics**: Provides a summary of captured images when stopping
    the capture process.'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**摘要统计**：在停止捕获过程时提供捕获图像的摘要。'
- en: 'Main Components:'
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要组件：
- en: '**Flask Web Application**: Handles routing and serves the web interface.'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Flask Web 应用程序**：处理路由并服务网络界面。'
- en: '**Picamera2 Integration**: Controls the Raspberry Pi camera.'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Picamera2 集成**：控制 Raspberry Pi 摄像头。'
- en: '**Threaded Frame Capture**: Ensures smooth live preview.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**线程化帧捕获**：确保实时预览流畅。'
- en: '**File Management**: Organizes captured images into labeled directories.'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文件管理**：将捕获的图像组织到标记的目录中。'
- en: 'Key Functions:'
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键功能：
- en: '`initialize_camera()`: Sets up the Picamera2 instance.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initialize_camera()`：设置 Picamera2 实例。'
- en: '`get_frame()`: Continuously captures frames for the live preview.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_frame()`：持续捕获用于实时预览的帧。'
- en: '`generate_frames()`: Yields frames for the live video feed.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_frames()`：为实时视频流提供帧。'
- en: '`shutdown_server()`: Sets the shutdown event, stops the camera, and shuts down
    the Flask server'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shutdown_server()`：设置关闭事件，停止摄像头并关闭 Flask 服务器'
- en: '`index()`: Handles the label input page.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index()`：处理标签输入页面。'
- en: '`capture_page()`: Displays the main capture interface.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`capture_page()`：显示主要捕获界面。'
- en: '`video_feed()`: Shows a live preview to position the camera'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`video_feed()`：显示实时预览以定位摄像头。'
- en: '`capture_image()`: Saves an image with the current label.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`capture_image()`：保存带有当前标签的图像。'
- en: '`stop()`: Stops the capture process and displays a summary.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stop()`：停止捕获过程并显示摘要。'
- en: 'Usage Flow:'
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用流程：
- en: Start the script on your Raspberry Pi.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的 Raspberry Pi 上启动脚本。
- en: Access the web interface from a browser.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从浏览器访问网络界面。
- en: Enter a label for the images you want to capture and press `Start Capture`.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为您想要捕获的图像输入标签并按 `开始捕获`。
- en: '![](../media/file831.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file831.png)'
- en: Use the live preview to position the camera.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用实时预览定位摄像头。
- en: Click `Capture Image` to save images under the current label.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 `捕获图像` 以保存当前标签下的图像。
- en: '![](../media/file832.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file832.png)'
- en: Change labels as needed for different categories, selecting `Change Label`.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据不同类别更改标签，选择 `更改标签`。
- en: Click `Stop Capture` when finished to see a summary.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后点击 `停止捕获` 以查看摘要。
- en: '![](../media/file833.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file833.png)'
- en: 'Technical Notes:'
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术说明：
- en: The script uses threading to handle concurrent frame capture and web serving.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该脚本使用线程处理并发帧捕获和网络服务。
- en: Images are saved with timestamps in their filenames for uniqueness.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像以其文件名中的时间戳保存，以确保唯一性。
- en: The web interface is responsive and can be accessed from mobile devices.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络界面是响应式的，可以从移动设备访问。
- en: 'Customization Possibilities:'
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定制化可能性：
- en: Adjust image resolution in the `initialize_camera()` function. Here we used
    QVGA <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>320</mn><mo>×</mo><mn>240</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(320\times
    240)</annotation></semantics>.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `initialize_camera()` 函数中调整图像分辨率。这里我们使用了 QVGA <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>320</mn><mo>×</mo><mn>240</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">(320\times 240)</annotation></semantics>。
- en: Modify the HTML templates for a different look and feel.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改 HTML 模板以获得不同的外观和感觉。
- en: Add additional image processing or analysis steps in the `capture_image()` function.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `capture_image()` 函数中添加额外的图像处理或分析步骤。
- en: 'Number of samples on Dataset:'
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集上的样本数量：
- en: Get around 60 images from each category (`periquito`, `robot` and `background`).
    Try to capture different angles, backgrounds, and light conditions. On the Raspi,
    we will end with a folder named `dataset`, witch contains 3 sub-folders *periquito,*
    *robot*, and *background*. one for each class of images.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 从每个类别（`periquito`、`robot` 和 `background`）获取大约 60 张图像。尽量捕捉不同的角度、背景和光照条件。在 Raspi
    上，我们将结束于一个名为 `dataset` 的文件夹，其中包含 3 个子文件夹 *periquito*、*robot* 和 *background*，每个文件夹对应于图像的一个类别。
- en: You can use `Filezilla` to transfer the created dataset to your main computer.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `Filezilla` 将创建的数据集传输到您的主计算机。
- en: Training the model with Edge Impulse Studio
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Edge Impulse Studio 训练模型
- en: 'We will use the Edge Impulse Studio to train our model. Go to the [Edge Impulse
    Page](https://edgeimpulse.com/), enter your account credentials, and create a
    new project:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Edge Impulse Studio 来训练我们的模型。访问 [Edge Impulse 页面](https://edgeimpulse.com/)，输入您的账户凭据，并创建一个新的项目：
- en: '![](../media/file834.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file834.png)'
- en: 'Here, you can clone a similar project: [Raspi - Img Class](https://studio.edgeimpulse.com/public/510251/live).'
  id: totrans-230
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里，您可以克隆一个类似的项目：[Raspi - Img Class](https://studio.edgeimpulse.com/public/510251/live)。
- en: Dataset
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集
- en: 'We will walk through four main steps using the EI Studio (or Studio). These
    steps are crucial in preparing our model for use on the Raspi: Dataset, Impulse,
    Tests, and Deploy (on the Edge Device, in this case, the Raspi).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 EI Studio（或 Studio）通过四个主要步骤进行操作。这些步骤对于准备我们的模型在 Raspi 上使用至关重要：数据集、Impulse、测试和部署（在这种情况下，是边缘设备，即
    Raspi）。
- en: Regarding the Dataset, it is essential to point out that our Original Dataset,
    captured with the Raspi, will be split into *Training*, *Validation*, and *Test*.
    The Test Set will be separated from the beginning and reserved for use only in
    the Test phase after training. The Validation Set will be used during training.
  id: totrans-233
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于数据集，重要的是指出，我们用 Raspi 捕获的原始数据集将被分成 *训练*、*验证* 和 *测试*。测试集将从一开始就分离出来，并在训练后的测试阶段使用。验证集将在训练期间使用。
- en: 'On Studio, follow the steps to upload the captured data:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Studio 上，按照步骤上传捕获的数据：
- en: Go to the `Data acquisition` tab, and in the `UPLOAD DATA` section, upload the
    files from your computer in the chosen categories.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到 `数据采集` 选项卡，在 `上传数据` 部分，上传您计算机上所选类别的文件。
- en: Leave to the Studio the splitting of the original dataset into *train and test*
    and choose the label about
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始数据集的拆分（*训练和测试*）以及选择标签的工作留给 Studio。
- en: 'Repeat the procedure for all three classes. At the end, you should see your
    “raw data” in the Studio:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有三个类别重复此过程。最后，您应该在 Studio 中看到您的“原始数据”：
- en: '![](../media/file835.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file835.png)'
- en: The Studio allows you to explore your data, showing a complete view of all the
    data in your project. You can clear, inspect, or change labels by clicking on
    individual data items. In our case, a straightforward project, the data seems
    OK.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Studio 允许您探索您的数据，展示您项目中所有数据的完整视图。您可以通过点击单个数据项来清除、检查或更改标签。在我们的简单项目中，数据看起来是好的。
- en: '![](../media/file836.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file836.png)'
- en: The Impulse Design
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Impulse 设计
- en: 'In this phase, we should define how to:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们应该定义如何：
- en: Pre-process our data, which consists of resizing the individual images and determining
    the `color depth` to use (be it RGB or Grayscale) and
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理我们的数据，这包括调整单个图像的大小以及确定要使用的 `颜色深度`（无论是 RGB 还是灰度）和
- en: Specify a Model. In this case, it will be the `Transfer Learning (Images)` to
    fine-tune a pre-trained MobileNet V2 image classification model on our data. This
    method performs well even with relatively small image datasets (around 180 images
    in our case).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定一个模型。在这种情况下，它将是`迁移学习（图像）`，用于在我们的数据上微调预训练的MobileNet V2图像分类模型。这种方法即使在相对较小的图像数据集（在我们的案例中约为180张图像）中也表现良好。
- en: Transfer Learning with MobileNet offers a streamlined approach to model training,
    which is especially beneficial for resource-constrained environments and projects
    with limited labeled data. MobileNet, known for its lightweight architecture,
    is a pre-trained model that has already learned valuable features from a large
    dataset (ImageNet).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MobileNet的迁移学习为模型训练提供了一种简化的方法，这对于资源受限的环境和具有有限标记数据的项目特别有益。MobileNet以其轻量级架构而闻名，是一个已经从大型数据集（ImageNet）中学习到有价值特征的预训练模型。
- en: '![](../media/file837.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file837.jpg)'
- en: By leveraging these learned features, we can train a new model for your specific
    task with fewer data and computational resources and achieve competitive accuracy.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用这些学习到的特征，我们可以用更少的数据和计算资源训练一个新的模型来完成您的特定任务，并实现有竞争力的准确率。
- en: '![](../media/file838.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file838.jpg)'
- en: This approach significantly reduces training time and computational cost, making
    it ideal for quick prototyping and deployment on embedded devices where efficiency
    is paramount.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法显著减少了训练时间和计算成本，使其非常适合快速原型设计和在嵌入式设备上的部署，在这些设备上效率至关重要。
- en: Go to the Impulse Design Tab and create the *impulse*, defining an image size
    of <semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation encoding="application/x-tex">160\times
    160</annotation></semantics> and squashing them (squared form, without cropping).
    Select Image and Transfer Learning blocks. Save the Impulse.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 转到“脉冲设计”标签页，创建一个*脉冲*，定义图像大小为<semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation
    encoding="application/x-tex">160\times 160</annotation></semantics>，并将它们压扁（平方形式，不裁剪）。选择图像和迁移学习模块。保存脉冲。
- en: '![](../media/file839.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file839.png)'
- en: Image Pre-Processing
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像预处理
- en: All the input QVGA/RGB565 images will be converted to 76,800 features <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>160</mn><mo>×</mo><mn>160</mn><mo>×</mo><mn>3</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(160\times
    160\times 3)</annotation></semantics>.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 所有输入的QVGA/RGB565图像都将转换为76,800个特征 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>160</mn><mo>×</mo><mn>160</mn><mo>×</mo><mn>3</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(160\times
    160\times 3)</annotation></semantics>.
- en: '![](../media/file840.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file840.png)'
- en: Press `Save parameters` and select `Generate features` in the next tab.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 按`保存参数`，然后在下一标签页中选择`生成特征`。
- en: Model Design
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型设计
- en: 'MobileNet is a family of efficient convolutional neural networks designed for
    mobile and embedded vision applications. The key features of MobileNet are:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet是一系列专为移动和嵌入式视觉应用设计的有效卷积神经网络。MobileNet的关键特性包括：
- en: 'Lightweight: Optimized for mobile devices and embedded systems with limited
    computational resources.'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 轻量级：针对具有有限计算资源的移动设备和嵌入式系统进行了优化。
- en: 'Speed: Fast inference times, suitable for real-time applications.'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 速度：快速推理时间，适用于实时应用。
- en: 'Accuracy: Maintains good accuracy despite its compact size.'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准确率：尽管体积紧凑，但仍然保持良好的准确率。
- en: '[MobileNetV2](https://arxiv.org/abs/1801.04381), introduced in 2018, improves
    the original MobileNet architecture. Key features include:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[MobileNetV2](https://arxiv.org/abs/1801.04381)，于2018年推出，改进了原始的MobileNet架构。关键特性包括：'
- en: 'Inverted Residuals: Inverted residual structures are used where shortcut connections
    are made between thin bottleneck layers.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向残差：在薄瓶颈层之间使用快捷连接的反向残差结构。
- en: 'Linear Bottlenecks: Removes non-linearities in the narrow layers to prevent
    the destruction of information.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性瓶颈：移除狭窄层中的非线性，以防止信息丢失。
- en: 'Depth-wise Separable Convolutions: Continues to use this efficient operation
    from MobileNetV1.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度可分离卷积：继续使用从MobileNetV1中继承的这种高效操作。
- en: In our project, we will do a `Transfer Learning` with the `MobileNetV2 160x160
    1.0`, which means that the images used for training (and future inference) should
    have an *input Size* of <semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation
    encoding="application/x-tex">160\times 160</annotation></semantics> pixels and
    a *Width Multiplier* of 1.0 (full width, not reduced). This configuration balances
    between model size, speed, and accuracy.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的项目中，我们将使用`MobileNetV2 160x160 1.0`进行`迁移学习`，这意味着用于训练（以及未来的推理）的图像应该具有一个*输入尺寸*为<semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation
    encoding="application/x-tex">160\times 160</annotation></semantics>像素和*宽度乘数*为1.0（全宽，未缩减）。这种配置在模型大小、速度和准确性之间取得了平衡。
- en: Model Training
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型训练
- en: Another valuable deep learning technique is **Data Augmentation**. Data augmentation
    improves the accuracy of machine learning models by creating additional artificial
    data. A data augmentation system makes small, random changes to the training data
    during the training process (such as flipping, cropping, or rotating the images).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有价值的深度学习技术是**数据增强**。数据增强通过创建额外的合成数据来提高机器学习模型的准确性。数据增强系统在训练过程中对训练数据进行小的、随机的更改（例如翻转、裁剪或旋转图像）。
- en: 'Looking under the hood, here you can see how Edge Impulse implements a data
    Augmentation policy on your data:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 查看内部结构，这里你可以看到Edge Impulse是如何在你的数据上实现数据增强策略的：
- en: '[PRE44]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Exposure to these variations during training can help prevent your model from
    taking shortcuts by “memorizing” superficial clues in your training data, meaning
    it may better reflect the deep underlying patterns in your dataset.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中接触这些变化可以帮助防止你的模型通过“记忆”训练数据中的表面线索来走捷径，这意味着它可能更好地反映数据集中的深层潜在模式。
- en: 'The final dense layer of our model will have 0 neurons with a 10% dropout for
    overfitting prevention. Here is the Training result:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的最终密集层将有0个神经元，并使用10%的dropout来防止过拟合。以下是训练结果：
- en: '![](../media/file841.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file841.png)'
- en: The result is excellent, with a reasonable 35 ms of latency (for a Raspi-4),
    which should result in around 30 fps (frames per second) during inference. A Raspi-Zero
    should be slower, and the Raspi-5, faster.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 结果非常出色，具有合理的35毫秒延迟（对于Raspi-4），在推理期间应达到大约30 fps（每秒帧数）。Raspi-Zero应该更慢，而Raspi-5应该更快。
- en: 'Trading off: Accuracy versus speed'
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 速度与准确性的权衡：
- en: 'If faster inference is needed, we should train the model using smaller alphas
    (0.35, 0.5, and 0.75) or even reduce the image input size, trading with accuracy.
    However, reducing the input image size and decreasing the alpha (width multiplier)
    can speed up inference for MobileNet V2, but they have different trade-offs. Let’s
    compare:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要更快的推理，我们应该使用较小的alpha值（0.35、0.5和0.75）或甚至减少图像输入尺寸，以准确性为代价。然而，减少输入图像尺寸和降低alpha（宽度乘数）可以加快MobileNet
    V2的推理速度，但它们有不同的权衡。让我们比较一下：
- en: 'Reducing Image Input Size:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少图像输入尺寸：
- en: 'Pros:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: Significantly reduces the computational cost across all layers.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显著降低了所有层的计算成本。
- en: Decreases memory usage.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少内存使用。
- en: It often provides a substantial speed boost.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通常提供显著的加速。
- en: 'Cons:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: It may reduce the model’s ability to detect small features or fine details.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可能会降低模型检测小特征或细微细节的能力。
- en: It can significantly impact accuracy, especially for tasks requiring fine-grained
    recognition.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以显著影响准确性，尤其是在需要精细识别的任务中。
- en: 'Reducing Alpha (Width Multiplier):'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少Alpha（宽度乘数）：
- en: 'Pros:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: Reduces the number of parameters and computations in the model.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少了模型中的参数和计算量。
- en: Maintains the original input resolution, potentially preserving more detail.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持原始输入分辨率，可能保留更多细节。
- en: It can provide a good balance between speed and accuracy.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以在速度和准确性之间提供良好的平衡。
- en: 'Cons:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: It may not speed up inference as dramatically as reducing input size.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可能不会像减少输入尺寸那样显著加快推理速度。
- en: It can reduce the model’s capacity to learn complex features.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以减少模型学习复杂特征的能力。
- en: 'Comparison:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 比较：
- en: 'Speed Impact:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 速度影响：
- en: Reducing input size often provides a more substantial speed boost because it
    reduces computations quadratically (halving both width and height reduces computations
    by about 75%).
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少输入尺寸通常可以提供更显著的加速，因为它将计算量平方减少（宽度和高各减半大约减少75%的计算量）。
- en: Reducing alpha provides a more linear reduction in computations.
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少alpha值可以更线性地减少计算量。
- en: 'Accuracy Impact:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准确性影响：
- en: Reducing input size can severely impact accuracy, especially when detecting
    small objects or fine details.
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少输入尺寸会严重影响准确性，尤其是在检测小对象或细微细节时。
- en: Reducing alpha tends to have a more gradual impact on accuracy.
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低alpha值往往对准确性的影响更为渐进。
- en: 'Model Architecture:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型架构：
- en: Changing input size doesn’t alter the model’s architecture.
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变输入尺寸不会改变模型的架构。
- en: Changing alpha modifies the model’s structure by reducing the number of channels
    in each layer.
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变alpha值会通过减少每层的通道数来修改模型的架构。
- en: 'Recommendation:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：
- en: If our application doesn’t require detecting tiny details and can tolerate some
    loss in accuracy, reducing the input size is often the most effective way to speed
    up inference.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们的应用程序不需要检测微小细节并且可以容忍一些准确性的损失，那么减少输入尺寸通常是加快推理的最有效方法。
- en: Reducing alpha might be preferable if maintaining the ability to detect fine
    details is crucial or if you need a more balanced trade-off between speed and
    accuracy.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果保持检测细微细节的能力至关重要或需要在速度和准确性之间取得更平衡的权衡，降低alpha可能更可取。
- en: 'For best results, you might want to experiment with both:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了获得最佳结果，你可能想尝试两者：
- en: Try MobileNet V2 with input sizes like <semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation
    encoding="application/x-tex">160\times 160</annotation></semantics> or <semantics><mrow><mn>92</mn><mo>×</mo><mn>92</mn></mrow><annotation
    encoding="application/x-tex">92\times 92</annotation></semantics>
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用输入尺寸如<semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation
    encoding="application/x-tex">160\times 160</annotation></semantics>或<semantics><mrow><mn>92</mn><mo>×</mo><mn>92</mn></mrow><annotation
    encoding="application/x-tex">92\times 92</annotation></semantics>
- en: Experiment with alpha values like 1.0, 0.75, 0.5 or 0.35.
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试alpha值如1.0、0.75、0.5或0.35。
- en: Always benchmark the different configurations on your specific hardware and
    with your particular dataset to find the optimal balance for your use case.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总是在你的特定硬件和特定数据集上对不同配置进行基准测试，以找到适合你用例的最佳平衡。
- en: Remember, the best choice depends on your specific requirements for accuracy,
    speed, and the nature of the images you’re working with. It’s often worth experimenting
    with combinations to find the optimal configuration for your particular use case.
  id: totrans-309
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 记住，最佳选择取决于你对准确性、速度以及你正在处理的图像性质的具体要求。通常值得尝试组合以找到特定用例的最佳配置。
- en: Model Testing
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型测试
- en: Now, you should take the data set aside at the start of the project and run
    the trained model using it as input. Again, the result is excellent (92.22%).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该在项目开始时将数据集放在一边，并使用它作为输入运行训练好的模型。结果再次非常出色（92.22%）。
- en: Deploying the model
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署模型
- en: As we did in the previous section, we can deploy the trained model as .tflite
    and use Raspi to run it using Python.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中所做的那样，我们可以将训练好的模型部署为.tflite格式，并使用Raspi通过Python运行它。
- en: 'On the `Dashboard` tab, go to Transfer learning model (int8 quantized) and
    click on the download icon:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在“仪表板”选项卡中，转到迁移学习模型（int8量化）并点击下载图标：
- en: '![](../media/file842.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file842.png)'
- en: Let’s also download the float32 version for comparison
  id: totrans-316
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们也下载float32版本以进行比较
- en: Transfer the model from your computer to the Raspi (./models), for example,
    using FileZilla. Also, capture some images for inference (./images).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型从你的计算机传输到Raspi（./models），例如，使用FileZilla。同时，捕获一些图像进行推理（./images）。
- en: 'Import the needed libraries:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE45]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Define the paths and labels:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 定义路径和标签：
- en: '[PRE46]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note that the models trained on the Edge Impulse Studio will output values with
    index 0, 1, 2, etc., where the actual labels will follow an alphabetic order.
  id: totrans-322
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，在Edge Impulse Studio上训练的模型将输出索引为0、1、2等值，而实际标签将遵循字母顺序。
- en: 'Load the model, allocate the tensors, and get the input and output tensor details:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 加载模型，分配张量，并获取输入和输出张量细节：
- en: '[PRE47]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'One important difference to note is that the `dtype` of the input details of
    the model is now `int8`, which means that the input values go from –128 to +127,
    while each pixel of our image goes from 0 to 255\. This means that we should pre-process
    the image to match it. We can check here:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个重要区别是，模型输入细节的`dtype`现在是`int8`，这意味着输入值从-128到+127，而我们的图像中的每个像素值从0到255。这意味着我们应该预处理图像以匹配它。我们可以在以下位置进行检查：
- en: '[PRE48]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'So, let’s open the image and show it:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们打开图像并显示它：
- en: '[PRE50]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![](../media/file843.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file843.png)'
- en: 'And perform the pre-processing:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 并进行预处理：
- en: '[PRE51]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Checking the input data, we can verify that the input tensor is compatible
    with what is expected by the model:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 检查输入数据，我们可以验证输入张量与模型期望的兼容性：
- en: '[PRE52]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, it is time to perform the inference. Let’s also calculate the latency
    of the model:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候进行推理了。让我们也计算模型的延迟：
- en: '[PRE54]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The model will take around 125ms to perform the inference in the Raspi-Zero,
    which is 3 to 4 times longer than a Raspi-5.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将在Raspi-Zero上大约需要125ms来完成推理，这比Raspi-5长3到4倍。
- en: Now, we can get the output labels and probabilities. It is also important to
    note that the model trained on the Edge Impulse Studio has a softmax in its output
    (different from the original Movilenet V2), and we should use the model’s raw
    output as the “probabilities.”
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以获取输出标签和概率。还应注意，在Edge Impulse Studio上训练的模型在其输出中有一个softmax（与原始Movilenet
    V2不同），我们应该使用模型的原始输出作为“概率”。
- en: '[PRE55]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](../media/file844.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file844.png)'
- en: 'Let’s modify the function created before so that we can handle different type
    of models:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改之前创建的函数，以便我们可以处理不同类型的模型：
- en: '[PRE56]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: And test it with different images and the int8 quantized model (**160x160 alpha
    =1.0**).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用不同的图像和int8量化模型（**160x160 alpha =1.0**）进行测试。
- en: '![](../media/file845.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file845.png)'
- en: 'Let’s download a smaller model, such as the one trained for the [Nicla Vision
    Lab](https://studio.edgeimpulse.com/public/353482/live) (int8 quantized model,
    96x96, alpha = 0.1), as a test. We can use the same function:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载一个较小的模型，例如为[Nicla Vision Lab](https://studio.edgeimpulse.com/public/353482/live)（int8量化模型，96x96，alpha
    = 0.1）训练的模型作为测试。我们可以使用相同的函数：
- en: '![](../media/file846.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file846.png)'
- en: The model lost some accuracy, but it is still OK once our model does not look
    for many details. Regarding latency, we are around **ten times faster** on the
    Raspi-Zero.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 模型丢失了一些精度，但一旦我们的模型不寻找太多细节，这仍然是可以接受的。关于延迟，我们在Raspi-Zero上大约快了十倍。
- en: Live Image Classification
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时图像分类
- en: Let’s develop an app to capture images with the USB camera in real time, showing
    its classification.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开发一个应用，实时捕捉USB摄像头的图像，并显示其分类。
- en: Using the nano on the terminal, save the code below, such as `img_class_live_infer.py`.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 使用终端上的nano保存以下代码，例如`img_class_live_infer.py`。
- en: '[PRE57]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'On the terminal, run:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端上运行：
- en: '[PRE58]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'And access the web interface:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 并访问Web界面：
- en: 'On the Raspberry Pi itself (if you have a GUI): Open a web browser and go to
    `http://localhost:5000`'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Raspberry Pi本身（如果您有GUI）：打开网页浏览器并访问`http://localhost:5000`
- en: 'From another device on the same network: Open a web browser and go to `http://<raspberry_pi_ip>:5000`
    (Replace `<raspberry_pi_ip>` with your Raspberry Pi’s IP address). For example:
    `http://192.168.4.210:5000/`'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从同一网络上的另一台设备：打开网页浏览器并访问`http://<raspberry_pi_ip>:5000`（将`<raspberry_pi_ip>`替换为您的Raspberry
    Pi的IP地址）。例如：`http://192.168.4.210:5000/`
- en: Here are some screenshots of the app running on an external desktop
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些应用程序在外部桌面运行时的截图
- en: '![](../media/file847.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file847.png)'
- en: 'Here, you can see the app running on the YouTube:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，您可以看到YouTube上运行的应用：
- en: '[https://www.youtube.com/watch?v=o1QsQrpCMw4](https://www.youtube.com/watch?v=o1QsQrpCMw4)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=o1QsQrpCMw4](https://www.youtube.com/watch?v=o1QsQrpCMw4)'
- en: The code creates a web application for real-time image classification using
    a Raspberry Pi, its camera module, and a TensorFlow Lite model. The application
    uses Flask to serve a web interface where is possible to view the camera feed
    and see live classification results.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 代码创建了一个使用Raspberry Pi、其摄像头模块和TensorFlow Lite模型的实时图像分类的Web应用。该应用使用Flask提供Web界面，可以查看摄像头流和实时分类结果。
- en: 'Key Components:'
  id: totrans-363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键组件：
- en: '**Flask Web Application**: Serves the user interface and handles requests.'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Flask Web应用**: 提供用户界面并处理请求。'
- en: '**PiCamera2**: Captures images from the Raspberry Pi camera module.'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**PiCamera2**: 从Raspberry Pi摄像头模块捕获图像。'
- en: '**TensorFlow Lite**: Runs the image classification model.'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**TensorFlow Lite**: 运行图像分类模型。'
- en: '**Threading**: Manages concurrent operations for smooth performance.'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多线程**: 管理并发操作以实现平滑性能。'
- en: 'Main Features:'
  id: totrans-368
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要功能：
- en: Live camera feed display
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时摄像头流显示
- en: Real-time image classification
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时图像分类
- en: Adjustable confidence threshold
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可调节的置信度阈值
- en: Start/Stop classification on demand
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按需启动/停止分类
- en: 'Code Structure:'
  id: totrans-373
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码结构：
- en: '**Imports and Setup**:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入和设置**:'
- en: Flask for web application
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flask用于Web应用
- en: PiCamera2 for camera control
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PiCamera2用于相机控制
- en: TensorFlow Lite for inference
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Lite用于推理
- en: Threading and Queue for concurrent operations
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多线程和队列用于并发操作
- en: '**Global Variables**:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**全局变量**:'
- en: Camera and frame management
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相机和帧管理
- en: Classification control
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类控制
- en: Model and label information
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型和标签信息
- en: '**Camera Functions**:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**相机功能**:'
- en: '`initialize_camera()`: Sets up the PiCamera2'
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initialize_camera()`: 设置PiCamera2'
- en: '`get_frame()`: Continuously captures frames'
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_frame()`: 持续捕获帧'
- en: '`generate_frames()`: Yields frames for the web feed'
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_frames()`: 为Web流生成帧'
- en: '**Model Functions**:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型功能**:'
- en: '`load_model()`: Loads the TFLite model'
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_model()`: 加载TFLite模型'
- en: '`classify_image()`: Performs inference on a single image'
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classify_image()`: 对单个图像进行推理'
- en: '**Classification Worker**:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分类工作器**:'
- en: Runs in a separate thread
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单独的线程中运行
- en: Continuously classifies frames when active
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 活跃时持续对帧进行分类
- en: Updates a queue with the latest results
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新队列以包含最新的结果
- en: '**Flask Routes**:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Flask路由**:'
- en: '`/`: Serves the main HTML page'
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/`: 提供主HTML页面'
- en: '`/video_feed`: Streams the camera feed'
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/video_feed`: 流式传输相机视频流'
- en: '`/start` and `/stop`: Controls classification'
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/start`和`/stop`: 控制分类'
- en: '`/update_confidence`: Adjusts the confidence threshold'
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/update_confidence`: 调整置信度阈值'
- en: '`/get_classification`: Returns the latest classification result'
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/get_classification`: 返回最新的分类结果'
- en: '**HTML Template**:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**HTML模板**:'
- en: Displays camera feed and classification results
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示相机视频流和分类结果
- en: Provides controls for starting/stopping and adjusting settings
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供启动/停止和调整设置的控件
- en: '**Main Execution**:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**主要执行**：'
- en: Initializes camera and starts necessary threads
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化相机并启动必要的线程
- en: Runs the Flask application
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Flask应用程序
- en: 'Key Concepts:'
  id: totrans-406
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键概念：
- en: '**Concurrent Operations**: Using threads to handle camera capture and classification
    separately from the web server.'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**并发操作**: 使用线程分别处理相机捕获和分类，而与Web服务器分离。'
- en: '**Real-time Updates**: Frequent updates to the classification results without
    page reloads.'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实时更新**: 无需页面刷新即可频繁更新分类结果。'
- en: '**Model Reuse**: Loading the TFLite model once and reusing it for efficiency.'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型重用**: 一次性加载TFLite模型并重复使用以提高效率。'
- en: '**Flexible Configuration**: Allowing users to adjust the confidence threshold
    on the fly.'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**灵活配置**: 允许用户动态调整置信度阈值。'
- en: 'Usage:'
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用方法：
- en: Ensure all dependencies are installed.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保所有依赖项都已安装。
- en: Run the script on a Raspberry Pi with a camera module.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在带有相机模块的Raspberry Pi上运行脚本。
- en: Access the web interface from a browser using the Raspberry Pi’s IP address.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Raspberry Pi的IP地址通过浏览器访问Web界面。
- en: Start classification and adjust settings as needed.
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要启动分类和调整设置。
- en: 'Summary:'
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述：
- en: Image classification has emerged as a powerful and versatile application of
    machine learning, with significant implications for various fields, from healthcare
    to environmental monitoring. This chapter has demonstrated how to implement a
    robust image classification system on edge devices like the Raspi-Zero and Raspi-5,
    showcasing the potential for real-time, on-device intelligence.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类已成为机器学习的一个强大且多用途的应用，对从医疗保健到环境监测的各个领域都有重大影响。本章展示了如何在边缘设备如Raspi-Zero和Raspi-5上实现稳健的图像分类系统，展示了实时、设备内智能的潜力。
- en: 'We’ve explored the entire pipeline of an image classification project, from
    data collection and model training using Edge Impulse Studio to deploying and
    running inferences on a Raspi. The process highlighted several key points:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了图像分类项目整个流程，从使用Edge Impulse Studio进行数据收集和模型训练到在Raspi上部署和运行推理。该过程强调了几个关键点：
- en: The importance of proper data collection and preprocessing for training effective
    models.
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确的数据收集和预处理对于训练有效模型的重要性。
- en: The power of transfer learning, allowing us to leverage pre-trained models like
    MobileNet V2 for efficient training with limited data.
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转移学习的力量，使我们能够利用像MobileNet V2这样的预训练模型，在数据有限的情况下进行高效训练。
- en: The trade-offs between model accuracy and inference speed, especially crucial
    for edge devices.
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型准确性与推理速度之间的权衡，这对于边缘设备尤其重要。
- en: The implementation of real-time classification using a web-based interface,
    demonstrating practical applications.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用基于Web界面的实时分类实现，展示了实际应用。
- en: The ability to run these models on edge devices like the Raspi opens up numerous
    possibilities for IoT applications, autonomous systems, and real-time monitoring
    solutions. It allows for reduced latency, improved privacy, and operation in environments
    with limited connectivity.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 能够在边缘设备如Raspi上运行这些模型，为物联网应用、自主系统和实时监控解决方案开辟了众多可能性。它允许降低延迟、提高隐私性，并在连接性有限的环境中运行。
- en: As we’ve seen, even with the computational constraints of edge devices, it’s
    possible to achieve impressive results in terms of both accuracy and speed. The
    flexibility to adjust model parameters, such as input size and alpha values, allows
    for fine-tuning to meet specific project requirements.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，即使在边缘设备的计算限制下，我们也能在准确性和速度方面取得令人印象深刻的成果。调整模型参数（如输入大小和alpha值）的灵活性允许进行微调以满足特定项目需求。
- en: Looking forward, the field of edge AI and image classification continues to
    evolve rapidly. Advances in model compression techniques, hardware acceleration,
    and more efficient neural network architectures promise to further expand the
    capabilities of edge devices in computer vision tasks.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，边缘AI和图像分类领域正迅速发展。模型压缩技术、硬件加速以及更高效的神经网络架构的进步有望进一步扩展边缘设备在计算机视觉任务中的能力。
- en: This project serves as a foundation for more complex computer vision applications
    and encourages further exploration into the exciting world of edge AI and IoT.
    Whether it’s for industrial automation, smart home applications, or environmental
    monitoring, the skills and concepts covered here provide a solid starting point
    for a wide range of innovative projects.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目作为更复杂计算机视觉应用的基础，并鼓励进一步探索令人兴奋的边缘AI和物联网世界。无论是工业自动化、智能家居应用还是环境监测，这里涵盖的技能和概念为各种创新项目提供了一个坚实的起点。
- en: Resources
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Dataset Example](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/IMG_CLASS/dataset)'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据集示例](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/IMG_CLASS/dataset)'
- en: '[Setup Test Notebook on a Raspi](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/setup_test.ipynb)'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在Raspi上设置测试笔记本](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/setup_test.ipynb)'
- en: '[Image Classification Notebook on a Raspi](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/10_Image_Classification.ipynb)'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Raspi上的图像分类笔记本](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/10_Image_Classification.ipynb)'
- en: '[CNN to classify Cifar-10 dataset at CoLab](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/2_Applications_Deploy/Class_16/cifar_10/CNN_Cifar_10_TFLite.ipynb#scrollTo=iiVBUpuHXEtw)'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在CoLab上使用CNN对Cifar-10数据集进行分类](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/2_Applications_Deploy/Class_16/cifar_10/CNN_Cifar_10_TFLite.ipynb#scrollTo=iiVBUpuHXEtw)'
- en: '[Cifar 10 - Image Classification on a Raspi](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/20_Cifar_10_Image_Classification.ipynb)'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cifar 10 - 在Raspi上进行图像分类](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/20_Cifar_10_Image_Classification.ipynb)'
- en: '[Python Scripts](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/IMG_CLASS/python_scripts)'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python脚本](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/IMG_CLASS/python_scripts)'
- en: '[Edge Impulse Project](https://studio.edgeimpulse.com/public/510251/live)'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Edge Impulse项目](https://studio.edgeimpulse.com/public/510251/live)'
