- en: Image Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../media/file812.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*DALL·E prompt - A cover image for an ‘Image Classification’ chapter in a Raspberry
    Pi tutorial, designed in the same vintage 1950s electronics lab style as previous
    covers. The scene should feature a Raspberry Pi connected to a camera module,
    with the camera capturing a photo of the small blue robot provided by the user.
    The robot should be placed on a workbench, surrounded by classic lab tools like
    soldering irons, resistors, and wires. The lab background should include vintage
    equipment like oscilloscopes and tube radios, maintaining the detailed and nostalgic
    feel of the era. No text or logos should be included.*'
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image classification is a fundamental task in computer vision that involves
    categorizing an image into one of several predefined classes. It’s a cornerstone
    of artificial intelligence, enabling machines to interpret and understand visual
    information in a way that mimics human perception.
  prefs: []
  type: TYPE_NORMAL
- en: Image classification refers to assigning a label or category to an entire image
    based on its visual content. This task is crucial in computer vision and has numerous
    applications across various industries. Image classification’s importance lies
    in its ability to automate visual understanding tasks that would otherwise require
    human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Applications in Real-World Scenarios
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Image classification has found its way into numerous real-world applications,
    revolutionizing various sectors:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Healthcare: Assisting in medical image analysis, such as identifying abnormalities
    in X-rays or MRIs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agriculture: Monitoring crop health and detecting plant diseases through aerial
    imagery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Automotive: Enabling advanced driver assistance systems and autonomous vehicles
    to recognize road signs, pedestrians, and other vehicles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Retail: Powering visual search capabilities and automated inventory management
    systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Security and Surveillance: Enhancing threat detection and facial recognition
    systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Environmental Monitoring: Analyzing satellite imagery for deforestation, urban
    planning, and climate change studies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of Running Classification on Edge Devices like Raspberry Pi
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Implementing image classification on edge devices such as the Raspberry Pi
    offers several compelling advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Low Latency: Processing images locally eliminates the need to send data to
    cloud servers, significantly reducing response times.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Offline Functionality: Classification can be performed without an internet
    connection, making it suitable for remote or connectivity-challenged environments.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Privacy and Security: Sensitive image data remains on the local device, addressing
    data privacy concerns and compliance requirements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cost-Effectiveness: Eliminates the need for expensive cloud computing resources,
    especially for continuous or high-volume classification tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scalability: Enables distributed computing architectures where multiple devices
    can work independently or in a network.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Energy Efficiency: Optimized models on dedicated hardware can be more energy-efficient
    than cloud-based solutions, which is crucial for battery-powered or remote applications.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Customization: Deploying specialized or frequently updated models tailored
    to specific use cases is more manageable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can create more responsive, secure, and efficient computer vision solutions
    by leveraging the power of edge devices like Raspberry Pi for image classification.
    This approach opens up new possibilities for integrating intelligent visual processing
    into various applications and environments.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we’ll explore how to implement and optimize image
    classification on the Raspberry Pi, harnessing these advantages to create powerful
    and efficient computer vision systems.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up the Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Updating the Raspberry Pi
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, ensure your Raspberry Pi is up to date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Installing Required Libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Install the necessary libraries for image processing and machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Setting up a Virtual Environment (Optional but Recommended)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Create a virtual environment to manage dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Installing TensorFlow Lite
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are interested in performing **inference**, which refers to executing a TensorFlow
    Lite model on a device to make predictions based on input data. To perform an
    inference with a TensorFlow Lite model, we must run it through an **interpreter**.
    The TensorFlow Lite interpreter is designed to be lean and fast. The interpreter
    uses a static graph ordering and a custom (less-dynamic) memory allocator to ensure
    minimal load, initialization, and execution latency.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the [TensorFlow Lite runtime](https://pypi.org/project/tflite-runtime/)
    for Raspberry Pi, a simplified library for running machine learning models on
    mobile and embedded devices, without including all TensorFlow packages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The wheel installed: `tflite_runtime-2.14.0-cp311-cp311-manylinux_2_34_aarch64.whl`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Installing Additional Python Libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Install required Python libraries for use with Image Classification:'
  prefs: []
  type: TYPE_NORMAL
- en: If you have another version of Numpy installed, first uninstall it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Install `version 1.23.2`, which is compatible with the tflite_runtime.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating a working directory:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you are working on the Raspi-Zero with the minimum OS (No Desktop), you
    may not have a user-pre-defined directory tree (you can check it with `ls`. So,
    let’s create one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: On the Raspi-5, the /Documents should be there.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Get a pre-trained Image Classification model**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'An appropriate pre-trained model is crucial for successful image classification
    on resource-constrained devices like the Raspberry Pi. **MobileNet** is designed
    for mobile and embedded vision applications with a good balance between accuracy
    and speed. Versions: MobileNetV1, MobileNetV2, MobileNetV3\. Let’s download the
    V2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Get its [labels](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/models/labels.txt):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the end, you should have the models in its directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file813.png)'
  prefs: []
  type: TYPE_IMG
- en: We will only need the `mobilenet_v2_1.0_224_quant.tflite` model and the `labels.txt`.
    You can delete the other files.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Setting up Jupyter Notebook (Optional)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you prefer using Jupyter Notebook for development:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To run Jupyter Notebook, run the command (change the IP address for yours):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'On the terminal, you can see the local URL address to open the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file814.png)'
  prefs: []
  type: TYPE_IMG
- en: You can access it from another device by entering the Raspberry Pi’s IP address
    and the provided token in a web browser (you can copy the token from the terminal).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file815.png)'
  prefs: []
  type: TYPE_IMG
- en: Define your working directory in the Raspi and create a new Python 3 notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Test your setup by running a simple Python script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You can create the Python script using nano on the terminal, saving it with
    `CTRL+0` + `ENTER` + `CTRL+X`
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file816.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And run it with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file817.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Or you can run it directly on the [Notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/setup_test.ipynb):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file818.png)'
  prefs: []
  type: TYPE_IMG
- en: Making inferences with Mobilenet V2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last section, we set up the environment, including downloading a popular
    pre-trained model, Mobilenet V2, trained on ImageNet’s <semantics><mrow><mn>224</mn><mo>×</mo><mn>224</mn></mrow><annotation
    encoding="application/x-tex">224\times 224</annotation></semantics> images (1.2
    million) for 1,001 classes (1,000 object categories plus 1 background). The model
    was converted to a compact 3.5 MB TensorFlow Lite format, making it suitable for
    the limited storage and memory of a Raspberry Pi.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file819.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s start a new [notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/10_Image_Classification.ipynb)
    to follow all the steps to classify one image:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the needed libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the TFLite model and allocate tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Get input and output tensors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Input details** will give us information about how the model should be fed
    with an image. The shape of (1, 224, 224, 3) informs us that an image with dimensions
    <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>224</mn><mo>×</mo><mn>224</mn><mo>×</mo><mn>3</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(224\times
    224\times 3)</annotation></semantics> should be input one by one (Batch Dimension:
    1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file820.png)'
  prefs: []
  type: TYPE_IMG
- en: The **output details** show that the inference will result in an array of 1,001
    integer values. Those values result from the image classification, where each
    value is the probability of that specific label being related to the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file821.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s also inspect the dtype of input details of the model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This shows that the input image should be raw pixels (0 - 255).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get a test image. You can transfer it from your computer or download
    one for testing. Let’s first create a folder under our working directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s load and display the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file822.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see the image size running the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'That shows us that the image is an RGB image with a width of 1600 and a height
    of 1600 pixels. So, to use our model, we should reshape it to (224, 224, 3) and
    add a batch dimension of 1, as defined in input details: (1, 224, 224, 3). The
    inference result, as shown in output details, will be an array with a 1001 size,
    as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file823.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, let’s reshape the image, add the batch dimension, and see the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The input_data shape is as expected: (1, 224, 224, 3)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s confirm the dtype of the input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The input data dtype is ‘uint8’, which is compatible with the dtype expected
    for the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the input_data, let’s run the interpreter and get the predictions (output):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The prediction is an array with 1001 elements. Let’s get the Top-5 indices
    where their elements have high values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The top_k_indices is an array with 5 elements: `array([283, 286, 282])`'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, 283, 286, 282, 288, and 479 are the image’s most probable classes. Having
    the index, we must find to what class it appoints (such as car, cat, or dog).
    The text file downloaded with the model has a label associated with each index
    from 0 to 1,000\. Let’s use a function to load the .txt file as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'And get the list, printing the labels associated with the indexes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: At least the four top indices are related to felines. The **prediction** content
    is the probability associated with each one of the labels. As we saw on output
    details, those values are quantized and should be dequantized and apply softmax.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s print the top-5 probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'For clarity, let’s create a function to relate the labels with the probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Define a general Image Classification function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s create a general function to give an image as input, and we get the Top-5
    possible classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'And loading some images for testing, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file824.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Testing with a model trained from scratch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s get a TFLite model trained from scratch. For that, you can follow the
    Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[CNN to classify Cifar-10 dataset](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/2_Applications_Deploy/Class_16/cifar_10/CNN_Cifar_10_TFLite.ipynb#scrollTo=iiVBUpuHXEtw)'
  prefs: []
  type: TYPE_NORMAL
- en: In the notebook, we trained a model using the CIFAR10 dataset, which contains
    60,000 images from 10 classes of CIFAR (*airplane, automobile, bird, cat, deer,
    dog, frog, horse, ship, and truck*). CIFAR has <semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation
    encoding="application/x-tex">32\times 32</annotation></semantics> color images
    (3 color channels) where the objects are not centered and can have the object
    with a background, such as airplanes that might have a cloudy sky behind them!
    In short, small but real images.
  prefs: []
  type: TYPE_NORMAL
- en: The CNN trained model (*cifar10_model.keras*) had a size of 2.0MB. Using the
    *TFLite Converter*, the model *cifar10.tflite* became with 674MB (around 1/3 of
    the original size).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file825.png)'
  prefs: []
  type: TYPE_IMG
- en: On the notebook [Cifar 10 - Image Classification on a Raspi with TFLite](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/20_Cifar_10_Image_Classification.ipynb)
    (which can be run over the Raspi), we can follow the same steps we did with the
    `mobilenet_v2_1.0_224_quant.tflite`. Below are examples of images using the *General
    Function for Image Classification* on a Raspi-Zero, as shown in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file826.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing Picamera2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Picamera2](https://github.com/raspberrypi/picamera2), a Python library for
    interacting with Raspberry Pi’s camera, is based on the *libcamera* camera stack,
    and the Raspberry Pi foundation maintains it. The Picamera2 library is supported
    on all Raspberry Pi models, from the Pi Zero to the RPi 5\. It is already installed
    system-wide on the Raspi, but we should make it accessible within the virtual
    environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, activate the virtual environment if it’s not already activated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create a .pth file in your virtual environment to add the system
    site-packages path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note: If your Python version differs, replace `python3.11` with the appropriate
    version.'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'After creating this file, try importing picamera2 in Python:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The above code will show the file location of the `picamera2` module itself,
    proving that the library can be accessed from the environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also list the available cameras in the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In my case, with a USB installed, I got:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file827.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we’ve confirmed picamera2 is working in the environment with an `index
    0`, let’s try a simple Python script to capture an image from your USB camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Use the Nano text editor, the Jupyter Notebook, or any other editor. Save this
    as a Python script (e.g., `capture_image.py`) and run it. This should capture
    an image from your camera and save it as “usb_camera_image.jpg” in the same directory
    as your script.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file828.png)'
  prefs: []
  type: TYPE_IMG
- en: If the Jupyter is open, you can see the captured image on your computer. Otherwise,
    transfer the file from the Raspi to your computer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file829.png)'
  prefs: []
  type: TYPE_IMG
- en: If you are working with a Raspi-5 with a whole desktop, you can open the file
    directly on the device.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Image Classification Project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we will develop a complete Image Classification project using the Edge
    Impulse Studio. As we did with the Movilinet V2, the trained and converted TFLite
    model will be used for inference.
  prefs: []
  type: TYPE_NORMAL
- en: The Goal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step in any ML project is to define its goal. In this case, it is
    to detect and classify two specific objects present in one image. For this project,
    we will use two small toys: a robot and a small Brazilian parrot (named Periquito).
    We will also collect images of a *background* where those two objects are absent.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file830.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Data Collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we have defined our Machine Learning project goal, the next and most crucial
    step is collecting the dataset. We can use a phone for the image capture, but
    we will use the Raspi here. Let’s set up a simple web server on our Raspberry
    Pi to view the `QVGA (320 x 240)` captured images in a browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s install Flask, a lightweight web framework for Python:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a new Python script combining image capture with a web server.
    We’ll call it `get_img_data.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Run this script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Access the web interface:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the Raspberry Pi itself (if you have a GUI): Open a web browser and go to
    `http://localhost:5000`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From another device on the same network: Open a web browser and go to `http://<raspberry_pi_ip>:5000`
    (Replace `<raspberry_pi_ip>` with your Raspberry Pi’s IP address). For example:
    `http://192.168.4.210:5000/`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This Python script creates a web-based interface for capturing and organizing
    image datasets using a Raspberry Pi and its camera. It’s handy for machine learning
    projects that require labeled image data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key Features:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Web Interface**: Accessible from any device on the same network as the Raspberry
    Pi.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Live Camera Preview**: This shows a real-time feed from the camera.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Labeling System**: Allows users to input labels for different categories
    of images.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Organized Storage**: Automatically saves images in label-specific subdirectories.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Per-Label Counters**: Keeps track of how many images are captured for each
    label.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Summary Statistics**: Provides a summary of captured images when stopping
    the capture process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Main Components:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Flask Web Application**: Handles routing and serves the web interface.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Picamera2 Integration**: Controls the Raspberry Pi camera.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Threaded Frame Capture**: Ensures smooth live preview.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**File Management**: Organizes captured images into labeled directories.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Key Functions:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`initialize_camera()`: Sets up the Picamera2 instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_frame()`: Continuously captures frames for the live preview.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_frames()`: Yields frames for the live video feed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shutdown_server()`: Sets the shutdown event, stops the camera, and shuts down
    the Flask server'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index()`: Handles the label input page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`capture_page()`: Displays the main capture interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`video_feed()`: Shows a live preview to position the camera'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`capture_image()`: Saves an image with the current label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stop()`: Stops the capture process and displays a summary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage Flow:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Start the script on your Raspberry Pi.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Access the web interface from a browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a label for the images you want to capture and press `Start Capture`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../media/file831.png)'
  prefs: []
  type: TYPE_IMG
- en: Use the live preview to position the camera.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `Capture Image` to save images under the current label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../media/file832.png)'
  prefs: []
  type: TYPE_IMG
- en: Change labels as needed for different categories, selecting `Change Label`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `Stop Capture` when finished to see a summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../media/file833.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Technical Notes:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The script uses threading to handle concurrent frame capture and web serving.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Images are saved with timestamps in their filenames for uniqueness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The web interface is responsive and can be accessed from mobile devices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Customization Possibilities:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Adjust image resolution in the `initialize_camera()` function. Here we used
    QVGA <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>320</mn><mo>×</mo><mn>240</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(320\times
    240)</annotation></semantics>.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify the HTML templates for a different look and feel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add additional image processing or analysis steps in the `capture_image()` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of samples on Dataset:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Get around 60 images from each category (`periquito`, `robot` and `background`).
    Try to capture different angles, backgrounds, and light conditions. On the Raspi,
    we will end with a folder named `dataset`, witch contains 3 sub-folders *periquito,*
    *robot*, and *background*. one for each class of images.
  prefs: []
  type: TYPE_NORMAL
- en: You can use `Filezilla` to transfer the created dataset to your main computer.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model with Edge Impulse Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the Edge Impulse Studio to train our model. Go to the [Edge Impulse
    Page](https://edgeimpulse.com/), enter your account credentials, and create a
    new project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file834.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, you can clone a similar project: [Raspi - Img Class](https://studio.edgeimpulse.com/public/510251/live).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will walk through four main steps using the EI Studio (or Studio). These
    steps are crucial in preparing our model for use on the Raspi: Dataset, Impulse,
    Tests, and Deploy (on the Edge Device, in this case, the Raspi).'
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the Dataset, it is essential to point out that our Original Dataset,
    captured with the Raspi, will be split into *Training*, *Validation*, and *Test*.
    The Test Set will be separated from the beginning and reserved for use only in
    the Test phase after training. The Validation Set will be used during training.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'On Studio, follow the steps to upload the captured data:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the `Data acquisition` tab, and in the `UPLOAD DATA` section, upload the
    files from your computer in the chosen categories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave to the Studio the splitting of the original dataset into *train and test*
    and choose the label about
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat the procedure for all three classes. At the end, you should see your
    “raw data” in the Studio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../media/file835.png)'
  prefs: []
  type: TYPE_IMG
- en: The Studio allows you to explore your data, showing a complete view of all the
    data in your project. You can clear, inspect, or change labels by clicking on
    individual data items. In our case, a straightforward project, the data seems
    OK.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file836.png)'
  prefs: []
  type: TYPE_IMG
- en: The Impulse Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this phase, we should define how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-process our data, which consists of resizing the individual images and determining
    the `color depth` to use (be it RGB or Grayscale) and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify a Model. In this case, it will be the `Transfer Learning (Images)` to
    fine-tune a pre-trained MobileNet V2 image classification model on our data. This
    method performs well even with relatively small image datasets (around 180 images
    in our case).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer Learning with MobileNet offers a streamlined approach to model training,
    which is especially beneficial for resource-constrained environments and projects
    with limited labeled data. MobileNet, known for its lightweight architecture,
    is a pre-trained model that has already learned valuable features from a large
    dataset (ImageNet).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file837.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By leveraging these learned features, we can train a new model for your specific
    task with fewer data and computational resources and achieve competitive accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file838.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This approach significantly reduces training time and computational cost, making
    it ideal for quick prototyping and deployment on embedded devices where efficiency
    is paramount.
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Impulse Design Tab and create the *impulse*, defining an image size
    of <semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation encoding="application/x-tex">160\times
    160</annotation></semantics> and squashing them (squared form, without cropping).
    Select Image and Transfer Learning blocks. Save the Impulse.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file839.png)'
  prefs: []
  type: TYPE_IMG
- en: Image Pre-Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the input QVGA/RGB565 images will be converted to 76,800 features <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>160</mn><mo>×</mo><mn>160</mn><mo>×</mo><mn>3</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(160\times
    160\times 3)</annotation></semantics>.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file840.png)'
  prefs: []
  type: TYPE_IMG
- en: Press `Save parameters` and select `Generate features` in the next tab.
  prefs: []
  type: TYPE_NORMAL
- en: Model Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MobileNet is a family of efficient convolutional neural networks designed for
    mobile and embedded vision applications. The key features of MobileNet are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lightweight: Optimized for mobile devices and embedded systems with limited
    computational resources.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Speed: Fast inference times, suitable for real-time applications.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Accuracy: Maintains good accuracy despite its compact size.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[MobileNetV2](https://arxiv.org/abs/1801.04381), introduced in 2018, improves
    the original MobileNet architecture. Key features include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inverted Residuals: Inverted residual structures are used where shortcut connections
    are made between thin bottleneck layers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Linear Bottlenecks: Removes non-linearities in the narrow layers to prevent
    the destruction of information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Depth-wise Separable Convolutions: Continues to use this efficient operation
    from MobileNetV1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our project, we will do a `Transfer Learning` with the `MobileNetV2 160x160
    1.0`, which means that the images used for training (and future inference) should
    have an *input Size* of <semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation
    encoding="application/x-tex">160\times 160</annotation></semantics> pixels and
    a *Width Multiplier* of 1.0 (full width, not reduced). This configuration balances
    between model size, speed, and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another valuable deep learning technique is **Data Augmentation**. Data augmentation
    improves the accuracy of machine learning models by creating additional artificial
    data. A data augmentation system makes small, random changes to the training data
    during the training process (such as flipping, cropping, or rotating the images).
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking under the hood, here you can see how Edge Impulse implements a data
    Augmentation policy on your data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Exposure to these variations during training can help prevent your model from
    taking shortcuts by “memorizing” superficial clues in your training data, meaning
    it may better reflect the deep underlying patterns in your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final dense layer of our model will have 0 neurons with a 10% dropout for
    overfitting prevention. Here is the Training result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file841.png)'
  prefs: []
  type: TYPE_IMG
- en: The result is excellent, with a reasonable 35 ms of latency (for a Raspi-4),
    which should result in around 30 fps (frames per second) during inference. A Raspi-Zero
    should be slower, and the Raspi-5, faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trading off: Accuracy versus speed'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If faster inference is needed, we should train the model using smaller alphas
    (0.35, 0.5, and 0.75) or even reduce the image input size, trading with accuracy.
    However, reducing the input image size and decreasing the alpha (width multiplier)
    can speed up inference for MobileNet V2, but they have different trade-offs. Let’s
    compare:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reducing Image Input Size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs: []
  type: TYPE_NORMAL
- en: Significantly reduces the computational cost across all layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decreases memory usage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It often provides a substantial speed boost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs: []
  type: TYPE_NORMAL
- en: It may reduce the model’s ability to detect small features or fine details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can significantly impact accuracy, especially for tasks requiring fine-grained
    recognition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reducing Alpha (Width Multiplier):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduces the number of parameters and computations in the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintains the original input resolution, potentially preserving more detail.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can provide a good balance between speed and accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs: []
  type: TYPE_NORMAL
- en: It may not speed up inference as dramatically as reducing input size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can reduce the model’s capacity to learn complex features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Speed Impact:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reducing input size often provides a more substantial speed boost because it
    reduces computations quadratically (halving both width and height reduces computations
    by about 75%).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing alpha provides a more linear reduction in computations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accuracy Impact:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reducing input size can severely impact accuracy, especially when detecting
    small objects or fine details.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing alpha tends to have a more gradual impact on accuracy.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model Architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Changing input size doesn’t alter the model’s architecture.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing alpha modifies the model’s structure by reducing the number of channels
    in each layer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recommendation:'
  prefs: []
  type: TYPE_NORMAL
- en: If our application doesn’t require detecting tiny details and can tolerate some
    loss in accuracy, reducing the input size is often the most effective way to speed
    up inference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reducing alpha might be preferable if maintaining the ability to detect fine
    details is crucial or if you need a more balanced trade-off between speed and
    accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For best results, you might want to experiment with both:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try MobileNet V2 with input sizes like <semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation
    encoding="application/x-tex">160\times 160</annotation></semantics> or <semantics><mrow><mn>92</mn><mo>×</mo><mn>92</mn></mrow><annotation
    encoding="application/x-tex">92\times 92</annotation></semantics>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment with alpha values like 1.0, 0.75, 0.5 or 0.35.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Always benchmark the different configurations on your specific hardware and
    with your particular dataset to find the optimal balance for your use case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remember, the best choice depends on your specific requirements for accuracy,
    speed, and the nature of the images you’re working with. It’s often worth experimenting
    with combinations to find the optimal configuration for your particular use case.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Model Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, you should take the data set aside at the start of the project and run
    the trained model using it as input. Again, the result is excellent (92.22%).
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we did in the previous section, we can deploy the trained model as .tflite
    and use Raspi to run it using Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the `Dashboard` tab, go to Transfer learning model (int8 quantized) and
    click on the download icon:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file842.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s also download the float32 version for comparison
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Transfer the model from your computer to the Raspi (./models), for example,
    using FileZilla. Also, capture some images for inference (./images).
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the needed libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the paths and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Note that the models trained on the Edge Impulse Studio will output values with
    index 0, 1, 2, etc., where the actual labels will follow an alphabetic order.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Load the model, allocate the tensors, and get the input and output tensor details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'One important difference to note is that the `dtype` of the input details of
    the model is now `int8`, which means that the input values go from –128 to +127,
    while each pixel of our image goes from 0 to 255\. This means that we should pre-process
    the image to match it. We can check here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let’s open the image and show it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file843.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And perform the pre-processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking the input data, we can verify that the input tensor is compatible
    with what is expected by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it is time to perform the inference. Let’s also calculate the latency
    of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The model will take around 125ms to perform the inference in the Raspi-Zero,
    which is 3 to 4 times longer than a Raspi-5.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can get the output labels and probabilities. It is also important to
    note that the model trained on the Edge Impulse Studio has a softmax in its output
    (different from the original Movilenet V2), and we should use the model’s raw
    output as the “probabilities.”
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file844.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s modify the function created before so that we can handle different type
    of models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: And test it with different images and the int8 quantized model (**160x160 alpha
    =1.0**).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file845.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s download a smaller model, such as the one trained for the [Nicla Vision
    Lab](https://studio.edgeimpulse.com/public/353482/live) (int8 quantized model,
    96x96, alpha = 0.1), as a test. We can use the same function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file846.png)'
  prefs: []
  type: TYPE_IMG
- en: The model lost some accuracy, but it is still OK once our model does not look
    for many details. Regarding latency, we are around **ten times faster** on the
    Raspi-Zero.
  prefs: []
  type: TYPE_NORMAL
- en: Live Image Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s develop an app to capture images with the USB camera in real time, showing
    its classification.
  prefs: []
  type: TYPE_NORMAL
- en: Using the nano on the terminal, save the code below, such as `img_class_live_infer.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'On the terminal, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'And access the web interface:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the Raspberry Pi itself (if you have a GUI): Open a web browser and go to
    `http://localhost:5000`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From another device on the same network: Open a web browser and go to `http://<raspberry_pi_ip>:5000`
    (Replace `<raspberry_pi_ip>` with your Raspberry Pi’s IP address). For example:
    `http://192.168.4.210:5000/`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here are some screenshots of the app running on an external desktop
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file847.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, you can see the app running on the YouTube:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=o1QsQrpCMw4](https://www.youtube.com/watch?v=o1QsQrpCMw4)'
  prefs: []
  type: TYPE_NORMAL
- en: The code creates a web application for real-time image classification using
    a Raspberry Pi, its camera module, and a TensorFlow Lite model. The application
    uses Flask to serve a web interface where is possible to view the camera feed
    and see live classification results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key Components:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Flask Web Application**: Serves the user interface and handles requests.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**PiCamera2**: Captures images from the Raspberry Pi camera module.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**TensorFlow Lite**: Runs the image classification model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Threading**: Manages concurrent operations for smooth performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Main Features:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Live camera feed display
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjustable confidence threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start/Stop classification on demand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code Structure:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Imports and Setup**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flask for web application
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: PiCamera2 for camera control
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Lite for inference
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Threading and Queue for concurrent operations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global Variables**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Camera and frame management
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification control
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Model and label information
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Camera Functions**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`initialize_camera()`: Sets up the PiCamera2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_frame()`: Continuously captures frames'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_frames()`: Yields frames for the web feed'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Functions**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`load_model()`: Loads the TFLite model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`classify_image()`: Performs inference on a single image'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification Worker**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Runs in a separate thread
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuously classifies frames when active
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates a queue with the latest results
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flask Routes**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`/`: Serves the main HTML page'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/video_feed`: Streams the camera feed'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/start` and `/stop`: Controls classification'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/update_confidence`: Adjusts the confidence threshold'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/get_classification`: Returns the latest classification result'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTML Template**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Displays camera feed and classification results
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides controls for starting/stopping and adjusting settings
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Main Execution**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initializes camera and starts necessary threads
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs the Flask application
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Key Concepts:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Concurrent Operations**: Using threads to handle camera capture and classification
    separately from the web server.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Real-time Updates**: Frequent updates to the classification results without
    page reloads.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Reuse**: Loading the TFLite model once and reusing it for efficiency.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Flexible Configuration**: Allowing users to adjust the confidence threshold
    on the fly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Usage:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ensure all dependencies are installed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the script on a Raspberry Pi with a camera module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Access the web interface from a browser using the Raspberry Pi’s IP address.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start classification and adjust settings as needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Summary:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image classification has emerged as a powerful and versatile application of
    machine learning, with significant implications for various fields, from healthcare
    to environmental monitoring. This chapter has demonstrated how to implement a
    robust image classification system on edge devices like the Raspi-Zero and Raspi-5,
    showcasing the potential for real-time, on-device intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve explored the entire pipeline of an image classification project, from
    data collection and model training using Edge Impulse Studio to deploying and
    running inferences on a Raspi. The process highlighted several key points:'
  prefs: []
  type: TYPE_NORMAL
- en: The importance of proper data collection and preprocessing for training effective
    models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The power of transfer learning, allowing us to leverage pre-trained models like
    MobileNet V2 for efficient training with limited data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The trade-offs between model accuracy and inference speed, especially crucial
    for edge devices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The implementation of real-time classification using a web-based interface,
    demonstrating practical applications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ability to run these models on edge devices like the Raspi opens up numerous
    possibilities for IoT applications, autonomous systems, and real-time monitoring
    solutions. It allows for reduced latency, improved privacy, and operation in environments
    with limited connectivity.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve seen, even with the computational constraints of edge devices, it’s
    possible to achieve impressive results in terms of both accuracy and speed. The
    flexibility to adjust model parameters, such as input size and alpha values, allows
    for fine-tuning to meet specific project requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Looking forward, the field of edge AI and image classification continues to
    evolve rapidly. Advances in model compression techniques, hardware acceleration,
    and more efficient neural network architectures promise to further expand the
    capabilities of edge devices in computer vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This project serves as a foundation for more complex computer vision applications
    and encourages further exploration into the exciting world of edge AI and IoT.
    Whether it’s for industrial automation, smart home applications, or environmental
    monitoring, the skills and concepts covered here provide a solid starting point
    for a wide range of innovative projects.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Dataset Example](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/IMG_CLASS/dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Setup Test Notebook on a Raspi](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/setup_test.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image Classification Notebook on a Raspi](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/10_Image_Classification.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CNN to classify Cifar-10 dataset at CoLab](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/2_Applications_Deploy/Class_16/cifar_10/CNN_Cifar_10_TFLite.ipynb#scrollTo=iiVBUpuHXEtw)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cifar 10 - Image Classification on a Raspi](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/notebooks/20_Cifar_10_Image_Classification.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Scripts](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/IMG_CLASS/python_scripts)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Edge Impulse Project](https://studio.edgeimpulse.com/public/510251/live)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
