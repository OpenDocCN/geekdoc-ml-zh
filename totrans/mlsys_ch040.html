<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch040.xhtml</title>
  <style>
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    /* Figure formatting */
    .quarto-layout-panel>figure>figcaption,
    .quarto-layout-panel>.panel-caption {
      margin-top: 10pt;
    }

    .quarto-layout-row {
      display: flex;
      align-items: flex-start;
    }

    .quarto-layout-valign-top {
      align-items: flex-start;
    }

    .quarto-layout-valign-bottom {
      align-items: flex-end;
    }

    .quarto-layout-valign-center {
      align-items: center;
    }

    .quarto-layout-cell {
      position: relative;
      margin-right: 20px;
    }

    .quarto-layout-cell:last-child {
      margin-right: 0;
    }

    .quarto-layout-cell figure,
    .quarto-layout-cell>p {
      margin: 0.2em;
    }

    .quarto-layout-cell .html-widget {
      width: 100% !important;
    }

    .quarto-layout-cell div figure p {
      margin: 0;
    }

    .quarto-layout-cell figure {
      display: inline-block;
      margin-inline-start: 0;
      margin-inline-end: 0;
    }

    .quarto-layout-cell table {
      display: inline-table;
    }

    .quarto-layout-cell-subref figcaption {
      font-style: italic;
      text-align: center;
    }

    .quarto-figure>figure {
      width: 100%;
    }

    .quarto-figure-left>figure>p {
      text-align: left;
    }

    .quarto-figure-center>figure>p {
      text-align: center;
    }

    .quarto-figure-right>figure>p {
      text-align: right;
    }

    figure>p:empty {
      display: none;
    }

    figure>p:first-child {
      margin-top: 0;
      margin-bottom: 0;
    }

    figure>figcaption {
      margin-top: 0.5em;
    }

    figcaption {
      font-size: 0.8em;
    }

    details {
      margin-bottom: 1em;
    }

    details[show] {
      margin-bottom: 0;
    }

    .quarto-unresolved-ref {
      font-weight: 600;
    }

    .quarto-cover-image {
      float: right;
      margin-left: 30px;
    }

    .cell-output-display {
      overflow-x: scroll;
    }

    .hidden {
      display: none;
    }
  </style>
</head>
<body epub:type="bodymatter">
<section id="image-classification-1" class="level1 unnumbered">
<h1 class="unnumbered">Image Classification</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file539.png" alt="" /></p>
<figcaption><em>DALL·E prompt - 1950s style cartoon illustration based on a real image by Marcelo Rovai</em></figcaption>
</figure>
</div>
<section id="sec-image-classification-overview-9a37" class="level2 unnumbered">
<h2 class="unnumbered">Overview</h2>
<p>We are increasingly facing an artificial intelligence (AI) revolution, where, as <a href="https://www.researchgate.net/figure/Gartner-2023-Artificial-intelligence-emerging-technologies-impact-radar-T-Nguyen-2023_fig1_372048156">Gartner</a> states, <strong>Edge AI and Computer Vision</strong> have a very high impact potential, and <strong>it is for now</strong>!</p>
<p>When we look into Machine Learning (ML) applied to vision, the first concept that greets us is <strong>Image Classification</strong>, a kind of ML’s <em>Hello World</em> that is both simple and profound!</p>
<p>The Seeed Studio XIAOML Kit provides a comprehensive hardware solution centered around the<a href="https://www.seeedstudio.com/xiao-series-page">XIAO ESP32-S3 Sense</a>, featuring an integrated <strong>OV3660</strong> camera and SD card support. Those features make the XIAO ESP32S3 Sense an excellent starting point for exploring TinyML vision AI.</p>
<p>In this Lab, we will explore Image Classification using the non-code tool <strong>SenseCraft AI</strong> and explore a more detailed development with <strong>Edge Impulse Studio</strong> and <strong>Arduino IDE</strong>.</p>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Learning Objectives</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p><strong>Deploy Pre-trained Models</strong> using SenseCraft AI Studio for immediate computer vision applications</p></li>
<li><p><strong>Collect and Manage Image Datasets</strong> for custom classification tasks with proper data organization</p></li>
<li><p><strong>Train Custom Image Classification Models</strong> using transfer learning with MobileNet V2 architecture</p></li>
<li><p><strong>Optimize Models for Edge Deployment</strong> through quantization and memory-efficient preprocessing</p></li>
<li><p><strong>Implement Post-processing Pipelines,</strong> including GPIO control and real-time inference integration</p></li>
<li><p><strong>Compare Development Approaches</strong> between no-code and advanced ML platforms for embedded applications</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="sec-image-classification-image-classification-d726" class="level2 unnumbered">
<h2 class="unnumbered">Image Classification</h2>
<p>Image classification is a fundamental task in computer vision that involves categorizing entire images into one of several predefined classes. This process entails analyzing the visual content of an image and assigning it a label from a fixed set of categories based on the dominant object or scene it depicts.</p>
<p>Image classification is crucial in various applications, ranging from organizing and searching through large databases of images in digital libraries and social media platforms to enabling autonomous systems to comprehend their surroundings. Common architectures that have significantly advanced the field of image classification include Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, and ResNet. These models have demonstrated remarkable accuracy on challenging datasets, such as <a href="https://www.image-net.org/index.php">ImageNet</a>, by learning hierarchical representations of visual data.</p>
<p>As the cornerstone of many computer vision systems, image classification drives innovation, laying the groundwork for more complex tasks like object detection and image segmentation, and facilitating a deeper understanding of visual data across various industries. So, let’s start exploring the <a href="https://sensecraft.seeed.cc/ai/view-model/60768-person-classification?tab=public">Person Classification</a> model (“Person - No Person”), a ready-to-use computer vision application on the <strong><a href="https://sensecraft.seeed.cc/ai/device/local/32">SenseCraft AI</a></strong>.</p>
<p> <img src="../media/file540.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<section id="sec-image-classification-image-classification-sensecraft-ai-workspace-68a8" class="level3 unnumbered">
<h3 class="unnumbered">Image Classification on the SenseCraft AI Workspace</h3>
<p>Start by connecting the XIAOML Kit (or just the XIAO ESP32S3 Sense, disconnected from the Expansion Board) to the computer via USB-C, and then open the <a href="https://sensecraft.seeed.cc/ai/device/local/32">SenseCraft AI Workspace</a> to connect it.</p>
<p> <img src="../media/file541.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>Once connected, select the option <code>[Select Model...]</code> and enter in the search window: “<em>Person Classification</em>”. From the options available, select the one trained over the MobileNet V2 (passing the mouse over the models will open a pop-up window with its main characteristics).</p>
<p> <img src="../media/file542.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>Click on the chosen model and confirm the deployment. A new firmware for the model should start uploading to our device.</p>
<blockquote>
<p>Note that the percentage of models downloaded and firmware uploaded will be displayed. If not, try disconnecting the device, then reconnect it and press the boot button.</p>
</blockquote>
<p>After the model is uploaded successfully, we can view the live feed from the XIAO camera and the classification result (<code>Person</code> or <code>Not a Person</code>) in the <strong>Preview</strong> area, along with the inference details displayed in the <strong>Device Logger</strong>.</p>
<blockquote>
<p>Note that we can also select our <strong>Inference Frame Interval</strong>, from “Real-Time” (Default) to 10 seconds, and the <strong>Mode</strong> (UART, I2C, etc) as the data is shared by the device (the default is UART via USB).</p>
</blockquote>
<p> <img src="../media/file543.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>At the Device Logger, we can see that the latency of the model is from 52 to 78 ms for pre-processing and around 532ms for inference, which will give us a total time of a little less than 600ms, or about <strong>1.7 Frames per second (FPS)</strong>.</p>
<blockquote>
<p>To run the Mobilenet V2 0.35, the XIAO had a peak current of 160mA at 5.23V, resulting in a <strong>power consumption of 830mW</strong>.</p>
</blockquote>
</section>
<section id="sec-image-classification-postprocessing-c011" class="level3 unnumbered">
<h3 class="unnumbered">Post-Processing</h3>
<p>An essential step in an Image Classification project pipeline is to define what we want to do with the inference result. So, imagine that we will use the XIAO to automatically turn on the room lights if a person is detected.</p>
<p> <img src="../media/file544.png" class="quarto-figure quarto-figure-center" style="width:75.0%" alt="" /></p>
<p>With the SebseCraft AI, we can do it on the <code>Output -&gt; GPIO</code> section. Click on the Add icon to trigger the action when event conditions are met. A pop-up window will open, where you can define the action to be taken. For example, if a person is detected with a confidence of more than 60% the internal <code>LED</code> should be ON. In a real scenario, a GPIO, for example, <code>D0</code>, <code>D1</code>, <code>D2</code>, <code>D11</code>, or <code>D12</code>, would be used to trigger a relay to turn on a light.</p>
<p> <img src="../media/file545.png" class="quarto-figure quarto-figure-center" style="width:65.0%" alt="" /></p>
<p>Once confirmed, the created <strong>Trigger Action</strong> will be shown. Press <code>Send</code> to upload the command to the XIAO.</p>
<p> <img src="../media/file546.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>Now, pointing the XIAO at a person will make the internal LED go ON.</p>
<p> <img src="../media/file547.png" class="quarto-figure quarto-figure-center" style="width:65.0%" alt="" /></p>
<blockquote>
<p>We will explore more trigger actions and post-processing techniques further in this lab.</p>
</blockquote>
</section>
</section>
<section id="sec-image-classification-image-classification-project-a2cf" class="level2 unnumbered">
<h2 class="unnumbered">An Image Classification Project</h2>
<p>Let’s create a simple Image Classification project using SenseCraft AI Studio. Below, we can see a typical machine learning pipeline that will be used in our project.</p>
<p> <img src="../media/file548.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>On SenseCraft AI Studio: Let’s open the tab <a href="https://sensecraft.seeed.cc/ai/training">Training</a>:</p>
<p> <img src="../media/file549.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>The default is to train a <code>Classification</code> model with a WebCam if it is available. Let’s select the <code>XIAOESP32S3 Sense</code> instead. Pressing the green button <code>[Connect]</code> will cause a Pop-Up window to appear. Select the corresponding Port and press the blue button <code>[Connect]</code>.</p>
<p> <img src="../media/file550.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>The image streamed from the Grove Vision AI V2 will be displayed.</p>
<section id="sec-image-classification-goal-8443" class="level3 unnumbered">
<h3 class="unnumbered">The Goal</h3>
<p>The first step, as we can see in the ML pipeline, is to define a goal. Let’s imagine that we have an industrial installation that should automatically sort wheels and boxes.</p>
<p> <img src="../media/file551.png" class="quarto-figure quarto-figure-center" style="width:75.0%" alt="" /></p>
<p>So, let’s simulate it, classifying, for example, a toy <code>box</code> and a toy <code>wheel</code>. We should also include a 3rd class of images, <code>background</code>, where there are no objects in the scene.</p>
<p> <img src="../media/file552.png" class="quarto-figure quarto-figure-center" style="width:75.0%" alt="" /></p>
</section>
<section id="sec-image-classification-data-collection-2d1a" class="level3 unnumbered">
<h3 class="unnumbered">Data Collection</h3>
<p>Let’s create the classes, following, for example, an alphabetical order:</p>
<ul>
<li>Class1: background</li>
<li>Class 2: box</li>
<li>Class 3: wheel</li>
</ul>
<p> <img src="../media/file553.png" class="quarto-figure quarto-figure-center" style="width:75.0%" alt="" /></p>
<p>Select one of the classes and keep pressing the green button (<code>Hold to Record</code>) under the preview area. The collected images (and their counting) will appear on the Image Samples Screen. Carefully and slowly, move the camera to capture different angles of the object. To modify the position or interfere with the image, release the green button, rearrange the object, and then hold it again to resume the capture.</p>
<p> <img src="../media/file554.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>After collecting the images, review them and delete any incorrect ones.</p>
<p> <img src="../media/file555.png" class="quarto-figure quarto-figure-center" style="width:75.0%" alt="" /></p>
<p>Collect around <strong>50 images</strong> from each class and go to Training Step.</p>
<blockquote>
<p>Note that it is possible to download the collected images to be used in another application, for example, with the Edge Impulse Studio.</p>
</blockquote>
</section>
<section id="sec-image-classification-training-afb2" class="level3 unnumbered">
<h3 class="unnumbered">Training</h3>
<p>Confirm if the correct device is selected (<code>XIAO ESP32S3 Sense</code>) and press <code>[Start Training]</code></p>
<p> <img src="../media/file556.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
</section>
<section id="sec-image-classification-test-6c2f" class="level3 unnumbered">
<h3 class="unnumbered">Test</h3>
<p>After training, the inference result can be previewed.</p>
<blockquote>
<p>Note that the model is not running on the device. We are, in fact, only capturing the images with the device and performing a <strong>live preview</strong> using the training model, which is running in the Studio.</p>
</blockquote>
<p> <img src="../media/file557.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>Now is the time to really deploy the model in the device.</p>
</section>
<section id="sec-image-classification-deployment-b07e" class="level3 unnumbered">
<h3 class="unnumbered">Deployment</h3>
<p>Select the trained model and <code>XIAO ESP32S3 Sense</code> at the <code>Supported Devices</code> window. And press <code>[Deploy to device]</code>.</p>
<p> <img src="../media/file558.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>The SeneCrafit AI will redirect us to the <strong>Vision Workplace</strong> tab. <code>Confirm</code> the deployment, select the Port, and <code>Connect</code> it.</p>
<p> <img src="../media/file559.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>The model will be flashed into the device. After an automatic reset, the model will start running on the device. On the Device Logger, we can see that the inference has a <strong>latency of approximately 426 ms</strong>, plus a <strong>pre-processing of around 110ms</strong>, corresponding to a <strong>frame rate of 1.8 frames per second (FPS)</strong>.</p>
<p>Also, note that in <strong>Settings</strong>, it is possible to adjust the model’s confidence.</p>
<p> <img src="../media/file560.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<blockquote>
<p>To run the Image Classification Model, the XIAO ESP32S3 had a peak current of 14mA at 5.23V, resulting in a <strong>power consumption of 730mW</strong>.</p>
</blockquote>
<p>As before, in the <strong>Output –&gt; GPIO</strong>, we can turn the GPIOs or the Internal LED ON based on the detected class. For example, the LED will be turned ON when the wheel is detected.</p>
<p> <img src="../media/file561.png" class="quarto-figure quarto-figure-center" style="width:65.0%" alt="" /></p>
</section>
<section id="sec-image-classification-saving-model-01c7" class="level3 unnumbered">
<h3 class="unnumbered">Saving the Model</h3>
<p>It is possible to save the model in the SenseCraft AI Studio. The Studio will retain all our models for later deployment. For that, return to the <code>Training</code> tab and select the button <code>[Save to SenseCraft</code>]:</p>
<p> <img src="../media/file562.png" class="quarto-figure quarto-figure-center" style="width:75.0%" alt="" /></p>
<p>Follow the instructions to enter the model’s name, description, image, and other details.</p>
<p> <img src="../media/file563.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>Note that the trained model (an Int8 MobileNet V2 with a size of 320KB) can be downloaded for further use or even analysis, for example, using <a href="https://github.com/lutzroeder/netron">Netron</a>. Note that the model uses images of size 224x224x3 as its Input Tensor. In the next step, we will use different hyperparameters on the Edge Impulse Studio.</p>
<p> <img src="../media/file564.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>Also, the model can be deployed again to the device at any time. Automatically, the <strong>Workspace</strong> will be open on the SenseCraft AI.</p>
</section>
</section>
<section id="sec-image-classification-image-classification-project-dataset-8079" class="level2 unnumbered">
<h2 class="unnumbered">Image Classification Project from a Dataset</h2>
<p>The primary objective of our project is to train a model and perform inference on the XIAO ESP32S3 Sense. For training, we should find some data <strong>(in fact, tons of data!)</strong>.</p>
<p><em>But as we already know, first of all, we need a goal! What do we want to classify?</em></p>
<p>With TinyML, a set of techniques associated with machine learning inference on embedded devices, we should limit the classification to three or four categories due to limitations (mainly memory). We can, for example, train the images captured for the Box versus Wheel, which can be downloaded from the SenseCraft AI Studio.</p>
<blockquote>
<p>Alternatively, we can use a completely new dataset, such as one that differentiates apples from bananas and potatoes, or other categories. If possible, try finding a specific dataset that includes images from those categories. <a href="https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition">Kaggle fruit-and-vegetable-image-recognition</a> is a good start.</p>
</blockquote>
<p>Let’s download the dataset captured in the previous section. Open the menu (3 dots) on each of the captured classes and select <code>Export Data</code>.</p>
<p> <img src="../media/file565.png" class="quarto-figure quarto-figure-center" style="width:75.0%" alt="" /></p>
<p>The dataset will be downloaded to the computer as a .ZIP file, with one file for each class. Save them in your working folder and unzip them. You should have three folders, one for each class.</p>
<p> <img src="../media/file566.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<blockquote>
<p>Optionally, you can add some fresh images, using, for example, the code discussed in the setup lab.</p>
</blockquote>
</section>
<section id="sec-image-classification-training-model-edge-impulse-studio-f530" class="level2 unnumbered">
<h2 class="unnumbered">Training the model with Edge Impulse Studio</h2>
<p>We will use the Edge Impulse Studio to train our model. <a href="https://www.edgeimpulse.com/">Edge Impulse</a> is a leading development platform for machine learning on edge devices.</p>
<p>Enter your account credentials (or create a free account) at Edge Impulse. Next, create a new project:</p>
<section id="sec-image-classification-data-acquisition-e25e" class="level3 unnumbered">
<h3 class="unnumbered">Data Acquisition</h3>
<p>Next, go to the <strong>Data acquisition</strong> section and there, select <code>+ Add data</code>. A pop-up window will appear. Select <code>UPLOAD DATA</code>.</p>
<p> <img src="../media/file567.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>After selection, a new Pop-Up window will appear, asking to update the data.</p>
<ul>
<li>In Upload mode: <code>select a folder</code> and press <code>[Choose Files]</code>.</li>
<li>Go to the folder that contains one of the classes and press <code>[Upload]</code></li>
</ul>
<p> <img src="../media/file568.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<ul>
<li>You will return automatically to the Upload data window.</li>
<li>Select <code>Automatically split between training and testing</code></li>
<li>And enter the label of the images that are in the folder.</li>
<li>Select <code>[Upload data]</code></li>
<li>At this point, the files will start to be uploaded, and after that, another Pop-Up window will appear asking if you are building an object detection project. Select <code>[no]</code></li>
</ul>
<p> <img src="../media/file569.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>Repeat the procedure for all classes. <strong>Do not forget to change the label’s name</strong>. If you forget and the images are uploaded, please note that they will be mixed in the Studio. Do not worry, you can manually move the data between classes further.</p>
<p>Close the Upload Data window and return to the <strong>Data acquisition</strong> page. We can see that all dataset was uploaded. Note that on the upper panel, we can see that we have 158 items, all of which are balanced. Also, 19% of the images were left for testing.</p>
<p> <img src="../media/file570.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
</section>
<section id="sec-image-classification-impulse-design-4bfa" class="level3 unnumbered">
<h3 class="unnumbered">Impulse Design</h3>
<blockquote>
<p>An impulse takes raw data (in this case, images), extracts features (resizes pictures), and then uses a learning block to classify new data.</p>
</blockquote>
<p>Classifying images is the most common application of deep learning, but a substantial amount of data is required to accomplish this task. We have around 50 images for each category. Is this number enough? Not at all! We will need thousands of images to “teach” or “model” each class, allowing us to differentiate them. However, we can resolve this issue by retraining a previously trained model using thousands of images. We refer to this technique as <strong>“Transfer Learning” (TL)</strong>. With TL, we can fine-tune a pre-trained image classification model on our data, achieving good performance even with relatively small image datasets, as in our case.</p>
<p> <img src="../media/file571.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>With TL, we can fine-tune a pre-trained image classification model on our data, performing well even with relatively small image datasets (our case).</p>
<p>So, starting from the raw images, we will resize them <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>96</mn><mo>×</mo><mn>96</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(96\times 96)</annotation></semantics></math> Pixels are fed to our Transfer Learning block. Let’s create an Impulse.</p>
<blockquote>
<p>At this point, we can also define our target device to monitor our “budget” (memory and latency). The XIAO ESP32S3 is not officially supported by Edge Impulse, so let’s consider the Espressif ESP-EYE, which is similar but slower.</p>
</blockquote>
<p> <img src="../media/file572.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>Save the Impulse, as shown above, and go to the <strong>Image</strong> section.</p>
</section>
<section id="sec-image-classification-preprocessing-feature-generation-ee2e" class="level3 unnumbered">
<h3 class="unnumbered">Pre-processing (Feature Generation)</h3>
<p>Besides resizing the images, we can convert them to grayscale or retain their original RGB color depth. Let’s select <code>[RGB]</code> in the <code>Image</code> section. Doing that, each data sample will have a dimension of 27,648 features (96x96x3). Pressing <code>[Save Parameters]</code> will open a new tab, <code>Generate Features</code>. Press the button <code>[Generate Features]</code>to generate the features.</p>
</section>
<section id="sec-image-classification-model-design-training-test-5f50" class="level3 unnumbered">
<h3 class="unnumbered">Model Design, Training, and Test</h3>
<p>In 2007, Google introduced <a href="https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html">MobileNetV1</a>. In 2018, <a href="https://arxiv.org/abs/1801.04381">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a>, was launched, and, in 2019, the V3. The Mobilinet is a family of general-purpose computer vision neural networks explicitly designed for mobile devices to support classification, detection, and other applications. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.</p>
<p>Although the base MobileNet architecture is already compact and has low latency, a specific use case or application may often require the model to be even smaller and faster. MobileNets introduce a straightforward parameter, <strong>α</strong> (alpha), called the width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is to thin a network uniformly at each layer.</p>
<p>Edge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images), with several different <strong>α</strong> values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at another extreme with MobileNet V1 and α=0.10 (around 53.2K RAM and 101K ROM).</p>
<blockquote>
<p>We will use the <strong>MobileNet V2 0.35</strong> as our base model (but a model with a greater alpha can be used here). The final layer of our model, preceding the output layer, will have 16 neurons with a 10% dropout rate for preventing overfitting.</p>
</blockquote>
<p>Another necessary technique to use with deep learning is <strong>data augmentation</strong>. Data augmentation is a method that can help improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).</p>
<p>Under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb1-1"><a href="#cb1-1"></a><span class="pp"># </span><span class="er">Implements the data augmentation policy</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>def augment_image<span class="op">(</span>image<span class="op">,</span> label<span class="op">):</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>    <span class="pp"># </span><span class="er">Flips the image randomly</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>    image <span class="op">=</span> tf<span class="op">.</span>image<span class="op">.</span>random_flip_left_right<span class="op">(</span>image<span class="op">)</span></span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a>    <span class="pp"># </span><span class="er">Increase the image size, then randomly crop it down to</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>    <span class="pp"># </span><span class="er">the original dimensions</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>    resize_factor <span class="op">=</span> random<span class="op">.</span>uniform<span class="op">(</span><span class="dv">1</span><span class="op">,</span> <span class="fl">1.2</span><span class="op">)</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>    new_height <span class="op">=</span> math<span class="op">.</span>floor<span class="op">(</span>resize_factor <span class="op">*</span> INPUT_SHAPE<span class="op">[</span><span class="dv">0</span><span class="op">])</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>    new_width <span class="op">=</span> math<span class="op">.</span>floor<span class="op">(</span>resize_factor <span class="op">*</span> INPUT_SHAPE<span class="op">[</span><span class="dv">1</span><span class="op">])</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>    image <span class="op">=</span> tf<span class="op">.</span>image<span class="op">.</span>resize_with_crop_or_pad<span class="op">(</span>image<span class="op">,</span> new_height<span class="op">,</span></span>
<span id="cb1-12"><a href="#cb1-12"></a>                                             new_width<span class="op">)</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>    image <span class="op">=</span> tf<span class="op">.</span>image<span class="op">.</span>random_crop<span class="op">(</span>image<span class="op">,</span> size<span class="op">=</span>INPUT_SHAPE<span class="op">)</span></span>
<span id="cb1-14"><a href="#cb1-14"></a></span>
<span id="cb1-15"><a href="#cb1-15"></a>    <span class="pp"># </span><span class="er">Vary the brightness of the image</span></span>
<span id="cb1-16"><a href="#cb1-16"></a>    image <span class="op">=</span> tf<span class="op">.</span>image<span class="op">.</span>random_brightness<span class="op">(</span>image<span class="op">,</span> max_delta<span class="op">=</span><span class="fl">0.2</span><span class="op">)</span></span>
<span id="cb1-17"><a href="#cb1-17"></a></span>
<span id="cb1-18"><a href="#cb1-18"></a>    <span class="cf">return</span> image<span class="op">,</span> label</span></code></pre></div>
<p>Now, let’s us define the hyperparameters:</p>
<ul>
<li>Epochs: 20,</li>
<li>Bach Size: 32</li>
<li>Learning Rate: 0.0005</li>
<li>Validation size: 20%</li>
</ul>
<p>And, so, we have as a training result:</p>
<p> <img src="../media/file573.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>The model profile predicts <strong>233 KB of RAM and 546 KB of Flash</strong>, indicating no problem with the Xiao ESP32S3, which has 8 MB of PSRAM. Additionally, the Studio indicates a <strong>latency of around 1160 ms</strong>, which is very high. However, this is to be expected, given that we are using the ESP-EYE, whose CPU is an Extensa LX6, and the ESP32S3 uses a newer and more powerful Xtensa LX7.</p>
<blockquote>
<p>With the test data, we also achieved 100% accuracy, even with a quantized INT8 model. This result is not typical in real projects, but our project here is relatively simple, with two objects that are very distinctive from each other.</p>
</blockquote>
</section>
</section>
<section id="sec-image-classification-model-deployment-068b" class="level2 unnumbered">
<h2 class="unnumbered">Model Deployment</h2>
<p>We can deploy the trained model:</p>
<ul>
<li>As <code>.TFLITE</code> to be used on the <strong>SenseCraft AI </strong></li>
<li>As an <code>Arduino Library</code> in the <strong>Edge Impulse Studio</strong>.</li>
</ul>
<p>Let’s start with the SenseCraft, which is more straightforward and more intuitive.</p>
<section id="sec-image-classification-model-deployment-sensecraft-ai-fe21" class="level3 unnumbered">
<h3 class="unnumbered">Model Deployment on the SenseCraft AI</h3>
<p>On the <strong>Dashboard</strong>, it is possible to download the trained model in several different formats. Let’s download <code>TensorFlow Lite (int8 quantized)</code>, which has a size of 623KB.</p>
<p> <img src="../media/file574.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>On <strong>SenseCraft AI Studio</strong>, go to the <code>Workspace</code> tab, select <code>XIAO ESP32S3</code>, the corresponding Port, and connect the device.</p>
<p>You should see the last model that was uploaded to the device. Select the green button <code>[Upload Model]</code>. A pop-up window will prompt you to enter the model name, the model file, and the class names (<strong>objects</strong>). We should use labels in alphabetical order: <code>0: background</code>, <code>1: box</code>, and <code>2: wheel</code>, and then press <code>[Send]</code>.</p>
<p> <img src="../media/file575.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>After a few seconds, the model will be uploaded (“flashed”) to our device, and the camera image will appear in real-time on the <strong>Preview</strong> Sector. The Classification result will be displayed under the image preview. It is also possible to select the <code>Confidence Threshold</code> of your inference using the cursor on <strong>Settings</strong>.</p>
<p>On the <strong>Device Logger</strong>, we can view the Serial Monitor, where we can observe the latency, which is approximately 81 ms for pre-processing and 205 ms for inference, <strong>corresponding to a frame rate of 3.4 frames per second (FPS)</strong>, what is double of we got, training the model on SenseCraft, because we are working with smaller images (96x96 versus 224x224).</p>
<blockquote>
<p>The total latency is around <strong>4 times faster</strong> than the estimation made in Edge Impulse Studio on an Xtensa LX6 CPU; now we are performing the inference on an Xtensa LX7 CPU.</p>
</blockquote>
<p> <img src="../media/file576.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<section id="sec-image-classification-postprocessing-3981" class="level4 unnumbered">
<h4 class="unnumbered">Post-Processing</h4>
<p>It is possible to obtain the output of a model inference, including Latency, Class ID, and Confidence, as shown on the Device Logger in SenseCraft AI. This allows us to utilize the <strong>XIAO ESP32S3 Sense as an AI sensor</strong>. In other words, we can retrieve the model data using different communication protocols such as MQTT, UART, I2C, or SPI, depending on our project requirements.</p>
<blockquote>
<p>The idea is similar to what we have done on the <a href="https://www.mlsysbook.ai/contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification#sec-image-classification-postprocessing-9610">Seeed Grove Vision AI V2 Image Classification Post-Processing Lab</a>.</p>
</blockquote>
<p>Below is an example of a connection using the I2C bus.</p>
<!--
![As a Sensor | Seeed Studio Wiki](images/png/iic.png){width=75% fig-align="center"}
--->
<p>Please refer to the <a href="https://wiki.seeedstudio.com/sensecraft-ai/tutorials/sensecraft-ai-output-libraries-xiao/">Seeed Studio Wiki</a> for more information.</p>
</section>
</section>
<section id="sec-image-classification-model-deployment-arduino-library-ei-studio-5579" class="level3 unnumbered">
<h3 class="unnumbered">Model Deployment as an Arduino Library at EI Studio</h3>
<p>On the <strong>Deploy</strong> section at Edge Impulse Studio, Select <code>Arduino library</code>, <code>TensorFlow Lite</code>, <code>Quantized(int8)</code>, and press <code>[Build]</code>. The trained model will be downloaded as a .zip Arduino library:</p>
<p> <img src="../media/file577.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
<p>Open your Arduino IDE, and under <strong>Sketch,</strong> go to <strong>Include Library</strong> and <strong>add .ZIP Library.</strong> Next, select the file downloaded from Edge Impulse Studio and press <code>[Open]</code>.</p>
<p> <img src="../media/file578.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
<p>Go to the Arduino IDE <code>Examples</code> and look for the project by its name (in this case: “Box_versus_Whell_…Interfering”. Open <code>esp32</code> -&gt; <code>esp32_camera</code>. The sketch <code>esp32_camera.ino</code> will be downloaded to the IDE.</p>
<p>This sketch was developed for the standard ESP32 and will not work with the XIAO ESP32S3 Sense. It should be modified. Let’s download the modified one from the project GitHub: <a href="https://github.com/Mjrovai/XIAO-ESP32S3-Sense/blob/main/XIAOML_Kit_code/image_class_XIAOML-Kit/image_class_XIAOML-Kit.ino">Image_class_XIAOML-Kit.ino</a>.</p>
<section id="sec-image-classification-xiao-esp32s3-image-classification-code-explained-5865" class="level4 unnumbered">
<h4 class="unnumbered">XIAO ESP32S3 Image Classification Code Explained</h4>
<p>The code captures images from the onboard camera, processes them, and classifies them (in this case, “Box”, “Wheel”, or “Background”) using the trained model on EI Studio. It runs continuously, performing real-time inference on the edge device.</p>
<p>In short,:</p>
<p>Camera → JPEG Image → RGB888 Conversion → Resize to 96x96 → Neural Network → Classification Results → Serial Output</p>
<section id="sec-image-classification-key-components-5a0e" class="level5 unnumbered">
<h5 class="unnumbered">Key Components</h5>
<ol type="1">
<li><strong>Library Includes and Dependencies</strong></li>
</ol>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1"></a><span class="pp">#include </span><span class="im">&lt;Box_versus_Wheel_-_XIAO_ESP32S3_inferencing.h&gt;</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="pp">#include </span><span class="im">&quot;edge-impulse-sdk/dsp/image/image.hpp&quot;</span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="pp">#include </span><span class="im">&quot;esp_camera.h&quot;</span></span></code></pre></div>
<ul>
<li><strong>Edge Impulse Inference Library</strong>: Contains our trained model and inference engine</li>
<li><strong>Image Processing</strong>: Provides functions for image manipulation</li>
<li><strong>ESP Camera</strong>: Hardware interface for the camera module</li>
</ul>
<ol start="2" type="1">
<li><strong>Camera Pin Configurations</strong></li>
</ol>
<p>The XIAO ESP32S3 Sense can work with different camera sensors (OV2640 or OV3660), which may have different pin configurations. The code defines three possible configurations:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb3-1"><a href="#cb3-1"></a><span class="co">// Configuration 1: Most common OV2640 configuration</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="pp">#define CONFIG_1_XCLK_GPIO_NUM    </span><span class="dv">10</span></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="pp">#define CONFIG_1_SIOD_GPIO_NUM    </span><span class="dv">40</span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="pp">#define CONFIG_1_SIOC_GPIO_NUM    </span><span class="dv">39</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co">// ... more pins</span></span></code></pre></div>
<p>This flexibility allows the code to automatically try different pin mappings if the first one doesn’t work, making it more robust across different hardware revisions.</p>
<ol start="3" type="1">
<li><strong>Memory Management Settings</strong></li>
</ol>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb4-1"><a href="#cb4-1"></a><span class="pp">#define EI_CAMERA_RAW_FRAME_BUFFER_COLS   </span><span class="dv">320</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="pp">#define EI_CAMERA_RAW_FRAME_BUFFER_ROWS   </span><span class="dv">240</span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="pp">#define EI_CLASSIFIER_ALLOCATION_HEAP      </span><span class="dv">1</span></span></code></pre></div>
<ul>
<li><strong>Frame Buffer Size</strong>: Defines the raw image size (320x240 pixels)</li>
<li><strong>Heap Allocation</strong>: Uses dynamic memory allocation for flexibility</li>
<li><strong>PSRAM Support</strong>: The ESP32S3 has 8MB of PSRAM for storing large data like images</li>
</ul>
</section>
<section id="sec-image-classification-setup-initialization-b8f7" class="level5 unnumbered">
<h5 class="unnumbered"><code>setup()</code> - Initialization</h5>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb5-1"><a href="#cb5-1"></a><span class="dt">void</span> setup<span class="op">()</span> <span class="op">{</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>    Serial<span class="op">.</span>begin<span class="op">(</span><span class="dv">115200</span><span class="op">);</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>    <span class="cf">while</span> <span class="op">(!</span>Serial<span class="op">);</span></span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a>    <span class="cf">if</span> <span class="op">(</span>ei_camera_init<span class="op">()</span> <span class="op">==</span> <span class="kw">false</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>        ei_printf<span class="op">(</span><span class="st">&quot;Failed to initialize Camera!</span><span class="sc">\r\n</span><span class="st">&quot;</span><span class="op">);</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>        ei_printf<span class="op">(</span><span class="st">&quot;Camera initialized</span><span class="sc">\r\n</span><span class="st">&quot;</span><span class="op">);</span></span>
<span id="cb5-9"><a href="#cb5-9"></a>    <span class="op">}</span></span>
<span id="cb5-10"><a href="#cb5-10"></a></span>
<span id="cb5-11"><a href="#cb5-11"></a>    ei_sleep<span class="op">(</span><span class="dv">2000</span><span class="op">);</span>  <span class="co">// Wait 2 seconds before starting</span></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="op">}</span></span></code></pre></div>
<p>This function:</p>
<ol type="1">
<li>Initializes serial communication for debugging output</li>
<li>Initializes the camera with automatic configuration detection</li>
<li>Waits 2 seconds before starting continuous inference</li>
</ol>
</section>
<section id="sec-image-classification-loop-main-processing-loop-f973" class="level5 unnumbered">
<h5 class="unnumbered"><code>loop()</code> - Main Processing Loop</h5>
<p>The loop performs these steps continuously:</p>
<p><strong>Step 1: Memory Allocation</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb6-1"><a href="#cb6-1"></a>snapshot_buf <span class="op">=</span> <span class="op">(</span><span class="dt">uint8_t</span><span class="op">*)</span>ps_malloc<span class="op">(</span>EI_CAMERA_RAW_FRAME_BUFFER_COLS <span class="op">*</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>                                   EI_CAMERA_RAW_FRAME_BUFFER_ROWS <span class="op">*</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>                                   EI_CAMERA_FRAME_BYTE_SIZE<span class="op">);</span></span></code></pre></div>
<p>Allocates memory for the image buffer, preferring PSRAM (faster external RAM) but falling back to regular heap if needed.</p>
<p><strong>Step 2: Image Capture</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb7-1"><a href="#cb7-1"></a><span class="cf">if</span> <span class="op">(</span>ei_camera_capture<span class="op">((</span><span class="dt">size_t</span><span class="op">)</span>EI_CLASSIFIER_INPUT_WIDTH<span class="op">,</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>                     <span class="op">(</span><span class="dt">size_t</span><span class="op">)</span>EI_CLASSIFIER_INPUT_HEIGHT<span class="op">,</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>                     snapshot_buf<span class="op">)</span> <span class="op">==</span> <span class="kw">false</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb7-4"><a href="#cb7-4"></a>    ei_printf<span class="op">(</span><span class="st">&quot;Failed to capture image</span><span class="sc">\r\n</span><span class="st">&quot;</span><span class="op">);</span></span>
<span id="cb7-5"><a href="#cb7-5"></a>    free<span class="op">(</span>snapshot_buf<span class="op">);</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>    <span class="cf">return</span><span class="op">;</span></span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="op">}</span></span></code></pre></div>
<p>Captures an image from the camera and stores it in the buffer.</p>
<p><strong>Step 3: Run Inference</strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb8-1"><a href="#cb8-1"></a><span class="dt">ei_impulse_result_t</span> result <span class="op">=</span> <span class="op">{</span> <span class="dv">0</span> <span class="op">};</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>EI_IMPULSE_ERROR err <span class="op">=</span> run_classifier<span class="op">(&amp;</span>signal<span class="op">,</span> <span class="op">&amp;</span>result<span class="op">,</span> <span class="kw">false</span><span class="op">);</span></span></code></pre></div>
<p>Runs the machine learning model on the captured image.</p>
<p><strong>Step 4: Output Results</strong></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb9-1"><a href="#cb9-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">uint16_t</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> EI_CLASSIFIER_LABEL_COUNT<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>    ei_printf<span class="op">(</span><span class="st">&quot;  </span><span class="sc">%s</span><span class="st">: </span><span class="sc">%.5f\r\n</span><span class="st">&quot;</span><span class="op">,</span></span>
<span id="cb9-3"><a href="#cb9-3"></a>              ei_classifier_inferencing_categories<span class="op">[</span>i<span class="op">],</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>              result<span class="op">.</span>classification<span class="op">[</span>i<span class="op">].</span>value<span class="op">);</span></span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="op">}</span></span></code></pre></div>
<p>Prints the classification results showing confidence scores for each category.</p>
</section>
<section id="sec-image-classification-ei_camera_init-smart-camera-initialization-e11e" class="level5 unnumbered">
<h5 class="unnumbered"><code>ei_camera_init()</code> - Smart Camera Initialization</h5>
<p>This function implements an intelligent initialization sequence:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb10-1"><a href="#cb10-1"></a><span class="dt">bool</span> ei_camera_init<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>    <span class="co">// Try Configuration 1 (OV2640 common)</span></span>
<span id="cb10-3"><a href="#cb10-3"></a>    update_camera_config<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb10-4"><a href="#cb10-4"></a>    <span class="dt">esp_err_t</span> err <span class="op">=</span> esp_camera_init<span class="op">(&amp;</span>camera_config<span class="op">);</span></span>
<span id="cb10-5"><a href="#cb10-5"></a>    <span class="cf">if</span> <span class="op">(</span>err <span class="op">==</span> ESP_OK<span class="op">)</span> <span class="cf">goto</span> camera_init_success<span class="op">;</span></span>
<span id="cb10-6"><a href="#cb10-6"></a></span>
<span id="cb10-7"><a href="#cb10-7"></a>    <span class="co">// Try Configuration 2 (OV3660)</span></span>
<span id="cb10-8"><a href="#cb10-8"></a>    esp_camera_deinit<span class="op">();</span></span>
<span id="cb10-9"><a href="#cb10-9"></a>    update_camera_config<span class="op">(</span><span class="dv">2</span><span class="op">);</span></span>
<span id="cb10-10"><a href="#cb10-10"></a>    err <span class="op">=</span> esp_camera_init<span class="op">(&amp;</span>camera_config<span class="op">);</span></span>
<span id="cb10-11"><a href="#cb10-11"></a>    <span class="cf">if</span> <span class="op">(</span>err <span class="op">==</span> ESP_OK<span class="op">)</span> <span class="cf">goto</span> camera_init_success<span class="op">;</span></span>
<span id="cb10-12"><a href="#cb10-12"></a></span>
<span id="cb10-13"><a href="#cb10-13"></a>    <span class="co">// Continue trying other configurations...</span></span>
<span id="cb10-14"><a href="#cb10-14"></a><span class="op">}</span></span></code></pre></div>
<p>The function:</p>
<ol type="1">
<li>Tries multiple pin configurations</li>
<li>Tests different clock frequencies (10MHz or 16MHz)</li>
<li>Attempts PSRAM first, then falls back to DRAM</li>
<li>Applies sensor-specific settings based on detected hardware</li>
</ol>
</section>
<section id="sec-image-classification-ei_camera_capture-image-processing-pipeline-da03" class="level5 unnumbered">
<h5 class="unnumbered"><code>ei_camera_capture()</code> - Image Processing Pipeline</h5>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb11-1"><a href="#cb11-1"></a><span class="dt">bool</span> ei_camera_capture<span class="op">(</span><span class="dt">uint32_t</span> img_width<span class="op">,</span> <span class="dt">uint32_t</span> img_height<span class="op">,</span> <span class="dt">uint8_t</span> <span class="op">*</span>out_buf<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>    <span class="co">// 1. Get frame from camera</span></span>
<span id="cb11-3"><a href="#cb11-3"></a>    <span class="dt">camera_fb_t</span> <span class="op">*</span>fb <span class="op">=</span> esp_camera_fb_get<span class="op">();</span></span>
<span id="cb11-4"><a href="#cb11-4"></a></span>
<span id="cb11-5"><a href="#cb11-5"></a>    <span class="co">// 2. Convert JPEG to RGB888 format</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>    <span class="dt">bool</span> converted <span class="op">=</span> fmt2rgb888<span class="op">(</span>fb<span class="op">-&gt;</span>buf<span class="op">,</span> fb<span class="op">-&gt;</span>len<span class="op">,</span> PIXFORMAT_JPEG<span class="op">,</span> snapshot_buf<span class="op">);</span></span>
<span id="cb11-7"><a href="#cb11-7"></a></span>
<span id="cb11-8"><a href="#cb11-8"></a>    <span class="co">// 3. Return frame buffer to camera driver</span></span>
<span id="cb11-9"><a href="#cb11-9"></a>    esp_camera_fb_return<span class="op">(</span>fb<span class="op">);</span></span>
<span id="cb11-10"><a href="#cb11-10"></a></span>
<span id="cb11-11"><a href="#cb11-11"></a>    <span class="co">// 4. Resize if needed</span></span>
<span id="cb11-12"><a href="#cb11-12"></a>    <span class="cf">if</span> <span class="op">(</span>do_resize<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-13"><a href="#cb11-13"></a>        ei<span class="op">::</span>image<span class="op">::</span>processing<span class="op">::</span>crop_and_interpolate_rgb888<span class="op">(...);</span></span>
<span id="cb11-14"><a href="#cb11-14"></a>    <span class="op">}</span></span>
<span id="cb11-15"><a href="#cb11-15"></a><span class="op">}</span></span></code></pre></div>
<p>This function:</p>
<ol type="1">
<li>Captures a JPEG image from the camera</li>
<li>Converts it to RGB888 format (required by the ML model)</li>
<li>Resizes the image to match the model’s input size (96x96 pixels)</li>
</ol>
</section>
</section>
</section>
<section id="sec-image-classification-inference-add8" class="level3 unnumbered">
<h3 class="unnumbered">Inference</h3>
<ul>
<li>Upload the code to the XIAO ESP32S3 Sense.</li>
</ul>
<blockquote>
<p>⚠️ <strong>Attention</strong></p>
<ul>
<li>The Xiao ESP32S3 <strong>MUST</strong> have the PSRAM enabled. You can check it on the Arduino IDE upper menu: <code>Tools</code>–&gt; <code>PSRAM:OPI PSRAM</code></li>
<li>The Arduino Boards package (<code>esp32 by Espressif Systems</code>) should be <strong>version 2.017</strong>. Do not update it</li>
</ul>
</blockquote>
<p> <img src="../media/file579.png" class="quarto-figure quarto-figure-center" style="width:90.0%" alt="" /></p>
<ul>
<li>Open the Serial Monitor</li>
<li>Point the camera at the objects, and check the result on the Serial Monitor.</li>
</ul>
<p> <img src="../media/file580.png" class="quarto-figure quarto-figure-center" style="width:90.0%" alt="" /></p>
</section>
<section id="sec-image-classification-postprocessing-dcc8" class="level3 unnumbered">
<h3 class="unnumbered">Post-Processing</h3>
<p>In edge AI applications, the inference result is only as valuable as our ability to act upon it. While serial output provides detailed information for debugging and development, real-world deployments require immediate, human-readable feedback that doesn’t depend on external monitors or connections.</p>
<p>The XIAOML Kit tiny 0.42” OLED display (72×40 pixels) serves as a crucial post-processing component that transforms raw ML inference results into immediate, human-readable feedback—displaying detected class names and confidence levels directly on the device, eliminating the need for external monitors and enabling truly standalone edge AI deployment in industrial, agricultural, or retail environments where instant visual confirmation of AI predictions is essential.</p>
<p>So, let’s modify the sketch to automatically adapt to the model trained on Edge Impulse by reading the class names and count directly from the model. The display will show abbreviated class names (3 letters) with larger fonts for better visibility on the tiny 72x40 pixel display. Download the code from the GitHub: <a href="https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code/XIAOML-Kit-Img_Class_OLED_Gen">XIAOML-Kit-Img_Class_OLED_Gen</a>.</p>
<p>Running the code, we can see the result:</p>
<p> <img src="../media/file581.png" class="quarto-figure quarto-figure-center" style="width:90.0%" alt="" /></p>
</section>
</section>
<section id="sec-image-classification-summary-f24b" class="level2 unnumbered">
<h2 class="unnumbered">Summary</h2>
<p>The XIAO ESP32S3 Sense is a remarkably capable and flexible platform for image classification applications. Through this lab, we’ve explored two distinct development approaches that cater to different skill levels and project requirements.</p>
<ul>
<li><p>The <strong>SenseCraft AI Studio</strong> provides an accessible entry point with its <strong>no-code interface</strong>, enabling rapid prototyping and deployment of pre-trained models like person detection. With real-time inference and integrated post-processing capabilities, it demonstrates how AI can be deployed without extensive programming or ML knowledge.</p></li>
<li><p>For more advanced applications, <strong>Edge Impulse Studio</strong> offers comprehensive machine learning pipeline tools, including custom dataset management, transfer learning with several pre-trained models, such as MobileNet, and model optimization.</p></li>
</ul>
<p>Key insights from this lab include the importance of image resolution trade-offs, the effectiveness of transfer learning for small datasets, and the practical considerations of edge AI deployment, such as power consumption and memory constraints.</p>
<p>The Lab demonstrates fundamental TinyML principles that extend beyond this specific hardware: resource-constrained inference, real-time processing requirements, and the complete pipeline from data collection through model deployment to practical applications. With built-in post-processing capabilities including GPIO control and communication protocols, the XIAO serves as more than just an inference engine—it becomes a complete AI sensor platform.</p>
<p>This foundation in image classification prepares you for more complex computer vision tasks while showcasing how modern edge AI makes sophisticated computer vision accessible, cost-effective, and deployable in real-world embedded applications ranging from industrial automation to smart home systems.</p>
</section>
<section id="sec-image-classification-resources-1cf2" class="level2 unnumbered">
<h2 class="unnumbered">Resources</h2>
<ul>
<li><a href="https://wiki.seeedstudio.com/xiao_esp32s3_getting_started/">Getting Started with the XIAO ESP32S3</a></li>
<li><a href="https://sensecraft.seeed.cc/ai/home">SenseCraft AI Studio Home</a></li>
<li><a href="https://sensecraft.seeed.cc/ai/device/local/32">SenseCraft Vision Workspace</a></li>
<li><a href="https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition">Dataset example</a></li>
<li><a href="https://studio.edgeimpulse.com/public/757065/live">Edge Impulse Project</a></li>
<li><a href="https://wiki.seeedstudio.com/sensecraft-ai/tutorials/sensecraft-ai-output-libraries-xiao/">XIAO as an AI Sensor</a></li>
<li><a href="https://github.com/Seeed-Studio/Seeed_Arduino_SSCMA">Seeed Arduino SSCMA Library</a></li>
<li><a href="https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code">XIAOML Kit Code</a></li>
</ul>
</section>
</section>
</body>
</html>
