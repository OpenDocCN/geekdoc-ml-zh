<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>5.1.3 Dimensionality reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>5.1.3 Dimensionality reduction</h1>
<blockquote>原文：<a href="https://huyenchip.com/ml-interviews-book/contents/5.1.3-dimensionality-reduction.html">https://huyenchip.com/ml-interviews-book/contents/5.1.3-dimensionality-reduction.html</a></blockquote>
                                
                                
<p><em>If some characters seem to be missing, it's because MathJax is not loaded correctly. Refreshing the page should fix it.</em></p>
<ol>
<li>[E] Why do we need dimensionality reduction?</li>
<li>[E] Eigendecomposition is a common factorization technique used for dimensionality reduction. Is the eigendecomposition of a matrix always unique?</li>
<li>[M] Name some applications of eigenvalues and eigenvectors.</li>
<li>[M] We want to do PCA on a dataset of multiple features in different ranges. For example, one is in the range 0-1 and one is in the range 10 - 1000. Will PCA work on this dataset?</li>
<li>[H] Under what conditions can one apply eigendecomposition? What about SVD?<ol>
<li>What is the relationship between SVD and eigendecomposition?</li>
<li>What’s the relationship between PCA and SVD?</li>
</ol>
</li>
<li>[H] How does t-SNE (T-distributed Stochastic Neighbor Embedding) work? Why do we need it?</li>
</ol>
<blockquote>
<p><strong>In case you need a refresh on PCA, here's an explanation without any math.</strong></p>
<p>Assume that your grandma likes wine and would like to find characteristics that best describe wine bottles sitting in her cellar. There are many characteristics we can use to describe a bottle of wine including age, price, color, alcoholic content, sweetness, acidity, etc. Many of these characteristics are related and therefore redundant. Is there a way we can choose fewer characteristics to describe our wine and answer questions such as: which two bottles of wine differ the most?</p>
<p>PCA is a technique to construct new characteristics out of the existing characteristics. For example, a new characteristic might be computed as <code>age - acidity + price</code> or something like that, which we call a linear combination.</p>
<p>To differentiate our wines, we'd like to find characteristics that strongly differ across wines. If we find a new characteristic that is the same for most of the wines, then it wouldn't be very useful. PCA looks for characteristics that show as much variation across wines as possible, out of all linear combinations of existing characteristics. These constructed characteristics are principal components of our wines.</p>
<p>If you want to see a more detailed, intuitive explanation of PCA with visualization, check out <a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579" target="_blank">amoeba's answer on StackOverflow</a>. This is possibly the best PCA explanation I've ever read.</p>
</blockquote>

                                
                                    
</body>
</html>