["```py\nimport numpy as np                                            # arrays and matrix math\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom scipy.ndimage import convolve1d\nseed = 73073                                                  # random number seed\n\nkr = 5; kloc = 45                                             # kernel radius, kernel location for example point\n\ndf_denpor = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/1D_por_perm_smooth.csv') # load data from Dr. Pyrcz's GitHub \n\nplt.subplot(121)                    \nplt.plot(df_denpor['Por'],df_denpor['Depth'],color='black',label='f(x)'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)')\nplt.ylim([100,0]); plt.xlim([2,22]); plt.title('Exhaustive Case: Original and Convolved Function'); plt.yticks(np.arange(100, -1, -5));\n\nsize = 2 * kr + 1                                             # make uniform kernel\nkernel = np.ones(size) / size                                 # normalize kernel to sum to one for unbiasedness\n\nconvolved = np.convolve(df_denpor['Por'].values, kernel, mode='same') # convolution\n\nk = len(kernel)\ntrim = k // 2  # how many values to trim from each edge\nconvolved_valid = convolved[trim:-trim] if trim > 0 else convolved\ndepth_valid = df_denpor['Depth'].values[trim:-trim] if trim > 0 else df_denpor['Depth'].values\nconvolved_pt = convolved_valid[np.abs(depth_valid - kloc).argmin()]; depth_pt = depth_valid[np.abs(depth_valid - kloc).argmin()]\nplt.plot(convolved_valid,depth_valid,color='red',label=r'$(f * g)(x)$'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)'); \nplt.ylim([100,0]); plt.xlim([2,22]); plt.legend(loc='lower left'); plt.plot([2,22],[depth_pt,depth_pt],color='red',ls='--')\nplt.scatter(convolved_pt,depth_pt,color='red',marker='o',edgecolor='black',zorder=10)\n\nplt.subplot(122)\nplt.plot(kernel,np.linspace(-1*kr+kloc,kr+kloc,2*kr+1),color='black',label=r'$g(\\tau)$'); plt.xlim(0.35,0.0); plt.ylim([100,0]); \nplt.yticks(np.arange(100, -1, -5)); plt.plot([kernel[0],0.0],[-1*kr+kloc,-1*kr+kloc],color='black'); \nplt.plot([kernel[0],0.0],[kr+kloc,kr+kloc],color='black'); plt.legend(loc='lower left'); plt.title('Kernel')\nplt.plot([0.35,0.0],[depth_pt,depth_pt],color='red',ls='--'); plt.ylabel('Depth (m)'); plt.xlabel('Weight (unitless)')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nimport astropy.convolution.convolve as convolve               # sparse data convolution\nfrac = 0.2\n\ndf_denpor['Por_Sparse'] = df_denpor['Por'].copy()\n\nnp.random.seed(seed = seed)\nnan_indices = np.random.choice(len(df_denpor), size=int(len(df_denpor)*(1.0-frac)), replace=False)\ndf_denpor.loc[nan_indices, 'Por_Sparse'] = np.nan\n\nplt.subplot(121)                    \nplt.scatter(df_denpor['Por_Sparse'],df_denpor['Depth'],color='black',label='f(x) Sparse'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)')\nplt.plot(df_denpor['Por'],df_denpor['Depth'],color='black',alpha=0.3,label='f(x) Exhaustive');\nplt.ylim([26,0]); plt.xlim([2,22]); plt.title('Sparse Case: Original and Convolved Function'); plt.yticks(np.arange(100, -1, -5));\n\nsize = 2 * kr + 1                                             # make uniform kernel\nkernel = np.ones(size) / size                                 # normalize kernel to sum to one for unbiasedness\n\nconvolved = convolve(df_denpor['Por_Sparse'].values,kernel,boundary='extend',nan_treatment='interpolate',normalize_kernel=True) # convolve\n\nk = len(kernel)\ntrim = k // 2  # how many values to trim from each edge\nconvolved_valid = convolved[trim:-trim] if trim > 0 else convolved\ndepth_valid = df_denpor['Depth'].values[trim:-trim] if trim > 0 else df_denpor['Depth'].values\nconvolved_pt = convolved_valid[np.abs(depth_valid - kloc).argmin()]; depth_pt = depth_valid[np.abs(depth_valid - kloc).argmin()]\nplt.plot(convolved_valid,depth_valid,color='red',label=r'$(f * g)(x)$'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)'); \nplt.ylim([100,0]); plt.xlim([2,22]); plt.legend(loc='lower left'); plt.plot([2,22],[depth_pt,depth_pt],color='red',ls='--')\nplt.scatter(convolved_pt,depth_pt,color='red',marker='o',edgecolor='black',zorder=10)\n\nplt.subplot(122)\nplt.plot(kernel,np.linspace(-1*kr+kloc,kr+kloc,2*kr+1),color='black',label=r'$g(\\tau)$'); plt.xlim(0.35,0.0); plt.ylim([100,0]); \nplt.yticks(np.arange(100, -1, -5)); plt.plot([kernel[0],0.0],[-1*kr+kloc,-1*kr+kloc],color='black'); \nplt.plot([kernel[0],0.0],[kr+kloc,kr+kloc],color='black'); plt.legend(loc='lower left'); plt.title('Kernel')\nplt.plot([0.35,0.0],[depth_pt,depth_pt],color='red',ls='--'); plt.ylabel('Depth (m)'); plt.xlabel('Weight (unitless)')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nWARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve] \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport pandas.plotting as pd_plot\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.preprocessing import StandardScaler              # standardize the features\nfrom sklearn.neighbors import KNeighborsRegressor             # for nearest k neighbours\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning\nfrom sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # suppress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef comma_format(x, pos):\n    return f'{int(x):,}'\n\ndef feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot\n    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1\n    plt.plot(pred,metric,color='black',zorder=20)\n    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)\n    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  \n    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)\n    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();\n    return\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    cbar = plt.colorbar(im, orientation = 'vertical')\n    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n\ndef visualize_model(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,title,axes_commas = True): # plots the data points and the decision tree prediction \n    n_classes = 10\n    cmap_temp = plt.cm.inferno\n    xplot_step = (x_max-x_min)/100; yplot_step = (y_max-y_min)/100\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step),\n                     np.arange(y_min, y_max, yplot_step))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=cmap_temp,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))\n\n    im = plt.scatter(xfeature,yfeature,s=30, c=response, marker=None, cmap=cmap_temp, norm=None, vmin=z_min, vmax=z_max, \n                     alpha=1.0, linewidths=0.8, edgecolors=\"black\",zorder=10)\n    plt.scatter(xfeature,yfeature,s=60, c='white', marker=None, cmap=cmap_temp, norm=None, vmin=z_min, vmax=z_max, \n                     alpha=1.0, linewidths=0.8, edgecolors=None,zorder=8)\n    plt.title(title); plt.xlabel(xfeature.name); plt.ylabel(yfeature.name)\n    cbar = plt.colorbar(im, orientation = 'vertical'); cbar.set_label(response.name, rotation=270, labelpad=20)\n    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n    if axes_commas == True:\n        plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n        plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n    return Z\n\ndef visualize_tuned_model(k_tuned,k_mat,score_mat):\n    plt.scatter(k_mat,score_mat,s=10.0, c=\"red\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, \n                linewidths=0.5, edgecolors=\"black\")\n    plt.plot([k_tuned,k_tuned],[0,10000000],color='black',linestyle=(6, (2,3)),label='tuned',zorder=1)\n    plt.title('k-fold Cross Validation Error (MSE) vs. k Nearest Neighbours'); plt.xlabel('Number of Nearest Neighbours')\n    plt.ylabel('Mean Square Error')\n    plt.xlim(k_min,k_max); plt.ylim(0,np.max(score_mat))\n\ndef check_model(model,xtrain,ytrain,xtest,ytest,ymin,ymax,rtrain,rtest,title): # plots the estimated vs. the actual \n    predict_train = model.predict(np.c_[xtrain,ytrain])\n    predict_test = model.predict(np.c_[xtest,ytest])\n    plt.scatter(rtrain,predict_train,s=None, c='darkorange',marker=None, cmap=None, norm=None, vmin=None, vmax=None, \n                alpha=0.8, linewidths=0.8, edgecolors=\"black\",label='Train')\n    plt.scatter(rtest,predict_test,s=None, c='red',marker=None, cmap=None, norm=None, vmin=None, vmax=None, \n                alpha=0.8, linewidths=0.8, edgecolors=\"black\",label='Test')\n    plt.title(title); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')\n    plt.xlim(ymin,ymax); plt.ylim(ymin,ymax)\n    plt.arrow(ymin,ymin,ymax,ymax,width=0.02,color='black',head_length=0.0,head_width=0.0)\n    MSE_train = metrics.mean_squared_error(rtrain,predict_train)\n    Var_Explained_train = metrics.explained_variance_score(rtrain,predict_train)\n    cor_train = math.sqrt(metrics.r2_score(rtrain,predict_train))\n    MSE_test = metrics.mean_squared_error(rtest,predict_test)\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.annotate('Train MSE: ' + str(f'{(np.round(MSE_train,2)):,}'),[0.05*(ymax-ymin)+ymin,0.95*(ymax-ymin)+ymin]) \n    plt.annotate('Test MSE:  ' + str(f'{(np.round(MSE_test,2)):,}'),[0.05*(ymax-ymin)+ymin,0.90*(ymax-ymin)+ymin])\n    add_grid(); plt.legend(loc='lower right')\n    # print('Mean Squared Error on Training = ', round(MSE_test,2),', Variance Explained =', round(Var_Explained,2),'Cor =', round(cor,2))\n\ndef weighted_percentile(data, weights, perc):                 # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) \n    ix = np.argsort(data)\n    data = data[ix] \n    weights = weights[ix] \n    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) \n    return np.interp(perc, cdf, data)\n\ndef histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram \n    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)\n    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')\n    plt.plot([avg,avg],[0.0,45],color = color)\n    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\ndf_load = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub \ndf_load = df_load.iloc[:,1:8]                                 # copy all rows and columns 1 through 8, note 0 column is removed\n\nresponse = 'Prod'                                             # specify the response feature\nadd_noise = True                                              # set True to add noise to response feature to demonstrate overfit\nnoise_stdev = 500                                             # amount of noise to add to response feature to demonstrate overfit\n\nnp.random.seed(seed = seed)                                   # set the random number seed\nif add_noise == True:\n    df_load[response] = df_load[response] + np.random.normal(loc=0.0,scale=noise_stdev,size = len(df_load))\n\nX = df_load.copy(deep = False)\nX = X.drop([response],axis='columns')                         # make predictor and response DataFrames\ny = df_load.loc[:,response]\n\nfeatures = X.columns.values.tolist() + [y.name]               # store the names of the features\npred = X.columns.values.tolist()\nresp = [y.name]\n\nXmin = [6.0,0.0,1.0,10.0,0.0,0.9]; Xmax = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minumum and maximum values for plotting\nymin = 1000.0; ymax = 9000.0\n\npredlabel = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\nresplabel = 'Normalized Initial Production (MCFPD)'\n\npredtitle = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n             'Total Organic Carbon','Vitrinite Reflectance']\nresptitle = 'Initial Production'\n\nfeaturelabel = predlabel + [resplabel]                        # make feature labels and titles for concise code\nfeaturetitle = predtitle + [resptitle]\n\nm = len(pred) + 1\nmpred = len(pred)\n\ndf = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) \n```", "```py\ndf.head(n=13) \n```", "```py\ndf.describe().transpose()                                     # calculate summary statistics for the data \n```", "```py\nnum = df._get_numeric_data()                                  # get the numerical values\nnum[num < 0] = 0                                              # truncate negative values to 0.0\ndf.describe().transpose()                                     # calculate summary statistics for the data \n```", "```py\ncorr_matrix = df.corr()\ncorrelation = corr_matrix.iloc[:,-1].values[:-1]\n\nplt.subplot(121)\nplot_corr(corr_matrix,'Correlation Matrix',1.0,0.5)           # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(pred,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp[0],'Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npairgrid = sns.PairGrid(df,vars=['Por','Perm','AI','Brittle','TOC','Prod']) # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nif1 = 0; if2 = 3                                              # selected predictor features\n\ntransform = StandardScaler();                                 # instantiate feature standardization method\n\nsel_pred = [pred[if1],pred[if2]]\nsel_features = pred + [resp]\n\nspredlabel = ['Standardized ' + element for element in predlabel] # standardized predictors list\n\nsel_spredlabel = [spredlabel[if1]] + [spredlabel[if2]] \n\nsel_spred = ['s' + element for element in sel_pred]           # standardized predictors list\n\ndf[sel_spred[0]] = transform.fit_transform(df.loc[:,sel_pred].values)[:,0] # standardize the data features to mean = 0, var = 1.0\ndf[sel_spred[1]] = transform.fit_transform(df.loc[:,sel_pred].values)[:,1] # standardize the data features to mean = 0, var = 1.0\n\nprint('Selected Predictor Features: ' + str(sel_pred))\nprint('Standardized Selected Predictor Features: ' + str(sel_spred))\nprint('Response Feature: ' + str([resp]))\ndf.head() \n```", "```py\nSelected Predictor Features: ['Por', 'Brittle']\nStandardized Selected Predictor Features: ['sPor', 'sBrittle']\nResponse Feature: [['Prod']] \n```", "```py\nprint('Backtransformed: \\n Por    Brittle')\ntransform.inverse_transform(df.loc[:,sel_spred])[:5,:]        # check the reverse standardization \n```", "```py\nBacktransformed: \n        Por    Brittle \n```", "```py\narray([[12.08, 81.4 ],\n       [12.38, 46.17],\n       [14.02, 72.8 ],\n       [17.67, 39.81],\n       [17.52, 10.94]]) \n```", "```py\nXmin = [5.0, 0.0]; Xmax = [25.0,100.0]                        # selected predictor features min and max \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(df.loc[:,sel_spred],df.loc[:,resp],test_size=0.25,random_state=73073)\ndf_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)\ndf_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(221)\nplt.hist(X_train[sel_spred[0]],alpha = 0.8,color = 'darkorange',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Train')\nplt.hist(X_test[sel_spred[0]],alpha = 0.8,color = 'red',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Test')\nplt.title(sel_spred[0]); plt.xlim(-3,3); plt.xlabel(sel_spredlabel[0]); plt.ylabel('Frequency'); add_grid(); plt.legend(loc='upper right')\n\nplt.subplot(222)\nplt.hist(X_train[sel_spred[1]],alpha = 0.8,color = 'darkorange',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Train')\nplt.hist(X_test[sel_spred[1]],alpha = 0.8,color = 'red',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Test')\nplt.title(sel_spred[1]); plt.xlim(-3,3); plt.xlabel(sel_spredlabel[1]); plt.ylabel('Frequency'); add_grid(); plt.legend(loc='upper right')\n\nplt.subplot(223)\nplt.hist(y_train[resp],alpha = 0.8,color = 'darkorange',edgecolor = 'black',bins=np.linspace(ymin,ymax,nbins),label='Train')\nplt.hist(y_test[resp],alpha = 0.8,color = 'red',edgecolor = 'black',bins=np.linspace(ymin,ymax,nbins),label='Test')\nplt.legend(loc='upper right'); plt.title(resp[0]); plt.xlim(ymin,ymax) \nplt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.xlabel(resplabel); plt.ylabel('Frequency'); add_grid()\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nplt.subplot(121)                                              # train data plot\nim = plt.scatter(X_train[sel_spred[0]],X_train[sel_spred[1]],s=None, c=y_train[resp[0]], marker=None, cmap=cmap, norm=None, vmin=ymin, \n                 vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\nplt.title('Train ' + resp[0] + ' vs. ' + sel_spred[1] + ' and ' + sel_spred[1]); plt.xlabel(sel_spredlabel[0]); plt.ylabel(sel_spredlabel[1])\nplt.xlim(-3,3); plt.ylim(-3,3)\ncbar = plt.colorbar(im, orientation = 'vertical')\ncbar.set_label(resplabel, rotation=270, labelpad=20)\ncbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\nadd_grid()\n\nplt.subplot(122)                                               # test data plot\nim = plt.scatter(X_test[sel_spred[0]],X_test[sel_spred[1]],s=None, c=y_test[resp[0]], marker=None, cmap=cmap, norm=None, vmin=ymin, \n                 vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\nplt.title('Test ' + resp[0] + ' vs. ' + sel_spred[1] + ' and ' + sel_spred[1]); plt.xlabel(sel_spredlabel[0]); plt.ylabel(sel_spredlabel[1])\nplt.xlim(-3,3); plt.ylim(-3,3)\ncbar = plt.colorbar(im, orientation = 'vertical')\ncbar.set_label(resplabel, rotation=270, labelpad=20)\ncbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\nadd_grid()\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nn_neighbours = 1; p = 2; weights = 'uniform'                 # model hyperparameters\nneigh = KNeighborsRegressor(weights = weights, n_neighbors=n_neighbours, p = p) # instantiate the prediction model \n```", "```py\nneigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data\n\nplt.subplot(221)                                              # training data vs. the model predictions\nZ = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,\n                    'Training Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(222)                                              # testing data vs. the model predictions\nvisualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,\n                'Testing Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\n# plt.subplot(223)                                              # model accuracy check\n# check_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,\n#             y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nn_neighbours = 5; p = 2; weights = 'uniform'                # model hyperparameters\nneigh = KNeighborsRegressor(weights = weights,n_neighbors=n_neighbours,p = p) # instantiate the prediction model\n\nneigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data\n\nplt.subplot(221)                                              # training data vs. the model predictions\nZ = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,\n                    'Training Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(222)                                              # testing data vs. the model predictions\nvisualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,\n                'Testing Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(223)                                              # model accuracy check\ncheck_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,\n            y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nn_neighbours = 1; p = 2; weights = 'uniform'                  # model hyperparameters\nneigh = KNeighborsRegressor(weights = weights,n_neighbors=n_neighbours,p = p) # instantiate the prediction model\n\nneigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data\n\nplt.subplot(221)                                              # training data vs. the model predictions\nZ = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,\n                    'Training Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(222)                                              # testing data vs. the model predictions\nvisualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,\n                'Testing Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(223)                                              # model accuracy check\ncheck_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,\n            y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nn_neighbours = 10; p = 1; weights = 'uniform'                 # model hyperparameters\nneigh = KNeighborsRegressor(weights = weights,n_neighbors=n_neighbours,p = p) # instantiate the prediction model\n\nneigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data\n\nplt.subplot(221)                                              # training data vs. the model predictions\nZ = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,\n                    'Training Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(222)                                              # testing data vs. the model predictions\nvisualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,\n                'Testing Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(223)                                              # model accuracy check\ncheck_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,\n            y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nk = 1                                                         # set initial, lowest k hyperparameter\ndist_error = []; unif_error = []; k_mat = []                  # make lists to store the results\nwhile k <= 150:                                               # loop over the k hyperparameter\n    neigh_dist = KNeighborsRegressor(weights = 'distance', n_neighbors=k, p = 2) # instantiate the model\n    neigh_dist_fit = neigh_dist.fit(X_train,y_train)          # train the model with the training data\n    y_pred = neigh_dist_fit.predict(X_test)                   # predict over the testing cases\n    MSE = metrics.mean_squared_error(y_test,y_pred)           # calculate the MSE testing\n    dist_error.append(MSE)                                    # add to the list of MSE\n\n    neigh_unif = KNeighborsRegressor(weights = 'uniform', n_neighbors=k, p = 2)\n    neigh_unif_fit = neigh_unif.fit(X_train,y_train)          # train the model with the training data\n    y_pred = neigh_unif_fit.predict(X_test)                   # predict over the testing cases\n    MSE = metrics.mean_squared_error(y_test,y_pred)           # calculate the MSE testing\n    unif_error.append(MSE)                                    # add to the list of MSE\n\n    k_mat.append(k)                                           # append k to an array for plotting\n    k = k + 1 \n```", "```py\nplt.subplot(111)\nplt.scatter(k_mat,dist_error,s=None, c='red',label = 'inverse distance weighted', marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\nplt.scatter(k_mat,unif_error,s=None, c='blue',label = 'arithmetic average', marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\nplt.title('Testing Error vs. Number of Nearest Neighbours'); plt.xlabel('Number of Nearest Neighbours'); plt.ylabel('Mean Square Error')\nplt.legend(); add_grid()\nplt.xlim(0,50); plt.ylim([0,750000])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8,wspace=0.15,hspace=0.2); plt.show() \n```", "```py\nscore = []                                                  # code modified from StackOverFlow by Dimosthenis\nk_mat = []\nfor k in range(1,150):\n    neigh_dist = KNeighborsRegressor(weights = 'distance', n_neighbors=k, p = 1)\n    scores = cross_val_score(estimator=neigh_dist, X= np.c_[df['sPor'],df['sBrittle']],y=df['Prod'], cv=4, n_jobs=4,\n                             scoring = \"neg_mean_squared_error\") # Perform 7-fold cross validation\n    score.append(abs(scores.mean()))\n    k_mat.append(k) \n```", "```py\nplt.figure(figsize=(8,6))\nplt.scatter(k_mat,score,s=None, c=\"red\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.5, edgecolors=\"black\")\nplt.title('k-fold Cross Validation Error (MSE) vs. k Nearest Neighbours'); plt.xlabel('Number of Nearest Neighbours'); plt.ylabel('Mean Square Error')\nplt.xlim(1,150); plt.ylim([0,1400000]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8,wspace=0.15,hspace=0.2); plt.show() \n```", "```py\nX_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(df.loc[:,sel_pred],df.loc[:,resp],test_size=0.25,random_state=73073)\n\nneigh = KNeighborsRegressor(weights = 'distance', n_neighbors=15, p = 1)\nneigh_fit = neigh.fit(X_train_orig,y_train_orig)                # train the model with the training data\n\nplt.subplot(121)                                              # training data vs. the model predictions\nZ = visualize_model(neigh_fit,X_train_orig[sel_pred[0]],Xmin[0],Xmax[0],X_train_orig[sel_pred[1]],Xmin[1],Xmax[1],y_train[resp[0]],ymin,ymax,\n                    'Training Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(122)                                              # testing data vs. the model predictions\nvisualize_model(neigh_fit,X_test_orig[sel_pred[0]],Xmin[0],Xmax[0],X_test_orig[sel_pred[1]],Xmin[1],Xmax[1],y_test[resp[0]],ymin,ymax,\n                'Testing Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nX_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(df.loc[:,sel_pred],df.loc[:,resp],test_size=0.25,random_state=73073)\n\nX_train_orig['Por'] = X_train_orig['Por']/100.0\nX_test_orig['Por'] = X_test_orig['Por']/100.0\n\nneigh = KNeighborsRegressor(weights = 'distance', n_neighbors=15, p = 1)\nneigh_fit = neigh.fit(X_train_orig,y_train_orig)                # train the model with the training data\n\nplt.subplot(121)                                              # training data vs. the model predictions\nZ = visualize_model(neigh_fit,X_train_orig[sel_pred[0]],Xmin[0]/100,Xmax[0]/100,X_train_orig[sel_pred[1]],Xmin[1],Xmax[1],y_train[resp[0]],ymin,ymax,\n                    'Training Data and k Nearest Neighbours',False)\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(122)                                              # testing data vs. the model predictions\nvisualize_model(neigh_fit,X_test_orig[sel_pred[0]],Xmin[0]/100,Xmax[0]/100,X_test_orig[sel_pred[1]],Xmin[1],Xmax[1],y_test[resp[0]],ymin,ymax,\n                'Testing Data and k Nearest Neighbours',False)\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nimport os                                                     # to set current working directory \n\nfolds = 4                                                   # number of k folds\nk_min = 1; k_max = 150                                       # range of k hyperparameter to consider\n\nX_pipe = df.loc[:,sel_pred]                                 # all the samples for the original features\ny_pipe = df.loc[:,resp[0]]                             # warning this becomes a series, 1D ndarray with label\n\npipe = Pipeline([                                           # the machine learning workflow as a pipeline object\n    ('scaler', StandardScaler()),\n    ('knear', KNeighborsRegressor())\n])\n\nparams = {                                                  # the machine learning workflow method's parameters\n    'scaler': [StandardScaler()],\n    'knear__n_neighbors': np.arange(k_min,k_max,1,dtype = int),\n    'knear__metric': ['euclidean'],\n    'knear__p': [2],\n    'knear__weights': ['distance']\n}\n\ngrid_cv_tuned = GridSearchCV(pipe, params, scoring = 'neg_mean_squared_error', # grid search cross validation \n                             cv=KFold(n_splits=folds,shuffle=False),\n                             refit = True)\ngrid_cv_tuned.fit(X_pipe,y_pipe)                                      # fit model with tuned hyperparameters\n\nplt.subplot(121)\nvisualize_tuned_model(grid_cv_tuned.best_params_['knear__n_neighbors'], # visualize the error vs. k \n                      grid_cv_tuned.cv_results_['param_knear__n_neighbors'],\n                      abs(grid_cv_tuned.cv_results_['mean_test_score']))              \n\nplt.subplot(122)                                            # visualize the tuned model\nvisualize_model(grid_cv_tuned,X[sel_pred[0]],Xmin[0],Xmax[0],X[sel_pred[1]],Xmin[1],Xmax[1],df[resp[0]],ymin,ymax,\n                'All Data and Tuned and Retrained k-Nearest Neighbours')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ngrid_cv_tuned.best_params_ \n```", "```py\n{'knear__metric': 'euclidean',\n 'knear__n_neighbors': 11,\n 'knear__p': 2,\n 'knear__weights': 'distance',\n 'scaler': StandardScaler()} \n```", "```py\ngrid_cv_tuned \n```", "```py\nGridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=False),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knear', KNeighborsRegressor())]),\n             param_grid={'knear__metric': ['euclidean'],\n                         'knear__n_neighbors': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n        40,  41,  42,  43,  44,  45,...\n        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 148, 149]),\n                         'knear__p': [2], 'knear__weights': ['distance'],\n                         'scaler': [StandardScaler()]},\n             scoring='neg_mean_squared_error')\n```", "```py\nGridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=False),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knear', KNeighborsRegressor())]),\n             param_grid={'knear__metric': ['euclidean'],\n                         'knear__n_neighbors': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n        40,  41,  42,  43,  44,  45,...\n        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 148, 149]),\n                         'knear__p': [2], 'knear__weights': ['distance'],\n                         'scaler': [StandardScaler()]},\n             scoring='neg_mean_squared_error')\n```", "```py\nPipeline(steps=[('scaler', StandardScaler()), ('knear', KNeighborsRegressor())])\n```", "```py\nStandardScaler()\n```", "```py\nKNeighborsRegressor()\n```", "```py\nimport numpy as np                                            # arrays and matrix math\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom scipy.ndimage import convolve1d\nseed = 73073                                                  # random number seed\n\nkr = 5; kloc = 45                                             # kernel radius, kernel location for example point\n\ndf_denpor = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/1D_por_perm_smooth.csv') # load data from Dr. Pyrcz's GitHub \n\nplt.subplot(121)                    \nplt.plot(df_denpor['Por'],df_denpor['Depth'],color='black',label='f(x)'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)')\nplt.ylim([100,0]); plt.xlim([2,22]); plt.title('Exhaustive Case: Original and Convolved Function'); plt.yticks(np.arange(100, -1, -5));\n\nsize = 2 * kr + 1                                             # make uniform kernel\nkernel = np.ones(size) / size                                 # normalize kernel to sum to one for unbiasedness\n\nconvolved = np.convolve(df_denpor['Por'].values, kernel, mode='same') # convolution\n\nk = len(kernel)\ntrim = k // 2  # how many values to trim from each edge\nconvolved_valid = convolved[trim:-trim] if trim > 0 else convolved\ndepth_valid = df_denpor['Depth'].values[trim:-trim] if trim > 0 else df_denpor['Depth'].values\nconvolved_pt = convolved_valid[np.abs(depth_valid - kloc).argmin()]; depth_pt = depth_valid[np.abs(depth_valid - kloc).argmin()]\nplt.plot(convolved_valid,depth_valid,color='red',label=r'$(f * g)(x)$'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)'); \nplt.ylim([100,0]); plt.xlim([2,22]); plt.legend(loc='lower left'); plt.plot([2,22],[depth_pt,depth_pt],color='red',ls='--')\nplt.scatter(convolved_pt,depth_pt,color='red',marker='o',edgecolor='black',zorder=10)\n\nplt.subplot(122)\nplt.plot(kernel,np.linspace(-1*kr+kloc,kr+kloc,2*kr+1),color='black',label=r'$g(\\tau)$'); plt.xlim(0.35,0.0); plt.ylim([100,0]); \nplt.yticks(np.arange(100, -1, -5)); plt.plot([kernel[0],0.0],[-1*kr+kloc,-1*kr+kloc],color='black'); \nplt.plot([kernel[0],0.0],[kr+kloc,kr+kloc],color='black'); plt.legend(loc='lower left'); plt.title('Kernel')\nplt.plot([0.35,0.0],[depth_pt,depth_pt],color='red',ls='--'); plt.ylabel('Depth (m)'); plt.xlabel('Weight (unitless)')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nimport astropy.convolution.convolve as convolve               # sparse data convolution\nfrac = 0.2\n\ndf_denpor['Por_Sparse'] = df_denpor['Por'].copy()\n\nnp.random.seed(seed = seed)\nnan_indices = np.random.choice(len(df_denpor), size=int(len(df_denpor)*(1.0-frac)), replace=False)\ndf_denpor.loc[nan_indices, 'Por_Sparse'] = np.nan\n\nplt.subplot(121)                    \nplt.scatter(df_denpor['Por_Sparse'],df_denpor['Depth'],color='black',label='f(x) Sparse'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)')\nplt.plot(df_denpor['Por'],df_denpor['Depth'],color='black',alpha=0.3,label='f(x) Exhaustive');\nplt.ylim([26,0]); plt.xlim([2,22]); plt.title('Sparse Case: Original and Convolved Function'); plt.yticks(np.arange(100, -1, -5));\n\nsize = 2 * kr + 1                                             # make uniform kernel\nkernel = np.ones(size) / size                                 # normalize kernel to sum to one for unbiasedness\n\nconvolved = convolve(df_denpor['Por_Sparse'].values,kernel,boundary='extend',nan_treatment='interpolate',normalize_kernel=True) # convolve\n\nk = len(kernel)\ntrim = k // 2  # how many values to trim from each edge\nconvolved_valid = convolved[trim:-trim] if trim > 0 else convolved\ndepth_valid = df_denpor['Depth'].values[trim:-trim] if trim > 0 else df_denpor['Depth'].values\nconvolved_pt = convolved_valid[np.abs(depth_valid - kloc).argmin()]; depth_pt = depth_valid[np.abs(depth_valid - kloc).argmin()]\nplt.plot(convolved_valid,depth_valid,color='red',label=r'$(f * g)(x)$'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)'); \nplt.ylim([100,0]); plt.xlim([2,22]); plt.legend(loc='lower left'); plt.plot([2,22],[depth_pt,depth_pt],color='red',ls='--')\nplt.scatter(convolved_pt,depth_pt,color='red',marker='o',edgecolor='black',zorder=10)\n\nplt.subplot(122)\nplt.plot(kernel,np.linspace(-1*kr+kloc,kr+kloc,2*kr+1),color='black',label=r'$g(\\tau)$'); plt.xlim(0.35,0.0); plt.ylim([100,0]); \nplt.yticks(np.arange(100, -1, -5)); plt.plot([kernel[0],0.0],[-1*kr+kloc,-1*kr+kloc],color='black'); \nplt.plot([kernel[0],0.0],[kr+kloc,kr+kloc],color='black'); plt.legend(loc='lower left'); plt.title('Kernel')\nplt.plot([0.35,0.0],[depth_pt,depth_pt],color='red',ls='--'); plt.ylabel('Depth (m)'); plt.xlabel('Weight (unitless)')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nWARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve] \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport pandas.plotting as pd_plot\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.preprocessing import StandardScaler              # standardize the features\nfrom sklearn.neighbors import KNeighborsRegressor             # for nearest k neighbours\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning\nfrom sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # suppress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef comma_format(x, pos):\n    return f'{int(x):,}'\n\ndef feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot\n    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1\n    plt.plot(pred,metric,color='black',zorder=20)\n    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)\n    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  \n    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)\n    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();\n    return\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    cbar = plt.colorbar(im, orientation = 'vertical')\n    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n\ndef visualize_model(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,title,axes_commas = True): # plots the data points and the decision tree prediction \n    n_classes = 10\n    cmap_temp = plt.cm.inferno\n    xplot_step = (x_max-x_min)/100; yplot_step = (y_max-y_min)/100\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step),\n                     np.arange(y_min, y_max, yplot_step))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=cmap_temp,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))\n\n    im = plt.scatter(xfeature,yfeature,s=30, c=response, marker=None, cmap=cmap_temp, norm=None, vmin=z_min, vmax=z_max, \n                     alpha=1.0, linewidths=0.8, edgecolors=\"black\",zorder=10)\n    plt.scatter(xfeature,yfeature,s=60, c='white', marker=None, cmap=cmap_temp, norm=None, vmin=z_min, vmax=z_max, \n                     alpha=1.0, linewidths=0.8, edgecolors=None,zorder=8)\n    plt.title(title); plt.xlabel(xfeature.name); plt.ylabel(yfeature.name)\n    cbar = plt.colorbar(im, orientation = 'vertical'); cbar.set_label(response.name, rotation=270, labelpad=20)\n    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n    if axes_commas == True:\n        plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n        plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n    return Z\n\ndef visualize_tuned_model(k_tuned,k_mat,score_mat):\n    plt.scatter(k_mat,score_mat,s=10.0, c=\"red\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, \n                linewidths=0.5, edgecolors=\"black\")\n    plt.plot([k_tuned,k_tuned],[0,10000000],color='black',linestyle=(6, (2,3)),label='tuned',zorder=1)\n    plt.title('k-fold Cross Validation Error (MSE) vs. k Nearest Neighbours'); plt.xlabel('Number of Nearest Neighbours')\n    plt.ylabel('Mean Square Error')\n    plt.xlim(k_min,k_max); plt.ylim(0,np.max(score_mat))\n\ndef check_model(model,xtrain,ytrain,xtest,ytest,ymin,ymax,rtrain,rtest,title): # plots the estimated vs. the actual \n    predict_train = model.predict(np.c_[xtrain,ytrain])\n    predict_test = model.predict(np.c_[xtest,ytest])\n    plt.scatter(rtrain,predict_train,s=None, c='darkorange',marker=None, cmap=None, norm=None, vmin=None, vmax=None, \n                alpha=0.8, linewidths=0.8, edgecolors=\"black\",label='Train')\n    plt.scatter(rtest,predict_test,s=None, c='red',marker=None, cmap=None, norm=None, vmin=None, vmax=None, \n                alpha=0.8, linewidths=0.8, edgecolors=\"black\",label='Test')\n    plt.title(title); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')\n    plt.xlim(ymin,ymax); plt.ylim(ymin,ymax)\n    plt.arrow(ymin,ymin,ymax,ymax,width=0.02,color='black',head_length=0.0,head_width=0.0)\n    MSE_train = metrics.mean_squared_error(rtrain,predict_train)\n    Var_Explained_train = metrics.explained_variance_score(rtrain,predict_train)\n    cor_train = math.sqrt(metrics.r2_score(rtrain,predict_train))\n    MSE_test = metrics.mean_squared_error(rtest,predict_test)\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.annotate('Train MSE: ' + str(f'{(np.round(MSE_train,2)):,}'),[0.05*(ymax-ymin)+ymin,0.95*(ymax-ymin)+ymin]) \n    plt.annotate('Test MSE:  ' + str(f'{(np.round(MSE_test,2)):,}'),[0.05*(ymax-ymin)+ymin,0.90*(ymax-ymin)+ymin])\n    add_grid(); plt.legend(loc='lower right')\n    # print('Mean Squared Error on Training = ', round(MSE_test,2),', Variance Explained =', round(Var_Explained,2),'Cor =', round(cor,2))\n\ndef weighted_percentile(data, weights, perc):                 # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) \n    ix = np.argsort(data)\n    data = data[ix] \n    weights = weights[ix] \n    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) \n    return np.interp(perc, cdf, data)\n\ndef histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram \n    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)\n    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')\n    plt.plot([avg,avg],[0.0,45],color = color)\n    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\ndf_load = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub \ndf_load = df_load.iloc[:,1:8]                                 # copy all rows and columns 1 through 8, note 0 column is removed\n\nresponse = 'Prod'                                             # specify the response feature\nadd_noise = True                                              # set True to add noise to response feature to demonstrate overfit\nnoise_stdev = 500                                             # amount of noise to add to response feature to demonstrate overfit\n\nnp.random.seed(seed = seed)                                   # set the random number seed\nif add_noise == True:\n    df_load[response] = df_load[response] + np.random.normal(loc=0.0,scale=noise_stdev,size = len(df_load))\n\nX = df_load.copy(deep = False)\nX = X.drop([response],axis='columns')                         # make predictor and response DataFrames\ny = df_load.loc[:,response]\n\nfeatures = X.columns.values.tolist() + [y.name]               # store the names of the features\npred = X.columns.values.tolist()\nresp = [y.name]\n\nXmin = [6.0,0.0,1.0,10.0,0.0,0.9]; Xmax = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minumum and maximum values for plotting\nymin = 1000.0; ymax = 9000.0\n\npredlabel = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\nresplabel = 'Normalized Initial Production (MCFPD)'\n\npredtitle = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n             'Total Organic Carbon','Vitrinite Reflectance']\nresptitle = 'Initial Production'\n\nfeaturelabel = predlabel + [resplabel]                        # make feature labels and titles for concise code\nfeaturetitle = predtitle + [resptitle]\n\nm = len(pred) + 1\nmpred = len(pred)\n\ndf = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) \n```", "```py\ndf.head(n=13) \n```", "```py\ndf.describe().transpose()                                     # calculate summary statistics for the data \n```", "```py\nnum = df._get_numeric_data()                                  # get the numerical values\nnum[num < 0] = 0                                              # truncate negative values to 0.0\ndf.describe().transpose()                                     # calculate summary statistics for the data \n```", "```py\ncorr_matrix = df.corr()\ncorrelation = corr_matrix.iloc[:,-1].values[:-1]\n\nplt.subplot(121)\nplot_corr(corr_matrix,'Correlation Matrix',1.0,0.5)           # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(pred,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp[0],'Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npairgrid = sns.PairGrid(df,vars=['Por','Perm','AI','Brittle','TOC','Prod']) # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nif1 = 0; if2 = 3                                              # selected predictor features\n\ntransform = StandardScaler();                                 # instantiate feature standardization method\n\nsel_pred = [pred[if1],pred[if2]]\nsel_features = pred + [resp]\n\nspredlabel = ['Standardized ' + element for element in predlabel] # standardized predictors list\n\nsel_spredlabel = [spredlabel[if1]] + [spredlabel[if2]] \n\nsel_spred = ['s' + element for element in sel_pred]           # standardized predictors list\n\ndf[sel_spred[0]] = transform.fit_transform(df.loc[:,sel_pred].values)[:,0] # standardize the data features to mean = 0, var = 1.0\ndf[sel_spred[1]] = transform.fit_transform(df.loc[:,sel_pred].values)[:,1] # standardize the data features to mean = 0, var = 1.0\n\nprint('Selected Predictor Features: ' + str(sel_pred))\nprint('Standardized Selected Predictor Features: ' + str(sel_spred))\nprint('Response Feature: ' + str([resp]))\ndf.head() \n```", "```py\nSelected Predictor Features: ['Por', 'Brittle']\nStandardized Selected Predictor Features: ['sPor', 'sBrittle']\nResponse Feature: [['Prod']] \n```", "```py\nprint('Backtransformed: \\n Por    Brittle')\ntransform.inverse_transform(df.loc[:,sel_spred])[:5,:]        # check the reverse standardization \n```", "```py\nBacktransformed: \n        Por    Brittle \n```", "```py\narray([[12.08, 81.4 ],\n       [12.38, 46.17],\n       [14.02, 72.8 ],\n       [17.67, 39.81],\n       [17.52, 10.94]]) \n```", "```py\nXmin = [5.0, 0.0]; Xmax = [25.0,100.0]                        # selected predictor features min and max \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(df.loc[:,sel_spred],df.loc[:,resp],test_size=0.25,random_state=73073)\ndf_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)\ndf_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(221)\nplt.hist(X_train[sel_spred[0]],alpha = 0.8,color = 'darkorange',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Train')\nplt.hist(X_test[sel_spred[0]],alpha = 0.8,color = 'red',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Test')\nplt.title(sel_spred[0]); plt.xlim(-3,3); plt.xlabel(sel_spredlabel[0]); plt.ylabel('Frequency'); add_grid(); plt.legend(loc='upper right')\n\nplt.subplot(222)\nplt.hist(X_train[sel_spred[1]],alpha = 0.8,color = 'darkorange',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Train')\nplt.hist(X_test[sel_spred[1]],alpha = 0.8,color = 'red',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Test')\nplt.title(sel_spred[1]); plt.xlim(-3,3); plt.xlabel(sel_spredlabel[1]); plt.ylabel('Frequency'); add_grid(); plt.legend(loc='upper right')\n\nplt.subplot(223)\nplt.hist(y_train[resp],alpha = 0.8,color = 'darkorange',edgecolor = 'black',bins=np.linspace(ymin,ymax,nbins),label='Train')\nplt.hist(y_test[resp],alpha = 0.8,color = 'red',edgecolor = 'black',bins=np.linspace(ymin,ymax,nbins),label='Test')\nplt.legend(loc='upper right'); plt.title(resp[0]); plt.xlim(ymin,ymax) \nplt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.xlabel(resplabel); plt.ylabel('Frequency'); add_grid()\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nplt.subplot(121)                                              # train data plot\nim = plt.scatter(X_train[sel_spred[0]],X_train[sel_spred[1]],s=None, c=y_train[resp[0]], marker=None, cmap=cmap, norm=None, vmin=ymin, \n                 vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\nplt.title('Train ' + resp[0] + ' vs. ' + sel_spred[1] + ' and ' + sel_spred[1]); plt.xlabel(sel_spredlabel[0]); plt.ylabel(sel_spredlabel[1])\nplt.xlim(-3,3); plt.ylim(-3,3)\ncbar = plt.colorbar(im, orientation = 'vertical')\ncbar.set_label(resplabel, rotation=270, labelpad=20)\ncbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\nadd_grid()\n\nplt.subplot(122)                                               # test data plot\nim = plt.scatter(X_test[sel_spred[0]],X_test[sel_spred[1]],s=None, c=y_test[resp[0]], marker=None, cmap=cmap, norm=None, vmin=ymin, \n                 vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\nplt.title('Test ' + resp[0] + ' vs. ' + sel_spred[1] + ' and ' + sel_spred[1]); plt.xlabel(sel_spredlabel[0]); plt.ylabel(sel_spredlabel[1])\nplt.xlim(-3,3); plt.ylim(-3,3)\ncbar = plt.colorbar(im, orientation = 'vertical')\ncbar.set_label(resplabel, rotation=270, labelpad=20)\ncbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\nadd_grid()\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nn_neighbours = 1; p = 2; weights = 'uniform'                 # model hyperparameters\nneigh = KNeighborsRegressor(weights = weights, n_neighbors=n_neighbours, p = p) # instantiate the prediction model \n```", "```py\nneigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data\n\nplt.subplot(221)                                              # training data vs. the model predictions\nZ = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,\n                    'Training Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(222)                                              # testing data vs. the model predictions\nvisualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,\n                'Testing Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\n# plt.subplot(223)                                              # model accuracy check\n# check_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,\n#             y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nn_neighbours = 5; p = 2; weights = 'uniform'                # model hyperparameters\nneigh = KNeighborsRegressor(weights = weights,n_neighbors=n_neighbours,p = p) # instantiate the prediction model\n\nneigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data\n\nplt.subplot(221)                                              # training data vs. the model predictions\nZ = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,\n                    'Training Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(222)                                              # testing data vs. the model predictions\nvisualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,\n                'Testing Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(223)                                              # model accuracy check\ncheck_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,\n            y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nn_neighbours = 1; p = 2; weights = 'uniform'                  # model hyperparameters\nneigh = KNeighborsRegressor(weights = weights,n_neighbors=n_neighbours,p = p) # instantiate the prediction model\n\nneigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data\n\nplt.subplot(221)                                              # training data vs. the model predictions\nZ = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,\n                    'Training Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(222)                                              # testing data vs. the model predictions\nvisualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,\n                'Testing Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(223)                                              # model accuracy check\ncheck_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,\n            y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nn_neighbours = 10; p = 1; weights = 'uniform'                 # model hyperparameters\nneigh = KNeighborsRegressor(weights = weights,n_neighbors=n_neighbours,p = p) # instantiate the prediction model\n\nneigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data\n\nplt.subplot(221)                                              # training data vs. the model predictions\nZ = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,\n                    'Training Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(222)                                              # testing data vs. the model predictions\nvisualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,\n                'Testing Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(223)                                              # model accuracy check\ncheck_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,\n            y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nk = 1                                                         # set initial, lowest k hyperparameter\ndist_error = []; unif_error = []; k_mat = []                  # make lists to store the results\nwhile k <= 150:                                               # loop over the k hyperparameter\n    neigh_dist = KNeighborsRegressor(weights = 'distance', n_neighbors=k, p = 2) # instantiate the model\n    neigh_dist_fit = neigh_dist.fit(X_train,y_train)          # train the model with the training data\n    y_pred = neigh_dist_fit.predict(X_test)                   # predict over the testing cases\n    MSE = metrics.mean_squared_error(y_test,y_pred)           # calculate the MSE testing\n    dist_error.append(MSE)                                    # add to the list of MSE\n\n    neigh_unif = KNeighborsRegressor(weights = 'uniform', n_neighbors=k, p = 2)\n    neigh_unif_fit = neigh_unif.fit(X_train,y_train)          # train the model with the training data\n    y_pred = neigh_unif_fit.predict(X_test)                   # predict over the testing cases\n    MSE = metrics.mean_squared_error(y_test,y_pred)           # calculate the MSE testing\n    unif_error.append(MSE)                                    # add to the list of MSE\n\n    k_mat.append(k)                                           # append k to an array for plotting\n    k = k + 1 \n```", "```py\nplt.subplot(111)\nplt.scatter(k_mat,dist_error,s=None, c='red',label = 'inverse distance weighted', marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\nplt.scatter(k_mat,unif_error,s=None, c='blue',label = 'arithmetic average', marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\nplt.title('Testing Error vs. Number of Nearest Neighbours'); plt.xlabel('Number of Nearest Neighbours'); plt.ylabel('Mean Square Error')\nplt.legend(); add_grid()\nplt.xlim(0,50); plt.ylim([0,750000])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8,wspace=0.15,hspace=0.2); plt.show() \n```", "```py\nscore = []                                                  # code modified from StackOverFlow by Dimosthenis\nk_mat = []\nfor k in range(1,150):\n    neigh_dist = KNeighborsRegressor(weights = 'distance', n_neighbors=k, p = 1)\n    scores = cross_val_score(estimator=neigh_dist, X= np.c_[df['sPor'],df['sBrittle']],y=df['Prod'], cv=4, n_jobs=4,\n                             scoring = \"neg_mean_squared_error\") # Perform 7-fold cross validation\n    score.append(abs(scores.mean()))\n    k_mat.append(k) \n```", "```py\nplt.figure(figsize=(8,6))\nplt.scatter(k_mat,score,s=None, c=\"red\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.5, edgecolors=\"black\")\nplt.title('k-fold Cross Validation Error (MSE) vs. k Nearest Neighbours'); plt.xlabel('Number of Nearest Neighbours'); plt.ylabel('Mean Square Error')\nplt.xlim(1,150); plt.ylim([0,1400000]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8,wspace=0.15,hspace=0.2); plt.show() \n```", "```py\nX_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(df.loc[:,sel_pred],df.loc[:,resp],test_size=0.25,random_state=73073)\n\nneigh = KNeighborsRegressor(weights = 'distance', n_neighbors=15, p = 1)\nneigh_fit = neigh.fit(X_train_orig,y_train_orig)                # train the model with the training data\n\nplt.subplot(121)                                              # training data vs. the model predictions\nZ = visualize_model(neigh_fit,X_train_orig[sel_pred[0]],Xmin[0],Xmax[0],X_train_orig[sel_pred[1]],Xmin[1],Xmax[1],y_train[resp[0]],ymin,ymax,\n                    'Training Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(122)                                              # testing data vs. the model predictions\nvisualize_model(neigh_fit,X_test_orig[sel_pred[0]],Xmin[0],Xmax[0],X_test_orig[sel_pred[1]],Xmin[1],Xmax[1],y_test[resp[0]],ymin,ymax,\n                'Testing Data and k Nearest Neighbours')\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nX_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(df.loc[:,sel_pred],df.loc[:,resp],test_size=0.25,random_state=73073)\n\nX_train_orig['Por'] = X_train_orig['Por']/100.0\nX_test_orig['Por'] = X_test_orig['Por']/100.0\n\nneigh = KNeighborsRegressor(weights = 'distance', n_neighbors=15, p = 1)\nneigh_fit = neigh.fit(X_train_orig,y_train_orig)                # train the model with the training data\n\nplt.subplot(121)                                              # training data vs. the model predictions\nZ = visualize_model(neigh_fit,X_train_orig[sel_pred[0]],Xmin[0]/100,Xmax[0]/100,X_train_orig[sel_pred[1]],Xmin[1],Xmax[1],y_train[resp[0]],ymin,ymax,\n                    'Training Data and k Nearest Neighbours',False)\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplot(122)                                              # testing data vs. the model predictions\nvisualize_model(neigh_fit,X_test_orig[sel_pred[0]],Xmin[0]/100,Xmax[0]/100,X_test_orig[sel_pred[1]],Xmin[1],Xmax[1],y_test[resp[0]],ymin,ymax,\n                'Testing Data and k Nearest Neighbours',False)\nplt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')\nplt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nimport os                                                     # to set current working directory \n\nfolds = 4                                                   # number of k folds\nk_min = 1; k_max = 150                                       # range of k hyperparameter to consider\n\nX_pipe = df.loc[:,sel_pred]                                 # all the samples for the original features\ny_pipe = df.loc[:,resp[0]]                             # warning this becomes a series, 1D ndarray with label\n\npipe = Pipeline([                                           # the machine learning workflow as a pipeline object\n    ('scaler', StandardScaler()),\n    ('knear', KNeighborsRegressor())\n])\n\nparams = {                                                  # the machine learning workflow method's parameters\n    'scaler': [StandardScaler()],\n    'knear__n_neighbors': np.arange(k_min,k_max,1,dtype = int),\n    'knear__metric': ['euclidean'],\n    'knear__p': [2],\n    'knear__weights': ['distance']\n}\n\ngrid_cv_tuned = GridSearchCV(pipe, params, scoring = 'neg_mean_squared_error', # grid search cross validation \n                             cv=KFold(n_splits=folds,shuffle=False),\n                             refit = True)\ngrid_cv_tuned.fit(X_pipe,y_pipe)                                      # fit model with tuned hyperparameters\n\nplt.subplot(121)\nvisualize_tuned_model(grid_cv_tuned.best_params_['knear__n_neighbors'], # visualize the error vs. k \n                      grid_cv_tuned.cv_results_['param_knear__n_neighbors'],\n                      abs(grid_cv_tuned.cv_results_['mean_test_score']))              \n\nplt.subplot(122)                                            # visualize the tuned model\nvisualize_model(grid_cv_tuned,X[sel_pred[0]],Xmin[0],Xmax[0],X[sel_pred[1]],Xmin[1],Xmax[1],df[resp[0]],ymin,ymax,\n                'All Data and Tuned and Retrained k-Nearest Neighbours')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ngrid_cv_tuned.best_params_ \n```", "```py\n{'knear__metric': 'euclidean',\n 'knear__n_neighbors': 11,\n 'knear__p': 2,\n 'knear__weights': 'distance',\n 'scaler': StandardScaler()} \n```", "```py\ngrid_cv_tuned \n```", "```py\nGridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=False),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knear', KNeighborsRegressor())]),\n             param_grid={'knear__metric': ['euclidean'],\n                         'knear__n_neighbors': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n        40,  41,  42,  43,  44,  45,...\n        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 148, 149]),\n                         'knear__p': [2], 'knear__weights': ['distance'],\n                         'scaler': [StandardScaler()]},\n             scoring='neg_mean_squared_error')\n```", "```py\nGridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=False),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knear', KNeighborsRegressor())]),\n             param_grid={'knear__metric': ['euclidean'],\n                         'knear__n_neighbors': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n        40,  41,  42,  43,  44,  45,...\n        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 148, 149]),\n                         'knear__p': [2], 'knear__weights': ['distance'],\n                         'scaler': [StandardScaler()]},\n             scoring='neg_mean_squared_error')\n```", "```py\nPipeline(steps=[('scaler', StandardScaler()), ('knear', KNeighborsRegressor())])\n```", "```py\nStandardScaler()\n```", "```py\nKNeighborsRegressor()\n```", "```py\nimport os                                                     # to set current working directory \n\nfolds = 4                                                   # number of k folds\nk_min = 1; k_max = 150                                       # range of k hyperparameter to consider\n\nX_pipe = df.loc[:,sel_pred]                                 # all the samples for the original features\ny_pipe = df.loc[:,resp[0]]                             # warning this becomes a series, 1D ndarray with label\n\npipe = Pipeline([                                           # the machine learning workflow as a pipeline object\n    ('scaler', StandardScaler()),\n    ('knear', KNeighborsRegressor())\n])\n\nparams = {                                                  # the machine learning workflow method's parameters\n    'scaler': [StandardScaler()],\n    'knear__n_neighbors': np.arange(k_min,k_max,1,dtype = int),\n    'knear__metric': ['euclidean'],\n    'knear__p': [2],\n    'knear__weights': ['distance']\n}\n\ngrid_cv_tuned = GridSearchCV(pipe, params, scoring = 'neg_mean_squared_error', # grid search cross validation \n                             cv=KFold(n_splits=folds,shuffle=False),\n                             refit = True)\ngrid_cv_tuned.fit(X_pipe,y_pipe)                                      # fit model with tuned hyperparameters\n\nplt.subplot(121)\nvisualize_tuned_model(grid_cv_tuned.best_params_['knear__n_neighbors'], # visualize the error vs. k \n                      grid_cv_tuned.cv_results_['param_knear__n_neighbors'],\n                      abs(grid_cv_tuned.cv_results_['mean_test_score']))              \n\nplt.subplot(122)                                            # visualize the tuned model\nvisualize_model(grid_cv_tuned,X[sel_pred[0]],Xmin[0],Xmax[0],X[sel_pred[1]],Xmin[1],Xmax[1],df[resp[0]],ymin,ymax,\n                'All Data and Tuned and Retrained k-Nearest Neighbours')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ngrid_cv_tuned.best_params_ \n```", "```py\n{'knear__metric': 'euclidean',\n 'knear__n_neighbors': 11,\n 'knear__p': 2,\n 'knear__weights': 'distance',\n 'scaler': StandardScaler()} \n```", "```py\ngrid_cv_tuned \n```", "```py\nGridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=False),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knear', KNeighborsRegressor())]),\n             param_grid={'knear__metric': ['euclidean'],\n                         'knear__n_neighbors': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n        40,  41,  42,  43,  44,  45,...\n        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 148, 149]),\n                         'knear__p': [2], 'knear__weights': ['distance'],\n                         'scaler': [StandardScaler()]},\n             scoring='neg_mean_squared_error')\n```", "```py\nGridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=False),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knear', KNeighborsRegressor())]),\n             param_grid={'knear__metric': ['euclidean'],\n                         'knear__n_neighbors': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n        40,  41,  42,  43,  44,  45,...\n        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 148, 149]),\n                         'knear__p': [2], 'knear__weights': ['distance'],\n                         'scaler': [StandardScaler()]},\n             scoring='neg_mean_squared_error')\n```", "```py\nPipeline(steps=[('scaler', StandardScaler()), ('knear', KNeighborsRegressor())])\n```", "```py\nStandardScaler()\n```", "```py\nKNeighborsRegressor()\n```"]