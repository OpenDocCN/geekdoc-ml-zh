- en: 8  Domain Knowledge
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 领域知识
- en: 原文：[https://ml-science-book.com/domain.html](https://ml-science-book.com/domain.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://ml-science-book.com/domain.html](https://ml-science-book.com/domain.html)
- en: '[Integrating Machine Learning Into Science](./part-two.html)'
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[将机器学习融入科学](./part-two.html)'
- en: '[8  Domain Knowledge](./domain.html)'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8 领域知识](./domain.html)'
- en: Machine learning algorithms learn models from data without needing much input.
    At least compared to other modeling approaches. But what about all the domain
    knowledge that you already have? Having models learn from data seems like the
    opposite of using domain knowledge – you let an algorithm identify the relevant
    patterns and differentiate between signal and noise.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法从数据中学习模型，而不需要太多的输入。至少与其他建模方法相比是这样。但关于你已有的所有领域知识怎么办呢？让模型从数据中学习似乎与使用领域知识相反——你让一个算法识别相关模式并区分信号和噪声。
- en: '*Domain knowledge* *Domain knowledge refers to facts that have been established
    in a field.*  *Can you use domain knowledge to guide the model? This chapter shows
    that you can, from the standard steps of translating the problem into a prediction
    task to more creative means such as designing custom evaluation metrics. The chapter
    focuses on ways to infuse domain knowledge directly into the model. Less well-known
    is that you can extract domain knowledge from the model: Domain knowledge constrains
    the model and affects its performance, which gives us information on how valuable
    the domain knowledge is for this prediction task. This makes machine learning
    a two-way street between domain knowledge and predictive performance.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*领域知识* 领域知识指的是在某个领域中已经确立的事实。*你能使用领域知识来指导模型吗？这一章节展示了你可以的，从将问题转化为预测任务的标准步骤到更富有创造性的方法，例如设计定制的评估指标。章节重点介绍了将领域知识直接注入模型的方法。不太为人所知的是，你还可以从模型中提取领域知识：领域知识限制了模型并影响其性能，这为我们提供了关于领域知识对于这一预测任务价值的信息。这使得机器学习成为领域知识和预测性能之间的双向通道。'
- en: 'Krarah and Rattle brainstormed about the problems with the tornado prediction
    system. Rattle had tunnel vision and looked only for reasons in her theory of
    generalization. Krarah shouted in frustration: “You’re not even using the knowledge
    about tornadoes, which we’ve accumulated over centuries. No wonder it doesn’t
    work!” This call from reality was sorely needed for Rattle. But how could she
    integrate domain knowledge into machine learning?'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Krarah和Rattle就龙卷风预测系统的问题进行了头脑风暴。Rattle有隧道视野，只在她的一般化理论中寻找原因。Krarah愤怒地喊道：“你甚至没有使用我们几个世纪以来积累的关于龙卷风的知识。难怪它不起作用！”这个来自现实世界的呼吁对Rattle来说非常迫切。但她如何将领域知识整合到机器学习中呢？
- en: '![](../Images/0870d97a75f46dc35663ec3021fc4309.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0870d97a75f46dc35663ec3021fc4309.png)'
- en: 8.1 Translate the question into a prediction task
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 将问题转化为预测任务
- en: The Ravens were wrong. Machine learning models are typically infused with lots
    of domain knowledge already, even when it may seem otherwise at first. Most domain
    knowledge comes from “translating” the scientific question into the prediction
    task. Predicting tornadoes from radar data and not from the number of people reporting
    knee aches might seem common sense. But as soon as you get into the details of
    designing the prediction task, you typically rely on deeper domain knowledge.
    Designing the prediction task might not always feel like infusing domain knowledge,
    especially when the prediction task is long established, like in weather forecasting.
    But for scientific problems that aren’t typically set up as prediction tasks,
    the translation process allows for a lot of creativity, which should be guided
    by domain knowledge.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 红雀的观点是错误的。机器学习模型通常已经融入了大量的领域知识，即使一开始看起来可能并非如此。大多数领域知识来自于“翻译”科学问题为预测任务。从雷达数据预测龙卷风而不是从报告膝盖疼痛的人数中预测，这似乎是常识。但一旦你深入到设计预测任务的细节中，你通常需要更深入的领域知识。设计预测任务可能并不总是感觉像是在注入领域知识，尤其是当预测任务已经长期确立，如天气预报。但对于那些通常不设定为预测任务的科学问题，翻译过程允许有很大的创造性，这应该由领域知识来引导。
- en: A big part of the translation is creating features and targets. Whether you
    predict intelligence from the income of the parents or chocolate consumption [[1]](references.html#ref-maurage2013does)
    makes for very different models. But if you are a typical reader of this book,
    you already know these things. Time to get to the juicy and often overlooked means
    to infuse domain knowledge.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译的大部分工作在于创建特征和目标。无论你是从父母的收入预测智力还是预测巧克力消费 [[1]](references.html#ref-maurage2013does)，都会形成非常不同的模型。但如果你是这本书的典型读者，你已经知道这些事情。现在是时候深入了解那些有趣且经常被忽视的将领域知识融入学习过程的方法了。
- en: 8.2 Constrain the model
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 约束模型
- en: 'Machine learning is praised for the flexibility to learn any function, but
    sometimes you might want the opposite and constrain the model. Model constraints
    are a direct way to incorporate domain knowledge into the learning process. Some
    examples of constraints that can be put on the relation between features and the
    prediction:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习因其能够学习任何函数的灵活性而受到赞誉，但有时你可能想要相反，并约束模型。模型约束是直接将领域知识融入学习过程的一种方式。以下是一些可以施加在特征与预测之间关系上的约束示例：
- en: '**Monotonicity:** The model’s predictions have to increase or decrease monotonically
    with increasing values of the feature in question. Example: Ensuring credit risk
    scores decrease monotonically with income.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单调性:** 模型的预测必须随着所讨论的特征值的增加而单调增加或减少。例如：确保信用风险评分随着收入的增加而单调减少。'
- en: '**Linearity:** Restrict the model to learn a linear relation between prediction
    and feature(s). Example: Predicting rent, where the price is linearly dependent
    on the size of the house.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性:** 限制模型学习预测与特征（们）之间的线性关系。例如：预测租金，其中价格与房屋大小呈线性相关。'
- en: '**Sparsity:** Restrict the number of features to be used for the prediction.
    Example: In genomic studies, identifying a small subset of genes responsible for
    a particular disease.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏性:** 限制用于预测的特征数量。例如：在基因组研究中，识别出负责特定疾病的小部分基因。'
- en: '**Smoothness:** Restrict the predictions to smoothly change when the input
    changes. Example: Predicting temperature as a function of altitude, where abrupt
    changes are not expected.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平滑性:** 限制预测在输入变化时平滑变化。例如：预测温度作为海拔的函数，其中不期望有突然的变化。'
- en: '**Cyclicity:** A smoothness constraint that ensures that the predictions for
    the low and high end of a feature “meet”. Example: Predicting electricity demand
    based on the time of the day, considering the cyclical nature of a day, including
    a smooth transition from 23:59 to 00:00.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期性:** 一种平滑性约束，确保特征的低值和高值预测“相遇”。例如：根据一天中的时间预测电力需求，考虑到一天的周期性，包括从23:59到00:00的平滑过渡。'
- en: '**Range constraints:** Restricting the domain of the predictions. For example,
    pH levels of a solution can only be between 0 and 14.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**范围约束:** 限制预测的域。例如，溶液的pH值只能在0到14之间。'
- en: 'How do you infuse these constraints into your models? How do you force a support
    vector machine to model one of the features linearly, but not all the other features?
    How do you make sure a random forest becomes sparse? Sometimes you can work with
    model-agnostic methods, such as feature pre-processing:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何将这些约束应用到你的模型中？你如何迫使支持向量机对某个特征进行线性建模，而对其他所有特征则不进行？你如何确保随机森林变得稀疏？有时你可以使用模型无关的方法，例如特征预处理：
- en: To obtain sparse models, use a feature selection step as pre-processing, followed
    by your usual machine learning pipeline.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要获得稀疏模型，使用特征选择步骤作为预处理，然后是你的常规机器学习流程。
- en: To model a feature cyclically, transform the feature into two features with
    sin and cos transformations [[2]](references.html#ref-ianlondonEncodingCyclicalContinuous2016).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要周期性地建模一个特征，将特征转换为具有正弦和余弦变换的两个特征 [[2]](references.html#ref-ianlondonEncodingCyclicalContinuous2016)。
- en: 'However, for other constraints, it is necessary to limit the model classes
    to ones that can handle the constraint. For instance, if you want your models
    to have monotonicity constraints for some features, you should only use model
    classes such as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于其他约束，有必要限制模型类别，使其能够处理约束。例如，如果你想对某些特征施加单调性约束，你应该只使用如下模型类别：
- en: Linear regression models.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归模型。
- en: XGBoost, a library for a gradient boosting algorithm, which allows setting a
    monotonicity constraint.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost，一个梯度提升算法的库，允许设置单调性约束。
- en: Neural networks with monotonicity constraints [[3]](references.html#ref-nguyenMonoNetInterpretableModels2019).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有单调性约束的神经网络 [[3]](references.html#ref-nguyenMonoNetInterpretableModels2019)。
- en: The more constraints you add, the smaller the pool of eligible model classes
    becomes. And the more specific the constraints, the more you depart from well-trodden
    paths, and you’ll find yourself using untested code from obscure machine learning
    publications instead of scikit-learn and other well-established machine learning
    libraries.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您添加的约束越多，合格的模型类别池就越小。约束越具体，您就越偏离了常见的路径，您可能会发现自己正在使用来自不为人知的机器学习出版物中的未经测试的代码，而不是scikit-learn和其他经过良好建立的机器学习库。
- en: 'If you find yourself wanting many model constraints that can’t be enforced
    in a model-agnostic way (e.g. feature engineering), you might want to look at
    frameworks and model families that are constraint-friendly, but also well-established:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您发现自己想要许多无法以模型无关的方式强制执行的模型约束（例如特征工程），您可能需要查看那些对约束友好的框架和模型家族，同时它们也是经过良好建立的：
- en: '**Model-based boosting** [[4]](references.html#ref-hofner2014model)**:** A
    gradient-boosting-based approach allowing you to pick constraints for each of
    the features.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的提升** [[4]](references.html#ref-hofner2014model)**：一种基于梯度提升的方法，允许您为每个特征选择约束条件。'
- en: '**Deep neural networks:** You can design many constraints through choices of
    the neural network architecture, loss function, and how the network is trained.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度神经网络**：您可以通过选择神经网络架构、损失函数以及网络训练方式来设计许多约束条件。'
- en: Regardless of whether you add constraints through feature engineering or restrict
    the model classes, ensure to also fit an unconstrained model. As stated in the
    beginning, domain knowledge doesn’t only flow in the direction of the model, but
    model performance may tell you something about the predictiveness of that domain
    knowledge. By comparing the performances of constrained and unconstrained models,
    you can evaluate the constraint. A decrease in predictive performance when adding
    constraints might suggest that the domain knowledge behind it isn’t as robust
    as initially thought (unless you can argue otherwise).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是通过特征工程添加约束还是限制模型类别，都确保也拟合一个无约束模型。正如开头所述，领域知识不仅流向模型，模型性能也可能告诉你关于该领域知识预测性的某些信息。通过比较有约束和无约束模型的性能，您可以评估约束。添加约束时预测性能的下降可能表明背后的领域知识不如最初想象的那么稳健（除非您能提出相反的论据）。
- en: 8.3 Design your performance metric
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 设计您的性能指标
- en: There’s a temptation to measure model performance with an off-the-shelf metric
    such as the mean squared error or the F1 metric. An off-the-shelf metric can be
    fine, but you might be better off designing a custom metric. The performance metric
    is an excellent, but often overlooked, way to incorporate domain knowledge into
    the model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种诱惑是使用现成的指标来衡量模型性能，例如均方误差或F1指标。现成的指标可能很好，但您可能设计一个定制的指标会更好。性能指标是结合领域知识的优秀方式，但常常被忽视。
- en: 'Poor modeling choices, lack of hyperparameter tuning, or bad features result
    in low predictive performance. That means the performance evaluation acts as a
    warning system. But if you’ve put an unsuitable warning system in place, there
    is no “meta-warning system” that will alert you. Except for reality, because at
    some point you realize that the model doesn’t fulfill its ulterior purpose. The
    lack of feedback makes it important to pick or design the right evaluation metric.
    Every metric makes judgments about how bad certain prediction errors are. For
    example, the mean squared error \(\frac{1}{n}\sum_{i=1}^{n}(y^{(i)} - \hat{f}(x^{(i)}))^2\)
    comes with the following judgments:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择不当、缺乏超参数调整或特征不佳会导致预测性能低下。这意味着性能评估充当了一个警告系统。但如果你已经建立了一个不合适的警告系统，就没有“元警告系统”会提醒你。除了现实之外，因为最终你会意识到模型没有达到其预期目的。缺乏反馈使得选择或设计正确的评估指标变得重要。每个指标都会对某些预测误差的严重性做出判断。例如，均方误差
    \(\frac{1}{n}\sum_{i=1}^{n}(y^{(i)} - \hat{f}(x^{(i)}))^2\) 包含以下判断：
- en: '**Symmetry:** Missing the outcome by -1 is as problematic as being off by +1.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对称性**：结果偏差为-1与偏差为+1一样有问题。'
- en: '**Equality:** Each data point’s prediction is equally important.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**等价性**：每个数据点的预测同等重要。'
- en: '**Non-linearity:** Being off by 2 is four times as bad as being off by 1.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非线性**：偏差为2比偏差为1糟糕四倍。'
- en: Designing these “judgments” is an opportunity for leveraging domain knowledge.
    You might weigh certain errors higher, weigh a subset of your data higher, or
    anticipate distribution shifts.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 设计这些“判断”是利用领域知识的机会。您可能会对某些错误赋予更高的权重，对数据子集赋予更高的权重，或者预测分布的变化。
- en: 'Let’s illustrate this with an example. Goschenhofer et al. [[5]](references.html#ref-goschenhofer2020wearable)
    developed a wearable-based Parkinson’s severity monitor that predicted disease
    severity (\(y\)) from severe slowing of movements (-4) to okay (0) to severe excessive
    movements (+4). Together with medical doctors, the authors identified three requirements
    for the performance metric: non-linearity, asymmetry, and lack of translation
    invariance. The metric should penalize larger errors more heavily. Particularly,
    errors with a sign change should be more costly. If the actual outcome is -1,
    a prediction of +1 is worse than -3 because getting the direction wrong is more
    harmful to the patient than an overestimation. Furthermore, diagnostic errors
    for patients with severe symptoms should have greater weight than errors for patients
    with mild symptoms. This is what the new performance metric looks like:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来说明这一点。Goschenhofer 等人 [[5]](references.html#ref-goschenhofer2020wearable)
    开发了一种基于可穿戴设备的帕金森病严重程度监测器，该监测器可以从严重动作缓慢（-4）预测疾病严重程度 (\(y\)) 到正常（0）再到严重过度动作（+4）。与医学医生合作，作者确定了性能指标的三项要求：非线性、非对称性和缺乏平移不变性。该指标应更严重地惩罚较大的误差。特别是，符号改变的误差应该更有代价。如果实际结果是
    -1，预测为 +1 比预测为 -3 更糟糕，因为方向错误对患者的危害比高估更大。此外，对于症状严重的患者的诊断错误应该比症状轻微的患者的错误有更大的权重。这就是新的性能指标的样子：
- en: \[\mathbb{P}(Y,X,\hat{f}) = \frac{1}{n}\sum_{i=1}^n\left[\frac{y^{(i)}}{4}\alpha
    + sign\left(y^{(i)} - \hat{f}(x^{(i)})\right)\right] \left(\hat{f}(x^{(i)}) -
    y^{(i)}\right)^2\]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbb{P}(Y,X,\hat{f}) = \frac{1}{n}\sum_{i=1}^n\left[\frac{y^{(i)}}{4}\alpha
    + sign\left(y^{(i)} - \hat{f}(x^{(i)})\right)\right] \left(\hat{f}(x^{(i)}) -
    y^{(i)}\right)^2\]
- en: At its core, it is the squared loss between model prediction and ground truth
    label but multiplied with a factor controlling over- and underestimation, steered
    by the parameter \(\alpha \in [-1,1]\), and a sign error penalty. This controls
    the symmetry (negative \(\alpha\) penalizes underestimation, positive \(\alpha\)
    penalizes overestimation, and 0 means symmetric loss). Based on feedback from
    medical experts, the authors chose \(\alpha=0.25\). By multiplying \(\alpha\)
    with the true label, the true label controls the direction of the asymmetric penalization.
    For example, classifying a true y=-1 as -3 “costs” 4, but if you classify it as
    +1, which is also 2 units apart, it costs +5\. That’s the custom “sign error”
    penalty of the metric. [Figure 8.1](#fig-custom-loss) visualizes the loss function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，它是模型预测与真实标签之间的平方损失，但乘以一个控制高估和低估的因子，由参数 \(\alpha \in [-1,1]\) 和符号错误惩罚控制。这控制了对称性（负
    \(\alpha\) 惩罚低估，正 \(\alpha\) 惩罚高估，0 表示对称损失）。根据医学专家的反馈，作者选择了 \(\alpha=0.25\)。通过将
    \(\alpha\) 与真实标签相乘，真实标签控制了非对称惩罚的方向。例如，将真实标签 y=-1 分类为 -3 “代价”为 4，但如果你将其分类为 +1，这也相距
    2 个单位，代价为 +5。这就是指标的自定义“符号错误”惩罚。[图 8.1](#fig-custom-loss) 可视化了损失函数。
- en: '![](../Images/232f9c4b3ed7e990d00cc85e35cfdd59.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/232f9c4b3ed7e990d00cc85e35cfdd59.png)'
- en: 'Figure 8.1: Custom Loss Function'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：自定义损失函数
- en: 8.4 Align loss and evaluation metric
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 对齐损失和评估指标
- en: 'Ideally, the metric you choose can be used as a loss function, the function
    that is directly optimized during the training. However, this might not always
    be possible:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你选择的指标可以用作损失函数，这是在训练过程中直接优化的函数。然而，这并不总是可能的：
- en: The performance metric might not be differentiable, but, for example, training
    neural networks requires a loss function with a gradient.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能指标可能不可微分，但例如，训练神经网络需要一个具有梯度的损失函数。
- en: Some model classes have a “built-in” loss function, like the linear regression
    model which optimizes the squared loss by default, or decision trees that optimize
    the Gini metric.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些模型类有“内置”的损失函数，例如默认优化平方损失的线性回归模型，或者优化 Gini 指标的决策树。
- en: The performance metric might be difficult to optimize because it is non-convex
    or infeasible to compute.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能指标可能难以优化，因为它可能不是凸的或者难以计算。
- en: The performance metric might only work on an aggregation of your data (e.g. F1
    score), but the model requires an instance-wise computable loss function.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能指标可能只适用于您数据的聚合（例如 F1 分数），但模型需要一个实例级可计算的损失函数。
- en: 'Here are some practical tips for working with loss functions and performance
    metrics:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于与损失函数和性能指标一起工作的实用技巧：
- en: If possible, design the performance metric so that it is convex and has a gradient.
    This way, you can use it directly as a loss function, at least for some model
    classes such as neural networks.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果可能的话，设计性能指标使其是凸的并且具有梯度。这样，你可以直接将其用作损失函数，至少对于一些模型类，如神经网络。
- en: 'If you can’t develop the perfect loss function, at least align the loss function
    with the performance metric: Do they at least have the same optimum? Do they model
    the same (dis)similarities? Do they both have the same scaling? Think linear versus
    squared.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你不能开发出完美的损失函数，至少使损失函数与性能指标保持一致：它们至少有相同的最佳值吗？它们是否模拟了相同的（不）相似性？它们是否都具有相同的缩放？考虑线性与平方的关系。
- en: Ensure that hyperparameter tuning and model selections are based on the performance
    metric and not the loss function.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保超参数调整和模型选择基于性能指标，而不是损失函数。
- en: If you have different options for loss functions, you can treat the loss as
    a hyperparameter.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有不同的损失函数选项，你可以将损失视为一个超参数。
- en: Even if the loss function optimized by some model class seems like a mismatch,
    train the model anyway and let the model selection process decide. In the case
    of a huge mismatch between loss and performance metrics, it should show up in
    the performance evaluation.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使某些模型类优化的损失函数看起来不匹配，也要训练模型，并让模型选择过程来决定。在损失与性能指标之间有巨大差异的情况下，它应该在性能评估中体现出来。
- en: 8.5 Make it physical
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 使其物理化
- en: The loss function is also a means to model physics. Many physical processes
    are guided by partial differential equations (PDEs). These equations are relevant
    in predicting the weather, simulating the climate, understanding how aircraft
    fly, and understanding fluid dynamics. However, simulating PDE-based systems can
    be computationally costly and slow. Wouldn’t it be great to use machine learning
    instead?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数也是建模物理的一种手段。许多物理过程都受偏微分方程（PDEs）的指导。这些方程在预测天气、模拟气候、理解飞机如何飞行和理解流体动力学方面都很有用。然而，基于PDE的系统模拟可能计算成本高且速度慢。使用机器学习不是更好吗？
- en: 'At first glance, these equations seem the opposite of what machine learning
    entails: They represent explicit physical laws, whereas neural networks are often
    seen as a mess of matrix multiplications. With physics-informed neural networks
    (PINNs) [[6]](references.html#ref-raissi2017physics), however, you can make a
    neural network learn physical laws/PDEs as well, making the neural network an
    emulator of a physical process. This leverages the property of neural networks
    being universal function approximators [[7]](references.html#ref-hornik1991approximation).
    The trick lies in using a loss function that minimizes a two-part MSE:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，这些方程似乎与机器学习的含义相反：它们代表显式的物理定律，而神经网络通常被视为矩阵乘法的混乱。然而，在物理信息神经网络（PINNs） [[6]](references.html#ref-raissi2017physics）中，你可以使神经网络学习物理定律/PDEs，使神经网络成为物理过程的模拟器。这利用了神经网络作为通用函数逼近器的特性
    [[7]](references.html#ref-hornik1991approximation)。技巧在于使用一个最小化两部分MSE的损失函数：
- en: \[MSE = MSE_u + MSE_{\hat{f}},\]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \[MSE = MSE_u + MSE_{\hat{f}},\]
- en: where \(MSE_u\) corresponds to the initial and boundary conditions of the system,
    and \(MSE_{\hat{f}}\) is the squared error between model output and the output
    of the original differential equations, based on collocation points, which are
    points in the domain where the PDE is enforced. Instead of purely emulating the
    process, physics-informed neural networks can also be used to discover the parameterization
    of PDEs from noisy measurements of the system [[8]](references.html#ref-raissi2017physicsa).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(MSE_u\) 对应于系统的初始条件和边界条件，而 \(MSE_{\hat{f}}\) 是基于配置点的模型输出与原始微分方程输出的平方误差，这些配置点是强加偏微分方程的域中的点。物理信息神经网络不仅可以通过模拟过程，还可以从系统的噪声测量中发现偏微分方程的参数化
    [[8]](references.html#ref-raissi2017physicsa)。
- en: Alternatively, instead of encoding PDEs via the loss function during training,
    you can also encode physics constraints directly into neural networks, resulting
    in what some refer to as Physics-encoded Neural Networks (PeNNs).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以在训练期间不通过损失函数编码偏微分方程，而是直接将物理约束编码到神经网络中，从而产生一些人称之为物理编码神经网络（PeNNs）。
- en: 8.6 Considering various evaluation metrics
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 考虑各种评估指标
- en: 'Supervised machine learning has a drawback: the evaluation is typically based
    on just one dimension, the performance metric. But life is never that simple!
    You might have multiple metrics that you care about. Fortunately, it is possible
    to train models that optimize different metrics, but it is not as simple as just
    having one metric.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习有一个缺点：评估通常只基于一个维度，即性能指标。但生活永远没有那么简单！你可能关心多个指标。幸运的是，可以训练优化不同指标的模型，但这并不像只有一个指标那么简单。
- en: Imagine you want to predict tornadoes in the next hour. We could have accuracy
    or F1 score as one metric, but there’s more to life than just being right all
    the time. You might also want to consider the complexity of the model, like how
    sparse it is, or even the average time it takes to make a prediction (because
    nobody likes waiting, especially if there’s a tornado looming).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你想要预测未来一小时的龙卷风。我们可以用准确率或F1分数作为一个指标，但生活不仅仅是总是正确。你可能还想考虑模型的复杂性，比如它有多稀疏，甚至预测平均所需时间（因为没有人喜欢等待，尤其是当龙卷风即将来临的时候）。
- en: 'There are two options to accomplish this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以实现这一点：
- en: Combine both metrics into one. This only works when you can quantify your desired
    trade-off between the two metrics.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这两个指标合并为一个。这只有在你可以量化两个指标之间所需的权衡时才有效。
- en: Use multi-objective optimization for hyperparameter tuning and model selection.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多目标优化进行超参数调整和模型选择。
- en: 'Here are a couple of metrics that you can optimize for, partially based on
    [[9]](references.html#ref-pfisterer2019multi):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '这里有一些你可以优化的指标，部分基于 [[9]](references.html#ref-pfisterer2019multi):'
- en: '**Predictive Performance**: Can be quantified using measures such as F1-Score,
    AUC, or mean absolute error.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测性能**：可以使用F1分数、AUC或平均绝对误差等指标来量化。'
- en: '**Interpretability** [[10]](references.html#ref-molnar2020quantifying): Metrics
    such as sparsity, interaction strength, and complexity of the main effects.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性** [[10]](references.html#ref-molnar2020quantifying)：如稀疏性、交互强度和主要效应的复杂性等指标。'
- en: '**Fairness**: Measured as disparate impact, equalized odds, or calibration.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平性**：通过差异影响、均衡机会或校准来衡量。'
- en: '**Robustness**: Measured via performance under perturbations, presence of adversarial
    examples, and under distribution shifts.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒性**：通过扰动下的性能、对抗样本的存在以及分布偏移来衡量。'
- en: '**Alignment with physics-based models** [[11]](references.html#ref-letzgus2023explainable)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与基于物理模型的契合度** [[11]](references.html#ref-letzgus2023explainable)'
- en: '**Inference time**'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理时间**'
- en: '**Memory requirements**'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存需求**'
- en: Let’s say you want to predict a disease outcome using gene expression data.
    From domain knowledge, you know that only a few genes might be relevant. But if
    you only optimize for predictive performance, the model might pick up more genes
    than necessary if it doesn’t hurt the performance bottom line. By also optimizing
    for sparsity in this example, you can find a model that satisfies both performance
    and sparsity[¹](#fn1). We’ve already discussed constraints in a previous section,
    but multi-objective optimization offers a slightly different approach to constraints.
    Optimizing for multiple objectives produces multiple “optimal” models. Each model
    is optimal regarding a different trade-off between the objectives, also known
    as “Pareto-efficient” models. Having multiple models can be both a bug and a feature.
    It is a bug because you typically want just one model. It is a feature because
    the set of Pareto efficient models allows you to choose the trade-off between
    the metrics that make sense based on domain knowledge. And that’s easier than
    having to decide beforehand how to balance both performance metrics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要使用基因表达数据来预测疾病结果。从领域知识来看，你知道只有少数基因可能相关。但如果只优化预测性能，模型可能会选择比必要的更多基因，只要它不会损害性能底线。通过在这个例子中也优化稀疏性，你可以找到一个既满足性能又满足稀疏性的模型[¹](#fn1)。我们已经在前面章节讨论了约束，但多目标优化提供了一种稍微不同的约束方法。优化多个目标会产生多个“最优”模型。每个模型都是针对不同目标之间的权衡而最优的，也称为“帕累托有效”模型。拥有多个模型既可以是一个缺陷，也可以是一个特性。它是一个缺陷，因为你通常只想有一个模型。它是一个特性，因为帕累托有效模型的集合允许你根据领域知识选择有意义的指标之间的权衡。而且这比事先决定如何平衡两个性能指标要容易得多。
- en: 'Again, we can observe the two-way street: Whether you collapse the objective
    into one function or embrace the multiplicity of models with different trade-offs
    – multi-objective optimization will enable you to explore different trade-offs
    and therefore evaluate domain knowledge.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以观察到这种双向的道路：无论你是将目标归纳为一个函数，还是接受不同权衡的模型多样性——多目标优化将使你能够探索不同的权衡，从而评估领域知识。
- en: 8.7 Use the right inductive biases and learn from them
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 使用正确的归纳偏差并从中学习
- en: '*Inductive bias* *The prior assumptions a machine learning algorithm imposes
    on the model to help generalize to unseen data.*  *Imagine a machine learning
    algorithm that produces “models” that only memorize training data. To “predict”,
    this model would have to check whether the new data point is in the training data.
    If so, the model can return the outcome for the training data point. If not, it
    can’t make a prediction. But what if a training data point is almost identical
    to the new one? Could you just use the same prediction? The model could then just
    return the prediction of the nearest data point. Or, to make it robust, the average
    prediction of multiple close data points from the training data. That’s the idea
    behind K-nearest neighbors.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*归纳偏差* *机器学习算法对模型施加的先验假设，以帮助推广到未见数据。* *想象一个机器学习算法，它产生的“模型”只记住训练数据。为了“预测”，这个模型必须检查新数据点是否在训练数据中。如果是，模型可以返回训练数据点的结果。如果不是，它就不能做出预测。但如果一个训练数据点几乎与新数据点相同呢？你能直接使用相同的预测吗？那么模型就可以返回最近数据点的预测。或者，为了使其更稳健，可以返回训练数据中多个接近数据点的平均预测。这就是K-最近邻背后的想法。*'
- en: But you could also have a model that makes different assumptions about how to
    generalize to new data, see [Figure 8.2](#fig-inductive). If you get a new data
    point and one of the features is slightly different from one of the training data
    points, you could interpolate linearly between the neighboring data points for
    this new data point. If you do that for all features and training data points,
    then you end up with a linear regression model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但你也可以有一个对如何推广到新数据有不同的假设的模型，见[图8.2](#fig-inductive)。如果你得到一个新的数据点，其中一个特征与训练数据点中的一个特征略有不同，你可以对这个新数据点之间的邻近数据点进行线性插值。如果你对所有特征和训练数据点都这样做，那么你最终会得到一个线性回归模型。
- en: '![](../Images/817947b21bd6d878c8d498ef7628f009.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/817947b21bd6d878c8d498ef7628f009.png)'
- en: 'Figure 8.2: Same data, different inductive biases.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：相同的数据，不同的归纳偏差。
- en: 'Or you could use another strategy: Divide your data into partitions which are
    defined by binary decisions in the features. The partitions are chosen so that
    the training data points within have a similar outcome. This comes with a beautiful
    generalization strategy: For each new data point, you just have to check in which
    data partition it falls, based on the feature values, and then take the average
    outcome of all the training data points as a prediction. This is the strategy
    that decision trees use.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以采用另一种策略：将你的数据划分为由特征中的二进制决策定义的分区。这些分区被选择，使得训练数据点具有相似的结果。这带来了一种美丽的推广策略：对于每个新的数据点，你只需根据特征值检查它属于哪个数据分区，然后取所有训练数据点的平均结果作为预测。这就是决策树使用的策略。
- en: All these examples show that each model class, like trees, k-nearest-neighbors,
    and linear models, comes with different inductive biases. The inductive bias is
    an instruction on how to generalize the relation between inputs and outputs to
    unseen data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些例子都表明，每个模型类别，如树、k-最近邻和线性模型，都带有不同的归纳偏差。归纳偏差是关于如何将输入和输出之间的关系推广到未见数据的一条指令。
- en: Try out models with different inductive biases. If a certain inductive bias
    seems to stand out, study how it relates to the studied phenomenon. Can you learn
    about your problem at hand?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试具有不同归纳偏差的模型。如果某个归纳偏差似乎突出，研究它如何与研究的现象相关。你能了解你手头的问题吗？
- en: A way to understand inductive biases better is by using tools from interpretable
    machine learning (see [Chapter 9](interpretability.html)). Interpretable machine
    learning can also be used to extract insights from the models. Domain knowledge
    often comes in the form of causal knowledge (see [Chapter 10](causality.html)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地理解归纳偏差，可以使用可解释机器学习工具（见第9章）。可解释机器学习还可以用于从模型中提取见解。领域知识通常以因果知识的形式出现（见第10章）。
- en: '[1]P. Maurage, A. Heeren, and M. Pesenti, “Does chocolate consumption really
    boost nobel award chances? The peril of over-interpreting correlations in health
    studies,” *The Journal of Nutrition*, vol. 143, no. 6, pp. 931–933, 2013, doi:
    [10.3945/jn.113.174813](https://doi.org/10.3945/jn.113.174813).[2]I. London, “Encoding
    cyclical continuous features - 24-hour time,” *Ian London’s Blog*. Jul. 2016\.
    Accessed: Sep. 27, 2023\. [Online]. Available: [https://ianlondon.github.io/posts/encoding-cyclical-features-24-hour-time/](https://ianlondon.github.io/posts/encoding-cyclical-features-24-hour-time/)[3]A.
    Nguyen and M. R. Martı́nez, “MonoNet: Towards Interpretable Models by Learning
    Monotonic Features.” arXiv, Sep. 2019\. doi: [10.48550/arXiv.1909.13611](https://doi.org/10.48550/arXiv.1909.13611).[4]B.
    Hofner, A. Mayr, N. Robinzonov, and M. Schmid, “Model-based boosting in r: A hands-on
    tutorial using the r package mboost,” *Computational statistics*, vol. 29, pp.
    3–35, 2014, doi: [10.1007/s00180-012-0382-5](https://doi.org/10.1007/s00180-012-0382-5).[5]J.
    Goschenhofer, F. M. Pfister, K. A. Yuksel, B. Bischl, U. Fietzek, and J. Thomas,
    “Wearable-based parkinson’s disease severity monitoring using deep learning,”
    in *Machine learning and knowledge discovery in databases: European conference,
    ECML PKDD 2019, würzburg, germany, september 16–20, 2019, proceedings, part III*,
    Springer, 2020, pp. 400–415\. doi: [10.1007/978-3-030-46133-1_24](https://doi.org/10.1007/978-3-030-46133-1_24).[6]M.
    Raissi, P. Perdikaris, and G. E. Karniadakis, “Physics Informed Deep Learning
    (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations.”
    arXiv, Nov. 2017\. doi: [10.48550/arXiv.1711.10561](https://doi.org/10.48550/arXiv.1711.10561).[7]K.
    Hornik, “Approximation capabilities of multilayer feedforward networks,” *Neural
    networks*, vol. 4, no. 2, pp. 251–257, 1991, doi: [10.1016/0893-6080(91)90009-T](https://doi.org/10.1016/0893-6080(91)90009-T).[8]M.
    Raissi, P. Perdikaris, and G. E. Karniadakis, “Physics Informed Deep Learning
    (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations.”
    arXiv, Nov. 2017\. doi: [10.48550/arXiv.1711.10566](https://doi.org/10.48550/arXiv.1711.10566).[9]F.
    Pfisterer, S. Coors, J. Thomas, and B. Bischl, “Multi-objective automatic machine
    learning with autoxgboostmc,” *arXiv preprint arXiv:1908.10796*, 2019, doi: [10.48550/arXiv.1908.10796](https://doi.org/10.48550/arXiv.1908.10796).[10]C.
    Molnar, G. Casalicchio, and B. Bischl, “Quantifying model complexity via functional
    decomposition for better post-hoc interpretability,” in *Machine learning and
    knowledge discovery in databases: International workshops of ECML PKDD 2019, würzburg,
    germany, september 16–20, 2019, proceedings, part i*, Springer, 2020, pp. 193–204\.
    doi: [10.1007/978-3-030-43823-4_17](https://doi.org/10.1007/978-3-030-43823-4_17).[11]S.
    Letzgus and K.-R. Müller, “An explainable AI framework for robust and transparent
    data-driven wind turbine power curve models,” *Energy and AI*, p. 100328, Dec.
    2023, doi: [10.1016/j.egyai.2023.100328](https://doi.org/10.1016/j.egyai.2023.100328).*
    ** * *'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Multi-objective optimization is not the only method to introduce sparsity into
    your model. Generally, regularization is employed, implying that only models with
    inherent sparsity, such as LASSO (a sparse linear model), are considered.[↩︎](#fnref1)**
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多目标优化并非是唯一将稀疏性引入模型的方法。通常，会采用正则化方法，这意味着只有具有固有稀疏性的模型，例如LASSO（一个稀疏线性模型），才会被考虑。[↩︎](#fnref1)**
