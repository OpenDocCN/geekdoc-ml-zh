["```py\n$ which python3\n/Users/parrt/anaconda3/bin/python3\n$ python3\nPython 3.6.5 |Anaconda custom (64-bit)| (default, Apr 26 2018, 08:42:37) \n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pandas as pd\n>>>\n```", "```py\n(C:\\Users\\parrt\\Anaconda3) C:\\Users\\parrt>python\nPython 3.6.5 |Anaconda custom (64-bit)| (default, Apr 26 2018, 08:42:37) \n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pandas as pd\n>>>\n```", "```py\n$ pip install rfpimp\n```", "```py\n$ cd data\n$ python prep-rent.py \nCreated rent.csv\nCreated rent-ideal.csv\n```", "```py\nimport pandas as pd # Import the library and give a short alias: pd\nrent = pd.read_csv(\"data/rent-ideal.csv\")\n\n```", "```py\nprint(rent.head(5))\n\n```", "```py\nprices = rent['price']\navg_rent = prices.mean()\nprint(f\"Average rent is ${avg_rent:.0f}\")\n\n```", "```py\nbybaths = rent.groupby(['bathrooms']).mean()\nbybaths = bybaths.reset_index() # overcome quirk in Pandas\nprint(bybaths[['bathrooms','price']]) # print just num baths, avg price\n\n```", "```py\nimport matplotlib.pyplot as plt\n\nbybaths.plot.line('bathrooms','price', style='-o')\nplt.show()\n```", "```py\nX, y = rent[['bedrooms','bathrooms','latitude','longitude']], rent['price']\n\n```", "```py\nprint(type(X), type(y))\n\n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=10) # use 10 trees\nrf.fit(X, y)\n\n```", "```py\nunknown_x = [2, 1, 40.7957, -73.97] # 2 bedrooms, 1 bathroom, ...\n\n```", "```py\npredicted_y = rf.predict([unknown_x])\nprint(predicted_y)\n\n```", "```py\nfrom sklearn.metrics import mean_absolute_error\n\npredictions = rf.predict(X)\ne = mean_absolute_error(y, predictions)\nep = e*100.0/y.mean()\nprint(f\"${e:.0f} average error; {ep:.2f}% error\")\n\n```", "```py\nX, y = rent[['latitude','longitude']], rent['price']\nrf = RandomForestRegressor(n_estimators=100)\nrf.fit(X, y)\nlocation_e = mean_absolute_error(y, rf.predict(X))\nlocation_ep = location_e*100.0/y.mean()\nprint(f\"${location_e:.0f} average error; {location_ep:.2f}% error\")\n\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX, y = rent[['bedrooms','bathrooms','latitude','longitude']], rent['price']\n# 20% of data goes into test set, 80% into training set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \n\nrf = RandomForestRegressor(n_estimators=10)\nrf.fit(X_train, y_train)\n\nvalidation_e = mean_absolute_error(y_test, rf.predict(X_test))\nprint(f\"${validation_e:.0f} average error; {validation_e*100.0/y.mean():.2f}% error\")\n\n```", "```py\nrf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train, y_train)\n\ne = mean_absolute_error(y_test, rf.predict(X_test))\nprint(f\"${e:.0f} average error; {e*100.0/y.mean():.2f}% error\")\n\n```", "```py\nfrom rfpimp import *\nrf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train, y_train)\nI = importances(rf, X_test, y_test)\nI\n```", "```py\nplot_importances(I, color='#4575b4', vscale=1.8)\n```", "```py\nI = importances(rf, X_test, y_test,\n                features=['bedrooms','bathrooms',['latitude','longitude']])\nplot_importances(I, color='#4575b4', vscale=1.8)\n```", "```py\nfrom sklearn.datasets import load_breast_cancer\nimport pandas as pd\n\ncancer = load_breast_cancer()\n\nX = cancer.data\ny = cancer.target\ndf = pd.DataFrame(X, columns=cancer.feature_names)\n\n```", "```py\nfeatures = ['radius error', 'texture error', 'concave points error',\n            'symmetry error', 'worst texture', 'worst smoothness',\n            'worst symmetry']\ndf = df[features] # select just these features\nprint(\"target[0:30] =\", y[0:30]) # show 30 values of malignant/benign target\ndf.head()\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.15)\n\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\n\ncl = RandomForestClassifier(n_estimators=300)\ncl.fit(X_train, y_train)\nvalidation_e = cl.score(X_test, y_test)\nprint(f\"{validation_e*100:.2f}% correct\")\n\n```", "```py\nfrom rfpimp import *\nI = importances(cl, X_test, y_test)\nplot_importances(I, color='#4575b4', vscale=1.4)\n```", "```py\nimport pandas as pd\naddr640 = pd.read_csv(\"data/640.csv\")\n\n```", "```py\nprint(addr640.digit.values)\naddr640 = addr640.drop('digit', axis=1) # drop digit column\n\n```", "```py\nsix_img_as_row = addr640.iloc[0].values  # digit '6' is first row\nimg28x28 = six_img_as_row.reshape(28,28) # unflatten as 2D array\nplt.imshow(img28x28, cmap='binary')\nplt.show()\n```", "```py\nsix_img_as_row[six_img_as_row>0] = 1  # convert 0..1 to 0 or 1\nsix_img_as_row = six_img_as_row.astype(int)\nimg28x28 = six_img_as_row.reshape(28,28)\ns = str(img28x28).replace(' ','')     # remove spaces\nprint(s)\n\n```", "```py\ndigits = pd.read_csv(\"data/mnist-10k-sample.csv\")\nimages = digits.drop('digit', axis=1) # get just pixels\ntargets = digits['digit']             # get just digit value\n\n```", "```py\nfig, axes = plt.subplots(10, 5, figsize=(4, 6.5)) # make 10x5 grid of plots\n\nfor i, ax in enumerate(axes.flat):\n    img_as_row = images.iloc[i].values\n    img28x28 = img_as_row.reshape(28,28)\n    ax.axis('off') # don't show x, y axes\n    ax.imshow(img28x28, cmap='binary')\n    ax.text(0, 8, targets[i], color='#313695', fontsize=18)\nplt.show()\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\n\ncl = RandomForestClassifier(n_estimators=900, n_jobs=-1)\ncl.fit(images, targets)\npred = cl.predict(addr640)\nprint(pred)\n```", "```py\nimport numpy as np;\nnp.set_printoptions(precision=3)\n\ndigit_values = range(10)\nprob = cl.predict_proba(addr640)\nprob_for_2nd_digit = prob[1]\nprint(prob_for_2nd_digit)\n\n```", "```py\npred_digit = np.argmax(prob_for_2nd_digit)\nprint(\"predicted digit is\", pred_digit)\n\n```", "```py\npred_digit = np.argmax(prob_for_2nd_digit)\nbars = plt.bar(digit_values, prob_for_2nd_digit, color='#4575b4')\nbars[pred_digit].set_color('#fdae61')\nplt.xlabel(\"predicted digit\")\nplt.xticks(digit_values)\nplt.ylabel(\"likelihood 2nd image\\nis a specific digit\")\nplt.show()\n```", "```py\nfours = images[targets==4] # find all \"4\" images\n\nfig, axes = plt.subplots(15, 8, figsize=(4,6.5))\nfor i, ax in enumerate(axes.flat):\n    img = fours.iloc[i,:].values.reshape(28,28)\n    ax.axis('off')\n    ax.imshow(img, cmap='binary')\n```", "```py\nX_train, X_test, y_train, y_test = \\\n    train_test_split(images, targets, test_size=.2)\n\ncl = RandomForestClassifier(n_estimators=900, n_jobs=-1)\ncl.fit(X_train, y_train)\nrfaccur = cl.score(X_test, y_test)\nprint(rfaccur)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n\n# create linear model\nlm = LogisticRegression(solver='newton-cg', multi_class='multinomial')\nlm.fit(X_train, y_train)\n\nlmaccur = lm.score(X_test, y_test)\nprint(lmaccur)\n```", "```py\nntrees = cl.n_estimators\nnnodes = sum([cl.estimators_[i].tree_.node_count for i in range(ntrees)])\nprint(f\"{nnodes:,}\") # print with commas\n\n```", "```py\nimport numpy as np\nX_train = np.array([1,2,3,4,5]).reshape(5,1) # 5 rows of 1 column\ny_train = np.array([1,2,3,4,5])              # 1 column\nX_test = np.array([6]).reshape(1,1)          # 1 row of 1 column\n\n```", "```py\nfrom sklearn.linear_model import LinearRegression   \nlm = LinearRegression()\nlm.fit(X_train,y_train)\nprint(\"y =\", lm.predict(X_test))\n\n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train,y_train)\nprint(\"y =\", rf.predict(X_test) )\n\n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n```", "```py\ns = m.score(X_test, y_test)    # measure performance\n```"]