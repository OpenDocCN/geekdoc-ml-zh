<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch041.xhtml</title>
  <style>
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    /* Figure formatting */
    .quarto-layout-panel>figure>figcaption,
    .quarto-layout-panel>.panel-caption {
      margin-top: 10pt;
    }

    .quarto-layout-row {
      display: flex;
      align-items: flex-start;
    }

    .quarto-layout-valign-top {
      align-items: flex-start;
    }

    .quarto-layout-valign-bottom {
      align-items: flex-end;
    }

    .quarto-layout-valign-center {
      align-items: center;
    }

    .quarto-layout-cell {
      position: relative;
      margin-right: 20px;
    }

    .quarto-layout-cell:last-child {
      margin-right: 0;
    }

    .quarto-layout-cell figure,
    .quarto-layout-cell>p {
      margin: 0.2em;
    }

    .quarto-layout-cell .html-widget {
      width: 100% !important;
    }

    .quarto-layout-cell div figure p {
      margin: 0;
    }

    .quarto-layout-cell figure {
      display: inline-block;
      margin-inline-start: 0;
      margin-inline-end: 0;
    }

    .quarto-layout-cell table {
      display: inline-table;
    }

    .quarto-layout-cell-subref figcaption {
      font-style: italic;
      text-align: center;
    }

    .quarto-figure>figure {
      width: 100%;
    }

    .quarto-figure-left>figure>p {
      text-align: left;
    }

    .quarto-figure-center>figure>p {
      text-align: center;
    }

    .quarto-figure-right>figure>p {
      text-align: right;
    }

    figure>p:empty {
      display: none;
    }

    figure>p:first-child {
      margin-top: 0;
      margin-bottom: 0;
    }

    figure>figcaption {
      margin-top: 0.5em;
    }

    figcaption {
      font-size: 0.8em;
    }

    details {
      margin-bottom: 1em;
    }

    details[show] {
      margin-bottom: 0;
    }

    .quarto-unresolved-ref {
      font-weight: 600;
    }

    .quarto-cover-image {
      float: right;
      margin-left: 30px;
    }

    .cell-output-display {
      overflow-x: scroll;
    }

    .hidden {
      display: none;
    }
  </style>
</head>
<body epub:type="bodymatter">
<section id="object-detection-1" class="level1 unnumbered">
<h1 class="unnumbered">Object Detection</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file582.png" alt="" /></p>
<figcaption><em>DALL·E prompt - Cartoon styled after 1950s animations, showing a detailed board with sensors, particularly a camera, on a table with patterned cloth. Behind the board, a computer with a large back showcases the Arduino IDE. The IDE’s content hints at LED pin assignments and machine learning inference for detecting spoken commands. The Serial Monitor, in a distinct window, reveals outputs for the commands ‘yes’ and ‘no’.</em></figcaption>
</figure>
</div>
<section id="sec-object-detection-overview-d035" class="level2 unnumbered">
<h2 class="unnumbered">Overview</h2>
<p>In the last section regarding Computer Vision (CV) and the XIAO ESP32S3, <em>Image Classification,</em> we learned how to set up and classify images with this remarkable development board. Continuing our CV journey, we will explore <strong>Object Detection</strong> on microcontrollers.</p>
<section id="sec-object-detection-object-detection-versus-image-classification-29e4" class="level3 unnumbered">
<h3 class="unnumbered">Object Detection versus Image Classification</h3>
<p>The main task with Image Classification models is to identify the most probable object category present on an image, for example, to classify between a cat or a dog, dominant “objects” in an image:</p>
<p> <img src="../media/file583.jpg" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
<p>But what happens if there is no dominant category in the image?</p>
<p> <img src="../media/file584.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
<p>An image classification model identifies the above image utterly wrong as an “ashcan,” possibly due to the color tonalities.</p>
<blockquote>
<p>The model used in the previous images is MobileNet, which is trained with a large dataset, <em>ImageNet</em>, running on a Raspberry Pi.</p>
</blockquote>
<p>To solve this issue, we need another type of model, where not only <strong>multiple categories</strong> (or labels) can be found but also <strong>where</strong> the objects are located on a given image.</p>
<p>As we can imagine, such models are much more complicated and bigger, for example, the <strong>MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset.</strong> This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:</p>
<p> <img src="../media/file585.png" alt="" /></p>
<p>Those models used for object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually has, at most, a few MB as in the case of the XIAO ESP32S3.</p>
</section>
<section id="sec-object-detection-innovative-solution-object-detection-fomo-7fb4" class="level3 unnumbered">
<h3 class="unnumbered">An Innovative Solution for Object Detection: FOMO</h3>
<p><a href="https://docs.edgeimpulse.com/docs/edge-impulse-studio/learning-blocks/object-detection/fomo-object-detection-for-constrained-devices">Edge Impulse launched in 2022, <strong>FOMO</strong> (Faster Objects, More Objects),</a> a novel solution to perform object detection on embedded devices, such as the Nicla Vision and Portenta (Cortex M7), on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM, ESP-EYE and XIAO ESP32S3 Sense).</p>
<p>In this Hands-On project, we will explore Object Detection using FOMO.</p>
<blockquote>
<p>To understand more about FOMO, you can go into the <a href="https://www.edgeimpulse.com/blog/announcing-fomo-faster-objects-more-objects">official FOMO announcement</a> by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.</p>
</blockquote>
</section>
</section>
<section id="sec-object-detection-object-detection-project-goal-d1c5" class="level2 unnumbered">
<h2 class="unnumbered">The Object Detection Project Goal</h2>
<p>All Machine Learning projects need to start with a detailed goal. Let’s assume we are in an industrial or rural facility and must sort and count <strong>oranges (fruits)</strong> and particular <strong>frogs (bugs)</strong>.</p>
<p> <img src="../media/file586.png" alt="" /></p>
<p>In other words, we should perform a multi-label classification, where each image can have three classes:</p>
<ul>
<li>Background (No objects)</li>
<li>Fruit</li>
<li>Bug</li>
</ul>
<p>Here are some not labeled image samples that we should use to detect the objects (fruits and bugs):</p>
<p> <img src="../media/file587.jpg" alt="" /></p>
<p>We are interested in which object is in the image, its location (centroid), and how many we can find on it. The object’s size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.</p>
<p>We will develop the project using the XIAO ESP32S3 for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let’s create a <em>raw dataset</em> (not labeled) with images that contain the objects to be detected.</p>
</section>
<section id="sec-object-detection-data-collection-897c" class="level2 unnumbered">
<h2 class="unnumbered">Data Collection</h2>
<p>You can capture images using the XIAO, your phone, or other devices. Here, we will use the XIAO with code from the Arduino IDE ESP32 library.</p>
<section id="sec-object-detection-collecting-dataset-xiao-esp32s3-9814" class="level3 unnumbered">
<h3 class="unnumbered">Collecting Dataset with the XIAO ESP32S3</h3>
<p>Open the Arduino IDE and select the XIAO_ESP32S3 board (and the port where it is connected). On <code>File &gt; Examples &gt; ESP32 &gt; Camera</code>, select <code>CameraWebServer</code>.</p>
<p>On the BOARDS MANAGER panel, confirm that you have installed the latest “stable” package.</p>
<blockquote>
<p>⚠️ <strong>Attention</strong></p>
<p>Alpha versions (for example, 3.x-alpha) do not work correctly with the XIAO and Edge Impulse. Use the last stable version (for example, 2.0.11) instead.</p>
</blockquote>
<p>You also should comment on all cameras’ models, except the XIAO model pins:</p>
<p><code>#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM</code></p>
<p>And on <code>Tools</code>, enable the PSRAM. Enter your wifi credentials and upload the code to the device:</p>
<p> <img src="../media/file588.jpg" alt="" /></p>
<p>If the code is executed correctly, you should see the address on the Serial Monitor:</p>
<p> <img src="../media/file589.png" alt="" /></p>
<p>Copy the address on your browser and wait for the page to be uploaded. Select the camera resolution (for example, QVGA) and select <code>[START STREAM]</code>. Wait for a few seconds/minutes, depending on your connection. You can save an image on your computer download area using the [Save] button.</p>
<p> <img src="../media/file590.jpg" alt="" /></p>
<p>Edge impulse suggests that the objects should be similar in size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try using mixed sizes and positions to see the result.</p>
<blockquote>
<p>We do not need to create separate folders for our images because each contains multiple labels.</p>
</blockquote>
<p>We suggest using around 50 images to mix the objects and vary the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.</p>
<blockquote>
<p>The stored images use a QVGA frame size of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>320</mn><mo>×</mo><mn>240</mn></mrow><annotation encoding="application/x-tex">320\times 240</annotation></semantics></math> and RGB565 (color pixel format).</p>
</blockquote>
<p>After capturing your dataset, <code>[Stop Stream]</code> and move your images to a folder.</p>
</section>
</section>
<section id="sec-object-detection-edge-impulse-studio-e39e" class="level2 unnumbered">
<h2 class="unnumbered">Edge Impulse Studio</h2>
<section id="sec-object-detection-setup-project-5423" class="level3 unnumbered">
<h3 class="unnumbered">Setup the project</h3>
<p>Go to <a href="https://www.edgeimpulse.com/">Edge Impulse Studio,</a> enter your credentials at <strong>Login</strong> (or create an account), and start a new project.</p>
<p> <img src="../media/file591.png" alt="" /></p>
<blockquote>
<p>Here, you can clone the project developed for this hands-on: <a href="https://studio.edgeimpulse.com/public/315759/latest">XIAO-ESP32S3-Sense-Object_Detection</a></p>
</blockquote>
<p>On your Project Dashboard, go down and on <strong>Project info</strong> and select <strong>Bounding boxes (object detection)</strong> and <strong>Espressif ESP-EYE</strong> (most similar to our board) as your Target Device:</p>
<p> <img src="../media/file592.png" alt="" /></p>
</section>
<section id="sec-object-detection-uploading-unlabeled-data-391f" class="level3 unnumbered">
<h3 class="unnumbered">Uploading the unlabeled data</h3>
<p>On Studio, go to the <code>Data acquisition</code> tab, and on the <code>UPLOAD DATA</code> section, upload files captured as a folder from your computer.</p>
<p> <img src="../media/file593.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
<blockquote>
<p>You can leave for the Studio to split your data automatically between Train and Test or do it manually. We will upload all of them as training.</p>
</blockquote>
<p> <img src="../media/file594.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
<p>All the not-labeled images (47) were uploaded but must be labeled appropriately before being used as a project dataset. The Studio has a tool for that purpose, which you can find in the link Labeling queue (47).</p>
<p>There are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):</p>
<ul>
<li>Using yolov5</li>
<li>Tracking objects between frames</li>
</ul>
<blockquote>
<p>Edge Impulse launched an <a href="https://docs.edgeimpulse.com/docs/edge-impulse-studio/data-acquisition/auto-labeler">auto-labeling feature</a> for Enterprise customers, easing labeling tasks in object detection projects.</p>
</blockquote>
<p>Ordinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, <em>partially</em> labeling the new ones (not all are correctly labeled).</p>
<blockquote>
<p>You can use the <a href="https://docs.edgeimpulse.com/docs/tools/edge-impulse-cli/cli-uploader#bounding-boxes">EI uploader</a> to import your data if you already have a labeled dataset containing bounding boxes.</p>
</blockquote>
</section>
<section id="sec-object-detection-labeling-dataset-01f8" class="level3 unnumbered">
<h3 class="unnumbered">Labeling the Dataset</h3>
<p>Starting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click <strong>Save labels</strong> to advance to the next item.</p>
<p> <img src="../media/file595.png" alt="" /></p>
<p>Continue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:</p>
<p> <img src="../media/file596.jpg" alt="" /></p>
<p>Next, review the labeled samples on the <code>Data acquisition</code> tab. If one of the labels is wrong, you can edit it using the <em>three dots</em> menu after the sample name:</p>
<p> <img src="../media/file597.png" alt="" /></p>
<p>You will be guided to replace the wrong label and correct the dataset.</p>
<p> <img src="../media/file598.jpg" alt="" /></p>
</section>
<section id="sec-object-detection-balancing-dataset-split-traintest-d2b0" class="level3 unnumbered">
<h3 class="unnumbered">Balancing the dataset and split Train/Test</h3>
<p>After labeling all data, it was realized that the class fruit had many more samples than the bug. So, 11 new and additional bug images were collected (ending with 58 images). After labeling them, it is time to select some images and move them to the test dataset. You can do it using the three-dot menu after the image name. I selected six images, representing 13% of the total dataset.</p>
<p> <img src="../media/file599.png" class="quarto-figure quarto-figure-center" style="width:90.0%" alt="" /></p>
</section>
</section>
<section id="sec-object-detection-impulse-design-f04a" class="level2 unnumbered">
<h2 class="unnumbered">The Impulse Design</h2>
<p>In this phase, you should define how to:</p>
<ul>
<li><strong>Pre-processing</strong> consists of resizing the individual images from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>320</mn><mo>×</mo><mn>240</mn></mrow><annotation encoding="application/x-tex">320\times 240</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>96</mn><mo>×</mo><mn>96</mn></mrow><annotation encoding="application/x-tex">96\times 96</annotation></semantics></math> and squashing them (squared form, without cropping). Afterward, the images are converted from RGB to Grayscale.</li>
<li><strong>Design a Model,</strong> in this case, “Object Detection.”</li>
</ul>
<p> <img src="../media/file600.png" class="quarto-figure quarto-figure-center" style="width:90.0%" alt="" /></p>
<section id="sec-object-detection-preprocessing-dataset-dedd" class="level3 unnumbered">
<h3 class="unnumbered">Preprocessing all dataset</h3>
<p>In this section, select <strong>Color depth</strong> as Grayscale, suitable for use with FOMO models and Save parameters.</p>
<p> <img src="../media/file601.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>The Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>96</mn><mo>×</mo><mn>96</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">96\times 96\times 1</annotation></semantics></math> images or 9,216 features.</p>
<p> <img src="../media/file602.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>The feature explorer shows that all samples evidence a good separation after the feature generation.</p>
<blockquote>
<p>Some samples seem to be in the wrong space, but clicking on them confirms the correct labeling.</p>
</blockquote>
</section>
</section>
<section id="sec-object-detection-model-design-training-test-6104" class="level2 unnumbered">
<h2 class="unnumbered">Model Design, Training, and Test</h2>
<p>We will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of <strong>background</strong> vs <strong>objects of interest</strong> (here, <em>boxes</em> and <em>wheels</em>).</p>
<p>FOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.</p>
<section id="sec-object-detection-fomo-works-d8be" class="level3 unnumbered">
<h3 class="unnumbered">How FOMO works?</h3>
<p>FOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>96</mn><mo>×</mo><mn>96</mn></mrow><annotation encoding="application/x-tex">96\times 96</annotation></semantics></math>, the grid would be <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn><mo>×</mo><mn>12</mn></mrow><annotation encoding="application/x-tex">12\times 12</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>96</mn><mi>/</mi><mn>8</mn><mo>=</mo><mn>12</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(96/8=12)</annotation></semantics></math>. Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as <em>background</em>). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.</p>
<p> <img src="../media/file603.png" class="quarto-figure quarto-figure-center" style="width:90.0%" alt="" /></p>
<p>For training, we should select a pre-trained model. Let’s use the <strong>FOMO (Faster Objects, More Objects) MobileNetV2 0.35.</strong> This model uses around 250 KB of RAM and 80 KB of ROM (Flash), which suits well with our board.</p>
<p> <img src="../media/file604.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
<p>Regarding the training hyper-parameters, the model will be trained with:</p>
<ul>
<li>Epochs: 60</li>
<li>Batch size: 32</li>
<li>Learning Rate: 0.001.</li>
</ul>
<p>For validation during training, 20% of the dataset (<em>validation_dataset</em>) will be spared. For the remaining 80% (<em>train_dataset</em>), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.</p>
<p>As a result, the model ends with an overall F1 score of 85%, similar to the result when using the test data (83%).</p>
<blockquote>
<p>Note that FOMO automatically added a 3rd label background to the two previously defined (<em>box</em> and <em>wheel</em>).</p>
</blockquote>
<p> <img src="../media/file605.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<blockquote>
<p>In object detection tasks, accuracy is generally not the primary <a href="https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/">evaluation metric.</a> Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.</p>
</blockquote>
</section>
<section id="sec-object-detection-test-model-live-classification-e906" class="level3 unnumbered">
<h3 class="unnumbered">Test model with “Live Classification”</h3>
<p>Once our model is trained, we can test it using the Live Classification tool. On the correspondent section, click on Connect a development board icon (a small MCU) and scan the QR code with your phone.</p>
<p> <img src="../media/file606.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>Once connected, you can use the smartphone to capture actual images to be tested by the trained model on Edge Impulse Studio.</p>
<p> <img src="../media/file607.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>One thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the setup). Try with 0.8 or more.</p>
</section>
</section>
<section id="sec-object-detection-deploying-model-arduino-ide-3755" class="level2 unnumbered">
<h2 class="unnumbered">Deploying the Model (Arduino IDE)</h2>
<p>Select the Arduino Library and Quantized (int8) model, enable the EON Compiler on the Deploy Tab, and press [Build].</p>
<p> <img src="../media/file608.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>Open your Arduino IDE, and under Sketch, go to Include Library and add.ZIP Library. Select the file you download from Edge Impulse Studio, and that’s it!</p>
<p> <img src="../media/file609.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>Under the Examples tab on Arduino IDE, you should find a sketch code (<code>esp32 &gt; esp32_camera</code>) under your project name.</p>
<p> <img src="../media/file610.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>You should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb1-1"><a href="#cb1-1"></a><span class="pp">#define PWDN_GPIO_NUM     </span><span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="pp">#define RESET_GPIO_NUM    </span><span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="pp">#define XCLK_GPIO_NUM     </span><span class="dv">10</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="pp">#define SIOD_GPIO_NUM     </span><span class="dv">40</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="pp">#define SIOC_GPIO_NUM     </span><span class="dv">39</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="pp">#define Y9_GPIO_NUM       </span><span class="dv">48</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="pp">#define Y8_GPIO_NUM       </span><span class="dv">11</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="pp">#define Y7_GPIO_NUM       </span><span class="dv">12</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="pp">#define Y6_GPIO_NUM       </span><span class="dv">14</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="pp">#define Y5_GPIO_NUM       </span><span class="dv">16</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="pp">#define Y4_GPIO_NUM       </span><span class="dv">18</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="pp">#define Y3_GPIO_NUM       </span><span class="dv">17</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="pp">#define Y2_GPIO_NUM       </span><span class="dv">15</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="pp">#define VSYNC_GPIO_NUM    </span><span class="dv">38</span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="pp">#define HREF_GPIO_NUM     </span><span class="dv">47</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="pp">#define PCLK_GPIO_NUM     </span><span class="dv">13</span></span></code></pre></div>
<p>Here you can see the resulting code:</p>
<p> <img src="../media/file611.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>Upload the code to your XIAO ESP32S3 Sense, and you should be OK to start detecting fruits and bugs. You can check the result on Serial Monitor.</p>
<section id="sec-object-detection-background-967b" class="level3 unnumbered">
<h3 class="unnumbered">Background</h3>
<p> <img src="../media/file612.png" class="quarto-figure quarto-figure-center" style="width:75.0%" alt="" /></p>
</section>
<section id="sec-object-detection-fruits-bdb2" class="level3 unnumbered">
<h3 class="unnumbered">Fruits</h3>
<p> <img src="../media/file613.png" class="quarto-figure quarto-figure-center" style="width:75.0%" alt="" /></p>
</section>
<section id="sec-object-detection-bugs-871e" class="level3 unnumbered">
<h3 class="unnumbered">Bugs</h3>
<p> <img src="../media/file614.png" class="quarto-figure quarto-figure-center" style="width:75.0%" alt="" /></p>
<p>Note that the model latency is 143 ms, and the frame rate per second is around 7 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>320</mn><mo>×</mo><mn>320</mn></mrow><annotation encoding="application/x-tex">320\times 320</annotation></semantics></math> model on a Raspberry Pi 4, the latency is around five times higher (around 1.5 fps).</p>
</section>
</section>
<section id="sec-object-detection-deploying-model-sensecraftwebtoolkit-968c" class="level2 unnumbered">
<h2 class="unnumbered">Deploying the Model (SenseCraft-Web-Toolkit)</h2>
<p>As discussed in the Image Classification chapter, verifying inference with Image models on Arduino IDE is very challenging because we can not see what the camera focuses on. Again, let’s use the <strong>SenseCraft-Web Toolkit</strong>.</p>
<p>Follow the following steps to start the SenseCraft-Web-Toolkit:</p>
<ol type="1">
<li>Open the <a href="https://seeed-studio.github.io/SenseCraft-Web-Toolkit/#/setup/process">SenseCraft-Web-Toolkit website.</a></li>
<li>Connect the XIAO to your computer:</li>
</ol>
<ul>
<li>Having the XIAO connected, select it as below:</li>
</ul>
<p> <img src="../media/file615.jpg" class="quarto-figure quarto-figure-center" style="width:75.0%" alt="" /></p>
<ul>
<li>Select the device/Port and press <code>[Connect]</code>:</li>
</ul>
<p> <img src="../media/file616.jpg" class="quarto-figure quarto-figure-center" style="width:75.0%" alt="" /></p>
<blockquote>
<p>You can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!</p>
</blockquote>
<p>In our case, we will use the blue button at the bottom of the page: <code>[Upload Custom AI Model]</code>.</p>
<p>But first, we must download from Edge Impulse Studio our <strong>quantized .tflite</strong> model.</p>
<ol start="3" type="1">
<li>Go to your project at Edge Impulse Studio, or clone this one:</li>
</ol>
<ul>
<li><a href="https://studio.edgeimpulse.com/public/228516/live">XIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN</a></li>
</ul>
<ol start="4" type="1">
<li>On <code>Dashboard</code>, download the model (“block output”): <code>Object Detection model - TensorFlow Lite (int8 quantized)</code></li>
</ol>
<p> <img src="../media/file617.jpg" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<ol start="5" type="1">
<li>On SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: <code>[Upload Custom AI Model]</code>. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):</li>
</ol>
<p> <img src="../media/file618.jpg" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<blockquote>
<p>Note that you should use the labels trained on EI Studio and enter them in alphabetic order (in our case, background, bug, fruit).</p>
</blockquote>
<p>After a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:</p>
<p> <img src="../media/file619.jpg" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>The detected objects will be marked (the centroid). You can select the Confidence of your inference cursor <code>Confidence</code> and <code>IoU</code>, which is used to assess the accuracy of predicted bounding boxes compared to truth bounding boxes.</p>
<p>Clicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, as we did with the Arduino IDE.</p>
<p> <img src="../media/file620.png" class="quarto-figure quarto-figure-center" style="width:85.0%" alt="" /></p>
<p>On Device Log, you will get information as:</p>
<ul>
<li>Preprocess time (image capture and Crop): 3 ms,</li>
<li>Inference time (model latency): 115 ms,</li>
<li>Postprocess time (display of the image and marking objects): 1 ms.</li>
<li>Output tensor (boxes), for example, one of the boxes: [[30,150, 20, 20,97, 2]]; where 30,150, 20, 20 are the coordinates of the box (around the centroid); 97 is the inference result, and 2 is the class (in this case 2: fruit).</li>
</ul>
<blockquote>
<p>Note that in the above example, we got 5 boxes because none of the fruits got 3 centroids. One solution will be post-processing, where we can aggregate close centroids in one.</p>
</blockquote>
<p>Here are other screenshots:</p>
<p> <img src="../media/file621.jpg" alt="" /></p>
</section>
<section id="sec-object-detection-summary-3edc" class="level2 unnumbered">
<h2 class="unnumbered">Summary</h2>
<p>FOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:</p>
<blockquote>
<p>FOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.</p>
</blockquote>
<p>Multiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices.</p>
</section>
<section id="sec-object-detection-resources-7d20" class="level2 unnumbered">
<h2 class="unnumbered">Resources</h2>
<ul>
<li><a href="https://studio.edgeimpulse.com/public/315759/latest">Edge Impulse Project</a></li>
</ul>
</section>
</section>
</body>
</html>
