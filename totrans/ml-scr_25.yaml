- en: Concept
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概念
- en: 原文：[https://dafriedman97.github.io/mlbook/content/c6/concept.html](https://dafriedman97.github.io/mlbook/content/c6/concept.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://dafriedman97.github.io/mlbook/content/c6/concept.html](https://dafriedman97.github.io/mlbook/content/c6/concept.html)
- en: 'Due to their high variance, decision trees often fail to reach a level of precision
    comparable to other predictive algorithms. In the previous chapter, we introduced
    several ways to minimize the variance of a single decision tree, such as through
    pruning or direct size regulation. This chapter discusses another approach: *ensemble
    methods*. Ensemble methods combine the output of multiple simple models, often
    called “learners”, in order to create a final model with lower variance.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们的方差很高，决策树往往无法达到与其他预测算法相当的程度。在前一章中，我们介绍了几种减少单个决策树方差的方法，例如通过剪枝或直接尺寸调节。本章讨论另一种方法：*集成方法*。集成方法结合了多个简单模型（通常称为“学习者”）的输出，以创建一个具有更低方差的最终模型。
- en: 'We will introduce ensemble methods in the context of tree-based learners, though
    ensemble methods can be applied to a wide range of learning algorithms. That said,
    the structure of decision trees makes ensemble methods particularly valuable.
    Here we discuss three tree-based ensemble methods: *bagging*, *random forests*,
    and *boosting*.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在基于树的学习者背景下介绍集成方法，尽管集成方法可以应用于广泛的机器学习算法。尽管如此，决策树的结构使得集成方法特别有价值。在这里，我们讨论三种基于树的集成方法：*袋装法*、*随机森林*和*提升法*。
- en: An example demonstrating the power of ensemble methods is given below. Using
    the `tips` dataset from `scikit-learn`, we build several tree-based learners.
    Then for \(b = 1, 2, \dots, 30\), we use bagging to average the results of the
    \(b\) learners. For each bagged model, we also calculate the out-of-sample \(RSS\).
    The blue line below shows the \(RSS\) for each bagged model—which clearly decreases
    with \(b\)—and the red line shows the \(RSS\) for a single decision tree. By averaging
    many trees, rather than relying on a single one, we are able to improve the precision
    of our model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 下面给出了一个展示集成方法强大功能的例子。使用来自`scikit-learn`的`tips`数据集，我们构建了几个基于树的学习者。然后对于 \(b =
    1, 2, \dots, 30\)，我们使用袋装法来平均 \(b\) 个学习者的结果。对于每个袋装模型，我们还计算了样本外 \(RSS\)。下面的蓝色线显示了每个袋装模型的
    \(RSS\)——它显然随着 \(b\) 的增加而减少——红色线显示了单个决策树的 \(RSS\)。通过平均许多树，而不是依赖于单一的一棵树，我们能够提高我们模型的精度。
- en: '![](../Images/5c969d05faacb005c4057705b08f0b77.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c969d05faacb005c4057705b08f0b77.png)'
