["```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn.tree import DecisionTreeRegressor                # decision tree method\nfrom sklearn.ensemble import GradientBoostingRegressor        # tree-based gradient boosting\nfrom sklearn.tree import _tree                                # for accessing tree information\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.preprocessing import StandardScaler              # standardize the features\nfrom sklearn.tree import export_graphviz                      # graphical visualization of trees\nfrom sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning\nfrom sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # suppress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef comma_format(x, pos):\n    return f'{int(x):,}'\n\ndef feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot\n    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1\n    plt.plot(pred,metric,color='black',zorder=20)\n    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)\n    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  \n    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)\n    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();\n    return\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef plot_CDF(data,color,alpha=1.0,lw=1,ls='solid',label='none'):\n    cumprob = (np.linspace(1,len(data),len(data)))/(len(data)+1)\n    plt.scatter(np.sort(data),cumprob,c=color,alpha=alpha,edgecolor='black',lw=lw,ls=ls,label=label,zorder=10)\n    plt.plot(np.sort(data),cumprob,c=color,alpha=alpha,lw=lw,ls=ls,zorder=8)\n\ndef visualize_model(model,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,\n                         ymax,title,Xname,yname,Xlabel,ylabel,annotate=True):# plots the data points and the decision tree prediction \n    cmap = plt.cm.inferno\n    X1plot_step = (Xmax[0] - Xmin[0])/300.0; X2plot_step = -1*(Xmax[1] - Xmin[1])/300.0 # resolution of the model visualization\n    XX1, XX2 = np.meshgrid(np.arange(Xmin[0], Xmax[0], X1plot_step), # set up the mesh\n                     np.arange(Xmax[1], Xmin[1], X2plot_step))\n    y_hat = model.predict(np.c_[XX1.ravel(), XX2.ravel()])    # predict with our trained model over the mesh\n    y_hat = y_hat.reshape(XX1.shape)\n\n    plt.imshow(y_hat,interpolation=None, aspect=\"auto\", extent=[Xmin[0],Xmax[0],Xmin[1],Xmax[1]], \n        vmin=ymin,vmax=ymax,alpha = 1.0,cmap=cmap,zorder=1)\n    sp = plt.scatter(X1_train,X2_train,s=None, c=y_train, marker='o', cmap=cmap, \n        norm=None, vmin=ymin, vmax=ymax, alpha=0.6, linewidths=0.3, edgecolors=\"black\", label = 'Train',zorder=10)\n    plt.scatter(X1_test,X2_test,s=None, c=y_test, marker='s', cmap=cmap, \n        norm=None, vmin=ymin, vmax=ymax, alpha=0.3, linewidths=0.3, edgecolors=\"black\", label = 'Test',zorder=10)\n    plt.title(title); plt.xlabel(Xlabel[0]); plt.ylabel(Xlabel[1])\n    plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n    cbar = plt.colorbar(sp, orientation = 'vertical')         # add the color bar\n    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n    cbar.set_label(ylabel, rotation=270, labelpad=20)\n    return y_hat\n\ndef check_model(model,X,y,ymin,ymax,ylabel,title): # get OOB MSE and cross plot a decision tree \n    y_hat = model.predict(X)\n    MSE_test = metrics.mean_squared_error(y,y_hat)\n    plt.scatter(y,y_hat,s=None, c='darkorange',marker=None,cmap=cmap,norm=None,vmin=None,vmax=None,alpha=0.8, \n                linewidths=1.0, edgecolors=\"black\")\n    plt.title(title); plt.xlabel('Truth: ' + str(ylabel)); plt.ylabel('Estimated: ' + str(ylabel))\n    plt.xlim(ymin,ymax); plt.ylim(ymin,ymax)\n    plt.plot([ymin,ymax],[ymin,ymax],color='black'); add_grid()\n    plt.annotate('Testing MSE: ' + str(f'{(np.round(MSE_test,2)):,.0f}'),[4200,2500])\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\nimport pandas as pd \n```", "```py\npd.read_csv() \n```", "```py\ndf = pd.read_csv(\"unconv_MV.csv\") \n```", "```py\ndf = df.sample(frac=.30, random_state = 73073); \ndf = df.reset_index() \n```", "```py\nadd_error = True                                              # add random error to the response feature\nstd_error = 1000                                               # standard deviation of random error, for demonstration only\nidata = 2\n\nif idata == 1:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\") # load the data from my github repo\n    df_load = df_load.sample(frac=.30, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n\nelif idata == 2:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv\") # load the data \n    df_load = df_load.sample(frac=.70, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n    df_load = df_load.rename(columns={\"Prod\": \"Production\"})\n\nyname = 'Production'; Xname = ['Por','Brittle']               # specify the predictor features (x2) and response feature (x1)\nXmin = [5.0,0.0]; Xmax = [25.0,100.0]                         # set minimums and maximums for visualization \nymin = 1500.0; ymax = 7000.0\nXlabel = ['Porosity','Brittleness']; ylabel = 'Production'    # specify the feature labels for plotting\nXunit = ['%','%']; yunit = 'MCFPD'\nXlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']\nylabelunit = ylabel + ' (' + yunit + ')'\n\nif add_error == True:                                         # method to add error\n    np.random.seed(seed=seed)                                 # set random number seed\n    df_load[yname] = df_load[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df_load)) # add noise\n    values = df_load._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray\n\ny = pd.DataFrame(df_load[yname])                              # extract selected features as X and y DataFrames\nX = df_load[Xname]\ndf = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) \n```", "```py\ncorr_matrix = df.corr()\ncorrelation = corr_matrix.iloc[:,-1].values[:-1]\n\nplt.subplot(121)\nplot_corr(corr_matrix,'Correlation Matrix',1.0,0.1)           # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(Xname,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + yname,'Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npairgrid = sns.PairGrid(df,vars=Xname+[yname])                # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split\ndf_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)\ndf_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) \n```", "```py\nprint('       Training DataFrame          Testing DataFrame')\ndisplay_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display \n```", "```py\n Training DataFrame          Testing DataFrame \n```", "```py\nprint('            Training DataFrame                      Testing DataFrame')    # custom function for side-by-side summary statistics\ndisplay_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) \n```", "```py\n Training DataFrame                      Testing DataFrame \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(131)                                              # predictor feature #1 histogram\nfreq1,_,_ = plt.hist(x=df_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[0]); add_grid()  \nplt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   \n\nplt.subplot(132)                                              # predictor feature #2 histogram\nfreq1,_,_ = plt.hist(x=df_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[1]); add_grid()  \nplt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   \n\nplt.subplot(133)                                              # predictor features #1 and #2 scatter plot\nplt.scatter(df_train[Xname[0]],df_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[Xname[0]],df_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.title(Xlabel[1] + ' vs ' +  Xlabel[0])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])\nplt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=0.8, wspace=0.3, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(131)                                              # predictor feature #1 CDF\nplot_CDF(X_train[Xname[0]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(X_test[Xname[0]],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(Xlabelunit[0]); plt.xlim(Xmin[0],Xmax[0]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(Xlabel[0] + ' Train and Test CDFs')\n\nplt.subplot(132)                                              # predictor feature #2 CDF\nplot_CDF(X_train[Xname[1]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(X_test[Xname[1]],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(Xlabelunit[1]); plt.xlim(Xmin[1],Xmax[1]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(Xlabel[1] + ' Train and Test CDFs')\n\nplt.subplot(133)                                              # response feature CDF\nplot_CDF(y_train[yname],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(y_test[yname],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(ylabelunit); plt.xlim(ymin,ymax); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(ylabel + ' Train and Test CDFs')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=0.8, wspace=0.3, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(111)                                              # visualize the train and test data in predictor feature space\nim = plt.scatter(X_train[Xname[0]],X_train[Xname[1]],s=None, c=y_train[yname], marker='o', cmap=cmap, \n    norm=None, vmin=ymin, vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors=\"black\", label = 'Train')\nplt.scatter(X_test[Xname[0]],X_test[Xname[1]],s=None, c=y_test[yname], marker='s', cmap=cmap, \n    norm=None, vmin=ymin, vmax=ymax, alpha=0.5, linewidths=0.3, edgecolors=\"black\", label = 'Test')\nplt.title('Training ' + ylabel + ' vs. ' + Xlabel[1] + ' and ' + Xlabel[0]); \nplt.xlabel(Xlabel[0] + ' (' + Xunit[0] + ')'); plt.ylabel(Xlabel[1] + ' (' + Xunit[1] + ')')\nplt.xlim(Xmin[0],Xmax[0]); plt.ylim(Xmin[1],Xmax[1]); plt.legend(loc = 'upper right'); add_grid()\ncbar = plt.colorbar(im, orientation = 'vertical')\ncbar.set_label(ylabel + ' (' + yunit + ')', rotation=270, labelpad=20)\ncbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nparams = {\n    'loss': 'squared_error'                                 # L2 Norm - least squares\n    'n_estimators': 1,                                      # number of trees\n    'max_depth': 1,                                         # maximum depth per tree\n    'learning_rate': 1,                                     \n    'criterion': 'squared_error'                            # tree construction criterion\n} \n```", "```py\nboost_tree = GradientBoostingRegressor(**params) \n```", "```py\nboot_tree.fit(X = predictors, y = response) \n```", "```py\nparams = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 1,                       \n    'learning_rate': 1.0,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nnum_trees = np.linspace(1,6,6)                              # build a list of numbers of trees \nboosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries\n\nindex = 1\nfor num_tree in num_trees:                                  # loop over number of trees\n    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))\n    boosting_models[index-1].fit(X = X_train, y = y_train)\n    score.append(boosting_models[index-1].score(X = X_test, y = y_test))\n    plt.subplot(2,3,index)\n    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3); plt.show() \n```", "```py\nindex = 1\nfor num_tree in num_trees:                                  # check over number of trees\n    plt.subplot(2,3,index)\n    check_model(boosting_models[index-1],X_test,y_test,ymin,ymax,ylabelunit,'Tree-Based Boosting with ' + str(int(num_tree)) + ' Trees')\n    index = index + 1\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nparams = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 1,                                         # maximum depth per tree\n    'learning_rate': 1,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nnum_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees \nboosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries\n\nindex = 1\nfor num_tree in num_trees:                                  # loop over number of trees\n    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))\n    boosting_models[index-1].fit(X = X_train, y = y_train)\n    score.append(boosting_models[index-1].score(X = X_test, y = y_test))\n    plt.subplot(2,3,index)\n    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nindex = 1\nfor num_tree in num_trees:                                  # check over number of trees\n    plt.subplot(2,3,index)\n    check_model(boosting_models[index-1],X_test,y_test,ymin,ymax,ylabelunit,'Tree-Based Boosting with ' + str(int(num_tree)) + ' Trees')\n    index = index + 1\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nparams = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 2,                                         # maximum depth per tree\n    'learning_rate': 1,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nnum_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees \nboosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries\n\nindex = 1\nfor num_tree in num_trees:                                  # loop over number of trees\n    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))\n    boosting_models[index-1].fit(X = X_train, y = y_train)\n    score.append(boosting_models[index-1].score(X = X_test, y = y_test))\n    plt.subplot(2,3,index)\n    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nparams = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 3,                                         # maximum depth per tree\n    'learning_rate': 1,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nnum_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees \nboosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries\n\nindex = 1\nfor num_tree in num_trees:                                  # loop over number of trees\n    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))\n    boosting_models[index-1].fit(X = X_train, y = y_train)\n    score.append(boosting_models[index-1].score(X = X_test, y = y_test))\n    plt.subplot(2,3,index)\n    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nparams = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 5,                                         # maximum depth per tree\n    'learning_rate': 1,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nnum_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees \nboosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries\n\nindex = 1\nfor num_tree in num_trees:                                  # loop over number of trees\n    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))\n    boosting_models[index-1].fit(X = X_train, y = y_train)\n    score.append(boosting_models[index-1].score(X = X_test, y = y_test))\n    plt.subplot(2,3,index)\n    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nindex = 1\nfor num_tree in num_trees:                                  # check over number of trees\n    plt.subplot(2,3,index)\n    check_model(boosting_models[index-1],X_test,y_test,ymin,ymax,ylabelunit,'Tree-Based Boosting with ' + str(int(num_tree)) + ' Trees')\n    index = index + 1\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nnum_trees = np.linspace(1,200,200)\nmax_features = 1\nMSE1_list = []; MSE2_list = []; MSE3_list = []; MSE4_list = [] \n\nparams1 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 1,                                         # maximum depth per tree\n    'learning_rate': 0.2,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nparams2 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 2,                                         # maximum depth per tree\n    'learning_rate': 0.2,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nparams3 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 3,                                         # maximum depth per tree\n    'learning_rate': 0.2,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nparams4 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 10,                                        # maximum depth per tree\n    'learning_rate': 0.2,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nindex = 1\nfor num_tree in num_trees:                                  # loop over number of trees in our random forest\n    boosting_model1 = GradientBoostingRegressor(n_estimators=int(num_tree),**params1).fit(X = X_train, y = y_train)\n    y_test1_hat = boosting_model1.predict(X_test); MSE1_list.append(metrics.mean_squared_error(y_test,y_test1_hat))\n\n    boosting_model2 = GradientBoostingRegressor(n_estimators=int(num_tree),**params2).fit(X = X_train, y = y_train)\n    y_test2_hat = boosting_model2.predict(X_test); MSE2_list.append(metrics.mean_squared_error(y_test,y_test2_hat))\n\n    boosting_model3 = GradientBoostingRegressor(n_estimators=int(num_tree),**params3).fit(X = X_train, y = y_train)\n    y_test3_hat = boosting_model3.predict(X_test); MSE3_list.append(metrics.mean_squared_error(y_test,y_test3_hat))\n\n    boosting_model4 = GradientBoostingRegressor(n_estimators=int(num_tree),**params4).fit(X = X_train, y = y_train)\n    y_test4_hat = boosting_model4.predict(X_test); MSE4_list.append(metrics.mean_squared_error(y_test,y_test4_hat))\n\n    index = index + 1\n\nplt.subplot(111)                                            # plot jackknife results for all cases\nplt.scatter(num_trees,MSE1_list,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 1\")\nplt.scatter(num_trees,MSE2_list,s=None,c='blue',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 2\")\nplt.scatter(num_trees,MSE3_list,s=None,c='black',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 3\")\nplt.scatter(num_trees,MSE4_list,s=None,c='green',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 10\")\n\nplt.title('Testing Mean Square Error vs. Number of Trees'); plt.xlabel('Number of Trees'); plt.ylabel('Test Mean Square Error')\nplt.xlim(0,200); plt.legend(loc='lower right'); add_grid()\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlearning_rates = np.arange(0.01,1.0,0.01)\nMSE1_list = []; MSE2_list = []; MSE3_list = []; MSE4_list = [] \n\nparams1 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 5,                                         # maximum depth per tree\n    'n_estimators': 40,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nparams2 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 5,                                         # maximum depth per tree\n    'n_estimators': 40,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nparams3 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 5,                                         # maximum depth per tree\n    'n_estimators': 40,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nparams4 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 5,                                        # maximum depth per tree\n    'n_estimators': 40,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nindex = 1\nfor learning_rate in learning_rates:                                  # loop over number of trees in our random forest\n    boosting_model1 = GradientBoostingRegressor(learning_rate = learning_rate,**params1).fit(X = X_train, y = y_train)\n    y_test1_hat = boosting_model1.predict(X_test); MSE1_list.append(metrics.mean_squared_error(y_test,y_test1_hat))\n\n    boosting_model2 = GradientBoostingRegressor(learning_rate = learning_rate,**params2).fit(X = X_train, y = y_train)\n    y_test2_hat = boosting_model2.predict(X_test); MSE2_list.append(metrics.mean_squared_error(y_test,y_test2_hat))\n\n    boosting_model3 = GradientBoostingRegressor(learning_rate = learning_rate,**params3).fit(X = X_train, y = y_train)\n    y_test3_hat = boosting_model3.predict(X_test); MSE3_list.append(metrics.mean_squared_error(y_test,y_test3_hat))\n\n    boosting_model4 = GradientBoostingRegressor(learning_rate = learning_rate,**params4).fit(X = X_train, y = y_train)\n    y_test4_hat = boosting_model4.predict(X_test); MSE4_list.append(metrics.mean_squared_error(y_test,y_test4_hat))\n\n    index = index + 1\n\nplt.subplot(111)                                            # plot jackknife results for all cases\nplt.scatter(learning_rates,MSE1_list,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 1\")\nplt.scatter(learning_rates,MSE2_list,s=None,c='blue',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 2\")\nplt.scatter(learning_rates,MSE3_list,s=None,c='black',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 3\")\nplt.scatter(learning_rates,MSE4_list,s=None,c='green',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 10\")\n\nplt.title('Testing Mean Square Error vs. Learning Rate'); plt.xlabel('Learning Rate'); plt.ylabel('Test Mean Square Error')\nplt.xlim(0.0,1.0); plt.legend(loc='upper left'); add_grid()\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nx1 = 0.25; x2 = 0.3                                           # predictor values for the prediction\n\npipe_boosting = Pipeline([                                      # the machine learning workflow as a pipeline object\n    ('boosting', GradientBoostingRegressor())\n])\n\nparams = {                                                    # the machine learning workflow method's parameters to search\n    'boosting__learning_rate': np.arange(0.1,2.0,0.2),\n    'boosting__n_estimators': np.arange(2,40,4,dtype = int),\n}\n\ntuned_boosting = GridSearchCV(pipe_boosting,params,scoring = 'neg_mean_squared_error', # hyperparameter tuning w. grid search k-fold cross validation \n                             refit = True)\ntuned_boosting.fit(X,y)                                         # tune the model with k-fold and then train the model will the data \n\nprint('Tuned hyperparameter: ' + str(tuned_boosting.best_params_))\n\nestimate = tuned_boosting.predict([[x1,x2]])[0]                 # make a prediction (no tuning shown)\nprint('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + \n      str(round(estimate,1)) + ' ' + yunit)                     # print results \n```", "```py\nTuned hyperparameter: {'boosting__learning_rate': 0.30000000000000004, 'boosting__n_estimators': 10}\nEstimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1979.7 MCFPD \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn.tree import DecisionTreeRegressor                # decision tree method\nfrom sklearn.ensemble import GradientBoostingRegressor        # tree-based gradient boosting\nfrom sklearn.tree import _tree                                # for accessing tree information\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.preprocessing import StandardScaler              # standardize the features\nfrom sklearn.tree import export_graphviz                      # graphical visualization of trees\nfrom sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning\nfrom sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # suppress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef comma_format(x, pos):\n    return f'{int(x):,}'\n\ndef feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot\n    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1\n    plt.plot(pred,metric,color='black',zorder=20)\n    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)\n    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  \n    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)\n    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();\n    return\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef plot_CDF(data,color,alpha=1.0,lw=1,ls='solid',label='none'):\n    cumprob = (np.linspace(1,len(data),len(data)))/(len(data)+1)\n    plt.scatter(np.sort(data),cumprob,c=color,alpha=alpha,edgecolor='black',lw=lw,ls=ls,label=label,zorder=10)\n    plt.plot(np.sort(data),cumprob,c=color,alpha=alpha,lw=lw,ls=ls,zorder=8)\n\ndef visualize_model(model,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,\n                         ymax,title,Xname,yname,Xlabel,ylabel,annotate=True):# plots the data points and the decision tree prediction \n    cmap = plt.cm.inferno\n    X1plot_step = (Xmax[0] - Xmin[0])/300.0; X2plot_step = -1*(Xmax[1] - Xmin[1])/300.0 # resolution of the model visualization\n    XX1, XX2 = np.meshgrid(np.arange(Xmin[0], Xmax[0], X1plot_step), # set up the mesh\n                     np.arange(Xmax[1], Xmin[1], X2plot_step))\n    y_hat = model.predict(np.c_[XX1.ravel(), XX2.ravel()])    # predict with our trained model over the mesh\n    y_hat = y_hat.reshape(XX1.shape)\n\n    plt.imshow(y_hat,interpolation=None, aspect=\"auto\", extent=[Xmin[0],Xmax[0],Xmin[1],Xmax[1]], \n        vmin=ymin,vmax=ymax,alpha = 1.0,cmap=cmap,zorder=1)\n    sp = plt.scatter(X1_train,X2_train,s=None, c=y_train, marker='o', cmap=cmap, \n        norm=None, vmin=ymin, vmax=ymax, alpha=0.6, linewidths=0.3, edgecolors=\"black\", label = 'Train',zorder=10)\n    plt.scatter(X1_test,X2_test,s=None, c=y_test, marker='s', cmap=cmap, \n        norm=None, vmin=ymin, vmax=ymax, alpha=0.3, linewidths=0.3, edgecolors=\"black\", label = 'Test',zorder=10)\n    plt.title(title); plt.xlabel(Xlabel[0]); plt.ylabel(Xlabel[1])\n    plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n    cbar = plt.colorbar(sp, orientation = 'vertical')         # add the color bar\n    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n    cbar.set_label(ylabel, rotation=270, labelpad=20)\n    return y_hat\n\ndef check_model(model,X,y,ymin,ymax,ylabel,title): # get OOB MSE and cross plot a decision tree \n    y_hat = model.predict(X)\n    MSE_test = metrics.mean_squared_error(y,y_hat)\n    plt.scatter(y,y_hat,s=None, c='darkorange',marker=None,cmap=cmap,norm=None,vmin=None,vmax=None,alpha=0.8, \n                linewidths=1.0, edgecolors=\"black\")\n    plt.title(title); plt.xlabel('Truth: ' + str(ylabel)); plt.ylabel('Estimated: ' + str(ylabel))\n    plt.xlim(ymin,ymax); plt.ylim(ymin,ymax)\n    plt.plot([ymin,ymax],[ymin,ymax],color='black'); add_grid()\n    plt.annotate('Testing MSE: ' + str(f'{(np.round(MSE_test,2)):,.0f}'),[4200,2500])\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\nimport pandas as pd \n```", "```py\npd.read_csv() \n```", "```py\ndf = pd.read_csv(\"unconv_MV.csv\") \n```", "```py\ndf = df.sample(frac=.30, random_state = 73073); \ndf = df.reset_index() \n```", "```py\nadd_error = True                                              # add random error to the response feature\nstd_error = 1000                                               # standard deviation of random error, for demonstration only\nidata = 2\n\nif idata == 1:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\") # load the data from my github repo\n    df_load = df_load.sample(frac=.30, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n\nelif idata == 2:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv\") # load the data \n    df_load = df_load.sample(frac=.70, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n    df_load = df_load.rename(columns={\"Prod\": \"Production\"})\n\nyname = 'Production'; Xname = ['Por','Brittle']               # specify the predictor features (x2) and response feature (x1)\nXmin = [5.0,0.0]; Xmax = [25.0,100.0]                         # set minimums and maximums for visualization \nymin = 1500.0; ymax = 7000.0\nXlabel = ['Porosity','Brittleness']; ylabel = 'Production'    # specify the feature labels for plotting\nXunit = ['%','%']; yunit = 'MCFPD'\nXlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']\nylabelunit = ylabel + ' (' + yunit + ')'\n\nif add_error == True:                                         # method to add error\n    np.random.seed(seed=seed)                                 # set random number seed\n    df_load[yname] = df_load[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df_load)) # add noise\n    values = df_load._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray\n\ny = pd.DataFrame(df_load[yname])                              # extract selected features as X and y DataFrames\nX = df_load[Xname]\ndf = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) \n```", "```py\ncorr_matrix = df.corr()\ncorrelation = corr_matrix.iloc[:,-1].values[:-1]\n\nplt.subplot(121)\nplot_corr(corr_matrix,'Correlation Matrix',1.0,0.1)           # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(Xname,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + yname,'Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npairgrid = sns.PairGrid(df,vars=Xname+[yname])                # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split\ndf_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)\ndf_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) \n```", "```py\nprint('       Training DataFrame          Testing DataFrame')\ndisplay_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display \n```", "```py\n Training DataFrame          Testing DataFrame \n```", "```py\nprint('            Training DataFrame                      Testing DataFrame')    # custom function for side-by-side summary statistics\ndisplay_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) \n```", "```py\n Training DataFrame                      Testing DataFrame \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(131)                                              # predictor feature #1 histogram\nfreq1,_,_ = plt.hist(x=df_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[0]); add_grid()  \nplt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   \n\nplt.subplot(132)                                              # predictor feature #2 histogram\nfreq1,_,_ = plt.hist(x=df_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[1]); add_grid()  \nplt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   \n\nplt.subplot(133)                                              # predictor features #1 and #2 scatter plot\nplt.scatter(df_train[Xname[0]],df_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[Xname[0]],df_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.title(Xlabel[1] + ' vs ' +  Xlabel[0])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])\nplt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=0.8, wspace=0.3, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(131)                                              # predictor feature #1 CDF\nplot_CDF(X_train[Xname[0]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(X_test[Xname[0]],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(Xlabelunit[0]); plt.xlim(Xmin[0],Xmax[0]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(Xlabel[0] + ' Train and Test CDFs')\n\nplt.subplot(132)                                              # predictor feature #2 CDF\nplot_CDF(X_train[Xname[1]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(X_test[Xname[1]],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(Xlabelunit[1]); plt.xlim(Xmin[1],Xmax[1]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(Xlabel[1] + ' Train and Test CDFs')\n\nplt.subplot(133)                                              # response feature CDF\nplot_CDF(y_train[yname],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(y_test[yname],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(ylabelunit); plt.xlim(ymin,ymax); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(ylabel + ' Train and Test CDFs')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=0.8, wspace=0.3, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(111)                                              # visualize the train and test data in predictor feature space\nim = plt.scatter(X_train[Xname[0]],X_train[Xname[1]],s=None, c=y_train[yname], marker='o', cmap=cmap, \n    norm=None, vmin=ymin, vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors=\"black\", label = 'Train')\nplt.scatter(X_test[Xname[0]],X_test[Xname[1]],s=None, c=y_test[yname], marker='s', cmap=cmap, \n    norm=None, vmin=ymin, vmax=ymax, alpha=0.5, linewidths=0.3, edgecolors=\"black\", label = 'Test')\nplt.title('Training ' + ylabel + ' vs. ' + Xlabel[1] + ' and ' + Xlabel[0]); \nplt.xlabel(Xlabel[0] + ' (' + Xunit[0] + ')'); plt.ylabel(Xlabel[1] + ' (' + Xunit[1] + ')')\nplt.xlim(Xmin[0],Xmax[0]); plt.ylim(Xmin[1],Xmax[1]); plt.legend(loc = 'upper right'); add_grid()\ncbar = plt.colorbar(im, orientation = 'vertical')\ncbar.set_label(ylabel + ' (' + yunit + ')', rotation=270, labelpad=20)\ncbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nparams = {\n    'loss': 'squared_error'                                 # L2 Norm - least squares\n    'n_estimators': 1,                                      # number of trees\n    'max_depth': 1,                                         # maximum depth per tree\n    'learning_rate': 1,                                     \n    'criterion': 'squared_error'                            # tree construction criterion\n} \n```", "```py\nboost_tree = GradientBoostingRegressor(**params) \n```", "```py\nboot_tree.fit(X = predictors, y = response) \n```", "```py\nparams = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 1,                       \n    'learning_rate': 1.0,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nnum_trees = np.linspace(1,6,6)                              # build a list of numbers of trees \nboosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries\n\nindex = 1\nfor num_tree in num_trees:                                  # loop over number of trees\n    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))\n    boosting_models[index-1].fit(X = X_train, y = y_train)\n    score.append(boosting_models[index-1].score(X = X_test, y = y_test))\n    plt.subplot(2,3,index)\n    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3); plt.show() \n```", "```py\nindex = 1\nfor num_tree in num_trees:                                  # check over number of trees\n    plt.subplot(2,3,index)\n    check_model(boosting_models[index-1],X_test,y_test,ymin,ymax,ylabelunit,'Tree-Based Boosting with ' + str(int(num_tree)) + ' Trees')\n    index = index + 1\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nparams = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 1,                                         # maximum depth per tree\n    'learning_rate': 1,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nnum_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees \nboosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries\n\nindex = 1\nfor num_tree in num_trees:                                  # loop over number of trees\n    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))\n    boosting_models[index-1].fit(X = X_train, y = y_train)\n    score.append(boosting_models[index-1].score(X = X_test, y = y_test))\n    plt.subplot(2,3,index)\n    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nindex = 1\nfor num_tree in num_trees:                                  # check over number of trees\n    plt.subplot(2,3,index)\n    check_model(boosting_models[index-1],X_test,y_test,ymin,ymax,ylabelunit,'Tree-Based Boosting with ' + str(int(num_tree)) + ' Trees')\n    index = index + 1\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nparams = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 2,                                         # maximum depth per tree\n    'learning_rate': 1,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nnum_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees \nboosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries\n\nindex = 1\nfor num_tree in num_trees:                                  # loop over number of trees\n    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))\n    boosting_models[index-1].fit(X = X_train, y = y_train)\n    score.append(boosting_models[index-1].score(X = X_test, y = y_test))\n    plt.subplot(2,3,index)\n    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nparams = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 3,                                         # maximum depth per tree\n    'learning_rate': 1,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nnum_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees \nboosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries\n\nindex = 1\nfor num_tree in num_trees:                                  # loop over number of trees\n    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))\n    boosting_models[index-1].fit(X = X_train, y = y_train)\n    score.append(boosting_models[index-1].score(X = X_test, y = y_test))\n    plt.subplot(2,3,index)\n    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nparams = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 5,                                         # maximum depth per tree\n    'learning_rate': 1,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nnum_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees \nboosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries\n\nindex = 1\nfor num_tree in num_trees:                                  # loop over number of trees\n    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))\n    boosting_models[index-1].fit(X = X_train, y = y_train)\n    score.append(boosting_models[index-1].score(X = X_test, y = y_test))\n    plt.subplot(2,3,index)\n    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nindex = 1\nfor num_tree in num_trees:                                  # check over number of trees\n    plt.subplot(2,3,index)\n    check_model(boosting_models[index-1],X_test,y_test,ymin,ymax,ylabelunit,'Tree-Based Boosting with ' + str(int(num_tree)) + ' Trees')\n    index = index + 1\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nnum_trees = np.linspace(1,200,200)\nmax_features = 1\nMSE1_list = []; MSE2_list = []; MSE3_list = []; MSE4_list = [] \n\nparams1 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 1,                                         # maximum depth per tree\n    'learning_rate': 0.2,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nparams2 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 2,                                         # maximum depth per tree\n    'learning_rate': 0.2,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nparams3 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 3,                                         # maximum depth per tree\n    'learning_rate': 0.2,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nparams4 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 10,                                        # maximum depth per tree\n    'learning_rate': 0.2,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nindex = 1\nfor num_tree in num_trees:                                  # loop over number of trees in our random forest\n    boosting_model1 = GradientBoostingRegressor(n_estimators=int(num_tree),**params1).fit(X = X_train, y = y_train)\n    y_test1_hat = boosting_model1.predict(X_test); MSE1_list.append(metrics.mean_squared_error(y_test,y_test1_hat))\n\n    boosting_model2 = GradientBoostingRegressor(n_estimators=int(num_tree),**params2).fit(X = X_train, y = y_train)\n    y_test2_hat = boosting_model2.predict(X_test); MSE2_list.append(metrics.mean_squared_error(y_test,y_test2_hat))\n\n    boosting_model3 = GradientBoostingRegressor(n_estimators=int(num_tree),**params3).fit(X = X_train, y = y_train)\n    y_test3_hat = boosting_model3.predict(X_test); MSE3_list.append(metrics.mean_squared_error(y_test,y_test3_hat))\n\n    boosting_model4 = GradientBoostingRegressor(n_estimators=int(num_tree),**params4).fit(X = X_train, y = y_train)\n    y_test4_hat = boosting_model4.predict(X_test); MSE4_list.append(metrics.mean_squared_error(y_test,y_test4_hat))\n\n    index = index + 1\n\nplt.subplot(111)                                            # plot jackknife results for all cases\nplt.scatter(num_trees,MSE1_list,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 1\")\nplt.scatter(num_trees,MSE2_list,s=None,c='blue',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 2\")\nplt.scatter(num_trees,MSE3_list,s=None,c='black',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 3\")\nplt.scatter(num_trees,MSE4_list,s=None,c='green',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 10\")\n\nplt.title('Testing Mean Square Error vs. Number of Trees'); plt.xlabel('Number of Trees'); plt.ylabel('Test Mean Square Error')\nplt.xlim(0,200); plt.legend(loc='lower right'); add_grid()\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlearning_rates = np.arange(0.01,1.0,0.01)\nMSE1_list = []; MSE2_list = []; MSE3_list = []; MSE4_list = [] \n\nparams1 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 5,                                         # maximum depth per tree\n    'n_estimators': 40,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nparams2 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 5,                                         # maximum depth per tree\n    'n_estimators': 40,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nparams3 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 5,                                         # maximum depth per tree\n    'n_estimators': 40,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nparams4 = {\n    'loss': 'squared_error',                                           # L2 Norm - least squares\n    'max_depth': 5,                                        # maximum depth per tree\n    'n_estimators': 40,\n    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training\n}\n\nindex = 1\nfor learning_rate in learning_rates:                                  # loop over number of trees in our random forest\n    boosting_model1 = GradientBoostingRegressor(learning_rate = learning_rate,**params1).fit(X = X_train, y = y_train)\n    y_test1_hat = boosting_model1.predict(X_test); MSE1_list.append(metrics.mean_squared_error(y_test,y_test1_hat))\n\n    boosting_model2 = GradientBoostingRegressor(learning_rate = learning_rate,**params2).fit(X = X_train, y = y_train)\n    y_test2_hat = boosting_model2.predict(X_test); MSE2_list.append(metrics.mean_squared_error(y_test,y_test2_hat))\n\n    boosting_model3 = GradientBoostingRegressor(learning_rate = learning_rate,**params3).fit(X = X_train, y = y_train)\n    y_test3_hat = boosting_model3.predict(X_test); MSE3_list.append(metrics.mean_squared_error(y_test,y_test3_hat))\n\n    boosting_model4 = GradientBoostingRegressor(learning_rate = learning_rate,**params4).fit(X = X_train, y = y_train)\n    y_test4_hat = boosting_model4.predict(X_test); MSE4_list.append(metrics.mean_squared_error(y_test,y_test4_hat))\n\n    index = index + 1\n\nplt.subplot(111)                                            # plot jackknife results for all cases\nplt.scatter(learning_rates,MSE1_list,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 1\")\nplt.scatter(learning_rates,MSE2_list,s=None,c='blue',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 2\")\nplt.scatter(learning_rates,MSE3_list,s=None,c='black',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 3\")\nplt.scatter(learning_rates,MSE4_list,s=None,c='green',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,\n            linewidths=0.3,edgecolors=\"black\",label = \"Tree Depth = 10\")\n\nplt.title('Testing Mean Square Error vs. Learning Rate'); plt.xlabel('Learning Rate'); plt.ylabel('Test Mean Square Error')\nplt.xlim(0.0,1.0); plt.legend(loc='upper left'); add_grid()\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nx1 = 0.25; x2 = 0.3                                           # predictor values for the prediction\n\npipe_boosting = Pipeline([                                      # the machine learning workflow as a pipeline object\n    ('boosting', GradientBoostingRegressor())\n])\n\nparams = {                                                    # the machine learning workflow method's parameters to search\n    'boosting__learning_rate': np.arange(0.1,2.0,0.2),\n    'boosting__n_estimators': np.arange(2,40,4,dtype = int),\n}\n\ntuned_boosting = GridSearchCV(pipe_boosting,params,scoring = 'neg_mean_squared_error', # hyperparameter tuning w. grid search k-fold cross validation \n                             refit = True)\ntuned_boosting.fit(X,y)                                         # tune the model with k-fold and then train the model will the data \n\nprint('Tuned hyperparameter: ' + str(tuned_boosting.best_params_))\n\nestimate = tuned_boosting.predict([[x1,x2]])[0]                 # make a prediction (no tuning shown)\nprint('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + \n      str(round(estimate,1)) + ' ' + yunit)                     # print results \n```", "```py\nTuned hyperparameter: {'boosting__learning_rate': 0.30000000000000004, 'boosting__n_estimators': 10}\nEstimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1979.7 MCFPD \n```"]