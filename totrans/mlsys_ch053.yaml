- en: Vision-Language Models (VLM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../media/file941.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*DALL·E prompt - A Raspberry Pi setup featuring vision tasks. The image shows
    a Raspberry Pi connected to a camera, with various computer vision tasks displayed
    visually around it, including object detection, image captioning, segmentation,
    and visual grounding. The Raspberry Pi is placed on a desk, with a display showing
    bounding boxes and annotations related to these tasks. The background should be
    a home workspace, with tools and devices typically used by developers and hobbyists.*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this hands-on lab, we will continuously explore AI applications at the Edge,
    from the basic setup of the Florence-2, Microsoft’s state-of-the-art vision foundation
    model, to advanced implementations on devices like the Raspberry Pi. We will learn
    to use Vision Language Models (VLMs) for tasks such as captioning, object detection,
    grounding, segmentation, and OCR on a Raspberry Pi.
  prefs: []
  type: TYPE_NORMAL
- en: Why Florence-2 at the Edge?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Florence-2](https://arxiv.org/abs/2311.06242) is a vision-language model open-sourced
    by Microsoft under the MIT license, which significantly advances vision-language
    models by combining a lightweight architecture with robust capabilities. Thanks
    to its training on the massive FLD-5B dataset, which contains 126 million images
    and 5.4 billion visual annotations, it achieves performance comparable to larger
    models. This makes Florence-2 ideal for deployment at the edge, where power and
    computational resources are limited.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this tutorial, we will explore how to use Florence-2 for real-time computer
    vision applications, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Image captioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual grounding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual grounding** involves linking textual descriptions to specific regions
    within an image. This enables the model to understand where particular objects
    or entities described in a prompt are in the image. For example, if the prompt
    is “a red car,” the model will identify and highlight the region where the red
    car is found in the image. Visual grounding is helpful for applications where
    precise alignment between text and visual content is needed, such as human-computer
    interaction, image annotation, and interactive AI systems.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In the tutorial, we will walk through:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Florence-2 on the Raspberry Pi
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running inference tasks such as object detection and captioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the model to get the best performance from the edge device
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring practical, real-world applications with fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Florence-2 Model Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Florence-2 utilizes a unified, prompt-based representation to handle various
    vision-language tasks. The model architecture consists of two main components:
    an **image encoder** and a **multi-modal transformer encoder-decoder**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file942.svg)'
  prefs: []
  type: TYPE_IMG
- en: '**Image Encoder**: The image encoder is based on the [DaViT (Dual Attention
    Vision Transformers) architecture](https://arxiv.org/abs/2204.03645). It converts
    input images into a series of visual token embeddings. These embeddings serve
    as the foundational representations of the visual content, capturing both spatial
    and contextual information about the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-Modal Transformer Encoder-Decoder**: Florence-2’s core is the multi-modal
    transformer encoder-decoder, which combines visual token embeddings from the image
    encoder with textual embeddings generated by a BERT-like model. This combination
    allows the model to simultaneously process visual and textual inputs, enabling
    a unified approach to tasks such as image captioning, object detection, and segmentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model’s training on the extensive FLD-5B dataset ensures it can effectively
    handle diverse vision tasks without requiring task-specific modifications. Florence-2
    uses textual prompts to activate specific tasks, making it highly flexible and
    capable of zero-shot generalization. For tasks like object detection or visual
    grounding, the model incorporates additional location tokens to represent regions
    within the image, ensuring a precise understanding of spatial relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Florence-2’s compact architecture and innovative training approach allow it
    to perform computer vision tasks accurately, even on resource-constrained devices
    like the Raspberry Pi.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Technical Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Florence-2 introduces several innovative features that set it apart:'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../media/file943.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Lightweight Design**: Two variants available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Florence-2-Base: 232 million parameters'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Florence-2-Large: 771 million parameters'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unified Representation**: Handles multiple vision tasks through a single
    architecture'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DaViT Vision Encoder**: Converts images into visual token embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer-based Multi-modal Encoder-Decoder**: Processes combined visual
    and text embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training Dataset (FLD-5B)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../media/file944.png)'
  prefs: []
  type: TYPE_IMG
- en: 126 million unique images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '5.4 billion comprehensive annotations, including:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 500M text annotations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.3B region-text annotations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.6B text-phrase-region annotations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated annotation pipeline using specialist models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterative refinement process for high-quality labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key Capabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Florence-2 excels in multiple vision tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot Performance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Image Captioning: Achieves 135.6 CIDEr score on COCO'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Visual Grounding: 84.4% recall@1 on Flickr30k'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Object Detection: 37.5 mAP on COCO val2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Referring Expression: 67.0% accuracy on RefCOCO'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuned Performance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Competitive with specialist models despite the smaller size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outperforms larger models in specific benchmarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient adaptation to new tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Florence-2 can be applied across various domains:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Content Understanding**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Automated image captioning for accessibility
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual content moderation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Media asset management
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E-commerce**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Product image analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual search
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated product tagging
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Healthcare**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Medical image analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Diagnostic assistance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Research data processing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security & Surveillance**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Object detection and tracking
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scene understanding
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing Florence-2 with other VLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Florence-2 stands out from other visual language models due to its impressive
    zero-shot capabilities. Unlike models like [Google PaliGemma](https://huggingface.co/blog/paligemma),
    which rely on extensive fine-tuning to adapt to various tasks, Florence-2 works
    right out of the box, as we will see in this lab. It can also compete with larger
    models like GPT-4V and Flamingo, which often have many more parameters but only
    sometimes match Florence-2’s performance. For example, Florence-2 achieves better
    zero-shot results than Kosmos-2 despite having over twice the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In benchmark tests, Florence-2 has shown remarkable performance in tasks like
    COCO captioning and referring expression comprehension. It outperformed models
    like PolyFormer and UNINEXT in object detection and segmentation tasks on the
    [COCO dataset](https://docs.ultralytics.com/datasets/detect/coco/). It is a highly
    competitive choice for real-world applications where both performance and resource
    efficiency are crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Setup and Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our choice of edge device is the Raspberry Pi 5 (Raspi-5). Its robust platform
    is equipped with the Broadcom BCM2712, a 2.4 GHz quad-core 64-bit Arm Cortex-A76
    CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts
    a VideoCore VII GPU, dual 4Kp60 HDMI® outputs with HDR, and a 4Kp60 HEVC decoder.
    Memory options include 4 GB and 8 GB of high-speed LPDDR4X SDRAM, with 8 GB being
    our choice to run Florence-2\. It also features expandable storage via a microSD
    card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid
    State Drives).
  prefs: []
  type: TYPE_NORMAL
- en: For real applications, SSDs are a better option than SD cards.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We suggest installing an Active Cooler, a dedicated clip-on cooling solution
    for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heat sink
    with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably
    under heavy loads, such as running Florense-2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file945.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Environment configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To run [Microsoft Florense-2](https://huggingface.co/microsoft/Florence-2-base)
    on the Raspberry Pi 5, we’ll need a few libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Transformers](https://huggingface.co/docs/transformers/en/index)**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Florence-2 uses the `transformers` library from Hugging Face for model loading
    and inference. This library provides the architecture for working with pre-trained
    vision-language models, making it easy to perform tasks like image captioning,
    object detection, and more. Essentially, `transformers` helps in interacting with
    the model, processing input prompts, and obtaining outputs.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PyTorch**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PyTorch is a deep learning framework that provides the infrastructure needed
    to run the Florence-2 model, which includes tensor operations, GPU acceleration
    (if a GPU is available), and model training/inference functionalities. The Florence-2
    model is trained in PyTorch, and we need it to leverage its functions, layers,
    and computation capabilities to perform inferences on the Raspberry Pi.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timm** (PyTorch Image Models):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Florence-2 uses `timm` to access efficient implementations of vision models
    and pre-trained weights. Specifically, the `timm` library is utilized for the
    **image encoder** part of Florence-2, particularly for managing the DaViT architecture.
    It provides model definitions and optimized code for common vision tasks and allows
    the easy integration of different backbones that are lightweight and suitable
    for edge devices.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Einops**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Einops` is a library for flexible and powerful tensor operations. It makes
    it easy to reshape and manipulate tensor dimensions, which is especially important
    for the multi-modal processing done in Florence-2\. Vision-language models like
    Florence-2 often need to rearrange image data, text embeddings, and visual embeddings
    to align correctly for the transformer blocks, and `einops` simplifies these complex
    operations, making the code more readable and concise.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In short, these libraries enable different essential components of Florence-2:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformers** and **PyTorch** are needed to load the model and run the inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timm** is used to access and efficiently implement the vision encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Einops** helps reshape data, facilitating the integration of visual and text
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these components work together to help Florence-2 run seamlessly on our
    Raspberry Pi, allowing it to perform complex vision-language tasks relatively
    quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering that the Raspberry Pi already has its OS installed, let’s use `SSH`
    to reach it from another computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And check the IP allocated to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`192.168.4.209`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file946.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Updating the Raspberry Pi**'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, ensure your Raspberry Pi is up to date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Initial setup for using PIP**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Install Dependencies**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s set up and activate a **Virtual Environment** for working with Florence-2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Install PyTorch**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s verify that PyTorch is correctly installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file947.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Install Transformers, Timm and Einops**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Install the model**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Jupyter Notebook and Python libraries**'
  prefs: []
  type: TYPE_NORMAL
- en: Installing a Jupyter Notebook to run and test our Python scripts is possible.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Testing the installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Running the Jupyter Notebook on the remote computer
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the above command on the SSH terminal, we can see the local URL address
    to open the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file948.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The notebook with the code used on this initial test can be found on the Lab
    GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[10-florence2_test.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/10-florence2_test.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can access it on the remote computer by entering the Raspberry Pi’s IP address
    and the provided token in a web browser (copy the entire URL from the terminal).
  prefs: []
  type: TYPE_NORMAL
- en: From the Home page, create a new notebook [`Python 3 (ipykernel)` ] and copy
    and paste the [example code](https://huggingface.co/microsoft/Florence-2-base#how-to-get-started-with-the-model)
    from Hugging Face Hub.
  prefs: []
  type: TYPE_NORMAL
- en: The code is designed to run Florence-2 on a given image to perform **object
    detection**. It loads the model, processes an image and a prompt, and then generates
    a response to identify and describe the objects in the image.
  prefs: []
  type: TYPE_NORMAL
- en: The **processor** helps prepare text and image inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **model** takes the processed inputs to generate a meaningful response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **post-processing** step refines the generated output into a more interpretable
    form, like bounding boxes for detected objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This workflow leverages the versatility of Florence-2 to handle **vision-language
    tasks** and is implemented efficiently using PyTorch, Transformers, and related
    image-processing tools.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break down the provided code step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Importing Required Libraries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**requests**: Used to make HTTP requests. In this case, it downloads an image
    from a URL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PIL (Pillow)**: Provides tools for manipulating images. Here, it’s used to
    open the downloaded image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**torch**: PyTorch is imported to handle tensor operations and determine the
    hardware availability (CPU or GPU).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**transformers**: This module provides easy access to Florence-2 by using `AutoProcessor`
    and `AutoModelForCausalLM` to load pre-trained models and process inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the Device and Data Type
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**Device Setup**: The code checks if a CUDA-enabled GPU is available (`torch.cuda.is_available()`).
    The device is set to “cuda:0” if a GPU is available. Otherwise, it defaults to
    `"cpu"` (our case here).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Type Setup**: If a GPU is available, `torch.float16` is chosen, which
    uses half-precision floats to speed up processing and reduce memory usage. On
    the CPU, it defaults to `torch.float32` to maintain compatibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the Model and Processor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Model Initialization**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`AutoModelForCausalLM.from_pretrained()`** loads the pre-trained Florence-2
    model from Microsoft’s repository on Hugging Face. The `torch_dtype` is set according
    to the available hardware (GPU/CPU), and `trust_remote_code=True` allows the use
    of any custom code that might be provided with the model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`.to(device)`** moves the model to the appropriate device (either CPU or
    GPU). In our case, it will be set to `CPU`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processor Initialization**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`AutoProcessor.from_pretrained()`** loads the processor for Florence-2\.
    The processor is responsible for transforming text and image inputs into a format
    the model can work with (e.g., encoding text, normalizing images, etc.).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Prompt Definition**: The string `"<OD>"` is used as a prompt. This refers
    to “Object Detection”, instructing the model to detect objects on the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading and Loading the Image
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Downloading the Image**: The **`requests.get()`** function fetches the image
    from the specified URL. The `stream=True` parameter ensures the image is streamed
    rather than downloaded completely at once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Opening the Image**: **`Image.open()`** opens the image so the model can
    process it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing Inputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Processing Input Data**: The **`processor()`** function processes the text
    (`prompt`) and the image (`image`). The `return_tensors="pt"` argument converts
    the processed data into PyTorch tensors, which are necessary for inputting data
    into the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Moving Inputs to Device**: **`.to(device, torch_dtype)`** moves the inputs
    to the correct device (CPU or GPU) and assigns the appropriate data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the Output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Model Generation**: **`model.generate()`** is used to generate the output
    based on the input data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`input_ids`**: Represents the tokenized form of the prompt.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`pixel_values`**: Contains the processed image data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`max_new_tokens=1024`**: Specifies the maximum number of new tokens to be
    generated in the response. This limits the response length.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`do_sample=False`**: Disables sampling; instead, the generation uses deterministic
    methods (beam search).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`num_beams=3`**: Enables beam search with three beams, which improves output
    quality by considering multiple possibilities during generation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoding the Generated Text
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**Batch Decode**: **`processor.batch_decode()`** decodes the generated IDs
    (tokens) into readable text. The `skip_special_tokens=False` parameter means that
    the output will include any special tokens that may be part of the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post-processing the Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Post-Processing**: **`processor.post_process_generation()`** is called to
    process the generated text further, interpreting it based on the task (`"<OD>"`
    for object detection) and the size of the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This function extracts specific information from the generated text, such as
    bounding boxes for detected objects, making the output more useful for visual
    tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Printing the Output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Finally, **`print(parsed_answer)`** displays the output, which could include
    object detection results, such as bounding box coordinates and labels for the
    detected objects in the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Result
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Running the code, we get as the Parsed Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let’s inspect the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file949.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By the Object Detection result, we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that at least a few objects were detected. We can also implement a
    code to draw the bounding boxes in the find objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**Box (x0, y0, x1, y1)**: Location tokens correspond to the top-left and bottom-right
    corners of a box.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And running
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file950.png)'
  prefs: []
  type: TYPE_IMG
- en: Florence-2 Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Florence-2 is designed to perform a variety of computer vision and vision-language
    tasks through `prompts`. These tasks can be activated by providing a specific
    textual prompt to the model, as we saw with `<OD>` (Object Detection).
  prefs: []
  type: TYPE_NORMAL
- en: Florence-2’s versatility comes from combining these prompts, allowing us to
    guide the model’s behavior to perform specific vision tasks. Changing the prompt
    allows us to adapt Florence-2 to different tasks without needing task-specific
    modifications in the architecture. This capability directly results from Florence-2’s
    unified model architecture and large-scale multi-task training on the FLD-5B dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the key tasks that Florence-2 can perform, along with example
    prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: Object Detection (OD)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prompt**: `"<OD>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description**: Identifies objects in an image and provides bounding boxes
    for each detected object. This task is helpful for applications like visual inspection,
    surveillance, and general object recognition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image Captioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prompt**: `"<CAPTION>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description**: Generates a textual description for an input image. This task
    helps the model describe what is happening in the image, providing a human-readable
    caption for content understanding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detailed Captioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prompt**: `"<DETAILED_CAPTION>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description**: Generates a more detailed caption with more nuanced information
    about the scene, such as the objects present and their relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual Grounding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prompt**: `"<CAPTION_TO_PHRASE_GROUNDING>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description**: Links a textual description to specific regions in an image.
    For example, given a prompt like “a green car,” the model highlights where the
    green car is in the image. This is useful for human-computer interaction, where
    you must find specific objects based on text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prompt**: `"<REFERRING_EXPRESSION_SEGMENTATION>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description**: Performs segmentation based on a referring expression, such
    as “the blue cup.” The model identifies and segments the specific region containing
    the object mentioned in the prompt (all related pixels).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense Region Captioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prompt**: `"<DENSE_REGION_CAPTION>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description**: Provides captions for multiple regions within an image, offering
    a detailed breakdown of all visible areas, including different objects and their
    relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OCR with Region
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prompt**: `"<OCR_WITH_REGION>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description**: Performs Optical Character Recognition (OCR) on an image and
    provides bounding boxes for the detected text. This is useful for extracting and
    locating textual information in images, such as reading signs, labels, or other
    forms of text in images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phrase Grounding for Specific Expressions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prompt**: `"<CAPTION_TO_PHRASE_GROUNDING>"` along with a specific expression,
    such as `"a wine glass"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description**: Locates the area in the image that corresponds to a specific
    textual phrase. This task allows for identifying particular objects or elements
    when prompted with a word or keyword.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open Vocabulary Object Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prompt**: `"<OPEN_VOCABULARY_OD>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description**: The model can detect objects without being restricted to a
    predefined list of classes, making it helpful in recognizing a broader range of
    items based on general visual understanding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring computer vision and vision-language tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For exploration, all codes can be found on the GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[20-florence_2.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/20-florence_2.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s use a couple of images created by Dall-E and upload them to the Rasp-5
    (FileZilla can be used for that). The images will be saved on a sub-folder named
    `images` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file951.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s create a function to facilitate our exploration and to keep track of
    the latency of the model for different tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Caption
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**1\. Dogs and Cats**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '**2\. Table**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Detailed Caption
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**1\. Dogs and Cats**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '**2\. Table**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: More Detailed Caption
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**1\. Dogs and Cats**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '**2\. Table**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We can note that the more detailed the caption task, the longer the latency
    and the possibility of mistakes (like “The image shows a group of four cats and
    a dog in a garden”, instead of two dogs and three cats).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Object Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can run the same previous function for object detection using the prompt
    `<OD>`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Only by the labels `[''cat,'' ''cat,'' ''cat,'' ''dog,'' ''dog'']` is it possible
    to see that the main objects in the image were captured. Let’s apply the function
    used before to draw the bounding boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file952.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s also do it with the Table image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file953.png)'
  prefs: []
  type: TYPE_IMG
- en: Dense Region Caption
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is possible to mix the classic Object Detection with the Caption task in
    specific sub-regions of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file954.png)'
  prefs: []
  type: TYPE_IMG
- en: Caption to Phrase Grounding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With this task, we can enter with a caption, such as “a wine glass”, “a wine
    bottle,” or “a half orange,” and Florence-2 will localize the object in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file955.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Cascade Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can also enter the image caption as the input text to push Florence-2 to
    find more objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Changing the task_prompt among `<CAPTION,>` `<DETAILED_CAPTION>` and `<MORE_DETAILED_CAPTION>`,
    we will get more objects in the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file956.png)'
  prefs: []
  type: TYPE_IMG
- en: Open Vocabulary Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`<OPEN_VOCABULARY_DETECTION>` allows Florence-2 to detect recognizable objects
    in an image without relying on a predefined list of categories, making it a versatile
    tool for identifying various items that may not have been explicitly labeled during
    training. Unlike `<CAPTION_TO_PHRASE_GROUNDING>`, which requires a specific text
    phrase to locate and highlight a particular object in an image, `<OPEN_VOCABULARY_DETECTION>`
    performs a broad scan to find and classify all objects present.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes `<OPEN_VOCABULARY_DETECTION>` particularly useful for applications
    where you need a comprehensive overview of everything in an image without prior
    knowledge of what to expect. Enter with a text describing specific objects not
    previously detected, resulting in their detection. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file957.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: Trying to use Florence-2 to find objects that were not found can leads
    to mistakes (see examples on the Notebook).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Referring expression segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can also segment a specific object in the image and give its description
    (caption), such as “a wine bottle” on the table image or “a German Sheppard” on
    the dogs_cats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Referring expression segmentation results format: `{''<REFERRING_EXPRESSION_SEGMENTATION>'':
    {''Polygons'': [[[polygon]], ...], ''labels'': ['''', '''', ...]}}`, one object
    is represented by a list of polygons. each polygon is `[x1, y1, x2, y2, ..., xn,
    yn]`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Polygon (x1, y1, …, xn, yn)**: Location tokens represent the vertices of
    a polygon in clockwise order.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'So, let’s first create a function to plot the segmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can run the functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file958.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Region to Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With this task, it is also possible to give the object coordinates in the image
    to segment it. The input format is `'<loc_x1><loc_y1><loc_x2><loc_y2>', [x1, y1,
    x2, y2]` , which is the quantized coordinates in [0, 999].
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, when running the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The results were:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the bboxes rounded coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We got the segmentation of the object on those coordinates (Latency: 83 seconds):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file959.png)'
  prefs: []
  type: TYPE_IMG
- en: Region to Texts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can also give the region (coordinates and ask for a caption):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The model identified an orange in that region. Let’s ask for a description:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the description did not provide more details, but it could. Try
    another example.
  prefs: []
  type: TYPE_NORMAL
- en: OCR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With Florence-2, we can perform Optical Character Recognition (OCR) on an image,
    getting what is written on it (`task_prompt = '<OCR>'` and also get the bounding
    boxes (location) for the detected text (`ask_prompt = '<OCR_WITH_REGION>'`). Those
    tasks can help extract and locate textual information in images, such as reading
    signs, labels, or other forms of text in images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s upload a flyer from a talk in Brazil to Raspi. Let’s test works in another
    language, here Portuguese):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file960.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s examine the image with `''<MORE_DETAILED_CAPTION>''` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The description is very accurate. Let’s get to the more important words with
    the task OCR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s locate the words in the flyer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also create a function to draw bounding boxes around the detected words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '![](../media/file961.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can inspect the detected words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Latency Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The latency observed for different tasks using Florence-2 on the Raspberry
    Pi (Raspi-5) varied depending on the complexity of the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image Captioning**: It took approximately 16-17 seconds to generate a caption
    for an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detailed Captioning**: Increased latency to around 25-27 seconds, requiring
    generating more nuanced scene descriptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More Detailed Captioning**: It took about 32-50 seconds, and the latency
    increased as the description grew more complex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object Detection**: It took approximately 20-41 seconds, depending on the
    image’s complexity and the number of detected objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual Grounding**: Approximately 15-16 seconds to localize specific objects
    based on textual prompts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OCR (Optical Character Recognition)**: Extracting text from an image took
    around 37-38 seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Segmentation and Region to Segmentation**: Segmentation tasks took considerably
    longer, with a latency of around 83-207 seconds, depending on the complexity and
    the number of regions to be segmented.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These latency times highlight the resource constraints of edge devices like
    the Raspberry Pi and emphasize the need to optimize the model and the environment
    to achieve real-time performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file962.png)'
  prefs: []
  type: TYPE_IMG
- en: Running complex tasks can use all 8 GB of the Raspi-5’s memory. For example,
    the above screenshot during the Florence OD task shows 4 CPUs at full speed and
    over 5 GB of memory in use. Consider increasing the SWAP memory to 2 GB.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Checking the CPU temperature with `vcgencmd measure_temp` , showed that temperature
    can go up to +80oC.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As explored in this lab, Florence supports many tasks out of the box, including
    captioning, object detection, OCR, and more. However, like other pre-trained foundational
    models, Florence-2 may need domain-specific knowledge. For example, it may need
    to improve with medical or satellite imagery. In such cases, **fine-tuning** with
    a custom dataset is necessary. The Roboflow tutorial, [How to Fine-tune Florence-2
    for Object Detection Tasks](https://blog.roboflow.com/fine-tune-florence-2-object-detection/),
    shows how to fine-tune Florence-2 on object detection datasets to improve model
    performance for our specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the above tutorial, it is possible to fine-tune the Florence-2 model
    to detect boxes and wheels used in previous labs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file963.png)'
  prefs: []
  type: TYPE_IMG
- en: It is important to note that after fine-tuning, the model can still detect classes
    that don’t belong to our custom dataset, like cats, dogs, grapes, etc, as seen
    before).
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete fine-tuning project using a previously annotated dataset in Roboflow
    and executed on CoLab can be found in the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In another example, in the post, [Fine-tuning Florence-2 - Microsoft’s Cutting-edge
    Vision Language Models](https://huggingface.co/blog/finetune-florence2), the authors
    show an example of fine-tuning Florence on `DocVQA`. The authors report that Florence
    2 can perform visual question answering (VQA), but the released models don’t include
    VQA capability.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Florence-2 offers a versatile and powerful approach to vision-language tasks
    at the edge, providing performance that rivals larger, task-specific models, such
    as YOLO for object detection, BERT/RoBERTa for text analysis, and specialized
    OCR models.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to its multi-modal transformer architecture, Florence-2 is more flexible
    than YOLO in terms of the tasks it can handle. These include object detection,
    image captioning, and visual grounding.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike **BERT**, which focuses purely on language, Florence-2 integrates vision
    and language, allowing it to excel in applications that require both modalities,
    such as image captioning and visual grounding.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, while traditional **OCR models** such as Tesseract and EasyOCR are
    designed solely for recognizing and extracting text from images, Florence-2’s
    OCR capabilities are part of a broader framework that includes contextual understanding
    and visual-text alignment. This makes it particularly useful for scenarios that
    require both reading text and interpreting its context within images.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, Florence-2 stands out for its ability to seamlessly integrate various
    vision-language tasks into a unified model that is efficient enough to run on
    edge devices like the Raspberry Pi. This makes it a compelling choice for developers
    and researchers exploring AI applications at the edge.
  prefs: []
  type: TYPE_NORMAL
- en: Key Advantages of Florence-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Unified Architecture**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Single model handles multiple vision tasks vs. specialized models (YOLO, BERT,
    Tesseract)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Eliminates the need for multiple model deployments and integrations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistent API and interface across tasks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance Comparison**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Object Detection: Comparable to YOLOv8 (~37.5 mAP on COCO vs. YOLOv8’s ~39.7
    mAP) despite being general-purpose'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text Recognition: Handles multiple languages effectively like specialized OCR
    models (Tesseract, EasyOCR)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language Understanding: Integrates BERT-like capabilities for text processing
    while adding visual context'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource Efficiency**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Base model (232M parameters) achieves strong results despite smaller size
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs effectively on edge devices (Raspberry Pi)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Single model deployment vs. multiple specialized models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Trade-offs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Performance vs. Specialized Models**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: YOLO series may offer faster inference for pure object detection
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized OCR models might handle complex document layouts better
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT/RoBERTa provide deeper language understanding for text-only tasks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource Requirements**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Higher latency on edge devices (15-200s depending on task)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires careful memory management on Raspberry Pi
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It may need optimization for real-time applications
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment Considerations**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initial setup is more complex than single-purpose models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires understanding of multiple task types and prompts
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning curve for optimal prompt engineering
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Best Use Cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Resource-Constrained Environments**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edge devices requiring multiple vision capabilities
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Systems with limited storage/deployment capacity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications needing flexible vision processing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-modal Applications**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Content moderation systems
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessibility tools
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Document analysis workflows
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rapid Prototyping**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quick deployment of vision capabilities
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing multiple vision tasks without separate models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Proof-of-concept development
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Future Implications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Florence-2 represents a shift toward unified vision models that could eventually
    replace task-specific architectures in many applications. While specialized models
    maintain advantages in specific scenarios, the convenience and efficiency of unified
    models like Florence-2 make them increasingly attractive for real-world deployments.
  prefs: []
  type: TYPE_NORMAL
- en: The lab demonstrates Florence-2’s viability on edge devices, suggesting future
    IoT, mobile computing, and embedded systems applications where deploying multiple
    specialized models would be impractical.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[10-florence2_test.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/10-florence2_test.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20-florence_2.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/20-florence_2.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
