<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Concept</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Concept</h1>
<blockquote>原文：<a href="https://dafriedman97.github.io/mlbook/content/c1/concept.html">https://dafriedman97.github.io/mlbook/content/c1/concept.html</a></blockquote>

<div class="math notranslate nohighlight">
\[
\newcommand{\sumN}{\sum_{n = 1}^N} 
\newcommand{\sumn}{\sum_n} 
\newcommand{\bx}{\mathbf{x}} 
\newcommand{\bbeta}{\boldsymbol{\beta}} 
\newcommand{\btheta}{\boldsymbol{\theta}} 
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}} 
\newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}} 
\newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}} 
\newcommand{\by}{\mathbf{y}} 
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\]</div>
<div class="section" id="model-structure">
<h2>Model Structure</h2>
<p><em>Linear regression</em> is a relatively simple method that is extremely widely-used. It is also a great stepping stone for more sophisticated methods, making it a natural algorithm to study first.</p>
<p>In linear regression, the target variable <span class="math notranslate nohighlight">\(y\)</span> is assumed to follow a linear function of one or more predictor variables, <span class="math notranslate nohighlight">\(x_1, \dots, x_D\)</span>, plus some random error. Specifically, we assume the model for the <span class="math notranslate nohighlight">\(n^\text{th}\)</span> observation in our sample is of the form</p>
<div class="math notranslate nohighlight">
\[
y_n = \beta_0 + \beta_1 x_{n1} + \dots + \beta_Dx_{nD} + \epsilon_n. 
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept term, <span class="math notranslate nohighlight">\(\beta_1\)</span> through <span class="math notranslate nohighlight">\(\beta_D\)</span> are the coefficients on our feature variables, and <span class="math notranslate nohighlight">\(\epsilon\)</span> is an error term that represents the difference between the true <span class="math notranslate nohighlight">\(y\)</span> value and the linear function of the predictors. Note that the terms with an <span class="math notranslate nohighlight">\(n\)</span> in the subscript differ between observations while the terms without (namely the <span class="math notranslate nohighlight">\(\beta\text{s}\)</span>) do not.</p>
<p>The math behind linear regression often becomes easier when we use vectors to represent our predictors and coefficients. Let’s define <span class="math notranslate nohighlight">\(\bx_n\)</span> and <span class="math notranslate nohighlight">\(\bbeta\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\bx_n &amp;= \begin{pmatrix} 1 &amp; x_{n1} &amp; \dots &amp; x_{nD} \end{pmatrix}^\top \\
\bbeta &amp;= \begin{pmatrix} \beta_0 &amp; \beta_1 &amp; \dots &amp; \beta_D \end{pmatrix}^\top.
\end{align}
\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\bx_n\)</span> includes a leading 1, corresponding to the intercept term <span class="math notranslate nohighlight">\(\beta_0\)</span>. Using these definitions, we can equivalently express <span class="math notranslate nohighlight">\(y_n\)</span> as</p>
<div class="math notranslate nohighlight">
\[
y_n = \bbeta^\top \bx_n + \epsilon_n.
\]</div>
<p>Below is an example of a dataset designed for linear regression. The input variable is generated randomly and the target variable is generated as a linear combination of that input variable plus an error term.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># generate data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">beta0</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>
<span class="n">beta1</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">beta0</span> <span class="o">+</span> <span class="n">beta1</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">e</span>
<span class="n">true_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">true_y</span> <span class="o">=</span> <span class="n">beta0</span> <span class="o">+</span> <span class="n">beta1</span><span class="o">*</span><span class="n">true_x</span>

<span class="c1"># plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">40</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Data'</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">true_x</span><span class="p">,</span> <span class="n">true_y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'True Model'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'x'</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s2">"$y = </span><span class="si">{</span><span class="n">beta0</span><span class="si">}</span><span class="s2"> + $</span><span class="si">{</span><span class="n">beta1</span><span class="si">}</span><span class="s2">$x + \epsilon$"</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/concept_2_0.png" src="../Images/60ca0e10dab7626e0f855f43f7f346e4.png" data-original-src="https://dafriedman97.github.io/mlbook/_images/concept_2_0.png"/>
</div>
</div>
</div>
<div class="section" id="parameter-estimation">
<h2>Parameter Estimation</h2>
<p>The previous section covers the entire structure we assume our data follows in linear regression. The machine learning task is then to estimate the parameters in <span class="math notranslate nohighlight">\(\bbeta\)</span>. These estimates are represented by <span class="math notranslate nohighlight">\(\hat{\beta}_0, \dots, \hat{\beta}_D\)</span> or <span class="math notranslate nohighlight">\(\bbetahat\)</span>. The estimates give us <em>fitted values</em> for our target variable, represented by <span class="math notranslate nohighlight">\(\hat{y}_n\)</span>.</p>
<p>This task can be accomplished in two ways which, though slightly different conceptually, are identical mathematically. The first approach is through the lens of <em>minimizing loss</em>. A common practice in machine learning is to choose a loss function that defines how well a model with a given set of parameter estimates the observed data. The most common loss function for linear regression is squared error loss. This says the <em>loss</em> of our model is proportional to the sum of squared differences between the true <span class="math notranslate nohighlight">\(y_n\)</span> values and the fitted values, <span class="math notranslate nohighlight">\(\hat{y}_n\)</span>. We then <em>fit</em> the model by finding the estimates <span class="math notranslate nohighlight">\(\bbetahat\)</span> that minimize this loss function. This approach is covered in the subsection <a class="reference internal" href="s1/loss_minimization.html"><span class="doc">Approach 1: Minimizing Loss</span></a>.</p>
<p>The second approach is through the lens of <em>maximizing likelihood</em>. Another common practice in machine learning is to model the target as a random variable whose distribution depends on one or more parameters, and then find the parameters that maximize its likelihood. Under this approach, we will represent the target with <span class="math notranslate nohighlight">\(Y_n\)</span> since we are treating it as a random variable. The most common model for <span class="math notranslate nohighlight">\(Y_n\)</span> in linear regression is a Normal random variable with mean <span class="math notranslate nohighlight">\(E(Y_n) = \bbeta^\top \bx_n\)</span>. That is, we assume</p>
<div class="math notranslate nohighlight">
\[
Y_n|\bx_n \sim \mathcal{N}(\bbeta^\top \bx_n, \sigma^2),
\]</div>
<p>and we find the values of <span class="math notranslate nohighlight">\(\bbetahat\)</span> to maximize the likelihood. This approach is covered in subsection <a class="reference internal" href="s1/likelihood_maximization.html"><span class="doc">Approach 2: Maximizing Likelihood</span></a>.</p>
<p>Once we’ve estimated <span class="math notranslate nohighlight">\(\bbeta\)</span>, our model is <em>fit</em> and we can make predictions. The below graph is the same as the one above but includes our estimated line-of-best-fit, obtained by calculating <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># generate data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">beta0</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>
<span class="n">beta1</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">beta0</span> <span class="o">+</span> <span class="n">beta1</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">e</span>
<span class="n">true_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">true_y</span> <span class="o">=</span> <span class="n">beta0</span> <span class="o">+</span> <span class="n">beta1</span><span class="o">*</span><span class="n">true_x</span>

<span class="c1"># estimate model </span>
<span class="n">beta1_hat</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span><span class="o">/</span><span class="nb">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">beta0_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta1_hat</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">fit_y</span> <span class="o">=</span> <span class="n">beta0_hat</span> <span class="o">+</span> <span class="n">beta1_hat</span><span class="o">*</span><span class="n">true_x</span>

<span class="c1"># plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">40</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Data'</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">true_x</span><span class="p">,</span> <span class="n">true_y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'True Model'</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">true_x</span><span class="p">,</span> <span class="n">fit_y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'purple'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Estimated Model'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'x'</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s2">"Linear Regression for $y = </span><span class="si">{</span><span class="n">beta0</span><span class="si">}</span><span class="s2"> + $</span><span class="si">{</span><span class="n">beta1</span><span class="si">}</span><span class="s2">$x + \epsilon$"</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/concept_4_0.png" src="../Images/c220c51daba256b0e1531ca574fb37aa.png" data-original-src="https://dafriedman97.github.io/mlbook/_images/concept_4_0.png"/>
</div>
</div>
</div>
<div class="section" id="extensions-of-ordinary-linear-regression">
<h2>Extensions of Ordinary Linear Regression</h2>
<p>There are many important extensions to linear regression which make the model more flexible. Those include <a class="reference internal" href="../c2/s1/regularized.html"><span class="doc">Regularized Regression</span></a>—which balances the bias-variance tradeoff for high-dimensional regression models—<a class="reference internal" href="../c2/s1/bayesian.html"><span class="doc">Bayesian Regression</span></a>—which allows for prior distributions on the coefficients—and <a class="reference internal" href="../c2/s1/GLMs.html"><span class="doc">GLMs</span></a>—which introduce non-linearity to regression models. These extensions are discussed in the next chapter.</p>
<div class="toctree-wrapper compound">
</div>
</div>
&#13;

<h2>Model Structure</h2>
<p><em>Linear regression</em> is a relatively simple method that is extremely widely-used. It is also a great stepping stone for more sophisticated methods, making it a natural algorithm to study first.</p>
<p>In linear regression, the target variable <span class="math notranslate nohighlight">\(y\)</span> is assumed to follow a linear function of one or more predictor variables, <span class="math notranslate nohighlight">\(x_1, \dots, x_D\)</span>, plus some random error. Specifically, we assume the model for the <span class="math notranslate nohighlight">\(n^\text{th}\)</span> observation in our sample is of the form</p>
<div class="math notranslate nohighlight">
\[
y_n = \beta_0 + \beta_1 x_{n1} + \dots + \beta_Dx_{nD} + \epsilon_n. 
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept term, <span class="math notranslate nohighlight">\(\beta_1\)</span> through <span class="math notranslate nohighlight">\(\beta_D\)</span> are the coefficients on our feature variables, and <span class="math notranslate nohighlight">\(\epsilon\)</span> is an error term that represents the difference between the true <span class="math notranslate nohighlight">\(y\)</span> value and the linear function of the predictors. Note that the terms with an <span class="math notranslate nohighlight">\(n\)</span> in the subscript differ between observations while the terms without (namely the <span class="math notranslate nohighlight">\(\beta\text{s}\)</span>) do not.</p>
<p>The math behind linear regression often becomes easier when we use vectors to represent our predictors and coefficients. Let’s define <span class="math notranslate nohighlight">\(\bx_n\)</span> and <span class="math notranslate nohighlight">\(\bbeta\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\bx_n &amp;= \begin{pmatrix} 1 &amp; x_{n1} &amp; \dots &amp; x_{nD} \end{pmatrix}^\top \\
\bbeta &amp;= \begin{pmatrix} \beta_0 &amp; \beta_1 &amp; \dots &amp; \beta_D \end{pmatrix}^\top.
\end{align}
\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\bx_n\)</span> includes a leading 1, corresponding to the intercept term <span class="math notranslate nohighlight">\(\beta_0\)</span>. Using these definitions, we can equivalently express <span class="math notranslate nohighlight">\(y_n\)</span> as</p>
<div class="math notranslate nohighlight">
\[
y_n = \bbeta^\top \bx_n + \epsilon_n.
\]</div>
<p>Below is an example of a dataset designed for linear regression. The input variable is generated randomly and the target variable is generated as a linear combination of that input variable plus an error term.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># generate data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">beta0</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>
<span class="n">beta1</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">beta0</span> <span class="o">+</span> <span class="n">beta1</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">e</span>
<span class="n">true_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">true_y</span> <span class="o">=</span> <span class="n">beta0</span> <span class="o">+</span> <span class="n">beta1</span><span class="o">*</span><span class="n">true_x</span>

<span class="c1"># plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">40</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Data'</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">true_x</span><span class="p">,</span> <span class="n">true_y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'True Model'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'x'</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s2">"$y = </span><span class="si">{</span><span class="n">beta0</span><span class="si">}</span><span class="s2"> + $</span><span class="si">{</span><span class="n">beta1</span><span class="si">}</span><span class="s2">$x + \epsilon$"</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/concept_2_0.png" src="../Images/60ca0e10dab7626e0f855f43f7f346e4.png" data-original-src="https://dafriedman97.github.io/mlbook/_images/concept_2_0.png"/>
</div>
</div>
&#13;

<h2>Parameter Estimation</h2>
<p>The previous section covers the entire structure we assume our data follows in linear regression. The machine learning task is then to estimate the parameters in <span class="math notranslate nohighlight">\(\bbeta\)</span>. These estimates are represented by <span class="math notranslate nohighlight">\(\hat{\beta}_0, \dots, \hat{\beta}_D\)</span> or <span class="math notranslate nohighlight">\(\bbetahat\)</span>. The estimates give us <em>fitted values</em> for our target variable, represented by <span class="math notranslate nohighlight">\(\hat{y}_n\)</span>.</p>
<p>This task can be accomplished in two ways which, though slightly different conceptually, are identical mathematically. The first approach is through the lens of <em>minimizing loss</em>. A common practice in machine learning is to choose a loss function that defines how well a model with a given set of parameter estimates the observed data. The most common loss function for linear regression is squared error loss. This says the <em>loss</em> of our model is proportional to the sum of squared differences between the true <span class="math notranslate nohighlight">\(y_n\)</span> values and the fitted values, <span class="math notranslate nohighlight">\(\hat{y}_n\)</span>. We then <em>fit</em> the model by finding the estimates <span class="math notranslate nohighlight">\(\bbetahat\)</span> that minimize this loss function. This approach is covered in the subsection <a class="reference internal" href="s1/loss_minimization.html"><span class="doc">Approach 1: Minimizing Loss</span></a>.</p>
<p>The second approach is through the lens of <em>maximizing likelihood</em>. Another common practice in machine learning is to model the target as a random variable whose distribution depends on one or more parameters, and then find the parameters that maximize its likelihood. Under this approach, we will represent the target with <span class="math notranslate nohighlight">\(Y_n\)</span> since we are treating it as a random variable. The most common model for <span class="math notranslate nohighlight">\(Y_n\)</span> in linear regression is a Normal random variable with mean <span class="math notranslate nohighlight">\(E(Y_n) = \bbeta^\top \bx_n\)</span>. That is, we assume</p>
<div class="math notranslate nohighlight">
\[
Y_n|\bx_n \sim \mathcal{N}(\bbeta^\top \bx_n, \sigma^2),
\]</div>
<p>and we find the values of <span class="math notranslate nohighlight">\(\bbetahat\)</span> to maximize the likelihood. This approach is covered in subsection <a class="reference internal" href="s1/likelihood_maximization.html"><span class="doc">Approach 2: Maximizing Likelihood</span></a>.</p>
<p>Once we’ve estimated <span class="math notranslate nohighlight">\(\bbeta\)</span>, our model is <em>fit</em> and we can make predictions. The below graph is the same as the one above but includes our estimated line-of-best-fit, obtained by calculating <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># generate data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">beta0</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>
<span class="n">beta1</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">beta0</span> <span class="o">+</span> <span class="n">beta1</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">e</span>
<span class="n">true_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">true_y</span> <span class="o">=</span> <span class="n">beta0</span> <span class="o">+</span> <span class="n">beta1</span><span class="o">*</span><span class="n">true_x</span>

<span class="c1"># estimate model </span>
<span class="n">beta1_hat</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span><span class="o">/</span><span class="nb">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">beta0_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta1_hat</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">fit_y</span> <span class="o">=</span> <span class="n">beta0_hat</span> <span class="o">+</span> <span class="n">beta1_hat</span><span class="o">*</span><span class="n">true_x</span>

<span class="c1"># plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">40</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Data'</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">true_x</span><span class="p">,</span> <span class="n">true_y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'True Model'</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">true_x</span><span class="p">,</span> <span class="n">fit_y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'purple'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Estimated Model'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'x'</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s2">"Linear Regression for $y = </span><span class="si">{</span><span class="n">beta0</span><span class="si">}</span><span class="s2"> + $</span><span class="si">{</span><span class="n">beta1</span><span class="si">}</span><span class="s2">$x + \epsilon$"</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/concept_4_0.png" src="../Images/c220c51daba256b0e1531ca574fb37aa.png" data-original-src="https://dafriedman97.github.io/mlbook/_images/concept_4_0.png"/>
</div>
</div>
&#13;

<h2>Extensions of Ordinary Linear Regression</h2>
<p>There are many important extensions to linear regression which make the model more flexible. Those include <a class="reference internal" href="../c2/s1/regularized.html"><span class="doc">Regularized Regression</span></a>—which balances the bias-variance tradeoff for high-dimensional regression models—<a class="reference internal" href="../c2/s1/bayesian.html"><span class="doc">Bayesian Regression</span></a>—which allows for prior distributions on the coefficients—and <a class="reference internal" href="../c2/s1/GLMs.html"><span class="doc">GLMs</span></a>—which introduce non-linearity to regression models. These extensions are discussed in the next chapter.</p>
<div class="toctree-wrapper compound">
</div>
    
</body>
</html>