- en: ML Operations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习操作
- en: '*DALL·E 3 Prompt: Create a detailed, wide rectangular illustration of an AI
    workflow. The image should showcase the process across six stages, with a flow
    from left to right: 1\. Data collection, with diverse individuals of different
    genders and descents using a variety of devices like laptops, smartphones, and
    sensors to gather data. 2\. Data processing, displaying a data center with active
    servers and databases with glowing lights. 3\. Model training, represented by
    a computer screen with code, neural network diagrams, and progress indicators.
    4\. Model evaluation, featuring people examining data analytics on large monitors.
    5\. Deployment, where the AI is integrated into robotics, mobile apps, and industrial
    equipment. 6\. Monitoring, showing professionals tracking AI performance metrics
    on dashboards to check for accuracy and concept drift over time. Each stage should
    be distinctly marked and the style should be clean, sleek, and modern with a dynamic
    and informative color scheme.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3 提示：创建一个详细、宽矩形的人工智能工作流程插图。图像应展示六个阶段的过程，从左到右流动：1. 数据收集，不同性别和血统的多样化个体使用各种设备（如笔记本电脑、智能手机和传感器）收集数据。2.
    数据处理，显示一个数据中心，活跃的服务器和带有发光灯的数据库。3. 模型训练，由带有代码、神经网络图和进度指示器的计算机屏幕表示。4. 模型评估，展示人们在大型显示器上检查数据分析。5.
    部署，AI集成到机器人、移动应用和工业设备中。6. 监控，显示专业人士在仪表板上跟踪AI性能指标，以检查准确性和概念漂移随时间的变化。每个阶段应清晰标记，风格应简洁、现代，并具有动态和富有信息量的色彩方案。*'
- en: '![](../media/file207.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file207.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*Why do machine learning prototypes that work perfectly in development often
    fail catastrophically when deployed to production environments?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么在开发环境中运行完美的机器学习原型在部署到生产环境时往往失败得非常惨烈？*'
- en: The transition from prototype models to reliable production systems presents
    significant engineering challenges. Research models trained on clean datasets
    encounter production environments with shifting data distributions, evolving user
    behaviors, and unexpected system failures. Unlike traditional software that executes
    deterministic logic, machine learning systems exhibit probabilistic behavior that
    degrades silently as real-world conditions diverge from training assumptions.
    This instability requires operational practices that detect performance degradation
    before affecting users, automatically retrain models as data evolves, and maintain
    system reliability despite prediction uncertainty. Success demands engineering
    disciplines that bridge experimental validation and production reliability, enabling
    organizations to deploy models that remain effective throughout their operational
    lifespan.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从原型模型到可靠的生产系统的过渡带来了重大的工程挑战。在干净数据集上训练的研究模型会遇到具有变化的数据分布、演变的用户行为和意外系统故障的生产环境。与执行确定性逻辑的传统软件不同，机器学习系统表现出概率行为，当现实条件与训练假设相偏离时，这种行为会无声地退化。这种不稳定性需要操作实践来检测性能退化，在影响用户之前自动重新训练模型，并在预测不确定性下保持系统可靠性。成功需要将实验验证和生产可靠性连接起来的工程学科，使组织能够部署在整个运营生命周期中保持有效的模型。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Differentiate between traditional software failures and ML system silent failures
    to explain why MLOps emerged as a distinct engineering discipline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分传统软件故障和机器学习系统的静默故障，以解释为什么MLOps成为一门独立的工程学科
- en: Analyze technical debt patterns (boundary erosion, correction cascades, data
    dependencies) in ML systems and propose systematic engineering solutions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析机器学习系统中的技术债务模式（边界侵蚀、纠正级联、数据依赖），并提出系统性的工程解决方案
- en: Design CI/CD pipelines that address ML-specific challenges including model validation,
    data versioning, and automated retraining workflows
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计解决机器学习特定挑战的CI/CD管道，包括模型验证、数据版本化和自动化重新训练工作流程
- en: Evaluate monitoring strategies for production ML systems that detect both traditional
    system metrics and ML-specific indicators like data drift and prediction confidence
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估用于检测传统系统指标和机器学习特定指标（如数据漂移和预测置信度）的生产机器学习系统的监控策略
- en: Implement deployment patterns for diverse environments including cloud services,
    edge devices, and federated learning systems
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施适用于不同环境（包括云服务、边缘设备和联邦学习系统）的部署模式
- en: Assess organizational maturity levels for effective MLOps implementation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估组织在有效实施MLOps方面的成熟度水平
- en: Compare MLOps adaptations across domains by analyzing how specialized requirements
    (healthcare, embedded systems) reshape operational frameworks
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分析特定要求（如医疗保健、嵌入式系统）如何重塑运营框架来比较MLOps在不同领域的适应性
- en: Create governance frameworks that ensure model reproducibility, auditability,
    and compliance in regulated environments
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建治理框架，确保在受监管环境中模型的可重复性、可审计性和合规性
- en: Introduction to Machine Learning Operations
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习操作简介
- en: 'Traditional software fails loudly with error messages and stack traces; machine
    learning systems fail silently. As introduced in [Chapter 1](ch007.xhtml#sec-introduction),
    the Silent Failure Problem is a defining characteristic of ML systems: performance
    degrades gradually as data distributions shift, user behaviors evolve, and model
    assumptions become outdated, all without raising any alarms. MLOps is the engineering
    discipline designed to make those silent failures visible and manageable. It provides
    the monitoring, automation, and governance required to ensure that data-driven
    systems remain reliable in production, even as the world around them changes.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 传统软件会通过错误信息和堆栈跟踪大声失败；机器学习系统则默默失败。如[第1章](ch007.xhtml#sec-introduction)所述，沉默失败问题是机器学习系统的定义特征：随着数据分布的变化、用户行为的演变和模型假设的过时，性能逐渐下降，而没有任何警报。MLOps是旨在使这些沉默失败变得可见和可管理的工程学科。它提供了监控、自动化和治理，以确保数据驱动系统在周围世界变化的情况下在生产中保持可靠性。
- en: Machine learning systems require more than algorithmic innovation; they need
    systematic engineering practices for reliable production deployment. While [Chapter 14](ch020.xhtml#sec-ondevice-learning)
    explored distributed learning under resource constraints and [Chapter 16](ch022.xhtml#sec-robust-ai)
    established fault tolerance methodologies, the security framework from [Chapter 15](ch021.xhtml#sec-security-privacy)
    becomes essential for production deployment. Machine Learning Operations (MLOps)[1](#fn1)
    provides the disciplinary framework that synthesizes these specialized capabilities
    into coherent production architectures. This operational discipline addresses
    the challenge of translating experimental success into sustainable system performance,
    integrating adaptive learning, security protocols, and resilience mechanisms within
    complex production ecosystems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统需要的不仅仅是算法创新；它们还需要系统性的工程实践以确保可靠的生产部署。虽然[第14章](ch020.xhtml#sec-ondevice-learning)探讨了资源受限下的分布式学习，[第16章](ch022.xhtml#sec-robust-ai)建立了容错方法，但[第15章](ch021.xhtml#sec-security-privacy)中的安全框架对于生产部署变得至关重要。机器学习操作（MLOps）[1](#fn1)提供了将这些专门能力综合成一致生产架构的学科框架。这种运营学科解决了将实验成功转化为可持续系统性能的挑战，将自适应学习、安全协议和弹性机制整合到复杂的生产生态系统中。
- en: MLOps ([Section 13.2.2](ch019.xhtml#sec-ml-operations-mlops-c12b)) systematically
    integrates machine learning methodologies, data science practices, and software
    engineering principles to enable automated, end-to-end lifecycle management. This
    operational paradigm bridges experimental validation and production deployment,
    ensuring that validated models maintain their performance characteristics while
    adapting to real-world operational environments.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps ([第13.2.2节](ch019.xhtml#sec-ml-operations-mlops-c12b)) 系统性地整合了机器学习方法、数据科学实践和软件工程原则，以实现自动化、端到端的生命周期管理。这种运营范式连接了实验验证和生产部署，确保验证过的模型在适应现实世界运营环境的同时保持其性能特征。
- en: Consider deploying a demand prediction system for ridesharing services. While
    controlled experimental validation may demonstrate superior accuracy and latency
    characteristics, production deployment introduces challenges that extend beyond
    algorithmic performance. Data streams exhibit varying quality, temporal patterns
    undergo seasonal variations, and prediction services must satisfy strict availability
    requirements while maintaining real-time response capabilities. MLOps provides
    the framework needed to address these operational complexities.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑部署一个针对共享出行服务的需求预测系统。虽然受控的实验验证可能显示出优越的准确性和延迟特性，但生产部署会引入超出算法性能的挑战。数据流表现出不同的质量，时间模式会经历季节性变化，预测服务必须在满足严格的可用性要求的同时保持实时响应能力。MLOps提供了解决这些运营复杂性的框架。
- en: As an engineering discipline, MLOps establishes standardized protocols, tools,
    and workflows that facilitate the transition of validated models from experimental
    environments to production systems. The discipline promotes collaboration by formalizing
    interfaces and defining responsibilities across traditionally isolated domains,
    including data science, machine learning engineering, and systems operations[2](#fn2).
    This approach enables continuous integration and deployment practices adapted
    for machine learning contexts, supporting iterative model refinement, validation,
    and deployment while preserving system stability and operational reliability.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一门工程学科，MLOps 建立了标准化的协议、工具和工作流程，这些有助于将验证过的模型从实验环境过渡到生产系统。该学科通过正式化接口和定义传统上隔离的领域（包括数据科学、机器学习工程和系统运营）的责任来促进协作[2](#fn2)。这种方法使得适应机器学习环境的持续集成和部署实践成为可能，支持迭代模型优化、验证和部署，同时保持系统稳定性和操作可靠性。
- en: Building on these operational foundations, mature MLOps methodologies transform
    how organizations manage machine learning systems through automation and monitoring
    frameworks. These practices enable continuous model retraining as new data becomes
    available, evaluation of alternative architectures against production baselines,
    controlled deployment of experimental modifications through graduated rollout
    strategies, and real-time performance assessment without compromising operational
    continuity. This operational flexibility ensures sustained model relevance while
    maintaining system reliability standards.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些操作基础之上，成熟的 MLOps 方法论通过自动化和监控框架改变了组织管理机器学习系统的方式。这些实践使得在新数据可用时能够持续进行模型再训练，评估与生产基线不同的架构，通过分阶段推出策略控制实验性修改的部署，以及在不影响操作连续性的情况下进行实时性能评估。这种操作灵活性确保了模型的相关性持续，同时保持系统可靠性标准。
- en: Beyond operational efficiency, MLOps encompasses governance frameworks and accountability
    mechanisms that become critical as systems scale. MLOps standardizes the tracking
    of model versions, data lineage documentation, and configuration parameter management,
    establishing reproducible and auditable artifact trails. This rigor proves essential
    in regulated domains where model interpretability and operational provenance constitute
    compliance requirements.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除去操作效率之外，MLOps 还包括治理框架和问责机制，这些在系统规模扩大时变得至关重要。MLOps 标准化了模型版本跟踪、数据血缘文档和配置参数管理，建立了可重复和可审计的工件轨迹。这种严谨性在模型可解释性和操作溯源构成合规要求的监管领域证明是必不可少的。
- en: The practical benefits of this methodological rigor become evident in organizational
    outcomes. Evidence demonstrates that organizations adopting mature MLOps methodologies
    achieve significant improvements in deployment reliability, accelerated time-to-market
    cycles, and enhanced system maintainability[3](#fn3). The disciplinary framework
    enables sustainable scaling of machine learning systems while preserving the performance
    characteristics validated during benchmarking phases, ensuring operational fidelity
    to experimental results.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法论严谨性的实际益处体现在组织成果上。证据表明，采用成熟的 MLOps 方法论的组织在部署可靠性、缩短上市周期和增强系统可维护性方面取得了显著改进[3](#fn3)。该学科框架使得机器学习系统的可持续扩展成为可能，同时保持基准测试阶段验证的性能特征，确保操作与实验结果的一致性。
- en: This methodology of machine learning operations provides the pathway for transforming
    theoretical innovations into sustainable production capabilities. This chapter
    establishes the engineering foundations needed to bridge the gap between experimentally
    validated systems and operationally reliable production deployments. The analysis
    focuses particularly on centralized cloud computing environments, where monitoring
    infrastructure and management capabilities enable the implementation of mature
    operational practices for large-scale machine learning systems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种机器学习操作方法论为将理论创新转化为可持续的生产能力提供了途径。本章建立了连接实验验证系统和操作可靠的生产部署之间差距所需的工程基础。分析特别关注集中式云计算环境，其中监控基础设施和管理能力使得实施成熟操作实践成为可能，以支持大规模机器学习系统。
- en: While [Chapter 10](ch016.xhtml#sec-model-optimizations) and [Chapter 9](ch015.xhtml#sec-efficient-ai)
    establish optimization foundations, this chapter extends these techniques to production
    contexts requiring continuous maintenance and monitoring. The empirical benchmarking
    approaches established in [Chapter 12](ch018.xhtml#sec-benchmarking-ai) provide
    the methodological foundation for production performance assessment, while system
    reliability patterns emerge as critical determinants of operational availability.
    MLOps integrates these diverse technical foundations into unified operational
    workflows, systematically addressing the fundamental challenge of transitioning
    from model development to sustainable production deployment.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然[第10章](ch016.xhtml#sec-model-optimizations)和[第9章](ch015.xhtml#sec-efficient-ai)建立了优化基础，但本章将这些技术扩展到需要持续维护和监控的生产环境。在第12章[（ch018.xhtml#sec-benchmarking-ai）]中建立的经验基准方法为生产性能评估提供了方法论基础，而系统可靠性模式成为运营可用性的关键决定因素。MLOps将这些不同的技术基础整合到统一的运维工作流程中，系统地解决从模型开发到可持续生产部署的基本挑战。
- en: This chapter examines the theoretical foundations and practical motivations
    underlying MLOps, traces its disciplinary evolution from DevOps methodologies,
    and identifies the principal challenges and established practices that inform
    its adoption in contemporary machine learning system architectures.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了MLOps的理论基础和实践动机，追溯了其从DevOps方法论中的学科演变，并确定了影响其在当代机器学习系统架构中采用的主要挑战和既定实践。
- en: Historical Context
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 历史背景
- en: Understanding this evolution from DevOps to MLOps clarifies why traditional
    operational practices require adaptation for machine learning systems. The following
    examination of this historical development reveals the specific challenges that
    motivated MLOps as a distinct discipline.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 理解从DevOps到MLOps的这种演变，可以阐明为什么传统的运维实践需要适应机器学习系统。以下对这一历史发展的考察揭示了推动MLOps作为一门独立学科的具体挑战。
- en: MLOps has its roots in DevOps, a set of practices that combines software development
    (Dev) and IT operations (Ops) to shorten the development lifecycle and support
    the continuous delivery of high-quality software. DevOps and MLOps both emphasize
    automation, collaboration, and iterative improvement. However, while DevOps emerged
    to address challenges in software deployment and operational management, MLOps
    evolved in response to the unique complexities of machine learning workflows,
    especially those involving data-driven components ([Breck et al. 2017b](ch058.xhtml#ref-breck2020ml)).
    Understanding this evolution is important for appreciating the motivations and
    structure of modern ML systems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps的根源在于DevOps，它是一套将软件开发（Dev）和IT运维（Ops）相结合的实践，以缩短开发周期并支持高质量软件的持续交付。DevOps和MLOps都强调自动化、协作和迭代改进。然而，尽管DevOps是为了解决软件部署和运维管理中的挑战而出现的，但MLOps是为了应对机器学习工作流程的独特复杂性而演变的，特别是那些涉及数据驱动组件的工作流程([Breck等人2017b](ch058.xhtml#ref-breck2020ml))。理解这一演变对于理解现代ML系统的动机和结构至关重要。
- en: DevOps
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DevOps
- en: The term DevOps was coined in 2009 by [Patrick Debois](https://www.jedi.be/),
    a consultant and Agile practitioner who organized the first [DevOpsDays](https://www.devopsdays.org/)
    conference in Ghent, Belgium. DevOps extended the principles of the [Agile](https://agilemanifesto.org/)
    movement, that emphasized close collaboration among development teams and rapid,
    iterative releases, by bringing IT operations into the fold.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: “DevOps”这个术语是在2009年由顾问和敏捷实践者[帕特里克·德博伊斯](https://www.jedi.be/)提出的，他在比利时根特组织了首届[DevOpsDays](https://www.devopsdays.org/)会议。DevOps通过将IT运维纳入其中，扩展了[敏捷](https://agilemanifesto.org/)运动的原则，该运动强调开发团队之间的紧密协作和快速、迭代的发布。
- en: This innovation addressed a core problem in traditional software pipelines,
    where development and operations teams worked in silos, creating inefficiencies,
    delays, and misaligned priorities. DevOps emerged as a response, advocating shared
    ownership, infrastructure as code[4](#fn4), and automation to streamline deployment
    pipelines.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这项创新解决了传统软件管道中的核心问题，即开发和运维团队在孤岛中工作，造成效率低下、延误和优先级错位。DevOps应运而生，倡导共同拥有权、基础设施即代码[4](#fn4)和自动化，以简化部署管道。
- en: To support these principles, tools such as [Jenkins](https://www.jenkins.io/)[5](#fn5),
    [Docker](https://www.docker.com/), and [Kubernetes](https://kubernetes.io/)[6](#fn6)[7](#fn7)
    became foundational for implementing continuous integration and continuous delivery
    (CI/CD) practices.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持这些原则，诸如[Jenkins](https://www.jenkins.io/)[5](#fn5)、[Docker](https://www.docker.com/)[6](#fn6)和[Kubernetes](https://kubernetes.io/)[7](#fn7)等工具成为实施持续集成和持续交付（CI/CD）实践的基础。
- en: Through automation and feedback loops, DevOps promotes collaboration while reducing
    time-to-release and improving software reliability. This success established the
    cultural and technical groundwork for extending similar principles to the ML domain.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自动化和反馈循环，DevOps促进了协作，同时减少了发布时间并提高了软件可靠性。这一成功为将类似原则扩展到机器学习领域奠定了文化和技术基础。
- en: MLOps
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLOps
- en: While DevOps achieved considerable success in traditional software deployment,
    machine learning systems introduced new challenges that required further adaptation.
    MLOps builds on the DevOps foundation but addresses the specific demands of ML
    system development and deployment. Where DevOps focuses on integrating and delivering
    deterministic software, MLOps must manage non-deterministic, data-dependent workflows.
    These workflows span data acquisition, preprocessing, model training, evaluation,
    deployment, and continuous monitoring (see [Figure 13.1](ch019.xhtml#fig-mlops-diagram)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然DevOps在传统软件部署中取得了相当大的成功，但机器学习系统引入了新的挑战，需要进一步的适应。MLOps建立在DevOps的基础上，但解决了ML系统开发和部署的具体需求。DevOps专注于集成和交付确定性软件，而MLOps必须管理非确定性、数据依赖的工作流程。这些工作流程包括数据采集、预处理、模型训练、评估、部署和持续监控（见[图13.1](ch019.xhtml#fig-mlops-diagram)）。
- en: '***Machine Learning Operations (MLOps)*** is the engineering discipline that
    manages the *end-to-end lifecycle* of machine learning systems in production,
    addressing the unique challenges of *data versioning*, *model evolution*, and
    *continuous retraining*.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习运营（MLOps）**是管理生产中机器学习系统**全生命周期**的工程学科，解决**数据版本控制**、**模型演变**和**持续重新训练**的独特挑战。'
- en: The operational complexity and business risk of deploying machine learning without
    systematic engineering practices becomes clear when examining real-world failures.
    Consider a retail company that deployed a recommendation model that initially
    boosted sales by 15%. However, due to a silent data drift issue, the model’s accuracy
    degraded over six months, eventually reducing sales by 5% compared to the original
    system. The problem went undetected because monitoring focused on system uptime
    rather than model performance metrics. The company lost an estimated $10 million
    in revenue before the issue was discovered during routine quarterly analysis.
    This scenario, common in early ML deployments, illustrates why MLOps, with its
    emphasis on continuous model monitoring and automated retraining, is not merely
    an engineering best practice, but a business necessity for organizations depending
    on machine learning systems for critical operations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当检查现实世界的失败时，不采用系统化工程实践部署机器学习的运营复杂性和商业风险变得显而易见。考虑一家零售公司，它部署了一个推荐模型，最初通过提高销售额15%来提升业绩。然而，由于一个静默的数据漂移问题，该模型的精度在六个月内逐渐下降，最终与原始系统相比减少了5%的销售额。由于监控的重点是系统正常运行时间而不是模型性能指标，问题未被发现。在常规季度分析期间发现问题时，公司损失了估计1000万美元的收入。这种场景在早期机器学习部署中很常见，说明了为什么MLOps，其强调持续模型监控和自动重新训练，不仅仅是一个工程最佳实践，而且是依赖机器学习系统进行关键运营的组织的一个商业必要性。
- en: This adaptation was driven by several recurring challenges in operationalizing
    machine learning that distinguished it from traditional software deployment. Data
    drift[8](#fn8), where shifts in input data distributions over time degrade model
    accuracy, requires continuous monitoring and automated retraining procedures.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种适应是由在机器学习运营中反复出现的几个挑战所驱动的，这些挑战使其与传统软件部署区分开来。数据漂移[8](#fn8)，即输入数据分布随时间的变化会降低模型精度，需要持续的监控和自动重新训练程序。
- en: Building on this data-centric challenge, reproducibility[9](#fn9) presents another
    issue. ML workflows lack standardized mechanisms to track code, datasets, configurations,
    and environments, making it difficult to reproduce past experiments ([Schelter
    et al. 2018](ch058.xhtml#ref-schelter2018automating)). The lack of explainability
    in complex models has driven demand for tools that increase model transparency
    and interpretability, particularly in regulated domains.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个以数据为中心的挑战的基础上，可重复性[9](#fn9)提出了另一个问题。机器学习工作流程缺乏标准化的机制来跟踪代码、数据集、配置和环境，这使得重现过去的实验变得困难([Schelter等人，2018](ch058.xhtml#ref-schelter2018automating))。复杂模型缺乏可解释性推动了工具的需求，这些工具可以增加模型的可透明性和可解释性，尤其是在受监管的领域。
- en: '![](../media/file208.svg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file208.svg)'
- en: 'Figure 13.1: **MLOps Lifecycle**: MLOps extends DevOps principles to manage
    the unique challenges of machine learning systems, including data versioning,
    model retraining, and continuous monitoring. This diagram outlines the iterative
    workflow encompassing data engineering, model development, and reliable deployment
    for sustained performance in production.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1：**MLOps生命周期**：MLOps将DevOps原则扩展到管理机器学习系统的独特挑战，包括数据版本控制、模型重新训练和持续监控。此图概述了包含数据工程、模型开发和可靠部署的迭代工作流程，以在生产中实现持续的性能。
- en: Beyond these foundational challenges, organizations face additional operational
    complexities. Post-deployment monitoring of model performance proves difficult,
    especially in detecting silent failures or changes in user behavior. The manual
    overhead involved in retraining and redeploying models creates friction in experimentation
    and iteration. Configuring and maintaining ML infrastructure is complex and error-prone,
    highlighting the need for platforms that offer optimized, modular, and reusable
    infrastructure. Together, these challenges form the foundation for MLOps practices
    that focus on automation, collaboration, and lifecycle management.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基础挑战之外，组织还面临着额外的运营复杂性。模型部署后的性能监控证明是困难的，尤其是在检测静默故障或用户行为变化方面。在重新训练和重新部署模型中涉及的手动开销在实验和迭代中产生了摩擦。配置和维护机器学习基础设施复杂且容易出错，突显了需要提供优化、模块化和可重用基础设施的平台。这些挑战共同构成了关注自动化、协作和生命周期管理的MLOps实践的基础。
- en: 'In response to these distinct challenges, the field developed specialized tools
    and workflows tailored to the ML lifecycle. Building on DevOps foundations while
    addressing ML-specific requirements, MLOps coordinates a broader stakeholder ecosystem
    and introduces specialized practices such as data versioning[10](#fn10), model
    versioning, and model monitoring that extend beyond traditional DevOps scope.
    These practices are detailed in [Table 13.1](ch019.xhtml#tbl-mlops):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些独特的挑战，该领域开发了专门的工具和工作流程，这些工具和工作流程针对机器学习生命周期进行了定制。在DevOps的基础上解决机器学习特定的需求，MLOps协调更广泛的利益相关者生态系统，并引入了数据版本控制[10](#fn10)、模型版本控制和模型监控等专门实践，这些实践超越了传统的DevOps范围。这些实践在[表 13.1](ch019.xhtml#tbl-mlops)中详细说明：
- en: 'Table 13.1: **MLOps vs. DevOps**: MLOps extends DevOps principles to address
    the unique requirements of machine learning systems, including data and model
    versioning, and continuous monitoring for model performance and data drift. This
    table clarifies how MLOps coordinates a broader range of stakeholders and emphasizes
    reproducibility and scalability beyond traditional software development workflows.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.1：**MLOps与DevOps**：MLOps将DevOps原则扩展到解决机器学习系统的独特需求，包括数据和模型版本控制，以及针对模型性能和数据漂移的持续监控。此表阐明了MLOps如何协调更广泛的利益相关者，并强调在传统软件开发工作流程之外的可重复性和可扩展性。
- en: '| **Aspect** | **DevOps** | **MLOps** |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **DevOps** | **MLOps** |'
- en: '| --- | --- | --- |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Objective** | Streamlining software development and operations processes
    | Optimizing the lifecycle of machine learning models |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| **目标** | 简化软件开发和运营流程 | 优化机器学习模型的整个生命周期 |'
- en: '| **Methodology** | Continuous Integration and Continuous Delivery (CI/CD)
    for software development | Similar to CI/CD but focuses on machine learning workflows
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **方法** | 软件开发的持续集成和持续交付（CI/CD） | 类似于CI/CD，但专注于机器学习工作流程 |'
- en: '| **Primary Tools** | Version control (Git), CI/CD tools (Jenkins, Travis CI),
    Configuration management (Ansible, Puppet) | Data versioning tools, Model training
    and deployment tools, CI/CD pipelines tailored for ML |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **主要工具** | 版本控制（Git）、CI/CD工具（Jenkins、Travis CI）、配置管理（Ansible、Puppet） | 数据版本控制工具、模型训练和部署工具、针对ML的CI/CD管道
    |'
- en: '| **Primary Concerns** | Code integration, Testing, Release management, Automation,
    Infrastructure as code | Data management, Model versioning, Experiment tracking,
    Model deployment, Scalability of ML workflows |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| **主要关注点** | 代码集成、测试、发布管理、自动化、基础设施即代码 | 数据管理、模型版本控制、实验跟踪、模型部署、ML 工作流程的可扩展性
    |'
- en: '| **Typical Outcomes** | Faster and more reliable software releases, Improved
    collaboration between development and operations teams | Efficient management
    and deployment of machine learning models, Enhanced collaboration between data
    scientists and engineers |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| **典型结果** | 更快、更可靠的软件发布，开发和运维团队之间的协作改进 | 机器学习模型的有效管理和部署，数据科学家和工程师之间的协作增强 |'
- en: With these foundational distinctions established, we must first understand the
    unique operational challenges that motivate sophisticated MLOps practices before
    examining the infrastructure and practices designed to address them.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '在确立了这些基础区别之后，我们必须首先了解推动复杂 MLOps 实践的独特运营挑战，然后再考察旨在解决这些挑战的基础设施和实践。 '
- en: Technical Debt and System Complexity
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术债务与系统复杂性
- en: While the DevOps foundation provides automation and collaboration principles,
    machine learning systems introduce unique forms of complexity that require engineering
    approaches to manage effectively. Unlike traditional software where broken code
    fails immediately, ML systems can degrade silently through data changes, model
    interactions, and evolving requirements. While federated learning systems face
    unique coordination challenges ([Chapter 14](ch020.xhtml#sec-ondevice-learning))
    and robust systems require careful monitoring ([Chapter 16](ch022.xhtml#sec-robust-ai)),
    all deployment contexts must balance operational efficiency with security requirements.
    Understanding these operational challenges, collectively known as technical debt,
    is essential for motivating the engineering solutions and practices that follow.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 DevOps 基础设施提供了自动化和协作原则，但机器学习系统引入了独特的复杂性形式，这需要工程方法来有效管理。与传统的软件不同，其中损坏的代码会立即失败，ML
    系统可以通过数据变化、模型交互和不断变化的需求悄无声息地退化。虽然联邦学习系统面临独特的协调挑战（[第 14 章](ch020.xhtml#sec-ondevice-learning)）和鲁棒系统需要仔细监控（[第
    16 章](ch022.xhtml#sec-robust-ai)），但所有部署环境都必须在运营效率与安全需求之间取得平衡。理解这些被称为技术债务的运营挑战对于激励后续的工程解决方案和实践至关重要。
- en: 'This complexity manifests as machine learning systems mature and scale, where
    they accumulate technical debt: the long-term cost of expedient design decisions
    made during development. Originally proposed in software engineering in the 1990s[11](#fn11),
    this metaphor compares shortcuts in implementation to financial debt: it may enable
    short-term velocity, but requires ongoing interest payments in the form of maintenance,
    refactoring, and systemic risk.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这种复杂性在机器学习系统成熟和扩展时显现出来，它们积累了技术债务：在开发期间做出的便捷设计决策的长期成本。最初在 1990 年代提出于软件工程中[11](#fn11)，这个比喻将实施中的捷径与金融债务相比较：它可能允许短期速度，但需要持续的利益支付，形式为维护、重构和系统风险。
- en: '![](../media/file209.svg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file209.svg)'
- en: 'Figure 13.2: **ML System Complexity**: Most engineering effort in a typical
    machine learning system concentrates on components surrounding the model itself
    (data collection, feature engineering, and system configuration) rather than the
    model code. This distribution underscores the operational challenges and potential
    for technical debt arising from these often-overlooked areas of an ML system.
    Source: ([Sculley et al. 2021](ch058.xhtml#ref-sculley2015hidden)).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2：**ML 系统复杂性**：在典型的机器学习系统中，大部分工程努力集中在围绕模型本身的组件上（数据收集、特征工程和系统配置），而不是模型代码。这种分布突出了运营挑战和潜在的技术债务，这些往往是被忽视的
    ML 系统领域。来源：([Sculley 等人 2021](ch058.xhtml#ref-sculley2015hidden))。
- en: 'These operational challenges manifest in several distinct patterns that teams
    encounter as their ML systems evolve. Rather than cataloging every debt pattern,
    we focus on representative examples that illustrate the engineering approaches
    MLOps provides. Each challenge emerges from unique characteristics of machine
    learning workflows: their reliance on data rather than deterministic logic, their
    statistical rather than exact behavior, and their tendency to create implicit
    dependencies through data flows rather than explicit interfaces.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些运营挑战在机器学习系统演变过程中表现为几种不同的模式，团队在遇到这些模式时。我们不是列举每一个债务模式，而是关注代表性的例子，这些例子说明了MLOps提供的工程方法。每个挑战都源于机器学习工作流程的独特特征：它们依赖于数据而不是确定性逻辑，它们具有统计行为而不是精确行为，以及它们倾向于通过数据流而不是显式接口创建隐式依赖。
- en: The following technical debt patterns demonstrate why traditional DevOps practices
    require extension for ML systems, motivating the infrastructure solutions presented
    in subsequent sections.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下技术债务模式展示了为什么传统的DevOps实践需要扩展以适应机器学习系统，这激励了后续章节中提出的基础设施解决方案。
- en: Building on this systems perspective, we examine key categories of technical
    debt unique to ML systems ([Figure 13.3](ch019.xhtml#fig-technical-debt-taxonomy)).
    Each subsection highlights common sources, illustrative examples, and engineering
    solutions that address these challenges. While some forms of debt may be unavoidable
    during early development, understanding their causes and impact enables engineers
    to design robust and maintainable ML systems through disciplined architectural
    practices and appropriate tooling choices.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这种系统视角，我们检查了机器学习系统特有的关键技术债务类别（[图13.3](ch019.xhtml#fig-technical-debt-taxonomy)）。每个小节都突出了常见的来源、示例和工程解决方案，这些解决方案解决了这些挑战。虽然某些形式的债务在早期开发过程中可能不可避免，但了解其成因和影响使得工程师能够通过纪律性的架构实践和适当的工具选择来设计健壮且可维护的机器学习系统。
- en: '![](../media/file210.svg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file210.svg)'
- en: 'Figure 13.3: **ML Technical Debt Taxonomy**: Machine learning systems accumulate
    distinct forms of technical debt that emerge from data dependencies, model interactions,
    and evolving requirements. This hub-and-spoke diagram illustrates the primary
    debt patterns: boundary erosion undermines modularity, correction cascades propagate
    fixes through dependencies, feedback loops create hidden coupling, while data,
    configuration, and pipeline debt reflect poorly managed artifacts and workflows.
    Understanding these patterns enables systematic engineering approaches to debt
    prevention and mitigation.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3：**机器学习技术债务分类法**：机器学习系统积累的技术债务形式来自数据依赖、模型交互和不断变化的需求。这个中心辐射图展示了主要的债务模式：边界侵蚀破坏了模块化，修正级联通过依赖关系传播修复，反馈循环创造了隐藏的耦合，而数据、配置和管道债务反映了管理不善的工件和工作流程。理解这些模式使得系统性地进行债务预防和缓解的工程方法成为可能。
- en: Boundary Erosion
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 边界侵蚀
- en: In traditional software systems, modularity and abstraction provide clear boundaries
    between components, allowing changes to be isolated and behavior to remain predictable.
    Machine learning systems, in contrast, tend to blur these boundaries. The interactions
    between data pipelines, feature engineering, model training, and downstream consumption
    often lead to tightly coupled components with poorly defined interfaces.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的软件系统中，模块化和抽象提供了组件之间的清晰边界，允许变化被隔离，行为保持可预测。相比之下，机器学习系统往往模糊了这些边界。数据管道、特征工程、模型训练和下游消费之间的交互往往导致紧密耦合的组件，接口定义不明确。
- en: This erosion of boundaries makes ML systems particularly vulnerable to cascading
    effects from even minor changes. A seemingly small update to a preprocessing step
    or feature transformation can propagate through the system in unexpected ways,
    breaking assumptions made elsewhere in the pipeline. This lack of encapsulation
    increases the risk of entanglement, where dependencies between components become
    so intertwined that local modifications require global understanding and coordination.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种边界的侵蚀使得机器学习系统特别容易受到即使是微小变化引起的级联效应的影响。对预处理步骤或特征转换的看似微小的更新可能会以意想不到的方式在整个系统中传播，破坏管道中其他地方做出的假设。这种缺乏封装增加了纠缠的风险，其中组件之间的依赖变得如此交织，以至于局部修改需要全局理解和协调。
- en: One manifestation of this problem is known as CACHE (Change Anything Changes
    Everything). When systems are built without strong boundaries, adjusting a feature
    encoding, model hyperparameter, or data selection criterion can affect downstream
    behavior in unpredictable ways. This inhibits iteration and makes testing and
    validation more complex. For example, changing the binning strategy of a numerical
    feature may cause a previously tuned model to underperform, triggering retraining
    and downstream evaluation changes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种问题的表现之一被称为CACHE（任何变化都会改变一切）。当系统在没有强大边界的情况下构建时，调整特征编码、模型超参数或数据选择标准可能会以不可预测的方式影响下游行为。这阻碍了迭代，使测试和验证更加复杂。例如，改变数值特征的分箱策略可能会导致先前调优的模型表现不佳，触发重新训练和下游评估变化。
- en: To mitigate boundary erosion, teams should prioritize architectural practices
    that support modularity and encapsulation. Designing components with well-defined
    interfaces allows teams to isolate faults, reason about changes, and reduce the
    risk of system-wide regressions. For instance, clearly separating data ingestion
    from feature engineering, and feature engineering from modeling logic, introduces
    layers that can be independently validated, monitored, and maintained.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻边界侵蚀，团队应优先考虑支持模块化和封装的架构实践。设计具有明确定义接口的组件，使团队能够隔离故障、推理变化并降低系统级回归的风险。例如，明确区分数据摄取与特征工程，以及特征工程与建模逻辑，引入了可以独立验证、监控和维护的层。
- en: Boundary erosion is often invisible in early development but becomes a significant
    burden as systems scale or require adaptation. However, established software engineering
    practices can effectively prevent and mitigate this problem. Proactive design
    decisions that preserve abstraction and limit interdependencies, combined with
    systematic testing and interface documentation, provide practical solutions for
    managing complexity and avoiding long-term maintenance costs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 边界侵蚀在早期开发中往往是不可见的，但随着系统规模的扩大或需要适应时，它成为一个重大的负担。然而，成熟的软件工程实践可以有效地预防和减轻这个问题。主动的设计决策，保留抽象并限制相互依赖性，结合系统性的测试和接口文档，为管理复杂性和避免长期维护成本提供了实际解决方案。
- en: This challenge arises because ML systems operate with statistical rather than
    logical guarantees, making traditional software engineering boundaries harder
    to enforce. Understanding why boundary erosion occurs so frequently requires examining
    how machine learning workflows differ from conventional software development.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个挑战产生的原因是机器学习系统以统计保证而非逻辑保证运行，这使得传统的软件工程边界更难执行。理解边界侵蚀为何如此频繁地发生需要检查机器学习工作流程与传统软件开发之间的差异。
- en: Boundary erosion in ML systems violates established software engineering principles,
    particularly the Law of Demeter and the principle of least knowledge. While traditional
    software achieves modularity through explicit interfaces and information hiding,
    ML systems create implicit couplings through data flows that bypass these explicit
    boundaries.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统中的边界侵蚀违反了既定的软件工程原则，特别是Demeter法则和最少知识原则。而传统的软件通过显式接口和信息隐藏实现模块化，机器学习系统通过绕过这些显式边界的流程创建隐式耦合。
- en: The CACHE phenomenon represents a breakdown of the Liskov Substitution Principle,
    where component modifications violate behavioral contracts expected by dependent
    components. Unlike traditional software with compile-time guarantees, ML systems
    operate with statistical behavior that creates inherently different coupling patterns.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: CACHE现象代表了Liskov替换原则的崩溃，其中组件修改违反了依赖组件预期的行为合同。与具有编译时保证的传统软件不同，机器学习系统以统计行为运行，这创造了本质上不同的耦合模式。
- en: The challenge lies in reconciling traditional modularity concepts with the inherently
    interconnected nature of ML workflows, where statistical dependencies and data-driven
    behavior create coupling patterns that traditional software engineering frameworks
    were not designed to handle.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战在于协调传统模块化概念与机器学习工作流程固有的相互关联性，其中统计依赖和数据驱动行为创造了传统软件工程框架未设计来处理的耦合模式。
- en: Correction Cascades
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 纠正级联
- en: As machine learning systems evolve, they often undergo iterative refinement
    to address performance issues, accommodate new requirements, or adapt to environmental
    changes. In well-engineered systems, such updates are localized and managed through
    modular changes. However, in ML systems, even small adjustments can trigger correction
    cascades, a sequence of dependent fixes that propagate backward and forward through
    the workflow.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统的演变，它们通常经历迭代优化来解决性能问题、适应新要求或适应环境变化。在精心设计的系统中，此类更新是局部化和通过模块化更改管理的。然而，在机器学习系统中，即使是小的调整也可能触发纠正级联，这是一系列依赖性修复，它们在工作流程中向前和向后传播。
- en: The diagram in [Figure 13.4](ch019.xhtml#fig-correction-cascades-flowchart)
    visualizes how these cascading effects propagate through ML system development.
    Understanding the structure of these cascades helps teams anticipate and mitigate
    their impact.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13.4](ch019.xhtml#fig-correction-cascades-flowchart)中的图表展示了这些级联效应如何在机器学习系统开发中传播。理解这些级联的结构有助于团队预测和减轻其影响。'
- en: '[Figure 13.4](ch019.xhtml#fig-correction-cascades-flowchart) illustrates how
    these cascades emerge across different stages of the ML lifecycle, from problem
    definition and data collection to model development and deployment. Each arc represents
    a corrective action, and the colors indicate different sources of instability,
    including inadequate domain expertise, brittle real-world interfaces, misaligned
    incentives, and insufficient documentation. The red arrows represent cascading
    revisions, while the dotted arrow at the bottom highlights a full system restart,
    a drastic but sometimes necessary outcome.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13.4](ch019.xhtml#fig-correction-cascades-flowchart)展示了这些级联效应如何在机器学习生命周期的不同阶段出现，从问题定义和数据收集到模型开发和部署。每个弧线代表一个纠正措施，颜色表示不同不稳定来源，包括不充分的领域专业知识、脆弱的现实世界接口、激励不匹配和文档不足。红色箭头代表级联修订，而底部虚线箭头突出显示了一个完整的系统重启，这是一种极端但有时是必要的后果。'
- en: '![](../media/file74.svg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file74.svg)'
- en: 'Figure 13.4: **Correction Cascades**: Iterative refinements in ML systems often
    trigger dependent fixes across the workflow, propagating from initial adjustments
    through data, model, and deployment stages. Color-coded arcs represent corrective
    actions stemming from sources of instability, while red arrows and the dotted
    line indicate escalating revisions, potentially requiring a full system restart.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4：**纠正级联**：在机器学习系统中，迭代优化经常触发工作流程中的依赖性修复，从初始调整通过数据、模型和部署阶段进行传播。彩色弧线表示源自不稳定来源的纠正措施，而红色箭头和虚线表示升级修订，可能需要完全的系统重启。
- en: 'One common source of correction cascades is sequential model development: reusing
    or fine-tuning existing models to accelerate development for new tasks. While
    this strategy is often efficient, it can introduce hidden dependencies that are
    difficult to unwind later. Assumptions baked into earlier models become implicit
    constraints for future models, limiting flexibility and increasing the cost of
    downstream corrections.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 纠正级联的一个常见来源是顺序模型开发：重用或微调现有模型以加速新任务的开发。虽然这种策略通常效率很高，但它可能会引入难以解开的隐藏依赖。早期模型中嵌入的假设成为未来模型的隐含约束，限制了灵活性并增加了下游纠正的成本。
- en: Consider a scenario where a team fine-tunes a customer churn prediction model
    for a new product. The original model may embed product-specific behaviors or
    feature encodings that are not valid in the new setting. As performance issues
    emerge, teams may attempt to patch the model, only to discover that the true problem
    lies several layers upstream, perhaps in the original feature selection or labeling
    criteria.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个场景，一个团队对一个新产品的客户流失预测模型进行微调。原始模型可能嵌入特定于产品的行为或特征编码，在新环境中可能不适用。随着性能问题出现，团队可能会尝试修补模型，但最终发现真正的问题可能位于几个层次之上，可能在原始特征选择或标记标准中。
- en: To avoid or reduce the impact of correction cascades, teams must make careful
    tradeoffs between reuse and redesign. Several factors influence this decision.
    For small, static datasets, fine-tuning may be appropriate. For large or rapidly
    evolving datasets, retraining from scratch provides greater control and adaptability.
    Fine-tuning also requires fewer computational resources, making it attractive
    in constrained settings. However, modifying foundational components later becomes
    extremely costly due to these cascading effects.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免或减少纠正级联的影响，团队必须在重用和重新设计之间做出谨慎的权衡。几个因素影响这一决策。对于小型、静态数据集，微调可能是合适的。对于大型或快速变化的数据集，从头开始重新训练提供了更大的控制和适应性。微调还需要的计算资源较少，使其在受限环境中具有吸引力。然而，由于这些级联效应，后来修改基础组件变得极其昂贵。
- en: Therefore, careful consideration should be given to introducing fresh model
    architectures, even if resource-intensive, to avoid correction cascades down the
    line. This approach may help mitigate the amplifying effects of issues downstream
    and reduce technical debt. However, there are still scenarios where sequential
    model building makes sense, necessitating a thoughtful balance between efficiency,
    flexibility, and long-term maintainability in the ML development process.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，应仔细考虑引入新的模型架构，即使资源密集，以避免后续出现纠正级联。这种方法可能有助于减轻下游问题的放大效应，并减少技术债务。然而，仍然存在一些场景，顺序模型构建是有意义的，需要仔细平衡机器学习开发过程中的效率、灵活性和长期可维护性。
- en: To understand why correction cascades occur so persistently in ML systems despite
    best practices, it helps to examine the underlying mechanisms that drive this
    phenomenon. The correction cascade pattern emerges from hidden feedback loops
    that violate system modularity principles established in software engineering.
    When model A’s outputs influence model B’s training data, this creates implicit
    dependencies that undermine modular design. These dependencies are particularly
    insidious because they operate through data flows rather than explicit code interfaces,
    making them invisible to traditional dependency analysis tools.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么尽管遵循最佳实践，纠正级联在机器学习系统中仍然如此持久，有助于检查驱动这一现象的潜在机制。纠正级联模式源于违反软件工程中建立的系统模块性原则的隐藏反馈循环。当模型A的输出影响模型B的训练数据时，这会创建出破坏模块化设计的隐式依赖。这些依赖尤其隐蔽，因为它们通过数据流而不是显式代码接口操作，使得它们对传统的依赖分析工具不可见。
- en: From a systems theory perspective, correction cascades represent instances of
    tight coupling between supposedly independent components. The cascade propagation
    follows power-law distributions, where small initial changes can trigger disproportionately
    large system-wide modifications. This phenomenon parallels the butterfly effect
    in complex systems, where minor perturbations amplify through nonlinear interactions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统理论的角度来看，纠正级联代表了看似独立的组件之间的紧密耦合实例。级联传播遵循幂律分布，其中小的初始变化可以触发不成比例的系统级重大修改。这一现象与复杂系统中的蝴蝶效应相似，其中微小的扰动通过非线性相互作用放大。
- en: Understanding these theoretical foundations helps engineers recognize that preventing
    correction cascades requires not just better tooling, but architectural decisions
    that preserve system modularity even in the presence of learning components. The
    challenge lies in designing ML systems that maintain loose coupling despite the
    inherently interconnected nature of data-driven workflows.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些理论基础有助于工程师认识到，防止纠正级联不仅需要更好的工具，还需要在存在学习组件的情况下，保持系统模块化的架构决策。挑战在于设计机器学习系统，即使数据驱动工作流程本质上具有互联性，也能保持松散耦合。
- en: Interface and Dependency Challenges
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 接口和依赖挑战
- en: 'Unlike traditional software where component interactions occur through explicit
    APIs, ML systems often develop implicit dependencies through data flows and shared
    outputs. Two critical patterns illustrate these challenges:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统软件中组件通过显式API进行交互不同，机器学习系统通常通过数据流和共享输出发展出隐式依赖。两个关键模式说明了这些挑战：
- en: '**Undeclared Consumers**: Model outputs frequently serve downstream components
    without formal tracking or interface contracts. When models evolve, these hidden
    dependencies can break silently. For example, a credit scoring model’s outputs
    might feed an eligibility engine, which influences future applicant pools and
    training data, creating untracked feedback loops that bias model behavior over
    time.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**未声明的消费者**：模型输出经常为下游组件提供服务，而没有正式的跟踪或接口合同。当模型演化时，这些隐藏的依赖关系可能会无声地中断。例如，信用评分模型的输出可能为资格引擎提供数据，这会影响未来的申请人池和训练数据，从而创建未跟踪的反馈循环，随着时间的推移，这些循环会偏袒模型行为。'
- en: '**Data Dependency Debt**: ML pipelines accumulate unstable and underutilized
    data dependencies that become difficult to trace or validate. Feature engineering
    scripts, data joins, and labeling conventions lack the dependency analysis tools
    available in traditional software development. When data sources change structure
    or distribution, downstream models can fail unexpectedly.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据依赖债务**：机器学习管道积累不稳定和利用率低的数据依赖，这些依赖难以追踪或验证。特征工程脚本、数据连接和标签约定缺乏传统软件开发中可用的依赖分析工具。当数据源的结构或分布发生变化时，下游模型可能会意外失败。'
- en: '**Engineering Solutions**: These challenges require systematic approaches including
    strict access controls for model outputs, formal interface contracts with documented
    schemas, data versioning and lineage tracking systems, and comprehensive monitoring
    of prediction usage patterns. The MLOps infrastructure patterns presented in subsequent
    sections provide concrete implementations of these solutions.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**工程解决方案**：这些挑战需要系统性的方法，包括对模型输出实施严格的访问控制、与文档化模式建立正式的接口合同、数据版本化和血缘跟踪系统，以及对预测使用模式的全面监控。后续章节中提出的MLOps基础设施模式提供了这些解决方案的具体实现。'
- en: System Evolution Challenges
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统演化挑战
- en: 'As ML systems mature, they face unique evolution challenges that differ fundamentally
    from traditional software:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统的成熟，它们面临着独特的演化挑战，这些挑战与传统软件有根本性的区别：
- en: '**Feedback Loops**: Models influence their own future behavior through the
    data they generate. Recommendation systems exemplify this: suggested items shape
    user clicks, which become training data, potentially creating self-reinforcing
    biases. These loops undermine data independence assumptions and can mask performance
    degradation for months.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**反馈循环**：模型通过生成数据影响它们自己的未来行为。推荐系统是这一点的例证：建议的项目塑造用户点击，这些点击成为训练数据，可能产生自我强化的偏差。这些循环破坏了数据独立性的假设，并且可能掩盖几个月的性能下降。'
- en: '**Pipeline and Configuration Debt**: ML workflows often evolve into “pipeline
    jungles” of ad hoc scripts and fragmented configurations. Without modular interfaces,
    teams build duplicate pipelines rather than refactor brittle ones, leading to
    inconsistent processing and maintenance burden.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**管道和配置债务**：机器学习工作流程往往演变成“管道丛林”，其中包含临时的脚本和碎片化的配置。没有模块化接口，团队会构建重复的管道而不是重构脆弱的管道，导致处理和维护负担不一致。'
- en: '**Early-Stage Shortcuts**: Rapid prototyping encourages embedding business
    logic in training code and undocumented configuration changes. While necessary
    for innovation, these shortcuts become liabilities as systems scale across teams.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**早期阶段捷径**：快速原型设计鼓励在训练代码中嵌入业务逻辑和未记录的配置更改。虽然这些捷径对于创新是必要的，但随着系统跨团队扩展，它们成为负债。'
- en: '**Engineering Solutions**: Managing evolution requires architectural discipline
    including cohort-based monitoring for loop detection, modular pipeline design
    with workflow orchestration tools, and treating configuration as a first-class
    system component with versioning and validation.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**工程解决方案**：管理演化需要架构纪律，包括基于群体的监控以检测循环、使用工作流程编排工具的模块化管道设计，以及将配置视为一等系统组件，并使用版本控制和验证。'
- en: Real-World Technical Debt Examples
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现实世界技术债务示例
- en: 'Hidden technical debt is not just theoretical; it has played a critical role
    in shaping the trajectory of real-world machine learning systems. These examples
    illustrate how unseen dependencies and misaligned assumptions can accumulate quietly,
    only to become major liabilities over time:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏的技术债务不仅仅是理论上的；它在塑造现实世界机器学习系统的轨迹中发挥了关键作用。以下示例说明了未看到的依赖关系和错误的假设如何悄然积累，最终成为重大的负债：
- en: 'YouTube: Feedback Loop Debt'
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: YouTube：反馈循环债务
- en: 'YouTube’s recommendation engine has faced repeated criticism for promoting
    sensational or polarizing content[12](#fn12). A large part of this stems from
    feedback loop debt: recommendations influence user behavior, which in turn becomes
    training data. Over time, this led to unintended content amplification. Mitigating
    this required substantial architectural overhauls, including cohort-based evaluation,
    delayed labeling, and more explicit disentanglement between engagement metrics
    and ranking logic.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube的推荐引擎因推广耸人听闻或两极分化内容而受到反复批评[12](#fn12)。这部分原因源于反馈循环债务：推荐影响用户行为，反过来又成为训练数据。随着时间的推移，这导致了内容无意中的放大。缓解这一问题需要重大的架构改造，包括基于群体的评估、延迟标记以及更明确的参与度指标和排名逻辑之间的解耦。
- en: 'Zillow: Correction Cascade Failure'
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Zillow：纠正级联失败
- en: Zillow’s home valuation model (Zestimate) faced significant correction cascades
    during its iBuying venture[13](#fn13). When initial valuation errors propagated
    into purchasing decisions, retroactive corrections triggered systemic instability
    that required data revalidation, model redesign, and eventually a full system
    rollback. The company shut down the iBuying arm in 2021, citing model unpredictability
    and data feedback effects as core challenges.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Zillow的房屋估值模型（Zestimate）在其iBuying业务期间经历了显著的纠正级联[13](#fn13)。当初始估值错误传播到购买决策中时，追溯性纠正引发了系统不稳定，需要数据重新验证、模型重新设计，最终导致整个系统回滚。该公司于2021年关闭了iBuying部门，称模型不可预测性和数据反馈效应为核心挑战。
- en: 'Tesla: Undeclared Consumer Debt'
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特斯拉：未声明的消费者债务
- en: In early deployments, Tesla’s Autopilot made driving decisions based on models
    whose outputs were repurposed across subsystems without clear boundaries. Over-the-air
    updates occasionally introduced silent behavior changes that affected multiple
    subsystems (e.g., lane centering and braking) in unpredictable ways. This entanglement
    illustrates undeclared consumer debt and the risks of skipping strict interface
    governance in ML-enabled safety-critical systems.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期部署中，特斯拉的Autopilot基于模型做出驾驶决策，这些模型的输出被重新用于多个子系统，而没有明确的边界。空中更新偶尔会引入无声的行为变化，以不可预测的方式影响多个子系统（例如，车道居中和制动）。这种纠缠说明了未声明的消费者债务以及跳过严格接口治理在机器学习赋能的安全关键系统中的风险。
- en: 'Facebook: Configuration Debt'
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Facebook：配置债务
- en: Facebook’s News Feed algorithm has undergone numerous iterations, often driven
    by rapid experimentation. However, the lack of consistent configuration management
    led to opaque settings that influenced content ranking without clear documentation.
    As a result, changes to the algorithm’s behavior were difficult to trace, and
    unintended consequences emerged from misaligned configurations. This situation
    highlights the importance of treating configuration as a first-class citizen in
    ML systems.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook的新闻推送算法经历了多次迭代，通常是由快速实验驱动的。然而，缺乏一致的配置管理导致了不透明的设置，这些设置影响了内容排名，但没有明确的文档记录。因此，算法行为的变更难以追踪，不匹配的配置产生了意外的后果。这种情况突出了在机器学习系统中将配置视为一等公民的重要性。
- en: 'These real-world examples demonstrate the pervasive nature of technical debt
    in ML systems and why traditional DevOps practices require systematic extension.
    The infrastructure and production operations sections that follow present concrete
    engineering solutions designed to address these specific challenges: feature stores
    address data dependency debt, versioning systems enable reproducible configurations,
    monitoring frameworks detect feedback loops, and modular pipeline architectures
    prevent technical debt accumulation. This understanding of operational challenges
    provides the essential motivation for the specialized MLOps tools and practices
    we examine next.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些现实世界的例子展示了机器学习系统中技术债务的普遍性以及为什么传统的DevOps实践需要系统性的扩展。以下的基础设施和生产操作部分展示了针对这些具体挑战设计的具体工程解决方案：特征存储解决数据依赖债务，版本控制系统实现可重复的配置，监控框架检测反馈循环，模块化管道架构防止技术债务积累。这种对操作挑战的理解为接下来我们考察的专业MLOps工具和实践提供了基本动机。
- en: Development Infrastructure and Automation
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发基础设施和自动化
- en: Building on the operational challenges established above, this section examines
    the infrastructure and development components that enable the specialized capabilities
    from preceding chapters while addressing systemic challenges. These foundational
    components must support federated learning coordination for edge devices ([Chapter 14](ch020.xhtml#sec-ondevice-learning)),
    implement secure model serving with privacy guarantees ([Chapter 15](ch021.xhtml#sec-security-privacy)),
    and maintain robustness monitoring for distribution shifts ([Chapter 16](ch022.xhtml#sec-robust-ai)).
    They form a layered architecture, as illustrated in Figure [Figure 13.5](ch019.xhtml#fig-ops-layers),
    that integrates these diverse requirements into a cohesive operational framework.
    Understanding how these components interact enables practitioners to design systems
    that simultaneously achieve edge efficiency, security compliance, and fault tolerance
    while maintaining operational sustainability.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述运营挑战的基础上，本节探讨了基础设施和开发组件，这些组件使前几章中提到的专用功能成为可能，同时解决系统性挑战。这些基础组件必须支持边缘设备联邦学习协调（[第14章](ch020.xhtml#sec-ondevice-learning)）、实现具有隐私保证的安全模型服务（[第15章](ch021.xhtml#sec-security-privacy)）以及维护对分布变化的鲁棒性监控（[第16章](ch022.xhtml#sec-robust-ai)）。它们形成一个分层架构，如图[图13.5](ch019.xhtml#fig-ops-layers)所示，将各种要求整合为一个统一的运营框架。了解这些组件如何相互作用，使从业者能够设计出同时实现边缘效率、安全合规性和容错性，同时保持运营可持续性的系统。
- en: '![](../media/file211.svg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file211.svg)'
- en: 'Figure 13.5: **MLOps Stack Layers**: Modular architecture organizes machine
    learning system components, from model development and orchestration to infrastructure,
    facilitating automation, reproducibility, and scalable deployment. Each layer
    builds upon the one below, enabling cross-team collaboration and supporting the
    entire ML lifecycle from initial experimentation to long-term production maintenance.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5：**MLOps堆栈层**：模块化架构组织机器学习系统组件，从模型开发和编排到基础设施，促进自动化、可重复性和可扩展部署。每一层都建立在下一层之上，促进跨团队合作，并支持整个ML生命周期，从初始实验到长期生产维护。
- en: Data Infrastructure and Preparation
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据基础设施和准备
- en: Reliable machine learning systems depend on structured, scalable, and repeatable
    handling of data. From the moment data is ingested to the point where it informs
    predictions, each stage must preserve quality, consistency, and traceability.
    In operational settings, data infrastructure supports not only initial development
    but also continual retraining, auditing, and serving, requiring systems that formalize
    the transformation and versioning of data throughout the ML lifecycle.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠的机器学习系统依赖于结构化、可扩展和可重复的数据处理。从数据被摄入的那一刻起，到它用于预测的那一刻，每个阶段都必须保持质量、一致性和可追溯性。在运营环境中，数据基础设施不仅支持初始开发，还支持持续的重训练、审计和服务，需要系统化地处理数据在整个机器学习生命周期中的转换和版本控制。
- en: Data Management
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据管理
- en: Building on the data engineering foundations from [Chapter 6](ch012.xhtml#sec-data-engineering),
    data collection, preprocessing, and feature transformation become formalized into
    systematic operational processes. Within MLOps, these tasks are scaled into repeatable,
    automated workflows that ensure data reliability, traceability, and operational
    efficiency. Data management, in this setting, extends beyond initial preparation
    to encompass the continuous handling of data artifacts throughout the lifecycle
    of a machine learning system.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 基于[第6章](ch012.xhtml#sec-data-engineering)中的数据工程基础，数据收集、预处理和特征转换被正式化为系统化的运营流程。在MLOps中，这些任务被扩展为可重复、自动化的工作流程，确保数据可靠性、可追溯性和运营效率。在这种设置中，数据管理不仅包括初始准备，还包括在整个机器学习系统生命周期中对数据工件持续的处理。
- en: Central to this operational foundation is dataset versioning, which enables
    reproducible model development by tracking data evolution (see [Section 13.4.1.3](ch019.xhtml#sec-ml-operations-versioning-lineage-deaa)
    for implementation details). Tools such as [DVC](https://dvc.org/) enable teams
    to version large datasets alongside code repositories managed by [Git](https://git-scm.com/),
    ensuring that data lineage is preserved and that experiments are reproducible.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这一运营基础的核心是数据集版本控制，它通过跟踪数据演变（参见[第13.4.1.3节](ch019.xhtml#sec-ml-operations-versioning-lineage-deaa)中的实现细节）来实现可重复的模型开发。例如，[DVC](https://dvc.org/)等工具使团队能够在由[Git](https://git-scm.com/)管理的代码存储库中版本控制大型数据集，确保数据血缘得到保留，实验可重复。
- en: This versioning foundation enables more sophisticated data management capabilities.
    Supervised learning pipelines, for instance, require consistent and well-managed
    annotation workflows. Labeling tools such as [Label Studio](https://labelstud.io/)
    support scalable, team-based annotation with integrated audit trails and version
    histories. These capabilities are essential in production settings, where labeling
    conventions evolve over time or require refinement across multiple iterations
    of a project.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本控制基础使得更复杂的数据管理能力成为可能。例如，监督学习管道需要一致和良好管理的注释工作流程。如[Label Studio](https://labelstud.io/)之类的标签工具支持可扩展的基于团队的注释，并具有集成审计跟踪和版本历史。这些能力在生产环境中至关重要，因为标签规范会随着时间的推移而演变，或需要在项目的多个迭代中进一步完善。
- en: Beyond annotation workflows, operational environments require data storage that
    supports secure, scalable, and collaborative access. Cloud-based object storage
    systems such as [Amazon S3](https://aws.amazon.com/s3/) and [Google Cloud Storage](https://cloud.google.com/storage)
    offer durability and fine-grained access control, making them well-suited for
    managing both raw and processed data artifacts. These systems frequently serve
    as the foundation for downstream analytics, model development, and deployment
    workflows.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 除了注释工作流程之外，操作环境需要支持安全、可扩展和协作访问的数据存储。基于云的对象存储系统，如[Amazon S3](https://aws.amazon.com/s3/)和[Google
    Cloud Storage](https://cloud.google.com/storage)，提供耐用性和细粒度访问控制，非常适合管理原始和经过处理的数据工件。这些系统通常作为下游分析、模型开发和部署工作流程的基础。
- en: Building on this storage foundation, MLOps teams construct automated data pipelines
    to transition from raw data to analysis- or inference-ready formats. These pipelines
    perform structured tasks such as data ingestion, schema validation, deduplication,
    transformation, and loading. Orchestration tools including [Apache Airflow](https://airflow.apache.org/),
    [Prefect](https://www.prefect.io/), and [dbt](https://www.getdbt.com/) are commonly
    used to define and manage these workflows. When managed as code, pipelines support
    versioning, modularity, and integration with CI/CD systems.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个存储基础上，MLOps团队构建自动化的数据管道，将原始数据转换为分析或推理准备好的格式。这些管道执行结构化任务，如数据摄取、模式验证、去重、转换和加载。包括[Apache
    Airflow](https://airflow.apache.org/)、[Prefect](https://www.prefect.io/)和[dbt](https://www.getdbt.com/)在内的编排工具通常用于定义和管理这些工作流程。当作为代码管理时，管道支持版本控制、模块化和与CI/CD系统的集成。
- en: As these automated pipelines scale across organizations, they naturally encounter
    the challenge of feature management at scale. An increasingly important element
    of modern data infrastructure is the feature store, a concept pioneered by Uber’s
    Michelangelo platform team in 2017\. They coined the term after realizing that
    feature engineering was being duplicated across hundreds of ML models. Their solution,
    a centralized “feature store”, became the template that inspired Feast, Tecton,
    and dozens of other platforms.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这些自动化管道在组织中的扩展，它们自然会遇到大规模特征管理的挑战。现代数据基础设施的一个重要元素是特征存储库，这一概念由Uber的Michelangelo平台团队在2017年首创。他们在意识到特征工程正在数百个机器学习模型中重复后创造了这个术语。他们的解决方案，一个集中的“特征存储库”，成为了启发Feast、Tecton和其他数十个平台的模板。
- en: Feature stores centralize engineered features for reuse across models and teams
    (detailed in [Section 13.4.1.2](ch019.xhtml#sec-ml-operations-feature-stores-e9a4)).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储库集中管理工程特征，以便在模型和团队之间重复使用（详细说明见[第13.4.1.2节](ch019.xhtml#sec-ml-operations-feature-stores-e9a4)）。
- en: To illustrate these concepts in practice, consider a predictive maintenance
    application in an industrial setting. A continuous stream of sensor data is ingested
    and joined with historical maintenance logs through a scheduled pipeline managed
    in Airflow. The resulting features, including rolling averages and statistical
    aggregates, are stored in a feature store for both retraining and low-latency
    inference. This pipeline is versioned, monitored, and integrated with the model
    registry, enabling full traceability from data to deployed model predictions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在实践中说明这些概念，可以考虑一个工业环境中的预测性维护应用。通过Airflow管理的预定管道，连续的传感器数据被摄取并与历史维护日志连接。这些结果特征，包括滚动平均值和统计聚合，存储在特征存储库中，用于重新训练和低延迟推理。此管道经过版本控制、监控，并与模型注册表集成，从而实现了从数据到部署模型预测的完整可追溯性。
- en: This comprehensive approach to data management extends far beyond ensuring data
    quality, establishing the operational backbone that enables model reproducibility,
    auditability, and sustained deployment at scale. Without robust data management,
    the integrity of downstream training, evaluation, and serving processes cannot
    be maintained, making feature stores a critical component of the infrastructure.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这种全面的数据管理方法远远超出了确保数据质量的范畴，建立了支持模型可重复性、可审计性和大规模持续部署的运营骨干。没有稳健的数据管理，下游训练、评估和服务过程的完整性无法得到保证，使得特征存储成为基础设施的关键组成部分。
- en: Feature Stores
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征存储
- en: Feature stores[14](#fn14) provide an abstraction layer between data engineering
    and machine learning. Their primary purpose is to enable consistent, reliable
    access to engineered features across training and inference workflows. In conventional
    pipelines, feature engineering logic is duplicated, manually reimplemented, or
    diverges across environments. This introduces risks of training-serving skew[15](#fn15)
    (where features differ between training and production), data leakage, and model
    drift.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储[14](#fn14)在数据工程和机器学习之间提供了一个抽象层。它们的主要目的是在训练和推理工作流程中实现工程特征的持续、可靠访问。在传统的管道中，特征工程逻辑被重复、手动重新实现或在不同的环境中出现分歧。这引入了训练-服务偏差[15](#fn15)（训练和生产的特征不同）、数据泄露和模型漂移的风险。
- en: To address these challenges, feature stores manage both offline (batch) and
    online (real-time) feature access in a centralized repository. This becomes critical
    when deploying the optimized models discussed in [Chapter 10](ch016.xhtml#sec-model-optimizations),
    where feature consistency across environments is essential for maintaining model
    accuracy. During training, features are computed and stored in a batch environment,
    typically in conjunction with historical labels. At inference time, the same transformation
    logic is applied to fresh data in an online serving system. This architecture
    ensures that models consume identical features in both contexts, promoting consistency
    and improving reliability.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，特征存储在集中式存储库中管理离线（批量）和在线（实时）特征访问。当部署第10章中讨论的优化模型时，这一点变得至关重要，因为环境间的特征一致性对于保持模型准确性至关重要。在训练期间，特征在批量环境中计算并存储，通常与历史标签一起。在推理时间，相同的转换逻辑应用于在线服务系统中的新鲜数据。这种架构确保模型在两种环境中都消耗相同特征，促进了一致性并提高了可靠性。
- en: Beyond consistency across training and serving environments, feature stores
    support versioning, metadata management, and feature reuse across teams. For example,
    a fraud detection model and a credit scoring model rely on overlapping transaction
    features, which can be centrally maintained, validated, and shared. This reduces
    engineering overhead and supports alignment across use cases.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在训练和部署环境中的一致性之外，特征存储还支持跨团队进行版本控制、元数据管理和特征重用。例如，欺诈检测模型和信用评分模型依赖于重叠的交易特征，这些特征可以集中维护、验证和共享。这减少了工程开销，并支持用例之间的对齐。
- en: Feature stores can be integrated with data pipelines and model registries, enabling
    lineage tracking and traceability. When a feature is updated or deprecated, dependent
    models are identified and retrained accordingly. This integration enhances the
    operational maturity of ML systems and supports auditing, debugging, and compliance
    workflows.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储可以与数据管道和模型注册表集成，从而实现血缘跟踪和可追溯性。当特征被更新或弃用时，相关的模型将被识别并相应地重新训练。这种集成增强了机器学习系统的运营成熟度，并支持审计、调试和合规工作流程。
- en: Versioning and Lineage
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 版本控制和血缘关系
- en: 'Versioning is essential to reproducibility and traceability in machine learning
    systems. Unlike traditional software, ML models depend on multiple changing artifacts:
    training data, feature engineering logic, trained model parameters, and configuration
    settings. To manage this complexity, MLOps practices enforce tracking of versions
    across all pipeline components.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制对于机器学习系统的可重复性和可追溯性至关重要。与传统的软件不同，机器学习模型依赖于多个不断变化的工件：训练数据、特征工程逻辑、训练模型参数和配置设置。为了管理这种复杂性，MLOps实践强制执行对所有管道组件的版本跟踪。
- en: At the foundation of this tracking system, data versioning allows teams to snapshot
    datasets at specific points in time and associate them with particular model runs.
    This includes both raw data (e.g., input tables or log streams) and processed
    artifacts (e.g., cleaned datasets or feature sets). By maintaining a direct mapping
    between model checkpoints and the data used for training, teams can audit decisions,
    reproduce results, and investigate regressions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在此跟踪系统的基石上，数据版本控制允许团队在特定时间点快照数据集，并将它们与特定的模型运行关联起来。这包括原始数据（例如，输入表或日志流）和经过处理的工件（例如，清洗后的数据集或特征集）。通过维护模型检查点和用于训练的数据之间的直接映射，团队能够审计决策、重现结果并调查回归。
- en: Complementing data versioning, model versioning involves registering trained
    models as immutable artifacts, alongside metadata such as training parameters,
    evaluation metrics, and environment specifications. These records are maintained
    in a model registry, which provides a structured interface for promoting, deploying,
    and rolling back model versions. Some registries also support lineage visualization,
    which traces the full dependency graph from raw data to deployed prediction.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据版本控制相辅相成的是，模型版本控制涉及将训练好的模型注册为不可变工件，同时包括训练参数、评估指标和环境规范等元数据。这些记录保存在模型注册表中，该注册表提供了一个结构化的接口，用于提升、部署和回滚模型版本。一些注册表还支持血缘可视化，它追踪从原始数据到部署预测的完整依赖图。
- en: 'These complementary versioning practices together form the lineage layer of
    an ML system. This layer enables introspection, experimentation, and governance.
    When a deployed model underperforms, lineage tools help teams answer questions
    such as:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这些互补的版本控制实践共同构成了机器学习系统的血缘层。这一层实现了自我检查、实验和治理。当部署的模型表现不佳时，血缘工具帮助团队回答诸如以下问题：
- en: Was the input distribution consistent with training data?
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入分布是否与训练数据一致？
- en: Did the feature definitions change?
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征定义是否发生了变化？
- en: Is the model version aligned with the serving infrastructure?
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型版本是否与服务基础设施对齐？
- en: By elevating versioning and lineage to first-class citizens in the system design,
    MLOps enables teams to build and maintain reliable, auditable, and evolvable ML
    workflows at scale.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将版本控制和血缘关系提升到系统设计中的第一类公民，MLOps使团队能够以规模化的方式构建和维护可靠、可审计和可演进的机器学习工作流程。
- en: Continuous Pipelines and Automation
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 持续管道和自动化
- en: Automation enables machine learning systems to evolve continuously in response
    to new data, shifting objectives, and operational constraints. Rather than treating
    development and deployment as isolated phases, automated pipelines allow for synchronized
    workflows that integrate data preprocessing, training, evaluation, and release.
    These pipelines underpin scalable experimentation and ensure the repeatability
    and reliability of model updates in production.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化使机器学习系统能够持续地根据新数据、变化的目标和运营限制进行演变。而不是将开发和部署视为孤立的阶段，自动化的管道允许同步的工作流程，这些工作流程整合了数据预处理、训练、评估和发布。这些管道为可扩展的实验提供了基础，并确保了生产中模型更新的可重复性和可靠性。
- en: CI/CD Pipelines
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CI/CD 管道
- en: While conventional software systems rely on continuous integration and continuous
    delivery (CI/CD) pipelines to ensure that code changes can be tested, validated,
    and deployed efficiently, machine learning systems require significant adaptations.
    In the context of machine learning systems, CI/CD pipelines must handle additional
    complexities introduced by data dependencies, model training workflows, and artifact
    versioning. These pipelines provide a structured mechanism to transition ML models
    from development into production in a reproducible, scalable, and automated manner.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然传统的软件系统依赖于持续集成和持续交付（CI/CD）管道以确保代码更改可以高效地进行测试、验证和部署，但机器学习系统需要重大的调整。在机器学习系统的背景下，CI/CD
    管道必须处理由数据依赖、模型训练工作流程和工件版本控制引入的额外复杂性。这些管道提供了一个结构化的机制，以可重复、可扩展和自动化的方式将机器学习模型从开发过渡到生产。
- en: 'Building on these adapted foundations, a typical ML CI/CD pipeline consists
    of several coordinated stages, including: checking out updated code, preprocessing
    input data, training a candidate model, validating its performance, packaging
    the model, and deploying it to a serving environment. In some cases, pipelines
    also include triggers for automatic retraining based on data drift or performance
    degradation. By codifying these steps, CI/CD pipelines[16](#fn16) reduce manual
    intervention, enforce quality checks, and support continuous improvement of deployed
    systems.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些改进的基础上，典型的机器学习CI/CD流水线包括几个协调阶段，包括：检出更新代码、预处理输入数据、训练候选模型、验证其性能、打包模型以及将其部署到服务环境。在某些情况下，流水线还包括基于数据漂移或性能退化的自动重训练触发器。通过将这些步骤编码化，CI/CD流水线[16](#fn16)
    减少了人工干预，强制执行质量检查，并支持已部署系统的持续改进。
- en: To support these complex workflows, a wide range of tools is available for implementing
    ML-focused CI/CD workflows. General-purpose CI/CD orchestrators such as [Jenkins](https://www.jenkins.io/),
    [CircleCI](https://circleci.com/), and [GitHub Actions](https://github.com/features/actions)[17](#fn17)
    manage version control events and execution logic. These tools integrate with
    domain-specific platforms such as [Kubeflow](https://www.kubeflow.org/)[18](#fn18),
    [Metaflow](https://metaflow.org/), and [Prefect](https://www.prefect.io/), which
    offer higher-level abstractions for managing ML tasks and workflows.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持这些复杂的流程，有一系列工具可用于实现专注于机器学习的CI/CD工作流程。通用CI/CD编排器，如[Jenkins](https://www.jenkins.io/)、[CircleCI](https://circleci.com/)和[GitHub
    Actions](https://github.com/features/actions)[17](#fn17)，管理版本控制事件和执行逻辑。这些工具与特定领域的平台集成，如[Kubeflow](https://www.kubeflow.org/)[18](#fn18)、[Metaflow](https://metaflow.org/)和[Prefect](https://www.prefect.io/)，它们为管理机器学习任务和工作流程提供了更高层次的抽象。
- en: '[Figure 13.6](ch019.xhtml#fig-ops-cicd) illustrates a representative CI/CD
    pipeline for machine learning systems. The process begins with a dataset and feature
    repository, from which data is ingested and validated. Validated data is then
    transformed for model training. A retraining trigger, such as a scheduled job
    or performance threshold, initiates this process automatically. Once training
    and hyperparameter tuning are complete, the resulting model undergoes evaluation
    against predefined criteria. If the model satisfies the required thresholds, it
    is registered in a model repository along with metadata, performance metrics,
    and lineage information. Finally, the model is deployed back into the production
    system, closing the loop and enabling continuous delivery of updated models.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13.6](ch019.xhtml#fig-ops-cicd) 展示了一个机器学习系统的代表性CI/CD流水线。该过程从数据集和特征库开始，从中摄取并验证数据。验证后的数据随后被转换以用于模型训练。重训练触发器，如计划任务或性能阈值，自动启动此过程。一旦训练和超参数调整完成，生成的模型将根据预定义的标准进行评估。如果模型满足所需的阈值，它将连同元数据、性能指标和血缘信息一起注册到模型库中。最后，模型被部署回生产系统，完成闭环，并实现更新模型的持续交付。'
- en: '![](../media/file212.svg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file212.svg)'
- en: 'Figure 13.6: **ML CI/CD Pipeline**: Automated workflows streamline model development
    by integrating version control, testing, and deployment, enabling continuous delivery
    of updated models to production. This pipeline emphasizes data and model validation,
    automated retraining triggers, and model registration with metadata for reproducibility
    and governance. Source: HarvardX.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6：**机器学习CI/CD流水线**：通过集成版本控制、测试和部署，自动工作流程简化了模型开发，使更新模型能够持续交付到生产环境中。此流水线强调数据与模型验证、自动重训练触发器以及带有元数据的模型注册，以实现可重复性和治理。来源：HarvardX。
- en: To illustrate these concepts in practice, consider an image classification model
    under active development. When a data scientist commits changes to a [GitHub](https://github.com/)
    repository, a Jenkins pipeline is triggered. The pipeline fetches the latest data,
    performs preprocessing, and initiates model training. Experiments are tracked
    using [MLflow](https://mlflow.org/), which logs metrics and stores model artifacts.
    After passing automated evaluation tests, the model is containerized and deployed
    to a staging environment using [Kubernetes](https://kubernetes.io/). If the model
    meets validation criteria in staging, the pipeline orchestrates controlled deployment
    strategies such as canary testing (detailed in [Section 13.4.2.3](ch019.xhtml#sec-ml-operations-model-validation-cb32)),
    gradually routing production traffic to the new model while monitoring key metrics
    for anomalies. In case of performance regressions, the system can automatically
    revert to a previous model version.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在实践中说明这些概念，可以考虑一个正在积极开发中的图像分类模型。当数据科学家向 [GitHub](https://github.com/) 仓库提交更改时，Jenkins
    流程被触发。该流程获取最新数据，执行预处理，并启动模型训练。实验使用 [MLflow](https://mlflow.org/) 进行跟踪，该工具记录指标并存储模型工件。通过自动评估测试后，模型被容器化并部署到预发布环境，使用的是
    [Kubernetes](https://kubernetes.io/)。如果模型在预发布环境中满足验证标准，流程将协调控制部署策略，如金丝雀测试（在第 [13.4.2.3
    节](ch019.xhtml#sec-ml-operations-model-validation-cb32)中详细说明），逐渐将生产流量路由到新模型，同时监控关键指标以检测异常。在性能回归的情况下，系统可以自动回滚到先前模型版本。
- en: Through these comprehensive automation capabilities, CI/CD pipelines play a
    central role in enabling scalable, repeatable, and safe deployment of machine
    learning models. By unifying the disparate stages of the ML workflow under continuous
    automation, these pipelines support faster iteration, improved reproducibility,
    and greater resilience in production systems. In mature MLOps environments, CI/CD
    is not an optional layer, but a foundational capability that transforms ad hoc
    experimentation into a structured and operationally sound development process.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些全面的自动化能力，CI/CD 流程在实现可扩展、可重复和安全的机器学习模型部署中发挥着核心作用。通过将 ML 工作流程的不同阶段统一到持续自动化之下，这些流程支持更快的迭代、更好的可重复性和生产系统中的更大弹性。在成熟的
    MLOps 环境中，CI/CD 不是一个可选层，而是一个基础能力，它将临时实验转变为结构化和操作上合理的开发过程。
- en: Training Pipelines
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练流程
- en: Model training is a central phase in the machine learning lifecycle, where algorithms
    are optimized to learn patterns from data. Building on the distributed training
    concepts covered in [Chapter 8](ch014.xhtml#sec-ai-training), we examine how training
    workflows are operationalized through systematic pipelines. Within an MLOps context,
    these activities are reframed as part of a reproducible, scalable, and automated
    pipeline that supports continual experimentation and reliable production deployment.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练是机器学习生命周期中的关键阶段，其中算法被优化以从数据中学习模式。在 [第 8 章](ch014.xhtml#sec-ai-training)
    中介绍的分布式训练概念的基础上，我们探讨如何通过系统流程实现训练工作流程的运营化。在 MLOps 的背景下，这些活动被重新定义为可重复、可扩展和自动化的流程的一部分，该流程支持持续实验和可靠的production部署。
- en: The foundation of operational training lies in modern machine learning frameworks
    such as [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/),
    and [Keras](https://keras.io/), which provide modular components for building
    and training models. The framework selection principles from [Chapter 7](ch013.xhtml#sec-ai-frameworks)
    become essential for production training pipelines requiring reliable scaling.
    These libraries include high-level abstractions for neural network components
    and training algorithms, enabling practitioners to prototype and iterate efficiently.
    When embedded into MLOps pipelines, these frameworks serve as the foundation for
    training processes that can be systematically scaled, tracked, and retrained.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 运营训练的基础在于现代机器学习框架，例如 [TensorFlow](https://www.tensorflow.org/)、[PyTorch](https://pytorch.org/)
    和 [Keras](https://keras.io/)，它们提供了构建和训练模型的模块化组件。来自 [第 7 章](ch013.xhtml#sec-ai-frameworks)
    的框架选择原则对于需要可靠扩展的生产训练流程至关重要。这些库包括神经网络组件和训练算法的高级抽象，使从业者能够高效地进行原型设计和迭代。当嵌入到 MLOps
    流程中时，这些框架成为可以系统扩展、跟踪和重新训练的训练过程的基础。
- en: Building on these framework foundations, reproducibility emerges as a key objective
    of MLOps. Training scripts and configurations are version-controlled using tools
    like [Git](https://git-scm.com/) and hosted on platforms such as [GitHub](https://github.com/).
    Interactive development environments, including [Jupyter](https://jupyter.org/)
    notebooks, encapsulate data ingestion, feature engineering, training routines,
    and evaluation logic in a unified format. These notebooks integrate into automated
    pipelines, allowing the same logic used for local experimentation to be reused
    for scheduled retraining in production systems.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些框架基础之上，可重复性成为MLOps的关键目标。使用如[Git](https://git-scm.com/)等工具对训练脚本和配置进行版本控制，并托管在如[GitHub](https://github.com/)等平台上。交互式开发环境，包括[Jupyter](https://jupyter.org/)笔记本，将数据摄取、特征工程、训练流程和评估逻辑封装在统一格式中。这些笔记本集成到自动化管道中，允许用于本地实验的逻辑在生产系统中的计划重新训练中被重用。
- en: Beyond ensuring reproducibility, automation further enhances model training
    by reducing manual effort and standardizing critical steps. MLOps workflows incorporate
    techniques such as [hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview),
    [neural architecture search](https://arxiv.org/abs/1808.05377), and [automatic
    feature selection](https://scikit-learn.org/stable/modules/feature_selection.html)
    to explore the design space efficiently. These tasks are orchestrated using CI/CD
    pipelines, which automate data preprocessing, model training, evaluation, registration,
    and deployment. For instance, a Jenkins pipeline triggers a retraining job when
    new labeled data becomes available. The resulting model is evaluated against baseline
    metrics, and if performance thresholds are met, it is deployed automatically.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 除了确保可重复性之外，自动化通过减少人工努力和标准化关键步骤进一步增强了模型训练。MLOps工作流程结合了如[超参数调整](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview)、[神经架构搜索](https://arxiv.org/abs/1808.05377)和[自动特征选择](https://scikit-learn.org/stable/modules/feature_selection.html)等技术，以有效地探索设计空间。这些任务通过CI/CD管道进行编排，自动化数据预处理、模型训练、评估、注册和部署。例如，当有新的标记数据可用时，Jenkins管道会触发重新训练作业。生成的模型将与基线指标进行比较，如果达到性能阈值，它将自动部署。
- en: Supporting these automated workflows, the increasing availability of cloud-based
    infrastructure has further expanded the reach of model training. This connects
    to the workflow orchestration patterns explored in [Chapter 5](ch011.xhtml#sec-ai-workflow),
    which provide the foundation for managing complex, multi-stage training processes
    across distributed systems. Cloud providers offer managed services that provision
    high-performance computing resources, which include GPU and TPU accelerators,
    on demand[19](#fn19). Depending on the platform, teams construct their own training
    workflows or rely on fully managed services such as [Vertex AI Fine Tuning](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models),
    which support automated adaptation of foundation models to new tasks. Nonetheless,
    hardware availability, regional access restrictions, and cost constraints remain
    important considerations when designing cloud-based training systems.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 支持这些自动化工作流程，云基础设施的日益可用进一步扩大了模型训练的覆盖范围。这连接到第5章中探讨的工作流程编排模式，这些模式为在分布式系统中管理复杂的多阶段训练过程提供了基础。云服务提供商提供托管服务，提供按需配置的高性能计算资源，包括GPU和TPU加速器[19](#fn19)。根据平台的不同，团队可以构建自己的训练工作流程或依赖完全托管的如[Vertex
    AI微调](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models)等服务，这些服务支持基础模型对新任务的自动适应。然而，在设计基于云的训练系统时，硬件可用性、区域访问限制和成本限制仍然是重要的考虑因素。
- en: To illustrate these integrated practices, consider a data scientist developing
    a neural network for image classification using a PyTorch notebook. The [fastai](https://www.fast.ai/)
    library is used to simplify model construction and training. The notebook trains
    the model on a labeled dataset, computes performance metrics, and tunes model
    configuration parameters. Once validated, the training script is version-controlled
    and incorporated into a retraining pipeline that is periodically triggered based
    on data updates or model performance monitoring.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这些集成实践，考虑一位数据科学家使用PyTorch笔记本开发用于图像分类的神经网络。使用了[fastai](https://www.fast.ai/)库来简化模型构建和训练。笔记本在标记数据集上训练模型，计算性能指标，并调整模型配置参数。一旦验证通过，训练脚本将进行版本控制并纳入一个基于数据更新或模型性能监控定期触发的重新训练管道。
- en: Through standardized workflows, versioned environments, and automated orchestration,
    MLOps enables the model training process to transition from ad hoc experimentation
    to a robust, repeatable, and scalable system. This not only accelerates development
    but also ensures that trained models meet production standards for reliability,
    traceability, and performance.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通过标准化工作流程、版本化环境和自动化编排，MLOps使模型训练过程从临时的实验转变为一个强大、可重复和可扩展的系统。这不仅加速了开发，还确保了训练模型满足生产标准，包括可靠性、可追溯性和性能。
- en: Model Validation
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型验证
- en: Before a machine learning model is deployed into production, it must undergo
    rigorous evaluation to ensure that it meets predefined performance, robustness,
    and reliability criteria. While earlier chapters discussed evaluation in the context
    of model development, MLOps reframes evaluation as a structured and repeatable
    process for validating operational readiness. It incorporates practices that support
    pre-deployment assessment, post-deployment monitoring, and automated regression
    testing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在将机器学习模型部署到生产环境之前，它必须经过严格的评估，以确保其满足预定义的性能、鲁棒性和可靠性标准。虽然前面的章节在模型开发的背景下讨论了评估，但MLOps将评估重新定义为验证操作准备就绪的有序和可重复的过程。它包含了支持部署前评估、部署后监控和自动化回归测试的实践。
- en: The evaluation process begins with performance testing against a holdout test
    set, a dataset not used during training or validation. This dataset is sampled
    from the same distribution as production data and is used to measure generalization.
    Core metrics such as [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision),
    [area under the curve (AUC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve),
    [precision](https://en.wikipedia.org/wiki/Precision_and_recall), [recall](https://en.wikipedia.org/wiki/Precision_and_recall),
    and [F1 score](https://en.wikipedia.org/wiki/F1_score) are computed to quantify
    model performance. These metrics are not only used at a single point in time but
    also tracked longitudinally to detect degradation, such as that caused by [data
    drift](https://www.ibm.com/cloud/learn/data-drift), where shifts in input distributions
    can reduce model accuracy over time (see [Figure 13.7](ch019.xhtml#fig-data-drift)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 评估过程从对保留测试集的性能测试开始，该数据集在训练或验证过程中未使用。此数据集从与生产数据相同的分布中采样，并用于衡量泛化能力。核心指标，如[准确率](https://en.wikipedia.org/wiki/Accuracy_and_precision)、[曲线下面积（AUC）](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)、[精确率](https://en.wikipedia.org/wiki/Precision_and_recall)、[召回率](https://en.wikipedia.org/wiki/Precision_and_recall)和[F1分数](https://en.wikipedia.org/wiki/F1_score)，被计算出来以量化模型性能。这些指标不仅用于某一时刻，而且纵向跟踪以检测退化，例如由[数据漂移](https://www.ibm.com/cloud/learn/data-drift)引起的退化，输入分布的变化会随着时间的推移降低模型准确性（参见[图13.7](ch019.xhtml#fig-data-drift)）。
- en: '![](../media/file213.svg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file213.svg)'
- en: 'Figure 13.7: **Data Drift Impact**: Declining model performance over time results
    from data drift, where the characteristics of production data diverge from the
    training dataset. Monitoring key metrics longitudinally allows MLOps engineers
    to detect this drift and trigger model retraining or data pipeline adjustments
    to maintain accuracy.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7：**数据漂移影响**：随着时间的推移，模型性能的下降是由数据漂移引起的，其中生产数据的特征与训练数据集相偏离。纵向监控关键指标允许MLOps工程师检测这种漂移并触发模型重新训练或数据管道调整，以保持准确性。
- en: Beyond static evaluation, MLOps encourages controlled deployment strategies
    that simulate production conditions while minimizing risk. One widely adopted
    method is [canary testing](https://martinfowler.com/bliki/CanaryRelease.html),
    in which the new model is deployed to a small fraction of users or queries. During
    this limited rollout, live performance metrics are monitored to assess system
    stability and user impact. For instance, an e-commerce platform deploys a new
    recommendation model to 5% of web traffic and observes metrics such as click-through
    rate, latency, and prediction accuracy. Only after the model demonstrates consistent
    and reliable performance is it promoted to full production.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 除了静态评估之外，MLOps鼓励采用受控部署策略，这些策略在模拟生产条件的同时最小化风险。一种广泛采用的方法是[金丝雀测试](https://martinfowler.com/bliki/CanaryRelease.html)，其中新模型被部署到一小部分用户或查询。在这次有限的推广期间，实时性能指标被监控以评估系统稳定性和用户影响。例如，一个电子商务平台将新的推荐模型部署到5%的网站流量中，并观察点击率、延迟和预测准确度等指标。只有当模型表现出一致和可靠的表现时，它才会被推广到全面生产。
- en: Cloud-based ML platforms further support model evaluation by enabling experiment
    logging, request replay, and synthetic test case generation. These capabilities
    allow teams to evaluate different models under identical conditions, facilitating
    comparisons and root-cause analysis. Tools such as [Weights and Biases](https://wandb.ai/)
    automate aspects of this process by capturing training artifacts, recording hyperparameter
    configurations, and visualizing performance metrics across experiments. These
    tools integrate directly into training and deployment pipelines, improving transparency
    and traceability.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 基于云的ML平台通过启用实验日志记录、请求回放和合成测试用例生成来进一步支持模型评估。这些功能允许团队在相同条件下评估不同的模型，促进比较和根本原因分析。例如，[Weights
    and Biases](https://wandb.ai/)等工具通过捕获训练工件、记录超参数配置和可视化实验中的性能指标来自动化这一过程。这些工具直接集成到训练和部署管道中，提高了透明度和可追溯性。
- en: While automation is central to MLOps evaluation practices, human oversight remains
    essential. Automated tests may fail to capture nuanced performance issues, such
    as poor generalization on rare subpopulations or shifts in user behavior. Therefore,
    teams combine quantitative evaluation with qualitative review, particularly for
    models deployed in high-stakes or regulated environments. This human-in-the-loop
    validation becomes especially critical for social impact applications, where model
    failures can have direct consequences on vulnerable populations.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自动化是MLOps评估实践的核心，但人工监督仍然至关重要。自动化测试可能无法捕捉到细微的性能问题，例如在罕见子群体上的泛化能力差或用户行为的变化。因此，团队将定量评估与定性审查相结合，尤其是在部署在高风险或受监管环境中的模型时。这种人工在环验证对于社会影响应用尤为重要，因为模型故障可能对易受伤害的群体产生直接后果。
- en: This multi-stage evaluation process bridges offline testing and live system
    monitoring, ensuring that models not only meet technical benchmarks but also behave
    predictably and responsibly under real-world conditions. These evaluation practices
    reduce deployment risk and help maintain the reliability of machine learning systems
    over time, completing the development infrastructure foundation necessary for
    production deployment.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个多阶段评估过程连接了离线测试和实时系统监控，确保模型不仅满足技术基准，而且在现实世界条件下表现出可预测和负责任的行为。这些评估实践降低了部署风险，并有助于随着时间的推移维护机器学习系统的可靠性，完成了生产部署所需的发展基础设施基础。
- en: Infrastructure Integration Summary
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础设施集成摘要
- en: The infrastructure and development components examined in this section establish
    the foundation for reliable machine learning operations. These systems transform
    ad hoc experimentation into structured workflows that support reproducibility,
    collaboration, and continuous improvement.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中检查的基础设施和开发组件为可靠的机器学习操作奠定了基础。这些系统将临时实验转变为结构化工作流程，支持可重复性、协作和持续改进。
- en: '**Data infrastructure** provides the foundation through feature stores that
    enable feature reuse across projects, versioning systems that track data lineage
    and evolution, and validation frameworks that ensure data quality throughout the
    pipeline. Building on the data management foundations from [Chapter 6](ch012.xhtml#sec-data-engineering),
    these components extend basic capabilities to production contexts where multiple
    teams and models depend on shared data assets.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据基础设施**通过特征存储提供基础，该存储允许跨项目重用特征，版本控制系统跟踪数据谱系和演变，以及验证框架确保整个管道中的数据质量。基于[第6章](ch012.xhtml#sec-data-engineering)中的数据管理基础，这些组件将基本能力扩展到生产环境，其中多个团队和模型依赖于共享的数据资产。'
- en: '**Continuous pipelines** automate the ML lifecycle through CI/CD systems adapted
    for machine learning workflows. Unlike traditional software CI/CD that focuses
    solely on code, ML pipelines orchestrate data validation, feature transformation,
    model training, and evaluation in integrated workflows. Training pipelines specifically
    manage the computationally intensive process of model development, coordinating
    resource allocation, hyperparameter optimization, and experiment tracking. These
    automated workflows enable teams to iterate rapidly while maintaining reproducibility
    and quality standards.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**持续管道**通过为机器学习工作流程定制的CI/CD系统自动化ML生命周期。与仅关注代码的传统软件CI/CD不同，ML管道在集成工作流程中协调数据验证、特征转换、模型训练和评估。训练管道特别管理模型开发的计算密集型过程，协调资源分配、超参数优化和实验跟踪。这些自动化工作流程使团队能够快速迭代，同时保持可重复性和质量标准。'
- en: '**Model validation** bridges development and production through systematic
    evaluation that extends beyond offline metrics. Validation strategies combine
    performance benchmarking on held-out datasets with canary testing in production
    environments, allowing teams to detect issues before full deployment. This multi-stage
    validation recognizes that models must perform not just on static test sets but
    under dynamic real-world conditions where data distributions shift and user behavior
    evolves.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型验证**通过超出离线指标的系统性评估将开发和生产连接起来。验证策略结合了保留数据集上的性能基准测试和在生产环境中的金丝雀测试，使团队能够在全面部署之前检测到问题。这种多阶段验证认识到模型不仅需要在静态测试集上表现良好，还需要在数据分布变化和用户行为演变的动态现实世界条件下表现良好。'
- en: 'These infrastructure components directly address the operational challenges
    identified earlier through systematic engineering capabilities:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基础设施组件通过系统性的工程能力直接解决之前确定的运营挑战：
- en: Feature stores and data versioning solve data dependency debt by ensuring consistent,
    tracked feature access across training and serving
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征存储和数据版本控制通过确保在训练和部署过程中对特征访问的一致性和可追踪性来解决数据依赖债务
- en: CI/CD pipelines and model registries prevent correction cascades through controlled
    deployment and rollback mechanisms
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CI/CD管道和模型注册表通过受控部署和回滚机制防止纠正级联
- en: Automated workflows and lineage tracking eliminate undeclared consumer risks
    via explicit dependency management
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化工作流程和谱系跟踪通过明确的依赖管理消除未声明的消费者风险
- en: Modular pipeline architectures avoid pipeline debt through reusable, well-defined
    component interfaces
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模块化管道架构通过可重用、定义良好的组件接口避免管道债务
- en: 'However, deploying a validated model represents only the beginning of the production
    journey. The infrastructure enables reliable model development, but production
    operations must address the dynamic challenges of maintaining system performance
    under real-world conditions: handling data drift, managing system failures, and
    adapting to evolving requirements without service disruption.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，部署一个经过验证的模型只是生产旅程的开始。该基础设施支持可靠地开发模型，但生产运营必须解决在现实世界条件下维持系统性能的动态挑战：处理数据漂移、管理系统故障以及适应不断变化的需求而不中断服务。
- en: Production Operations
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产运营
- en: 'Building directly on the infrastructure foundation established above, production
    operations transform validated models into reliable services that maintain performance
    under real-world conditions. These operations must handle the diverse requirements
    established in preceding chapters: managing model updates across distributed edge
    devices without centralized visibility ([Chapter 14](ch020.xhtml#sec-ondevice-learning)),
    maintaining security controls during runtime inference and model updates ([Chapter 15](ch021.xhtml#sec-security-privacy)),
    and detecting performance degradation from adversarial attacks or distribution
    shifts ([Chapter 16](ch022.xhtml#sec-robust-ai)). This operational layer implements
    monitoring, governance, and deployment strategies that enable these specialized
    capabilities to function together reliably at scale.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述基础设施基础上直接构建，生产操作将验证后的模型转化为在现实世界条件下保持性能的可靠服务。这些操作必须处理前几章中确立的多样化需求：在分布式边缘设备上管理模型更新而无需集中可见性（[第14章](ch020.xhtml#sec-ondevice-learning)）、在运行时推理和模型更新期间维护安全控制（[第15章](ch021.xhtml#sec-security-privacy)）以及检测来自对抗攻击或分布变化的性能退化（[第16章](ch022.xhtml#sec-robust-ai)）。这一操作层实现了监控、治理和部署策略，使这些专门功能能够在大规模下可靠地协同工作。
- en: This section explores the deployment patterns, serving infrastructure, monitoring
    systems, and governance frameworks that transform validated models into production
    services capable of operating reliably at scale.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了将验证后的模型转化为能够可靠地在大规模下运行的生产服务的部署模式、服务基础设施、监控系统以及治理框架。
- en: Production operations introduce challenges that extend beyond model development.
    Deployed systems must handle variable loads, maintain consistent latency under
    diverse conditions, recover gracefully from failures, and adapt to evolving data
    distributions without disrupting service. These requirements demand specialized
    infrastructure, monitoring capabilities, and operational practices that complement
    the development workflows established in the previous section.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 生产操作引入了超出模型开发的挑战。部署的系统必须处理可变负载，在多种条件下保持一致的延迟，优雅地从故障中恢复，并在不中断服务的情况下适应不断变化的数据分布。这些需求要求专门的架构、监控能力和操作实践，以补充上一节中确立的开发工作流程。
- en: Model Deployment and Serving
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型部署和服务
- en: Once a model has been trained and validated, it must be integrated into a production
    environment where it can deliver predictions at scale. This process involves packaging
    the model with its dependencies, managing versions, and deploying it in a way
    that aligns with performance, reliability, and governance requirements. Deployment
    transforms a static artifact into a live system component. Serving ensures that
    the model is accessible, reliable, and efficient in responding to inference requests.
    Together, these components bridge model development and real-world impact.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型经过训练和验证，就必须将其集成到生产环境中，以便能够大规模地提供预测。这个过程包括将模型及其依赖项打包，管理版本，并以符合性能、可靠性和治理要求的方式部署。部署将静态工件转化为活生生的系统组件。服务确保模型可访问、可靠，并且在响应推理请求时高效。这些组件共同连接了模型开发和现实世界的影响。
- en: Model Deployment
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型部署
- en: Teams need to properly package, test, and track ML models to reliably deploy
    them to production. MLOps introduces frameworks and procedures for actively versioning,
    deploying, monitoring, and updating models in sustainable ways.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 团队需要正确打包、测试和跟踪机器学习模型，以便可靠地将它们部署到生产环境中。MLOps引入了框架和程序，以可持续的方式积极版本控制、部署、监控和更新模型。
- en: One common approach to deployment involves containerizing models using containerization
    technologies[20](#fn20). This packaging approach ensures smooth portability across
    environments, making deployment consistent and predictable.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的一种常见方法涉及使用容器化技术[20](#fn20)对模型进行容器化。这种打包方法确保了环境间的平滑可移植性，使部署一致且可预测。
- en: Production deployment requires frameworks that handle model packaging, versioning,
    and integration with serving infrastructure. Tools like MLflow and model registries
    manage these deployment artifacts, while serving-specific frameworks (detailed
    in the Inference Serving section) handle the runtime optimization and scaling
    requirements.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 生产部署需要处理模型打包、版本管理和与服务基础设施集成的框架。像MLflow和模型注册表这样的工具管理这些部署工件，而服务特定的框架（在推理服务部分详细说明）处理运行时优化和扩展需求。
- en: Before full-scale rollout, teams deploy updated models to staging or QA environments[21](#fn21)
    to rigorously test performance.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在全面推广之前，团队将更新的模型部署到预发布或 QA 环境中[21](#fn21)，以严格测试性能。
- en: Techniques such as shadow deployments, canary testing[22](#fn22), and blue-green
    deployment[23](#fn23) are used to validate new models incrementally. As described
    in our evaluation frameworks, these controlled deployment strategies enable safe
    model validation in production. Robust rollback procedures are essential to handle
    unexpected issues, reverting systems to the previous stable model version to ensure
    minimal disruption.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用如影子部署、金丝雀测试[22](#fn22) 和蓝绿部署[23](#fn23) 等技术逐步验证新模型。正如我们在评估框架中所描述的，这些受控部署策略使生产中的模型验证更加安全。稳健的回滚程序对于处理意外问题至关重要，可以将系统回滚到先前的稳定模型版本，以确保最小化中断。
- en: 'When canary deployments reveal problems at partial traffic levels (e.g., issues
    appearing at 30% traffic but not at 5%), teams need systematic debugging strategies.
    Effective diagnosis requires correlating multiple signals: performance metrics
    from [Chapter 12](ch018.xhtml#sec-benchmarking-ai), data distribution analysis
    to detect drift, and feature importance shifts that might explain degradation.
    Teams maintain debug toolkits including A/B test[24](#fn24) analysis frameworks,
    feature attribution tools, and data slice analyzers that identify which subpopulations
    are experiencing degraded performance.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当金丝雀部署在部分流量级别（例如，在 30% 流量中出现问题但不在 5% 流量中出现）时，团队需要系统性的调试策略。有效的诊断需要关联多个信号：来自 [第
    12 章](ch018.xhtml#sec-benchmarking-ai) 的性能指标、数据分布分析以检测漂移，以及可能解释性能下降的特征重要性变化。团队维护调试工具包，包括
    A/B 测试[24](#fn24) 分析框架、特征归因工具和数据切片分析器，以确定哪些子群体正在经历性能下降。
- en: Integration with CI/CD pipelines further automates the deployment and rollback
    process, enabling efficient iteration cycles.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CI/CD 管道集成进一步自动化了部署和回滚过程，使迭代周期更高效。
- en: Model registries, such as [Vertex AI’s model registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction),
    act as centralized repositories for storing and managing trained models. These
    registries not only facilitate version comparisons but also often include access
    to base models, which may be open source, proprietary, or a hybrid (e.g., [LLAMA](https://ai.meta.com/llama/)).
    Deploying a model from the registry to an inference endpoint is streamlined, handling
    resource provisioning, model weight downloads, and hosting.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 模型注册库，如 [Vertex AI 的模型注册库](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)，作为存储和管理训练模型的集中式仓库。这些注册库不仅便于版本比较，而且通常包括对基础模型的访问，这些模型可能是开源的、专有的或混合的（例如，[LLAMA](https://ai.meta.com/llama/)）。从注册库将模型部署到推理端点是简化的，包括资源分配、模型权重下载和托管。
- en: Inference endpoints typically expose the deployed model via REST APIs for real-time
    predictions. Depending on performance requirements, teams can configure resources,
    such as GPU accelerators, to meet latency and throughput targets. Some providers
    also offer flexible options like serverless[25](#fn25) or batch inference, eliminating
    the need for persistent endpoints and enabling cost-efficient, scalable deployments.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 推理端点通常通过 REST API 部署模型以进行实时预测。根据性能要求，团队可以配置资源，例如 GPU 加速器，以满足延迟和吞吐量目标。一些提供商还提供灵活的选项，如无服务器[25](#fn25)
    或批量推理，从而消除对持久端点的需求，并实现成本效益高、可扩展的部署。
- en: To maintain lineage and auditability, teams track model artifacts, including
    scripts, weights, logs, and metrics, using tools like [MLflow](https://mlflow.org/)[26](#fn26).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持可追溯性和可审计性，团队使用 [MLflow](https://mlflow.org/)[26](#fn26) 等工具跟踪模型工件，包括脚本、权重、日志和指标。
- en: By leveraging these tools and practices, teams can deploy ML models resiliently,
    ensuring smooth transitions between versions, maintaining production stability,
    and optimizing performance across diverse use cases.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用这些工具和实践，团队可以可靠地部署机器学习模型，确保版本之间的平稳过渡，保持生产稳定性，并优化各种用例的性能。
- en: Inference Serving
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理服务
- en: Once a model has been deployed, the final stage in operationalizing machine
    learning is to make it accessible to downstream applications or end-users. Serving
    infrastructure provides the interface between trained models and real-world systems,
    enabling predictions to be delivered reliably and efficiently. In large-scale
    settings, such as social media platforms or e-commerce services, serving systems
    may process tens of trillions of inference queries per day ([C.-J. Wu et al. 2019](ch058.xhtml#ref-wu2019machine)).
    The measurement frameworks established in [Chapter 12](ch018.xhtml#sec-benchmarking-ai)
    become essential for validating performance claims and establishing production
    baselines. Meeting such demand requires careful design to balance latency, scalability,
    and robustness.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被部署，机器学习运营的最后阶段就是使其对下游应用程序或最终用户可用。服务基础设施在训练模型和现实世界系统之间提供了接口，使预测能够可靠且高效地交付。在大型设置中，如社交媒体平台或电子商务服务，服务系统每天可能处理数十万亿个推理查询（[C.-J.
    Wu等，2019](ch058.xhtml#ref-wu2019machine)）。第12章中建立的测量框架对于验证性能声明和建立生产基线变得至关重要。满足这种需求需要仔细设计，以平衡延迟、可扩展性和鲁棒性。
- en: To address these challenges, production-grade serving frameworks have emerged.
    Tools such as [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)[27](#fn27),
    [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server)[28](#fn28),
    and [KServe](https://kserve.github.io/website/latest/)[29](#fn29) provide standardized
    mechanisms for deploying, versioning, and scaling machine learning models across
    heterogeneous infrastructure. These frameworks abstract many of the lower-level
    concerns, allowing teams to focus on system behavior, integration, and performance
    targets.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，生产级服务框架已经出现。例如，[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)[27](#fn27)、[NVIDIA
    Triton Inference Server](https://developer.nvidia.com/triton-inference-server)[28](#fn28)和[KServe](https://kserve.github.io/website/latest/)[29](#fn29)提供了在异构基础设施上部署、版本控制和扩展机器学习模型的标准化机制。这些框架抽象了许多底层关注点，使团队能够专注于系统行为、集成和性能目标。
- en: 'Model serving architectures are typically designed around three broad paradigms:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务架构通常围绕三个广泛范式设计：
- en: Online Serving, which provides low-latency, real-time predictions for interactive
    systems such as recommendation engines or fraud detection.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在线服务，它为推荐引擎或欺诈检测等交互式系统提供低延迟、实时的预测。
- en: Offline Serving, which processes large batches of data asynchronously, typically
    in scheduled jobs used for reporting or model retraining.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 离线服务，它异步处理大量数据，通常用于计划中的作业，用于报告或模型重新训练。
- en: Near-Online (Semi-Synchronous) Serving, which offers a balance between latency
    and throughput, appropriate for scenarios like chatbots or semi-interactive analytics.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 近在线（半同步）服务，它提供了延迟和吞吐量之间的平衡，适用于聊天机器人或半交互式分析等场景。
- en: Each of these approaches introduces different constraints in terms of availability,
    responsiveness, and throughput. The efficiency techniques from [Chapter 9](ch015.xhtml#sec-efficient-ai)
    become crucial for meeting these performance requirements, particularly when serving
    models at scale. Serving systems are therefore constructed to meet specific Service
    Level Agreements (SLAs)[30](#fn30) and Service Level Objectives (SLOs)[31](#fn31),
    which quantify acceptable performance boundaries along dimensions such as latency,
    error rates, and uptime. Achieving these goals requires a range of optimizations
    in request handling, scheduling, and resource allocation.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法中的每一种都在可用性、响应性和吞吐量方面引入了不同的约束。第9章中提到的效率技术对于满足这些性能要求至关重要，尤其是在大规模部署模型时。因此，服务系统被构建来满足特定的服务水平协议（SLAs）[30](#fn30)和服务水平目标（SLOs）[31](#fn31)，这些协议量化了可接受的性能边界，如延迟、错误率和正常运行时间。实现这些目标需要在请求处理、调度和资源分配方面进行一系列优化。
- en: A number of serving system design strategies are commonly employed to meet these
    requirements. Request scheduling and batching aggregate inference requests to
    improve throughput and hardware utilization. For instance, Clipper ([Crankshaw
    et al. 2017](ch058.xhtml#ref-crankshaw2017clipper)) applies batching and caching
    to reduce response times in online settings. Model instance selection and routing
    dynamically assign requests to model variants based on system load or user-defined
    constraints; INFaaS ([Romero et al. 2021](ch058.xhtml#ref-romero2021infaas)) illustrates
    this approach by optimizing accuracy-latency trade-offs across variant models.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些需求，通常会采用多种服务系统设计策略。请求调度和批处理将推理请求聚合起来，以提高吞吐量和硬件利用率。例如，Clipper ([Crankshaw
    等人 2017](ch058.xhtml#ref-crankshaw2017clipper)) 通过批处理和缓存技术，在在线环境中减少响应时间。模型实例选择和路由根据系统负载或用户定义的约束动态地将请求分配给模型变体；INFaaS
    ([Romero 等人 2021](ch058.xhtml#ref-romero2021infaas)) 通过在变体模型之间优化准确度-延迟权衡来说明这种方法。
- en: '**Request scheduling and batching**: Efficiently manages incoming ML inference
    requests, optimizing performance through smart queuing and grouping strategies.
    Systems like Clipper ([Crankshaw et al. 2017](ch058.xhtml#ref-crankshaw2017clipper))
    introduce low-latency online prediction serving with caching and batching techniques.'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**请求调度和批处理**：通过智能排队和分组策略有效地管理传入的机器学习推理请求，通过优化性能。像Clipper ([Crankshaw 等人 2017](ch058.xhtml#ref-crankshaw2017clipper))
    这样的系统通过缓存和批处理技术引入了低延迟的在线预测服务。'
- en: '**Model instance selection and routing**: Intelligent algorithms direct requests
    to appropriate model versions or instances. INFaaS ([Romero et al. 2021](ch058.xhtml#ref-romero2021infaas))
    explores this by generating model-variants and efficiently exploring the trade-off
    space based on performance and accuracy requirements.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型实例选择和路由**：智能算法将请求引导到适当的模型版本或实例。INFaaS ([Romero 等人 2021](ch058.xhtml#ref-romero2021infaas))
    通过生成模型变体并根据性能和准确度要求有效地探索权衡空间来探索这一点。'
- en: '**Load balancing**: Distributes workloads evenly across multiple serving instances.
    MArk (Model Ark) ([C. Zhang et al. 2019](ch058.xhtml#ref-zhang2019mark)) demonstrates
    effective load balancing techniques for ML serving systems.'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**负载均衡**：在多个服务实例之间均匀分配工作负载。MARK (模型Ark) ([C. Zhang 等人 2019](ch058.xhtml#ref-zhang2019mark))
    展示了用于机器学习服务系统的有效负载均衡技术。'
- en: '**Model instance autoscaling**: Dynamically adjusts capacity based on demand.
    Both INFaaS ([Romero et al. 2021](ch058.xhtml#ref-romero2021infaas)) and MArk
    ([C. Zhang et al. 2019](ch058.xhtml#ref-zhang2019mark)) incorporate autoscaling
    capabilities to handle workload fluctuations efficiently.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型实例自动扩展**：根据需求动态调整容量。INFaaS ([Romero 等人 2021](ch058.xhtml#ref-romero2021infaas))
    和 MArk ([C. Zhang 等人 2019](ch058.xhtml#ref-zhang2019mark)) 都集成了自动扩展功能，以有效地处理工作负载波动。'
- en: '**Model orchestration**: Manages model execution, enabling parallel processing
    and strategic resource allocation. AlpaServe ([Z. Li et al. 2023](ch058.xhtml#ref-li2023alpaserve))
    demonstrates advanced techniques for handling large models and complex serving
    scenarios.'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型编排**：管理模型执行，实现并行处理和战略资源分配。AlpaServe ([Z. Li 等人 2023](ch058.xhtml#ref-li2023alpaserve))
    展示了处理大型模型和复杂服务场景的高级技术。'
- en: '**Execution time prediction**: Systems like Clockwork ([Gujarati et al. 2020](ch058.xhtml#ref-gujarati2020serving))
    focus on high-performance serving by predicting execution times of individual
    inferences and efficiently using hardware accelerators.'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行时间预测**：像Clockwork ([Gujarati 等人 2020](ch058.xhtml#ref-gujarati2020serving))
    这样的系统通过预测单个推理的执行时间并有效地使用硬件加速器来关注高性能服务。'
- en: In more complex inference scenarios, model orchestration coordinates the execution
    of multi-stage models or distributed components. AlpaServe ([Z. Li et al. 2023](ch058.xhtml#ref-li2023alpaserve))
    exemplifies this by enabling efficient serving of large foundation models through
    coordinated resource allocation. Finally, execution time prediction enables systems
    to anticipate latency for individual requests. Clockwork ([Gujarati et al. 2020](ch058.xhtml#ref-gujarati2020serving))
    uses this capability to reduce tail latency and improve scheduling efficiency
    under high load.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在更复杂的推理场景中，模型编排协调多阶段模型或分布式组件的执行。AlpaServe ([Z. Li 等人 2023](ch058.xhtml#ref-li2023alpaserve))
    通过协调资源分配，通过高效地提供大型基础模型来举例说明这一点。最后，执行时间预测使系统能够预测单个请求的延迟。Clockwork ([Gujarati 等人
    2020](ch058.xhtml#ref-gujarati2020serving)) 利用这一功能在负载高峰下减少尾部延迟并提高调度效率。
- en: While these systems differ in implementation, they collectively illustrate the
    critical techniques that underpin scalable and responsive ML-as-a-Service infrastructure.
    [Table 13.2](ch019.xhtml#tbl-serving-techniques) summarizes these strategies and
    highlights representative systems that implement them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些系统在实现上有所不同，但它们共同展示了支撑可扩展和响应式ML-as-a-Service基础设施的关键技术。[表13.2](ch019.xhtml#tbl-serving-techniques)总结了这些策略，并突出了实现它们的代表性系统。
- en: 'Table 13.2: **Serving System Techniques**: Scalable ML-as-a-service infrastructure
    relies on techniques like request scheduling and instance selection to optimize
    resource utilization and reduce latency under high load. The table summarizes
    key strategies and representative systems (clipper, for example) that implement
    them for efficient deployment of machine learning models.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 表13.2：**服务系统技术**：可扩展的ML-as-a-service基础设施依赖于请求调度和实例选择等技术来优化资源利用并减少高负载下的延迟。该表总结了关键策略和实现它们的代表性系统（例如clipper）。
- en: '| **Technique** | **Description** | **Example System** |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **描述** | **示例系统** |'
- en: '| --- | --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Request Scheduling & Batching** | Groups inference requests to improve
    throughput and reduce overhead | Clipper |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| **请求调度与批处理** | 将推理请求分组以提高吞吐量和降低开销 | Clipper |'
- en: '| **Instance Selection & Routing** | Dynamically assigns requests to model
    variants based on constraints | INFaaS |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **实例选择与路由** | 根据约束动态将请求分配给模型变体 | INFaaS |'
- en: '| **Load Balancing** | Distributes traffic across replicas to prevent bottlenecks
    | MArk |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| **负载均衡** | 在副本之间分配流量以防止瓶颈 | MArk |'
- en: '| **Autoscaling** | Adjusts model instances to match workload demands | INFaaS,
    MArk |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **自动扩展** | 调整模型实例以匹配工作负载需求 | INFaaS, MArk |'
- en: '| **Model Orchestration** | Coordinates execution across model components or
    pipelines | AlpaServe |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| **模型编排** | 协调模型组件或管道的执行 | AlpaServe |'
- en: '| **Execution Time Prediction** | Forecasts latency to optimize request scheduling
    | Clockwork |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **执行时间预测** | 预测延迟以优化请求调度 | Clockwork |'
- en: Together, these strategies form the foundation of robust model serving systems.
    When effectively integrated, they enable machine learning applications to meet
    performance targets while maintaining system-level efficiency and scalability.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这些策略共同构成了强大模型服务系统的基石。当有效集成时，它们使机器学习应用能够满足性能目标，同时保持系统级效率和可扩展性。
- en: Edge AI Deployment Patterns
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 边缘AI部署模式
- en: Edge AI represents a major shift in deployment architecture where machine learning
    inference occurs at or near the data source, rather than in centralized cloud
    infrastructure. This paradigm addresses critical constraints including latency
    requirements, bandwidth limitations, privacy concerns, and connectivity constraints
    that characterize real-world operational environments. According to industry projections,
    75% of ML inference will occur at the edge by 2025, making edge deployment patterns
    essential knowledge for MLOps practitioners ([Reddi et al. 2019a](ch058.xhtml#ref-reddi2023mlperf)).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘AI代表了部署架构的重大转变，其中机器学习推理发生在数据源附近，而不是在集中式云基础设施中。这种范式解决了包括延迟要求、带宽限制、隐私问题和连接性限制在内的关键约束，这些是现实世界操作环境的特征。根据行业预测，到2025年，75%的机器学习推理将在边缘发生，这使得边缘部署模式对于MLOps从业者来说是必备的知识([Reddi等人
    2019a](ch058.xhtml#ref-reddi2023mlperf))。
- en: Edge deployment introduces unique operational challenges that distinguish it
    from traditional cloud-centric MLOps. Resource constraints on edge devices require
    aggressive model optimization techniques including quantization, pruning, and
    knowledge distillation to achieve sub-1 MB memory footprints while maintaining
    acceptable accuracy. Power budgets for edge devices typically range from 10 mW
    for IoT sensors to 45 W for automotive systems, demanding power-aware inference
    scheduling and thermal management strategies. Real-time requirements for safety-critical
    applications necessitate deterministic inference timing with worst-case execution
    time guarantees under 10 ms for collision avoidance systems and sub-100 ms for
    interactive robotics applications.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘部署引入了独特的操作挑战，这些挑战使其与传统以云为中心的MLOps区分开来。边缘设备上的资源限制需要采用包括量化、剪枝和知识蒸馏在内的激进模型优化技术，以实现小于1
    MB的内存占用，同时保持可接受的准确性。边缘设备的功耗预算通常从物联网传感器的10 mW到汽车系统的45 W不等，这要求进行功耗感知的推理调度和热管理策略。对于安全关键型应用，实时需求需要确定性的推理时间，对于碰撞避免系统，最坏情况下的执行时间保证小于10
    ms，对于交互式机器人应用，小于100 ms。
- en: The operational architecture for edge AI systems typically follows hierarchical
    deployment patterns that distribute intelligence across multiple tiers. Sensor-level
    processing handles immediate data filtering and feature extraction with microcontroller-class
    devices consuming 1-100 mW. Edge gateway processing performs intermediate inference
    tasks using application processors with 1-10 W power budgets. Cloud coordination
    manages model distribution, aggregated learning, and complex reasoning tasks requiring
    GPU-class computational resources. This hierarchy enables system-wide optimization
    where computationally expensive operations migrate to higher tiers while latency-critical
    decisions remain local.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘AI系统的操作架构通常遵循分层部署模式，将智能分布在多个层级。传感器级处理使用微控制器级设备进行即时数据过滤和特征提取，功耗为1-100毫瓦。边缘网关处理使用具有1-10瓦功率预算的应用处理器执行中间推理任务。云协调管理模型分发、聚合学习和需要GPU级计算资源的复杂推理任务。这种层次结构使系统能够进行全局优化，其中计算密集型操作迁移到更高层级，而延迟关键决策保持本地化。
- en: The most resource-constrained edge AI scenarios involve TinyML deployment patterns,
    targeting microcontroller-based inference with memory constraints under 1 MB and
    power consumption measured in milliwatts. TinyML deployment requires specialized
    inference engines such as TensorFlow Lite Micro, CMSIS-NN, and hardware-specific
    optimized libraries that eliminate dynamic memory allocation and minimize computational
    overhead. Model architectures must be co-designed with hardware constraints, favoring
    depthwise convolutions, binary neural networks, and pruned models that achieve
    90%+ sparsity while maintaining task-specific accuracy requirements.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 最资源受限的边缘AI场景涉及TinyML部署模式，针对基于微控制器的推理，内存限制在1 MB以下，功耗以毫瓦计量。TinyML部署需要专门的推理引擎，如TensorFlow
    Lite Micro、CMSIS-NN以及针对特定硬件优化的库，这些库消除了动态内存分配并最小化了计算开销。模型架构必须与硬件约束协同设计，优先考虑深度卷积、二值神经网络和剪枝模型，这些模型在保持任务特定精度要求的同时实现了90%以上的稀疏度。
- en: Mobile AI operations extend this edge deployment paradigm to smartphones and
    tablets with moderate computational capabilities and strict power efficiency requirements.
    Mobile deployment leverages hardware acceleration through Neural Processing Units
    (NPUs), GPU compute shaders, and specialized instruction sets to achieve inference
    performance targets of 5-50 ms latency with power consumption under 500 mW. Mobile
    AI operations require sophisticated power management including dynamic frequency
    scaling, thermal throttling coordination, and background inference scheduling
    that balances performance against battery life and user experience constraints.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 移动AI操作将这种边缘部署范式扩展到具有适度计算能力和严格功耗要求的智能手机和平板电脑。移动部署利用神经网络单元（NPUs）、GPU计算着色器和专用指令集通过硬件加速来实现5-50毫秒的延迟目标，功耗低于500毫瓦。移动AI操作需要复杂的电源管理，包括动态频率缩放、热管理协调和后台推理调度，以平衡性能、电池寿命和用户体验限制。
- en: Critical operational capabilities for deployed edge systems include over-the-air
    model updates, which enable maintenance for systems that cannot be physically
    accessed. OTA update pipelines must implement secure, verified model distribution
    that prevents malicious model injection while ensuring update integrity through
    cryptographic signatures and rollback mechanisms. Edge devices require differential
    compression techniques that minimize bandwidth usage by transmitting only model
    parameter changes rather than complete model artifacts. Update scheduling must
    account for device connectivity patterns, power availability, and operational
    criticality to prevent update-induced service disruptions.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 部署边缘系统的关键操作能力包括空中模型更新，这允许对无法物理访问的系统进行维护。OTA更新管道必须实现安全、经过验证的模型分发，防止恶意模型注入，并通过加密签名和回滚机制确保更新完整性。边缘设备需要差分压缩技术，通过仅传输模型参数变化而不是完整的模型工件来最小化带宽使用。更新调度必须考虑设备连接模式、电源可用性和操作关键性，以防止更新引起的服务中断。
- en: Production edge AI systems implement real-time constraint management through
    systematic approaches to deadline analysis and resource allocation. Worst-case
    execution time (WCET) analysis ensures that inference operations complete within
    specified timing bounds even under adverse conditions including thermal throttling,
    memory contention, and interrupt service routines. Resource reservation mechanisms
    guarantee computational bandwidth for safety-critical inference tasks while enabling
    best-effort execution of non-critical workloads. Graceful degradation strategies
    enable systems to maintain essential functionality when resources become constrained
    by reducing model complexity, inference frequency, or feature completeness.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 生产级边缘人工智能系统通过系统性的截止时间分析和资源分配方法实现实时约束管理。最坏情况执行时间（WCET）分析确保即使在包括热管理、内存竞争和中断服务例程等不利条件下，推理操作也能在指定的时序范围内完成。资源预留机制确保关键推理任务的计算带宽，同时允许非关键工作负载的最佳努力执行。优雅降级策略使系统在资源受限时通过减少模型复杂性、推理频率或特征完整性来维持基本功能。
- en: Edge-cloud coordination patterns enable hybrid deployment architectures that
    optimize the distribution of inference workloads across computational tiers. Adaptive
    offloading strategies dynamically route inference requests between edge and cloud
    resources based on current system load, network conditions, and latency requirements.
    Feature caching at edge gateways reduces redundant computation by storing frequently
    accessed intermediate representations while maintaining data freshness through
    cache invalidation policies. Federated learning coordination enables edge devices
    to contribute to model improvement without transmitting raw data, addressing privacy
    constraints while maintaining system-wide learning capabilities.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘-云协调模式使混合部署架构优化推理工作负载在计算层之间的分布。自适应卸载策略根据当前系统负载、网络条件和延迟要求，动态地在边缘和云资源之间路由推理请求。在边缘网关中缓存特征可以减少冗余计算，通过存储频繁访问的中间表示，同时通过缓存失效策略保持数据的新鲜度。联邦学习协调使边缘设备能够在不传输原始数据的情况下贡献于模型改进，解决隐私约束的同时保持系统级的学习能力。
- en: The operational complexity of edge AI deployment requires specialized monitoring
    and debugging approaches adapted to resource-constrained environments. Lightweight
    telemetry systems capture essential performance metrics including inference latency,
    power consumption, and accuracy indicators while minimizing overhead on edge devices.
    Remote debugging capabilities enable engineers to diagnose deployed systems through
    secure channels that preserve privacy while providing sufficient visibility into
    system behavior. Health monitoring systems track device-level conditions including
    thermal status, battery levels, and connectivity quality to predict maintenance
    requirements and prevent catastrophic failures.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘人工智能部署的操作复杂性需要针对资源受限环境进行专门的监控和调试方法。轻量级遥测系统捕捉关键性能指标，包括推理延迟、功耗和准确性指标，同时最小化对边缘设备的开销。远程调试功能使工程师能够通过安全的通道诊断已部署的系统，在保护隐私的同时，提供对系统行为的充分可见性。健康监控系统跟踪设备级条件，包括热状态、电池水平和连接质量，以预测维护需求并防止灾难性故障。
- en: Resource constraint analysis underpins successful edge AI deployment by systematically
    modeling the trade-offs between computational capability, power consumption, memory
    utilization, and inference accuracy. Power budgeting frameworks establish operational
    envelopes that define sustainable workload configurations under varying environmental
    conditions and usage patterns. Memory optimization hierarchies guide the selection
    of model compression techniques, from parameter reduction through structural simplification
    to architectural modifications that reduce computational requirements.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 资源约束分析通过系统地建模计算能力、功耗、内存利用率和推理精度之间的权衡，为边缘人工智能的成功部署奠定基础。电力预算框架建立操作范围，定义在不同环境条件和使用模式下的可持续工作负载配置。内存优化层次结构指导选择模型压缩技术，从参数减少到结构简化，再到降低计算需求的架构修改。
- en: Edge AI deployment represents the operational frontier where MLOps practices
    must adapt to the physical constraints and distributed complexity of real-world
    systems. Success requires not only technical expertise in model optimization and
    embedded systems but also systematic approaches to distributed system management,
    security, and reliability engineering that ensure deployed systems remain functional
    across diverse operational environments.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘人工智能部署代表了 MLOps 实践必须适应的运营前沿，即现实世界系统的物理约束和分布式复杂性。成功不仅需要模型优化和嵌入式系统方面的技术专长，还需要系统性的分布式系统管理、安全和可靠性工程方法，以确保部署的系统能够在不同的运营环境中保持功能。
- en: Resource Management and Performance Monitoring
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 资源管理和性能监控
- en: The operational stability of a machine learning system depends on the robustness
    of its underlying infrastructure. Compute, storage, and networking resources must
    be provisioned, configured, and scaled to accommodate training workloads, deployment
    pipelines, and real-time inference. Beyond infrastructure provisioning, effective
    observability practices ensure that system behavior can be monitored, interpreted,
    and acted upon as conditions change.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统的运行稳定性取决于其底层基础设施的稳健性。计算、存储和网络资源必须进行配置、分配和扩展，以适应训练工作负载、部署管道和实时推理。除了基础设施配置之外，有效的可观察性实践确保系统能够在条件变化时进行监控、解释和采取行动。
- en: Infrastructure Management
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基础设施管理
- en: Scalable, resilient infrastructure is a foundational requirement for operationalizing
    machine learning systems. As models move from experimentation to production, MLOps
    teams must ensure that the underlying computational resources can support continuous
    integration, large-scale training, automated deployment, and real-time inference.
    This requires managing infrastructure not as static hardware, but as a dynamic,
    programmable, and versioned system.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展、弹性基础设施是实施机器学习系统的基本要求。随着模型从实验阶段过渡到生产阶段，MLOps 团队必须确保底层计算资源能够支持持续集成、大规模训练、自动化部署和实时推理。这需要将基础设施管理视为动态的、可编程的、版本化的系统，而不是静态的硬件。
- en: 'To achieve this, teams adopt the practice of Infrastructure as Code (IaC),
    a paradigm that transforms how computing infrastructure is managed. Rather than
    manually configuring servers, networks, and storage through graphical interfaces
    or command-line tools, a process prone to human error and difficult to reproduce,
    IaC treats infrastructure configuration as software code. This code describes
    the desired state of infrastructure resources in text files that are version-controlled,
    reviewed, and automatically executed. Just as software developers write code to
    define application behavior, infrastructure engineers write code to define computing
    environments. This transformation brings software engineering best practices to
    infrastructure management: changes are tracked through version control, configurations
    can be tested before deployment, and entire environments can be reliably reproduced
    from their code definitions.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，团队采用基础设施即代码（IaC）的实践，这是一种改变计算基础设施管理方式的范例。而不是通过图形界面或命令行工具手动配置服务器、网络和存储，这个过程容易出错且难以重现，IaC
    将基础设施配置视为软件代码。此代码以文本文件的形式描述了基础设施资源的期望状态，这些文件是版本控制的、经过审查的，并且可以自动执行。正如软件开发者编写代码来定义应用程序行为一样，基础设施工程师编写代码来定义计算环境。这种转变将软件工程的最佳实践引入基础设施管理：通过版本控制跟踪更改，在部署前测试配置，并可以从其代码定义中可靠地重现整个环境。
- en: Tools such as [Terraform](https://www.terraform.io/), [AWS CloudFormation](https://aws.amazon.com/cloudformation/),
    and [Ansible](https://www.ansible.com/) support this paradigm by enabling teams
    to version infrastructure definitions alongside application code. In MLOps settings,
    Terraform is widely used to provision and manage resources across public cloud
    platforms such as [AWS](https://aws.amazon.com/), [Google Cloud Platform](https://cloud.google.com/),
    and [Microsoft Azure](https://azure.microsoft.com/).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [Terraform](https://www.terraform.io/)、[AWS CloudFormation](https://aws.amazon.com/cloudformation/)
    和 [Ansible](https://www.ansible.com/) 等工具通过使团队能够与应用程序代码一起版本控制基础设施定义来支持这一范式。在 MLOps
    设置中，Terraform 广泛用于在公共云平台（如 [AWS](https://aws.amazon.com/)、[Google Cloud Platform](https://cloud.google.com/)
    和 [Microsoft Azure](https://azure.microsoft.com/)）上配置和管理资源。
- en: Infrastructure management spans the full lifecycle of ML systems. During model
    training, teams use IaC scripts to allocate compute instances with GPU or TPU
    accelerators, configure distributed storage, and deploy container clusters. These
    configurations ensure that data scientists and ML engineers access reproducible
    environments with the required computational capacity. Because infrastructure
    definitions are stored as code, they are audited, reused, and integrated into
    CI/CD pipelines to ensure consistency across environments.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施管理涵盖了机器学习系统的整个生命周期。在模型训练期间，团队使用IaC脚本来分配带有GPU或TPU加速器的计算实例，配置分布式存储，并部署容器集群。这些配置确保数据科学家和机器学习工程师能够访问具有所需计算能力的可重复环境。由于基础设施定义被存储为代码，它们可以接受审计、重用，并集成到CI/CD管道中，以确保跨环境的一致性。
- en: Containerization plays a critical role in making ML workloads portable and consistent.
    Tools like [Docker](https://www.docker.com/) encapsulate models and their dependencies
    into isolated units, while orchestration systems such as [Kubernetes](https://kubernetes.io/)
    manage containerized workloads across clusters. These systems enable rapid deployment,
    resource allocation, and scaling, capabilities that are essential in production
    environments where workloads can vary dynamically.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 容器化在使机器学习工作负载可移植和一致化方面发挥着关键作用。像[Docker](https://www.docker.com/)这样的工具将模型及其依赖项封装成独立的单元，而像[Kubernetes](https://kubernetes.io/)这样的编排系统则管理跨集群的容器化工作负载。这些系统使得快速部署、资源分配和扩展成为可能，这些能力在生产环境中至关重要，因为工作负载可以动态变化。
- en: To handle changes in workload intensity, including spikes during hyperparameter
    tuning and surges in prediction traffic, teams rely on cloud elasticity and autoscaling[32](#fn32).
    Cloud platforms support on-demand provisioning and horizontal scaling of infrastructure
    resources. [Autoscaling mechanisms](https://aws.amazon.com/autoscaling/) automatically
    adjust compute capacity based on usage metrics, enabling teams to optimize for
    both performance and cost-efficiency.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理工作负载强度的变化，包括超参数调整期间的峰值和预测流量激增，团队依赖云弹性自动扩展[32](#fn32)。云平台支持按需提供和水平扩展基础设施资源。[自动扩展机制](https://aws.amazon.com/autoscaling/)根据使用指标自动调整计算能力，使团队能够在性能和成本效率之间进行优化。
- en: Infrastructure in MLOps is not limited to the cloud. Many deployments span on-premises,
    cloud, and edge environments, depending on latency, privacy, or regulatory constraints.
    A robust infrastructure management strategy must accommodate this diversity by
    offering flexible deployment targets and consistent configuration management across
    environments.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLOps中，基础设施不仅限于云。许多部署跨越本地、云和边缘环境，这取决于延迟、隐私或监管限制。一个强大的基础设施管理策略必须通过提供灵活的部署目标和跨环境的一致配置管理来适应这种多样性。
- en: To illustrate, consider a scenario in which a team uses Terraform to deploy
    a Kubernetes cluster on Google Cloud Platform. The cluster is configured to host
    containerized TensorFlow models that serve predictions via HTTP APIs. As user
    demand increases, Kubernetes automatically scales the number of pods to handle
    the load. Meanwhile, CI/CD pipelines update the model containers based on retraining
    cycles, and monitoring tools track cluster performance, latency, and resource
    utilization. All infrastructure components, ranging from network configurations
    to compute quotas, are managed as version-controlled code, ensuring reproducibility
    and auditability.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个场景为例，一个团队使用Terraform在Google Cloud Platform上部署一个Kubernetes集群。该集群配置为托管容器化的TensorFlow模型，通过HTTP
    API提供预测服务。随着用户需求的增加，Kubernetes自动扩展pods的数量以处理负载。同时，CI/CD管道根据重新训练周期更新模型容器，监控工具跟踪集群性能、延迟和资源利用率。从网络配置到计算配额的所有基础设施组件都作为版本控制的代码进行管理，确保可重复性和可审计性。
- en: By adopting Infrastructure as Code, leveraging cloud-native orchestration, and
    supporting automated scaling, MLOps teams gain the ability to provision and maintain
    the resources required for machine learning at production scale. This infrastructure
    layer underpins the entire MLOps stack, enabling reliable training, deployment,
    and serving workflows.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用基础设施即代码、利用云原生编排和支持自动扩展，MLOps团队能够以生产规模配置和维护机器学习所需的资源。这一基础设施层支撑着整个MLOps堆栈，使得可靠的训练、部署和服务工作流程成为可能。
- en: 'While these foundational capabilities address infrastructure provisioning and
    management, the operational reality of ML systems introduces unique resource optimization
    challenges that extend beyond traditional web service scaling patterns. Infrastructure
    resource management in MLOps becomes a multi-dimensional optimization problem,
    requiring teams to balance competing objectives: computational cost, model accuracy,
    inference latency, and training throughput.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些基础能力解决了基础设施的提供和管理问题，但机器学习系统的运营现实引入了独特的资源优化挑战，这些挑战超出了传统Web服务扩展模式。在MLOps中，基础设施资源管理成为一个多维度的优化问题，需要团队平衡相互竞争的目标：计算成本、模型精度、推理延迟和训练吞吐量。
- en: ML workloads exhibit different resource consumption patterns compared to stateless
    web applications. Training workloads demonstrate bursty resource requirements,
    scaling from zero to thousands of GPUs during model development phases, then returning
    to minimal consumption during validation periods. This creates a tension between
    resource utilization efficiency and time-to-insight that traditional scaling approaches
    cannot adequately address. Conversely, inference workloads present steady resource
    consumption patterns with strict latency requirements that must be maintained
    under variable traffic patterns.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 与无状态Web应用相比，机器学习工作负载表现出不同的资源消耗模式。训练工作负载显示出突发性资源需求，在模型开发阶段从零扩展到数千个GPU，然后在验证期间回到最小消耗。这导致了资源利用效率与洞察力获取时间之间的紧张关系，传统扩展方法无法充分解决。相反，推理工作负载呈现稳定的资源消耗模式，具有严格的延迟要求，必须在变化的流量模式下保持。
- en: The optimization challenge intensifies when considering the interdependencies
    between training frequency, model complexity, and serving infrastructure costs.
    Effective resource management requires holistic approaches that model the entire
    system rather than optimizing individual components in isolation, taking into
    account factors such as data pipeline throughput, model retraining schedules,
    and serving capacity planning.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑到训练频率、模型复杂性和服务基础设施成本之间的相互依赖性时，优化挑战变得更加严峻。有效的资源管理需要整体方法，而不是孤立优化单个组件，需要考虑数据管道吞吐量、模型重新训练计划和服务器容量规划等因素。
- en: 'Hardware-aware resource optimization emerges as a critical operational discipline
    that bridges infrastructure efficiency with model performance. Production MLOps
    teams must establish utilization targets that balance cost efficiency against
    operational reliability: GPU utilization should consistently exceed 80% for batch
    training workloads to justify hardware costs, while serving workloads require
    sustained utilization above 60% to maintain economically viable inference operations.
    Memory bandwidth utilization patterns become equally important, as underutilized
    memory interfaces indicate suboptimal data pipeline configurations that can degrade
    training throughput by 30-50%.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件感知的资源优化成为一项关键的操作学科，它将基础设施效率与模型性能联系起来。生产MLOps团队必须建立利用目标，在成本效率和运营可靠性之间取得平衡：对于批量训练工作负载，GPU利用率应持续超过80%以证明硬件成本合理，而服务工作负载需要维持超过60%的持续利用率以保持经济可行的推理操作。内存带宽利用模式同样重要，因为未充分利用的内存接口表明数据管道配置不佳，这可能会降低训练吞吐量30-50%。
- en: 'Operational resource allocation extends beyond simple utilization metrics to
    encompass power budget management across mixed workloads. Production deployments
    typically allocate 60-70% of power budgets to training operations during development
    cycles, reserving 30-40% for sustained inference workloads. This allocation shifts
    dynamically based on business priorities: recommendation systems might reallocate
    power toward inference during peak traffic periods, while research environments
    prioritize training resource availability. Thermal management considerations become
    operational constraints rather than hardware design concerns, as sustained high-utilization
    workloads must be scheduled with cooling capacity limitations and thermal throttling
    thresholds that can impact SLA compliance.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 运营资源分配不仅限于简单的利用率指标，还包括跨混合工作负载的电力预算管理。生产部署通常在开发周期中将60-70%的电力预算分配给训练操作，保留30-40%用于持续的推理工作负载。这种分配会根据业务优先级动态调整：推荐系统可能在流量高峰期间重新分配推理的电力，而研究环境则优先考虑训练资源的可用性。热管理考虑因素成为运营限制，而不是硬件设计问题，因为持续的高利用率工作负载必须根据冷却能力限制和热限制阈值进行调度，这些限制可能会影响服务等级协议的遵守。
- en: Model and Infrastructure Monitoring
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型和基础设施监控
- en: Monitoring is a critical function in MLOps, enabling teams to maintain operational
    visibility over machine learning systems deployed in production. Once a model
    is live, it becomes exposed to real-world inputs, evolving data distributions,
    and shifting user behavior. Without continuous monitoring, it becomes difficult
    to detect performance degradation, data quality issues, or system failures in
    a timely manner.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 监控是MLOps中的关键功能，使团队能够在生产中部署的机器学习系统中保持操作可见性。一旦模型上线，它就会接触到现实世界的输入、不断变化的数据分布和用户行为的转变。没有持续的监控，及时检测性能下降、数据质量问题或系统故障变得困难。
- en: Effective monitoring spans both model behavior and infrastructure performance.
    On the model side, teams track metrics such as accuracy, precision, recall, and
    the [confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)
    using live or sampled predictions. By evaluating these metrics over time, they
    can detect whether the model’s performance remains stable or begins to drift.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的监控涵盖了模型行为和基础设施性能。在模型方面，团队通过实时或样本预测跟踪诸如准确率、精确度、召回率和[混淆矩阵](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)等指标。通过随时间评估这些指标，他们可以检测模型性能是否保持稳定或开始漂移。
- en: 'Production ML systems face model drift[33](#fn33) (see [Section 13.4.2.3](ch019.xhtml#sec-ml-operations-model-validation-cb32)
    for detailed analysis), which manifests in two main forms:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 生产级机器学习系统面临模型漂移[33](#fn33)（参见[第13.4.2.3节](ch019.xhtml#sec-ml-operations-model-validation-cb32)以获取详细分析），这表现为两种主要形式：
- en: Concept drift[34](#fn34) occurs when the underlying relationship between features
    and targets evolves. For example, during the COVID-19 pandemic, purchasing behavior
    shifted dramatically, invalidating many previously accurate recommendation models.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念漂移[34](#fn34)发生在特征与目标之间的基本关系演变时。例如，在COVID-19大流行期间，购买行为发生了巨大变化，使许多之前准确的建议模型失效。
- en: Data drift refers to shifts in the input data distribution itself. In applications
    such as self-driving cars, this may result from seasonal changes in weather, lighting,
    or road conditions, all of which affect the model’s inputs.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据漂移指的是输入数据分布本身的变动。在自动驾驶汽车等应用中，这可能是由于天气、光照或道路条件等季节性变化引起的，所有这些都影响模型的输入。
- en: 'Beyond these recognized drift patterns lies a more insidious challenge: gradual
    long-term degradation that evades standard detection thresholds. Unlike sudden
    distribution shifts that trigger immediate alerts, some models experience performance
    erosion over months through imperceptible daily changes. For instance, e-commerce
    recommendation systems may lose 0.05% accuracy daily as user preferences evolve,
    accumulating to 15% degradation over a year without triggering monthly drift alerts.
    Seasonal patterns compound this complexity: a model trained in summer may perform
    well through autumn but fail catastrophically in winter conditions it never observed.
    Detecting such gradual degradation requires specialized monitoring approaches:
    establishing performance baselines across multiple time horizons (daily, weekly,
    quarterly), implementing sliding window comparisons that detect slow trends, and
    maintaining seasonal performance profiles that account for cyclical patterns.
    Teams often discover these degradations only through quarterly business reviews
    when cumulative impact becomes visible, emphasizing the need for multi-timescale
    monitoring strategies.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些已知的漂移模式之外，还存在一个更隐蔽的挑战：逐渐的长期退化，它逃避了标准的检测阈值。与触发即时警报的突然分布变化不同，一些模型可能通过难以察觉的每日变化在数月内经历性能侵蚀。例如，电子商务推荐系统可能每天损失0.05%的准确性，随着用户偏好的变化，一年内累积到15%的退化，而不会触发每月的漂移警报。季节性模式增加了这种复杂性：在夏季训练的模型可能在秋季表现良好，但在它从未观察过的冬季条件下可能失败。检测这种逐渐的退化需要专门的监控方法：在多个时间跨度（每日、每周、季度）上建立性能基线，实施滑动窗口比较以检测缓慢趋势，并维护考虑周期性模式的季节性性能配置文件。团队通常只在季度业务审查中通过累积影响变得明显时发现这些退化，这强调了需要多时间尺度监控策略的需求。
- en: 'In addition to model-level monitoring, infrastructure-level monitoring tracks
    indicators such as CPU and GPU utilization, memory and disk consumption, network
    latency, and service availability. These signals help ensure that the system remains
    performant and responsive under varying load conditions. Hardware-aware monitoring
    extends these basic metrics to capture resource efficiency patterns critical for
    operational success: GPU memory bandwidth utilization, power consumption relative
    to computational output, and thermal envelope adherence across sustained workloads.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型级别的监控外，基础设施级别的监控跟踪CPU和GPU利用率、内存和磁盘消耗、网络延迟以及服务可用性等指标。这些信号有助于确保系统在变化的负载条件下保持高性能和响应性。硬件感知监控将这些基本指标扩展到捕获对运营成功至关重要的资源效率模式：GPU内存带宽利用率、与计算输出相关的功耗，以及在整个工作负载中的热封装遵守情况。
- en: Building on the monitoring infrastructure outlined above, production systems
    must track hardware efficiency metrics that directly impact operational costs
    and model performance. GPU utilization monitoring should distinguish between compute-bound
    and memory-bound operations, as identical 90% utilization metrics can represent
    vastly different operational efficiency depending on bottleneck location. Memory
    bandwidth monitoring becomes essential for detecting suboptimal data loading patterns
    that manifest as high GPU utilization with low computational throughput. Power
    efficiency metrics, measured as operations per watt, enable teams to optimize
    mixed workload scheduling for both cost and environmental impact.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述监控基础设施的基础上，生产系统必须跟踪直接影响运营成本和模型性能的硬件效率指标。GPU利用率监控应区分计算密集型和内存密集型操作，因为相同的90%利用率指标可能代表完全不同的操作效率，这取决于瓶颈位置。内存带宽监控对于检测表现为高GPU利用率但计算吞吐量低的不理想数据加载模式至关重要。以每瓦操作数衡量的能效指标使团队能够优化混合工作负载的调度，以实现成本和环境影响的最优化。
- en: Thermal monitoring integrates into operational scheduling decisions, particularly
    for sustained high-utilization deployments where thermal throttling can degrade
    performance unpredictably. Modern MLOps monitoring dashboards incorporate thermal
    headroom metrics that guide workload distribution across available hardware, preventing
    thermal-induced performance degradation that can violate inference latency SLAs.
    Tools such as [Prometheus](https://prometheus.io/)[35](#fn35), [Grafana](https://grafana.com/),
    and [Elastic](https://www.elastic.co/) are widely used to collect, aggregate,
    and visualize these operational metrics. These tools often integrate into dashboards
    that offer real-time and historical views of system behavior.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 热量监控集成到操作调度决策中，尤其是在持续高利用率部署中，热限制可能会不可预测地降低性能。现代MLOps监控仪表板包含了热容量指标，这些指标指导工作负载在可用硬件之间的分配，防止由热量引起的性能退化，这可能会违反推理延迟SLA。工具如[Prometheus](https://prometheus.io/)[35](#fn35)、[Grafana](https://grafana.com/)和[Elastic](https://www.elastic.co/)被广泛用于收集、聚合和可视化这些操作指标。这些工具通常集成到提供实时和历史系统行为视图的仪表板中。
- en: Proactive alerting mechanisms are configured to notify teams when anomalies
    or threshold violations occur[36](#fn36). For example, a sustained drop in model
    accuracy may trigger an alert to investigate potential drift, prompting retraining
    with updated data. Similarly, infrastructure alerts can signal memory saturation
    or degraded network performance, allowing engineers to take corrective action
    before failures propagate.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 积极的警报机制被配置为在出现异常或阈值违规时通知团队[36](#fn36)。例如，模型准确性的持续下降可能会触发一个警报来调查潜在的漂移，提示使用更新数据重新训练。同样，基础设施警报可以指示内存饱和或网络性能下降，允许工程师在故障传播之前采取纠正措施。
- en: Ultimately, robust monitoring enables teams to detect problems before they escalate,
    maintain high service availability, and preserve the reliability and trustworthiness
    of machine learning systems. In the absence of such practices, models may silently
    degrade or systems may fail under load, undermining the effectiveness of the ML
    pipeline as a whole.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，强大的监控能力使团队能够在问题升级之前发现它们，保持高服务可用性，并保护机器学习系统的可靠性和可信度。如果没有这样的实践，模型可能会无声地退化，或者在负载下系统可能会失败，从而削弱了整个机器学习管道的有效性。
- en: 'The monitoring systems themselves require resilience planning to prevent operational
    blind spots. When primary monitoring infrastructure fails, such as Prometheus
    experiencing downtime or Grafana becoming unavailable, teams risk operating blind
    during critical periods. Production-grade MLOps implementations therefore maintain
    redundant monitoring pathways: secondary metric collectors that activate during
    primary system failures, local logging that persists when centralized systems
    fail, and heartbeat checks that detect monitoring system outages. Some organizations
    implement cross-monitoring where separate infrastructure monitors the monitoring
    systems themselves, ensuring that observation failures trigger immediate alerts
    through alternative channels such as PagerDuty or direct notifications. This defense-in-depth
    approach prevents the catastrophic scenario where both models and their monitoring
    systems fail simultaneously without detection.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 监控系统本身需要具备弹性规划以防止操作盲点。当主监控基础设施出现故障，例如Prometheus出现停机或Grafana不可用时，团队在关键时期可能会面临操作盲区。因此，生产级别的MLOps实现维护冗余的监控路径：在主系统故障时激活的二级指标收集器、在集中式系统故障时持续存在的本地日志，以及检测监控系统故障的心跳检查。一些组织实施跨监控，其中独立的基础设施监控器监控监控系统本身，确保观察失败通过替代渠道（如PagerDuty或直接通知）触发即时警报。这种多层次防御方法防止了同时发生模型及其监控系统故障而未被发现这种灾难性场景。
- en: The complexity of monitoring resilience increases significantly in distributed
    deployments. Multi-region ML systems introduce additional coordination challenges
    that extend beyond simple redundancy. In such environments, monitoring becomes
    a distributed coordination problem requiring consensus mechanisms for consistent
    system state assessment. Traditional centralized monitoring assumes a single point
    of truth, but distributed ML systems must reconcile potentially conflicting observations
    across data centers.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式部署中，监控弹性的复杂性显著增加。多区域机器学习系统引入了额外的协调挑战，这些挑战超出了简单冗余的范畴。在这样的环境中，监控成为一个分布式协调问题，需要共识机制来评估一致的系统状态。传统的集中式监控假设有一个单一的真实点，但分布式机器学习系统必须在数据中心之间解决可能存在冲突的观察结果。
- en: 'This distributed monitoring challenge manifests in three critical areas: consensus-based
    alerting to prevent false positives from network partitions, coordinated circuit
    breaker states[37](#fn37) to maintain system-wide consistency during failures,
    and distributed metric aggregation that preserves temporal ordering across regions
    with variable network latencies. The coordination overhead scales quadratically
    with the number of monitoring nodes, creating a tension between observability
    coverage and system complexity.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分布式监控挑战在三个关键领域体现出来：基于共识的警报以防止网络分区导致的误报、协调的断路器状态[37](#fn37)以在故障期间保持系统一致性，以及分布式指标聚合，在具有可变网络延迟的区域中保持时间顺序。协调开销与监控节点数量呈二次方增长，在可见性覆盖和系统复杂性之间产生紧张关系。
- en: To address these challenges, teams often implement hierarchical monitoring architectures
    where regional monitors report to global coordinators through eventual consistency
    models rather than requiring strong consistency for every metric. This approach
    balances monitoring granularity against the computational cost of maintaining
    distributed consensus, enabling scalable observability without overwhelming the
    system with coordination overhead.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，团队通常会实施分层监控架构，其中区域监控器通过最终一致性模型向全局协调员报告，而不是要求每个指标都具备强一致性。这种方法在监控粒度与维护分布式共识的计算成本之间取得平衡，使得可扩展的可见性不会因协调开销而压倒系统。
- en: Model Governance and Team Coordination
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型治理和团队协调
- en: Successful MLOps implementation requires robust governance frameworks and effective
    collaboration across diverse teams and stakeholders. This section examines the
    policies, practices, and organizational structures necessary for responsible and
    effective machine learning operations. We explore model governance principles
    that ensure transparency and accountability, cross-functional collaboration strategies
    that bridge technical and business teams, and stakeholder communication approaches
    that align expectations and facilitate decision-making.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的MLOps实施需要稳健的治理框架以及跨不同团队和利益相关者的有效协作。本节探讨了负责和有效的机器学习操作所需的政策、实践和组织结构。我们探讨了确保透明度和问责制的模型治理原则，连接技术和业务团队的跨职能协作策略，以及协调期望并促进决策的利益相关者沟通方法。
- en: Model Governance
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型治理
- en: As machine learning systems become increasingly embedded in decision-making
    processes, governance has emerged as a critical pillar of MLOps. Governance encompasses
    the policies, practices, and tools that ensure ML models operate transparently,
    fairly, and in compliance with ethical and regulatory standards. Without proper
    governance, deployed models may produce biased or opaque decisions, leading to
    significant legal, reputational, and societal risks. Ethical considerations and
    bias mitigation techniques provide the foundation for implementing these governance
    frameworks.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统在决策过程中的日益嵌入，治理已成为MLOps的关键支柱。治理包括确保ML模型透明、公平并符合伦理和监管标准的政策、实践和工具。没有适当的治理，部署的模型可能会产生有偏见的或模糊的决定，导致重大的法律、声誉和社会风险。伦理考量以及偏见缓解技术是实施这些治理框架的基础。
- en: Governance begins during the model development phase, where teams implement
    techniques to increase transparency and explainability. For example, methods such
    as [SHAP](https://github.com/slundberg/shap)[38](#fn38) and [LIME](https://github.com/marcotcr/lime)
    offer post hoc explanations of model predictions by identifying which input features
    were most influential in a particular decision. These interpretability techniques
    complement security measures that address how to protect both model integrity
    and data privacy in production environments. These techniques allow auditors,
    developers, and non-technical stakeholders to better understand how and why a
    model behaves the way it does.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 治理始于模型开发阶段，团队在此阶段实施技术以增加透明度和可解释性。例如，[SHAP](https://github.com/slundberg/shap)[38](#fn38)和[LIME](https://github.com/marcotcr/lime)等方法通过识别在特定决策中最具影响力的输入特征，对模型预测进行事后解释。这些可解释性技术补充了安全措施，这些措施解决如何在生产环境中保护模型完整性和数据隐私。这些技术允许审计员、开发人员和非技术利益相关者更好地理解模型如何以及为什么以这种方式表现。
- en: In addition to interpretability, fairness is a central concern in governance.
    Bias detection tools analyze model outputs across different demographic groups,
    including those defined by age, gender, or ethnicity, to identify disparities
    in performance. For instance, a model used for loan approval must not systematically
    disadvantage certain populations. MLOps teams employ pre-deployment audits on
    curated, representative datasets to evaluate fairness, robustness, and overall
    model behavior before a system is put into production.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可解释性之外，公平性在治理中也是一个核心关注点。偏见检测工具分析模型输出在不同人口群体之间的差异，包括由年龄、性别或种族定义的群体，以识别性能差异。例如，用于贷款批准的模型不得系统地损害某些群体。MLOps团队在系统投入生产之前，在精心挑选的代表性数据集上执行预部署审计，以评估公平性、鲁棒性和整体模型行为。
- en: Governance also extends into the post-deployment phase. As introduced in the
    previous section on monitoring, teams must track for concept drift, where the
    statistical relationships between features and labels evolve over time. Such drift
    can undermine the fairness or accuracy of a model, particularly if the shift disproportionately
    affects a specific subgroup. By analyzing logs and user feedback, teams can identify
    recurring failure modes, unexplained model outputs, or emerging disparities in
    treatment across user segments.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 治理还扩展到部署后的阶段。如前节所述的监控部分所介绍，团队必须跟踪概念漂移，即特征和标签之间的统计关系随时间演变。这种漂移可能会损害模型的公平性或准确性，尤其是如果这种转变不成比例地影响特定子群体。通过分析日志和用户反馈，团队可以识别重复的故障模式、未解释的模型输出或用户群体之间出现的处理差异。
- en: Supporting this lifecycle approach to governance are platforms and toolkits
    that integrate governance functions into the broader MLOps stack. For example,
    [Watson OpenScale](https://www.ibm.com/cloud/watson-openscale) provides built-in
    modules for explainability, bias detection, and monitoring. These tools allow
    governance policies to be encoded as part of automated pipelines, ensuring that
    checks are consistently applied throughout development, evaluation, and production.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 支持这种治理生命周期方法的是将治理功能集成到更广泛的MLOps堆栈中的平台和工具包。例如，[Watson OpenScale](https://www.ibm.com/cloud/watson-openscale)
    提供了内置的模块，用于可解释性、偏差检测和监控。这些工具允许将治理策略编码为自动化管道的一部分，确保在整个开发、评估和生产过程中始终如一地应用检查。
- en: 'Ultimately, governance focuses on three core objectives: transparency, fairness,
    and compliance. Transparency ensures that models are interpretable and auditable.
    Fairness promotes equitable treatment across user groups. Compliance ensures alignment
    with legal and organizational policies. Embedding governance practices throughout
    the MLOps lifecycle transforms machine learning from a technical artifact into
    a trustworthy system capable of serving societal and organizational goals.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，治理关注三个核心目标：透明度、公平性和合规性。透明度确保模型可解释和可审计。公平性促进用户群体间的公平对待。合规性确保与法律和组织政策的保持一致。在整个MLOps生命周期中嵌入治理实践，将机器学习从一项技术成果转变为一个值得信赖的系统，能够服务于社会和组织目标。
- en: Cross-Functional Collaboration
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨职能协作
- en: Machine learning systems are developed and maintained by multidisciplinary teams,
    including data scientists, ML engineers, software developers, infrastructure specialists,
    product managers, and compliance officers. As these roles span different domains
    of expertise, effective communication and collaboration are essential to ensure
    alignment, efficiency, and system reliability. MLOps fosters this cross-functional
    integration by introducing shared tools, processes, and artifacts that promote
    transparency and coordination across the machine learning lifecycle.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统是由多学科团队开发和维护的，包括数据科学家、ML工程师、软件开发人员、基础设施专家、产品经理和合规官员。由于这些角色跨越不同的专业知识领域，有效的沟通和协作对于确保一致性、效率和系统可靠性至关重要。MLOps通过引入共享工具、流程和工件来促进跨职能集成，这些工具和工件促进了机器学习生命周期中的透明度和协调。
- en: Collaboration begins with consistent tracking of experiments, model versions,
    and metadata. Tools such as [MLflow](https://mlflow.org/) provide a structured
    environment for logging experiments, capturing parameters, recording evaluation
    metrics, and managing trained models through a centralized registry. This registry
    serves as a shared reference point for all team members, enabling reproducibility
    and easing handoff between roles. Integration with version control systems such
    as [GitHub](https://github.com/) and [GitLab](https://about.gitlab.com/) further
    streamlines collaboration by linking code changes with model updates and pipeline
    triggers.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 协作始于对实验、模型版本和元数据的持续跟踪。例如，[MLflow](https://mlflow.org/) 提供了一个结构化的环境，用于记录实验、捕获参数、记录评估指标，并通过集中式注册表管理训练模型。这个注册表作为所有团队成员的共享参考点，促进了可重复性和角色间移交的简化。与版本控制系统如
    [GitHub](https://github.com/) 和 [GitLab](https://about.gitlab.com/) 的集成进一步通过将代码更改与模型更新和管道触发器链接起来，简化了协作。
- en: In addition to tracking infrastructure, teams benefit from platforms that support
    exploratory collaboration. [Weights & Biases](https://wandb.ai/) is one such platform
    that allows data scientists to visualize experiment metrics, compare training
    runs, and share insights with peers. Features such as live dashboards and experiment
    timelines facilitate discussion and decision-making around model improvements,
    hyperparameter tuning, or dataset refinements. These collaborative environments
    reduce friction in model development by making results interpretable and reproducible
    across the team.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 除了跟踪基础设施之外，团队还可以从支持探索性协作的平台中受益。[Weights & Biases](https://wandb.ai/) 就是这样的平台之一，它允许数据科学家可视化实验指标、比较训练运行并与同行分享见解。诸如实时仪表板和实验时间线等特性，促进了关于模型改进、超参数调整或数据集精炼的讨论和决策。这些协作环境通过使结果在整个团队中可解释和可重复，减少了模型开发中的摩擦。
- en: Beyond model tracking, collaboration also depends on shared understanding of
    data semantics and usage. Establishing common data contexts, by means of glossaries,
    data dictionaries, schema references, and lineage documentation, ensures that
    all stakeholders interpret features, labels, and statistics consistently. This
    is particularly important in large organizations, where data pipelines may evolve
    independently across teams or departments.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型跟踪之外，协作还依赖于对数据语义和使用的共同理解。通过术语表、数据字典、模式引用和血缘文档等方式建立共同的数据上下文，确保所有利益相关者对特征、标签和统计数据保持一致的解释。这在大型组织中尤为重要，因为数据管道可能在不同团队或部门之间独立演变。
- en: For example, a data scientist working on an anomaly detection model may use
    Weights & Biases to log experiment results and visualize performance trends. These
    insights are shared with the broader team to inform feature engineering decisions.
    Once the model reaches an acceptable performance threshold, it is registered in
    MLflow along with its metadata and training lineage. This allows an ML engineer
    to pick up the model for deployment without ambiguity about its provenance or
    configuration.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一位在异常检测模型上工作的数据科学家可能会使用Weights & Biases来记录实验结果并可视化性能趋势。这些见解与更广泛的团队共享，以指导特征工程决策。一旦模型达到可接受的性能阈值，它就会在MLflow中注册，包括其元数据和训练血缘。这允许ML工程师在没有关于其来源或配置的模糊性的情况下部署模型。
- en: By integrating collaborative tools, standardized documentation, and transparent
    experiment tracking, MLOps removes communication barriers that have traditionally
    slowed down ML workflows. It enables distributed teams to operate cohesively,
    accelerating iteration cycles and improving the reliability of deployed systems.
    However, effective MLOps extends beyond internal team coordination to encompass
    the broader communication challenges that arise when technical teams interface
    with business stakeholders.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 通过整合协作工具、标准化文档和透明的实验跟踪，MLOps消除了传统上减缓ML工作流程的沟通障碍。它使分布式团队能够协同工作，加速迭代周期，并提高部署系统的可靠性。然而，有效的MLOps不仅超越了内部团队协调，还包括了当技术团队与商业利益相关者接口时出现的更广泛的沟通挑战。
- en: Stakeholder Communication
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 利益相关者沟通
- en: Effective MLOps extends beyond technical implementation to encompass the strategic
    communication challenges that arise when translating complex machine learning
    realities into business language. Unlike traditional software systems with deterministic
    behavior, machine learning systems exhibit probabilistic performance, data dependencies,
    and degradation patterns that stakeholders often find counterintuitive. This communication
    gap can undermine project success even when technical execution remains sound.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的MLOps不仅超越了技术实现的范畴，还包括了在将复杂的机器学习现实转化为商业语言时出现的战略沟通挑战。与具有确定性行为的传统软件系统不同，机器学习系统表现出概率性性能、数据依赖性和退化模式，这些往往是利益相关者难以理解的。这种沟通差距可能会在技术执行仍然稳健的情况下，破坏项目的成功。
- en: The most common communication challenge emerges from oversimplified improvement
    requests. Product managers frequently propose directives such as “make the model
    more accurate” without understanding the underlying trade-offs that govern model
    performance. Effective MLOps communication reframes these requests by presenting
    concrete options with explicit costs. For instance, improving accuracy from 85%
    to 87% might require collecting four times more training data over three weeks
    while doubling inference latency from 50 ms to 120 ms. By articulating these specific
    constraints, MLOps practitioners transform vague requests into informed business
    decisions.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的沟通挑战源于过于简化的改进请求。产品经理经常提出诸如“使模型更准确”的指令，而不了解管理模型性能的潜在权衡。有效的MLOps沟通通过提出具有明确成本的明确选项来重新构建这些请求。例如，将准确性从85%提高到87%，可能需要在三周内收集四倍多的训练数据，同时将推理延迟从50
    ms加倍到120 ms。通过阐述这些具体限制，MLOps从业者将模糊的请求转化为明智的商业决策。
- en: Similarly, translating technical metrics into business impact requires consistent
    frameworks that connect model performance to operational outcomes. A 5% accuracy
    improvement appears modest in isolation, but contextualizing this change as “reducing
    false fraud alerts from 1,000 to 800 daily customer friction incidents” provides
    actionable business context. When infrastructure changes affect user experience,
    such as p99 latency degradation from 200 ms to 500 ms potentially causing 15%
    user abandonment based on conversion analytics, stakeholders can evaluate technical
    trade-offs against business priorities.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，将技术指标转化为业务影响需要一致的框架，将模型性能与运营成果联系起来。5%的准确率提升在孤立的情况下看似微不足道，但将这种变化置于“将每日客户摩擦事件从1,000减少到800”的背景下，提供了可操作的业务背景。当基础设施变更影响用户体验，例如p99延迟从200
    ms下降到500 ms，根据转化分析可能导致15%的用户流失，利益相关者可以评估技术权衡与业务优先级。
- en: Incident communication presents another critical operational challenge. When
    models degrade or require rollbacks, maintaining stakeholder trust depends on
    clear categorization of failure modes. Temporary performance fluctuations represent
    normal system variation, while data drift indicates planned maintenance requirements,
    and system failures demand immediate rollback procedures. Establishing regular
    performance reporting cadences preemptively addresses stakeholder concerns about
    model reliability and creates shared understanding of acceptable operational boundaries.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 事件沟通又提出了另一个关键运营挑战。当模型退化或需要回滚时，维持利益相关者信任取决于对故障模式的清晰分类。暂时的性能波动代表正常系统变化，数据漂移表明计划维护需求，系统故障则要求立即回滚程序。建立定期的性能报告周期可以预先解决利益相关者对模型可靠性的担忧，并形成对可接受运营边界的共同理解。
- en: 'Resource justification requires translating technical infrastructure requirements
    into business value propositions. Rather than requesting “8 A100 GPUs for model
    training,” effective communication frames investments as “infrastructure to reduce
    experiment cycle time from 2 weeks to 3 days, enabling 4x faster feature iteration.”
    Timeline estimation must account for realistic development proportions: data preparation
    typically consumes 60% of project duration, model development 25%, and deployment
    monitoring 15%. Communicating these proportions helps stakeholders understand
    why model training represents only a fraction of total delivery timelines.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 资源论证需要将技术基础设施需求转化为业务价值主张。与其请求“8个A100 GPU用于模型训练”，有效的沟通应将投资框架为“基础设施以将实验周期时间从2周缩短至3天，实现4倍的特征迭代速度。”时间线估计必须考虑实际开发比例：数据准备通常消耗项目持续时间的60%，模型开发25%，部署监控15%。沟通这些比例有助于利益相关者理解为什么模型训练只占总交付时间线的一小部分。
- en: 'Consider a fraud detection team implementing model improvements for a financial
    services platform. When stakeholders request enhanced accuracy, the team responds
    with a structured proposal: increasing detection rates from 92% to 94% requires
    integrating external data sources, extending training duration by two weeks, and
    accepting 30% higher infrastructure costs. However, this improvement would prevent
    an estimated $2 million in annual fraud losses while reducing false positive alerts
    that currently affect 50,000 customers monthly. This communication approach enables
    informed decision-making by connecting technical capabilities to business outcomes.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个为金融服务平台实施模型改进的欺诈检测团队。当利益相关者要求提高准确性时，团队以结构化的提案回应：将检测率从92%提高到94%需要整合外部数据源，将训练时间延长两周，并接受30%更高的基础设施成本。然而，这种改进将防止估计的200万美元的年度欺诈损失，同时减少目前每月影响5万名客户的误报警报。这种沟通方法通过将技术能力与业务成果联系起来，使利益相关者能够做出明智的决策。
- en: Through disciplined stakeholder communication, MLOps practitioners maintain
    organizational support for machine learning investments while establishing realistic
    expectations about system capabilities and operational requirements. This communication
    competency proves as essential as technical expertise for sustaining successful
    machine learning operations in production environments.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 通过纪律性的利益相关者沟通，MLOps实践者维持对机器学习投资的机构支持，同时建立对系统能力和运营要求的现实预期。这种沟通能力对于在生产环境中维持成功的机器学习运营与专业技术一样至关重要。
- en: With the infrastructure and production operations framework established, we
    now examine the organizational structure required to implement these practices
    effectively.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立了基础设施和生产操作框架之后，我们现在来探讨实施这些实践所需的组织结构。
- en: 'One common source of correction cascades is sequential model development: reusing
    or fine-tuning existing models to accelerate development for new tasks. While
    this strategy is often efficient, it can introduce hidden dependencies that are
    difficult to unwind later. Assumptions baked into earlier models become implicit
    constraints for future models, limiting flexibility and increasing the cost of
    downstream corrections.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的校正级联来源是序列模型开发：重用或微调现有模型以加速新任务的开发。虽然这种策略通常效率很高，但它可能会引入难以在以后解开隐藏的依赖关系。早期模型中固化的假设成为未来模型的隐含约束，限制了灵活性并增加了下游校正的成本。
- en: Consider a scenario where a team fine-tunes a customer churn prediction model
    for a new product. The original model may embed product-specific behaviors or
    feature encodings that are not valid in the new setting. As performance issues
    emerge, teams may attempt to patch the model, only to discover that the true problem
    lies several layers upstream, perhaps in the original feature selection or labeling
    criteria.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个团队为新产品微调客户流失预测模型的场景。原始模型可能包含特定于产品的行为或特征编码，在新环境中不适用。当性能问题出现时，团队可能会尝试修补模型，却发现真正的问题可能位于几个层次之上，可能在原始特征选择或标注标准中。
- en: To avoid or reduce the impact of correction cascades, teams must make careful
    tradeoffs between reuse and redesign. Several factors influence this decision.
    For small, static datasets, fine-tuning may be appropriate. For large or rapidly
    evolving datasets, retraining from scratch provides greater control and adaptability.
    Fine-tuning also requires fewer computational resources, making it attractive
    in constrained settings. However, modifying foundational components later becomes
    extremely costly due to these cascading effects.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免或减少校正级联的影响，团队必须在重用和重新设计之间做出谨慎的权衡。有几个因素会影响这个决定。对于小型、静态数据集，微调可能是合适的。对于大型或快速变化的数据集，从头开始重新训练提供了更大的控制和适应性。微调还需要较少的计算资源，使其在受限环境中具有吸引力。然而，由于这些级联效应，后来修改基础组件的成本变得极高。
- en: Therefore, careful consideration should be given to introducing fresh model
    architectures, even if resource-intensive, to avoid correction cascades down the
    line. This approach may help mitigate the amplifying effects of issues downstream
    and reduce technical debt. However, there are still scenarios where sequential
    model building makes sense, necessitating a thoughtful balance between efficiency,
    flexibility, and long-term maintainability in the ML development process.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，应仔细考虑引入新的模型架构，即使资源密集，以避免后续的校正级联。这种方法可能有助于减轻下游问题的放大效应并减少技术债务。然而，仍然存在一些场景，序列模型构建是有意义的，这需要在机器学习开发过程中在效率、灵活性和长期可维护性之间进行深思熟虑的平衡。
- en: To understand why correction cascades occur so persistently in ML systems despite
    best practices, it helps to examine the underlying mechanisms that drive this
    phenomenon. The correction cascade pattern emerges from hidden feedback loops
    that violate system modularity principles established in software engineering.
    When model A’s outputs influence model B’s training data, this creates implicit
    dependencies that undermine modular design. These dependencies are particularly
    insidious because they operate through data flows rather than explicit code interfaces,
    making them invisible to traditional dependency analysis tools.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解为什么尽管遵循最佳实践，校正级联在机器学习系统中仍然如此持久地发生，有助于检查驱动这一现象的潜在机制。校正级联模式源于违反软件工程中建立的系统模块化原则的隐藏反馈循环。当模型A的输出影响模型B的训练数据时，这会创建隐含的依赖关系，破坏模块化设计。这些依赖关系尤其狡猾，因为它们通过数据流而不是显式的代码接口操作，使它们对传统的依赖关系分析工具不可见。
- en: From a systems theory perspective, correction cascades represent instances of
    tight coupling between supposedly independent components. The cascade propagation
    follows power-law distributions, where small initial changes can trigger disproportionately
    large system-wide modifications. This phenomenon parallels the butterfly effect
    in complex systems, where minor perturbations amplify through nonlinear interactions.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统理论的角度来看，纠正级联代表了看似独立组件之间紧密耦合的实例。级联传播遵循幂律分布，其中小的初始变化可以触发不成比例的系统级重大修改。这一现象与复杂系统中的蝴蝶效应相似，其中微小的扰动通过非线性相互作用放大。
- en: Understanding these theoretical foundations helps engineers recognize that preventing
    correction cascades requires not just better tooling, but architectural decisions
    that preserve system modularity even in the presence of learning components. The
    challenge lies in designing ML systems that maintain loose coupling despite the
    inherently interconnected nature of data-driven workflows.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些理论基础有助于工程师认识到，防止纠正级联不仅需要更好的工具，还需要在存在学习组件的情况下，保持系统模块化的架构决策。挑战在于设计机器学习系统，即使数据驱动工作流程本质上相互关联，也能保持松散耦合。
- en: 'Table 13.3: **Technical Debt Patterns**: Machine learning systems accumulate
    distinct forms of technical debt that emerge from data dependencies, model interactions,
    and evolving operational contexts. This table summarizes the primary debt patterns,
    their causes, symptoms, and recommended mitigation strategies to guide practitioners
    in recognizing and addressing these challenges systematically.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 表13.3：**技术债务模式**：机器学习系统积累来自数据依赖、模型交互和不断变化的操作环境的不同形式的技术债务。本表总结了主要债务模式、其原因、症状和推荐的缓解策略，以指导从业者系统地识别和解决这些挑战。
- en: '| **Debt Pattern** | **Primary Cause** | **Key Symptoms** | **Mitigation Strategies**
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| **债务模式** | **主要原因** | **关键症状** | **缓解策略** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Boundary Erosion** | Tightly coupled components, unclear interfaces | Changes
    cascade unpredictably, CACHE principle violations | Enforce modular interfaces,
    design for encapsulation |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| **边界侵蚀** | 紧密耦合的组件，接口不明确 | 变化不可预测地级联，CACHE原则违反 | 强制模块化接口，设计封装 |'
- en: '| **Correction Cascades** | Sequential model dependencies, inherited assumptions
    | Upstream fixes break downstream systems, escalating revisions | Careful reuse
    vs. redesign tradeoffs, clear versioning |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| **纠正级联** | 顺序模型依赖，继承的假设 | 上游修复破坏下游系统，升级修订 | 小心重用与重新设计的权衡，清晰的版本控制 |'
- en: '| **Undeclared Consumers** | Informal output sharing, untracked dependencies
    | Silent breakage from model updates, hidden feedback loops | Strict access controls,
    formal interface contracts, usage monitoring |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| **未声明的消费者** | 非正式输出共享，未跟踪的依赖 | 模型更新导致的静默破坏，隐藏的反馈循环 | 严格的访问控制，正式的接口合同，使用监控
    |'
- en: '| **Data Dependency Debt** | Unstable or underutilized data inputs | Model
    failures from data changes, brittle feature pipelines | Data versioning, lineage
    tracking, leave-one-out analysis |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| **数据依赖债务** | 不稳定或未充分利用的数据输入 | 数据变化导致模型失败，脆弱的特征管道 | 数据版本控制，血缘跟踪，留一分析 |'
- en: '| **Feedback Loops** | Model outputs influence future training data | Self-reinforcing
    behavior, hidden performance degradation | Cohort-based monitoring, canary deployments,
    architectural isolation |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| **反馈循环** | 模型输出影响未来的训练数据 | 自我强化的行为，隐藏的性能下降 | 基于群体的监控，金丝雀部署，架构隔离 |'
- en: '| **Pipeline Debt** | Ad hoc workflows, lack of standard interfaces | Fragile
    execution, duplication, maintenance burden | Modular design, workflow orchestration
    tools, shared libraries |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| **管道债务** | 临时工作流程，缺乏标准接口 | 执行脆弱，重复，维护负担 | 模块化设计，工作流程编排工具，共享库 |'
- en: '| **Configuration Debt** | Fragmented settings, poor versioning | Irreproducible
    results, silent failures, tuning opacity | Version control, validation, structured
    formats, automation |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| **配置债务** | 碎片化设置，版本控制不佳 | 不可复现的结果，静默故障，调整不透明 | 版本控制，验证，结构化格式，自动化 |'
- en: '| **Early-Stage Debt** | Rapid prototyping shortcuts, tight code-logic coupling
    | Inflexibility as systems scale, difficult team collaboration | Flexible foundations,
    intentional debt tracking, planned refactoring |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| **早期债务** | 快速原型简略，紧密的代码逻辑耦合 | 系统扩展时的不灵活性，团队合作困难 | 灵活的基础设施，有意债务跟踪，计划重构 |'
- en: Managing Hidden Technical Debt
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理隐藏的技术债务
- en: While the examples discussed highlight the consequences of hidden technical
    debt in large-scale systems, they also offer valuable lessons for how such debt
    can be surfaced, controlled, and ultimately reduced. Managing hidden debt requires
    more than reactive fixes; it demands a deliberate and forward-looking approach
    to system design, team workflows, and tooling choices. The following sections
    of this chapter present systematic solutions to each debt pattern identified in
    [Table 13.3](ch019.xhtml#tbl-technical-debt-summary).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然所讨论的例子突出了大型系统中隐藏技术债务的后果，但它们也提供了关于如何揭示、控制和最终减少此类债务的宝贵经验。管理隐藏债务需要不仅仅是反应性的修复；它需要一种深思熟虑且前瞻性的系统设计、团队工作流程和工具选择方法。本章的以下部分提出了对[表13.3](ch019.xhtml#tbl-technical-debt-summary)中确定的每种债务模式的系统解决方案。
- en: A foundational principle is to treat data and configuration as integral parts
    of the system architecture, not as peripheral artifacts. As shown in [Figure 13.2](ch019.xhtml#fig-technical-debt),
    the bulk of an ML system lies outside the model code itself, in components like
    feature engineering, configuration, monitoring, and serving infrastructure. These
    surrounding layers often harbor the most persistent forms of debt, particularly
    when changes are made without systematic tracking or validation. The MLOps Infrastructure
    and Development section that follows addresses these challenges through feature
    stores, data versioning systems, and continuous pipeline frameworks specifically
    designed to manage data and configuration complexity.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基本的原则是将数据和配置视为系统架构的组成部分，而不是外围的工件。如图[图13.2](ch019.xhtml#fig-technical-debt)所示，机器学习系统的大部分内容都位于模型代码本身之外，在特征工程、配置、监控和服务基础设施等组件中。这些外围层通常隐藏着最持久的债务形式，尤其是在没有系统跟踪或验证的情况下进行更改时。接下来的MLOps基础设施和开发部分通过特征存储、数据版本控制系统和专门设计来管理数据和配置复杂性的持续管道框架来应对这些挑战。
- en: Versioning data transformations, labeling conventions, and training configurations
    enables teams to reproduce past results, localize regressions, and understand
    the impact of design choices over time. Tools that enable this, such as [DVC](https://dvc.org/)
    for data versioning, [Hydra](https://hydra.cc/) for configuration management,
    and [MLflow](https://mlflow.org/) for experiment tracking, help ensure that the
    system remains traceable as it evolves. Version control must extend beyond the
    model checkpoint to include the data and configuration context in which it was
    trained and evaluated.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据转换、标签约定和训练配置进行版本控制，使团队能够重现过去的结果，定位回归，并了解设计选择随时间推移的影响。能够实现这一点的工具，如用于数据版本控制的[DVC](https://dvc.org/)、用于配置管理的[Hydra](https://hydra.cc/)和用于实验跟踪的[MLflow](https://mlflow.org/)，有助于确保系统在演变过程中保持可追溯性。版本控制必须扩展到模型检查点之外，包括训练和评估时的数据和配置上下文。
- en: Another key strategy is encapsulation through modular interfaces. The cascading
    failures seen in tightly coupled systems highlight the importance of defining
    clear boundaries between components. Without well-specified APIs or contracts,
    changes in one module can ripple unpredictably through others. By contrast, systems
    designed around loosely coupled components, in which each module has well-defined
    responsibilities and limited external assumptions, are far more resilient to change.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键策略是通过模块化接口进行封装。紧密耦合系统中看到的级联故障突出了在组件之间定义清晰边界的重要性。如果没有明确指定的API或合同，一个模块的变化可能会在其他模块中不可预测地产生连锁反应。相比之下，围绕松耦合组件设计的系统，其中每个模块都有明确的责任和有限的对外假设，对变化具有更强的适应性。
- en: Encapsulation also supports dependency awareness, reducing the likelihood of
    undeclared consumers silently reusing outputs or internal representations. This
    is especially important in feedback-prone systems, where hidden dependencies can
    introduce behavioral drift over time. Exposing outputs through audited, documented
    interfaces makes it easier to reason about their use and to trace downstream effects
    when models evolve.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 封装还支持依赖关系意识，减少了未声明消费者静默重用输出或内部表示的可能性。这在反馈敏感的系统中尤为重要，因为隐藏的依赖关系可能会随着时间的推移引入行为漂移。通过经过审计和文档化的接口公开输出，使得推理其使用和追踪模型演变时的下游影响变得更加容易。
- en: Observability and monitoring further strengthen a system’s defenses against
    hidden debt. While static validation may catch errors during development, many
    forms of ML debt only manifest during deployment, especially in dynamic environments.
    Monitoring distribution shifts, feature usage patterns, and cohort-specific performance
    metrics helps detect degradation early, before it impacts users or propagates
    into future training data. The Production Operations section details these monitoring
    systems, governance frameworks, and deployment strategies, including canary deployments
    and progressive rollouts that are essential tools for limiting risk while allowing
    systems to evolve.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 可观测性和监控进一步增强了系统对隐藏债务的防御能力。虽然静态验证可能在开发过程中捕获错误，但许多形式的机器学习债务仅在部署期间显现，尤其是在动态环境中。监控分布变化、特征使用模式和特定群体的性能指标有助于在影响用户或传播到未来训练数据之前及早检测到退化。生产操作部分详细介绍了这些监控系统、治理框架和部署策略，包括金丝雀部署和渐进式发布，这些是限制风险同时允许系统演化的关键工具。
- en: Teams should also invest in institutional practices that periodically surface
    and address technical debt. Debt reviews, pipeline audits, and schema validation
    sprints serve as checkpoints where teams step back from rapid iteration and assess
    the system’s overall health. These reviews create space for refactoring, pruning
    unused features, consolidating redundant logic, and reasserting boundaries that
    may have eroded over time. The Roles and Responsibilities section examines how
    data engineers, ML engineers, and other specialists collaborate to implement these
    practices across the organization.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 团队还应投资于制度实践，定期浮现并解决技术债务。债务审查、管道审计和模式验证冲刺是团队从快速迭代中退出来评估系统整体健康的检查点。这些审查为重构、修剪未使用的功能、整合冗余逻辑和重新确立可能随时间侵蚀的边界创造了空间。角色和责任部分探讨了数据工程师、机器学习工程师和其他专家如何协作在组织内实施这些实践。
- en: Finally, the management of technical debt must be aligned with a broader cultural
    commitment to maintainability. This means prioritizing long-term system integrity
    over short-term velocity, especially once systems reach maturity or are integrated
    into critical workflows. It also means recognizing when debt is strategic, which
    is incurred deliberately to facilitate exploration, and ensuring it is tracked
    and revisited before it becomes entrenched.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，技术债务的管理必须与更广泛的文化承诺保持一致，即维护系统的可维护性。这意味着在系统成熟或集成到关键工作流程之后，优先考虑长期系统完整性而非短期速度。这也意味着要认识到何时债务是战略性的，即故意承担以促进探索，并确保在债务根深蒂固之前对其进行跟踪和重新审视。
- en: In all cases, managing hidden technical debt is not about eliminating complexity,
    but about designing systems that can accommodate it without becoming brittle.
    Through architectural discipline, thoughtful tooling, and a willingness to refactor,
    ML practitioners can build systems that remain flexible and reliable, even as
    they scale and evolve. The Operational System Design section provides frameworks
    for assessing organizational maturity and designing systems that systematically
    address these debt patterns, while the Case Studies demonstrate how these principles
    apply in real-world contexts.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有情况下，管理隐藏的技术债务不是关于消除复杂性，而是设计能够容纳它而不会变得脆弱的系统。通过架构纪律、深思熟虑的工具和愿意重构的意愿，机器学习从业者可以构建即使在扩展和演变过程中也能保持灵活和可靠的系统。操作系统设计部分提供了评估组织成熟度和设计系统以系统地解决这些债务模式的框架，而案例研究展示了这些原则如何在现实世界情境中应用。
- en: Summary
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Technical debt in machine learning systems is both pervasive and distinct from
    debt encountered in traditional software engineering. While the original metaphor
    of financial debt highlights the tradeoff between speed and long-term cost, the
    analogy falls short in capturing the full complexity of ML systems. In machine
    learning, debt often arises not only from code shortcuts but also from entangled
    data dependencies, poorly understood feedback loops, fragile pipelines, and configuration
    sprawl. Unlike financial debt, which can be explicitly quantified, ML technical
    debt is largely hidden, emerging only as systems scale, evolve, or fail.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统中的技术债务既普遍又与传统软件工程中遇到的债务不同。虽然原始的金融债务隐喻突出了速度和长期成本之间的权衡，但这种类比在捕捉机器学习系统的全部复杂性方面存在不足。在机器学习中，债务不仅可能来自代码捷径，还可能来自纠缠的数据依赖、难以理解的反馈循环、脆弱的管道和配置蔓延。与可以明确量化的金融债务不同，机器学习技术债务在很大程度上是隐藏的，只有在系统扩展、演变或失败时才会显现。
- en: This chapter has outlined several forms of ML-specific technical debt, each
    rooted in different aspects of the system lifecycle. Boundary erosion undermines
    modularity and makes systems difficult to reason about. Correction cascades illustrate
    how local fixes can ripple through a tightly coupled workflow. Undeclared consumers
    and feedback loops introduce invisible dependencies that challenge traceability
    and reproducibility. Data and configuration debt reflect the fragility of inputs
    and parameters that are poorly managed, while pipeline and change adaptation debt
    expose the risks of inflexible architectures. Early-stage debt reminds us that
    even in the exploratory phase, decisions should be made with an eye toward future
    extensibility.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了几种机器学习特定的技术债务形式，每种形式都根植于系统生命周期的不同方面。边界侵蚀破坏了模块化，使系统难以推理。纠正级联说明了局部修复如何在一个紧密耦合的工作流程中产生连锁反应。未声明的消费者和反馈循环引入了看不见的依赖关系，挑战了可追溯性和可重复性。数据和配置债务反映了管理不善的输入和参数的脆弱性，而管道和变更适应性债务揭示了僵化架构的风险。早期阶段的债务提醒我们，即使在探索阶段，决策也应着眼于未来的可扩展性。
- en: 'The common thread across all these debt types is the need for systematic engineering
    approaches and system-level thinking. ML systems are not just code; they are evolving
    ecosystems of data, models, infrastructure, and teams that can be effectively
    managed through disciplined engineering practices. Managing technical debt requires
    architectural discipline, robust tooling, and a culture that values maintainability
    alongside innovation. It also requires engineering judgment: recognizing when
    debt is strategic and ensuring it is tracked and addressed before it becomes entrenched.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些债务类型的共同点是需要对系统性的工程方法和系统级思维。机器学习系统不仅仅是代码；它们是数据、模型、基础设施和团队的不断发展的生态系统，可以通过严格的工程实践有效地进行管理。管理技术债务需要架构纪律、强大的工具和重视可维护性与创新的文化。它还需要工程判断：识别何时债务是战略性的，并确保在它根深蒂固之前进行跟踪和处理。
- en: As machine learning becomes increasingly central to production systems, engineering
    teams can successfully address these challenges through the systematic practices,
    infrastructure components, and organizational structures detailed in this chapter.
    Understanding and addressing hidden technical debt not only improves reliability
    and scalability, but also empowers teams to iterate faster, collaborate more effectively,
    and sustain the long-term evolution of their systems through proven engineering
    methodologies.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习在生产系统中的日益中心化，工程团队能够通过本章详细阐述的系统化实践、基础设施组件和组织结构来成功地解决这些挑战。理解和解决隐藏的技术债务不仅提高了可靠性和可扩展性，而且使团队能够更快地迭代、更有效地协作，并通过经过验证的工程方法维持其系统的长期演变。
- en: However, implementing these systematic practices and infrastructure components
    requires more than just technical solutions. It demands coordinated contributions
    from professionals with diverse expertise working together effectively.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实施这些系统化实践和基础设施组件需要不仅仅是技术解决方案。它需要来自具有不同专业知识的专业人士的协调贡献，并且能够有效地合作。
- en: Roles and Responsibilities
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 角色和职责
- en: The operational frameworks, infrastructure components, and governance practices
    examined in the previous sections depend fundamentally on coordinated contributions
    from professionals with diverse technical and organizational expertise. Unlike
    traditional software engineering workflows, machine learning introduces additional
    complexity through its reliance on dynamic data, iterative experimentation, and
    probabilistic model behavior. As a result, no single role can independently manage
    the end-to-end machine learning lifecycle. [Figure 13.8](ch019.xhtml#fig-roles-and-responsibilities)
    provides a high level overview of how these roles relate to each other.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中考察的操作框架、基础设施组件和治理实践，从根本上依赖于来自具有不同技术和组织专业知识的专业人士的协调贡献。与传统的软件工程工作流程不同，机器学习通过其依赖动态数据、迭代实验和概率模型行为引入了额外的复杂性。因此，没有单一角色可以独立管理整个机器学习生命周期。[图13.8](ch019.xhtml#fig-roles-and-responsibilities)提供了这些角色之间相互关系的高级概述。
- en: '![](../media/file214.svg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file214.svg)'
- en: 'Figure 13.8: **AI Development Strategies**: Model-centric and data-centric
    approaches represent complementary strategies for improving AI system performance;
    model-centric AI prioritizes architectural innovation, while data-centric AI focuses
    on enhancing data quality and representativeness to drive model improvements.
    Effective AI systems often require coordinated investment in both model and data
    improvements to achieve optimal results.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8：**人工智能开发策略**：以模型为中心和数据为中心的方法是提高人工智能系统性能的互补策略；以模型为中心的人工智能优先考虑架构创新，而以数据为中心的人工智能则专注于提升数据质量和代表性以推动模型改进。有效的AI系统通常需要协调投资于模型和数据改进，以实现最佳结果。
- en: 'Following the MLOps principles established in [Section 13.2.2](ch019.xhtml#sec-ml-operations-mlops-c12b),
    these specialized roles align around a shared objective: delivering reliable,
    scalable, and maintainable machine learning systems in production environments.
    From designing robust data pipelines to deploying and monitoring models in live
    systems, effective collaboration depends on the disciplinary coordination that
    MLOps facilitates across data engineering, statistical modeling, software development,
    infrastructure management, and project coordination.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循在[第13.2.2节](ch019.xhtml#sec-ml-operations-mlops-c12b)中建立的MLOps原则，这些专业角色围绕一个共同目标进行对齐：在生产环境中交付可靠、可扩展和可维护的机器学习系统。从设计健壮的数据管道到在实时系统中部署和监控模型，有效的协作依赖于MLOps促进的数据工程、统计建模、软件开发、基础设施管理和项目协调之间的学科协调。
- en: Roles
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 角色列表
- en: '[Table 13.4](ch019.xhtml#tbl-mlops-roles) introduces the key roles that participate
    in MLOps and outlines their primary responsibilities. Understanding these roles
    not only clarifies the scope of skills required to support production ML systems
    but also helps frame the collaborative workflows and handoffs that drive the operational
    success of machine learning at scale.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[表13.4](ch019.xhtml#tbl-mlops-roles)介绍了参与MLOps的关键角色，并概述了他们的主要职责。理解这些角色不仅明确了支持生产级机器学习系统所需的技能范围，还有助于构建推动大规模机器学习运营成功的协作工作流程和交接。'
- en: 'Table 13.4: **MLOps Roles & Responsibilities**: Effective machine learning
    system operation requires a collaborative team with clearly defined roles (data
    engineers, data scientists, and others), each contributing specialized expertise
    throughout the entire lifecycle from data preparation to model deployment and
    monitoring. Understanding these roles clarifies skill requirements and promotes
    efficient workflows for scaling machine learning solutions.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 表13.4：**MLOps角色与职责**：有效的机器学习系统运营需要一个具有明确定义角色的协作团队（数据工程师、数据科学家等），每个成员在整个生命周期中从数据准备到模型部署和监控贡献专业专长。理解这些角色明确了技能要求，并促进了扩展机器学习解决方案的高效工作流程。
- en: '| **Role** | **Primary Focus** | **Core Responsibilities Summary** | **MLOps
    Lifecycle Alignment** |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| **角色** | **主要关注点** | **核心职责摘要** | **MLOps生命周期对齐** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Data Engineer** | Data preparation and infrastructure | Build and maintain
    pipelines; ensure quality, structure, and lineage of data | Data ingestion, transformation
    |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| **数据工程师** | 数据准备和基础设施 | 构建和维护管道；确保数据的质量、结构和血缘 | 数据摄取、转换 |'
- en: '| **Data Scientist** | Model development and experimentation | Formulate tasks;
    build and evaluate models; iterate using feedback and error analysis | Modeling
    and evaluation |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| **数据科学家** | 模型开发和实验 | 制定任务；构建和评估模型；使用反馈和错误分析进行迭代 | 建模和评估 |'
- en: '| **ML Engineer** | Production integration and scalability | Operationalize
    models; implement serving logic; manage performance and retraining | Deployment
    and inference |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| **机器学习工程师** | 生产集成和可扩展性 | 实施模型；实现服务逻辑；管理性能和再训练 | 部署和推理 |'
- en: '| **DevOps Engineer** | Infrastructure orchestration and automation | Manage
    compute infrastructure; implement CI/CD; monitor systems and workflows | Training,
    deployment, monitoring |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| **DevOps工程师** | 基础设施编排和自动化 | 管理计算基础设施；实施CI/CD；监控系统和工作流程 | 训练、部署、监控 |'
- en: '| **Project Manager** | Coordination and delivery oversight | Align goals;
    manage schedules and milestones; enable cross-team execution | Planning and integration
    |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| **项目经理** | 协调和交付监督 | 对齐目标；管理进度和里程碑；促进跨团队执行 | 规划和集成 |'
- en: '| **Responsible AI** | Ethics, fairness, and governance | Monitor bias and
    fairness; enforce transparency and | Evaluation and governance |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| **负责任的人工智能** | 道德、公平和治理 | 监测偏差和公平性；强制透明度 | 评估和治理 |'
- en: '| **Lead** |  | compliance standards |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| **铅** |  | 合规标准 |  |'
- en: '| **Security & Privacy** | System protection and data integrity | Secure data
    and models; implement privacy controls; | Data handling and compliance |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| **安全与隐私** | 系统保护和数据完整性 | 保护数据和模型；实施隐私控制； | 数据处理和合规 |'
- en: '| **Engineer** |  | ensure system resilience |  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| **工程师** |  | 确保系统弹性 |  |'
- en: Data Engineers
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据工程师
- en: Data engineers are responsible for constructing and maintaining the data infrastructure
    that underpins machine learning systems. Their primary focus is to ensure that
    data is reliably collected, processed, and made accessible in formats suitable
    for analysis, feature extraction, model training, and inference. In the context
    of MLOps, data engineers play a foundational role by building the **data infrastructure**
    components discussed earlier, including feature stores, data versioning systems,
    and validation frameworks, that enable scalable and reproducible data pipelines
    supporting the end-to-end machine learning lifecycle.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师负责构建和维护支撑机器学习系统的数据基础设施。他们的主要焦点是确保数据能够可靠地收集、处理，并以适合分析、特征提取、模型训练和推理的格式提供。在MLOps的背景下，数据工程师通过构建之前讨论过的**数据基础设施**组件，包括特征存储库、数据版本控制系统和验证框架，发挥基础性作用，这些组件能够支持可扩展和可重复的数据管道，从而支持端到端的机器学习生命周期。
- en: 'A core responsibility of data engineers is data ingestion: extracting data
    from diverse operational sources such as transactional databases, web applications,
    log streams, and sensors. This data is typically transferred to centralized storage
    systems, such as cloud-based object stores (e.g., Amazon S3, Google Cloud Storage),
    which provide scalable and durable repositories for both raw and processed datasets.
    These ingestion workflows are orchestrated using scheduling and workflow tools
    such as Apache Airflow, Prefect, or dbt ([Kampakis 2020](ch058.xhtml#ref-garg2020practical)).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师的核心责任之一是数据摄取：从各种运营源中提取数据，如事务数据库、Web应用、日志流和传感器。这些数据通常被传输到集中式存储系统，如基于云的对象存储（例如，Amazon
    S3、Google Cloud Storage），这些系统为原始和加工后的数据集提供可扩展和持久的存储库。这些摄取工作流程是通过调度和工作流程工具（如Apache
    Airflow、Prefect或dbt ([Kampakis 2020](ch058.xhtml#ref-garg2020practical)))进行编排的。
- en: Once ingested, the data must be transformed into structured, analysis-ready
    formats. This transformation process includes handling missing or malformed values,
    resolving inconsistencies, performing joins across heterogeneous sources, and
    computing derived attributes required for downstream tasks. Data engineers implement
    these transformations through modular pipelines that are version-controlled and
    designed for fault tolerance and reusability. Structured outputs are often loaded
    into cloud-based data warehouses such as Snowflake, Redshift, or BigQuery, or
    stored in feature stores for use in machine learning applications.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦摄取，数据必须被转换成结构化、分析准备好的格式。这个转换过程包括处理缺失或格式不正确的值，解决不一致性，在异构源之间执行连接，以及计算下游任务所需的派生属性。数据工程师通过模块化管道实现这些转换，这些管道是版本控制的，旨在具有容错性和可重用性。结构化输出通常被加载到基于云的数据仓库中，如Snowflake、Redshift或BigQuery，或者存储在特征存储库中，用于机器学习应用。
- en: In addition to managing data pipelines, data engineers are responsible for provisioning
    and optimizing the infrastructure that supports data-intensive workflows. This
    includes configuring distributed storage systems, managing compute clusters, and
    maintaining metadata catalogs that document data schemas, lineage, and access
    controls. To ensure reproducibility and governance, data engineers implement dataset
    versioning, maintain historical snapshots, and enforce data retention and auditing
    policies.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 除了管理数据管道外，数据工程师还负责提供和优化支持数据密集型工作流程的基础设施。这包括配置分布式存储系统、管理计算集群，以及维护记录数据模式、血缘和访问控制的元数据目录。为了确保可重复性和治理，数据工程师实施数据集版本控制，维护历史快照，并执行数据保留和审计政策。
- en: For example, in a manufacturing application, data engineers may construct an
    Airflow pipeline that ingests time-series sensor data from programmable logic
    controllers (PLCs)[39](#fn39) on the factory floor.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在制造应用中，数据工程师可能会构建一个Airflow管道，从工厂地面的可编程逻辑控制器（PLCs）[39](#fn39)中摄取时间序列传感器数据。
- en: The raw data is cleaned, joined with product metadata, and aggregated into statistical
    features such as rolling averages and thresholds. The processed features are stored
    in a Snowflake data warehouse, where they are consumed by downstream modeling
    and inference workflows.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据经过清理，与产品元数据合并，并汇总成统计特征，如滚动平均值和阈值。处理后的特征存储在Snowflake数据仓库中，在那里它们被下游建模和推理工作流程所消费。
- en: Through their design and maintenance of robust data infrastructure, data engineers
    enable the consistent and efficient delivery of high-quality data. Their contributions
    ensure that machine learning systems are built on reliable inputs, supporting
    reproducibility, scalability, and operational stability across the MLOps pipeline.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设计和维护稳健的数据基础设施，数据工程师确保了高质量数据的持续和高效交付。他们的贡献确保机器学习系统建立在可靠的输入之上，支持MLOps管道的可重复性、可扩展性和运营稳定性。
- en: To illustrate this responsibility in practice, [Listing 13.1](ch019.xhtml#lst-data-engineer)
    shows a simplified example of a daily Extract-Transform-Load (ETL) pipeline implemented
    using Apache Airflow. This workflow automates the ingestion and transformation
    of raw sensor data, preparing it for downstream machine learning tasks.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一责任在实践中的体现，[列表13.1](ch019.xhtml#lst-data-engineer)展示了使用Apache Airflow实现的简化每日提取-转换-加载（ETL）管道的示例。此工作流程自动化了原始传感器数据的摄取和转换，为下游机器学习任务做准备。
- en: 'Listing 13.1: **Daily ETL Pipeline**: Automates the ingestion and transformation
    of raw sensor data for downstream ML tasks, highlighting the role of apache airflow
    in orchestrating workflow tasks.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.1：**每日ETL管道**：自动化原始传感器数据的摄取和转换，以支持下游机器学习任务，突出了Apache Airflow在编排工作流程任务中的作用。
- en: '[PRE0]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Data Scientists
  id: totrans-357
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据科学家
- en: Data scientists are responsible for designing, developing, and evaluating machine
    learning models. Their role centers on transforming business or operational problems
    into formal learning tasks, selecting appropriate algorithms, and optimizing model
    performance through statistical and computational techniques. Within the MLOps
    lifecycle, data scientists operate at the intersection of exploratory analysis
    and model development, contributing directly to the creation of predictive or
    decision-making capabilities.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家负责设计、开发和评估机器学习模型。他们的角色集中在将业务或运营问题转化为正式的学习任务，选择合适的算法，并通过统计和计算技术优化模型性能。在MLOps生命周期中，数据科学家在探索性分析和模型开发之间运作，直接参与到预测或决策能力的创建中。
- en: The process typically begins by collaborating with stakeholders to define the
    problem space and establish success criteria. This includes formulating the task
    in machine learning terms, including classification, regression, or forecasting,
    and identifying suitable evaluation metrics to quantify model performance. These
    metrics, such as accuracy, precision, recall, area under the curve (AUC), or F1
    score, provide objective measures for comparing model alternatives and guiding
    iterative improvements ([Rainio, Teuho, and Klén 2024](ch058.xhtml#ref-rainio2024evaluation)).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这个过程从与利益相关者合作，定义问题空间和建立成功标准开始。这包括用机器学习术语制定任务，包括分类、回归或预测，并确定合适的评估指标来量化模型性能。这些指标，如准确率、精确率、召回率、曲线下面积（AUC）或F1分数，为比较模型替代方案和指导迭代改进提供了客观的衡量标准（[Rainio,
    Teuho, 和 Klén 2024](ch058.xhtml#ref-rainio2024evaluation)）。
- en: Data scientists conduct exploratory data analysis (EDA) to assess data quality,
    identify patterns, and uncover relationships that inform feature selection and
    engineering. This stage may involve statistical summaries, visualizations, and
    hypothesis testing to evaluate the data’s suitability for modeling. Based on these
    findings, relevant features are constructed or selected in collaboration with
    data engineers to ensure consistency across development and deployment environments.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家通过探索性数据分析（EDA）来评估数据质量、识别模式并揭示关系，这些关系有助于特征选择和工程。这一阶段可能涉及统计摘要、可视化以及假设检验来评估数据建模的适用性。基于这些发现，与数据工程师合作构建或选择相关特征，以确保开发与部署环境的一致性。
- en: Model development involves selecting appropriate learning algorithms and constructing
    architectures suited to the task and data characteristics. Data scientists employ
    machine learning libraries such as TensorFlow, PyTorch, or scikit-learn to implement
    and train models. Hyperparameter tuning, regularization strategies, and cross-validation
    are used to optimize performance on validation datasets while mitigating overfitting.
    Throughout this process, tools for experiment tracking, including MLflow and Weights
    & Biases, are often used to log configuration settings, evaluation results, and
    model artifacts.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 模型开发涉及选择合适的机器学习算法并构建适合任务和数据特性的架构。数据科学家使用TensorFlow、PyTorch或scikit-learn等机器学习库来实现和训练模型。超参数调整、正则化策略和交叉验证用于在验证数据集上优化性能，同时减轻过拟合。在整个过程中，通常会使用实验跟踪工具，如MLflow和Weights
    & Biases，来记录配置设置、评估结果和模型工件。
- en: Once a candidate model demonstrates acceptable performance, it undergoes validation
    through testing on holdout datasets. In addition to aggregate performance metrics,
    data scientists perform error analysis to identify failure modes, outliers, or
    biases that may impact model reliability or fairness. These insights often motivate
    iterations on data processing, feature engineering, or model refinement.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦候选模型表现出可接受的性能，它将通过在保留数据集上进行测试进行验证。除了聚合性能指标外，数据科学家还会进行错误分析，以识别可能影响模型可靠性和公平性的失败模式、异常值或偏差。这些见解通常促使对数据处理、特征工程或模型精炼进行迭代。
- en: Data scientists also participate in post-deployment monitoring and retraining
    workflows. They assist in analyzing data drift, interpreting shifts in model performance,
    and incorporating new data to maintain predictive accuracy over time. In collaboration
    with ML engineers, they define retraining strategies and evaluate the impact of
    updated models on operational metrics.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家还参与部署后的监控和重新训练工作流程。他们协助分析数据漂移，解释模型性能的变化，并整合新数据以保持预测精度。与机器学习工程师合作，他们定义重新训练策略并评估更新模型对运营指标的影响。
- en: For example, in a retail forecasting scenario, a data scientist may develop
    a sequence model using TensorFlow to predict product demand based on historical
    sales, product attributes, and seasonal indicators. The model is evaluated using
    root mean squared error (RMSE) on withheld data, refined through hyperparameter
    tuning, and handed off to ML engineers for deployment. Following deployment, the
    data scientist continues to monitor model accuracy and guides retraining using
    new transactional data.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在零售预测场景中，数据科学家可能会使用TensorFlow开发一个序列模型，根据历史销售、产品属性和季节性指标来预测产品需求。该模型使用保留数据的均方根误差（RMSE）进行评估，通过超参数调整进行精炼，然后转交给机器学习工程师进行部署。部署后，数据科学家继续监控模型精度，并使用新的交易数据指导重新训练。
- en: Through experimentation and model development, data scientists contribute the
    core analytical functionality of machine learning systems. Their work transforms
    raw data into predictive insights and supports the continuous improvement of deployed
    models through evaluation and refinement.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实验和模型开发，数据科学家为机器学习系统的核心分析功能做出了贡献。他们的工作将原始数据转化为预测洞察，并通过评估和精炼支持部署模型的持续改进。
- en: To illustrate these responsibilities in a practical context, [Listing 13.2](ch019.xhtml#lst-data-scientist)
    presents a minimal example of a sequence model built using TensorFlow. This model
    is designed to forecast product demand based on historical sales patterns and
    other input features.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在一个实际情境中说明这些职责，[列表13.2](ch019.xhtml#lst-data-scientist)展示了使用TensorFlow构建的序列模型的简化示例。此模型旨在根据历史销售模式和其它输入特征预测产品需求。
- en: 'Listing 13.2: **Sequence Model**: A sequence model architecture can forecast
    future product demand based on historical sales patterns and other features, highlighting
    the importance of time-series data in predictive modeling through This example.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.2：**序列模型**：序列模型架构可以根据历史销售模式和其它特征预测未来的产品需求，通过此示例突出了时间序列数据在预测建模中的重要性。
- en: '[PRE1]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ML Engineers
  id: totrans-369
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 机器学习工程师
- en: Machine learning engineers are responsible for translating experimental models
    into reliable, scalable systems that can be integrated into real-world applications.
    Positioned at the intersection of data science and software engineering, ML engineers
    ensure that models developed in research environments can be deployed, monitored,
    and maintained within production infrastructure. Their work bridges the gap between
    prototyping and operationalization, enabling machine learning to deliver sustained
    value in practice.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工程师负责将实验模型转化为可靠、可扩展的系统，这些系统可以集成到现实世界的应用中。位于数据科学和软件工程的交汇点，ML工程师确保在研究环境中开发的模型可以在生产基础设施中部署、监控和维护。他们的工作弥合了原型设计和实际应用之间的差距，使机器学习能够在实践中持续产生价值。
- en: A core responsibility of ML engineers is to take trained models and encapsulate
    them within modular, maintainable components. This often involves refactoring
    code for robustness, implementing model interfaces, and building application programming
    interfaces (APIs) that expose model predictions to downstream systems. Frameworks
    such as Flask and FastAPI are commonly used to construct lightweight, RESTful
    services for model inference. To support portability and environment consistency,
    models and their dependencies are typically containerized using Docker and managed
    within orchestration systems like Kubernetes.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ML工程师的核心责任之一是将训练好的模型封装在模块化、可维护的组件中。这通常涉及重构代码以提高鲁棒性、实现模型接口和构建应用程序编程接口（API），以便将模型预测暴露给下游系统。Flask和FastAPI等框架通常用于构建轻量级的RESTful服务以进行模型推理。为了支持可移植性和环境一致性，模型及其依赖项通常使用Docker容器化，并在Kubernetes等编排系统中进行管理。
- en: ML engineers also oversee the integration of models into **continuous pipelines**
    and implement the **deployment and serving** infrastructure discussed in the production
    operations section. These pipelines automate the retraining, testing, and deployment
    of models, ensuring that updated models are validated against performance benchmarks
    before being promoted to production. Practices such as the **canary testing**
    strategies outlined earlier, A/B testing, and staged rollouts allow for gradual
    transitions and reduce the risk of regressions. In the event of model degradation,
    rollback procedures are used to restore previously validated versions.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ML工程师还负责将模型集成到**持续管道**中，并实施生产操作部分中讨论的**部署和托管**基础设施。这些管道自动化模型的重新训练、测试和部署，确保更新后的模型在投入生产前经过性能基准的验证。如前所述的**金丝雀测试**策略、A/B测试和分阶段推出允许逐步过渡并降低回归风险。在模型退化的情况下，使用回滚程序恢复先前验证的版本。
- en: Operational efficiency is another key area of focus. ML engineers apply a range
    of optimization techniques, including model quantization, pruning, and batch serving,
    to meet latency, throughput, and cost constraints. In systems that support multiple
    models, they may implement mechanisms for dynamic model selection or concurrent
    serving. These optimizations are closely coupled with infrastructure provisioning,
    which often includes the configuration of GPUs or other specialized accelerators.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 运营效率是另一个关键关注领域。ML工程师应用一系列优化技术，包括模型量化、剪枝和批量服务，以满足延迟、吞吐量和成本限制。在支持多个模型的系统中，他们可能实施动态模型选择或并发服务的机制。这些优化与基础设施配置紧密耦合，通常包括GPU或其他专用加速器的配置。
- en: Post-deployment, ML engineers play a critical role in monitoring model behavior.
    They configure telemetry systems[40](#fn40) to track latency, failure rates, and
    resource usage, and they instrument prediction pipelines with logging and alerting
    mechanisms.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 部署后，ML工程师在监控模型行为方面发挥着关键作用。他们配置遥测系统[40](#fn40)以跟踪延迟、失败率和资源使用情况，并使用日志记录和警报机制对预测管道进行仪表化。
- en: In collaboration with data scientists and DevOps engineers, they respond to
    changes in system behavior, trigger retraining workflows, and ensure that models
    continue to meet service-level objectives.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据科学家和DevOps工程师合作，他们响应系统行为的改变，触发重新训练工作流程，并确保模型持续满足服务级别目标。
- en: For example, consider a financial services application where a data science
    team has developed a fraud detection model using TensorFlow. An ML engineer packages
    the model for deployment using TensorFlow Serving, configures a REST API for integration
    with the transaction pipeline, and sets up a CI/CD pipeline in Jenkins to automate
    updates. They implement logging and monitoring using Prometheus and Grafana, and
    configure rollback logic to revert to the prior model version if performance deteriorates.
    This production infrastructure enables the model to operate continuously and reliably
    under real-world workloads.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个金融服务应用程序，其中数据科学团队使用TensorFlow开发了一个欺诈检测模型。ML工程师使用TensorFlow Serving打包模型以进行部署，配置REST
    API以与交易管道集成，并在Jenkins中设置CI/CD管道来自动化更新。他们使用Prometheus和Grafana实现日志记录和监控，并配置回滚逻辑，以便在性能下降时回滚到先前的模型版本。这个生产基础设施使得模型能够在真实世界的工作负载下持续且可靠地运行。
- en: 'Through their focus on software robustness, deployment automation, and operational
    monitoring, ML engineers play a critical role in transitioning machine learning
    models from experimental artifacts into trusted components of production systems.
    These responsibilities vary significantly by organization size: at startups, ML
    engineers often span the entire stack from data pipeline development to model
    deployment, while at large technology companies like Meta or Google, they typically
    specialize in specific areas such as serving infrastructure or feature engineering.
    Mid-sized companies often have ML engineers owning end-to-end responsibility for
    specific model domains (e.g., recommendation systems), balancing breadth and specialization.
    To illustrate these responsibilities in a practical context, [Listing 13.3](ch019.xhtml#lst-ml-engineer)
    presents a minimal example of a REST API built with FastAPI for serving a trained
    TensorFlow model. This service exposes model predictions for use in downstream
    applications.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 通过关注软件健壮性、部署自动化和运营监控，ML工程师在将机器学习模型从实验性工件转变为生产系统可信赖组件的过程中发挥着关键作用。这些职责因组织规模而异：在初创公司，ML工程师通常从数据管道开发到模型部署覆盖整个技术栈，而在像Meta或Google这样的大型科技公司，他们通常在特定领域如服务基础设施或特征工程方面进行专业化。中等规模的公司通常让ML工程师负责特定模型领域（例如，推荐系统）的端到端责任，平衡广度和专业化。为了在实践环境中说明这些职责，[列表13.3](ch019.xhtml#lst-ml-engineer)展示了使用FastAPI构建的用于服务训练好的TensorFlow模型的REST
    API的最小示例。此服务公开模型预测，供下游应用程序使用。
- en: 'Listing 13.3: **FastAPI Service**: Wraps a TensorFlow model to provide real-time
    demand predictions, illustrating how ML engineers integrate models into production
    systems.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.3：**FastAPI服务**：将TensorFlow模型包装起来以提供实时需求预测，展示了ML工程师如何将模型集成到生产系统中。
- en: '[PRE2]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: DevOps Engineers
  id: totrans-380
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DevOps工程师
- en: DevOps engineers are responsible for provisioning, managing, and automating
    the infrastructure that supports the development, deployment, and monitoring of
    machine learning systems. Originating from the broader discipline of software
    engineering, the role of the DevOps engineer in MLOps extends traditional responsibilities
    to accommodate the specific demands of data- and model-driven workflows. Their
    expertise in cloud computing, automation pipelines, and infrastructure as code
    (IaC) enables scalable and reliable machine learning operations.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps工程师负责提供、管理和自动化支持机器学习系统开发、部署和监控的基础设施。这一角色起源于更广泛的软件工程学科，在MLOps中，DevOps工程师的职责扩展到适应数据驱动和模型驱动的特定工作流程的需求。他们在云计算、自动化管道和基础设施即代码（IaC）方面的专业知识使得机器学习操作可扩展且可靠。
- en: A central task for DevOps engineers is the configuration and orchestration of
    compute infrastructure used throughout the ML lifecycle. This includes provisioning
    virtual machines, storage systems, and accelerators such as GPUs and TPUs using
    IaC tools like Terraform, AWS CloudFormation, or Ansible. Infrastructure is typically
    containerized using Docker and managed through orchestration platforms such as
    Kubernetes, which allow teams to deploy, scale, and monitor workloads across distributed
    environments.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps工程师的中心任务是配置和编排整个机器学习生命周期中使用的计算基础设施。这包括使用Terraform、AWS CloudFormation或Ansible等IaC工具提供虚拟机、存储系统和GPU、TPU等加速器。基础设施通常使用Docker容器化，并通过Kubernetes等编排平台进行管理，这些平台允许团队在分布式环境中部署、扩展和监控工作负载。
- en: DevOps engineers design and implement CI/CD pipelines tailored to machine learning
    workflows. These pipelines automate the retraining, testing, and deployment of
    models in response to code changes or data updates. Tools such as Jenkins, GitHub
    Actions, or GitLab CI are used to trigger model workflows, while platforms like
    MLflow and Kubeflow facilitate experiment tracking, model registration, and artifact
    versioning. By codifying deployment logic, these pipelines reduce manual effort,
    increase reproducibility, and enable faster iteration cycles.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps工程师设计和实施针对机器学习工作流程的CI/CD管道。这些管道自动化模型的重训练、测试和部署，以响应代码更改或数据更新。Jenkins、GitHub
    Actions或GitLab CI等工具用于触发模型工作流程，而MLflow和Kubeflow等平台则促进实验跟踪、模型注册和工件版本控制。通过将部署逻辑编码化，这些管道减少了人工工作量，增加了可重复性，并使迭代周期更快。
- en: Monitoring is another critical area of responsibility. DevOps engineers configure
    telemetry systems to collect metrics related to both model and infrastructure
    performance. Tools such as Prometheus, Grafana, and the ELK stack[41](#fn41) (Elasticsearch,
    Logstash, Kibana) are widely used to build dashboards, set thresholds, and generate
    alerts.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 监控是另一个关键责任领域。DevOps工程师配置遥测系统以收集与模型和基础设施性能相关的指标。Prometheus、Grafana和ELK堆栈（Elasticsearch、Logstash、Kibana）等工具被广泛用于构建仪表板、设置阈值和生成警报。
- en: These systems allow teams to detect anomalies in latency, throughput, resource
    utilization, or prediction behavior and respond proactively to emerging issues.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统允许团队检测延迟、吞吐量、资源利用或预测行为的异常，并主动应对出现的问题。
- en: To ensure compliance and operational discipline, DevOps engineers also implement
    governance mechanisms that enforce consistency and traceability. This includes
    versioning of infrastructure configurations, automated validation of deployment
    artifacts, and auditing of model updates. In collaboration with ML engineers and
    data scientists, they enable reproducible and auditable model deployments aligned
    with organizational and regulatory requirements.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保合规性和操作纪律，DevOps工程师还实施了治理机制，以强制执行一致性和可追溯性。这包括基础设施配置的版本控制、部署工件的自动化验证以及模型更新的审计。与ML工程师和数据科学家合作，他们使模型部署可重复且可审计，符合组织和管理要求。
- en: For instance, in a financial services application, a DevOps engineer may configure
    a Kubernetes cluster on AWS to support both model training and online inference.
    Using Terraform, the infrastructure is defined as code and versioned alongside
    the application repository. Jenkins is used to automate the deployment of models
    registered in MLflow, while Prometheus and Grafana provide real-time monitoring
    of API latency, resource usage, and container health.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在金融服务应用中，DevOps工程师可能在AWS上配置Kubernetes集群以支持模型训练和在线推理。使用Terraform，基础设施被定义为代码，并与应用程序仓库一起进行版本控制。Jenkins用于自动化部署在MLflow中注册的模型，而Prometheus和Grafana提供API延迟、资源使用和容器健康状况的实时监控。
- en: By abstracting and automating the infrastructure that underlies ML workflows,
    DevOps engineers enable scalable experimentation, robust deployment, and continuous
    monitoring. Their role ensures that machine learning systems can operate reliably
    under production constraints, with minimal manual intervention and maximal operational
    efficiency. To illustrate these responsibilities in a practical context, [Listing 13.4](ch019.xhtml#lst-devops-engineer)
    presents an example of using Terraform to provision a GPU-enabled virtual machine
    on Google Cloud Platform for model training and inference workloads.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 通过抽象和自动化支撑ML工作流程的基础设施，DevOps工程师实现了可扩展的实验、稳健的部署和持续的监控。他们的角色确保机器学习系统可以在生产约束下可靠运行，最小化人工干预并最大化操作效率。为了说明这些责任在实际环境中的应用，[列表13.4](ch019.xhtml#lst-devops-engineer)展示了使用Terraform在Google
    Cloud Platform上为模型训练和推理工作负载配置启用GPU的虚拟机的示例。
- en: 'Listing 13.4: **GPU-Enabled Infrastructure**: This configuration ensures efficient
    model training and inference by leveraging a specific machine type and GPU accelerator
    on Google cloud platform.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.4：**启用GPU的基础设施**：此配置通过利用Google云平台上的特定机器类型和GPU加速器，确保模型训练和推理的高效性。
- en: '[PRE3]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Project Managers
  id: totrans-391
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 项目经理
- en: Project managers play a critical role in coordinating the activities, resources,
    and timelines involved in delivering machine learning systems. While they do not
    typically develop models or write code, project managers are essential to aligning
    interdisciplinary teams, tracking progress against objectives, and ensuring that
    MLOps initiatives are completed on schedule and within scope. Their work enables
    effective collaboration among data scientists, engineers, product stakeholders,
    and infrastructure teams, translating business goals into actionable technical
    plans.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 项目经理在协调交付机器学习系统所涉及的活动、资源和时间表方面发挥着关键作用。虽然他们通常不开发模型或编写代码，但项目经理对于协调跨学科团队、跟踪进度以实现目标以及确保MLOps项目按时按范围完成至关重要。他们的工作使得数据科学家、工程师、产品利益相关者和基础设施团队能够有效协作，将业务目标转化为可执行的技术计划。
- en: At the outset of a project, project managers work with organizational stakeholders
    to define goals, success metrics, and constraints. This includes clarifying the
    business objectives of the machine learning system, identifying key deliverables,
    estimating timelines, and setting performance benchmarks. These definitions serve
    as the foundation for resource allocation, task planning, and risk assessment
    throughout the lifecycle of the project.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目开始时，项目经理与组织利益相关者合作，定义目标、成功指标和限制条件。这包括阐明机器学习系统的业务目标、确定关键可交付成果、估计时间表和设定性能基准。这些定义构成了项目生命周期中资源分配、任务规划和风险评估的基础。
- en: Once the project is initiated, project managers are responsible for developing
    and maintaining a detailed execution plan. This plan outlines major phases of
    work, such as data collection, model development, infrastructure provisioning,
    deployment, and monitoring. Dependencies between tasks are identified and managed
    to ensure smooth handoffs between roles, while milestones and checkpoints are
    used to assess progress and adjust schedules as necessary.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦项目启动，项目经理负责制定和维护详细的执行计划。该计划概述了工作的主要阶段，例如数据收集、模型开发、基础设施配置、部署和监控。识别并管理任务之间的依赖关系，以确保角色之间的顺利交接，同时使用里程碑和检查点来评估进度并必要时调整计划。
- en: Throughout execution, project managers facilitate coordination across teams.
    This includes organizing meetings, tracking deliverables, resolving blockers,
    and escalating issues when necessary. Documentation, progress reports, and status
    updates are maintained to provide visibility across the organization and ensure
    that all stakeholders are informed of project developments. Communication is a
    central function of the role, serving to reduce misalignment and clarify expectations
    between technical contributors and business decision-makers.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行过程中，项目经理促进团队间的协调。这包括组织会议、跟踪可交付成果、解决障碍并在必要时升级问题。维护文档、进度报告和状态更新，以在整个组织中提供可见性并确保所有利益相关者了解项目进展。沟通是该角色的核心功能，有助于减少技术贡献者和商业决策者之间的不一致，并明确期望。
- en: In addition to managing timelines and coordination, project managers oversee
    the budgeting and resourcing aspects of MLOps initiatives. This may involve evaluating
    cloud infrastructure costs, negotiating access to compute resources, and ensuring
    that appropriate personnel are assigned to each phase of the project. By maintaining
    visibility into both technical and organizational considerations, project managers
    help align technical execution with strategic priorities.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 除了管理时间表和协调外，项目经理还监督MLOps项目的预算和资源配置方面。这可能涉及评估云基础设施成本、协商计算资源访问以及确保适当的人员分配到项目的每个阶段。通过保持对技术和组织考量的可见性，项目经理帮助将技术执行与战略优先事项对齐。
- en: For example, consider a company seeking to reduce customer churn using a predictive
    model. The project manager coordinates with data engineers to define data requirements,
    with data scientists to prototype and evaluate models, with ML engineers to package
    and deploy the final model, and with DevOps engineers to provision the necessary
    infrastructure and monitoring tools. The project manager tracks progress through
    phases such as data pipeline readiness, baseline model evaluation, deployment
    to staging, and post-deployment monitoring, adjusting the project plan as needed
    to respond to emerging challenges.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一家公司试图使用预测模型来减少客户流失。项目经理与数据工程师协调以定义数据需求，与数据科学家原型化和评估模型，与ML工程师打包和部署最终模型，并与DevOps工程师提供必要的基础设施和监控工具。项目经理通过数据管道准备就绪、基线模型评估、部署到预生产和部署后监控等阶段跟踪进度，根据需要调整项目计划以应对新兴的挑战。
- en: By orchestrating collaboration across diverse roles and managing the complexity
    inherent in machine learning initiatives, project managers enable MLOps teams
    to deliver systems that are both technically robust and aligned with organizational
    goals. Their contributions ensure that the operationalization of machine learning
    is not only feasible, but repeatable, accountable, and efficient. To illustrate
    these responsibilities in a practical context, [Listing 13.5](ch019.xhtml#lst-project-manager)
    presents a simplified example of a project milestone tracking structure using
    JSON. This format is commonly used to integrate with tools like JIRA or project
    dashboards to monitor progress across machine learning initiatives.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 通过协调不同角色之间的协作并管理机器学习项目固有的复杂性，项目经理使MLOps团队能够交付既在技术上稳健又与组织目标一致的系统。他们的贡献确保机器学习的实施不仅可行，而且可重复、可问责和高效。为了在实践环境中说明这些职责，[列表13.5](ch019.xhtml#lst-project-manager)展示了使用JSON的简化项目里程碑跟踪结构示例。这种格式通常用于与JIRA或项目仪表板等工具集成，以监控机器学习项目的进度。
- en: 'Listing 13.5: **Milestone Tracking Structure**: This JSON format organizes
    project phases like data readiness and model deployment, highlighting progress
    and risk management for machine learning initiatives.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.5：**里程碑跟踪结构**：此JSON格式组织项目阶段，如数据准备和模型部署，突出机器学习项目的进度和风险管理。
- en: '[PRE4]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Responsible AI Lead
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 负责人工智能领导
- en: The Responsible AI Lead is tasked with ensuring that machine learning systems
    operate in ways that are transparent, fair, accountable, and compliant with ethical
    and regulatory standards. As machine learning is increasingly embedded in socially
    impactful domains such as healthcare, finance, and education, the need for systematic
    governance has grown. This role reflects a growing recognition that technical
    performance alone is insufficient; ML systems must also align with broader societal
    values.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 负责人工智能领导负责确保机器学习系统以透明、公平、可问责和符合伦理及监管标准的方式运行。随着机器学习越来越多地嵌入到社会影响领域，如医疗保健、金融和教育，系统治理的需求也在增长。这一角色反映了日益增长的共识，即仅技术性能是不够的；ML系统还必须与更广泛的社会价值观相一致。
- en: At the model development stage, Responsible AI Leads support practices that
    enhance interpretability and transparency. They work with data scientists and
    ML engineers to assess which features contribute most to model predictions, evaluate
    whether certain groups are disproportionately affected, and document model behavior
    through structured reporting mechanisms. Post hoc explanation methods, such as
    attribution techniques, are often reviewed in collaboration with this role to
    support downstream accountability.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型开发阶段，负责人工智能领导支持增强可解释性和透明度的实践。他们与数据科学家和ML工程师合作，评估哪些特征对模型预测贡献最大，评估是否某些群体受到不成比例的影响，并通过结构化报告机制记录模型行为。通常与这一角色合作审查事后解释方法，如归因技术，以支持下游问责。
- en: Another key responsibility is fairness assessment. This involves defining fairness
    criteria in collaboration with stakeholders, auditing model outputs for performance
    disparities across demographic groups, and guiding interventions, including reweighting,
    re-labeling, or constrained optimization, to mitigate potential harms. These assessments
    are often incorporated into model validation pipelines to ensure that they are
    systematically enforced before deployment.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键责任是公平性评估。这涉及与利益相关者合作定义公平性标准，审计模型输出在人口统计学群体间的性能差异，并指导干预措施，包括重新加权、重新标记或约束优化，以减轻潜在的危害。这些评估通常纳入模型验证流程中，以确保在部署前系统性地执行。
- en: In post-deployment settings, Responsible AI Leads help monitor systems for drift,
    bias amplification, and unanticipated behavior. They may also oversee the creation
    of documentation artifacts such as model cards or datasheets for datasets, which
    serve as tools for transparency and reproducibility. In regulated sectors, this
    role collaborates with legal and compliance teams to meet audit requirements and
    ensure that deployed models remain aligned with external mandates.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署后环境中，负责任的人工智能负责人帮助监控系统漂移、偏差放大和未预见的异常行为。他们还可能监督创建文档工件，如模型卡片或数据集数据表，这些工件作为透明度和可重复性的工具。在受监管的行业中，这一角色与法律和合规团队合作，以满足审计要求并确保部署的模型与外部要求保持一致。
- en: For example, in a hiring recommendation system, a Responsible AI Lead may oversee
    an audit that compares model outcomes across gender and ethnicity, guiding the
    team to adjust the training pipeline to reduce disparities while preserving predictive
    accuracy. They also ensure that decision rationales are documented and reviewable
    by both technical and non-technical stakeholders.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个招聘推荐系统中，负责任的人工智能负责人可能监督一个审计，比较模型结果在性别和种族方面的差异，指导团队调整训练流程以减少差异同时保持预测准确性。他们还确保决策理由被记录并可供技术和非技术利益相关者审查。
- en: The integration of ethical review and governance into the ML development process
    enables the Responsible AI Lead to support systems that are not only technically
    robust, but also socially responsible and institutionally accountable. To illustrate
    these responsibilities in a practical context, [Listing 13.6](ch019.xhtml#lst-responsible-ai)
    presents an example of using the Aequitas library to audit a model for group-based
    disparities. This example evaluates statistical parity across demographic groups
    to assess potential fairness concerns prior to deployment.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 将道德审查和治理纳入机器学习开发过程，使负责任的人工智能负责人能够支持既技术稳健又负责任、机构问责的系统。为了在实践环境中说明这些责任，[列表 13.6](ch019.xhtml#lst-responsible-ai)
    展示了使用 Aequitas 库审计基于群体差异的模型的一个示例。此示例评估人口统计学群体之间的统计平衡，以评估部署前的潜在公平性问题。
- en: 'Listing 13.6: **Fairness Audit**: Evaluates model outcomes to identify gender
    disparities using aequitas, ensuring socially responsible AI systems.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.6：**公平性审计**：使用 aequitas 评估模型结果，以识别性别差异，确保负责任的人工智能系统。
- en: '[PRE5]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Security and Privacy Engineer
  id: totrans-410
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安全与隐私工程师
- en: The Security and Privacy Engineer is responsible for safeguarding machine learning
    systems against adversarial threats and privacy risks. As ML systems increasingly
    rely on sensitive data and are deployed in high-stakes environments, security
    and privacy become essential dimensions of system reliability. This role brings
    expertise in both traditional security engineering and ML-specific threat models,
    ensuring that systems are resilient to attack and compliant with data protection
    requirements.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 安全与隐私工程师负责保护机器学习系统免受对抗性威胁和隐私风险。随着机器学习系统越来越多地依赖于敏感数据，并在高风险环境中部署，安全和隐私成为系统可靠性的基本维度。这一角色结合了传统安全工程和机器学习特定威胁模型的专业知识，确保系统对攻击具有弹性，并符合数据保护要求。
- en: At the data level, Security and Privacy Engineers help enforce access control,
    encryption, and secure handling of training and inference data. They collaborate
    with data engineers to apply privacy-preserving techniques, such as data anonymization,
    secure aggregation, or differential privacy, particularly when sensitive personal
    or proprietary data is used. These mechanisms are designed to reduce the risk
    of data leakage while retaining the utility needed for model training.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据层面，安全与隐私工程师帮助实施访问控制、加密以及训练和推理数据的妥善处理。他们与数据工程师合作，应用隐私保护技术，如数据匿名化、安全聚合或差分隐私，特别是在使用敏感的个人或专有数据时。这些机制旨在降低数据泄露的风险，同时保留模型训练所需的效用。
- en: In the modeling phase, this role advises on techniques that improve robustness
    against adversarial manipulation. This may include detecting poisoning attacks
    during training, mitigating model inversion or membership inference risks, and
    evaluating the susceptibility of models to adversarial examples. They also assist
    in designing model architectures and training strategies that balance performance
    with safety constraints.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模阶段，此角色建议采用提高对抗性操作鲁棒性的技术。这可能包括在训练期间检测投毒攻击、减轻模型反演或成员推理风险，以及评估模型对对抗性示例的敏感性。他们还协助设计平衡性能与安全约束的模型架构和训练策略。
- en: During deployment, Security and Privacy Engineers implement controls to protect
    the model itself, including endpoint hardening, API rate limiting, and access
    logging. In settings where models are exposed externally, including public-facing
    APIs, they may also deploy monitoring systems that detect anomalous access patterns
    or query-based attacks intended to extract model parameters or training data.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署期间，安全与隐私工程师实施控制措施以保护模型本身，包括端点加固、API 速率限制和访问日志记录。在模型对外公开的环境中，包括面向公众的 API，他们还可能部署监控系统，以检测异常访问模式或基于查询的攻击，这些攻击旨在提取模型参数或训练数据。
- en: For instance, in a medical diagnosis system trained on patient data, a Security
    and Privacy Engineer might implement differential privacy during model training
    and enforce strict access controls on the model’s inference interface. They would
    also validate that model explanations do not inadvertently expose sensitive information,
    and monitor post-deployment activity for potential misuse.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个基于患者数据的医疗诊断系统中，安全与隐私工程师可能在模型训练期间实施差分隐私，并在模型的推理接口上强制执行严格的数据访问控制。他们还会验证模型解释不会无意中泄露敏感信息，并监控部署后的活动以防止潜在的滥用。
- en: Through proactive design and continuous oversight, Security and Privacy Engineers
    ensure that ML systems uphold confidentiality, integrity, and availability. Their
    work is especially critical in domains where trust, compliance, and risk mitigation
    are central to system deployment and long-term operation. To illustrate these
    responsibilities in a practical context, [Listing 13.7](ch019.xhtml#lst-security-privacy)
    presents an example of training a model using differential privacy techniques
    with TensorFlow Privacy. This approach helps protect sensitive information in
    the training data while preserving model utility.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 通过主动设计和持续监督，安全与隐私工程师确保机器学习系统维护机密性、完整性和可用性。他们的工作在信任、合规性和风险缓解是系统部署和长期运营核心的领域尤为重要。为了在实践环境中说明这些责任，[列表
    13.7](ch019.xhtml#lst-security-privacy) 展示了使用 TensorFlow Privacy 的差分隐私技术训练模型的一个示例。这种方法有助于保护训练数据中的敏感信息，同时保留模型效用。
- en: 'Listing 13.7: **Differentially Private Training**: To train a machine learning
    model using differential privacy techniques in TensorFlow Privacy, ensuring sensitive
    data protection while maintaining predictive performance via This code snippet.
    *Source: TensorFlow Privacy Documentation*'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.7：**差分隐私训练**：使用 TensorFlow Privacy 中的差分隐私技术训练机器学习模型，通过此代码片段确保敏感数据保护的同时保持预测性能。*来源：TensorFlow
    Privacy 文档*
- en: '[PRE6]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Intersections and Handoffs
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交汇点和交接
- en: While each role in MLOps carries distinct responsibilities, the successful deployment
    and operation of machine learning systems depends on seamless collaboration across
    functional boundaries. Machine learning workflows are inherently interdependent,
    with critical handoff points connecting data acquisition, model development, system
    integration, and operational monitoring. Understanding these intersections is
    essential for designing processes that are both efficient and resilient.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MLOps中的每个角色都有独特的责任，但机器学习系统的成功部署和运行依赖于跨职能边界的无缝协作。机器学习工作流程本质上是相互依赖的，关键交接点连接着数据采集、模型开发、系统集成和运营监控。理解这些交叉点对于设计既高效又具有弹性的流程至关重要。
- en: One of the earliest and most critical intersections occurs between data engineers
    and data scientists. Data engineers construct and maintain the pipelines that
    ingest and transform raw data, while data scientists depend on these pipelines
    to access clean, structured, and well-documented datasets for analysis and modeling.
    Misalignment at this stage, including undocumented schema changes or inconsistent
    feature definitions, can lead to downstream errors that compromise model quality
    or reproducibility.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 最早且最关键的交叉点之一发生在数据工程师和数据科学家之间。数据工程师构建和维护摄取和转换原始数据的管道，而数据科学家依赖这些管道来访问用于分析和建模的干净、结构化和良好记录的数据集。在此阶段的不一致，包括未记录的模式更改或不一致的特征定义，可能导致下游错误，损害模型质量或可重复性。
- en: Once a model is developed, the handoff to ML engineers requires a careful transition
    from research artifacts to production-ready components. ML engineers must understand
    the assumptions and requirements of the model to implement appropriate interfaces,
    optimize runtime performance, and integrate it into the broader application ecosystem.
    This step often requires iteration, especially when models developed in experimental
    environments must be adapted to meet latency, throughput, or resource constraints
    in production.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型开发完成，将模型交付给机器学习工程师需要从研究工件到生产就绪组件的谨慎过渡。机器学习工程师必须理解模型的假设和要求，以实现适当的接口，优化运行时性能，并将其集成到更广泛的应用生态系统中。这一步骤通常需要迭代，尤其是在实验环境中开发的模型必须适应生产中的延迟、吞吐量或资源限制时。
- en: As models move toward deployment, DevOps engineers play the role in provisioning
    infrastructure, managing CI/CD pipelines, and instrumenting monitoring systems.
    Their collaboration with ML engineers ensures that model deployments are automated,
    repeatable, and observable. They also coordinate with data scientists to define
    alerts and thresholds that guide performance monitoring and retraining decisions.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型向部署迈进时，DevOps工程师在提供基础设施、管理CI/CD管道和配置监控系统方面发挥作用。他们与机器学习工程师的合作确保模型部署是自动化的、可重复的且可观察的。他们还与数据科学家协调，定义警报和阈值，以指导性能监控和再训练决策。
- en: Project managers provide the organizational glue across these technical domains.
    They ensure that handoffs are anticipated, roles are clearly defined, and dependencies
    are actively managed. In particular, project managers help maintain continuity
    by documenting assumptions, tracking milestone readiness, and facilitating communication
    between teams. This coordination reduces friction and enables iterative development
    cycles that are both agile and accountable.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 项目经理在这些技术领域之间提供组织上的粘合剂。他们确保交接是预期的，角色定义明确，依赖关系得到积极管理。特别是，项目经理通过记录假设、跟踪里程碑准备情况和促进团队间的沟通来帮助保持连续性。这种协调减少了摩擦，并使敏捷且负责任的迭代开发周期成为可能。
- en: For example, in a real-time recommendation system, data engineers maintain the
    data ingestion pipeline and feature store, data scientists iterate on model architectures
    using historical clickstream data, ML engineers deploy models as containerized
    microservices[42](#fn42), and DevOps engineers monitor inference latency and availability.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个实时推荐系统中，数据工程师维护数据摄取管道和特征存储，数据科学家使用历史点击流数据迭代模型架构，机器学习工程师将模型作为容器化的微服务[42](#fn42)部署，DevOps工程师监控推理延迟和可用性。
- en: Each role contributes to a different layer of the stack, but the overall functionality
    depends on reliable transitions between each phase of the lifecycle. These role
    interactions illustrate that MLOps is not simply a collection of discrete tasks,
    but a continuous, collaborative process ([Figure 13.9](ch019.xhtml#fig-mlops-handoffs)).
    Designing for clear handoffs, shared tools, and well-defined interfaces is essential
    for ensuring that machine learning systems can evolve, scale, and perform reliably
    over time.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 每个角色都对堆栈的不同层做出贡献，但整体功能取决于生命周期每个阶段之间可靠的过渡。这些角色之间的交互表明，MLOps不仅仅是离散任务的集合，而是一个持续、协作的过程（[图13.9](ch019.xhtml#fig-mlops-handoffs)）。为了确保机器学习系统可以随着时间的推移而演变、扩展并可靠地运行，设计清晰的手续、共享工具和明确定义接口是至关重要的。
- en: '![](../media/file215.svg)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file215.svg)'
- en: 'Figure 13.9: **MLOps Role Handoffs Workflow**: Machine learning workflows require
    systematic handoffs between specialized roles, with each role producing specific
    artifacts that become inputs for downstream activities. Critical handoff points
    (H1-H3) represent coordination moments where clear interfaces, shared understanding,
    and documented requirements become essential for system reliability. Feedback
    loops enable continuous improvement based on production performance data.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9：**MLOps角色交接工作流程**：机器学习工作流程需要在专业角色之间进行系统性的交接，每个角色产生特定的工件，这些工件成为下游活动的输入。关键交接点（H1-H3）代表协调时刻，此时清晰的接口、共享理解和文档化的需求对于系统可靠性至关重要。反馈循环使基于生产性能数据的持续改进成为可能。
- en: Evolving Roles and Specializations
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 角色和专业化的发展
- en: As machine learning systems mature and organizations adopt MLOps practices at
    scale, the structure and specialization of roles often evolve. In early-stage
    environments, individual contributors may take on multiple responsibilities, such
    as a data scientist who also builds data pipelines or manages model deployment.
    However, as systems grow in complexity and teams expand, responsibilities tend
    to become more differentiated, giving rise to new roles and more structured organizational
    patterns.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统的成熟和组织在更大规模上采用MLOps实践，角色的结构和专业化往往也会演变。在早期环境阶段，个人贡献者可能承担多个职责，例如数据科学家同时构建数据管道或管理模型部署。然而，随着系统复杂性的增长和团队的扩大，职责往往变得更加分化，从而产生了新的角色和更结构化的组织模式。
- en: One emerging trend is the formation of dedicated ML platform teams, which focus
    on building shared infrastructure and tooling to support experimentation, deployment,
    and monitoring across multiple projects. These teams often abstract common workflows,
    including data versioning, model training orchestration, and CI/CD integration,
    into reusable components or internal platforms. This approach reduces duplication
    of effort and accelerates development by enabling application teams to focus on
    domain-specific problems rather than underlying systems engineering.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 一种新兴趋势是成立专门的机器学习平台团队，这些团队专注于构建共享的基础设施和工具，以支持多个项目之间的实验、部署和监控。这些团队通常将常见的工作流程，包括数据版本控制、模型训练编排和持续集成/持续部署（CI/CD）集成，抽象为可重用组件或内部平台。这种方法减少了工作重复，并通过使应用团队能够专注于特定领域的问题而不是底层系统工程来加速开发。
- en: In parallel, hybrid roles have emerged to bridge gaps between traditional boundaries.
    For example, full-stack ML engineers combine expertise in modeling, software engineering,
    and infrastructure to own the end-to-end deployment of ML models. Similarly, ML
    enablement roles, including MLOps engineers and applied ML specialists, focus
    on helping teams adopt best practices, integrate tooling, and scale workflows
    efficiently. These roles are especially valuable in organizations with diverse
    teams that vary in ML maturity or technical specialization.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，混合角色已经出现，以弥合传统边界之间的差距。例如，全栈机器学习工程师结合建模、软件工程和基础设施的专长，负责机器学习模型的端到端部署。同样，机器学习赋能角色，包括MLOps工程师和应用机器学习专家，专注于帮助团队采用最佳实践、集成工具和高效扩展工作流程。这些角色在拥有多样化团队且在机器学习成熟度或技术专业化方面存在差异的组织中尤其有价值。
- en: The structure of MLOps teams also varies based on organizational scale, industry,
    and regulatory requirements. In smaller organizations or startups, teams are often
    lean and cross-functional, with close collaboration and informal processes. In
    contrast, larger enterprises may formalize roles and introduce governance frameworks
    to manage compliance, data security, and model risk. Highly regulated sectors,
    including finance, healthcare, and defense, often require additional roles focused
    on validation, auditing, and documentation to meet external reporting obligations.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps团队的结构也因组织规模、行业和监管要求而异。在较小的组织或初创公司中，团队通常规模较小且跨职能，具有紧密的协作和非正式流程。相比之下，大型企业可能会正式化角色并引入治理框架来管理合规性、数据安全和模型风险。高度监管的行业，如金融、医疗保健和防御，通常需要额外的角色专注于验证、审计和文档，以满足外部报告义务。
- en: As [Table 13.5](ch019.xhtml#tbl-mlops-evolution) indicates, the boundaries between
    roles are not rigid. Effective MLOps practices rely on shared understanding, documentation,
    and tools that facilitate communication and coordination across teams. Encouraging
    interdisciplinary fluency, including enabling data scientists to understand deployment
    workflows and DevOps engineers to interpret model monitoring metrics, enhances
    organizational agility and resilience.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表13.5](ch019.xhtml#tbl-mlops-evolution)所示，角色之间的界限并不严格。有效的MLOps实践依赖于共享的理解、文档和工具，这些工具促进了团队间的沟通和协调。鼓励跨学科流畅性，包括使数据科学家能够理解部署工作流程，以及使DevOps工程师能够解释模型监控指标，从而增强组织的敏捷性和弹性。
- en: 'Table 13.5: **Role Evolution**: MLOps roles increasingly specialize as systems
    mature, demanding cross-functional collaboration between data engineers, data
    scientists, and ML engineers to bridge data preparation, model building, and deployment
    challenges. Expanding responsibilities, such as feature store management and model
    validation, reflect the growing need for robust, ethical, and scalable machine
    learning infrastructure.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 表13.5：**角色演变**：随着系统的成熟，MLOps角色越来越专业化，需要数据工程师、数据科学家和机器学习工程师之间的跨职能协作，以解决数据准备、模型构建和部署挑战。扩展的责任，如特征存储管理和模型验证，反映了对强大、道德和可扩展机器学习基础设施日益增长的需求。
- en: '| **Role** | **Key Intersections** | **Evolving Patterns and Specializations**
    |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| **角色** | **关键交叉点** | **演变模式和专业化** |'
- en: '| --- | --- | --- |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Data Engineer** | Works with data scientists to define features and pipelines
    | Expands into real-time data systems and feature store platforms |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| **数据工程师** | 与数据科学家合作定义特征和管道 | 扩展到实时数据系统和特征存储平台 |'
- en: '| **Data Scientist** | Relies on data engineers for clean inputs; collaborates
    with ML engineers | Takes on model validation, interpretability, and ethical considerations
    |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| **数据科学家** | 依赖于数据工程师提供干净的数据输入；与机器学习工程师协作 | 承担模型验证、可解释性和伦理考量 |'
- en: '| **ML Engineer** | Receives models from data scientists; works with DevOps
    to deploy and monitor | Transitions into platform engineering or full-stack ML
    roles |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| **机器学习工程师** | 从数据科学家那里接收模型；与DevOps合作部署和监控 | 转变为平台工程或全栈机器学习角色 |'
- en: '| **DevOps Engineer** | Supports ML engineers with infrastructure, CI/CD, and
    observability | Evolves into MLOps platform roles; integrates governance and security
    tooling |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| **DevOps工程师** | 为机器学习工程师提供基础设施、CI/CD和可观察性支持 | 发展为MLOps平台角色；整合治理和安全工具 |'
- en: '| **Project Manager** | Coordinates across all roles; tracks progress and communication
    | Specializes into ML product management as systems scale |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| **项目经理** | 协调所有角色；跟踪进度和沟通 | 随着系统规模扩大而专业化为机器学习产品管理 |'
- en: '| **Responsible AI Lead** | Collaborates with data scientists and PMs to evaluate
    fairness and compliance | Role emerges as systems face regulatory scrutiny or
    public exposure |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| **负责任的人工智能负责人** | 与数据科学家和项目经理合作评估公平性和合规性 | 角色在系统面临监管审查或公众曝光时出现 |'
- en: '| **Security & Privacy** | Works with DevOps and ML Engineers to | Role formalizes
    as privacy regulations |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| **安全与隐私** | 与DevOps和机器学习工程师合作 | 角色正式化为隐私法规 |'
- en: '| **Engineer** | secure data pipelines and model interfaces | (e.g., GDPR,
    HIPAA) apply to ML workflows |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| **工程师** | 确保数据管道和模型接口的安全 | （例如，GDPR、HIPAA）适用于机器学习工作流程 |'
- en: As machine learning becomes increasingly central to modern software systems,
    roles will continue to adapt in response to emerging tools, methodologies, and
    system architectures. Recognizing the dynamic nature of these responsibilities
    allows teams to allocate resources effectively, design adaptable workflows, and
    foster collaboration that is essential for sustained success in production-scale
    machine learning.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习在现代软件系统中的地位日益重要，角色将继续适应新兴的工具、方法和系统架构。认识到这些责任的动态性，团队可以有效地分配资源，设计可适应的工作流程，并促进对于生产规模机器学习持续成功至关重要的协作。
- en: The specialized roles and cross-functional collaboration patterns described
    above do not emerge in isolation. They evolve alongside the technical and organizational
    maturity of ML systems themselves. Understanding this co-evolution between roles,
    infrastructure, and operational practices provides essential context for designing
    sustainable MLOps implementations.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 上文所述的专门角色和跨职能协作模式并非孤立出现。它们与机器学习系统本身的技术和组织成熟度同步发展。理解角色、基础设施和运营实践之间的这种协同进化，对于设计可持续的MLOps实施提供了基本背景。
- en: System Design and Maturity Framework
  id: totrans-448
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统设计和成熟度框架
- en: Building on the infrastructure components, production operations, and organizational
    roles established earlier, we now examine how these elements integrate into coherent
    operational systems. Machine learning systems do not operate in isolation. Their
    effectiveness depends not only on the quality of the underlying models, but also
    on the maturity of the organizational and technical processes that support them.
    This section explores how operational maturity shapes system architecture and
    provides frameworks for designing MLOps implementations that address the operational
    challenges identified at the chapter’s beginning. Operational maturity refers
    to the degree to which ML workflows are automated, reproducible, monitored, and
    aligned with broader engineering and governance practices. While early-stage efforts
    may rely on ad hoc scripts and manual interventions, production-scale systems
    require deliberate design choices that support long-term sustainability, reliability,
    and adaptability. This section examines how different levels of operational maturity
    influence system architecture, infrastructure design, and organizational structure,
    providing a lens through which to interpret the broader MLOps landscape ([Paleyes,
    Urma, and Lawrence 2022b](ch058.xhtml#ref-kreuzberger2022machine)).
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立早期的基础设施组件、生产运营和组织角色之后，我们现在探讨这些元素如何整合成连贯的运营系统。机器学习系统并非孤立运作。它们的有效性不仅取决于底层模型的质量，还取决于支持它们的组织和技术流程的成熟度。本节探讨了运营成熟度如何塑造系统架构，并为设计解决章节开头确定的运营挑战的MLOps实施提供框架。运营成熟度指的是机器学习工作流程自动化的程度、可重复性、监控程度以及与更广泛的工程和治理实践的协调一致。虽然早期工作可能依赖于临时脚本和手动干预，但生产规模系统需要支持长期可持续性、可靠性和适应性的有意设计选择。本节探讨了不同层次的运营成熟度如何影响系统架构、基础设施设计和组织结构，提供了一个解读更广泛的MLOps景观的视角（[Paleyes,
    Urma, and Lawrence 2022b](ch058.xhtml#ref-kreuzberger2022machine)）。
- en: Operational Maturity
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运营成熟度
- en: 'Operational maturity in machine learning refers to the extent to which an organization
    can reliably develop, deploy, and manage ML systems in a repeatable and scalable
    manner. Unlike the maturity of individual models or algorithms, operational maturity
    reflects systemic capabilities: how well a team or organization integrates infrastructure,
    automation, monitoring, governance, and collaboration into the ML lifecycle.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的运营成熟度指的是一个组织可靠地以可重复和可扩展的方式开发、部署和管理机器学习系统的程度。与单个模型或算法的成熟度不同，运营成熟度反映了系统能力：一个团队或组织将基础设施、自动化、监控、治理和协作整合到机器学习生命周期中的程度。
- en: Low-maturity environments often rely on manual workflows, loosely coupled components,
    and ad hoc experimentation. While sufficient for early-stage research or low-risk
    applications, such systems tend to be brittle, difficult to reproduce, and highly
    sensitive to data or code changes. As ML systems are deployed at scale, these
    limitations quickly become barriers to sustained performance, trust, and accountability.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 低成熟度环境通常依赖于手动工作流程、松散耦合的组件和临时实验。虽然对于早期研究或低风险应用来说足够，但这类系统往往脆弱、难以复制，并且对数据或代码变更高度敏感。随着机器学习系统规模的扩大，这些限制很快成为持续性能、信任和问责制的障碍。
- en: In contrast, high-maturity environments implement modular, versioned, and automated
    workflows that allow models to be developed, validated, and deployed in a controlled
    and observable fashion. Data lineage is preserved across transformations; model
    behavior is continuously monitored and evaluated; and infrastructure is provisioned
    and managed as code. These practices reduce operational friction, enable faster
    iteration, and support robust decision-making in production ([A. Chen et al. 2020](ch058.xhtml#ref-zaharia2018accelerating)).
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，高成熟度环境实施模块化、版本化和自动化的工作流程，允许模型以受控和可观察的方式开发、验证和部署。数据血缘在转换过程中得到保留；模型行为持续监控和评估；基础设施以代码形式提供和管理。这些实践减少了操作摩擦，使迭代更快，并支持生产中的稳健决策([A.
    Chen等人 2020](ch058.xhtml#ref-zaharia2018accelerating))。
- en: 'Operational maturity is not solely a function of tool adoption. While technologies
    such as CI/CD pipelines, model registries, and observability stacks play a role,
    maturity centers on system integration and coordination: how data engineers, data
    scientists, and operations teams collaborate through shared interfaces, standardized
    workflows, and automated handoffs. It is this integration that distinguishes mature
    ML systems from collections of loosely connected artifacts.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 操作成熟度不仅仅是工具采用的功能。虽然CI/CD管道、模型注册表和可观察性堆栈等技术发挥作用，但成熟度集中在系统集成和协调上：数据工程师、数据科学家和运维团队如何通过共享接口、标准化工作流程和自动化交接进行协作。正是这种集成将成熟的机器学习系统与松散连接的工件集合区分开来。
- en: Maturity Levels
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成熟度级别
- en: While operational maturity exists on a continuum, it is useful to distinguish
    between broad stages that reflect how ML systems evolve from research prototypes
    to production-grade infrastructure. These stages are not strict categories, but
    rather indicative of how organizations gradually adopt practices that support
    reliability, scalability, and observability.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然操作成熟度存在于连续体上，但区分反映机器学习系统如何从研究原型发展到生产级基础设施的广泛阶段是有用的。这些阶段不是严格的类别，而是表明组织逐渐采用支持可靠性、可扩展性和可观察性的实践。
- en: 'At the lowest level of maturity, ML workflows are ad hoc: experiments are run
    manually, models are trained on local machines, and deployment involves hand-crafted
    scripts or manual intervention. Data pipelines may be fragile or undocumented,
    and there is limited ability to trace how a deployed model was produced. These
    environments may be sufficient for prototyping, but they are ill-suited for ongoing
    maintenance or collaboration.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在最低成熟度级别，机器学习工作流程是临时的：实验是手动运行的，模型在本地机器上训练，部署涉及手工脚本或手动干预。数据管道可能脆弱或未记录，难以追踪已部署模型的产生过程。这些环境可能足以进行原型设计，但它们不适合持续维护或协作。
- en: As maturity increases, workflows become more structured and repeatable. Teams
    begin to adopt version control, automated training pipelines, and centralized
    model storage. Monitoring and testing frameworks are introduced, and retraining
    workflows become more systematic. Systems at this level can support limited scale
    and iteration but still rely heavily on human coordination.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 随着成熟度的提高，工作流程变得更加结构化和可重复。团队开始采用版本控制、自动训练管道和集中式模型存储。引入了监控和测试框架，重新训练工作流程变得更加系统化。处于这一级别的系统可以支持有限的规模和迭代，但仍严重依赖人工协调。
- en: At the highest levels of maturity, ML systems are fully integrated with infrastructure-as-code,
    continuous delivery pipelines, and automated monitoring. Data lineage, feature
    reuse, and model validation are encoded into the development process. Governance
    is embedded throughout the system, allowing for traceability, auditing, and policy
    enforcement. These environments support large-scale deployment, rapid experimentation,
    and adaptation to changing data and system conditions.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高成熟度级别，机器学习系统与基础设施即代码、持续交付管道和自动化监控完全集成。数据血缘、特征重用和模型验证编码到开发过程中。治理嵌入到整个系统中，允许可追溯性、审计和政策执行。这些环境支持大规模部署、快速实验和对变化的数据和系统条件的适应。
- en: This progression, summarized in [Table 13.6](ch019.xhtml#tbl-maturity-levels),
    offers a system-level framework for analyzing ML operational practices. It emphasizes
    architectural cohesion and lifecycle integration over tool selection, guiding
    the design of scalable and maintainable learning systems.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 这种进展，总结在[表13.6](ch019.xhtml#tbl-maturity-levels)中，为分析机器学习操作实践提供了一个系统级框架。它强调架构的凝聚力和生命周期集成，而不是工具的选择，指导设计可扩展和可维护的学习系统。
- en: '| **Maturity Level** | **System Characteristics** | **Typical Outcomes** |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| **成熟度级别** | **系统特性** | **典型结果** |'
- en: '| --- | --- | --- |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Ad Hoc** | Manual data processing, local training, no version control,
    unclear ownership | Fragile workflows, difficult to reproduce or debug |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| **临时** | 手动数据处理、本地训练、无版本控制、不明确的归属 | 工作流程脆弱，难以重现或调试 |'
- en: '| **Repeatable** | Automated training pipelines, basic CI/CD, centralized model
    storage, some monitoring | Improved reproducibility, limited scalability |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| **可重复** | 自动化训练管道、基本的CI/CD、集中式模型存储、一些监控 | 提高可重复性，有限的扩展性 |'
- en: '| **Scalable** | Fully automated workflows, integrated observability, infrastructure-as-code,
    governance | High reliability, rapid iteration, production-grade ML |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| **可扩展** | 完全自动化的工作流程、集成可观察性、基础设施即代码、治理 | 高可靠性、快速迭代、生产级机器学习 |'
- en: These maturity levels provide a systems lens through which to evaluate ML operations,
    not in terms of specific tools adopted, but in how reliably and cohesively a system
    supports the full machine learning lifecycle. Understanding this progression prepares
    practitioners to identify design bottlenecks and prioritize investments that support
    long-term system sustainability.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 这些成熟度级别提供了一个系统视角来评估机器学习操作，不是基于采用的具体工具，而是基于系统如何可靠和一致地支持整个机器学习生命周期。理解这一进展使从业者能够识别设计瓶颈并优先考虑支持长期系统可持续性的投资。
- en: 'Table 13.6: **Maturity Progression**: Machine learning operational practices
    evolve from manual, fragile workflows toward fully integrated, automated systems,
    impacting reproducibility and scalability. This table outlines key characteristics
    and outcomes at different maturity levels, emphasizing architectural cohesion
    and lifecycle integration for building maintainable learning systems.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 表13.6：**成熟度进展**：机器学习操作实践从手动、脆弱的工作流程发展到完全集成、自动化的系统，影响可重复性和扩展性。本表概述了不同成熟度级别的关键特性和结果，强调构建可维护学习系统的架构一致性和生命周期集成。
- en: '| **Maturity Level** | **System Characteristics** | **Typical Outcomes** |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| **成熟度级别** | **系统特性** | **典型结果** |'
- en: '| --- | --- | --- |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Ad Hoc** | Manual data processing, local training, no version control,
    unclear ownership | Fragile workflows, difficult to reproduce or debug |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| **临时** | 手动数据处理、本地训练、无版本控制、不明确的归属 | 工作流程脆弱，难以重现或调试 |'
- en: '| **Repeatable** | Automated training pipelines, basic CI/CD, centralized model
    storage, some monitoring | Improved reproducibility, limited scalability |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| **可重复** | 自动化训练管道、基本的CI/CD、集中式模型存储、一些监控 | 提高可重复性，有限的扩展性 |'
- en: '| **Scalable** | Fully automated workflows, integrated observability, infrastructure-as-code,
    governance | High reliability, rapid iteration, production-grade ML |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| **可扩展** | 完全自动化的工作流程、集成可观察性、基础设施即代码、治理 | 高可靠性、快速迭代、生产级机器学习 |'
- en: System Design Implications
  id: totrans-473
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统设计影响
- en: As machine learning operations mature, the underlying system architecture evolves
    in response. Operational maturity is not just an organizational concern; it has
    direct consequences for how ML systems are structured, deployed, and maintained.
    Each level of maturity introduces new expectations around modularity, automation,
    monitoring, and fault tolerance, shaping the design space in both technical and
    procedural terms.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习操作的成熟，底层系统架构随之演变。运营成熟度不仅是一个组织问题；它对ML系统的结构、部署和维护有直接影响。每个成熟度级别都引入了关于模块化、自动化、监控和容错的新期望，从技术和程序方面塑造了设计空间。
- en: In low-maturity environments, ML systems are often constructed around monolithic
    scripts and tightly coupled components. Data processing logic may be embedded
    directly within model code, and configurations are managed informally. These architectures,
    while expedient for rapid experimentation, lack the separation of concerns needed
    for maintainability, version control, or safe iteration. As a result, teams frequently
    encounter regressions, silent failures, and inconsistent performance across environments.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 在低成熟度环境中，ML系统通常围绕单体脚本和紧密耦合的组件构建。数据处理逻辑可能直接嵌入到模型代码中，配置管理是非正式的。这些架构虽然便于快速实验，但缺乏维护性、版本控制或安全迭代所需的问题分离。因此，团队经常遇到回归、静默失败和跨环境性能不一致的问题。
- en: As maturity increases, modular abstractions begin to emerge. Feature engineering
    is decoupled from model logic, pipelines are defined declaratively, and system
    boundaries are enforced through APIs and orchestration frameworks. These changes
    support reproducibility and enable teams to scale development across multiple
    contributors or applications. Infrastructure becomes programmable through configuration
    files, and model artifacts are promoted through standardized deployment stages.
    This architectural discipline allows systems to evolve predictably, even as requirements
    shift or data distributions change.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 随着成熟度的提高，模块化抽象开始出现。特征工程与模型逻辑解耦，管道以声明性方式定义，系统边界通过API和编排框架强制执行。这些变化支持可重复性，并使团队能够在多个贡献者或应用程序之间扩展开发。通过配置文件，基础设施变得可编程，模型工件通过标准化的部署阶段进行推广。这种架构纪律使系统能够可预测地演变，即使需求或数据分布发生变化。
- en: 'At high levels of maturity, ML systems exhibit properties commonly found in
    production-grade software systems: stateless services, contract-driven interfaces,
    environment isolation, and observable execution. Design patterns such as feature
    stores, model registries, and infrastructure-as-code become foundational. Crucially,
    system behavior is not inferred from static assumptions, but monitored in real
    time and adapted as needed. This enables feedback-driven development and supports
    closed-loop systems where data, models, and infrastructure co-evolve.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在高成熟度水平上，机器学习系统表现出在生产级软件系统中常见的属性：无状态服务、合同驱动接口、环境隔离和可观察执行。特征存储、模型注册和基础设施即代码等设计模式成为基础。关键的是，系统行为不是从静态假设中推断出来的，而是实时监控并根据需要调整。这使反馈驱动开发成为可能，并支持数据、模型和基础设施共同演变的闭环系统。
- en: 'In each case, operational maturity is not an external constraint but an architectural
    force: it governs how complexity is managed, how change is absorbed, and how the
    system can scale in the face of threats to service uptime (see [Figure 13.10](ch019.xhtml#fig-uptime-iceberg)).
    Design decisions that disregard these constraints may function under ideal conditions,
    but fail under real-world pressures such as latency requirements, drift, outages,
    or regulatory audits. Understanding this relationship between maturity and design
    is essential for building resilient machine learning systems that sustain performance
    over time.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，运营成熟度都不是外部约束，而是一种架构力量：它决定了如何管理复杂性，如何吸收变化，以及系统如何在服务正常运行时间的威胁面前进行扩展（参见[图13.10](ch019.xhtml#fig-uptime-iceberg)）。忽视这些约束的设计决策可能在理想条件下运行，但在现实世界的压力下，如延迟要求、漂移、故障或监管审计下可能会失败。理解成熟度与设计之间的关系对于构建能够持续性能的弹性机器学习系统至关重要。
- en: '![](../media/file216.svg)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file216.svg)'
- en: 'Figure 13.10: **Uptime Dependency Stack**: Robust ML service uptime relies
    on monitoring a layered stack of interdependent components, from infrastructure
    to model performance, mirroring the complexity of modern software systems. Operational
    maturity necessitates observing this entire stack to proactively address potential
    failures and maintain service levels under varying conditions.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10：**正常运行时间依赖栈**：稳健的机器学习服务正常运行时间依赖于从基础设施到模型性能的分层互依组件，反映了现代软件系统的复杂性。运营成熟度要求观察整个栈，以主动解决潜在故障并在各种条件下维持服务水平。
- en: Design Patterns and Anti-Patterns
  id: totrans-481
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设计模式和反模式
- en: The structure of the teams involved in building and maintaining machine learning
    systems plays a significant role in determining operational outcomes. As ML systems
    grow in complexity and scale, organizational patterns must evolve to reflect the
    interdependence between data, modeling, infrastructure, and governance. While
    there is no single ideal structure, certain patterns consistently support operational
    maturity, whereas others tend to hinder it.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 参与构建和维护机器学习系统的团队结构在决定运营结果方面发挥着重要作用。随着机器学习系统在复杂性和规模上的增长，组织模式必须演变以反映数据、建模、基础设施和治理之间的相互依赖性。虽然没有单一的理想结构，但某些模式始终支持运营成熟度，而其他模式则往往阻碍其发展。
- en: In mature environments, organizational design emphasizes clear ownership, cross-functional
    collaboration, and interface discipline between roles. For instance, platform
    teams may take responsibility for shared infrastructure, tooling, and CI/CD pipelines,
    while domain teams focus on model development and business alignment. This separation
    of concerns enables reuse, standardization, and parallel development. Interfaces
    between teams, including feature definitions, data schemas, and deployment targets,
    are well-defined and versioned, reducing friction and ambiguity.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在成熟的环境中，组织设计强调明确的职责归属、跨职能协作以及角色间的接口纪律。例如，平台团队可能负责共享的基础设施、工具和CI/CD管道，而领域团队则专注于模型开发和业务对齐。这种关注点的分离促进了重用、标准化和并行开发。团队间的接口，包括功能定义、数据模式和部署目标，都得到了良好的定义和版本控制，减少了摩擦和歧义。
- en: One effective pattern is the creation of a centralized MLOps team that provides
    shared services to multiple model development groups. This team maintains tooling
    for model training, validation, deployment, and monitoring, and may operate as
    an internal platform provider. Such structures promote consistency, reduce duplicated
    effort, and accelerate onboarding for new projects. Alternatively, some organizations
    adopt a federated model, embedding MLOps engineers within product teams while
    maintaining a central architectural function to guide system-wide integration.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有效的模式是创建一个中央MLOps团队，为多个模型开发小组提供共享服务。这个团队维护模型训练、验证、部署和监控的工具，并可能作为内部平台提供商运营。这样的结构促进了一致性，减少了重复工作，并加速了新项目的入职。另一些组织采用联邦模型，将MLOps工程师嵌入到产品团队中，同时保持一个中央架构功能来指导系统级集成。
- en: In contrast, anti-patterns often emerge when responsibilities are fragmented
    or poorly aligned. One common failure mode is the tool-first approach, in which
    teams adopt infrastructure or automation tools without first defining the processes
    and roles that should govern their use. This can result in fragile pipelines,
    unclear handoffs, and duplicated effort. Another anti-pattern is siloed experimentation,
    where data scientists operate in isolation from production engineers, leading
    to models that are difficult to deploy, monitor, or retrain effectively.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当职责分散或对齐不佳时，反模式往往会出现。一种常见的失败模式是“工具优先”的方法，其中团队在未首先定义应规范其使用的流程和角色的情况下就采用基础设施或自动化工具。这可能导致脆弱的管道、不明确的交接和重复的工作。另一种反模式是孤岛式实验，数据科学家在生产工程师的隔离下工作，导致难以部署、监控或有效重新训练的模型。
- en: Organizational drift is another subtle challenge. As teams scale, undocumented
    workflows and informal agreements may become entrenched, increasing the cost of
    coordination and reducing transparency. Without deliberate system design and process
    review, even previously functional structures can accumulate technical and organizational
    debt.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 组织漂移是另一个微妙挑战。随着团队规模的扩大，未记录的工作流程和非正式协议可能变得根深蒂固，增加了协调成本并降低了透明度。没有故意的系统设计和流程审查，甚至之前功能性的结构也可能积累技术和组织债务。
- en: Ultimately, organizational maturity must co-evolve with system complexity. Teams
    must establish communication patterns, role definitions, and accountability structures
    that reinforce the principles of modularity, automation, and observability. Operational
    excellence in machine learning is not just a matter of technical capability; it
    is the product of coordinated, intentional systems thinking across human and computational
    boundaries.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，组织成熟度必须与系统复杂性协同进化。团队必须建立沟通模式、角色定义和问责结构，以加强模块化、自动化和可观察性的原则。机器学习的运营卓越不仅仅是技术能力的问题；它是跨越人类和计算边界的协调、有意系统思维的结果。
- en: The organizational patterns described above must be supported by technical architectures
    that can handle the unique reliability challenges of ML systems. MLOps inherits
    many reliability challenges from distributed systems but adds unique complications
    through learning components. Traditional reliability patterns require adaptation
    to account for the probabilistic nature of ML systems and the dynamic behavior
    of learning components.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 上述组织模式必须由能够处理机器学习系统独特可靠性挑战的技术架构支持。MLOps从分布式系统继承了众多可靠性挑战，但通过学习组件增加了独特的复杂性。传统的可靠性模式需要适应，以考虑到机器学习系统的概率性质和学习组件的动态行为。
- en: Circuit breaker patterns must account for model-specific failure modes, where
    prediction accuracy degradation requires different thresholds than service availability
    failures. Bulkhead patterns become critical when isolating experimental model
    versions from production traffic, requiring resource partitioning strategies that
    prevent resource exhaustion in one model from affecting others. The Byzantine
    fault tolerance problem takes on new characteristics in MLOps environments, where
    “Byzantine” behavior includes models producing plausible but incorrect outputs
    rather than obvious failures.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 断路器模式必须考虑到模型特定的故障模式，其中预测精度下降需要与服务可用性故障不同的阈值。当需要隔离实验性模型版本与生产流量时，舱壁模式变得至关重要，需要资源分区策略，以防止一个模型资源耗尽影响其他模型。在MLOps环境中，拜占庭容错问题具有新的特征，其中“拜占庭”行为包括模型产生看似合理但错误的输出，而不是明显的故障。
- en: Traditional consensus algorithms focus on agreement among correct nodes, but
    ML systems require consensus about model correctness when ground truth may be
    delayed or unavailable. This necessitates probabilistic agreement protocols that
    can operate under uncertainty, using techniques from distributed machine learning
    to aggregate model decisions across replicas while accounting for potential model
    drift or adversarial inputs. These reliability patterns form the theoretical foundation
    for operational practices that distinguish robust MLOps implementations from fragile
    ones.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的共识算法侧重于正确节点之间的共识，但ML系统在真实情况可能延迟或不可用的情况下，需要关于模型正确性的共识。这需要概率性共识协议，在不确定性下运行，使用分布式机器学习技术来聚合模型决策，同时考虑到潜在的模型漂移或对抗性输入。这些可靠性模式构成了区分稳健MLOps实现和脆弱实现的操作实践的理论基础。
- en: Contextualizing MLOps
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将MLOps置于具体情境中
- en: The operational maturity of a machine learning system is not an abstract ideal;
    it is realized in concrete systems with physical, organizational, and regulatory
    constraints. While the preceding sections have outlined best practices for mature
    MLOps, which include CI/CD, monitoring, infrastructure provisioning, and governance,
    these practices are rarely deployed in pristine, unconstrained environments. In
    reality, every ML system operates within a specific context that shapes how MLOps
    workflows are implemented, prioritized, and adapted.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统的运行成熟度不是一个抽象的理想；它是在具有物理、组织和监管约束的具体系统中实现的。虽然前几节概述了成熟的MLOps的最佳实践，包括持续集成/持续部署（CI/CD）、监控、基础设施提供和治理，但这些实践很少在原始、无约束的环境中部署。实际上，每个ML系统都在一个特定的环境中运行，这决定了MLOps工作流程的实施、优先级和适应性。
- en: System constraints may arise from the physical environment in which a model
    is deployed, such as limitations in compute, memory, or power. These are common
    in edge and embedded systems, where models must run under strict latency and resource
    constraints. Connectivity limitations, such as intermittent network access or
    bandwidth caps, further complicate model updates, monitoring, and telemetry collection.
    In high-assurance domains, including healthcare, finance, and industrial control
    systems, governance, traceability, and fail-safety may take precedence over throughput
    or latency. These factors do not simply influence system performance; they alter
    how MLOps pipelines must be designed and maintained.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 系统约束可能源于模型部署的物理环境，例如计算、内存或功率的限制。这些在边缘和嵌入式系统中很常见，在这些系统中，模型必须在严格的延迟和资源约束下运行。连接性限制，如间歇性网络访问或带宽限制，进一步复杂化了模型更新、监控和遥测收集。在高保证领域，包括医疗保健、金融和工业控制系统，治理、可追溯性和容错可能比吞吐量或延迟更重要。这些因素不仅影响系统性能；它们还改变了MLOps管道的设计和维护方式。
- en: For instance, a standard CI/CD pipeline for retraining and deployment may be
    infeasible in environments where direct access to the model host is not possible.
    In such cases, teams must implement alternative delivery mechanisms, such as over-the-air
    updates, that account for reliability, rollback capability, and compatibility
    across heterogeneous devices. Similarly, monitoring practices that assume full
    visibility into runtime behavior may need to be reimagined using indirect signals,
    coarse-grained telemetry, or on-device anomaly detection. Even the simple task
    of collecting training data may be limited by privacy concerns, device-level storage
    constraints, or legal restrictions on data movement.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在无法直接访问模型主机的情况下，标准的CI/CD流水线可能不可行。在这种情况下，团队必须实施替代的交付机制，如空中更新，这些机制考虑到可靠性、回滚能力和跨异构设备的兼容性。同样，假设能够全面了解运行时行为的监控实践可能需要通过间接信号、粗粒度遥测或设备异常检测来重新构想。即使是收集训练数据的简单任务也可能受到隐私问题、设备级存储限制或数据流动的法律限制的限制。
- en: 'These adaptations should not be interpreted as deviations from maturity, but
    rather as expressions of maturity under constraint. A well-engineered ML system
    accounts for the realities of its operating environment and revises its operational
    practices accordingly. This is the essence of systems thinking in MLOps: applying
    general principles while designing for specificity.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 这些适应性调整不应被理解为成熟度的偏离，而应被视为在约束条件下的成熟表现。一个精心设计的机器学习系统会考虑到其运营环境的现实情况，并相应地调整其运营实践。这是MLOps中系统思维的精髓：在具体设计时应用一般原则。
- en: As we turn to the chapters ahead, we will encounter several of these contextual
    factors, including on-device learning, privacy preservation, safety and robustness,
    and sustainability. Each presents not just a technical challenge but a system-level
    constraint that reshapes how machine learning is practiced and maintained at scale.
    Understanding MLOps in context is therefore not optional; it is foundational to
    building ML systems that are viable, trustworthy, and effective in the real world.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们转向接下来的章节时，我们将遇到这些上下文因素中的几个，包括设备学习、隐私保护、安全性和鲁棒性以及可持续性。每个都不仅是一个技术挑战，而且是一个系统级约束，它重塑了大规模实践和维护机器学习的方式。因此，在上下文中理解MLOps不是可选的；它是构建在现实世界中可行、值得信赖和有效的机器学习系统的基础。
- en: Future Operational Considerations
  id: totrans-497
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 未来运营考虑因素
- en: 'As this chapter has shown, the deployment and maintenance of machine learning
    systems require more than technical correctness at the model level. They demand
    architectural coherence, organizational alignment, and operational maturity. The
    progression from ad hoc experimentation to scalable, auditable systems reflects
    a broader shift: machine learning is no longer confined to research environments;
    it is a core component of production infrastructure.'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章所示，机器学习系统的部署和维护不仅需要在模型层面保证技术正确性，还需要架构一致性、组织协调和运营成熟度。从临时实验到可扩展、可审计系统的转变反映了更广泛的转变：机器学习不再局限于研究环境；它已成为生产基础设施的核心组成部分。
- en: 'Understanding the maturity of an ML system helps clarify what challenges are
    likely to emerge and what forms of investment are needed to address them. Early-stage
    systems benefit from process discipline and modular abstraction; mature systems
    require automation, governance, and resilience. Design choices made at each stage
    influence the pace of experimentation, the robustness of deployed models, and
    the ability to integrate evolving requirements: technical, organizational, and
    regulatory.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 理解机器学习系统的成熟度有助于明确可能出现的挑战以及解决这些挑战所需的投资形式。早期系统受益于流程纪律和模块化抽象；成熟系统需要自动化、治理和弹性。每个阶段的设计选择都会影响实验的速度、部署模型的鲁棒性以及整合不断变化要求的能力：技术、组织和监管。
- en: This systems-oriented view of MLOps also sets the stage for the next phase of
    this book. The specialized operational contexts examined in subsequent chapters,
    edge computing ([Chapter 14](ch020.xhtml#sec-ondevice-learning)), adversarial
    robustness ([Chapter 16](ch022.xhtml#sec-robust-ai)), and privacy-preserving deployment
    ([Chapter 15](ch021.xhtml#sec-security-privacy)), each require adaptations of
    the foundational MLOps principles established here. These topics represent not
    merely extensions of model performance, but domains in which operational maturity
    directly enables feasibility, safety, and long-term value.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 这种以系统为导向的MLOps视角也为本书的下一阶段奠定了基础。后续章节中考察的特定操作环境，包括边缘计算（[第14章](ch020.xhtml#sec-ondevice-learning)）、对抗鲁棒性（[第16章](ch022.xhtml#sec-robust-ai)）和隐私保护部署（[第15章](ch021.xhtml#sec-security-privacy)），每个都需要对这里确立的基础MLOps原则进行调整。这些主题不仅代表了模型性能的扩展，而且代表了操作成熟度直接促进可行性、安全性和长期价值的领域。
- en: Operational maturity is therefore not the end of the machine learning system
    lifecycle; it is the foundation upon which production-grade, responsible, and
    adaptive systems are built. The following chapters explore what it takes to build
    such systems under domain-specific constraints, further expanding the scope of
    what it means to engineer machine learning at scale.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，操作成熟度并不是机器学习系统生命周期的终点；它是构建生产级、负责任和自适应系统的基础。接下来的章节将探讨在特定领域约束下构建此类系统所需的内容，进一步扩大了在规模上工程化机器学习的含义。
- en: Enterprise-Scale ML Systems
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 企业级机器学习系统
- en: At the highest levels of operational maturity, some organizations are implementing
    what can be characterized as AI factories. There are specialized computing infrastructures
    designed to manage the entire AI lifecycle at unprecedented scale. These represent
    the logical extension of the scalable maturity level discussed earlier, where
    fully automated workflows, integrated observability, and infrastructure-as-code
    principles are applied to intelligence manufacturing rather than traditional software
    delivery.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作成熟度的最高水平，一些组织正在实施可以被称为AI工厂的东西。这些组织拥有专门计算基础设施，旨在以前所未有的规模管理整个AI生命周期。这些基础设施代表了之前讨论的可扩展成熟水平的逻辑延伸，其中完全自动化的工作流程、集成可观察性和基础设施即代码原则被应用于智能制造，而不是传统的软件交付。
- en: AI factories emerge when organizations need to optimize not just individual
    model deployments, but entire AI production pipelines that support multiple concurrent
    models, diverse inference patterns, and continuous high-volume operations. The
    computational demands driving this evolution include post-training scaling, where
    fine-tuning models for specific applications requires significantly more compute
    during inference than initial training, and test-time scaling, where advanced
    AI applications employ iterative reasoning that can consume orders of magnitude
    more computational resources than traditional inference patterns. Unlike traditional
    data centers designed for general-purpose computing, these systems are specifically
    architected for AI workloads, emphasizing inference performance, energy efficiency,
    and the ability to transform raw data into actionable intelligence at scale.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 当组织需要优化不仅仅是单个模型部署，而是支持多个并发模型、多样化推理模式和连续高量级操作的整个AI生产管道时，AI工厂便应运而生。推动这一演变的计算需求包括训练后扩展，即针对特定应用对模型进行微调需要在推理阶段比初始训练阶段显著增加计算量，以及测试时扩展，即高级AI应用采用迭代推理，其计算资源消耗可能比传统推理模式高几个数量级。与为通用计算设计的传统数据中心不同，这些系统专门为AI工作负载而设计，强调推理性能、能源效率和大规模将原始数据转化为可操作智能的能力。
- en: The operational challenges in AI factories extend the principles we have discussed.
    They require sophisticated resource allocation across heterogeneous workloads,
    system-level observability that correlates performance across multiple models,
    and fault tolerance mechanisms that can handle cascading failures across interdependent
    AI systems. These systems are not merely scaled versions of traditional MLOps
    deployments, but a qualitatively different approach to managing AI infrastructure
    that may influence how the field evolves as AI becomes increasingly central to
    organizational strategy and value creation.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能工厂的运营挑战扩展了我们讨论的原则。它们需要在异构工作负载之间进行复杂的资源分配，系统级可观察性以关联多个模型之间的性能，以及能够处理跨相互依赖的人工智能系统级联故障的容错机制。这些系统不仅仅是传统MLOps部署的扩展版本，而是一种管理人工智能基础设施的质的变化方法，这可能会影响该领域随着人工智能在组织战略和价值创造中日益成为核心而如何发展。
- en: Investment and Return on Investment
  id: totrans-506
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 投资与投资回报率
- en: While the operational benefits of MLOps are substantial, implementing mature
    MLOps practices requires significant organizational investment in infrastructure,
    tooling, and specialized personnel. Understanding the costs and expected returns
    helps organizations make informed decisions about MLOps adoption and maturity
    progression.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然MLOps的运营效益很大，但实施成熟的MLOps实践需要组织在基础设施、工具和专门人员方面的重大投资。了解成本和预期回报有助于组织在MLOps采用和成熟度进展方面做出明智的决定。
- en: Building a mature MLOps platform typically represents a multi-year, multi-million
    dollar investment for enterprise-scale deployments. Organizations must invest
    in specialized infrastructure including feature stores, model registries, orchestration
    platforms, and monitoring systems. Additionally, they need dedicated platform
    teams with expertise spanning data engineering, machine learning, and DevOps,
    roles that command premium salaries in competitive markets. The initial setup
    costs for comprehensive MLOps infrastructure often range from $500,000 to $5 million
    annually, depending on scale and complexity requirements.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 构建成熟的MLOps平台通常代表企业级部署的多年、数百万美元的投资。组织必须在包括特征存储、模型注册、编排平台和监控系统在内的专用基础设施上进行投资。此外，他们还需要拥有跨数据工程、机器学习和DevOps的专业知识，这些角色在竞争激烈的市场中享有高薪。全面的MLOps基础设施的初始设置成本通常每年在50万美元到500万美元之间，具体取决于规模和复杂性的要求。
- en: However, the return on investment becomes compelling when considering the operational
    improvements that mature MLOps enables. Organizations with established MLOps practices
    report reducing model deployment time from months to days or weeks, dramatically
    accelerating time-to-market for ML-driven products and features. Model failure
    rates in production decrease from approximately 80% in ad hoc environments to
    less than 20% in mature MLOps implementations, reducing costly debugging cycles
    and improving system reliability. Perhaps most significantly, mature MLOps platforms
    enable organizations to manage hundreds or thousands of models simultaneously,
    creating economies of scale that justify the initial infrastructure investment.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当考虑到成熟的MLOps带来的运营改进时，投资回报率变得非常有吸引力。拥有成熟MLOps实践的组织报告称，将模型部署时间从几个月缩短到几天或几周，大大加快了以机器学习为驱动的产品和功能的上市时间。在生产环境中，模型故障率从约80%的临时环境降低到成熟的MLOps实施中的不到20%，减少了昂贵的调试周期并提高了系统可靠性。也许最重要的是，成熟的MLOps平台使组织能够同时管理数百或数千个模型，创造了规模经济，这证明了初始基础设施投资的合理性。
- en: The ROI calculation must also account for reduced operational overhead and improved
    team productivity. Automated retraining pipelines eliminate manual effort required
    for model updates, while standardized deployment processes reduce the specialized
    knowledge needed for each model release. Feature reuse across teams prevents duplicated
    engineering effort, and systematic monitoring reduces the time spent diagnosing
    performance issues. Organizations frequently report 30-50% improvements in data
    science team productivity after implementing comprehensive MLOps platforms, as
    teams can focus on model development rather than operational concerns.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 投资回报率的计算还必须考虑降低运营成本和提高团队生产力。自动化的重新训练管道消除了模型更新所需的手动工作，而标准化的部署流程减少了每个模型发布所需的专门知识。跨团队的特征重用防止了重复的工程工作，而系统性的监控减少了诊断性能问题所花费的时间。组织通常报告，在实施全面的MLOps平台后，数据科学团队的效率提高了30-50%，因为团队可以专注于模型开发而不是运营问题。
- en: '**Investment Timeline and Considerations**'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '**投资时间表和考虑因素**'
- en: '**Year 1**: Foundation building with basic CI/CD, monitoring, and containerization
    ($1-2 M investment) - Focus on preventing the most costly failures through basic
    automation - Expected ROI: Reduced failure rates and faster debugging cycles'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一年**: 建立基础，包括基本的CI/CD、监控和容器化（投资1-2M） - 重点关注通过基本自动化防止最昂贵的故障 - 预期回报：降低故障率和加快调试周期'
- en: '**Year 2-3**: Platform maturation with advanced features like automated retraining,
    sophisticated monitoring, and feature stores ($2-3 M additional investment) -
    Enables scaling to dozens of concurrent models - Expected ROI: Significant productivity
    gains and deployment velocity improvements'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二年-第三年**: 平台成熟，具备自动化再训练、复杂监控和特征存储等高级功能（额外投资2-3M） - 使扩展到数十个并发模型成为可能 - 预期回报：显著的生产力提升和部署速度改进'
- en: '**Year 3+**: Optimization and specialization for domain-specific requirements
    ($500 K-1 M annual maintenance) - Platform supports hundreds of models with minimal
    incremental effort - Expected ROI: Economies of scale and competitive advantage
    through ML capabilities'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三年及以上**: 针对特定领域需求的优化和专业化（年维护费500K-1M） - 平台支持数百个模型，仅需最小增量努力 - 预期回报：通过机器学习能力实现规模经济和竞争优势'
- en: The strategic value of MLOps extends beyond operational efficiency to enable
    organizational capabilities that would be impossible without systematic engineering
    practices. Mature MLOps platforms support rapid experimentation, controlled A/B
    testing of model variations, and real-time adaptation to changing conditions,
    capabilities that can provide competitive advantages worth far more than the initial
    investment. Organizations should view MLOps not merely as an operational necessity,
    but as foundational infrastructure that enables sustained innovation in machine
    learning applications.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps的战略价值不仅超越了运营效率，还使组织具备了没有系统化工程实践就无法实现的能力。成熟的MLOps平台支持快速实验、模型变体的受控A/B测试以及实时适应变化条件，这些能力可以提供比初始投资价值更高的竞争优势。组织应将MLOps不仅仅视为运营的必要性，而是作为使机器学习应用持续创新的基础设施。
- en: Having established the conceptual frameworks, from operational challenges through
    infrastructure components, production operations, organizational roles, and maturity
    models, we now examine how these elements combine in practice. The following case
    studies demonstrate how the theoretical principles translate into concrete implementation
    choices, showing both the universal applicability of MLOps concepts and their
    domain-specific adaptations.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在确立了从运营挑战到基础设施组件、生产操作、组织角色和成熟度模型的概念框架之后，我们现在考察这些元素在实际中的结合方式。以下案例研究展示了理论原则如何转化为具体实施选择，展示了MLOps概念的普遍适用性和特定领域的适应性。
- en: Case Studies
  id: totrans-517
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究
- en: The operational design principles, technical debt patterns, and maturity frameworks
    examined throughout this chapter come together in real-world implementations that
    demonstrate their practical importance. These case studies explicitly illustrate
    how the operational challenges identified earlier, from data dependency debt to
    feedback loops, manifest in production systems, and how the infrastructure components,
    monitoring strategies, and cross-functional roles work together to address them.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中考察的操作设计原则、技术债务模式以及成熟度框架，在现实世界的实施中汇聚在一起，展示了它们的实际重要性。这些案例研究明确说明了之前确定的操作挑战，从数据依赖债务到反馈循环，如何在生产系统中体现，以及基础设施组件、监控策略和跨职能角色如何协同工作以解决这些问题。
- en: We examine two cases that represent distinct deployment contexts, each requiring
    domain-specific adaptations of standard MLOps practices while maintaining the
    core principles of automated pipelines, cross-functional collaboration, and continuous
    monitoring. The Oura Ring case study demonstrates how pipeline debt and configuration
    management challenges play out in resource-constrained edge environments, where
    traditional MLOps infrastructure must be adapted for embedded systems. The ClinAIOps
    case study shows how feedback loops and governance requirements drive specialized
    operational frameworks in healthcare, where human-AI collaboration and regulatory
    compliance reshape standard MLOps practices.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考察了两个代表不同部署环境的案例，每个案例都需要对标准 MLOps 实践进行特定领域的调整，同时保持自动化管道、跨职能协作和持续监控的核心原则。Oura
    Ring 案例研究展示了管道债务和配置管理挑战如何在资源受限的边缘环境中体现，其中传统的 MLOps 基础设施必须适应嵌入式系统。ClinAIOps 案例研究展示了反馈循环和治理要求如何推动医疗保健中的专业运营框架，其中人机协作和法规遵从重塑了标准
    MLOps 实践。
- en: Through these cases, we trace specific connections between the theoretical frameworks
    presented earlier and their practical implementation. Each example demonstrates
    how organizations navigate the operational challenges discussed at the chapter’s
    beginning while implementing the infrastructure and production operations detailed
    in the middle sections. The cases show how role specialization and operational
    maturity directly impact system design choices and long-term sustainability.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些案例，我们追踪了之前提出的理论框架与其实际应用之间的具体联系。每个例子都展示了组织如何在实施中间部分详细说明的基础设施和生产操作的同时，应对章节开头讨论的操作挑战。这些案例显示了角色专业化和运营成熟度如何直接影响系统设计选择和长期可持续性。
- en: Oura Ring Case Study
  id: totrans-521
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Oura Ring 案例研究
- en: The Oura Ring represents a compelling example of MLOps practices applied to
    consumer wearable devices, where embedded machine learning must operate under
    strict resource constraints while delivering accurate health insights. This case
    study demonstrates how systematic data collection, model development, and deployment
    practices enable successful embedded ML systems. We examine the development context
    and motivation, data acquisition and preprocessing challenges, model development
    approaches, and deployment considerations for resource-constrained environments.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: Oura Ring 代表了将 MLOps 实践应用于消费级可穿戴设备的引人入胜的例子，其中嵌入式机器学习必须在严格的资源限制下运行，同时提供准确的健康洞察。本案例研究展示了系统性的数据收集、模型开发和部署实践如何使嵌入式
    ML 系统成功。我们考察了开发环境和动机、数据获取和预处理挑战、模型开发方法以及资源受限环境下的部署考虑。
- en: Context and Motivation
  id: totrans-523
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 环境与动机
- en: The Oura Ring is a consumer-grade wearable device designed to monitor sleep,
    activity, and physiological recovery through embedded sensing and computation.
    By measuring signals such as motion, heart rate, and body temperature, the device
    estimates sleep stages and delivers personalized feedback to users. Unlike traditional
    cloud-based systems, much of the Oura Ring’s data processing and inference occurs
    directly on the device, making it a practical example of embedded machine learning
    in production.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: Oura Ring 是一款消费级可穿戴设备，旨在通过嵌入式感应和计算监测睡眠、活动和生理恢复。通过测量运动、心率以及体温等信号，该设备估算睡眠阶段并向用户提供个性化反馈。与传统的基于云的系统不同，Oura
    Ring 的数据处理和推理大部分直接在设备上完成，使其成为生产中嵌入式机器学习的实际例子。
- en: The central objective for the development team was to improve the device’s accuracy
    in classifying sleep stages, aligning its predictions more closely with those
    obtained through polysomnography (PSG)[43](#fn43), the clinical gold standard
    for sleep monitoring. Initial evaluations revealed a 62% correlation between the
    Oura Ring’s predictions and PSG-derived labels, in contrast to the 82–83% correlation
    observed between expert human scorers. This discrepancy highlighted both the promise
    and limitations of the initial model, prompting an effort to re-evaluate data
    collection, preprocessing, and model development workflows. The case illustrates
    the importance of robust MLOps practices, particularly when operating under the
    constraints of embedded systems.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开发团队来说，核心目标是提高设备在分类睡眠阶段时的准确性，使其预测结果更接近通过多导睡眠图（PSG）[43](#fn43)获得的临床黄金标准。初步评估显示，Oura戒指的预测与PSG得出的标签之间有62%的相关性，而专家评分员之间观察到的是82-83%的相关性。这种差异突出了初始模型的潜力和局限性，促使重新评估数据收集、预处理和模型开发工作流程。这个案例说明了稳健的MLOps实践的重要性，尤其是在嵌入式系统约束下操作时。
- en: Data Acquisition and Preprocessing
  id: totrans-526
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据获取和预处理
- en: To overcome the performance limitations of the initial model, the Oura team
    focused on constructing a robust, diverse dataset grounded in clinical standards.
    They designed a large-scale sleep study involving 106 participants from three
    continents, including Asia, Europe, and North America, capturing broad demographic
    variability across age, gender, and lifestyle. During the study, each participant
    wore the Oura Ring while simultaneously undergoing polysomnography (PSG), the
    clinical gold standard for sleep staging. This pairing enabled the creation of
    a high-fidelity labeled dataset aligning wearable sensor data with validated sleep
    annotations.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服初始模型的性能限制，Oura团队专注于构建一个基于临床标准的稳健、多样化的数据集。他们设计了一项大规模睡眠研究，涉及来自亚洲、欧洲和北美洲的106名参与者，捕捉了年龄、性别和生活方式方面的广泛人口统计学差异。在研究期间，每位参与者都佩戴了Oura戒指，并同时进行多导睡眠图（PSG），这是睡眠分期临床上的黄金标准。这种配对使得能够创建一个高保真度标记的数据集，将可穿戴传感器数据与验证过的睡眠注释相匹配。
- en: In total, the study yielded 440 nights of data and over 3,400 hours of time-synchronized
    recordings. This dataset captured not only physiological diversity but also variability
    in environmental and behavioral factors, which is critical for generalizing model
    performance across a real-world user base.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 总共，该研究产生了440个晚上的数据，以及超过3,400小时的时间同步记录。这个数据集不仅捕捉了生理多样性，还包括了环境和行为因素的变化，这对于在现实世界用户群体中推广模型性能至关重要。
- en: To manage the complexity and scale of this dataset, the team implemented automated
    data pipelines for ingestion, cleaning, and preprocessing. Physiological signals,
    comprising heart rate, motion, and body temperature, were extracted and validated
    using structured workflows. Leveraging the Edge Impulse platform[44](#fn44), they
    consolidated raw inputs from multiple sources, resolved temporal misalignments,
    and structured the data for downstream model development. These workflows address
    the **data dependency debt** patterns identified earlier. By implementing robust
    versioning and lineage tracking, the team avoided the unstable data dependencies
    that commonly plague embedded ML systems. The structured approach to pipeline
    automation also mitigates **pipeline debt**, ensuring that data processing remains
    maintainable as the system scales across different hardware configurations and
    user populations.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 为了管理这个数据集的复杂性和规模，团队实施了自动化的数据管道，用于数据摄取、清洗和预处理。通过结构化的工作流程提取和验证了生理信号，包括心率、运动和体温。利用Edge
    Impulse平台[44](#fn44)，他们整合了来自多个来源的原始输入，解决了时间错位问题，并为下游模型开发结构化了数据。这些工作流程解决了之前识别出的**数据依赖债务**模式。通过实施稳健的版本控制和血缘跟踪，团队避免了通常困扰嵌入式ML系统的数据依赖不稳定问题。对管道自动化的结构化方法也减轻了**管道债务**，确保数据处理在系统扩展到不同的硬件配置和用户群体时保持可维护性。
- en: Model Development and Evaluation
  id: totrans-530
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型开发和评估
- en: With a high-quality, clinically labeled dataset in place, the Oura team advanced
    to the development and evaluation of machine learning models designed to classify
    sleep stages. Recognizing the operational constraints of wearable devices, model
    design prioritized efficiency and interpretability alongside predictive accuracy.
    Rather than employing complex architectures typical of server-scale deployments,
    the team selected models that could operate within the ring’s limited memory and
    compute budget.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立了高质量的、临床标注的数据集之后，Oura团队进一步发展到开发和评估旨在分类睡眠阶段的机器学习模型。认识到可穿戴设备的操作限制，模型设计优先考虑效率、可解释性和预测准确性。而不是采用服务器规模部署中典型的复杂架构，团队选择了可以在戒指有限的内存和计算预算内运行的模型。
- en: Two model configurations were explored. The first used only accelerometer data,
    representing a lightweight architecture optimized for minimal energy consumption
    and low-latency inference. The second model incorporated additional physiological
    inputs, including heart rate variability and body temperature, enabling the capture
    of autonomic nervous system activity and circadian rhythms, factors known to correlate
    with sleep stage transitions.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 探索了两种模型配置。第一种仅使用加速度计数据，代表了一种轻量级架构，优化了最小能量消耗和低延迟推理。第二种模型包含了额外的生理输入，包括心率变异性体温，能够捕捉自主神经系统活动和昼夜节律，这些因素已知与睡眠阶段转换相关。
- en: To evaluate performance, the team applied five-fold cross-validation[45](#fn45)
    and benchmarked the models against the gold-standard PSG annotations. Through
    iterative tuning of hyperparameters and refinement of input features, the enhanced
    models achieved a correlation accuracy of 79%, representing a significant improvement
    from baseline toward the clinical benchmark.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估性能，团队应用了五折交叉验证[45](#fn45)并将模型与金标准的PSG标注进行了基准测试。通过迭代调整超参数和细化输入特征，增强的模型实现了79%的相关准确性，这比基线向临床基准的改进是显著的。
- en: These performance gains did not result solely from architectural innovation.
    Instead, they reflect the broader impact of an MLOps approach that integrated
    data collection, reproducible training pipelines, and disciplined evaluation practices.
    The careful management of hyperparameters and feature configurations demonstrates
    effective mitigation of configuration debt. By maintaining structured documentation
    and version control of model parameters, the team avoided the fragmented settings
    that often undermine embedded ML deployments. This approach required close collaboration
    between data scientists (who designed the model architectures), ML engineers (who
    optimized for embedded constraints), and DevOps engineers (who managed the deployment
    pipeline), illustrating the role specialization discussed earlier in action.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 这些性能提升并非仅仅源于架构创新。相反，它们反映了MLOps方法更广泛的影响，该方法整合了数据收集、可复制的训练管道和纪律性的评估实践。对超参数和特征配置的精心管理展示了有效缓解配置债务的效果。通过维护结构化的文档和模型参数的版本控制，团队避免了经常破坏嵌入式ML部署的碎片化设置。这种方法需要数据科学家（设计模型架构的人）、ML工程师（优化嵌入式约束的人）和DevOps工程师（管理部署管道的人）之间的紧密合作，这展示了之前讨论过的角色专业化的实际应用。
- en: Deployment and Iteration
  id: totrans-535
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署和迭代
- en: Following model validation, the Oura team transitioned to deploying the trained
    models onto the ring’s embedded hardware. Deployment in this context required
    careful accommodation of strict constraints on memory, compute, and power. The
    lightweight model, which relied solely on accelerometer input, was particularly
    well-suited for real-time inference on-device, delivering low-latency predictions
    with minimal energy usage. In contrast, the more complex model, which utilized
    additional physiological signals, including heart rate variability and temperature,
    was deployed selectively, where higher predictive fidelity was required and system
    resources permitted.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型验证之后，Oura团队过渡到将训练好的模型部署到戒指的嵌入式硬件上。在这种背景下，部署需要仔细适应对内存、计算和功率的严格限制。仅依赖加速度计输入的轻量级模型特别适合在设备上实时推理，以最低的能量消耗提供低延迟的预测。相比之下，更复杂的模型利用了额外的生理信号，包括心率变异性温度，这些信号被选择性地部署，在需要更高预测精度且系统资源允许的情况下。
- en: To facilitate reliable and scalable deployment, the team developed a modular
    toolchain for converting trained models into optimized formats suitable for embedded
    execution. This process included model compression techniques such as quantization
    and pruning, which reduced model size while preserving accuracy. Models were packaged
    with their preprocessing routines and deployed using over-the-air (OTA)[46](#fn46)
    update mechanisms, ensuring consistency across devices in the field.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 为了促进可靠和可扩展的部署，团队开发了一个模块化工具链，用于将训练好的模型转换为适合嵌入式执行的优化格式。这个过程包括量化、剪枝等模型压缩技术，在保持准确性的同时减小了模型大小。模型与其预处理程序一起打包，并使用空中（OTA）[46](#fn46)更新机制进行部署，确保现场设备的一致性。
- en: Instrumentation was built into the deployment pipeline to support post-deployment
    observability.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表系统被集成到部署管道中，以支持部署后的可观察性。
- en: 'This stage illustrates key practices of MLOps in embedded systems: resource-aware
    model packaging, OTA deployment infrastructure, and continuous performance monitoring.
    It reinforces the importance of designing systems for adaptability and iteration,
    ensuring that ML models remain accurate and reliable under real-world operating
    conditions.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 此阶段展示了嵌入式系统中MLOps的关键实践：资源感知模型打包、OTA部署基础设施和持续性能监控。这强调了为适应性和迭代设计系统的重要性，确保机器学习模型在现实世界的操作条件下保持准确性和可靠性。
- en: Key Operational Insights
  id: totrans-540
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键操作见解
- en: The Oura Ring case study demonstrates how the operational challenges identified
    earlier manifest in edge environments and how systematic engineering practices
    address them. The team’s success in building modular tiered architectures with
    clear interfaces between components avoided the “pipeline jungle” problem while
    enabling runtime tradeoffs between accuracy and efficiency through standardized
    deployment patterns. The transition from 62% to clinical-grade accuracy required
    systematic configuration management across data collection protocols, model architectures,
    and deployment targets, with structured versioning that enabled reproducible experiments
    and prevented the fragmented settings that often plague embedded ML systems. The
    large-scale sleep study with PSG ground truth established stable, validated data
    foundations, and by investing in high-quality labeling and standardized collection
    protocols, the team avoided the unstable dependencies that frequently undermine
    wearable device accuracy. Success emerged from coordinated collaboration across
    data engineers, ML researchers, embedded systems developers, and operations personnel,
    reflecting the organizational maturity required to manage complex ML systems beyond
    individual technical components.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: Oura Ring案例研究展示了之前识别出的操作挑战如何在边缘环境中体现，以及系统化工程实践如何解决这些问题。团队在构建具有组件之间清晰接口的模块化分层架构方面的成功，避免了“管道丛林”问题，并通过标准化部署模式实现了运行时在准确性和效率之间的权衡。从62%到临床级准确性的转变，需要跨数据收集协议、模型架构和部署目标进行系统配置管理，以及结构化版本控制，这使可重复实验成为可能并防止了经常困扰嵌入式ML系统的碎片化设置。通过大规模睡眠研究和PSG地面真相，建立了稳定、验证的数据基础，通过投资于高质量的标注和标准化收集协议，团队避免了经常破坏可穿戴设备准确性的不稳定依赖。成功来自于数据工程师、ML研究人员、嵌入式系统开发人员和运营人员之间的协调合作，这反映了管理复杂ML系统所需的组织成熟度，而不仅仅是单个技术组件。
- en: This case exemplifies how MLOps principles adapt to domain-specific constraints
    while maintaining core engineering rigor. However, when machine learning systems
    move beyond consumer devices into clinical applications, even greater operational
    complexity emerges, requiring frameworks that address not just technical challenges
    but regulatory compliance, patient safety, and clinical decision-making processes.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 此案例说明了MLOps原则如何适应特定领域的约束，同时保持核心工程严谨性。然而，当机器学习系统从消费设备扩展到临床应用时，会涌现出更大的操作复杂性，需要解决不仅包括技术挑战，还包括法规遵从性、患者安全和临床决策过程等框架。
- en: ClinAIOps Case Study
  id: totrans-543
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ClinAIOps案例研究
- en: Building on the Oura Ring’s demonstration of embedded MLOps, the deployment
    of machine learning systems in healthcare presents both a significant opportunity
    and a unique challenge that extends beyond resource constraints. While traditional
    MLOps frameworks offer structured practices for managing model development, deployment,
    and monitoring, they often fall short in domains that require extensive human
    oversight, domain-specific evaluation, and ethical governance. Medical health
    monitoring, especially through continuous therapeutic monitoring (CTM)[47](#fn47),
    is one such domain where MLOps must evolve to meet the demands of real-world clinical
    integration.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在Oura Ring嵌入式MLOps演示的基础上，医疗保健中机器学习系统的部署既是一个重大的机会，也是一个独特的挑战，它超越了资源限制。虽然传统的MLOps框架提供了管理模型开发、部署和监控的结构化实践，但它们在需要广泛人类监督、特定领域评估和伦理治理的领域往往不足。医疗健康监测，特别是通过连续治疗监测（CTM）[47](#fn47)，就是这样一种领域，MLOps必须演变以满足现实世界临床整合的需求。
- en: CTM leverages wearable sensors and devices to collect rich streams of physiological
    and behavioral data from patients in real time.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: CTM利用可穿戴传感器和设备实时收集来自患者的丰富生理和行为数据。
- en: However, the mere deployment of ML models is insufficient to realize these benefits.
    AI systems must be integrated into clinical workflows, aligned with regulatory
    requirements, and designed to augment rather than replace human decision-making.
    The traditional MLOps paradigm, which focuses on automating pipelines for model
    development and serving, does not adequately account for the complex sociotechnical
    landscape of healthcare, where patient safety, clinician judgment, and ethical
    constraints must be prioritized. The privacy and security considerations inherent
    in healthcare AI, including data protection, regulatory compliance, and secure
    computation, are examined in depth in [Chapter 15](ch021.xhtml#sec-security-privacy).
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅部署ML模型是不够实现这些好处的。AI系统必须集成到临床工作流程中，符合监管要求，并设计为增强而不是取代人类决策。传统的MLOps范式，它侧重于自动化模型开发和服务的管道，没有充分考虑到医疗保健的复杂社会技术景观，在那里患者安全、临床医生判断和伦理约束必须优先考虑。医疗保健AI固有的隐私和安全考虑，包括数据保护、合规性和安全计算，在[第15章](ch021.xhtml#sec-security-privacy)中进行了深入探讨。
- en: This case study explores ClinAIOps, a framework proposed for operationalizing
    AI in clinical environments ([E. Chen et al. 2023](ch058.xhtml#ref-chen2023framework)).
    Where the Oura Ring case demonstrated how MLOps principles adapt to resource constraints,
    ClinAIOps shows how they must evolve to address regulatory and human-centered
    requirements. Unlike conventional MLOps, ClinAIOps directly addresses the **feedback
    loop** challenges identified earlier by designing them into the system architecture
    rather than treating them as technical debt. The framework’s structured coordination
    between patients, clinicians, and AI systems represents a practical implementation
    of the **governance and collaboration** components discussed in the production
    operations section. ClinAIOps also exemplifies how **operational maturity** evolves
    in specialized domains—requiring not just technical sophistication but domain-specific
    adaptations that maintain the core MLOps principles while addressing regulatory
    and ethical constraints.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究探讨了ClinAIOps，这是一个为在临床环境中实施AI而提出的框架([E. Chen et al. 2023](ch058.xhtml#ref-chen2023framework))。在Oura
    Ring案例展示了MLOps原则如何适应资源限制的情况下，ClinAIOps展示了它们必须如何演变以解决监管和以人为本的要求。与传统的MLOps不同，ClinAIOps直接解决了之前识别出的**反馈循环**挑战，通过将它们设计到系统架构中而不是将其视为技术债务来处理。该框架在患者、临床医生和AI系统之间的结构化协调代表了生产操作部分中讨论的**治理和协作**组件的实际实施。ClinAIOps还展示了**运营成熟度**如何在专业领域演变——不仅需要技术复杂性，还需要特定领域的适应，以在解决监管和伦理约束的同时保持核心MLOps原则。
- en: 'To understand why ClinAIOps represents a necessary evolution from traditional
    MLOps, we must first examine where standard operational practices fall short in
    clinical environments:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么ClinAIOps代表了从传统MLOps向必要演变的步骤，我们必须首先检查标准操作实践在临床环境中的不足之处：
- en: MLOps focuses primarily on the model lifecycle (e.g., training, deployment,
    monitoring), whereas healthcare requires coordination among diverse human actors,
    such as patients, clinicians, and care teams.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLOps主要关注模型的生命周期（例如，训练、部署、监控），而医疗保健需要协调各种不同的人类参与者，如患者、临床医生和护理团队。
- en: Traditional MLOps emphasizes automation and system reliability, but clinical
    decision-making hinges on personalized care, interpretability, and shared accountability.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统的MLOps强调自动化和系统可靠性，但临床决策取决于个性化护理、可解释性和共同问责制。
- en: The ethical, regulatory, and safety implications of AI-driven healthcare demand
    governance frameworks that go beyond technical monitoring.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI驱动医疗保健的伦理、监管和安全影响需要超越技术监控的治理框架。
- en: Clinical validation requires not just performance metrics but evidence of safety,
    efficacy, and alignment with care standards.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 临床验证不仅需要性能指标，还需要安全、有效性和与护理标准一致性的证据。
- en: Health data is highly sensitive, and systems must comply with strict privacy
    and security regulations, considerations that traditional MLOps frameworks do
    not fully address.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 健康数据极其敏感，系统必须遵守严格的隐私和安全法规，这是传统MLOps框架未能完全解决的问题。
- en: 'In light of these gaps, ClinAIOps presents an alternative: a framework for
    embedding ML into healthcare in a way that balances technical rigor with clinical
    utility, operational reliability with ethical responsibility. The remainder of
    this case study introduces the ClinAIOps framework and its feedback loops, followed
    by a detailed walkthrough of a hypertension management example that illustrates
    how AI can be effectively integrated into routine clinical practice.'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些差距，ClinAIOps提出了一种替代方案：一个将机器学习嵌入医疗保健的框架，该框架在技术严谨性与临床实用性、操作可靠性与道德责任之间取得平衡。本案例研究的其余部分介绍了ClinAIOps框架及其反馈循环，随后详细介绍了高血压管理示例，说明了AI如何有效地融入常规临床实践。
- en: Feedback Loops
  id: totrans-555
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 反馈循环
- en: At the core of the ClinAIOps framework are three interlocking feedback loops
    that enable the safe, effective, and adaptive integration of machine learning
    into clinical practice. As illustrated in [Figure 13.11](ch019.xhtml#fig-clinaiops),
    these loops are designed to coordinate inputs from patients, clinicians, and AI
    systems, facilitating data-driven decision-making while preserving human accountability
    and clinical oversight.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: ClinAIOps框架的核心是三个相互锁定的反馈循环，这些循环使机器学习能够安全、有效和自适应地融入临床实践。如图13.11所示，这些循环旨在协调患者、临床医生和AI系统的输入，促进数据驱动决策，同时保持人类问责制和临床监督。
- en: '![](../media/file217.svg)'
  id: totrans-557
  prefs: []
  type: TYPE_IMG
  zh: '![ ClinAIOps反馈循环](../media/file217.svg)'
- en: 'Figure 13.11: **ClinAIOps Feedback Loops**: The cyclical framework coordinates
    data flow between patients, clinicians, and AI systems to support continuous model
    improvement and safe clinical integration. These interconnected loops enable iterative
    refinement of AI models based on real-world performance and clinical feedback,
    fostering trust and accountability in healthcare applications. Source: ([E. Chen
    et al. 2023](ch058.xhtml#ref-chen2023framework)).'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11：**ClinAIOps反馈循环**：循环框架协调患者、临床医生和AI系统之间的数据流动，以支持持续模型改进和安全的临床整合。这些相互关联的循环使AI模型能够根据实际表现和临床反馈进行迭代优化，促进医疗保健应用中的信任和问责制。来源：([E.
    Chen等人 2023](ch058.xhtml#ref-chen2023framework))。
- en: 'In this model, the patient is central: contributing real-world physiological
    data, reporting outcomes, and serving as the primary beneficiary of optimized
    care. The clinician interprets this data in context, provides clinical judgment,
    and oversees treatment adjustments. Meanwhile, the AI system continuously analyzes
    incoming signals, surfaces actionable insights, and learns from feedback to improve
    its recommendations.'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，患者是核心：贡献现实世界的生理数据，报告结果，并作为优化护理的主要受益者。临床医生在特定背景下解释这些数据，提供临床判断，并监督治疗调整。同时，AI系统持续分析传入的信号，呈现可操作的见解，并从反馈中学习以改进其建议。
- en: 'Each feedback loop plays a distinct yet interconnected role:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 每个反馈循环都扮演着独特但相互关联的角色：
- en: The patient-AI loop captures and interprets real-time physiological data, generating
    tailored treatment suggestions.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者与AI循环捕捉并解释实时生理数据，生成定制化的治疗方案。
- en: The Clinician-AI loop ensures that AI-generated recommendations are reviewed,
    vetted, and refined under professional supervision.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 临床医生与AI循环确保AI生成的建议在专业监督下得到审查、验证和改进。
- en: The Patient-Clinician loop supports shared decision-making, empowering patients
    and clinicians to collaboratively set goals and interpret data trends.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者与临床医生循环支持共同决策，赋予患者和临床医生共同设定目标和解释数据趋势的权力。
- en: Together, these loops enable adaptive personalization of care. They help calibrate
    AI system behavior to the evolving needs of each patient, maintain clinician control
    over treatment decisions, and promote continuous model improvement based on real-world
    feedback. By embedding AI within these structured interactions, instead of isolating
    it as a standalone tool, ClinAIOps provides a blueprint for responsible and effective
    AI integration into clinical workflows.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 这些循环共同实现了护理的适应性个性化。它们帮助调整人工智能系统的行为以适应每个患者的不断变化的需求，保持临床医生对治疗决策的控制，并基于现实世界反馈促进模型的持续改进。通过将这些结构化交互嵌入人工智能中，而不是将其作为独立的工具，ClinAIOps为负责任和有效地将人工智能集成到临床工作流程中提供了一个蓝图。
- en: Patient-AI Loop
  id: totrans-565
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 患者-人工智能循环
- en: The patient–AI loop enables personalized and timely therapy optimization by
    leveraging continuous physiological data collected through wearable devices. Patients
    are equipped with sensors such as smartwatches, skin patches, or specialized biosensors
    that passively capture health-related signals in real-world conditions. For instance,
    a patient managing diabetes may wear a continuous glucose monitor, while individuals
    with cardiovascular conditions may use ECG-enabled wearables to track cardiac
    rhythms.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 患者与人工智能的循环通过利用通过可穿戴设备收集的连续生理数据来实现个性化的及时治疗方案优化。患者配备了如智能手表、皮肤贴片或专用生物传感器等传感器，在现实世界条件下被动地捕获与健康相关的信号。例如，管理糖尿病的患者可能佩戴连续葡萄糖监测器，而心血管疾病患者可能使用带有ECG功能的可穿戴设备来跟踪心脏节律。
- en: The AI system continuously analyzes these data streams in conjunction with relevant
    clinical context drawn from the patient’s electronic medical records, including
    diagnoses, lab values, prescribed medications, and demographic information. Using
    this holistic view, the AI model generates individualized recommendations for
    treatment adjustments, such as modifying dosage levels, altering administration
    timing, or flagging anomalous trends for review.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统持续分析这些数据流，并结合从患者的电子病历中提取的相关临床背景，包括诊断、实验室值、处方药物和人口统计信息。利用这种全面的视角，人工智能模型生成针对治疗调整的个性化建议，例如调整剂量水平、改变给药时间或标记异常趋势以供审查。
- en: To ensure both responsiveness and safety, treatment suggestions are tiered.
    Minor adjustments that fall within clinician-defined safety thresholds may be
    acted upon directly by the patient, empowering self-management while reducing
    clinical burden. More significant changes require review and approval by a healthcare
    provider. This structure maintains human oversight while enabling high-frequency,
    data-driven adaptation of therapies.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保响应性和安全性，治疗方案被分级。在临床医生定义的安全阈值范围内的微小调整可以直接由患者执行，从而赋予患者自我管理的能力，同时减少临床负担。更重大的变化需要医疗保健提供者的审查和批准。这种结构保持了人工监督，同时使治疗的高频、数据驱动适应成为可能。
- en: By enabling real-time, tailored interventions, including automatic insulin dosing
    adjustments based on glucose trends, this loop exemplifies how machine learning
    can close the feedback gap between sensing and treatment, allowing for dynamic,
    context-aware care outside of traditional clinical settings.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实现实时、定制的干预措施，包括基于葡萄糖趋势的自动胰岛素剂量调整，这个循环展示了机器学习如何缩小感知和治疗之间的反馈差距，允许在传统临床环境之外进行动态、上下文感知的护理。
- en: Clinician-AI Loop
  id: totrans-570
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 医生-人工智能循环
- en: The clinician–AI loop introduces a critical layer of human oversight into the
    process of AI-assisted therapeutic decision-making. In this loop, the AI system
    generates treatment recommendations and presents them to the clinician along with
    concise, interpretable summaries of the underlying patient data. These summaries
    may include longitudinal trends, sensor-derived metrics, and contextual factors
    extracted from the electronic health record.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 医生-人工智能循环将关键的人工监督层引入了人工智能辅助治疗决策的过程。在这个循环中，人工智能系统生成治疗方案，并将其与患者数据的简洁、可解释的摘要一起呈现给临床医生。这些摘要可能包括纵向趋势、传感器生成的指标以及从电子健康记录中提取的上下文因素。
- en: For example, an AI model might recommend a reduction in antihypertensive medication
    dosage for a patient whose blood pressure has remained consistently below target
    thresholds. The clinician reviews the recommendation in the context of the patient’s
    broader clinical profile and may choose to accept, reject, or modify the proposed
    change. This feedback, in turn, contributes to the continuous refinement of the
    model, improving its alignment with clinical practice.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个AI模型可能会建议减少一个血压持续低于目标阈值的患者的抗高血压药物剂量。医生会在患者的更广泛的临床背景中审查这一建议，并可选择接受、拒绝或修改所提出的变更。这种反馈反过来又有助于模型的持续优化，提高其与临床实践的契合度。
- en: Crucially, clinicians also define the operational boundaries within which the
    AI system can autonomously issue recommendations. These constraints ensure that
    only low-risk adjustments are automated, while more significant decisions require
    human approval. This preserves clinical accountability, supports patient safety,
    and enhances trust in AI-supported workflows.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，医生还定义了AI系统可以自主发布建议的操作边界。这些限制确保只有低风险的调整被自动化，而更重大的决策需要人类批准。这保留了临床问责制，支持患者安全，并增强了基于人工智能的工作流程的信任。
- en: The clinician–AI loop exemplifies a hybrid model of care in which AI augments
    rather than replaces human expertise. By enabling efficient review and oversight
    of algorithmic outputs, it facilitates the integration of machine intelligence
    into clinical practice while preserving the role of the clinician as the final
    decision-maker.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 医生与人工智能之间的循环体现了一种混合护理模式，其中人工智能补充而非取代人类专业知识。通过使算法输出的有效审查和监督成为可能，它促进了机器智能融入临床实践，同时保留了医生作为最终决策者的角色。
- en: Patient-Clinician Loop
  id: totrans-575
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 患者与医生之间的循环
- en: 'The patient–clinician loop enhances the quality of clinical interactions by
    shifting the focus from routine data collection to higher-level interpretation
    and shared decision-making. With AI systems handling data aggregation and basic
    trend analysis, clinicians are freed to engage more meaningfully with patients:
    reviewing patterns, contextualizing insights, and setting personalized health
    goals.'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 患者与医生之间的循环通过将重点从常规数据收集转移到更高层次的解释和共同决策，从而提高了临床互动的质量。随着人工智能系统处理数据聚合和基本趋势分析，医生得以更深入地与患者互动：审查模式、情境化洞察，并设定个性化的健康目标。
- en: For example, in managing diabetes, a clinician may use AI-summarized data to
    guide a discussion on dietary habits and physical activity, tailoring recommendations
    to the patient’s specific glycemic trends. Rather than adhering to fixed follow-up
    intervals, visit frequency can be adjusted dynamically based on patient progress
    and stability, ensuring that care delivery remains responsive and efficient.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在管理糖尿病时，医生可能会使用AI总结的数据来引导关于饮食习惯和身体活动的讨论，根据患者的特定血糖趋势定制建议。而不是遵循固定的随访间隔，可以根据患者的进展和稳定性动态调整访问频率，确保护理提供保持响应和高效。
- en: This feedback loop positions the clinician not merely as a prescriber but as
    a coach and advisor, interpreting data through the lens of patient preferences,
    lifestyle, and clinical judgment. It reinforces the therapeutic alliance by fostering
    collaboration and mutual understanding, key elements in personalized and patient-centered
    care.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 这个反馈循环将医生定位为不仅仅是处方者，而是教练和顾问，通过患者的偏好、生活方式和临床判断来解读数据。它通过促进合作和相互理解来加强治疗联盟，这些是个性化以患者为中心护理的关键要素。
- en: Hypertension Case Example
  id: totrans-579
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高血压案例示例
- en: To concretize the principles of ClinAIOps, consider the management of hyper­ten­sion,
    a condition affecting nearly half of adults in the United States (48.1%, or approximately
    119.9 million individuals, according to the Centers for Disease Control and Prevention).
    Effective hypertension control often requires individualized, ongoing adjustments
    to therapy, making it an ideal candidate for continuous therapeutic monitoring.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 为了具体说明ClinAIOps的原则，可以考虑高血压的管理，这是一种影响美国近一半成年人（48.1%，或约1.199亿人，根据疾病控制与预防中心的数据）的疾病。有效的血压控制通常需要个性化的、持续的疗法调整，使其成为持续治疗监测的理想候选者。
- en: ClinAIOps offers a structured framework for managing hypertension by integrating
    wearable sensing technologies, AI-driven recommendations, and clinician oversight
    into a cohesive feedback system. In this context, wearable devices equipped with
    photoplethysmography (PPG) and electrocardiography (ECG) sensors passively capture
    cardiovascular data, which can be analyzed in near-real-time to inform treatment
    adjustments. These inputs are augmented by behavioral data (e.g., physical activity)
    and medication adherence logs, forming the basis for an adaptive and responsive
    treatment regimen.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: ClinAIOps 通过将可穿戴传感技术、AI 驱动的建议和临床医生监督整合到一个统一的反馈系统中，为管理高血压提供了一个结构化的框架。在这种情况下，配备光电容积描记法（PPG）和心电图（ECG）传感器的可穿戴设备被动地捕获心血管数据，这些数据可以近乎实时地分析，以指导治疗调整。这些输入通过行为数据（例如，身体活动）和药物依从性日志得到增强，形成了一个自适应和响应的治疗方案的基础。
- en: The following subsections detail how the patient–AI, clinician–AI, and patient–clinician
    loops apply in this setting, illustrating the practical implementation of ClinAIOps
    for a widespread and clinically significant condition.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的子节详细说明了患者-AI、临床医生-AI 和患者-临床医生循环如何应用于此设置，展示了 ClinAIOps 在广泛且具有临床意义的条件下的实际应用。
- en: Data Collection
  id: totrans-583
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据收集
- en: In a ClinAIOps-based hypertension management system, data collection is centered
    on continuous, multimodal physiological monitoring. Wrist-worn devices equipped
    with photoplethysmography (PPG)[48](#fn48) and electrocardiography (ECG) sensors
    provide noninvasive estimates of blood pressure ([Q. Zhang, Zhou, and Zeng 2017](ch058.xhtml#ref-zhang2017highly)).
    These wearables also include accelerometers to capture physical activity patterns,
    enabling contextual interpretation of blood pressure fluctuations in relation
    to movement and exertion.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于 ClinAIOps 的血压管理系统中，数据收集集中在连续的多模态生理监测上。配备光电容积描记法（PPG）[48](#fn48) 和心电图（ECG）传感器的腕戴设备提供非侵入性的血压估计
    ([Q. Zhang, Zhou, and Zeng 2017](ch058.xhtml#ref-zhang2017highly))。这些可穿戴设备还包括加速度计来捕捉身体活动模式，从而能够根据运动和努力来解释血压波动。
- en: Complementary data inputs include self-reported logs of antihypertensive medication
    intake, specifying dosage and timing, as well as demographic attributes and clinical
    history extracted from the patient’s electronic health record. Together, these
    heterogeneous data streams form a rich, temporally aligned dataset that captures
    both physiological states and behavioral factors influencing blood pressure regulation.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 补充的数据输入包括自我报告的抗高血压药物摄入日志，指定剂量和时间，以及从患者的电子健康记录中提取的人口统计属性和临床病史。这些异构数据流共同形成了一个丰富、时间对齐的数据集，捕捉了影响血压调节的生理状态和行为因素。
- en: By integrating real-world sensor data with longitudinal clinical information,
    this integrated data foundation enables the development of personalized, context-aware
    models for adaptive hypertension management.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将现实世界的传感器数据与纵向临床信息相结合，这个综合数据基础使得开发个性化的、上下文感知的适应高血压管理模式成为可能。
- en: AI Model
  id: totrans-587
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: AI 模型
- en: The AI component in a ClinAIOps-driven hypertension management system is designed
    to operate directly on the device or in close proximity to the patient, enabling
    near real-time analysis and decision support. The model ingests continuous streams
    of blood pressure estimates, circadian rhythm indicators, physical activity levels,
    and medication adherence patterns to generate individualized therapeutic recommendations.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: ClinAIOps 驱动的血压管理系统中的 AI 组件旨在直接在设备上或在患者附近运行，以实现近乎实时的分析和决策支持。该模型摄取连续的血压估计、昼夜节律指标、身体活动水平和药物依从性模式，以生成个性化的治疗方案。
- en: Using machine learning techniques, the model infers optimal medication dosing
    and timing strategies to maintain target blood pressure levels. Minor dosage adjustments
    that fall within predefined safety thresholds can be communicated directly to
    the patient, while recommendations involving more substantial modifications are
    routed to the supervising clinician for review and approval.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习技术，该模型推断出最佳的药物剂量和时机策略，以维持目标血压水平。在预定义的安全阈值内的微小剂量调整可以直接传达给患者，而涉及更大修改的建议则被路由到监督临床医生进行审查和批准。
- en: The model supports continual refinement through a feedback mechanism that incorporates
    clinician decisions and patient outcomes. By integrating this observational data
    into subsequent training iterations, the system incrementally improves its predictive
    accuracy and clinical utility. The overarching objective is to enable fully personalized,
    adaptive blood pressure management that evolves in response to each patient’s
    physiological and behavioral profile.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型通过结合临床医生的决定和患者结果的反馈机制，支持持续的改进。通过将此类观察数据整合到后续的训练迭代中，系统逐步提高其预测准确性和临床效用。总体目标是实现完全个性化的、适应性强的血压管理，这种管理能够根据每个患者的生理和行为特征进行演变。
- en: Patient-AI Loop
  id: totrans-591
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 患者-人工智能循环
- en: The patient-AI loop facilitates timely, personalized medication adjustments
    by delivering AI-generated recommendations directly to the patient through a wearable
    device or associated mobile application. When the model identifies a minor dosage
    modification that falls within a pre-approved safety envelope, the patient may
    act on the suggestion independently, enabling a form of autonomous, yet bounded,
    therapeutic self-management.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 患者人工智能循环通过将人工智能生成的建议直接通过可穿戴设备或相关移动应用程序发送给患者，促进及时、个性化的药物调整。当模型识别出在预先批准的安全范围内的微小剂量调整时，患者可以独立采取建议，从而实现一种自主但受限的治疗自我管理形式。
- en: For recommendations involving significant changes to the prescribed regimen,
    the system defers to clinician oversight, ensuring medical accountability and
    compliance with regulatory standards. This loop empowers patients to engage actively
    in their care while maintaining a safeguard for clinical appropriateness.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 对于涉及对处方方案进行重大更改的建议，系统会推迟至临床医生监督，确保医疗责任和符合监管标准。这个循环使患者能够积极参与自己的护理，同时保持对临床适宜性的保护。
- en: By enabling personalized, data-driven feedback on a daily basis, the patient-AI
    loop supports improved adherence and therapeutic outcomes. It operationalizes
    a key principle of ClinAIOps, by closing the loop between continuous monitoring
    and adaptive intervention, while preserving the patient’s role as an active agent
    in the treatment process.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供每日个性化的数据驱动反馈，患者-人工智能循环支持提高依从性和治疗效果。它通过关闭连续监测和适应性干预之间的循环，同时保持患者在治疗过程中的主动作用，实现了
    ClinAIOps 的一个关键原则。
- en: Clinician-AI Loop
  id: totrans-595
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 临床医生-人工智能循环
- en: The clinician-AI loop ensures medical oversight by placing healthcare providers
    at the center of the decision-making process. Clinicians receive structured summaries
    of the patient’s longitudinal blood pressure patterns, visualizations of adherence
    behaviors, and relevant contextual data aggregated from wearable sensors and electronic
    health records. These insights support efficient and informed review of the AI
    system’s recommended medication adjustments.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 临床医生-人工智能循环通过将医疗保健提供者置于决策过程的核心，确保医疗监督。临床医生会收到患者纵向血压模式的结构化摘要、依从性行为的可视化以及从可穿戴传感器和电子健康记录中汇总的相关上下文数据。这些见解支持对人工智能系统推荐的药物调整进行高效和有信息量的审查。
- en: Before reaching the patient, the clinician evaluates each proposed dosage change,
    choosing to approve, modify, or reject the recommendation based on their professional
    judgment and understanding of the patient’s broader clinical profile. Clinicians
    define the operational boundaries within which the AI may act autonomously, specifying
    thresholds for dosage changes that can be enacted without direct review.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 在接触到患者之前，临床医生会评估每个建议的剂量变化，根据他们的专业判断和对患者更广泛临床状况的理解，选择批准、修改或拒绝该建议。
- en: When the system detects blood pressure trends indicative of clinical risk, including
    persistent hypotension or a hypertensive crisis, it generates alerts for immediate
    clinician intervention. These capabilities preserve the clinician’s authority
    over treatment while enhancing their ability to manage patient care proactively
    and at scale.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统检测到表明临床风险的血压趋势，包括持续的低血压或高血压危象时，它会生成警报，以便临床医生立即干预。这些功能保留了临床医生对治疗的权威，同时增强了他们主动和大规模管理患者护理的能力。
- en: This loop exemplifies the principles of accountability, safety, and human-in-the-loop
    governance, ensuring that AI functions as a supportive tool rather than an autonomous
    agent in therapeutic decision-making.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 这个循环展示了问责制、安全性和闭环治理的原则，确保人工智能在治疗决策中作为辅助工具而不是自主代理。
- en: Patient-Clinician Loop
  id: totrans-600
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 患者与医生循环
- en: As illustrated in [Figure 13.12](ch019.xhtml#fig-interactive-loop), the patient-clinician
    loop emphasizes collaboration, context, and continuity in care. Rather than devoting
    in-person visits to basic data collection or medication reconciliation, clinicians
    engage with patients to interpret high-level trends derived from continuous monitoring.
    These discussions focus on modifiable factors such as diet, physical activity,
    sleep quality, and stress management, enabling a more holistic approach to blood
    pressure control.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图13.12](ch019.xhtml#fig-interactive-loop)所示，患者-医生循环强调护理中的协作、情境和连续性。医生们不再将面对面访问用于基本数据收集或药物核对，而是与患者互动，解释从持续监测中得出的高级趋势。这些讨论集中在可修改的因素上，如饮食、身体活动、睡眠质量和压力管理，从而实现血压控制的更全面方法。
- en: '![](../media/file218.svg)'
  id: totrans-602
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file218.svg)'
- en: 'Figure 13.12: **Patient-Clinician Interaction**: Continuous monitoring data
    informs collaborative discussions between patients and clinicians, shifting focus
    from data collection to actionable insights for lifestyle modifications and improved
    health management. This loop prioritizes patient engagement and contextual understanding
    to facilitate personalized care beyond traditional clinical visits. Source: ([E.
    Chen et al. 2023](ch058.xhtml#ref-chen2023framework)).'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.12：**患者与医生互动**：持续监测数据为患者和医生之间的协作讨论提供信息，将重点从数据收集转移到生活方式改变和改善健康管理可操作见解。这个循环优先考虑患者参与和情境理解，以促进超越传统临床访问的个性化护理。来源：([E.
    Chen等人 2023](ch058.xhtml#ref-chen2023framework))。
- en: The dynamic nature of continuous data allows for flexible scheduling of appointments
    based on clinical need rather than fixed intervals. For example, patients exhibiting
    stable blood pressure trends may be seen less frequently, while those experiencing
    variability may receive more immediate follow-up. This adaptive cadence enhances
    resource efficiency while preserving care quality.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 持续数据的动态特性允许根据临床需求灵活安排预约，而不是固定的间隔。例如，血压趋势稳定的患者可能被看到较少，而那些经历变化的患者可能会得到更及时的跟进。这种适应性节奏提高了资源效率，同时保持了护理质量。
- en: By offloading routine monitoring and dose titration to AI-assisted systems,
    clinicians are better positioned to offer personalized counseling and targeted
    interventions. The result is a more meaningful patient-clinician relationship
    that supports shared decision-making and long-term wellness. This loop exemplifies
    how ClinAIOps frameworks can shift clinical interactions from transactional to
    transformational, supporting proactive care, patient empowerment, and improved
    health outcomes.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将常规监测和剂量调整卸载到人工智能辅助系统中，医生可以更好地提供个性化咨询和针对性干预。结果是更富有意义的患者-医生关系，支持共同决策和长期健康。这个循环展示了ClinAIOps框架如何将临床互动从交易性转变为变革性，支持主动护理、患者赋权和改善健康结果。
- en: MLOps vs ClinAIOps Comparison
  id: totrans-606
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MLOps与ClinAIOps比较
- en: The hypertension case study illustrates why traditional MLOps frameworks are
    often insufficient for high-stakes, real-world domains such as clinical healthcare.
    While conventional MLOps excels at managing the technical lifecycle of machine
    learning models, including training, deployment, and monitoring, it generally
    lacks the constructs necessary for coordinating human decision-making, managing
    clinical workflows, and safeguarding ethical accountability.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 高血压案例研究说明了为什么传统的MLOps框架通常不足以应对临床医疗等高风险、现实世界的领域。虽然传统的MLOps在管理机器学习模型的技术生命周期方面表现出色，包括训练、部署和监控，但它通常缺乏协调人类决策、管理临床工作流程和保障道德责任所需的构造。
- en: In contrast, the ClinAIOps framework extends beyond technical infrastructure
    to support complex sociotechnical systems. Rather than treating the model as the
    final decision-maker, ClinAIOps embeds machine learning into a broader context
    where clinicians, patients, and systems stakeholders collaboratively shape treatment
    decisions.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，ClinAIOps框架超越了技术基础设施，以支持复杂的社会技术系统。ClinAIOps不是将模型视为最终决策者，而是将机器学习嵌入到一个更广泛的环境中，在那里医生、患者和系统利益相关者共同塑造治疗决策。
- en: 'Several limitations of a traditional MLOps approach become apparent when applied
    to a clinical setting like hypertension management:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 将传统MLOps方法应用于高血压管理等临床环境时，其几个局限性变得明显：
- en: '**Data availability and feedback**: Traditional pipelines rely on pre-collected
    datasets. ClinAIOps enables ongoing data acquisition and iterative feedback from
    clinicians and patients.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可用性和反馈**：传统管道依赖于预先收集的数据集。ClinAIOps使持续数据获取和从临床医生和患者那里获得迭代反馈成为可能。'
- en: '**Trust and interpretability**: MLOps may lack transparency mechanisms for
    end users. ClinAIOps maintains clinician oversight, ensuring recommendations remain
    actionable and trustworthy.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信任与可解释性**：MLOps可能缺乏对最终用户透明的机制。ClinAIOps维持临床医生的监督，确保推荐保持可操作和可信。'
- en: '**Behavioral and motivational factors**: MLOps focuses on model outputs. ClinAIOps
    recognizes the need for patient coaching, adherence support, and personalized
    engagement.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行为和动机因素**：MLOps专注于模型输出。ClinAIOps认识到患者辅导、依从性支持和个性化参与的需求。'
- en: '**Safety and liability**: MLOps does not account for medical risk. ClinAIOps
    retains human accountability and provides structured boundaries for autonomous
    decisions.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全与责任**：MLOps没有考虑医疗风险。ClinAIOps保留人类责任并提供自主决策的结构化边界。'
- en: '**Workflow integration**: Traditional systems may exist in silos. ClinAIOps
    aligns incentives and communication across stakeholders to ensure clinical adoption.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作流程集成**：传统系统可能存在于孤岛中。ClinAIOps协调利益相关者的激励和沟通，以确保临床采用。'
- en: As shown in [Table 13.7](ch019.xhtml#tbl-clinical_ops), the key distinction
    lies in how ClinAIOps integrates technical systems with human oversight, ethical
    principles, and care delivery processes. Rather than replacing clinicians, the
    framework augments their capabilities while preserving their central role in therapeutic
    decision-making.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表13.7](ch019.xhtml#tbl-clinical_ops)所示，关键区别在于ClinAIOps如何将技术系统与人类监督、伦理原则和护理交付流程相结合。该框架并非取代临床医生，而是增强其能力，同时保持他们在治疗决策中的核心作用。
- en: 'Table 13.7: **Clinical AI Operations**: Traditional MLOps focuses on model
    performance, while ClinAIOps integrates technical systems with clinical workflows,
    ethical considerations, and ongoing feedback loops to ensure safe, trustworthy,
    and effective AI assistance in healthcare settings. This table emphasizes that
    ClinAIOps prioritizes human oversight and accountability alongside automation,
    addressing unique challenges in clinical decision-making that standard MLOps pipelines
    often overlook.'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 表13.7：**临床AI操作**：传统MLOps关注模型性能，而ClinAIOps将技术系统与临床工作流程、伦理考虑和持续反馈循环相结合，以确保在医疗环境中提供安全、可信和有效的AI辅助。此表强调ClinAIOps优先考虑人类监督和责任，同时实现自动化，解决标准MLOps管道通常忽视的临床决策的独特挑战。
- en: '|  | **Traditional MLOps** | **ClinAIOps** |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '|  | **传统MLOps** | **ClinAIOps** |'
- en: '| --- | --- | --- |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Focus** | ML model development and deployment | Coordinating human and
    AI decision-making |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| **重点** | 机器学习模型开发和部署 | 协调人类和AI决策 |'
- en: '| **Stakeholders** | Data scientists, IT engineers | Patients, clinicians,
    AI developers |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| **利益相关者** | 数据科学家、IT工程师 | 患者、临床医生、AI开发者 |'
- en: '| **Feedback loops** | Model retraining, monitoring | Patient-AI, clinician-AI,
    patient-clinician |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| **反馈循环** | 模型重新训练、监控 | 患者-AI、临床医生-AI、患者-临床医生 |'
- en: '| **Objective** | Operationalize ML deployments | Optimize patient health outcomes
    |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| **目标** | 实施ML部署 | 优化患者健康结果 |'
- en: '| **Processes** | Automated pipelines and infrastructure | Integrates clinical
    workflows and oversight |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| **流程** | 自动化管道和基础设施 | 整合临床工作流程和监督 |'
- en: '| **Data considerations** | Building training datasets | Privacy, ethics, protected
    health information |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| **数据考虑** | 构建训练数据集 | 隐私、伦理、受保护的健康信息 |'
- en: '| **Model validation** | Testing model performance metrics | Clinical evaluation
    of recommendations |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| **模型验证** | 测试模型性能指标 | 临床评估推荐 |'
- en: '| **Implementation** | Focuses on technical integration | Aligns incentives
    of human stakeholders |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| **实施** | 专注于技术集成 | 对齐人类利益相关者的激励 |'
- en: Successfully deploying AI in complex domains such as healthcare requires more
    than developing and operationalizing performant machine learning models. As demonstrated
    by the hypertension case, effective integration depends on aligning AI systems
    with clinical workflows, human expertise, and patient needs. Technical performance
    alone is insufficient; deployment must account for ethical oversight, stakeholder
    coordination, and continuous adaptation to dynamic clinical contexts.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 在复杂领域如医疗保健中成功部署人工智能，不仅需要开发和运营性能良好的机器学习模型。正如高血压案例所示，有效的集成取决于将人工智能系统与临床工作流程、人类专业知识和患者需求相一致。仅仅技术性能是不够的；部署必须考虑到道德监督、利益相关者协调和对动态临床环境的持续适应。
- en: The ClinAIOps framework specifically addresses the operational challenges identified
    earlier, demonstrating how they manifest in healthcare contexts. Rather than treating
    feedback loops as technical debt, ClinAIOps explicitly architects them as beneficial
    system features, with patient-AI, clinician-AI, and patient-clinician loops creating
    intentional feedback mechanisms that improve care quality while maintaining safety
    through human oversight. The structured interface between AI recommendations and
    clinical decision-making eliminates hidden dependencies, ensuring clinicians maintain
    explicit control over AI outputs and preventing the silent breakage that occurs
    when model updates unexpectedly affect downstream systems. Clear delineation of
    AI responsibilities for monitoring and recommendations versus human responsibilities
    for diagnosis and treatment decisions prevents the gradual erosion of system boundaries
    that undermines reliability in complex ML systems. The framework’s emphasis on
    regulatory compliance, ethical oversight, and clinical validation creates systematic
    approaches to configuration management that prevent the ad hoc practices accumulating
    governance debt in healthcare AI systems. By embedding AI within collaborative
    clinical ecosystems, ClinAIOps demonstrates how operational challenges can be
    transformed from liabilities into systematic design opportunities, reframing AI
    not as an isolated technical artifact but as a component of a broader sociotechnical
    system designed to advance health outcomes while maintaining the engineering rigor
    essential for production ML systems.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: ClinAIOps框架专门解决了之前识别出的操作挑战，展示了它们在医疗保健环境中的表现。ClinAIOps不是将反馈循环视为技术债务，而是明确地将它们构建为有益的系统功能，通过患者-人工智能、临床医生-人工智能和患者-临床医生循环创建有意的反馈机制，在保持通过人工监督的安全性的同时提高护理质量。人工智能建议与临床决策之间的结构化界面消除了隐藏的依赖关系，确保临床医生对人工智能输出保持明确控制，防止模型更新意外影响下游系统时发生的无声故障。对人工智能在监控和建议与人类在诊断和治疗决策中的责任进行明确划分，防止了系统边界逐渐侵蚀，这会损害复杂机器学习系统的可靠性。该框架对法规遵从性、道德监督和临床验证的重视，为配置管理创造了系统方法，防止了在医疗保健人工智能系统中积累治理债务的临时做法。通过将人工智能嵌入协作临床生态系统中，ClinAIOps展示了如何将操作挑战从负债转变为系统设计机会，重新定义人工智能不仅仅是一个孤立的技术工件，而是作为一个更广泛的社会技术系统的一部分，旨在提高健康结果，同时保持生产机器学习系统所必需的工程严谨性。
- en: Fallacies and Pitfalls
  id: totrans-629
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谬误和陷阱
- en: Machine learning operations introduces unique complexities that distinguish
    it from traditional software deployment, yet many teams underestimate these differences
    and attempt to apply conventional practices without adaptation. The probabilistic
    nature of ML systems, the central role of data quality, and the need for continuous
    model maintenance create operational challenges that require specialized approaches
    and tooling.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习操作引入了独特的复杂性，这使得它与传统的软件部署不同，然而许多团队低估了这些差异，并试图在不做适应的情况下应用传统实践。机器学习系统的概率性质、数据质量的核心作用以及持续模型维护的需求，创造了需要专门方法和工具的操作挑战。
- en: '**Fallacy:** *MLOps is just applying traditional DevOps practices to machine
    learning models.*'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *MLOps只是将传统的DevOps实践应用于机器学习模型。*'
- en: This misconception leads teams to apply conventional software deployment practices
    to ML systems without understanding their unique characteristics. Traditional
    software has deterministic behavior and clear input-output relationships, while
    ML systems exhibit probabilistic behavior, data dependencies, and model drift.
    Standard CI/CD pipelines fail to account for data validation, model performance
    monitoring, or retraining triggers that are essential for ML systems. Feature
    stores, model registries, and drift detection require specialized infrastructure
    not present in traditional DevOps. Effective MLOps requires dedicated practices
    designed for the stochastic and data-dependent nature of machine learning systems.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 这种误解导致团队在没有理解其独特特征的情况下，将传统的软件部署实践应用于机器学习系统。传统软件具有确定性行为和清晰的输入输出关系，而机器学习系统表现出概率性行为、数据依赖性和模型漂移。标准的CI/CD管道未能考虑到数据验证、模型性能监控或重新训练触发器，这些对于机器学习系统是必不可少的。特征存储、模型注册表和漂移检测需要传统DevOps中不存在的专业基础设施。有效的MLOps需要为机器学习系统的随机性和数据依赖性设计专门的实践。
- en: '**Pitfall:** *Treating model deployment as a one-time event rather than an
    ongoing process.*'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *将模型部署视为一次性事件，而不是持续的过程。*'
- en: Many teams view model deployment as the final step in the ML lifecycle, similar
    to shipping software releases. This approach ignores the reality that ML models
    degrade over time due to data drift, changing user behavior, and evolving business
    requirements. Production models require continuous monitoring, performance evaluation,
    and potential retraining or replacement. Without ongoing operational support,
    deployed models become unreliable and may produce increasingly poor results. Successful
    MLOps treats deployment as the beginning of a model’s operational lifecycle rather
    than its conclusion.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 许多团队将模型部署视为机器生命周期中的最后一步，类似于软件发布。这种方法忽略了现实情况，即由于数据漂移、用户行为变化和业务需求的发展，机器学习模型会随着时间的推移而退化。生产模型需要持续监控、性能评估以及可能的重新训练或替换。没有持续的操作支持，部署的模型会变得不可靠，并可能产生越来越差的结果。成功的MLOps将部署视为模型运营生命周期的开始，而不是其结束。
- en: '**Fallacy:** *Automated retraining ensures optimal model performance without
    human oversight.*'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *自动重新训练确保了模型性能的最优化，无需人工监督。*'
- en: This belief assumes that automated pipelines can handle all aspects of model
    maintenance without human intervention. While automation is essential for scalable
    MLOps, it cannot handle all scenarios that arise in production. Automated retraining
    might perpetuate biases present in new training data, fail to detect subtle quality
    issues, or trigger updates during inappropriate times. Complex failure modes,
    regulatory requirements, and business logic changes require human judgment and
    oversight. Effective MLOps balances automation with appropriate human checkpoints
    and intervention capabilities.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信念假设自动管道可以处理模型维护的所有方面，无需人工干预。虽然自动化对于可扩展的MLOps至关重要，但它无法处理生产中出现的所有场景。自动重新训练可能会延续新训练数据中存在的偏差，未能检测到微妙的质量问题，或在不当的时间触发更新。复杂的故障模式、监管要求和企业逻辑变化需要人类的判断和监督。有效的MLOps在自动化与适当的人工检查点和干预能力之间取得平衡。
- en: '**Pitfall:** *Focusing on technical infrastructure while neglecting organizational
    and process alignment.*'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *专注于技术基础设施，而忽视组织和流程的协调。*'
- en: Organizations often invest heavily in MLOps tooling and platforms without addressing
    the cultural and process changes required for successful implementation. MLOps
    requires close collaboration between data scientists, engineers, and business
    stakeholders with different backgrounds, priorities, and communication styles.
    Without clear roles, responsibilities, and communication protocols, sophisticated
    technical infrastructure fails to deliver operational benefits. Successful MLOps
    implementation requires organizational transformation that aligns incentives,
    establishes shared metrics, and creates collaborative workflows across functional
    boundaries.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 组织往往在解决成功实施所需的文化和流程变化之前，大量投资于MLOps工具和平台。MLOps需要数据科学家、工程师和不同背景、优先级和沟通风格的商业利益相关者之间的紧密合作。如果没有明确的角色、责任和沟通协议，复杂的技术基础设施就无法带来运营效益。成功的MLOps实施需要组织转型，以协调激励措施，建立共享指标，并在功能边界内创建协作工作流程。
- en: Summary
  id: totrans-639
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Machine learning operations provides the comprehensive framework that integrates
    the specialized capabilities explored throughout this book into cohesive production
    systems. The preceding chapters established critical operational requirements:
    [Chapter 14](ch020.xhtml#sec-ondevice-learning) demonstrated federated learning
    and edge adaptation under severe constraints, [Chapter 15](ch021.xhtml#sec-security-privacy)
    developed privacy-preserving techniques and secure model serving, and [Chapter 16](ch022.xhtml#sec-robust-ai)
    presented fault tolerance mechanisms for unpredictable environments. This chapter
    revealed how MLOps orchestrates these diverse capabilities through systematic
    engineering practices—data pipeline automation, model versioning, infrastructure
    orchestration, and continuous monitoring—that enable edge learning, security controls,
    and robustness mechanisms to function together reliably at scale. The evolution
    from isolated technical solutions to integrated operational frameworks reflects
    the maturity of ML systems engineering as a discipline capable of delivering sustained
    value in production environments.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习运营提供了一个综合框架，将本书中探索的专门能力整合成一致的生产系统。前几章确立了关键运营要求：[第14章](ch020.xhtml#sec-ondevice-learning)展示了在严格约束下的联邦学习和边缘适应，[第15章](ch021.xhtml#sec-security-privacy)开发了隐私保护技术和安全模型服务，[第16章](ch022.xhtml#sec-robust-ai)提出了不可预测环境的容错机制。本章揭示了MLOps如何通过系统化的工程实践——数据管道自动化、模型版本控制、基础设施编排和持续监控——来协调这些不同的能力，使边缘学习、安全控制和鲁棒性机制能够在规模上可靠地协同工作。从孤立的技术解决方案到集成运营框架的演变反映了机器学习系统工程作为一个学科的成熟，它能够在生产环境中持续创造价值。
- en: The operational challenges of machine learning systems span technical, organizational,
    and domain-specific dimensions that require sophisticated coordination across
    multiple stakeholders and system components. Data drift detection and model retraining
    pipelines must operate continuously to maintain system performance as real-world
    conditions change. Infrastructure automation enables reproducible deployments
    across diverse environments while version control systems track the complex relationships
    between code, data, and model artifacts. The monitoring frameworks discussed earlier
    must capture both traditional system metrics and ML-specific indicators like prediction
    confidence, feature distribution shifts, and model fairness metrics. The integration
    of these operational capabilities creates robust feedback loops that enable systems
    to adapt to changing conditions while maintaining reliability and performance
    guarantees.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统的运营挑战涵盖了技术、组织和特定领域维度，需要多个利益相关者和系统组件之间的复杂协调。数据漂移检测和模型重新训练管道必须持续运行，以保持系统性能，因为现实条件发生变化。基础设施自动化使得可以在各种环境中进行可重复的部署，同时版本控制系统跟踪代码、数据和模型工件之间的复杂关系。前面讨论的监控框架必须捕获传统系统指标和ML特定指标，如预测置信度、特征分布变化和模型公平性指标。这些运营能力的集成创建了强大的反馈循环，使系统能够适应变化条件，同时保持可靠性和性能保证。
- en: '**Key Takeaways**'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: MLOps provides the comprehensive framework integrating specialized capabilities
    from edge learning ([Chapter 14](ch020.xhtml#sec-ondevice-learning)), security
    ([Chapter 15](ch021.xhtml#sec-security-privacy)), and robustness ([Chapter 16](ch022.xhtml#sec-robust-ai))
    into cohesive production systems
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLOps提供了一个综合框架，将边缘学习（[第14章](ch020.xhtml#sec-ondevice-learning)）、安全（[第15章](ch021.xhtml#sec-security-privacy)）和鲁棒性（[第16章](ch022.xhtml#sec-robust-ai)）的专门能力整合成一致的生产系统
- en: Technical debt patterns like feedback loops and data dependencies require systematic
    engineering solutions through feature stores, versioning systems, and monitoring
    frameworks
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术债务模式，如反馈循环和数据依赖，需要通过特征存储、版本控制系统和监控框架的系统工程解决方案
- en: 'Infrastructure components directly address operational challenges: CI/CD pipelines
    prevent correction cascades, model registries enable controlled rollbacks, and
    orchestration tools manage distributed deployments'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施组件直接解决运营挑战：CI/CD管道防止纠正级联，模型注册表实现可控回滚，编排工具管理分布式部署
- en: Production operations must simultaneously handle federated edge updates, maintain
    privacy guarantees, and detect adversarial degradation through unified monitoring
    and governance
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产操作必须同时处理联邦边缘更新、维护隐私保证以及通过统一监控和管理检测对抗性退化
- en: Domain-specific frameworks like ClinAIOps transform operational challenges into
    design opportunities, showing how MLOps adapts to specialized requirements while
    maintaining engineering rigor
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专门领域的框架，如 ClinAIOps，将运营挑战转化为设计机会，展示了 MLOps 如何适应专业需求同时保持工程严谨性
- en: The MLOps framework presented in this chapter represents the culmination of
    the specialized capabilities developed throughout Part IV. The edge learning techniques
    from [Chapter 14](ch020.xhtml#sec-ondevice-learning) require MLOps adaptations
    for distributed model updates without centralized visibility. The security mechanisms
    from [Chapter 15](ch021.xhtml#sec-security-privacy) depend on MLOps infrastructure
    for secure model deployment and privacy-preserving training pipelines. The robustness
    strategies from [Chapter 16](ch022.xhtml#sec-robust-ai) rely on MLOps monitoring
    to detect distribution shifts and trigger appropriate mitigations. As machine
    learning systems mature from experimental prototypes to production services, MLOps
    provides the essential engineering discipline that enables these specialized capabilities
    to work together reliably. The operational excellence principles developed through
    MLOps practice ensure that AI systems remain trustworthy, maintainable, and effective
    in addressing real-world challenges at scale, transforming the promise of machine
    learning into sustained operational value.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中提出的 MLOps 框架代表了第四部分开发的专业能力的集大成。第 14 章（ch020.xhtml#sec-ondevice-learning）中提到的边缘学习技术需要
    MLOps 的适应以实现分布式模型更新而不需要集中可见性。第 15 章（ch021.xhtml#sec-security-privacy）中的安全机制依赖于
    MLOps 基础设施以实现安全的模型部署和隐私保护的训练流程。第 16 章（ch022.xhtml#sec-robust-ai）中的鲁棒性策略依赖于 MLOps
    监控来检测分布变化并触发适当的缓解措施。随着机器学习系统从实验原型成熟到生产服务，MLOps 提供了必要的工程学科，使这些专业能力能够可靠地协同工作。通过
    MLOps 实践开发出的运营卓越原则确保 AI 系统能够保持可信、可维护，并在规模上有效地解决现实世界的挑战，将机器学习的承诺转化为持续的运营价值。
- en: '* * *'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
