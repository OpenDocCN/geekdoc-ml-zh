["```py\nfrom sklearn.model_selection import train_test_split\ndf = df.sample(frac=1) # shuffle data\ndf_dev, df_test = train_test_split(df, test_size=0.15)\ndf_train, df_valid = train_test_split(df_dev, test_size=0.15)\n```", "```py\nfrom sklearn.model_selection import cross_val_score\nrf = RandomForestRegressor(...)\nscores = cross_val_score(rf, X, y, cv=5) # k=5\nprint(scores.mean())\n```", "```py\ndf = df.sort_values('saledate')\nn_valid = 12000  # same as Kaggle's test set size\nn_train = len(df)-n_valid\ndf_train = df[:n_train].reset_index(drop=True)\ndf_valid = df[n_train:].reset_index(drop=True)\n```", "```py\ndef clean(df):\n    del df['MachineID'] # dataset has inconsistencies\n    del df['SalesID']   # unique sales ID so not generalizer\n\n    df['auctioneerID'] = df['auctioneerID'].astype(str)\n\n    df_normalize_strings(df)\n\n    extract_sizes(df, 'Tire_Size')\n    extract_sizes(df, 'Undercarriage_Pad_Width')\n\n    df.loc[df['YearMade']<1950, 'YearMade'] = np.nan\n    df.loc[df.eval(\"saledate.dt.year < YearMade\"), 'YearMade'] = \\\n        df['saledate'].dt.year\n\n    df.loc[df.eval(\"MachineHoursCurrentMeter==0\"),\n           'MachineHoursCurrentMeter'] = np.nan\n\n```", "```py\ndef df_order_product_size(df):\n    sizes = {np.nan:0, 'mini':1, 'compact':1, 'small':2, 'medium':3,\n             'large / medium':4, 'large':5}\n    df['ProductSize'] = df['ProductSize'].map(sizes).values\n\n```", "```py\ndef onehot(df, colname):\n    ascat = df[colname].astype('category').cat.as_ordered()\n    onehot = pd.get_dummies(df[colname], prefix=colname, dtype=bool)\n    del df[colname]\n    df = pd.concat([df, onehot], axis=1)\n    # return altered dataframe and column training categories\n    return df, ascat.cat.categories\n\n```", "```py\ndef split_fiProductClassDesc(df):\n    df_split = df.fiProductClassDesc.str.split(' - ',expand=True).values\n    df['fiProductClassDesc'] = df_split[:,0] \n    df['fiProductClassSpec'] = df_split[:,1] # temporary column\n    pattern = r'([0-9.\\+]*)(?: to ([0-9.\\+]*)|\\+) ([a-zA-Z ]*)'\n    spec = df['fiProductClassSpec']\n    df_split = spec.str.extract(pattern, expand=True).values\n    df['fiProductClassSpec_lower'] = pd.to_numeric(df_split[:,0])\n    df['fiProductClassSpec_upper'] = pd.to_numeric(df_split[:,1])\n    df['fiProductClassSpec_units'] = df_split[:,2]\n    del df['fiProductClassSpec'] # remove temporary column\n\n```", "```py\ndef feature_eng(X): # for later use\n    df_split_dates(X, 'saledate')\n    df_order_product_size(X)\n    split_fiProductClassDesc(X)\n\n    X, hf_cats = onehot(X, 'Hydraulics_Flow')\n    # normalize categories first then one-hot encode\n    X['Enclosure'] = X['Enclosure'].replace('erops w ac', 'erops ac')\n    X['Enclosure'] = X['Enclosure'].replace('no rops', np.nan)\n    X, enc_cats = onehot(X, 'Enclosure')\n    catencoders = {'Hydraulics_Flow':hf_cats,\n                   'Enclosure':enc_cats}\n\n    return X, catencoders\n\n```", "```py\ndef df_fix_missing_nums(df:pd.DataFrame) -> dict:\n    medians = {}  # column name to median\n    for colname in df.columns:\n        if is_numeric_dtype(df[colname]):\n            medians[colname] = df[colname].median(skipna=True)\n            fix_missing_num(df, colname)\n    return medians\n\n```", "```py\n{'ModelID': 4642.0,\n 'datasource': 136.0,\n 'YearMade': 2002.0,\n 'MachineHoursCurrentMeter': 3290.0,\n 'saledate': 1.2511584e+18,\n ...}\n```", "```py\ndef df_string_to_cat(df:pd.DataFrame) -> dict:\n    catencoders = {}\n    for colname in df.columns:\n        if is_string_dtype(df[colname]) or is_object_dtype(df[colname]):\n            df[colname] = df[colname].astype('category').cat.as_ordered()\n            catencoders[colname] = df[colname].cat.categories\n    return catencoders\n\n```", "```py\nIndex(['multi shank', 'single shank', 'yes'], dtype='object')\n```", "```py\ndef numericalize(X, catencoders):\n    medians = df_fix_missing_nums(X)            \n    e = df_string_to_cat(X)\n    catencoders.update(e)\n    df_cat_to_catcode(X)    \n    return medians\n\n```", "```py\ndf = pd.read_feather(\"data/bulldozer-train.feather\")\ndf = df.iloc[-100_000:] # same 100,000 records as before\nX, y = df.drop('SalePrice', axis=1), df['SalePrice']\n\n```", "```py\ny = np.log(y)\nclean(X)\nX, catencoders = feature_eng(X)\nmedians = numericalize(X, catencoders)\n\n```", "```py\nrf, r2_train = test(X, y, n_estimators=150)\n\n```", "```py\ndef df_fix_missing_test_nums(df_test, medians):\n    for colname in medians:\n        df_test[colname+'_na'] = pd.isnull(df_test[colname])\n        df_test[colname].fillna(medians[colname], inplace=True)\n\n```", "```py\ndef df_apply_cats(df_test:pd.DataFrame, catencoders:dict):\n    for colname,encoder in catencoders.items():\n        # encode with categories from training set\n        df_test[colname] = \\\n            pd.Categorical(df_test[colname],\n                           categories=encoder, ordered=True)\n\n```", "```py\ndef onehot_apply_cats(df_test, colname, catencoders):\n    df_test[colname] = \\\n        pd.Categorical(df_test[colname],\n                       categories=catencoders[colname],\n                       ordered=True)\n    onehot = pd.get_dummies(df_test[colname], prefix=colname, dtype=bool)\n    del df_test[colname]\n    df_test = pd.concat([df_test, onehot], axis=1)\n    del catencoders[colname] # simplify df_apply_cats()\n    return df_test\n\n```", "```py\ndef feature_eng_test(df_test, catencoders):\n    df_split_dates(df_test, 'saledate')\n    df_order_product_size(df_test)\n    split_fiProductClassDesc(df_test)\n\n    df_test = onehot_apply_cats(df_test, 'Hydraulics_Flow', catencoders)\n    df_test['Enclosure'] = df_test['Enclosure'].replace('erops w ac', 'erops ac')\n    df_test['Enclosure'] = df_test['Enclosure'].replace('no rops', np.nan)\n    df_test = onehot_apply_cats(df_test, 'Enclosure', catencoders)\n\n    return df_test\n\n```", "```py\ndef numericalize_test(df_test:pd.DataFrame, medians:dict, catencoders:dict):\n    df_apply_cats(df_test, catencoders)\n    df_fix_missing_test_nums(df_test, medians)\n    df_cat_to_catcode(df_test)\n\n```", "```py\ndef sanity_check(df):\n    for col in df.columns:\n        if is_string_dtype(df[col]) or is_object_dtype(df[col]):\n            print(f\"Col {col} is still a string\")\n        if df[col].isnull().any():\n            print(f\"Col {col} still has missing values\")\n\ndef check_types(df1,df2):\n    if df1.shape[1] != df2.shape[1]:\n        print(f\"Num columns differs: {df1.shape[1]} != {df2.shape[1]}\")\n    cols1 = set(df1.columns)\n    cols2 = set(df2.columns)\n    if cols1 != cols2:\n        print(f\"Column names differ:\")\n        if len(cols1-cols2)>0:\n            print(f\"\\tIn df1 not df2: {cols1-cols2}\")\n        if len(cols2-cols1)>0:\n            print(f\"\\tIn df2 not df1: {cols2-cols1}\")\n    for col in cols1.intersection(cols2): # check those in common\n        if df1[col].dtype != df2[col].dtype:\n            print(f\"Col {col} dtypes differ {df1[col].dtype} != {df2[col].dtype}\")\n```", "```py\nsanity_check(X)\nsanity_check(X_valid)\ncheck_types(X, X_valid)\n```", "```py\ndf_valid = pd.read_feather(\"data/bulldozer-valid.feather\")\nX_valid, y_valid = df_valid.drop('SalePrice', axis=1), df_valid['SalePrice']\n\ny_valid = np.log(y_valid)\nclean(X_valid)\nX_valid = feature_eng_test(X_valid, catencoders)\nnumericalize_test(X_valid, medians, catencoders)\n\n```", "```py\ndef MAE(y_pred, y_true):\n    return np.mean(np.abs(y_pred - y_true))\n```", "```py\ndef MSE(y_pred, y_true):\n    return np.mean((y_pred - y_true)**2)\n```", "```py\ndef RMSE(y_pred, y_true):\n    return np.sqrt(MSE(y_pred, y_true))\n```", "```py\nX_valid = X_valid.reindex(columns=X.columns)\n\n```", "```py\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\ny_pred = rf.predict(X_valid)\n# Use np.exp(y_valid) to get back into dollars space\nmae_valid_baseline = mean_absolute_error(np.exp(y_valid), np.exp(y_pred))\nrmsle_valid_baseline = np.sqrt( mean_squared_error(y_valid, y_pred) )\nr2_valid_baseline = rf.score(X_valid, y_valid)\nprint(f\"Validation R^2 {r2_valid_baseline:.5f}, \"+\n      f\"RMSLE {rmsle_valid_baseline:.5f}, \"+\n      f\"MAE ${mae_valid_baseline:.0f}\")\n\n```", "```py\ndf = pd.read_feather(\"data/bulldozer-train.feather\")\ndf = df.query('saledate.dt.year>=2007').copy()\nX, y = df.drop('SalePrice', axis=1), df['SalePrice']\n\n```", "```py\ny = np.log(y)\nclean(X)\nX, catencoders = feature_eng(X)\nmedians = numericalize(X, catencoders)\n\ndf_valid = pd.read_feather(\"data/bulldozer-valid.feather\")\nX_valid, y_valid = df_valid.drop('SalePrice', axis=1), df_valid['SalePrice']\ny_valid = np.log(y_valid)\nclean(X_valid)\nX_valid = feature_eng_test(X_valid, catencoders)\ndf_apply_cats(X_valid, catencoders)\ndf_fix_missing_test_nums(X_valid, medians)\ndf_cat_to_catcode(X_valid)\n\n```", "```py\ndef test_valid(X, y, X_valid, y_valid, n_estimators=200,\n               max_features='auto', min_samples_leaf=1):\n    X_valid = X_valid.reindex(columns=X.columns)\n    rf = RandomForestRegressor(n_estimators=n_estimators,\n                               n_jobs=-1,\n                               oob_score=True,\n                               max_features=max_features, \n                               min_samples_leaf=min_samples_leaf)\n    rf.fit(X, y)\n    n = rfnnodes(rf)\n    h = np.median(rfmaxdepths(rf))\n    y_pred = rf.predict(X_valid)\n    mae_valid = mean_absolute_error(np.exp(y_valid), np.exp(y_pred))\n    rmsle_valid = np.sqrt( mean_squared_error(y_valid, y_pred) )\n    r2_score_valid = rf.score(X_valid, y_valid)\n    print(f\"OOB R^2 {rf.oob_score_:.5f} using {n:,d} tree nodes {h} median tree height\")\n    print(f\"Validation R^2 {r2_score_valid:.5f}, RMSLE {rmsle_valid:.5f}, MAE ${mae_valid:.0f}\")\n    return rf, r2_score_valid, rmsle_valid, mae_valid\n\n```", "```py\nrf, r2_score_2007, rmsle_2007, mae_2007 = \\\n    test_valid(X, y, X_valid, y_valid)\n\n```", "```py\nntrees = 200\nminleaf = 1\nfor maxf in np.arange(.1,.6,.1):\n    print(f\"n_estimators={ntrees}, max_features={maxf:.1f}, min_samples_leaf={minleaf}\")\n    test_valid(X, y, X_valid, y_valid,\n               max_features=maxf, min_samples_leaf=minleaf)\n\n```", "```py\nmaxf = .3\nfor minleaf in range(2,7):\n   print(f\"n_estimators={ntrees}, max_features={maxf}, min_samples_leaf={minleaf}\")\n   test_valid(X, y, X_valid, y_valid,\n              max_features=maxf, min_samples_leaf=minleaf)\n\n```", "```py\nrf, r2_score_valid, rmsle_valid, mae_valid = \\\n    test_valid(X, y, X_valid, y_valid,\n               max_features=.3, min_samples_leaf=2)\n\n```", "```py\ndef select_features(X, y, X_valid, y_valid, drop=0.10):\n   min_rmsle = 99999\n   X_valid = X_valid.reindex(columns=X.columns)\n   rf, _, rmsle, _ = test_valid(X, y, X_valid, y_valid,\n                                max_features=.3, min_samples_leaf=2)\n   I = importances(rf, X_valid, y_valid)\n   features = list(I.index)\n   keep = best_features = features\n   n = int(.9/drop) # how many iterations? get to 90%\n   for i in range(1,n+1):\n       X2 = X[keep]\n       X_valid2 = X_valid[keep]\n       print(f\"\\nNum features = {len(keep)}\")\n       rf2, _, rmsle, _ = test_valid(X2, y, X_valid2, y_valid,\n                                     max_features=.3, min_samples_leaf=2)\n       if rmsle < min_rmsle:\n           min_rmsle = rmsle\n           best_features = keep\n       I2 = importances(rf2, X_valid2, y_valid) # recompute since collinear\n       features = list(I2.index)\n       keep = features[0:int(len(features)*(1-drop))]\n\n   return min_rmsle, best_features\n\n```", "```py\nmin_rmsle, best_features = \\\n    select_features(X, y, X_valid, y_valid, drop=0.10)\nprint(f\"{len(best_features)} features is best:\")\nprint(best_features)\n```", "```py\nOOB R^2 0.92191 using 0 tree nodes 0 median tree height\nValidation R^2 0.89617, RMSLE 0.23296, MAE $5682\n...\nNum features = 53\nOOB R^2 0.91312 using 0 tree nodes 0 median tree height\nValidation R^2 0.89148, RMSLE 0.23816, MAE $5996\n...\nNum features = 33\nOOB R^2 0.90824 using 0 tree nodes 0 median tree height\nValidation R^2 0.89824, RMSLE 0.23062, MAE $5649\n33 features is best:\n['YearMade', 'ProductSize', 'fiProductClassSpec_lower', ...]\n```", "```py\nX = X[best_features]\nX_valid = X_valid[best_features]\nrf, r2_score_bestf, rmsle_bestf, mae_bestf = \\\n    test_valid(X, y, X_valid, y_valid,\n               max_features=.3, min_samples_leaf=2)\n\n```", "```py\ny_valid_pred = rf.predict(X_valid)\nunderprediction = np.mean(y_valid-y_valid_pred)\ndollars = np.mean(np.exp(y_valid)-np.exp(y_valid_pred))\nprint(f\"Model underpredicts by ${dollars:.0f}, {underprediction:.5f} log(dollars)\")\n\n```", "```py\ny_valid_pred = rf.predict(X_valid) + underprediction\nmae_best = mean_absolute_error(np.exp(y_valid), np.exp(y_valid_pred))\nrmsle_best = np.sqrt( mean_squared_error(y_valid, y_valid_pred) )\nr2_score_best = r2_score(y_valid, y_valid_pred)\nprint(f\"Adjusted-model validation R^2 {r2_score_best:.5f}, RMSLE {rmsle_best:.5f}, MAE {mae_best:.0f}\")\n\n```", "```py\ndf = pd.read_feather(\"data/bulldozer-train-all.feather\")\ndf = df.query('saledate.dt.year>=2007').copy()\nX, y = df.drop('SalePrice', axis=1), df['SalePrice']\ny = np.log(y)\nclean(X)\nX, catencoders = feature_eng(X)\nmedians = numericalize(X, catencoders)\nX = X[best_features]\n\n```", "```py\ndf_test = pd.read_feather(\"data/bulldozer-test.feather\")\nX_test, y_test = df_test.drop('SalePrice', axis=1), df_test['SalePrice']\ny_test = np.log(y_test)\nclean(X_test)\nX_test = feature_eng_test(X_test, catencoders)\ndf_apply_cats(X_test, catencoders)\ndf_fix_missing_test_nums(X_test, medians)\ndf_cat_to_catcode(X_test)\nX_test = X_test[best_features]\n\n```", "```py\nrf, r2_score_test, rmsle_test, mae_test = \\\n    test_valid(X, y + underprediction,\n               X_test, y_test,\n               max_features=.3, min_samples_leaf=2)\n\n```"]