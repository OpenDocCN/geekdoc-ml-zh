<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Ridge Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Ridge Regression</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ridge_regression.html">https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ridge_regression.html</a></blockquote>

<p>Michael J. Pyrcz, Professor, The University of Texas at Austin</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
<p>Chapter of e-book â€œApplied Machine Learning in Python: a Hands-on Guide with Codeâ€.</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite this e-Book as:</p>
<p>Pyrcz, M.J., 2024, <em>Applied Machine Learning in Python: A Hands-on Guide with Code</em> [e-book]. Zenodo. doi:10.5281/zenodo.15169138 <a class="reference external" href="https://doi.org/10.5281/zenodo.15169138"><img alt="DOI" src="../Images/7e4ea662f44af1eae87e87ecbb962ff4.png" data-original-src="https://zenodo.org/badge/863274676.svg"/></a></p>
</div>
<p>The workflows in this book and more are available here:</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite the MachineLearningDemos GitHub Repository as:</p>
<p>Pyrcz, M.J., 2024, <em>MachineLearningDemos: Python Machine Learning Demonstration Workflows Repository</em> (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312. GitHub repository: <a class="github reference external" href="https://github.com/GeostatsGuy/MachineLearningDemos">GeostatsGuy/MachineLearningDemos</a> <a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.13835312"><img alt="DOI" src="../Images/4e3a59c17d684b06a170c4af84e0f631.png" data-original-src="https://zenodo.org/badge/862519860.svg"/></a></p>
</div>
<p>By Michael J. Pyrcz <br/>
Â© Copyright 2024.</p>
<p>This chapter is a tutorial for / demonstration of <strong>Ridge Regression</strong>.</p>
<p><strong>YouTube Lecture</strong>: check out my lectures on:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl">Introduction to Machine Learning</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/0fzbyhWiP84">Linear Regression</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/pMGO40yXZ5Y?si=ygJAheyX-v2BmSiR">Ridge Regression</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/cVFYhlCCI_8?si=NbwIDaZj30vxezn2">LASSO Regression</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/JmxGlrurQp0?si=vuF1TXDbZkyRC1j-">Norms</a></p></li>
</ul>
<p>These lectures are all part of my <a class="reference external" href="https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&amp;si=XonjO2wHdXffMpeI">Machine Learning Course</a> on YouTube with linked well-documented Python workflows and interactive dashboards. My goal is to share accessible, actionable, and repeatable educational content. If you want to know about my motivation, check out <a class="reference external" href="https://michaelpyrcz.com/my-story">Michaelâ€™s Story</a>.</p>
<section id="motivations-for-ridge-regression">
<h2>Motivations for Ridge Regression</h2>
<p>Hereâ€™s a simple workflow, demonstration of ridge regression and comparison to linear regression for machine learning-based predictions. Why start with linear regression?</p>
<ul class="simple">
<li><p>Linear regression is the simplest parametric predictive machine learning model</p></li>
<li><p>We learn about training machine learning models with an analytical solution calculated from the derivative of training MSE</p></li>
<li><p>Getâ€™s us started with the concepts of loss functions and norms</p></li>
<li><p>We have access to analytics expressions for confidence intervals for model uncertainty and hypothesis tests for parameter significance</p></li>
</ul>
<p>Why cover ridge regression next?</p>
<ul class="simple">
<li><p>Some times linear regression is not simple enough and we actually need a simpler model!</p></li>
<li><p>Introduce the concept of model regularization and hyperparameter tuning</p></li>
</ul>
<p>Hereâ€™s some basic details about predictive machine learning ridge regression models, letâ€™s start with linear regression first and build to ridge regression:</p>
</section>
<section id="linear-regression">
<h2>Linear Regression</h2>
<p>Linear regression for prediction, letâ€™s start by looking at a linear model fit to a set of data.</p>
<figure style="text-align: center;">
  <img src="../Images/d82dccdcda485413554e49d73e4d1fc8.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ridge/linear_example.png"/>
  <figcaption style="text-align: center;">Example linear regression model.</figcaption>
</figure>
<p>Letâ€™s start by defining some terms,</p>
<ul class="simple">
<li><p><strong>predictor feature</strong> - an input feature for the prediction model, given we are only discussing linear regression and not multilinear regression we have only one predictor feature, <span class="math notranslate nohighlight">\(x\)</span>. On out plots (including above) the predictor feature is on the x-axis.</p></li>
<li><p><strong>response feature</strong> - the output feature for the prediction model, in this case, <span class="math notranslate nohighlight">\(y\)</span>. On our plots (including above) the response feature is on the y-axis.</p></li>
</ul>
<p>Now, here are some key aspects of linear regression:</p>
<p><strong>Parametric Model</strong></p>
<p>This is a parametric predictive machine learning model, we accept an a prior assumption of linearity and then gain a very low parametric representation that is easy to train without a onerous amount of data.</p>
<ul class="simple">
<li><p>the fit model is a simple weighted linear additive model based on all the available features, <span class="math notranslate nohighlight">\(x_1,\ldots,x_m\)</span>.</p></li>
<li><p>the parametric model takes the form of:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0
\]</div>
<p>Hereâ€™s the visualization of the linear model parameters,</p>
<figure style="text-align: center;">
  <img src="../Images/175e41e10e74a46b4a56258ccdfb94c0.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ridge/linear_model.png"/>
  <figcaption style="text-align: center;">The linear model parameters.</figcaption>
</figure>
<p><strong>Least Squares</strong></p>
<p>The analytical solution for the model parameters, <span class="math notranslate nohighlight">\(b_1,\ldots,b_m,b_0\)</span>, is available for the L2 norm loss function, the errors are summed and squared known a least squares.</p>
<ul class="simple">
<li><p>we minimize the error, residual sum of squares (RSS) over the training data:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the actual response feature values and <span class="math notranslate nohighlight">\(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\)</span> are the model predictions, over the <span class="math notranslate nohighlight">\(\alpha = 1,\ldots,n\)</span> training data.</p>
<p>Hereâ€™s a visualization of the L2 norm loss function, MSE,</p>
<figure style="text-align: center;">
  <img src="../Images/b25cdf17fb10f18b362f50ba655df92b.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ridge/linear_MSE.png"/>
  <figcaption style="text-align: center;">The linear model loss function, mean square error.</figcaption>
</figure>
<ul class="simple">
<li><p>this may be simplified as the sum of square error over the training data,</p></li>
</ul>
<p>\begin{equation}
\sum_{i=1}^n (\Delta y_i)^2
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(\Delta y_i\)</span> is actual response feature observation <span class="math notranslate nohighlight">\(y_i\)</span> minus the model prediction <span class="math notranslate nohighlight">\(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\)</span>, over the <span class="math notranslate nohighlight">\(i = 1,\ldots,n\)</span> training data.</p>
<p><strong>Assumptions</strong></p>
<p>There are important assumption with our linear regression model,</p>
<ul class="simple">
<li><p><strong>Error-free</strong> - predictor variables are error free, not random variables</p></li>
<li><p><strong>Linearity</strong> - response is linear combination of feature(s)</p></li>
<li><p><strong>Constant Variance</strong> - error in response is constant over predictor(s) value</p></li>
<li><p><strong>Independence of Error</strong> - error in response are uncorrelated with each other</p></li>
<li><p><strong>No multicollinearity</strong> - none of the features are redundant with other features</p></li>
</ul>
</section>
<section id="id1">
<h2>Ridge Regression</h2>
<p>With ridge regression we add a hyperparameter, <span class="math notranslate nohighlight">\(\lambda\)</span>, to our minimization, with a shrinkage penalty term, <span class="math notranslate nohighlight">\(\sum_{j=1}^m b_{\alpha}^2\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m b_{\alpha}^2
\]</div>
<p>As a result, ridge regression training integrates two and often competing goals to find the model parameters,</p>
<ul class="simple">
<li><p>find the model parameters that minimize the error with training data</p></li>
<li><p>minimize the slope parameters towards zero</p></li>
</ul>
<p>Note: lambda does not include the intercept, <span class="math notranslate nohighlight">\(b_0\)</span>.</p>
<p>The <span class="math notranslate nohighlight">\(\lambda\)</span> is a hyperparameter that controls the degree of fit of the model and may be related to the model bias-variance trade-off.</p>
<figure style="text-align: center;">
  <img src="../Images/2a291289b4f4b55ba9adbb7538140a67.png" style="display: block; margin: 0 auto; width: 60%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ridge/ridge_tuning.png"/>
  <figcaption style="text-align: center;"> Testing error components and expected test square error for ridge regression.</figcaption>
</figure>
<ul class="simple">
<li><p>for <span class="math notranslate nohighlight">\(\lambda \rightarrow 0\)</span> the solution approaches linear regression, there is no bias (relative to a linear model fit), but the model variance is likely higher</p></li>
<li><p>as <span class="math notranslate nohighlight">\(\lambda\)</span> increases the model variance decreases and the model bias increases, the model becomes simpler</p></li>
<li><p>for <span class="math notranslate nohighlight">\(\lambda \rightarrow \infty\)</span> the model parameters <span class="math notranslate nohighlight">\(b_1,\ldots,b_m\)</span> shrink to 0.0 and the model predictions approaches the training data response feature mean</p></li>
</ul>
<p>Hereâ€™s a variety of ridge regression models for various <span class="math notranslate nohighlight">\(\lamda\)</span> values that will be calculated in the workflow below,</p>
<figure style="text-align: center;">
  <img src="../Images/4f26df3583b9f434159b8bbb15cc292b.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ridge/ridge_models.png"/>
  <figcaption style="text-align: center;"> Ridge regression models with low to high \(\lambda\) hyperparameter values.</figcaption>
</figure>
</section>
<section id="train-test-data-split-for-cross-validation-based-hyperparameter-tuning">
<h2>Train / Test Data Split for Cross Validation-based Hyperparameter Tuning</h2>
<p>The available data is split into training and testing subsets.</p>
<ul class="simple">
<li><p>in general 15-30% of the data is withheld from training to apply as testing data</p></li>
<li><p>testing data selection should be fair, the same difficulty of predictions (offset/different from the training data) <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0920410521015023">Fair Train-test Split Paper</a> (Salazar et al., 2022).</p></li>
<li><p>there are various other methods including train, validation and testing splits, and k-fold cross validation</p></li>
</ul>
<p>Fundamentally, all these methods proceed by training models with training data and testing model accuracy over withheld testing data for a range of hyperparameters. Then the hyperparameters are selected that minimize error with the withheld testing dataset, this is hyperparameter tuning.</p>
<ul class="simple">
<li><p>after hyperparameter tuning the model is retrained with all of the data with the selected hyperparameters.</p></li>
<li><p>the train, validation and testing approach then uses a 2nd withheld subset of the dataset to check the tuned model with data not applied for model training nor model tuning.</p></li>
</ul>
<p>In the following workflow we will use the train and test approach for hyperparameter tuning. Hereâ€™s more details and a summary of the related concepts.</p>
</section>
<section id="model-parameter-training">
<h2>Model Parameter Training</h2>
<p>The training data is applied to train the model parameters such that the model minimizes mismatch with the training data</p>
<ul class="simple">
<li><p>it is common to use <strong>mean square error</strong> (known as a <strong>L2 norm</strong>) as a loss function summarizing the model mismatch</p></li>
<li><p><strong>minimizing the loss function</strong> for simple models an analytical solution may be available, but for most machine this requires an iterative optimization method to find the best model parameters</p></li>
</ul>
<p>Hereâ€™s the derivation of the analytical solution, starting with the loss function with, least squares (left) and regularization (right) terms,</p>
<div class="math notranslate nohighlight">
\[
\left( \text{RSS} + \text{shrinkage} \right) = \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m b_{\alpha}^2
\]</div>
<p>letâ€™s convert this into matrix notation for convenience,</p>
<div class="math notranslate nohighlight">
\[
\left( \text{RSS} + \text{shrinkage} \right) = (ğ‘¦âˆ’ğ‘‹\beta)^ğ‘‡ (ğ‘¦âˆ’ğ‘‹\beta)+\lambda \beta^ğ‘‡ \beta
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is a vector of the model parameters, <span class="math notranslate nohighlight">\(y\)</span> is a vector of response feature values, and <span class="math notranslate nohighlight">\(X\)</span> is a matrix of predictor feature values, both over the training data.</p>
<p>We can expand the quadratic term,</p>
<div class="math notranslate nohighlight">
\[
\left( \text{RSS} + \text{shrinkage} \right) = ğ‘¦âˆ’2ğ‘‹^ğ‘‡ ğ‘¦\beta+(ğ‘‹^ğ‘‡ ğ‘‹) \beta^ğ‘‡ \beta+\lambda \beta^ğ‘‡ \beta
\]</div>
<p>We take the partial derivative with respect to the model parameters and set it equal to 0.0,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial \beta} \left( \text{RSS} + \text{shrinkage} \right) = -2X^T y + 2(X^T X) \beta + 2\lambda \beta = 0
\]</div>
<p>Now we can further simplify with minor adjustments.</p>
<div class="math notranslate nohighlight">
\[
2(ğ‘‹^ğ‘‡ ğ‘‹) \beta + 2 \lambda \beta = 2ğ‘‹^ğ‘‡ ğ‘¦
\]</div>
<p>Divide both sides by 2,</p>
<div class="math notranslate nohighlight">
\[
(ğ‘‹^ğ‘‡ ğ‘‹) \beta + \lambda \beta = ğ‘‹^ğ‘‡ ğ‘¦
\]</div>
<p>Group the common terms,</p>
<div class="math notranslate nohighlight">
\[
(ğ‘‹^ğ‘‡ ğ‘‹+ \lambda I) \beta =ğ‘‹^ğ‘‡ ğ‘¦
\]</div>
<p>where <span class="math notranslate nohighlight">\(I\)</span> is an identity matrix. Now we can solve for our ridge regression parameters,</p>
<div class="math notranslate nohighlight">
\[
ğ›½=\left(ğ‘‹^ğ‘‡ ğ‘‹+\lambda I \right)^{-1} ğ‘‹^ğ‘‡ ğ‘¦
\]</div>
<p>Note, <span class="math notranslate nohighlight">\(ğ‘‹^ğ‘‡ ğ‘‹+\lambda I\)</span> is generally invertible, so this is solvable.</p>
<p>This process is repeated over a range of model complexities specified by hyperparameters.</p>
</section>
<section id="interpretation-of-regularization">
<h2>Interpretation of Regularization</h2>
<p>Another interpretation and motivation for ridge regression,</p>
<ul class="simple">
<li><p>when <span class="math notranslate nohighlight">\(ğ‘š \ge ğ‘›\)</span>, linear regression is ill-posed problem, i.e., a problem with more than 1 solution</p></li>
<li><p>for ill-posed problems we introduce some constraints or regularization to limit the solutions</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/3b29636566b2ebc1625773201be9a303.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ridge/regularization.png"/>
  <figcaption style="text-align: center;"> Ill-posed linear regression model (left), and well-posed with additional shrinkage constraint ridge regression (right).
</figcaption>
</figure>
</section>
<section id="model-hyperparameter-tuning">
<h2>Model Hyperparameter Tuning</h2>
<p>The withheld testing data is retrieved and loss function (usually the <strong>L2 norm</strong> again) is calculated to summarize the error over the testing data</p>
<ul class="simple">
<li><p>this is repeated over the range of specified hyperparameters</p></li>
<li><p>the model complexity / hyperparameters that minimize the loss function / error summary in testing is selected</p></li>
</ul>
<p>This is known are model hyperparameter tuning.</p>
</section>
<section id="model-overfit">
<h2>Model Overfit</h2>
<p>More model complexity / flexibility than can be justified with the available data, data accuracy, frequency and coverage</p>
<ul class="simple">
<li><p>Model explains â€œidiosyncrasiesâ€ of the data, capturing data noise/error in the model</p></li>
<li><p>High accuracy in training, but low accuracy in testing / real-world use away from training data cases â€“ poor ability of the model to generalize</p></li>
</ul>
<p>If the model error in testing is increasing while the model error in training is decreasing, this is an indicator of an overfit model.</p>
</section>
<section id="load-the-required-libraries">
<h2>Load the Required Libraries</h2>
<p>We will also need some standard packages. These should have been installed with Anaconda 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="o">%</span><span class="k">matplotlib</span> inline                                         
<span class="n">suppress_warnings</span> <span class="o">=</span> <span class="kc">False</span>
<span class="kn">import</span> <span class="nn">os</span>                                                     <span class="c1"># to set current working directory </span>
<span class="kn">import</span> <span class="nn">os</span>                                                     <span class="c1"># to set current working directory </span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                                            <span class="c1"># ndarrays and matrix math</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>                                      <span class="c1"># statistical methods</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>                                           <span class="c1"># DataFrames</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>                               <span class="c1"># for plotting</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span> <span class="n">AutoMinorLocator</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>      <span class="c1"># specific measures to check our models</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>                              <span class="c1"># linear regression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>                        <span class="c1"># ridge regression implemented in scikit learn</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>           <span class="c1"># multi-processor K-fold crossvalidation</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>          <span class="c1"># train and test split</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">HTML</span>                     <span class="c1"># custom displays</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>                                         <span class="c1"># default color bar, no bias and friendly for color vision defeciency</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># grid behind plotting elements</span>
<span class="k">if</span> <span class="n">suppress_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>  
    <span class="kn">import</span> <span class="nn">warnings</span>                                           <span class="c1"># supress any warnings for this demonstration</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>                
</pre></div>
</div>
</div>
</div>
<p>If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing â€˜python -m pip install [package-name]â€™. More assistance is available with the respective package docs.</p>
</section>
<section id="declare-functions">
<h2>Declare Functions</h2>
<p>Letâ€™s define a function to streamline the addition specified percentiles and major and minor gridlines to our plots.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">weighted_percentile</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">perc</span><span class="p">):</span>                 <span class="c1"># calculate weighted percentile </span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> 
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> 
    <span class="n">cdf</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">perc</span><span class="p">,</span> <span class="n">cdf</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="c1"># Function from iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049</span>

<span class="k">def</span> <span class="nf">histogram_bounds</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">color</span><span class="p">):</span>                   <span class="c1"># add uncertainty bounds to a histogram          </span>
    <span class="n">p10</span> <span class="o">=</span> <span class="n">weighted_percentile</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="mf">0.1</span><span class="p">);</span> <span class="n">avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">);</span> <span class="n">p90</span> <span class="o">=</span> <span class="n">weighted_percentile</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="mf">0.9</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">p10</span><span class="p">,</span><span class="n">p10</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">45</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">'dashed'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">avg</span><span class="p">,</span><span class="n">avg</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">45</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">p90</span><span class="p">,</span><span class="n">p90</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">45</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">'dashed'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add_grid</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks </span>

<span class="k">def</span> <span class="nf">display_sidebyside</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>                                <span class="c1"># display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)</span>
    <span class="n">html_str</span> <span class="o">=</span> <span class="s1">''</span>
    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="n">html_str</span> <span class="o">+=</span> <span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="o">.</span><span class="n">to_html</span><span class="p">()</span>  <span class="c1"># Using .head() for the first few rows</span>
    <span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s1">'&lt;div style="display: flex;"&gt;</span><span class="si">{</span><span class="n">html_str</span><span class="si">}</span><span class="s1">&lt;/div&gt;'</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="set-the-working-directory">
<h2>Set the Working Directory</h2>
<p>I always like to do this so I donâ€™t lose files and to simplify subsequent read and writes (avoid including the full address each time).  Also, in this case make sure to place the required (see below) data file in this working directory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1">#os.chdir("C:\PGE337")                                        # set the working directory</span>
</pre></div>
</div>
</div>
</div>
<p>You will have to update the part in quotes with your own working directory and the format is different on a Mac (e.g. â€œ~/PGEâ€).</p>
</section>
<section id="loading-tabular-data">
<h2>Loading Tabular Data</h2>
<p>Hereâ€™s the command to load our comma delimited data file in to a Pandasâ€™ DataFrame object.</p>
<p>Letâ€™s load the provided multivariate, spatial dataset â€˜unconv_MV.csvâ€™. This dataset has variables from 1,000 unconventional wells including:</p>
<ul class="simple">
<li><p>density (<span class="math notranslate nohighlight">\(g/cm^{3}\)</span>)</p></li>
<li><p>porosity (volume %)</p></li>
</ul>
<p>Note, the dataset is synthetic.</p>
<p>We load it with the pandas â€˜read_csvâ€™ function into a DataFrame we called â€˜my_dataâ€™ and then preview it to make sure it loaded correctly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">add_error</span> <span class="o">=</span> <span class="kc">True</span>                                              <span class="c1"># add random error to the response feature</span>
<span class="n">std_error</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">;</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">71071</span>

<span class="n">yname</span> <span class="o">=</span> <span class="s1">'Porosity'</span><span class="p">;</span> <span class="n">xname</span> <span class="o">=</span> <span class="s1">'Density'</span>                         <span class="c1"># specify the predictor features (x2) and response feature (x1)</span>
<span class="n">xmin</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">;</span> <span class="n">xmax</span> <span class="o">=</span> <span class="mf">2.5</span>                                        <span class="c1"># set minimums and maximums for visualization </span>
<span class="n">ymin</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span> <span class="n">ymax</span> <span class="o">=</span> <span class="mf">25.0</span>    
<span class="n">xlabel</span> <span class="o">=</span> <span class="s1">'Porosity'</span><span class="p">;</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">'Density'</span>                       <span class="c1"># specify the feature labels for plotting</span>
<span class="n">yunit</span> <span class="o">=</span> <span class="s1">'%'</span><span class="p">;</span> <span class="n">xunit</span> <span class="o">=</span> <span class="s1">'$g/cm^</span><span class="si">{3}</span><span class="s1">$'</span>    
<span class="n">Xlabelunit</span> <span class="o">=</span> <span class="n">xlabel</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span>
<span class="n">ylabelunit</span> <span class="o">=</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span>

<span class="c1">#df = pd.read_csv("Density_Por_data.csv")                     # load the data from local current directory</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="sa">r</span><span class="s2">"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv"</span><span class="p">)</span> <span class="c1"># load the data from my github repo</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">73073</span><span class="p">);</span> <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span> <span class="c1"># extract 30% random to reduce the number of data</span>

<span class="k">if</span> <span class="n">add_error</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>                                         <span class="c1"># method to add error</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>                                 <span class="c1"># set random number seed</span>
    <span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span><span class="n">scale</span><span class="o">=</span><span class="n">std_error</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span> <span class="c1"># add noise</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">_get_numeric_data</span><span class="p">();</span> <span class="n">values</span><span class="p">[</span><span class="n">values</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>   <span class="c1"># set negative to 0 in a shallow copy ndarray</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-test-split">
<h2>Train-Test Split</h2>
<p>For simplicity we apply a random train-test split with the train_test_split function from scikit-learn package, model_selection module.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">73073</span><span class="p">)</span> <span class="c1"># train and test split</span>
<span class="c1"># y_train = pd.DataFrame({yname:y_train.values}); y_test = pd.DataFrame({yname:y_test.values}) # optional to ensure response is a DataFrame</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>                         <span class="c1"># features as 1D vectors</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>

<span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                <span class="c1"># features as train and test DataFrames</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualize-the-dataframe">
<h2>Visualize the DataFrame</h2>
<p>Visualizing the DataFrame is useful first check of the data.</p>
<ul class="simple">
<li><p>many things can go wrong, e.g., we loaded the wrong data, all the features did not load, etc.</p></li>
</ul>
<p>We can preview by utilizing the â€˜headâ€™ DataFrame member function (with a nice and clean format, see below).</p>
<ul class="simple">
<li><p>we have a custom function to preview the training and testing DataFrames side-by-side.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="s1">'   Training DataFrame      Testing DataFrame'</span><span class="p">)</span>
<span class="n">display_sidebyside</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span><span class="n">df_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>   Training DataFrame      Testing DataFrame
</pre></div>
</div>
<div class="output text_html"><div style="display: flex;"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Density</th>
      <th>Porosity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>24</th>
      <td>1.778580</td>
      <td>11.426485</td>
    </tr>
    <tr>
      <th>101</th>
      <td>2.410560</td>
      <td>8.488544</td>
    </tr>
    <tr>
      <th>88</th>
      <td>2.216014</td>
      <td>10.133693</td>
    </tr>
    <tr>
      <th>79</th>
      <td>1.631896</td>
      <td>12.712326</td>
    </tr>
    <tr>
      <th>58</th>
      <td>1.528019</td>
      <td>16.129542</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Density</th>
      <th>Porosity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>59</th>
      <td>1.742534</td>
      <td>15.380154</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.404932</td>
      <td>13.710628</td>
    </tr>
    <tr>
      <th>35</th>
      <td>1.552713</td>
      <td>14.131878</td>
    </tr>
    <tr>
      <th>92</th>
      <td>1.762359</td>
      <td>11.154896</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1.885087</td>
      <td>9.403056</td>
    </tr>
  </tbody>
</table></div></div></div>
</div>
</section>
<section id="summary-statistics-for-tabular-data">
<h2>Summary Statistics for Tabular Data</h2>
<p>There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames. The describe command provides count, mean, minimum, maximum in a nice data table.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="s1">'     Training DataFrame         Testing DataFrame'</span><span class="p">)</span>
<span class="n">display_sidebyside</span><span class="p">(</span><span class="n">df_train</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s1">'count'</span><span class="p">,</span> <span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'std'</span><span class="p">,</span> <span class="s1">'min'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">]],</span><span class="n">df_test</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s1">'count'</span><span class="p">,</span> <span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'std'</span><span class="p">,</span> <span class="s1">'min'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>     Training DataFrame         Testing DataFrame
</pre></div>
</div>
<div class="output text_html"><div style="display: flex;"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Density</th>
      <th>Porosity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>78.000000</td>
      <td>78.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1.739027</td>
      <td>12.501465</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.302510</td>
      <td>3.428260</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.996736</td>
      <td>3.276449</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2.410560</td>
      <td>21.660179</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Density</th>
      <th>Porosity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>27.000000</td>
      <td>27.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1.734710</td>
      <td>12.380796</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.247761</td>
      <td>2.916045</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.067960</td>
      <td>7.894595</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2.119652</td>
      <td>18.133771</td>
    </tr>
  </tbody>
</table></div></div></div>
</div>
</section>
<section id="visualize-the-data">
<h2>Visualize the Data</h2>
<p>Letâ€™s check the consistency and coverage of training and testing with histograms and scatter plots.</p>
<ul class="simple">
<li><p>check to make sure the train and test data cover the range of possible feature combinations</p></li>
<li><p>ensure we are not extrapolating beyond the training data with the testing cases</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nbins</span> <span class="o">=</span> <span class="mi">20</span>                                                    <span class="c1"># number of histogram bins</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Density'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Porosity'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>                                              <span class="c1"># plot the model</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Porosity vs Density'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="c1">#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf')   </span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c4e4535126491e78be365e71b1c9a03a2d1b7af5ddc9027eed6eced1b9c42f72.png" src="../Images/019f46cd3338e55a8b84bcd57a899945.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/c4e4535126491e78be365e71b1c9a03a2d1b7af5ddc9027eed6eced1b9c42f72.png"/>
</div>
</div>
</section>
<section id="linear-regression-model">
<h2>Linear Regression Model</h2>
<p>Letâ€™s first calculate the linear regression model. We use scikit learn and then extend the same workflow to ridge regression.</p>
<ul class="simple">
<li><p>we are building a model, <span class="math notranslate nohighlight">\(\phi = f(\rho)\)</span>, where <span class="math notranslate nohighlight">\(\phi\)</span> is porosity and <span class="math notranslate nohighlight">\(\rho\)</span> is density.</p></li>
<li><p>we could also say, we have â€œporosity regressed on densityâ€.</p></li>
</ul>
<p>Our model has this specific equation,</p>
<div class="math notranslate nohighlight">
\[
\phi = b_1 \times \rho + b_0
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">linear_reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>                  <span class="c1"># instantiate the model</span>

<span class="n">linear_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span> <span class="c1"># train the model parameters</span>
<span class="n">x_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">y_model_linear</span> <span class="o">=</span> <span class="n">linear_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>   <span class="c1"># predict at the withheld test data </span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot the data, model with model parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_model</span><span class="p">,</span><span class="n">y_model_linear</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Linear Regression'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Linear Regression Model Parameters:'</span><span class="p">,[</span><span class="mf">1.86</span><span class="p">,</span><span class="mi">18</span><span class="p">])</span> <span class="c1"># add the model to the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_1$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">linear_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">17</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_0$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">linear_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Linear Regression Model, Porosity = f(Density)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9068634e78d0710f8ba9632a527aa661f2ca3527b1121d2db728317558851841.png" src="../Images/5bc6600437501d419bf56af2e1dc727d.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/9068634e78d0710f8ba9632a527aa661f2ca3527b1121d2db728317558851841.png"/>
</div>
</div>
<p>You may have noticed the additional reshape operation applied to the predictor feature in the predict function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">y_linear_model</span> <span class="o">=</span> <span class="n">linear_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>   <span class="c1"># predict at the withheld test data </span>
</pre></div>
</div>
<p>This is needed because scikit-learn assumes more than one predictor feature; therefore, expects a 2D array of samples (rows) and features (columns), but we have only a 1D vector.</p>
<ul class="simple">
<li><p>the reshape operation turns the 1D vector into a 2D vector with only 1 column</p></li>
</ul>
</section>
<section id="linear-regression-model-checks">
<h2>Linear Regression Model Checks</h2>
<p>Letâ€™s run some quick model checks.  Much more could be done, but I limit this for brevity here.</p>
<ul class="simple">
<li><p>see the Linear Regression chapter for more information and checks</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y_pred_linear</span> <span class="o">=</span> <span class="n">linear_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># predict at test data</span>
<span class="n">r_squared_linear</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_linear</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>                                              <span class="c1"># plot testing diagnostics </span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_model</span><span class="p">,</span><span class="n">y_model_linear</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Linear Regression'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="c1"># plt.scatter(df_test[xname], y_pred,color='grey',edgecolor='black',s = 40, alpha = 1.0, label = 'predictions',zorder=100)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span> <span class="n">y_pred_linear</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span> <span class="n">y_pred_linear</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'*'</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">320</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Predictions'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">idata</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">]],[</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">yname</span><span class="p">],</span>
                        <span class="n">y_pred_linear</span><span class="p">[</span><span class="n">idata</span><span class="p">]],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">'test $\Delta_</span><span class="si">{y_i}</span><span class="s1">$'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">]],[</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">yname</span><span class="p">],</span>
                        <span class="n">y_pred_linear</span><span class="p">[</span><span class="n">idata</span><span class="p">]],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Linear Regression Model Parameters:'</span><span class="p">,[</span><span class="mf">1.86</span><span class="p">,</span><span class="mi">18</span><span class="p">])</span> <span class="c1"># add the model to the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_1$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">linear_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">17</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_0$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">linear_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$r^2$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">r_squared_linear</span><span class="p">,</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">15</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Linear Regression Model, Porosity = f(Density)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">])</span>

<span class="n">y_res_linear</span> <span class="o">=</span> <span class="n">y_pred_linear</span> <span class="o">-</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">'Porosity'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>     <span class="c1"># calculate the test residual</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y_res_linear</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Error Residual at Testing Data"</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' True - Estimate (%)'</span><span class="p">);</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Test Error Residual:'</span><span class="p">,[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mf">4.7</span><span class="p">])</span> <span class="c1"># add residual summary statistics</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$\overline{\Delta</span><span class="si">{y}</span><span class="s1">}$: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">y_res_linear</span><span class="p">),</span><span class="mi">2</span><span class="p">)),[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mf">4.4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$\sigma_{\Delta</span><span class="si">{y}</span><span class="s1">}$: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y_res_linear</span><span class="p">),</span><span class="mi">2</span><span class="p">)),[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mf">4.1</span><span class="p">])</span>
<span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d10062259023c26d588429dde18deab7d42abe383ae44ad7fa6d4c7b96ef620e.png" src="../Images/208a933176542d3185b7f4f4c9a1bf61.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/d10062259023c26d588429dde18deab7d42abe383ae44ad7fa6d4c7b96ef620e.png"/>
</div>
</div>
</section>
<section id="ridge-regression-model">
<h2>Ridge Regression Model</h2>
<p>Letâ€™s replace the scikit-learn linear regression method with the scikit-learn ridge regression method.</p>
<ul class="simple">
<li><p>note, we must now set the <span class="math notranslate nohighlight">\(\lambda\)</span> hyperparameter.</p></li>
<li><p>in scikit-learn the hyperparameter(s) is(are) set with the instantiation of the model</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">lam</span> <span class="o">=</span> <span class="mf">13.0</span>                                                     <span class="c1"># lambda hyperparameter</span>

<span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>                                  <span class="c1"># instantiate the model</span>

<span class="n">ridge_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span> <span class="c1"># train the model parameters</span>
<span class="n">x_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">y_model_ridge</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>      <span class="c1"># predict with the fit model</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot the data, model with model parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_model</span><span class="p">,</span><span class="n">y_model_ridge</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Linear Regression'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Ridge Regression Model Parameters:'</span><span class="p">,[</span><span class="mf">1.86</span><span class="p">,</span><span class="mi">18</span><span class="p">])</span> <span class="c1"># add the model to the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_1$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">17</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_0$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Ridge Model, Regression of '</span> <span class="o">+</span> <span class="n">yname</span> <span class="o">+</span> <span class="s1">' on '</span> <span class="o">+</span> <span class="n">xname</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">' with a $\lambda = $'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">lam</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/61654e5169f1e8d42162279fae72b88a9d9108ce66bb01b9fd227b101c160daf.png" src="../Images/05ecf578788df580a4c1f1f25e9ffcb9.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/61654e5169f1e8d42162279fae72b88a9d9108ce66bb01b9fd227b101c160daf.png"/>
</div>
</div>
<p>Letâ€™s repeat the simple model checks that we applied with our linear regression model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y_pred_ridge</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># predict at test data</span>
<span class="n">r_squared</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_ridge</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>                                              <span class="c1"># plot testing diagnostics </span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_model</span><span class="p">,</span><span class="n">y_model_ridge</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Linear Regression'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span> <span class="n">y_pred_ridge</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span> <span class="n">y_pred_ridge</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'*'</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">320</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'predictions'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">idata</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">]],[</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">yname</span><span class="p">],</span>
                        <span class="n">y_pred_ridge</span><span class="p">[</span><span class="n">idata</span><span class="p">]],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">'test $\Delta_</span><span class="si">{y_i}</span><span class="s1">$'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">]],[</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">yname</span><span class="p">],</span>
                        <span class="n">y_pred_ridge</span><span class="p">[</span><span class="n">idata</span><span class="p">]],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Ridge Regression Model Parameters:'</span><span class="p">,[</span><span class="mf">1.81</span><span class="p">,</span><span class="mi">18</span><span class="p">])</span> <span class="c1"># add the model to the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_1$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">17</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_0$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Ridge Model, Regression of '</span> <span class="o">+</span> <span class="n">yname</span> <span class="o">+</span> <span class="s1">' on '</span> <span class="o">+</span> <span class="n">xname</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">' with a $\lambda = $'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">lam</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">])</span>

<span class="n">y_res_ridge</span> <span class="o">=</span> <span class="n">y_pred_ridge</span> <span class="o">-</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">'Porosity'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>       <span class="c1"># calculate the test residual</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y_res_ridge</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Error Residual at Testing Data"</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' True - Estimate (%)'</span><span class="p">);</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">y_res_ridge</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Residual Average = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">y_res_ridge</span><span class="p">),</span><span class="mi">2</span><span class="p">)),[</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">y_res_ridge</span><span class="p">)</span><span class="o">+</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">2.5</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Test Error Residual:'</span><span class="p">,[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mf">4.7</span><span class="p">])</span> <span class="c1"># add residual summary statistics</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$\overline{\Delta</span><span class="si">{y}</span><span class="s1">}$: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">y_res_ridge</span><span class="p">),</span><span class="mi">2</span><span class="p">)),[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mf">4.4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$\sigma_{\Delta</span><span class="si">{y}</span><span class="s1">}$: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y_res_ridge</span><span class="p">),</span><span class="mi">2</span><span class="p">)),[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mf">4.1</span><span class="p">])</span>
<span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bd6d04e063283738954b9b88fa6ed77d11eeec140126dcf4052ef6d9523d2c48.png" src="../Images/2e6dac5a06cdc0e91c454e5758d2a7af.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/bd6d04e063283738954b9b88fa6ed77d11eeec140126dcf4052ef6d9523d2c48.png"/>
</div>
</div>
<p>Interesting, we explained less variance and have a larger residual standard deviation (more error).</p>
<ul class="simple">
<li><p>ridge regression for our arbitrarily selected hyperparameter, <span class="math notranslate nohighlight">\(\lambda\)</span>, actually reduced both testing variance explained and accuracy</p></li>
<li><p>this is not surprising, we are not actually tuning the hyperparameter to get the best model!</p></li>
</ul>
</section>
<section id="investigating-the-lambda-hyperparameter">
<h2>Investigating the Lambda Hyperparameter</h2>
<p>Letâ€™s loop over multiple lambda values - from 0 to 100 and observe the change in:</p>
<ul class="simple">
<li><p>training and testing, mean square error (MSE) and variance explained</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Arrays to store the results</span>
<span class="n">ncases</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">lamd_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">ncases</span><span class="p">)</span>
<span class="n">x_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">var_explained_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">);</span> <span class="n">var_explained_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">)</span>
<span class="n">mse_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">);</span> <span class="n">mse_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ilam</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">)):</span>                           <span class="c1"># Loop over all lambda values</span>
    <span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lamd_mat</span><span class="p">[</span><span class="n">ilam</span><span class="p">])</span>
    <span class="n">ridge_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span> <span class="c1"># fit model</span>

    <span class="n">y_model</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>        <span class="c1"># predict with the fit model  </span>
    <span class="n">y_pred_train</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># predict with the fit model   </span>
    <span class="n">var_explained_train</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span>
    <span class="n">mse_train</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span> 
    
    <span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">var_explained_test</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>
    <span class="n">mse_test</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>    
   
    <span class="k">if</span> <span class="n">ilam</span> <span class="o">&lt;=</span> <span class="mi">7</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">ilam</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_model</span><span class="p">,</span><span class="n">y_model</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Linear Regression'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Ridge Regression Model Parameters:'</span><span class="p">,[</span><span class="mf">1.86</span><span class="p">,</span><span class="mi">18</span><span class="p">])</span> <span class="c1"># add the model to the plot</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_1$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">17</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_0$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Ridge Model, Regression of '</span> <span class="o">+</span> <span class="n">yname</span> <span class="o">+</span> <span class="s1">' on '</span> <span class="o">+</span> <span class="n">xname</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">' with a $\lambda = $'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">[</span><span class="n">ilam</span><span class="p">],</span><span class="mi">4</span><span class="p">)))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">4.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2a222870bfcb75734199aa94ba43bb3842ecda84cd1bf44eee6b1daf798f4199.png" src="../Images/5a585689234bfbaf29513933357bc5cb.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/2a222870bfcb75734199aa94ba43bb3842ecda84cd1bf44eee6b1daf798f4199.png"/>
</div>
</div>
<p>We can observe from the first 8 cases above of the trained ridge regression model that increase in the <span class="math notranslate nohighlight">\(\lambda\)</span> hyperparameter decreases the slope of the linear fit.</p>
<p>Letâ€™s plot the mean squaure error and variance explained over train and test datasets.</p>
<p>Recall, the variance explained, <span class="math notranslate nohighlight">\(R^2\)</span> , is given by,</p>
<div class="math notranslate nohighlight">
\[
R^2 = 1 - \frac{\text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(SS_{\text{residual}}\)</span> is the sum of squares of residuals (or errors), and <span class="math notranslate nohighlight">\(SS_{\text{total}}\)</span> is the total sum of squares (the variance of the observed data).</p>
<p>and the Mean Squared Error (MSE) is given by,</p>
<div class="math notranslate nohighlight">
\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the actual value, <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted value, and <span class="math notranslate nohighlight">\(n\)</span> is the number of data points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">ncases</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">lamd_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">ncases</span><span class="p">)</span>
<span class="n">x_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">var_explained_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">);</span> <span class="n">var_explained_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">)</span>
<span class="n">mse_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">);</span> <span class="n">mse_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ilam</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">)):</span>                           <span class="c1"># Loop over all lambda values</span>
    <span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lamd_mat</span><span class="p">[</span><span class="n">ilam</span><span class="p">])</span>
    <span class="n">ridge_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span> <span class="c1"># fit model</span>

    <span class="n">y_model</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>        <span class="c1"># predict with the fit model  </span>
    <span class="n">y_pred_train</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># predict with the fit model   </span>
    <span class="n">var_explained_train</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span>
    <span class="n">mse_train</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span> 
    
    <span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">var_explained_test</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>
    <span class="n">mse_test</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">,</span> <span class="n">var_explained_train</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">,</span> <span class="n">var_explained_test</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Variance Explained vs. Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Variance Explained'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="mf">10000.</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">minorticks_on</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">':'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>  

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">,</span> <span class="n">mse_train</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">,</span> <span class="n">mse_test</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'MSE vs. Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Mean Square Error'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="mf">10000.</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">15.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">minorticks_on</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">':'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>  

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/eaa10f0fe491c5af34e0480460db637c0b45df49bd3270493df83ecce6357f75.png" src="../Images/cebb95f748df4c664c9a127975a7c87b.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/eaa10f0fe491c5af34e0480460db637c0b45df49bd3270493df83ecce6357f75.png"/>
</div>
</div>
<p>We observe that as we increase the lambda parameter the variance explained decreases and the mean square error increases.</p>
<ul class="simple">
<li><p>this makes sense as the data has a consistent linear trend and as the slope â€˜shrinksâ€™ to zero the error increases and the variance explained decreases</p></li>
<li><p>there could be other cases where the reduced slope actually performs better in testing. For example with sparse and noisy data.</p></li>
</ul>
</section>
<section id="model-variance">
<h2>Model Variance</h2>
<p>Now letâ€™s explore the concept of model variance, an important part of machine learning accuracy in testing.</p>
<ul class="simple">
<li><p>the sensitivity of the model to the specific training data</p></li>
<li><p>as <span class="math notranslate nohighlight">\(\lambda\)</span> increases the sensitivity to the training data, model variance decreases</p></li>
</ul>
<p>Letâ€™s demonstrate this with this workflow:</p>
<ul class="simple">
<li><p>loop over multiple lambda values</p>
<ul>
<li><p>loop over multiple bootstrap samples of the data</p>
<ul>
<li><p>calculate the ridge regression fit (slope)</p></li>
</ul>
</li>
<li><p>calculate the variance of these bootstrap results</p></li>
</ul>
</li>
</ul>
<p>WARNING: this will take several minutes to run</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">L</span> <span class="o">=</span> <span class="mi">200</span>                                                       <span class="c1"># the number of bootstrap realizations </span>

<span class="n">nsamples</span> <span class="o">=</span> <span class="mi">20</span>                                                 <span class="c1"># the number of samples in each bootstrap realization</span>
<span class="n">nlambda</span> <span class="o">=</span> <span class="mi">100</span>                                                 <span class="c1"># number of lambda values to evaluate</span>

<span class="n">coef_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>                                        <span class="c1"># declare arrays to store the results</span>
<span class="n">variance_coef</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nlambda</span><span class="p">)</span>

<span class="c1">#lamd_mat = np.linspace(0.0,100.0,nlambda) </span>
<span class="n">lambda_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">nlambda</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ilam</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">lambda_mat</span><span class="p">)):</span>                         <span class="c1"># loop over all lambda values                   </span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>                                     <span class="c1"># loop over all bootstrap realizations</span>
        <span class="n">df_sample</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="n">nsamples</span><span class="p">)</span>                   <span class="c1"># random sample (1 bootstrap)</span>
        <span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lambda_mat</span><span class="p">[</span><span class="n">ilam</span><span class="p">])</span>             <span class="c1"># instantiate model</span>
        <span class="n">ridge_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_sample</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">nsamples</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">df_sample</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span> <span class="c1"># fit model</span>
        <span class="n">coef_mat</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>                      <span class="c1"># get the slope parameter</span>
    <span class="n">variance_coef</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">coef_mat</span><span class="p">)</span>                    <span class="c1"># calculate the variance of the slopes over the L bootstraps</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_mat</span><span class="p">,</span> <span class="n">variance_coef</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Slope Variance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Model Variance vs. Lambda Hyperparameter'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Model Variance - Variance of the Slope Parameter, $b_1$'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">100000.</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'both'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5e13984f63c906b044e58c06c1851dc67ab928545f9bfb881f3e02ebb1002cf1.png" src="../Images/42462583008a06affc490158f6f84d94.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/5e13984f63c906b044e58c06c1851dc67ab928545f9bfb881f3e02ebb1002cf1.png"/>
</div>
</div>
<p>The result is as expected, with increase in <span class="math notranslate nohighlight">\(\lambda\)</span> hyperparameter the sensitivity of the model to the training data is decreased.</p>
</section>
<section id="k-fold-cross-validation">
<h2>k-fold Cross Validation</h2>
<p>It would be useful to conduct a complete k-fold validation to evaluate the testing error vs. the hyperparameter lambda for model tuning.</p>
<ul class="simple">
<li><p>the following code is provided to do this</p></li>
<li><p>once again, with a single predictor feature we must reshape to a 2D array</p></li>
</ul>
<p>We loop over 100 <span class="math notranslate nohighlight">\(\lambda\)</span> values from 0.01 to 100,000,</p>
<ul class="simple">
<li><p>get the negative mean square error for each of the 4 k-folds</p></li>
<li><p>then we take the average and apply the absolute value</p></li>
</ul>
<p>Why work with negative mean square error? Simple, to use the functionality in scikit-learn that optimizes by maximization and for consistency with other scores like <span class="math notranslate nohighlight">\(r^2\)</span> where larger values are better.</p>
<ul class="simple">
<li><p>I find negative MSE confusing, so for plotting I use the absolute value to convert the values to strictly positive.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">score</span> <span class="o">=</span> <span class="p">[]</span>                                                    <span class="c1"># code modified from StackOverFlow by Dimosthenis</span>

<span class="n">nlambda</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">lambda_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">nlambda</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ilam</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nlambda</span><span class="p">):</span>
    <span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lambda_mat</span><span class="p">[</span><span class="n">ilam</span><span class="p">])</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">ridge_reg</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Density'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">'Porosity'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s2">"neg_mean_squared_error"</span><span class="p">)</span> <span class="c1"># Perform 10-fold cross validation</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_mat</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Test MSA'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Ridge Regression Test Mean Square Error vs. Lambda Hyperparameter'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Test Mean Square Error'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">1.0e-2</span><span class="p">,</span><span class="mf">1.0e5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="mf">20.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Linear Regression'</span><span class="p">,[</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">12.5</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Mean'</span><span class="p">,[</span><span class="mi">1100</span><span class="p">,</span><span class="mf">14.5</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span><span class="mi">100000</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'both'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2e65418d331d427736b40372288b8cec15d2e1bd38e995d14e8e51014c0e10d7.png" src="../Images/8eb67ef97a23db1eb0ad78518b9e99c2.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/2e65418d331d427736b40372288b8cec15d2e1bd38e995d14e8e51014c0e10d7.png"/>
</div>
</div>
<p>From the above we observe 3 phases,</p>
<ul class="simple">
<li><p>left - the test MSE levels out at a minimum, the model has converged on linear regression.</p></li>
<li><p>right - the test MSE levels out at a maximum, the model has converged on the predictor feature mean, i.e., model parameter slope is 0.0.</p></li>
<li><p>center - transitional between both cases</p></li>
</ul>
<p>In this case we see that the linear regression model (<span class="math notranslate nohighlight">\(\lambda = 0.0\)</span>) is the best model! If ridge regression is the optimum model the test mean square error would minimize between linear regression and mean.</p>
<ul class="simple">
<li><p>this commonly occurs for datasets with issues with noise, data paucity and high dimensionality.</p></li>
</ul>
<p>our model is the same as linear regression.</p>
<ul class="simple">
<li><p>could we create a situation where the best model is not linear regression? I.e., were regularization is helpful?</p></li>
<li><p>yes, we can. Letâ€™s remove most the samples to create data paucity and add a lot of noise!</p></li>
</ul>
<p>Admittedly, I iterated the random seeds for the sample and noise to get this result.</p>
<ul class="simple">
<li><p>few data (low <span class="math notranslate nohighlight">\(n\)</span>) and high dimensionality (high <span class="math notranslate nohighlight">\(m\)</span>) will generally result in LASSO outperforming linear regression</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df_sample</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">noise_stdev</span> <span class="o">=</span> <span class="mf">3.0</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">df_sample</span><span class="p">[</span><span class="s1">'Porosity'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_sample</span><span class="p">[</span><span class="s1">'Porosity'</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">noise_stdev</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">df_sample</span><span class="p">))</span>

<span class="n">score</span> <span class="o">=</span> <span class="p">[]</span>                                                    <span class="c1"># code modified from StackOverFlow by Dimosthenis</span>

<span class="n">nlambda</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">lambda_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">nlambda</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ilam</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nlambda</span><span class="p">):</span>
    <span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lambda_mat</span><span class="p">[</span><span class="n">ilam</span><span class="p">])</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">ridge_reg</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span> <span class="n">df_sample</span><span class="p">[</span><span class="s1">'Density'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
                             <span class="n">y</span><span class="o">=</span><span class="n">df_sample</span><span class="p">[</span><span class="s1">'Porosity'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s2">"neg_mean_squared_error"</span><span class="p">)</span> <span class="c1"># Perform 10-fold cross validation</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_mat</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Test MSA'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Ridge Regression Test Mean Square Error vs. Ridge Hyperparameter'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Test Mean Square Error'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">1.0e-4</span><span class="p">,</span><span class="mf">1.0e3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="mf">20.0</span><span class="p">);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mf">0.07</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Linear Regression'</span><span class="p">,[</span><span class="mf">0.0007</span><span class="p">,</span><span class="mf">12.5</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'Ridge Tuned $\lambda$'</span><span class="p">,[</span><span class="mf">0.055</span><span class="p">,</span><span class="mf">12.5</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Mean'</span><span class="p">,[</span><span class="mf">7.4</span><span class="p">,</span><span class="mf">14.5</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mf">0.0001</span><span class="p">,</span><span class="mf">0.001</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">100000</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'both'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ac7782f4fe7198210e3ac58ca9cbea5c3c4e0c30335c332fac08ce3ae63077a9.png" src="../Images/b011c5d0305db3ebe120ee6826faed3c.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/ac7782f4fe7198210e3ac58ca9cbea5c3c4e0c30335c332fac08ce3ae63077a9.png"/>
</div>
</div>
</section>
<section id="comments">
<h2>Comments</h2>
<p>This was a basic treatment of ridge regression. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a> and the YouTube lecture links at the start of this chapter with resource links in the videosâ€™ descriptions.</p>
<p>I hope this is helpful,</p>
<p><em>Michael</em></p>
</section>
<section id="about-the-author">
<h2>About the Author</h2>
<figure style="text-align: center;">
  <img src="../Images/eb709b2c0a0c715da01ae0165efdf3b2.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/intro/michael_pyrcz_officeshot_jacket.jpg"/>
  <figcaption style="text-align: center;"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.
</figcaption>
</figure>
<p>Michael Pyrcz is a professor in the <a class="reference external" href="https://cockrell.utexas.edu/faculty-directory/alphabetical/p">Cockrell School of Engineering</a>, and the <a class="reference external" href="https://www.jsg.utexas.edu/researcher/michael_pyrcz/">Jackson School of Geosciences</a>, at <a class="reference external" href="https://www.utexas.edu/">The University of Texas at Austin</a>, where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,</p>
<ul class="simple">
<li><p>the principal investigator of the <a class="reference external" href="https://fri.cns.utexas.edu/energy-analytics">Energy Analytics</a> freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin</p></li>
<li><p>an associate editor for <a class="reference external" href="https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board">Computers and Geosciences</a>, and a board member for <a class="reference external" href="https://link.springer.com/journal/11004/editorial-board">Mathematical Geosciences</a>, the International Association for Mathematical Geosciences.</p></li>
</ul>
<p>Michael has written over 70 <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en">peer-reviewed publications</a>, a <a class="reference external" href="https://pypi.org/project/geostatspy/">Python package</a> for spatial data analytics, co-authored a textbook on spatial data analytics, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a> and author of two recently released e-books, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy</a> and <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html">Applied Machine Learning in Python: a Hands-on Guide with Code</a>.</p>
<p>All of Michaelâ€™s university lectures are available on his <a class="reference external" href="https://www.youtube.com/@GeostatsGuyLectures">YouTube Channel</a> with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his <a class="reference external" href="https://github.com/GeostatsGuy">GitHub account</a>, to support any interested students and working professionals with evergreen content. To find out more about Michaelâ€™s work and shared educational resources visit his <span class="xref myst">Website</span>.</p>
</section>
<section id="want-to-work-together">
<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? Iâ€™d be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>Iâ€™m always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
</section>
&#13;

<h2>Motivations for Ridge Regression</h2>
<p>Hereâ€™s a simple workflow, demonstration of ridge regression and comparison to linear regression for machine learning-based predictions. Why start with linear regression?</p>
<ul class="simple">
<li><p>Linear regression is the simplest parametric predictive machine learning model</p></li>
<li><p>We learn about training machine learning models with an analytical solution calculated from the derivative of training MSE</p></li>
<li><p>Getâ€™s us started with the concepts of loss functions and norms</p></li>
<li><p>We have access to analytics expressions for confidence intervals for model uncertainty and hypothesis tests for parameter significance</p></li>
</ul>
<p>Why cover ridge regression next?</p>
<ul class="simple">
<li><p>Some times linear regression is not simple enough and we actually need a simpler model!</p></li>
<li><p>Introduce the concept of model regularization and hyperparameter tuning</p></li>
</ul>
<p>Hereâ€™s some basic details about predictive machine learning ridge regression models, letâ€™s start with linear regression first and build to ridge regression:</p>
&#13;

<h2>Linear Regression</h2>
<p>Linear regression for prediction, letâ€™s start by looking at a linear model fit to a set of data.</p>
<figure style="text-align: center;">
  <img src="../Images/d82dccdcda485413554e49d73e4d1fc8.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ridge/linear_example.png"/>
  <figcaption style="text-align: center;">Example linear regression model.</figcaption>
</figure>
<p>Letâ€™s start by defining some terms,</p>
<ul class="simple">
<li><p><strong>predictor feature</strong> - an input feature for the prediction model, given we are only discussing linear regression and not multilinear regression we have only one predictor feature, <span class="math notranslate nohighlight">\(x\)</span>. On out plots (including above) the predictor feature is on the x-axis.</p></li>
<li><p><strong>response feature</strong> - the output feature for the prediction model, in this case, <span class="math notranslate nohighlight">\(y\)</span>. On our plots (including above) the response feature is on the y-axis.</p></li>
</ul>
<p>Now, here are some key aspects of linear regression:</p>
<p><strong>Parametric Model</strong></p>
<p>This is a parametric predictive machine learning model, we accept an a prior assumption of linearity and then gain a very low parametric representation that is easy to train without a onerous amount of data.</p>
<ul class="simple">
<li><p>the fit model is a simple weighted linear additive model based on all the available features, <span class="math notranslate nohighlight">\(x_1,\ldots,x_m\)</span>.</p></li>
<li><p>the parametric model takes the form of:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0
\]</div>
<p>Hereâ€™s the visualization of the linear model parameters,</p>
<figure style="text-align: center;">
  <img src="../Images/175e41e10e74a46b4a56258ccdfb94c0.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ridge/linear_model.png"/>
  <figcaption style="text-align: center;">The linear model parameters.</figcaption>
</figure>
<p><strong>Least Squares</strong></p>
<p>The analytical solution for the model parameters, <span class="math notranslate nohighlight">\(b_1,\ldots,b_m,b_0\)</span>, is available for the L2 norm loss function, the errors are summed and squared known a least squares.</p>
<ul class="simple">
<li><p>we minimize the error, residual sum of squares (RSS) over the training data:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the actual response feature values and <span class="math notranslate nohighlight">\(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\)</span> are the model predictions, over the <span class="math notranslate nohighlight">\(\alpha = 1,\ldots,n\)</span> training data.</p>
<p>Hereâ€™s a visualization of the L2 norm loss function, MSE,</p>
<figure style="text-align: center;">
  <img src="../Images/b25cdf17fb10f18b362f50ba655df92b.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ridge/linear_MSE.png"/>
  <figcaption style="text-align: center;">The linear model loss function, mean square error.</figcaption>
</figure>
<ul class="simple">
<li><p>this may be simplified as the sum of square error over the training data,</p></li>
</ul>
<p>\begin{equation}
\sum_{i=1}^n (\Delta y_i)^2
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(\Delta y_i\)</span> is actual response feature observation <span class="math notranslate nohighlight">\(y_i\)</span> minus the model prediction <span class="math notranslate nohighlight">\(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\)</span>, over the <span class="math notranslate nohighlight">\(i = 1,\ldots,n\)</span> training data.</p>
<p><strong>Assumptions</strong></p>
<p>There are important assumption with our linear regression model,</p>
<ul class="simple">
<li><p><strong>Error-free</strong> - predictor variables are error free, not random variables</p></li>
<li><p><strong>Linearity</strong> - response is linear combination of feature(s)</p></li>
<li><p><strong>Constant Variance</strong> - error in response is constant over predictor(s) value</p></li>
<li><p><strong>Independence of Error</strong> - error in response are uncorrelated with each other</p></li>
<li><p><strong>No multicollinearity</strong> - none of the features are redundant with other features</p></li>
</ul>
&#13;

<h2>Ridge Regression</h2>
<p>With ridge regression we add a hyperparameter, <span class="math notranslate nohighlight">\(\lambda\)</span>, to our minimization, with a shrinkage penalty term, <span class="math notranslate nohighlight">\(\sum_{j=1}^m b_{\alpha}^2\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m b_{\alpha}^2
\]</div>
<p>As a result, ridge regression training integrates two and often competing goals to find the model parameters,</p>
<ul class="simple">
<li><p>find the model parameters that minimize the error with training data</p></li>
<li><p>minimize the slope parameters towards zero</p></li>
</ul>
<p>Note: lambda does not include the intercept, <span class="math notranslate nohighlight">\(b_0\)</span>.</p>
<p>The <span class="math notranslate nohighlight">\(\lambda\)</span> is a hyperparameter that controls the degree of fit of the model and may be related to the model bias-variance trade-off.</p>
<figure style="text-align: center;">
  <img src="../Images/2a291289b4f4b55ba9adbb7538140a67.png" style="display: block; margin: 0 auto; width: 60%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ridge/ridge_tuning.png"/>
  <figcaption style="text-align: center;"> Testing error components and expected test square error for ridge regression.</figcaption>
</figure>
<ul class="simple">
<li><p>for <span class="math notranslate nohighlight">\(\lambda \rightarrow 0\)</span> the solution approaches linear regression, there is no bias (relative to a linear model fit), but the model variance is likely higher</p></li>
<li><p>as <span class="math notranslate nohighlight">\(\lambda\)</span> increases the model variance decreases and the model bias increases, the model becomes simpler</p></li>
<li><p>for <span class="math notranslate nohighlight">\(\lambda \rightarrow \infty\)</span> the model parameters <span class="math notranslate nohighlight">\(b_1,\ldots,b_m\)</span> shrink to 0.0 and the model predictions approaches the training data response feature mean</p></li>
</ul>
<p>Hereâ€™s a variety of ridge regression models for various <span class="math notranslate nohighlight">\(\lamda\)</span> values that will be calculated in the workflow below,</p>
<figure style="text-align: center;">
  <img src="../Images/4f26df3583b9f434159b8bbb15cc292b.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ridge/ridge_models.png"/>
  <figcaption style="text-align: center;"> Ridge regression models with low to high \(\lambda\) hyperparameter values.</figcaption>
</figure>
&#13;

<h2>Train / Test Data Split for Cross Validation-based Hyperparameter Tuning</h2>
<p>The available data is split into training and testing subsets.</p>
<ul class="simple">
<li><p>in general 15-30% of the data is withheld from training to apply as testing data</p></li>
<li><p>testing data selection should be fair, the same difficulty of predictions (offset/different from the training data) <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0920410521015023">Fair Train-test Split Paper</a> (Salazar et al., 2022).</p></li>
<li><p>there are various other methods including train, validation and testing splits, and k-fold cross validation</p></li>
</ul>
<p>Fundamentally, all these methods proceed by training models with training data and testing model accuracy over withheld testing data for a range of hyperparameters. Then the hyperparameters are selected that minimize error with the withheld testing dataset, this is hyperparameter tuning.</p>
<ul class="simple">
<li><p>after hyperparameter tuning the model is retrained with all of the data with the selected hyperparameters.</p></li>
<li><p>the train, validation and testing approach then uses a 2nd withheld subset of the dataset to check the tuned model with data not applied for model training nor model tuning.</p></li>
</ul>
<p>In the following workflow we will use the train and test approach for hyperparameter tuning. Hereâ€™s more details and a summary of the related concepts.</p>
&#13;

<h2>Model Parameter Training</h2>
<p>The training data is applied to train the model parameters such that the model minimizes mismatch with the training data</p>
<ul class="simple">
<li><p>it is common to use <strong>mean square error</strong> (known as a <strong>L2 norm</strong>) as a loss function summarizing the model mismatch</p></li>
<li><p><strong>minimizing the loss function</strong> for simple models an analytical solution may be available, but for most machine this requires an iterative optimization method to find the best model parameters</p></li>
</ul>
<p>Hereâ€™s the derivation of the analytical solution, starting with the loss function with, least squares (left) and regularization (right) terms,</p>
<div class="math notranslate nohighlight">
\[
\left( \text{RSS} + \text{shrinkage} \right) = \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m b_{\alpha}^2
\]</div>
<p>letâ€™s convert this into matrix notation for convenience,</p>
<div class="math notranslate nohighlight">
\[
\left( \text{RSS} + \text{shrinkage} \right) = (ğ‘¦âˆ’ğ‘‹\beta)^ğ‘‡ (ğ‘¦âˆ’ğ‘‹\beta)+\lambda \beta^ğ‘‡ \beta
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is a vector of the model parameters, <span class="math notranslate nohighlight">\(y\)</span> is a vector of response feature values, and <span class="math notranslate nohighlight">\(X\)</span> is a matrix of predictor feature values, both over the training data.</p>
<p>We can expand the quadratic term,</p>
<div class="math notranslate nohighlight">
\[
\left( \text{RSS} + \text{shrinkage} \right) = ğ‘¦âˆ’2ğ‘‹^ğ‘‡ ğ‘¦\beta+(ğ‘‹^ğ‘‡ ğ‘‹) \beta^ğ‘‡ \beta+\lambda \beta^ğ‘‡ \beta
\]</div>
<p>We take the partial derivative with respect to the model parameters and set it equal to 0.0,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial \beta} \left( \text{RSS} + \text{shrinkage} \right) = -2X^T y + 2(X^T X) \beta + 2\lambda \beta = 0
\]</div>
<p>Now we can further simplify with minor adjustments.</p>
<div class="math notranslate nohighlight">
\[
2(ğ‘‹^ğ‘‡ ğ‘‹) \beta + 2 \lambda \beta = 2ğ‘‹^ğ‘‡ ğ‘¦
\]</div>
<p>Divide both sides by 2,</p>
<div class="math notranslate nohighlight">
\[
(ğ‘‹^ğ‘‡ ğ‘‹) \beta + \lambda \beta = ğ‘‹^ğ‘‡ ğ‘¦
\]</div>
<p>Group the common terms,</p>
<div class="math notranslate nohighlight">
\[
(ğ‘‹^ğ‘‡ ğ‘‹+ \lambda I) \beta =ğ‘‹^ğ‘‡ ğ‘¦
\]</div>
<p>where <span class="math notranslate nohighlight">\(I\)</span> is an identity matrix. Now we can solve for our ridge regression parameters,</p>
<div class="math notranslate nohighlight">
\[
ğ›½=\left(ğ‘‹^ğ‘‡ ğ‘‹+\lambda I \right)^{-1} ğ‘‹^ğ‘‡ ğ‘¦
\]</div>
<p>Note, <span class="math notranslate nohighlight">\(ğ‘‹^ğ‘‡ ğ‘‹+\lambda I\)</span> is generally invertible, so this is solvable.</p>
<p>This process is repeated over a range of model complexities specified by hyperparameters.</p>
&#13;

<h2>Interpretation of Regularization</h2>
<p>Another interpretation and motivation for ridge regression,</p>
<ul class="simple">
<li><p>when <span class="math notranslate nohighlight">\(ğ‘š \ge ğ‘›\)</span>, linear regression is ill-posed problem, i.e., a problem with more than 1 solution</p></li>
<li><p>for ill-posed problems we introduce some constraints or regularization to limit the solutions</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/3b29636566b2ebc1625773201be9a303.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ridge/regularization.png"/>
  <figcaption style="text-align: center;"> Ill-posed linear regression model (left), and well-posed with additional shrinkage constraint ridge regression (right).
</figcaption>
</figure>
&#13;

<h2>Model Hyperparameter Tuning</h2>
<p>The withheld testing data is retrieved and loss function (usually the <strong>L2 norm</strong> again) is calculated to summarize the error over the testing data</p>
<ul class="simple">
<li><p>this is repeated over the range of specified hyperparameters</p></li>
<li><p>the model complexity / hyperparameters that minimize the loss function / error summary in testing is selected</p></li>
</ul>
<p>This is known are model hyperparameter tuning.</p>
&#13;

<h2>Model Overfit</h2>
<p>More model complexity / flexibility than can be justified with the available data, data accuracy, frequency and coverage</p>
<ul class="simple">
<li><p>Model explains â€œidiosyncrasiesâ€ of the data, capturing data noise/error in the model</p></li>
<li><p>High accuracy in training, but low accuracy in testing / real-world use away from training data cases â€“ poor ability of the model to generalize</p></li>
</ul>
<p>If the model error in testing is increasing while the model error in training is decreasing, this is an indicator of an overfit model.</p>
&#13;

<h2>Load the Required Libraries</h2>
<p>We will also need some standard packages. These should have been installed with Anaconda 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="o">%</span><span class="k">matplotlib</span> inline                                         
<span class="n">suppress_warnings</span> <span class="o">=</span> <span class="kc">False</span>
<span class="kn">import</span> <span class="nn">os</span>                                                     <span class="c1"># to set current working directory </span>
<span class="kn">import</span> <span class="nn">os</span>                                                     <span class="c1"># to set current working directory </span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                                            <span class="c1"># ndarrays and matrix math</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>                                      <span class="c1"># statistical methods</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>                                           <span class="c1"># DataFrames</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>                               <span class="c1"># for plotting</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span> <span class="n">AutoMinorLocator</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>      <span class="c1"># specific measures to check our models</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>                              <span class="c1"># linear regression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>                        <span class="c1"># ridge regression implemented in scikit learn</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>           <span class="c1"># multi-processor K-fold crossvalidation</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>          <span class="c1"># train and test split</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">HTML</span>                     <span class="c1"># custom displays</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>                                         <span class="c1"># default color bar, no bias and friendly for color vision defeciency</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># grid behind plotting elements</span>
<span class="k">if</span> <span class="n">suppress_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>  
    <span class="kn">import</span> <span class="nn">warnings</span>                                           <span class="c1"># supress any warnings for this demonstration</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>                
</pre></div>
</div>
</div>
</div>
<p>If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing â€˜python -m pip install [package-name]â€™. More assistance is available with the respective package docs.</p>
&#13;

<h2>Declare Functions</h2>
<p>Letâ€™s define a function to streamline the addition specified percentiles and major and minor gridlines to our plots.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">weighted_percentile</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">perc</span><span class="p">):</span>                 <span class="c1"># calculate weighted percentile </span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> 
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> 
    <span class="n">cdf</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">perc</span><span class="p">,</span> <span class="n">cdf</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="c1"># Function from iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049</span>

<span class="k">def</span> <span class="nf">histogram_bounds</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">color</span><span class="p">):</span>                   <span class="c1"># add uncertainty bounds to a histogram          </span>
    <span class="n">p10</span> <span class="o">=</span> <span class="n">weighted_percentile</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="mf">0.1</span><span class="p">);</span> <span class="n">avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">);</span> <span class="n">p90</span> <span class="o">=</span> <span class="n">weighted_percentile</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="mf">0.9</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">p10</span><span class="p">,</span><span class="n">p10</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">45</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">'dashed'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">avg</span><span class="p">,</span><span class="n">avg</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">45</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">p90</span><span class="p">,</span><span class="n">p90</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">45</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">'dashed'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add_grid</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks </span>

<span class="k">def</span> <span class="nf">display_sidebyside</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>                                <span class="c1"># display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)</span>
    <span class="n">html_str</span> <span class="o">=</span> <span class="s1">''</span>
    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="n">html_str</span> <span class="o">+=</span> <span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="o">.</span><span class="n">to_html</span><span class="p">()</span>  <span class="c1"># Using .head() for the first few rows</span>
    <span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s1">'&lt;div style="display: flex;"&gt;</span><span class="si">{</span><span class="n">html_str</span><span class="si">}</span><span class="s1">&lt;/div&gt;'</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Set the Working Directory</h2>
<p>I always like to do this so I donâ€™t lose files and to simplify subsequent read and writes (avoid including the full address each time).  Also, in this case make sure to place the required (see below) data file in this working directory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1">#os.chdir("C:\PGE337")                                        # set the working directory</span>
</pre></div>
</div>
</div>
</div>
<p>You will have to update the part in quotes with your own working directory and the format is different on a Mac (e.g. â€œ~/PGEâ€).</p>
&#13;

<h2>Loading Tabular Data</h2>
<p>Hereâ€™s the command to load our comma delimited data file in to a Pandasâ€™ DataFrame object.</p>
<p>Letâ€™s load the provided multivariate, spatial dataset â€˜unconv_MV.csvâ€™. This dataset has variables from 1,000 unconventional wells including:</p>
<ul class="simple">
<li><p>density (<span class="math notranslate nohighlight">\(g/cm^{3}\)</span>)</p></li>
<li><p>porosity (volume %)</p></li>
</ul>
<p>Note, the dataset is synthetic.</p>
<p>We load it with the pandas â€˜read_csvâ€™ function into a DataFrame we called â€˜my_dataâ€™ and then preview it to make sure it loaded correctly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">add_error</span> <span class="o">=</span> <span class="kc">True</span>                                              <span class="c1"># add random error to the response feature</span>
<span class="n">std_error</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">;</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">71071</span>

<span class="n">yname</span> <span class="o">=</span> <span class="s1">'Porosity'</span><span class="p">;</span> <span class="n">xname</span> <span class="o">=</span> <span class="s1">'Density'</span>                         <span class="c1"># specify the predictor features (x2) and response feature (x1)</span>
<span class="n">xmin</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">;</span> <span class="n">xmax</span> <span class="o">=</span> <span class="mf">2.5</span>                                        <span class="c1"># set minimums and maximums for visualization </span>
<span class="n">ymin</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span> <span class="n">ymax</span> <span class="o">=</span> <span class="mf">25.0</span>    
<span class="n">xlabel</span> <span class="o">=</span> <span class="s1">'Porosity'</span><span class="p">;</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">'Density'</span>                       <span class="c1"># specify the feature labels for plotting</span>
<span class="n">yunit</span> <span class="o">=</span> <span class="s1">'%'</span><span class="p">;</span> <span class="n">xunit</span> <span class="o">=</span> <span class="s1">'$g/cm^</span><span class="si">{3}</span><span class="s1">$'</span>    
<span class="n">Xlabelunit</span> <span class="o">=</span> <span class="n">xlabel</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span>
<span class="n">ylabelunit</span> <span class="o">=</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span>

<span class="c1">#df = pd.read_csv("Density_Por_data.csv")                     # load the data from local current directory</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="sa">r</span><span class="s2">"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv"</span><span class="p">)</span> <span class="c1"># load the data from my github repo</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">73073</span><span class="p">);</span> <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span> <span class="c1"># extract 30% random to reduce the number of data</span>

<span class="k">if</span> <span class="n">add_error</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>                                         <span class="c1"># method to add error</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>                                 <span class="c1"># set random number seed</span>
    <span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span><span class="n">scale</span><span class="o">=</span><span class="n">std_error</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span> <span class="c1"># add noise</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">_get_numeric_data</span><span class="p">();</span> <span class="n">values</span><span class="p">[</span><span class="n">values</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>   <span class="c1"># set negative to 0 in a shallow copy ndarray</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Train-Test Split</h2>
<p>For simplicity we apply a random train-test split with the train_test_split function from scikit-learn package, model_selection module.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">73073</span><span class="p">)</span> <span class="c1"># train and test split</span>
<span class="c1"># y_train = pd.DataFrame({yname:y_train.values}); y_test = pd.DataFrame({yname:y_test.values}) # optional to ensure response is a DataFrame</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>                         <span class="c1"># features as 1D vectors</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>

<span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                <span class="c1"># features as train and test DataFrames</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Visualize the DataFrame</h2>
<p>Visualizing the DataFrame is useful first check of the data.</p>
<ul class="simple">
<li><p>many things can go wrong, e.g., we loaded the wrong data, all the features did not load, etc.</p></li>
</ul>
<p>We can preview by utilizing the â€˜headâ€™ DataFrame member function (with a nice and clean format, see below).</p>
<ul class="simple">
<li><p>we have a custom function to preview the training and testing DataFrames side-by-side.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="s1">'   Training DataFrame      Testing DataFrame'</span><span class="p">)</span>
<span class="n">display_sidebyside</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span><span class="n">df_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>   Training DataFrame      Testing DataFrame
</pre></div>
</div>
<div class="output text_html"><div style="display: flex;"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Density</th>
      <th>Porosity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>24</th>
      <td>1.778580</td>
      <td>11.426485</td>
    </tr>
    <tr>
      <th>101</th>
      <td>2.410560</td>
      <td>8.488544</td>
    </tr>
    <tr>
      <th>88</th>
      <td>2.216014</td>
      <td>10.133693</td>
    </tr>
    <tr>
      <th>79</th>
      <td>1.631896</td>
      <td>12.712326</td>
    </tr>
    <tr>
      <th>58</th>
      <td>1.528019</td>
      <td>16.129542</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Density</th>
      <th>Porosity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>59</th>
      <td>1.742534</td>
      <td>15.380154</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.404932</td>
      <td>13.710628</td>
    </tr>
    <tr>
      <th>35</th>
      <td>1.552713</td>
      <td>14.131878</td>
    </tr>
    <tr>
      <th>92</th>
      <td>1.762359</td>
      <td>11.154896</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1.885087</td>
      <td>9.403056</td>
    </tr>
  </tbody>
</table></div></div></div>
</div>
&#13;

<h2>Summary Statistics for Tabular Data</h2>
<p>There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames. The describe command provides count, mean, minimum, maximum in a nice data table.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="s1">'     Training DataFrame         Testing DataFrame'</span><span class="p">)</span>
<span class="n">display_sidebyside</span><span class="p">(</span><span class="n">df_train</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s1">'count'</span><span class="p">,</span> <span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'std'</span><span class="p">,</span> <span class="s1">'min'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">]],</span><span class="n">df_test</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s1">'count'</span><span class="p">,</span> <span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'std'</span><span class="p">,</span> <span class="s1">'min'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>     Training DataFrame         Testing DataFrame
</pre></div>
</div>
<div class="output text_html"><div style="display: flex;"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Density</th>
      <th>Porosity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>78.000000</td>
      <td>78.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1.739027</td>
      <td>12.501465</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.302510</td>
      <td>3.428260</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.996736</td>
      <td>3.276449</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2.410560</td>
      <td>21.660179</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Density</th>
      <th>Porosity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>27.000000</td>
      <td>27.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1.734710</td>
      <td>12.380796</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.247761</td>
      <td>2.916045</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.067960</td>
      <td>7.894595</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2.119652</td>
      <td>18.133771</td>
    </tr>
  </tbody>
</table></div></div></div>
</div>
&#13;

<h2>Visualize the Data</h2>
<p>Letâ€™s check the consistency and coverage of training and testing with histograms and scatter plots.</p>
<ul class="simple">
<li><p>check to make sure the train and test data cover the range of possible feature combinations</p></li>
<li><p>ensure we are not extrapolating beyond the training data with the testing cases</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nbins</span> <span class="o">=</span> <span class="mi">20</span>                                                    <span class="c1"># number of histogram bins</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Density'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Porosity'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>                                              <span class="c1"># plot the model</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Porosity vs Density'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="c1">#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf')   </span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c4e4535126491e78be365e71b1c9a03a2d1b7af5ddc9027eed6eced1b9c42f72.png" src="../Images/019f46cd3338e55a8b84bcd57a899945.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/c4e4535126491e78be365e71b1c9a03a2d1b7af5ddc9027eed6eced1b9c42f72.png"/>
</div>
</div>
&#13;

<h2>Linear Regression Model</h2>
<p>Letâ€™s first calculate the linear regression model. We use scikit learn and then extend the same workflow to ridge regression.</p>
<ul class="simple">
<li><p>we are building a model, <span class="math notranslate nohighlight">\(\phi = f(\rho)\)</span>, where <span class="math notranslate nohighlight">\(\phi\)</span> is porosity and <span class="math notranslate nohighlight">\(\rho\)</span> is density.</p></li>
<li><p>we could also say, we have â€œporosity regressed on densityâ€.</p></li>
</ul>
<p>Our model has this specific equation,</p>
<div class="math notranslate nohighlight">
\[
\phi = b_1 \times \rho + b_0
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">linear_reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>                  <span class="c1"># instantiate the model</span>

<span class="n">linear_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span> <span class="c1"># train the model parameters</span>
<span class="n">x_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">y_model_linear</span> <span class="o">=</span> <span class="n">linear_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>   <span class="c1"># predict at the withheld test data </span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot the data, model with model parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_model</span><span class="p">,</span><span class="n">y_model_linear</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Linear Regression'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Linear Regression Model Parameters:'</span><span class="p">,[</span><span class="mf">1.86</span><span class="p">,</span><span class="mi">18</span><span class="p">])</span> <span class="c1"># add the model to the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_1$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">linear_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">17</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_0$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">linear_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Linear Regression Model, Porosity = f(Density)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9068634e78d0710f8ba9632a527aa661f2ca3527b1121d2db728317558851841.png" src="../Images/5bc6600437501d419bf56af2e1dc727d.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/9068634e78d0710f8ba9632a527aa661f2ca3527b1121d2db728317558851841.png"/>
</div>
</div>
<p>You may have noticed the additional reshape operation applied to the predictor feature in the predict function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">y_linear_model</span> <span class="o">=</span> <span class="n">linear_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>   <span class="c1"># predict at the withheld test data </span>
</pre></div>
</div>
<p>This is needed because scikit-learn assumes more than one predictor feature; therefore, expects a 2D array of samples (rows) and features (columns), but we have only a 1D vector.</p>
<ul class="simple">
<li><p>the reshape operation turns the 1D vector into a 2D vector with only 1 column</p></li>
</ul>
&#13;

<h2>Linear Regression Model Checks</h2>
<p>Letâ€™s run some quick model checks.  Much more could be done, but I limit this for brevity here.</p>
<ul class="simple">
<li><p>see the Linear Regression chapter for more information and checks</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y_pred_linear</span> <span class="o">=</span> <span class="n">linear_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># predict at test data</span>
<span class="n">r_squared_linear</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_linear</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>                                              <span class="c1"># plot testing diagnostics </span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_model</span><span class="p">,</span><span class="n">y_model_linear</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Linear Regression'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="c1"># plt.scatter(df_test[xname], y_pred,color='grey',edgecolor='black',s = 40, alpha = 1.0, label = 'predictions',zorder=100)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span> <span class="n">y_pred_linear</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span> <span class="n">y_pred_linear</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'*'</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">320</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Predictions'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">idata</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">]],[</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">yname</span><span class="p">],</span>
                        <span class="n">y_pred_linear</span><span class="p">[</span><span class="n">idata</span><span class="p">]],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">'test $\Delta_</span><span class="si">{y_i}</span><span class="s1">$'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">]],[</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">yname</span><span class="p">],</span>
                        <span class="n">y_pred_linear</span><span class="p">[</span><span class="n">idata</span><span class="p">]],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Linear Regression Model Parameters:'</span><span class="p">,[</span><span class="mf">1.86</span><span class="p">,</span><span class="mi">18</span><span class="p">])</span> <span class="c1"># add the model to the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_1$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">linear_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">17</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_0$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">linear_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$r^2$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">r_squared_linear</span><span class="p">,</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">15</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Linear Regression Model, Porosity = f(Density)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">])</span>

<span class="n">y_res_linear</span> <span class="o">=</span> <span class="n">y_pred_linear</span> <span class="o">-</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">'Porosity'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>     <span class="c1"># calculate the test residual</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y_res_linear</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Error Residual at Testing Data"</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' True - Estimate (%)'</span><span class="p">);</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Test Error Residual:'</span><span class="p">,[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mf">4.7</span><span class="p">])</span> <span class="c1"># add residual summary statistics</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$\overline{\Delta</span><span class="si">{y}</span><span class="s1">}$: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">y_res_linear</span><span class="p">),</span><span class="mi">2</span><span class="p">)),[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mf">4.4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$\sigma_{\Delta</span><span class="si">{y}</span><span class="s1">}$: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y_res_linear</span><span class="p">),</span><span class="mi">2</span><span class="p">)),[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mf">4.1</span><span class="p">])</span>
<span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d10062259023c26d588429dde18deab7d42abe383ae44ad7fa6d4c7b96ef620e.png" src="../Images/208a933176542d3185b7f4f4c9a1bf61.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/d10062259023c26d588429dde18deab7d42abe383ae44ad7fa6d4c7b96ef620e.png"/>
</div>
</div>
&#13;

<h2>Ridge Regression Model</h2>
<p>Letâ€™s replace the scikit-learn linear regression method with the scikit-learn ridge regression method.</p>
<ul class="simple">
<li><p>note, we must now set the <span class="math notranslate nohighlight">\(\lambda\)</span> hyperparameter.</p></li>
<li><p>in scikit-learn the hyperparameter(s) is(are) set with the instantiation of the model</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">lam</span> <span class="o">=</span> <span class="mf">13.0</span>                                                     <span class="c1"># lambda hyperparameter</span>

<span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>                                  <span class="c1"># instantiate the model</span>

<span class="n">ridge_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span> <span class="c1"># train the model parameters</span>
<span class="n">x_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">y_model_ridge</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>      <span class="c1"># predict with the fit model</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot the data, model with model parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_model</span><span class="p">,</span><span class="n">y_model_ridge</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Linear Regression'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Ridge Regression Model Parameters:'</span><span class="p">,[</span><span class="mf">1.86</span><span class="p">,</span><span class="mi">18</span><span class="p">])</span> <span class="c1"># add the model to the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_1$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">17</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_0$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Ridge Model, Regression of '</span> <span class="o">+</span> <span class="n">yname</span> <span class="o">+</span> <span class="s1">' on '</span> <span class="o">+</span> <span class="n">xname</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">' with a $\lambda = $'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">lam</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/61654e5169f1e8d42162279fae72b88a9d9108ce66bb01b9fd227b101c160daf.png" src="../Images/05ecf578788df580a4c1f1f25e9ffcb9.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/61654e5169f1e8d42162279fae72b88a9d9108ce66bb01b9fd227b101c160daf.png"/>
</div>
</div>
<p>Letâ€™s repeat the simple model checks that we applied with our linear regression model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y_pred_ridge</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># predict at test data</span>
<span class="n">r_squared</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_ridge</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>                                              <span class="c1"># plot testing diagnostics </span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_model</span><span class="p">,</span><span class="n">y_model_ridge</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Linear Regression'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span> <span class="n">y_pred_ridge</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span> <span class="n">y_pred_ridge</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'*'</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">320</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'predictions'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">idata</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">]],[</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">yname</span><span class="p">],</span>
                        <span class="n">y_pred_ridge</span><span class="p">[</span><span class="n">idata</span><span class="p">]],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">'test $\Delta_</span><span class="si">{y_i}</span><span class="s1">$'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">xname</span><span class="p">]],[</span><span class="n">df_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="n">yname</span><span class="p">],</span>
                        <span class="n">y_pred_ridge</span><span class="p">[</span><span class="n">idata</span><span class="p">]],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Ridge Regression Model Parameters:'</span><span class="p">,[</span><span class="mf">1.81</span><span class="p">,</span><span class="mi">18</span><span class="p">])</span> <span class="c1"># add the model to the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_1$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">17</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_0$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Ridge Model, Regression of '</span> <span class="o">+</span> <span class="n">yname</span> <span class="o">+</span> <span class="s1">' on '</span> <span class="o">+</span> <span class="n">xname</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">' with a $\lambda = $'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">lam</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">])</span>

<span class="n">y_res_ridge</span> <span class="o">=</span> <span class="n">y_pred_ridge</span> <span class="o">-</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">'Porosity'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>       <span class="c1"># calculate the test residual</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y_res_ridge</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Error Residual at Testing Data"</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' True - Estimate (%)'</span><span class="p">);</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">y_res_ridge</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Residual Average = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">y_res_ridge</span><span class="p">),</span><span class="mi">2</span><span class="p">)),[</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">y_res_ridge</span><span class="p">)</span><span class="o">+</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">2.5</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Test Error Residual:'</span><span class="p">,[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mf">4.7</span><span class="p">])</span> <span class="c1"># add residual summary statistics</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$\overline{\Delta</span><span class="si">{y}</span><span class="s1">}$: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">y_res_ridge</span><span class="p">),</span><span class="mi">2</span><span class="p">)),[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mf">4.4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$\sigma_{\Delta</span><span class="si">{y}</span><span class="s1">}$: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y_res_ridge</span><span class="p">),</span><span class="mi">2</span><span class="p">)),[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mf">4.1</span><span class="p">])</span>
<span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bd6d04e063283738954b9b88fa6ed77d11eeec140126dcf4052ef6d9523d2c48.png" src="../Images/2e6dac5a06cdc0e91c454e5758d2a7af.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/bd6d04e063283738954b9b88fa6ed77d11eeec140126dcf4052ef6d9523d2c48.png"/>
</div>
</div>
<p>Interesting, we explained less variance and have a larger residual standard deviation (more error).</p>
<ul class="simple">
<li><p>ridge regression for our arbitrarily selected hyperparameter, <span class="math notranslate nohighlight">\(\lambda\)</span>, actually reduced both testing variance explained and accuracy</p></li>
<li><p>this is not surprising, we are not actually tuning the hyperparameter to get the best model!</p></li>
</ul>
&#13;

<h2>Investigating the Lambda Hyperparameter</h2>
<p>Letâ€™s loop over multiple lambda values - from 0 to 100 and observe the change in:</p>
<ul class="simple">
<li><p>training and testing, mean square error (MSE) and variance explained</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Arrays to store the results</span>
<span class="n">ncases</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">lamd_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">ncases</span><span class="p">)</span>
<span class="n">x_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">var_explained_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">);</span> <span class="n">var_explained_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">)</span>
<span class="n">mse_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">);</span> <span class="n">mse_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ilam</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">)):</span>                           <span class="c1"># Loop over all lambda values</span>
    <span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lamd_mat</span><span class="p">[</span><span class="n">ilam</span><span class="p">])</span>
    <span class="n">ridge_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span> <span class="c1"># fit model</span>

    <span class="n">y_model</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>        <span class="c1"># predict with the fit model  </span>
    <span class="n">y_pred_train</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># predict with the fit model   </span>
    <span class="n">var_explained_train</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span>
    <span class="n">mse_train</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span> 
    
    <span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">var_explained_test</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>
    <span class="n">mse_test</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>    
   
    <span class="k">if</span> <span class="n">ilam</span> <span class="o">&lt;=</span> <span class="mi">7</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">ilam</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_model</span><span class="p">,</span><span class="n">y_model</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Linear Regression'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">],</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Ridge Regression Model Parameters:'</span><span class="p">,[</span><span class="mf">1.86</span><span class="p">,</span><span class="mi">18</span><span class="p">])</span> <span class="c1"># add the model to the plot</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_1$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">17</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$b_0$ :'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="mi">2</span><span class="p">)),[</span><span class="mf">1.97</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Ridge Model, Regression of '</span> <span class="o">+</span> <span class="n">yname</span> <span class="o">+</span> <span class="s1">' on '</span> <span class="o">+</span> <span class="n">xname</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">' with a $\lambda = $'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">[</span><span class="n">ilam</span><span class="p">],</span><span class="mi">4</span><span class="p">)))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">xunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">yname</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">4.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2a222870bfcb75734199aa94ba43bb3842ecda84cd1bf44eee6b1daf798f4199.png" src="../Images/5a585689234bfbaf29513933357bc5cb.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/2a222870bfcb75734199aa94ba43bb3842ecda84cd1bf44eee6b1daf798f4199.png"/>
</div>
</div>
<p>We can observe from the first 8 cases above of the trained ridge regression model that increase in the <span class="math notranslate nohighlight">\(\lambda\)</span> hyperparameter decreases the slope of the linear fit.</p>
<p>Letâ€™s plot the mean squaure error and variance explained over train and test datasets.</p>
<p>Recall, the variance explained, <span class="math notranslate nohighlight">\(R^2\)</span> , is given by,</p>
<div class="math notranslate nohighlight">
\[
R^2 = 1 - \frac{\text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(SS_{\text{residual}}\)</span> is the sum of squares of residuals (or errors), and <span class="math notranslate nohighlight">\(SS_{\text{total}}\)</span> is the total sum of squares (the variance of the observed data).</p>
<p>and the Mean Squared Error (MSE) is given by,</p>
<div class="math notranslate nohighlight">
\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the actual value, <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted value, and <span class="math notranslate nohighlight">\(n\)</span> is the number of data points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">ncases</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">lamd_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">ncases</span><span class="p">)</span>
<span class="n">x_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">var_explained_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">);</span> <span class="n">var_explained_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">)</span>
<span class="n">mse_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">);</span> <span class="n">mse_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ncases</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ilam</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">)):</span>                           <span class="c1"># Loop over all lambda values</span>
    <span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lamd_mat</span><span class="p">[</span><span class="n">ilam</span><span class="p">])</span>
    <span class="n">ridge_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span> <span class="c1"># fit model</span>

    <span class="n">y_model</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>        <span class="c1"># predict with the fit model  </span>
    <span class="n">y_pred_train</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># predict with the fit model   </span>
    <span class="n">var_explained_train</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span>
    <span class="n">mse_train</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span> 
    
    <span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">var_explained_test</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>
    <span class="n">mse_test</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">,</span> <span class="n">var_explained_train</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">,</span> <span class="n">var_explained_test</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Variance Explained vs. Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Variance Explained'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="mf">10000.</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">minorticks_on</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">':'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>  

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">,</span> <span class="n">mse_train</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamd_mat</span><span class="p">,</span> <span class="n">mse_test</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'MSE vs. Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Mean Square Error'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="mf">10000.</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">15.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">minorticks_on</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">':'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>  

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/eaa10f0fe491c5af34e0480460db637c0b45df49bd3270493df83ecce6357f75.png" src="../Images/cebb95f748df4c664c9a127975a7c87b.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/eaa10f0fe491c5af34e0480460db637c0b45df49bd3270493df83ecce6357f75.png"/>
</div>
</div>
<p>We observe that as we increase the lambda parameter the variance explained decreases and the mean square error increases.</p>
<ul class="simple">
<li><p>this makes sense as the data has a consistent linear trend and as the slope â€˜shrinksâ€™ to zero the error increases and the variance explained decreases</p></li>
<li><p>there could be other cases where the reduced slope actually performs better in testing. For example with sparse and noisy data.</p></li>
</ul>
&#13;

<h2>Model Variance</h2>
<p>Now letâ€™s explore the concept of model variance, an important part of machine learning accuracy in testing.</p>
<ul class="simple">
<li><p>the sensitivity of the model to the specific training data</p></li>
<li><p>as <span class="math notranslate nohighlight">\(\lambda\)</span> increases the sensitivity to the training data, model variance decreases</p></li>
</ul>
<p>Letâ€™s demonstrate this with this workflow:</p>
<ul class="simple">
<li><p>loop over multiple lambda values</p>
<ul>
<li><p>loop over multiple bootstrap samples of the data</p>
<ul>
<li><p>calculate the ridge regression fit (slope)</p></li>
</ul>
</li>
<li><p>calculate the variance of these bootstrap results</p></li>
</ul>
</li>
</ul>
<p>WARNING: this will take several minutes to run</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">L</span> <span class="o">=</span> <span class="mi">200</span>                                                       <span class="c1"># the number of bootstrap realizations </span>

<span class="n">nsamples</span> <span class="o">=</span> <span class="mi">20</span>                                                 <span class="c1"># the number of samples in each bootstrap realization</span>
<span class="n">nlambda</span> <span class="o">=</span> <span class="mi">100</span>                                                 <span class="c1"># number of lambda values to evaluate</span>

<span class="n">coef_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>                                        <span class="c1"># declare arrays to store the results</span>
<span class="n">variance_coef</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nlambda</span><span class="p">)</span>

<span class="c1">#lamd_mat = np.linspace(0.0,100.0,nlambda) </span>
<span class="n">lambda_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">nlambda</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ilam</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">lambda_mat</span><span class="p">)):</span>                         <span class="c1"># loop over all lambda values                   </span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>                                     <span class="c1"># loop over all bootstrap realizations</span>
        <span class="n">df_sample</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="n">nsamples</span><span class="p">)</span>                   <span class="c1"># random sample (1 bootstrap)</span>
        <span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lambda_mat</span><span class="p">[</span><span class="n">ilam</span><span class="p">])</span>             <span class="c1"># instantiate model</span>
        <span class="n">ridge_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_sample</span><span class="p">[</span><span class="n">xname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">nsamples</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">df_sample</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span> <span class="c1"># fit model</span>
        <span class="n">coef_mat</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>                      <span class="c1"># get the slope parameter</span>
    <span class="n">variance_coef</span><span class="p">[</span><span class="n">ilam</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">coef_mat</span><span class="p">)</span>                    <span class="c1"># calculate the variance of the slopes over the L bootstraps</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_mat</span><span class="p">,</span> <span class="n">variance_coef</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Slope Variance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Model Variance vs. Lambda Hyperparameter'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Model Variance - Variance of the Slope Parameter, $b_1$'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">100000.</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'both'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5e13984f63c906b044e58c06c1851dc67ab928545f9bfb881f3e02ebb1002cf1.png" src="../Images/42462583008a06affc490158f6f84d94.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/5e13984f63c906b044e58c06c1851dc67ab928545f9bfb881f3e02ebb1002cf1.png"/>
</div>
</div>
<p>The result is as expected, with increase in <span class="math notranslate nohighlight">\(\lambda\)</span> hyperparameter the sensitivity of the model to the training data is decreased.</p>
&#13;

<h2>k-fold Cross Validation</h2>
<p>It would be useful to conduct a complete k-fold validation to evaluate the testing error vs. the hyperparameter lambda for model tuning.</p>
<ul class="simple">
<li><p>the following code is provided to do this</p></li>
<li><p>once again, with a single predictor feature we must reshape to a 2D array</p></li>
</ul>
<p>We loop over 100 <span class="math notranslate nohighlight">\(\lambda\)</span> values from 0.01 to 100,000,</p>
<ul class="simple">
<li><p>get the negative mean square error for each of the 4 k-folds</p></li>
<li><p>then we take the average and apply the absolute value</p></li>
</ul>
<p>Why work with negative mean square error? Simple, to use the functionality in scikit-learn that optimizes by maximization and for consistency with other scores like <span class="math notranslate nohighlight">\(r^2\)</span> where larger values are better.</p>
<ul class="simple">
<li><p>I find negative MSE confusing, so for plotting I use the absolute value to convert the values to strictly positive.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">score</span> <span class="o">=</span> <span class="p">[]</span>                                                    <span class="c1"># code modified from StackOverFlow by Dimosthenis</span>

<span class="n">nlambda</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">lambda_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">nlambda</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ilam</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nlambda</span><span class="p">):</span>
    <span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lambda_mat</span><span class="p">[</span><span class="n">ilam</span><span class="p">])</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">ridge_reg</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Density'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">'Porosity'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s2">"neg_mean_squared_error"</span><span class="p">)</span> <span class="c1"># Perform 10-fold cross validation</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_mat</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Test MSA'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Ridge Regression Test Mean Square Error vs. Lambda Hyperparameter'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Test Mean Square Error'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">1.0e-2</span><span class="p">,</span><span class="mf">1.0e5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="mf">20.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Linear Regression'</span><span class="p">,[</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">12.5</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Mean'</span><span class="p">,[</span><span class="mi">1100</span><span class="p">,</span><span class="mf">14.5</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span><span class="mi">100000</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'both'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2e65418d331d427736b40372288b8cec15d2e1bd38e995d14e8e51014c0e10d7.png" src="../Images/8eb67ef97a23db1eb0ad78518b9e99c2.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/2e65418d331d427736b40372288b8cec15d2e1bd38e995d14e8e51014c0e10d7.png"/>
</div>
</div>
<p>From the above we observe 3 phases,</p>
<ul class="simple">
<li><p>left - the test MSE levels out at a minimum, the model has converged on linear regression.</p></li>
<li><p>right - the test MSE levels out at a maximum, the model has converged on the predictor feature mean, i.e., model parameter slope is 0.0.</p></li>
<li><p>center - transitional between both cases</p></li>
</ul>
<p>In this case we see that the linear regression model (<span class="math notranslate nohighlight">\(\lambda = 0.0\)</span>) is the best model! If ridge regression is the optimum model the test mean square error would minimize between linear regression and mean.</p>
<ul class="simple">
<li><p>this commonly occurs for datasets with issues with noise, data paucity and high dimensionality.</p></li>
</ul>
<p>our model is the same as linear regression.</p>
<ul class="simple">
<li><p>could we create a situation where the best model is not linear regression? I.e., were regularization is helpful?</p></li>
<li><p>yes, we can. Letâ€™s remove most the samples to create data paucity and add a lot of noise!</p></li>
</ul>
<p>Admittedly, I iterated the random seeds for the sample and noise to get this result.</p>
<ul class="simple">
<li><p>few data (low <span class="math notranslate nohighlight">\(n\)</span>) and high dimensionality (high <span class="math notranslate nohighlight">\(m\)</span>) will generally result in LASSO outperforming linear regression</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df_sample</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">noise_stdev</span> <span class="o">=</span> <span class="mf">3.0</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">df_sample</span><span class="p">[</span><span class="s1">'Porosity'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_sample</span><span class="p">[</span><span class="s1">'Porosity'</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">noise_stdev</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">df_sample</span><span class="p">))</span>

<span class="n">score</span> <span class="o">=</span> <span class="p">[]</span>                                                    <span class="c1"># code modified from StackOverFlow by Dimosthenis</span>

<span class="n">nlambda</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">lambda_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">nlambda</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ilam</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nlambda</span><span class="p">):</span>
    <span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lambda_mat</span><span class="p">[</span><span class="n">ilam</span><span class="p">])</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">ridge_reg</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span> <span class="n">df_sample</span><span class="p">[</span><span class="s1">'Density'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
                             <span class="n">y</span><span class="o">=</span><span class="n">df_sample</span><span class="p">[</span><span class="s1">'Porosity'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s2">"neg_mean_squared_error"</span><span class="p">)</span> <span class="c1"># Perform 10-fold cross validation</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_mat</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Test MSA'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Ridge Regression Test Mean Square Error vs. Ridge Hyperparameter'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Lambda'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Test Mean Square Error'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">1.0e-4</span><span class="p">,</span><span class="mf">1.0e3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="mf">20.0</span><span class="p">);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mf">0.07</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Linear Regression'</span><span class="p">,[</span><span class="mf">0.0007</span><span class="p">,</span><span class="mf">12.5</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">'Ridge Tuned $\lambda$'</span><span class="p">,[</span><span class="mf">0.055</span><span class="p">,</span><span class="mf">12.5</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Mean'</span><span class="p">,[</span><span class="mf">7.4</span><span class="p">,</span><span class="mf">14.5</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mf">90.0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mf">0.0001</span><span class="p">,</span><span class="mf">0.001</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">100000</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'both'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ac7782f4fe7198210e3ac58ca9cbea5c3c4e0c30335c332fac08ce3ae63077a9.png" src="../Images/b011c5d0305db3ebe120ee6826faed3c.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/ac7782f4fe7198210e3ac58ca9cbea5c3c4e0c30335c332fac08ce3ae63077a9.png"/>
</div>
</div>
&#13;

<h2>Comments</h2>
<p>This was a basic treatment of ridge regression. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a> and the YouTube lecture links at the start of this chapter with resource links in the videosâ€™ descriptions.</p>
<p>I hope this is helpful,</p>
<p><em>Michael</em></p>
&#13;

<h2>About the Author</h2>
<figure style="text-align: center;">
  <img src="../Images/eb709b2c0a0c715da01ae0165efdf3b2.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/intro/michael_pyrcz_officeshot_jacket.jpg"/>
  <figcaption style="text-align: center;"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.
</figcaption>
</figure>
<p>Michael Pyrcz is a professor in the <a class="reference external" href="https://cockrell.utexas.edu/faculty-directory/alphabetical/p">Cockrell School of Engineering</a>, and the <a class="reference external" href="https://www.jsg.utexas.edu/researcher/michael_pyrcz/">Jackson School of Geosciences</a>, at <a class="reference external" href="https://www.utexas.edu/">The University of Texas at Austin</a>, where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,</p>
<ul class="simple">
<li><p>the principal investigator of the <a class="reference external" href="https://fri.cns.utexas.edu/energy-analytics">Energy Analytics</a> freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin</p></li>
<li><p>an associate editor for <a class="reference external" href="https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board">Computers and Geosciences</a>, and a board member for <a class="reference external" href="https://link.springer.com/journal/11004/editorial-board">Mathematical Geosciences</a>, the International Association for Mathematical Geosciences.</p></li>
</ul>
<p>Michael has written over 70 <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en">peer-reviewed publications</a>, a <a class="reference external" href="https://pypi.org/project/geostatspy/">Python package</a> for spatial data analytics, co-authored a textbook on spatial data analytics, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a> and author of two recently released e-books, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy</a> and <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html">Applied Machine Learning in Python: a Hands-on Guide with Code</a>.</p>
<p>All of Michaelâ€™s university lectures are available on his <a class="reference external" href="https://www.youtube.com/@GeostatsGuyLectures">YouTube Channel</a> with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his <a class="reference external" href="https://github.com/GeostatsGuy">GitHub account</a>, to support any interested students and working professionals with evergreen content. To find out more about Michaelâ€™s work and shared educational resources visit his <span class="xref myst">Website</span>.</p>
&#13;

<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? Iâ€™d be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>Iâ€™m always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
    
</body>
</html>