<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Probability Concepts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Probability Concepts</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_probability.html">https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_probability.html</a></blockquote>

<p>Michael J. Pyrcz, Professor, The University of Texas at Austin</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
<p>Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with Code‚Äù.</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite this e-Book as:</p>
<p>Pyrcz, M.J., 2024, <em>Applied Machine Learning in Python: A Hands-on Guide with Code</em> [e-book]. Zenodo. doi:10.5281/zenodo.15169138 <a class="reference external" href="https://doi.org/10.5281/zenodo.15169138"><img alt="DOI" src="../Images/7e4ea662f44af1eae87e87ecbb962ff4.png" data-original-src="https://zenodo.org/badge/863274676.svg"/></a></p>
</div>
<p>The workflows in this book and more are available here:</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite the MachineLearningDemos GitHub Repository as:</p>
<p>Pyrcz, M.J., 2024, <em>MachineLearningDemos: Python Machine Learning Demonstration Workflows Repository</em> (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312. GitHub repository: <a class="github reference external" href="https://github.com/GeostatsGuy/MachineLearningDemos">GeostatsGuy/MachineLearningDemos</a> <a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.13835312"><img alt="DOI" src="../Images/4e3a59c17d684b06a170c4af84e0f631.png" data-original-src="https://zenodo.org/badge/862519860.svg"/></a></p>
</div>
<p>By Michael J. Pyrcz <br/>
¬© Copyright 2024.</p>
<p>This chapter is a summary of <strong>Probability Concepts</strong> including essential concepts:</p>
<ul class="simple">
<li><p>Motivation and Application to Model Uncertainty</p></li>
<li><p>Approaches to Calculate Probability</p></li>
<li><p>Probability Operators</p></li>
<li><p>Marginal, Conditional and Joint Probability</p></li>
<li><p>Independence Checks</p></li>
<li><p>Bayesian Updating</p></li>
</ul>
<p><strong>YouTube Lecture</strong>: check out my lecture on:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://youtu.be/jl14s8jvXcc?si=TA1YAG_LVWAXMeik">Probability and Statistics</a></p></li>
</ul>
<p>I also have more comprehensive probability content from my Data Analytics and Geostatistics course:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://youtu.be/IGPayWv1BBM?si=K2zI0qBQV3FvJ9fM">Probability</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/NSvyljWT4mw?si=c0HepAkQwLDx3TCb">Frequentist Probability</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/Ppwfr8H177M?si=NYBOi8zTCAxJEpGl">Bayesian Probability</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/kxjnPVyuuo8?si=FI3Tiu72Wc5Neunm">Joint, Marginal, and Conditional Probability</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/kxjnPVyuuo8?si=FI3Tiu72Wc5Neunm">Joint, Marginal, and Conditional Probability</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/D1UKZGOYDOg?si=uFSAB0xLWsr80TYj">Bayesian Coin Example</a></p></li>
</ul>
<p>For convenience here‚Äôs a summary of the salient points.</p>
<section id="motivation-for-probability">
<h2>Motivation for Probability</h2>
<p>Why cover probability at the beginning of an e-book or course on machine learning?</p>
<ol class="arabic simple">
<li><p><strong>Model Formulation</strong> - many of our machine learning models are formulated with probability concepts, for example, naive Bayes classification is derived from Bayes‚Äô Theorem and Bayesian linear regression estimates the probability distributions for our model parameters.</p></li>
<li><p><strong>Data Cleaning and Preparation</strong> - is 90% of any machine learning workflow and we cannot complete these steps without understanding our data distributions and statistics and all of these are based on probability.</p></li>
<li><p><strong>Loss Functions and Optimization</strong> - many of our machine learning models are trained through optimization of a loss function that relies on probability for stochastic steps or even directly in the loss function as is the case for maximum likelihood estimation.</p></li>
<li><p><strong>Tuning Machine Learning Models</strong> - machine learning model tuning to reduce model overfit is based on the concept of expected model performance in the presence of various uncertainty sources, and probability is the language of statistical expectation and uncertainty.</p></li>
<li><p><strong>Machine Learning Model Choice</strong> - You will make choices between frequentist and Bayesian predictive machine learning models and their differences are the result of two distinct approaches to calculate probability and build models.</p></li>
<li><p><strong>Real World Applications</strong> - to make the best choices in machine learning workflows design and to use our results to support decisions we must integrate probability concepts Using our models in the real world.</p></li>
</ol>
<p>Let us build our machine learning skills on a solid foundation of probability!</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Probability</p>
<p>Probability is an essential prerequisite for machine learning.</p>
</div>
<p>Now we get started by defining probability and then we will be ready to talk about ways to calculate it.</p>
</section>
<section id="what-is-probability">
<h2>What is Probability?</h2>
<p>To understand what is probability consider Kolmogorov‚Äôs 3 axioms for probabilities, i.e., the rules that any measure of probability must honor,</p>
<figure style="text-align: center;">
  <img src="../Images/53594a139b9058c07be33e4cff134ea4.png" style="display: block; margin: 0 auto; width: 30%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/kolmogorov.png"/>
  <figcaption style="text-align: center;"> Andrey Kolmogorov (1903 ‚Äì 1987), Soviet mathematician, photo taken 1972 and from image from https://culturemath.ens.fr/thematiques/biographie/life-and-work-kolmogorov).
</figcaption>
</figure>
<ol class="arabic simple">
<li><p><strong>non-negativity</strong> - probability of an event is a non-negative number</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
P(ùê¥) \ge 0
\]</div>
<p>¬†¬†¬†¬†¬† imagine negative probability!</p>
<ol class="arabic simple" start="2">
<li><p><strong>normalization</strong> - probability of the entire sample space is one (unity), also known as probability closure</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
P(\Omega) = 1
\]</div>
<p>¬†¬†¬†¬†¬† something happens!</p>
<ol class="arabic simple" start="3">
<li><p><strong>additivity</strong> - addition of probability of mutually exclusive events for unions</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
P\left(‚ãÉ_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i)
\]</div>
<p>¬†¬†¬†¬†¬† e.g., probability of <span class="math notranslate nohighlight">\(A_1\)</span> and <span class="math notranslate nohighlight">\(A_2\)</span> mutual exclusive events is, <span class="math notranslate nohighlight">\(Prob(A_1 + A_2) = Prob(A_1) + Prob(A_2)\)</span></p>
<ul class="simple">
<li><p>note our concise notation, <span class="math notranslate nohighlight">\(P(\cdot)\)</span>, for probability of event in the bracket occurring.</p></li>
</ul>
<p>This is a good place to start for valid probabilities, we will refine this later. Now we can ask,</p>
</section>
<section id="how-to-calculate-probability">
<h2>How to Calculate Probability?</h2>
<p>There are 3 probability perspectives that can be applied to calculate probabilities,</p>
<ol class="arabic simple">
<li><p><strong>Probability by long-term frequencies</strong> (Frequentist Probability),</p></li>
</ol>
<ul class="simple">
<li><p>probability as ratio of outcomes from an experiment</p></li>
<li><p>requires repeated observations from a controlled experiment</p></li>
</ul>
<p>¬†¬†¬†¬†¬† for example, you flip a coin 100 times and count the number of outcomes with heads <span class="math notranslate nohighlight">\(n(\text{heads})\)</span> and then calculate this ratio,</p>
<div class="math notranslate nohighlight">
\[
P(\text{heads}) = \frac{n(\text{heads})}{n}
\]</div>
<p>¬†¬†¬†¬†¬† this is the frequentist approach to calculate probabilities. The experiment is the set of coin tosses.</p>
<ul class="simple">
<li><p>one issue with frequentist probabilities is, can we now use this probability of heads outside the experiment? For example,</p>
<ul>
<li><p>on another day?</p></li>
<li><p>a different person tossing the coin?</p></li>
<li><p>a different coin?</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Probability by physical tendencies or propensities</strong> (Engineering Probability),</p></li>
</ol>
<ul class="simple">
<li><p>probability calculated from knowledge about the system</p></li>
<li><p>we could know the probability of coin toss outcomes without the experiment</p></li>
</ul>
<p>¬†¬†¬†¬†¬† this is the engineering approach to probability, model the system and use this model, i.e., the physics of the system to calculate probability of outcomes</p>
<ul class="simple">
<li><p>did you know that there is a <span class="math notranslate nohighlight">\(\frac{1}{6,000}\)</span> probability of a coin landing and staying upright on its edge? This could  be lower depending of manner of tossing, coin thickness and characteristics of the surface.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Probability by Degrees of belief</strong> (Bayesian Probability),</p></li>
</ol>
<ul class="simple">
<li><p>first we specify the scientific concept of ‚Äúbelief‚Äù as your opinion based on all your knowledge and experience.</p></li>
<li><p>our model integrates our certainty about a result and data</p></li>
<li><p>this is very flexible, we can assign probability to any event, and includes a framework for updating with new information</p></li>
</ul>
<p>¬†¬†¬†¬†¬† if this is your approach, then you are using the Bayesian approach for probability. If not, before you dismiss this approach let me make a couple arguments,</p>
<ul class="simple">
<li><p>you may be bothered by this idea of belief, as it may seem subjective compared to the probabilities objectively measured from a frequentist‚Äôs experiment but,</p>
<ul>
<li><p>to use the frequentist probability you have to make the subjective decision to apply it outside the experiment, i.e., we need to go beyond a single coin!</p></li>
<li><p>the Bayesian probability approach includes objective probabilities from experiments, but it also allows for integration of our expert knowledge</p></li>
</ul>
</li>
</ul>
</section>
<section id="a-warning-about-calculating-probability">
<h2>A Warning about Calculating Probability</h2>
<p>Statistics can be misused or even abused, leading to flawed conclusions and poor decision-making. When probabilities are calculated improperly or misinterpreted, it can result in significant consequences. Here are a few examples of what can go wrong:</p>
<ol class="arabic simple">
<li><p><strong>Insufficient sampling</strong> - there are various rules of thumb about what constitutes a small sample size, i.e., too small for inference about the population parameters and too small to train a reliable prediction model. Some say 7, some say 30, indubitably there is a minimum sample size.</p></li>
</ol>
<ul class="simple">
<li><p>in general, our models will communicate increasing uncertainty as the number of data decreases, but at some point these models break!</p></li>
<li><p>consider bootstrap, yes as the number of samples decrease the uncertainty in the statistic reported by bootstrap increases, but this uncertainty distribution is centered on the sample statistic! In other words, we need enough samples to have a reasonable estimate of the uncertainty model‚Äôs expectation in the first place.</p></li>
<li><p>of course, there is minimum number of samples to complete a mathematical operation, for example principal components analysis can only calculate <span class="math notranslate nohighlight">\(n-1\)</span> principal components (where <span class="math notranslate nohighlight">\(n\)</span> is the number of samples) and we cannot fit a linear regression model to <span class="math notranslate nohighlight">\(n=1\)</span> samples. Best practice is to stay very far away (have many more samples) from any of these algorithm limitations.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Biased sampling</strong> - in general our probability calculations will not automatically debias the sample data. Any bias in the samples will pass bias through to the probabilities representing our uncertainty model.</p></li>
</ol>
<ul class="simple">
<li><p>for example, a biased sample mean will bias simple kriging estimates away from data. Geostatistical simulation reproduces the entire input feature distribution, so any bias in any part of the distribution will be passed to the spatial models.</p></li>
<li><p>also, there is bad data. When we use Bayesian methods and rely on expert experience, that is not a license to say anything and defend it as belief. On the contrary, we must rigorously document and defend all our choices</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Unskilled practice and a lack of rigor</strong> - there are mistakes that can be made with probabilities and some of them are shockingly common.</p></li>
</ol>
<ul class="simple">
<li><p>our first line of defense is to understand what our methods are doing under the hood! This will help us recognize logical inconsistencies as we build our workflows.</p></li>
<li><p>our second line of defense is to check every step in our workflows. Like an accountant we must close the loops with all our probability calculations, a process that accountants call reconciliation. Like a software engineer we must unit test every operation to ensure we don‚Äôt introduce errors as we update our probability workflows.</p></li>
<li><p>for example, if we perform primary feature cosimulation with the same random number seed as the secondary feature simulation we will introduce artificial correlation between the simulated primary and secondary features that will dramatically change the conditional and joint probabilities.</p></li>
<li><p>in another example, if we use information from the likelihood probabilities to inform the prior probabilities, we will significantly under-estimate the uncertainty. This error is a form of information leakage, descriptively we state it as ‚Äúdouble dipping‚Äù from the information.</p></li>
</ul>
<p>Please remember these warnings as you proceed below and onward as you build your own data science workflows.</p>
</section>
<section id="venn-diagrams">
<h2>Venn Diagrams</h2>
<p>Venn Diagrams are a tool to communicate probability.</p>
<ul class="simple">
<li><p>representing the possible events or outcomes (<span class="math notranslate nohighlight">\(A, B,\ldots\)</span>) of an experiment within the collection of all possible events or outcomes, the sample space (<span class="math notranslate nohighlight">\(\Omega\)</span>).</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/c212b1fc6b3c5da35b7df52769a0af42.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn.png"/>
  <figcaption style="text-align: center;"> Simple example Venn diagram.
</figcaption>
</figure>
<p>What do we learn from this Venn diagram?</p>
<ol class="arabic simple">
<li><p>the relative size of regions for each event within is the relative probability of occurrence, probability of <span class="math notranslate nohighlight">\(B\)</span> is greater than probability of <span class="math notranslate nohighlight">\(A\)</span></p></li>
<li><p>the overlap over events is the probability of joint occurrence, there is 0.0 probability of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> occurring together</p></li>
</ol>
<p>Let us include a more practical Venn diagram to ensure this concept is not too abstract. Here‚Äôs a Venn diagram from 3,000 core samples with interpreted facies</p>
<ul class="simple">
<li><p>the events are sandstone (Sm) and mudstone (Fm)</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/824ce0a24892a79f960068d34d629915.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_rock.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing the facies assignments for 3,000 core samples.
</figcaption>
</figure>
<p>What do we learn from this Venn diagram?</p>
<ul class="simple">
<li><p>mudstone is more likely than sandstone over these core samples</p></li>
<li><p>there are a lot of samples that are neither sandstone nor mudstone, the white space within <span class="math notranslate nohighlight">\(\Omega\)</span></p></li>
<li><p>there are samples that are interpreted as both sandstone and mudstone, i.e., interbedded sandstone-mudstone</p></li>
<li><p>do not forget to draw and label the <span class="math notranslate nohighlight">\(\Omega\)</span> box or we can‚Äôt understand the relative probability of the events</p></li>
<li><p>we can use any convenient shape to represent an event</p></li>
</ul>
<p>In summary, Venn diagrams are an excellent tool to visualize probability, so we will use them to visualize and teach probability here.</p>
</section>
<section id="frequentist-probability">
<h2>Frequentist Probability</h2>
<p>We now provide an extended definition for the frequentist approach for probability. A measure of the likelihood that an event will occur based on frequencies observed from an experiment.</p>
<ul class="simple">
<li><p>For random experiments and well-defined settings, we calculate probability as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A) = \lim_{n \to \infty} \frac{n(A)}{n}
\]</div>
<p>where:</p>
<p><span class="math notranslate nohighlight">\(n(A)\)</span> = number of times event <span class="math notranslate nohighlight">\(A\)</span> occurred
<span class="math notranslate nohighlight">\(n\)</span> = number of trails</p>
<ul class="simple">
<li><p>we use limit notation above to indicate sufficient sampling and that the solution converges and improves accuracy as we introduce more samples from our experiment</p></li>
</ul>
<p>For example,</p>
<ul class="simple">
<li><p>probability of drilling a dry hole, encountering sandstone, and exceeding a rock porosity of <span class="math notranslate nohighlight">\(15\%\)</span> at a location (<span class="math notranslate nohighlight">\(\bf{u}_{\alpha}\)</span>) based on historical results for drilling in this reservoir</p></li>
</ul>
<p>Now we walk-through various probability operations from the frequentist perspective, may I ask for patience from my Bayesian friends as we will later return to the Bayesian perspective for probability.</p>
</section>
<section id="probability-operations">
<h2>Probability Operations</h2>
<p><strong>Union of Events</strong> - for example, all outcomes in the sample space that belong to either event <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span></p>
<ul class="simple">
<li><p>for the union of events operator, we use the word ‚Äúor‚Äù and the mathematics symbol, <span class="math notranslate nohighlight">\(cup\)</span>, using set notation we state the samples in the union <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span> as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A \cup B = \{x: x \in A \text{ or } x \in B\}
\]</div>
<ul class="simple">
<li><p>and the probability notation for a union as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A \cup B)
\]</div>
<p>Here‚Äôs a Venn diagram illustrating the union <span class="math notranslate nohighlight">\(P(A \cup B)\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/01902ece9dac16a02139346ccccf2574.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_union.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing union probability operator for events \(A\) or \(B\). Note, the legend on the top right is included to clarify the plot but is not part of the Venn diagram.
</figcaption>
</figure>
<p><strong>Intersection of Events</strong> - for example, all outcomes in the sample space that belong to both events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span></p>
<ul class="simple">
<li><p>for the intersection of events operator, we use the word ‚Äúand‚Äù and the mathematics symbol <span class="math notranslate nohighlight">\(cap\)</span>, using set notation we state the samples in the intersection <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A \cap B = \{x: x \in A \text{ and } x \in B\}
\]</div>
<ul class="simple">
<li><p>and the probability notation for an intersection as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A \cap B) 
\]</div>
<ul class="simple">
<li><p>or with the common probability shorthand as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B)
\]</div>
<p>¬†¬†¬†¬†¬† we will call this a joint probability later. Here is a Venn diagram illustrating the intersection <span class="math notranslate nohighlight">\(P(A \cap B)\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/55a85b2ac3f3804377f80585821678e0.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_intersection.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing intersection probability operator for events \(A\) and \(B\). Note, the legend on the top right is included to clarify the plot but is not part of the Venn diagram.
</figcaption>
</figure>
<p><strong>Compliment of Events</strong> - for example, all outcomes in the sample space that do not belong to event <span class="math notranslate nohighlight">\(A\)</span></p>
<ul class="simple">
<li><p>for the compliment of event(s) operator we use the word ‚Äúnot‚Äù and the mathematics symbol <span class="math notranslate nohighlight">\(^c\)</span>, using set notation, we state the samples in the compliment of <span class="math notranslate nohighlight">\(A\)</span> as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A^c = \{x: x \notin A\}
\]</div>
<ul class="simple">
<li><p>and the probability notation for a compliment as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A^c) 
\]</div>
<p>Here‚Äôs a Venn diagram illustrating the compliment <span class="math notranslate nohighlight">\(P(A^c)\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/507be2ebc15778352071fdbdd52d2a3c.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_compliment.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing probability compliment operator for events \(A\). Note, the legend on the top right is included to clarify the plot but is not part of the Venn diagram.
</figcaption>
</figure>
<p><strong>Mutually Exclusive Events</strong> - for example, events do not intersect or do not have any common outcomes, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> do not occur at the same time.</p>
<ul class="simple">
<li><p>using set notation, we state events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are mutually exclusive as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A \cap B = \{x: x \in A \text{ and } x \in B \} = \emptyset
\]</div>
<ul class="simple">
<li><p>and the probability notation for mutually exclusive as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A \cap B) = 0.0
\]</div>
<ul class="simple">
<li><p>or with the common probability shorthand as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B) = 0.0
\]</div>
<p>Here‚Äôs a Venn diagram illustrating mutual exclusive events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/c212b1fc6b3c5da35b7df52769a0af42.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing events \(A\) and \(B\) as mutually exclusive.
</figcaption>
</figure>
<p><strong>Exhaustive, Mutually Exclusive Events</strong> the sequence of events whose union is equal to the sample space, all-possible events (<span class="math notranslate nohighlight">\(\Omega\)</span>) and there is no intersection between the events:</p>
<ul class="simple">
<li><p>using set notation, we state events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are exhaustive as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\{x: x \in A \text{ or } x \in B \} = \Omega
\]</div>
<ul class="simple">
<li><p>and events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are mutually exclusive as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\{x: x \in A \text{ and } x \in B \} = \emptyset
\]</div>
<ul class="simple">
<li><p>and the probability notation for exhaustive events as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A \cup B) = 1.0
\]</div>
<ul class="simple">
<li><p>and for mutually exhaustive events as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B) = 0.0
\]</div>
<p>Here‚Äôs a Venn diagram illustrating mutual exclusive, exhaustive events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/02c7a7dba945af2e442593dbf746edb5.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_mutual.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing events \(A\) and \(B\) as exhaustive, mutually exclusive.
</figcaption>
</figure>
<p><strong>Combinations of Operators</strong> - we can use these probability operators with any number of events to communicate complicated probability cases. For example, let us define these events,</p>
<ul class="simple">
<li><p>Event <span class="math notranslate nohighlight">\(A\)</span>: oil present (<span class="math notranslate nohighlight">\(A^c\)</span>: dry hole)</p></li>
<li><p>Event <span class="math notranslate nohighlight">\(B\)</span>: ùëÜùëö (<span class="math notranslate nohighlight">\(B^c\)</span>: ùêπùëö)</p></li>
<li><p>Event <span class="math notranslate nohighlight">\(C\)</span>: porosity ‚â• 15% (<span class="math notranslate nohighlight">\(C^c\)</span>: porosity &lt; 15%)</p></li>
</ul>
<p>What is the probability of dry hole with massive sandstone (ùëÜùëö) and porosity ‚â• 15%?</p>
<div class="math notranslate nohighlight">
\[
P(A^c \cap B \cap C) = \frac{\text{Area}(A^c \cap B \cap C)}{\text{Area}(\Omega)}
\]</div>
<p>Here‚Äôs a Venn diagram representing this case.</p>
<figure style="text-align: center;">
  <img src="../Images/fe9499a30db96f497a94f9d209947cec.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_complicated.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing a more complicated case with 3 events, \(A, B, C\), with compliment and intersection probability operators.
</figcaption>
</figure>
</section>
<section id="constraints-on-probability">
<h2>Constraints on Probability</h2>
<p>Now that we have defined probability notation and probability operations, now we return the idea of permissible probability values expressed by Kolmogorov. We can now build on Kolmogorov‚Äôs probability axioms with this set of probability constraints,</p>
<p>Non-negativity, Normalization constraints include,</p>
<ul class="simple">
<li><p>Probability is bounded,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
0.0 \le P(A) \le 1.0
\]</div>
<p>¬†¬†¬†¬†¬† probability must be between 0.0 and 1.0 (including 0.0 and 1.0)</p>
<ul class="simple">
<li><p>Probability closure,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(\Omega) = 1.0
\]</div>
<p>¬†¬†¬†¬†¬† probability of any event is 1.0</p>
<div class="math notranslate nohighlight">
\[
P(A) + P(A^c) = 1.0 
\]</div>
<p>¬†¬†¬†¬†¬† probability of A or not A is 1.0</p>
<ul class="simple">
<li><p>Probability for null sets,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(\emptyset) = 0.0
\]</div>
<p>¬†¬†¬†¬†¬† probability of nothing happens is zero</p>
<p>Now we use these probability concepts and notation to append more essential complicated probability concepts.</p>
</section>
<section id="probability-addition-rule">
<h2>Probability Addition Rule</h2>
<p>What is the probability for a union of events? Remember that union is an ‚Äúor‚Äù operation represented by the <span class="math notranslate nohighlight">\(\cup\)</span> notation. Consider,</p>
<div class="math notranslate nohighlight">
\[
P(A \cup B)
\]</div>
<p>inspection of the previously shown Venn diagram indicates that we can not calculate the union , <span class="math notranslate nohighlight">\(P(A \cup B)\)</span>, by just summing <span class="math notranslate nohighlight">\(P(A)\)</span> and <span class="math notranslate nohighlight">\(P(B)\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/55a85b2ac3f3804377f80585821678e0.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_intersection.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing events \(A\) and \(B\), showing the intersection of \(A\) and \(B\) that will be double counted if we calculate the union and \(A\) or \(B\) as the sum of \(A\) and \(B\). Note, the legend on the top right is included to clarify the plot but is not part of the Venn diagram.
</figcaption>
</figure>
<p>The sum of the probabilities <span class="math notranslate nohighlight">\(P(A)\)</span> and <span class="math notranslate nohighlight">\(P(B)\)</span> will double count the intersection, <span class="math notranslate nohighlight">\(P(A \cap B)\)</span>, so we must subtract it,</p>
<div class="math notranslate nohighlight">
\[
P(A \cup B) = P(A) + P(B) - P(A \cap B)
\]</div>
<p>Yes, there is a general expression for any number of events to calculate the probability of the union,</p>
<ul class="simple">
<li><p>I‚Äôm not going to include it here but suffice it to say that it is a book keeping nightmare given the combinatorial of intersections that can be counted too many times!</p></li>
</ul>
<p>There is a much simpler case. If the events are mutually exclusive then there is no intersection, <span class="math notranslate nohighlight">\(P(A,B) = 0.0\)</span>, and we can just sum the probabilities.</p>
<ul class="simple">
<li><p>for the general case of mutually exclusive for any number of events,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A_i \cap A_j = \emptyset, \quad \forall \quad i \ne j
\]</div>
<p>then we can write this general equation for the addition rule for any number of mutually exclusive events,</p>
<div class="math notranslate nohighlight">
\[
P\left( \bigcup_{i=1}^k A_i \right) = \sum_{i=1}^k P(A_i)
\]</div>
<p>Here‚Äôs a Venn diagram showing 4 mutual exclusive events, <span class="math notranslate nohighlight">\(A_1, A_2, A_3, A_4\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/aecd4b1580232c0f55e753bad39f1ba7.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_addition.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing 4 events, \(A_1, A_2, A_3, A_4\), that are all mutually exclusive. We can calculate the probability of the union of these events as the sum of probabilities of each.
</figcaption>
</figure>
</section>
<section id="conditional-probability">
<h2>Conditional Probability</h2>
<p>What is the probability of an event given another event has occurred? To discuss this, let us get more specific, what is the probability of event <span class="math notranslate nohighlight">\(B\)</span> given event <span class="math notranslate nohighlight">\(A\)</span> has occurred?</p>
<ul class="simple">
<li><p>to express this we use the notation, <span class="math notranslate nohighlight">\(P(B|A)\)</span></p></li>
<li><p>we read this notation as, ‚Äúprobability of B given A‚Äù</p></li>
</ul>
<p>This is an example of a conditional probability. We calculate conditional probability with this equation,</p>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A \cap B)}{P(A)}
\]</div>
<p>This may seem complicated, but we can easily visualize and understand this equation from our Venn diagram.</p>
<figure style="text-align: center;">
  <img src="../Images/7e6c3488677335aca9cd1bdaaa70f6bb.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_conditional1.png"/>
  <figcaption style="text-align: center;"> To calculate conditional probability we "shrink our universe" to the given condition. For the probability of $B$ given $A$ we shrink our Venn diagram to \(A\).
</figcaption>
</figure>
<p>after we shrink our universe, our <span class="math notranslate nohighlight">\(\Omega\)</span> is only event <span class="math notranslate nohighlight">\(A\)</span>! Now we can easily see that the conditional probability is simply the probability of the intersection of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> divided by the probability of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/7b40d3731ffed4bc1939cfc97dfdce13.png" style="display: block; margin: 0 auto; width: 40%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_conditional2.png"/>
  <figcaption style="text-align: center;"> After we "shrink our universe" to the condition, we can see conditional probability equation clearly as the intersection of \(A\) and \(B\) divided by the probability of \(A\).
</figcaption>
</figure>
<p>Now we introduce a couple examples to test our knowledge of conditional probability,</p>
<p><strong>Conditional Probability Example #1</strong> - for this Venn diagram provide the conditional probabilities, <span class="math notranslate nohighlight">\(P(A|B)\)</span> and <span class="math notranslate nohighlight">\(P(B|A)\)</span>, in terms of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/5ca9cd3deee70b9c41f0865eb8f1bc9b.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_conditional_ex1.png"/>
  <figcaption style="text-align: center;"> Venn diagram to check your understanding of conditional probability.
</figcaption>
</figure>
<p>This is simple to solve, the answer is 0.0 for both conditional probability, because to calculate conditional probability the numerator is the probability, <span class="math notranslate nohighlight">\(P(A,B)\)</span>, and this is 0.0 if there is no overlap in the Venn diagram.</p>
<ul class="simple">
<li><p>also, since <span class="math notranslate nohighlight">\(P(B,A) = P(B,A)\)</span> the numerator is the same for both conditional probabilities,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B,A)}{P(B)} = \frac{0.0}{P(B)} = 0.0
\]</div>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A,B)}{P(A)} = \frac{0.0}{P(A)} = 0.0
\]</div>
<p>Now we attempt a more challenging example.</p>
<p><strong>Conditional Probability Example #2</strong> - for this Venn diagram provide the conditional probabilities, <span class="math notranslate nohighlight">\(P(A|B)\)</span> and <span class="math notranslate nohighlight">\(P(B|A)\)</span>, in terms of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/97292e6e32b997818928d4ab73818c14.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_conditional_ex2.png"/>
  <figcaption style="text-align: center;"> Venn diagram to check your understanding of conditional probability.
</figcaption>
</figure>
<p>This one is more interesting. Let us look at each conditional probability one at a time.</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B,A)}{P(B)}
\]</div>
<p>notice that <span class="math notranslate nohighlight">\(P(B,A) = P(B)\)</span> since all <span class="math notranslate nohighlight">\(B\)</span> is inside <span class="math notranslate nohighlight">\(A\)</span>, so we can substitute <span class="math notranslate nohighlight">\(P(B)\)</span> for <span class="math notranslate nohighlight">\(P(B,A)\)</span> above,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B)}{P(B)} = 1.0
\]</div>
<p>this makes sense, given <span class="math notranslate nohighlight">\(B\)</span> then <span class="math notranslate nohighlight">\(A\)</span> must occur, the conditional probability is 1.0. Now we calculate the other conditional probability,</p>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A,B)}{P(A)} = \frac{P(B,A)}{P(A)} 
\]</div>
<p>just a reminder that <span class="math notranslate nohighlight">\(P(B,A) = P(A,B)\)</span> so we can use our previous result and substitute, <span class="math notranslate nohighlight">\(P(B)\)</span> for <span class="math notranslate nohighlight">\(P(B,A)\)</span> above.</p>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A,B)}{P(A)} = \frac{P(B,A)}{P(A)} = \frac{P(B)}{P(A)}
\]</div>
<p>If you understand these two examples, then you have a good foundation in conditional probability,</p>
<ul class="simple">
<li><p>and I can challenge you with a more complicated without conditional probabilities and learn an interesting general case!</p></li>
<li><p>let us take this Venn diagram with 3 events (<span class="math notranslate nohighlight">\(A, B, C\)</span>) and all possible event intersections labeled.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/63615b4387d8a1e2080ddc3fc29f841c.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_conditional3.png"/>
  <figcaption style="text-align: center;"> A more complicated case of conditional probability dealing with 3 events, \(A, B, C\).
</figcaption>
</figure>
<p>now we calculate the conditional probability,</p>
<div class="math notranslate nohighlight">
\[
P(C|B \cap A) = \frac{P(A \cap B \cap C}{P(A \cap B}
\]</div>
<p>Recalling the definition of conditional probability,</p>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A \cap B)}{P(A)}
\]</div>
<p>so we can reorder this to get,</p>
<div class="math notranslate nohighlight">
\[
\frac{P(A \cap B)} = P(B|A) \cdot {P(A)}
\]</div>
<p>spoiler alert, later we will define this at the multiplication rule, but for now we can substitute this as the denominator for the conditional probability, <span class="math notranslate nohighlight">\(P(C|B \cap A)\)</span>, to get,</p>
<div class="math notranslate nohighlight">
\[
P(C|B \cap A) = \frac{P(A \cap B \cap C}{P(B|A) \cdot P(A)}
\]</div>
<p>now we can reorder this as,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B \cap C) = P(C|B \cap A) \cdot P(B|A) \cdot P(A)
\]</div>
<p>Do you see the pattern? We can calculate high order probability intersections as a sequential set, product sum of marginal probability and then growing conditional probabilities. Let me state it with words,</p>
<ul class="simple">
<li><p>probability of <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(C\)</span> is the probability of <span class="math notranslate nohighlight">\(A\)</span> times the probability of <span class="math notranslate nohighlight">\(B\)</span> given <span class="math notranslate nohighlight">\(A\)</span>, times the probability of <span class="math notranslate nohighlight">\(C\)</span> given <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
</ul>
<p>In fact, we can generalize this for any number of events as,</p>
<div class="math notranslate nohighlight">
\[
P(A_1 \cap A_2 \cap \ldots \cap A_n) = P(A_n|A_{n-1} \cap \ldots \cap A_1) \cdot P(A_{n-1}|A_{n-2} \cap \ldots \cap A_1) \cdot \ldots \cdot P(A_2|A_1) \cdot P(A_1)
\]</div>
<p>or more compactly,</p>
<div class="math notranslate nohighlight">
\[
P(A_1, A_2, \ldots , A_n) = P(A_n|A_{n-1}, \ldots , A_1) \cdot P(A_{n-1}|A_{n-2}, \ldots , A_1) \cdot \ldots \cdot P(A_2|A_1) \cdot P(A_1)
\]</div>
<p>Are we just showing off now? No! This example teaches us a lot about how conditional probabilities work.</p>
<ul class="simple">
<li><p>also, this is exactly the concept used with sequential Gaussian simulation where we sample sequentially from conditional distributions instead of from the high order joint probability, an impossible task without the sequential approach!</p></li>
</ul>
<p>Now we are ready to summarize marginal, conditional and joint probabilities together.</p>
</section>
<section id="marginal-conditional-and-joint-probabilities">
<h2>Marginal, Conditional and Joint Probabilities</h2>
<p>Now we define marginal, conditional and joint probability, provide notation and discuss how to calculate them:</p>
<p><strong>Marginal Probability</strong> - probability of an event, irrespective of any other event,</p>
<div class="math notranslate nohighlight">
\[
P(A), P(B)
\]</div>
<ul class="simple">
<li><p>marginal probabilities are calculated as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A) = \frac{n(A)}{n(\Omega)}
\]</div>
<ul class="simple">
<li><p>marginal probabilities may be calculated from joint probabilities through the process of marginalization,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A) = \int_{-\infty}^{\infty} P(A,B) dB
\]</div>
<ul class="simple">
<li><p>where we integrate over all cases of the other events, <span class="math notranslate nohighlight">\(B\)</span>, to remove them.</p></li>
<li><p>given discrete cases of event <span class="math notranslate nohighlight">\(B\)</span> we can simply sum the probabilities over all cases of <span class="math notranslate nohighlight">\(B\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A) = \sum_{i=1}^{k_B} P(A,B)
\]</div>
<p><strong>Conditional Probability</strong> - probability of an event, given another event has already occurred,</p>
<div class="math notranslate nohighlight">
\[
P(A \text{ given } B), P(B \text{ given } A)
\]</div>
<ul class="simple">
<li><p>or with compaction probability notation,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A|B), P(B|A)
\]</div>
<ul class="simple">
<li><p>see the previous section for more information, but for symmetry we repeat the method to calculate conditional probability as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A,B)}{P(A)}
\]</div>
<ul class="simple">
<li><p>of course, as we saw in our conditional probability examples above, we cannot switch the order of the events,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(B|A) \ne P(A|B)
\]</div>
<p><strong>Joint Probability</strong> - probability of multiple events occurring together,</p>
<div class="math notranslate nohighlight">
\[
P(A \text{ and } B), P(B \text{ and } A)
\]</div>
<ul class="simple">
<li><p>or with compaction probability notation,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A \cap B), P(B \cap A)
\]</div>
<ul class="simple">
<li><p>or even more compact as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B), P(B,A)
\]</div>
<ul class="simple">
<li><p>of course, the ordering does not matter for joint probabilities,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B) = P(B,A)
\]</div>
<ul class="simple">
<li><p>we calculate joint probabilities as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B) = \frac{n(A,B)}{n(\Omega)}
\]</div>
<p>To clarify the concept of marginal, conditional and joint probability (and distributions), I have coded a set of interactive Python dashboards, <a class="reference external" href="https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Marginal_Joint_Conditional_Probability.ipynb">Interactive_Marginal_Joint_Conditional_Probability</a>,</p>
<ul class="simple">
<li><p>that provides a dataset and interactively calculates and compares marginal, conditional and joint probabilities and visualizes and compares the marginal and conditional distributions.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/ada0a5ff99a013044e2ec05654f7710a.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/interactive_joint.png"/>
  <figcaption style="text-align: center;"> One of the interactive Python dashboards from my series on marginal, conditional and joint probability and distributions.
</figcaption>
</figure>
</section>
<section id="probability-multiplication-rule">
<h2>Probability Multiplication Rule</h2>
<p>As shown above, we can calculate the joint probability of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> as the product of the conditional probability of <span class="math notranslate nohighlight">\(B\)</span> given <span class="math notranslate nohighlight">\(A\)</span> with the marginal probability of <span class="math notranslate nohighlight">\(A\)</span>,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B) = P(A,B) = P(B|A) \cdot P(A)
\]</div>
<p>and of course, we can also state,</p>
<div class="math notranslate nohighlight">
\[
P(B \cap A) = P(B,A) = P(A|B) \cdot P(B)
\]</div>
<p>this is know as the multiplication rule, we use the multiplication rule to develop the definition of independence.</p>
</section>
<section id="independent-events">
<h2>Independent Events</h2>
<p>Given the probability multiplication rule,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B) = P(A,B) = P(B|A) \cdot P(A)
\]</div>
<p>now we ask the question, what is the conditional probability <span class="math notranslate nohighlight">\(P(B|A)\)</span> if events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent? Given independence between <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> we would expect that knowing about <span class="math notranslate nohighlight">\(A\)</span> will not impact the outcome of <span class="math notranslate nohighlight">\(B\)</span>,</p>
<div class="math notranslate nohighlight">
\[
P(B|A) = P(B)
\]</div>
<p>if we substitute this into <span class="math notranslate nohighlight">\(P(A \cap B) = P(A,B) = P(B|A) \cdot P(A)\)</span> we get,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B) = P(B) \cdot P(A)
\]</div>
<p>by the same logic we can show,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = P(A)
\]</div>
<p>Now we can summarize this as, events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent if and only if the following relations are true,</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(P(A \cap B) = P(A) \cdot P(B)\)</span> - joint probability is the product of the marginal probabilities</p></li>
<li><p><span class="math notranslate nohighlight">\(P(A|B) = P(A)\)</span> - conditional is the marginal probability</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|A) = P(B)\)</span> - conditional probability is the marginal probability</p></li>
</ol>
<p>If any of these are violated, we can suspect that there exists some form of relationship.</p>
<ul class="simple">
<li><p>we leave the significance of this result outside the scope for this chapter on probability</p></li>
<li><p>certainly, if we can assume independence this simplifies our data science workflows</p></li>
</ul>
<p><strong>Independence Example #1</strong> - check independence for the following dataset.</p>
<figure style="text-align: center;">
  <img src="../Images/222dd21803448a6193e3a86b018195b1.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/ind_table.png"/>
  <figcaption style="text-align: center;"> If event \(A_1\) is Fm facies in Middle Unit (blue circles) and event \(A_2\) is Sm facies in Bottom Unit (red circles), and joint cases of event \(A_1\) and \(A_2\) (black rectangles for joint \(A_1\) and \(A_2\) occur together) are \(A_1\) and \(A_2\) independent events?
</figcaption>
</figure>
<p>To check for independence, we calculate the marginal and joint probabilities,</p>
<div class="math notranslate nohighlight">
\[
P(A_1) = \frac{5}{10} \quad P(A_2) = \frac{6}{10} \quad P(A_1,A_2) = \frac{2}{10}
\]</div>
<p>now we check one of the conditions for independence, as the joint probability as product of the marginal probabilities,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A_1 \cap A_2) = P(A_1) \cdot P(A_2)\)</span> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <span class="math notranslate nohighlight">\(0.2 \ne 0.5 \cdot 0.6\)</span>, <span class="math notranslate nohighlight">\(\therefore\)</span> we suspect a relationship between events <span class="math notranslate nohighlight">\(A_1\)</span> and <span class="math notranslate nohighlight">\(A_2\)</span></p></li>
</ul>
</section>
<section id="bayesian-probability">
<h2>Bayesian Probability</h2>
<p>Now we finally dive into the Bayesian approach to probability, a very flexible approach that can be applied to assign probability to anything, and includes a framework for updating with new information.</p>
<p>When I say that the Bayesian approach can assign probability to anything, I realize this needs some further explanation,</p>
<ul class="simple">
<li><p>of course, remember my statements in the ‚ÄúA Warning about Calculating Probability‚Äù section above, there are times when we do not have enough data, and there are workflows that are incorrect.</p></li>
</ul>
<p>I mean that any type of information can be encoded into the Bayesian framework, including belief</p>
<p>Let us discuss this supported by a great example from the introduction chapter of Sivia (1996), ‚ÄúWhat is the Mass of Jupyter?‚Äù.</p>
<figure style="text-align: center;">
  <img src="../Images/b1f7bb75c53f50d876649588c61eed7e.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/Jupiter.png"/>
  <figcaption style="text-align: center;"> Jupyter image from New Horizons Long Range Imager (LORRI), taken at 57 million km on January 2007. Image from https://en.wikipedia.org/wiki/Jupiter#/media/File:Jupiter_New_Horizons.jpg.
</figcaption>
</figure>
<p>What would be frequentist and Bayesian approaches to calculate the probabilities for the uncertainty model of the mass of Jupiter?</p>
<ul class="simple">
<li><p>Frequentist perspective - calculate the cumulative distribution function by measuring the mass of enough Jupiter-like planets from many star systems.</p></li>
<li><p>Bayesian - form a prior probability and update with any available information.</p></li>
</ul>
<p>Now to anyone saying, the frequentist approach is possible, remember the first exoplanets were discovered in 1995 by astronomers Michel Mayor and Didier Queloz.</p>
<ul class="simple">
<li><p>not coincidentally, the first discovered exoplanet was a Jupyter-mass hot, gas giant, as these are easier to see with the Doppler-based wobble method, because near-to-star gas giants have greater pull on their star resulting in a relatively  high amplitude and frequency that can be readily observed.</p></li>
<li><p>when Sivia was authoring his book, there were no known exoplanets.</p></li>
</ul>
<p>So, we didn‚Äôt have other Jupiter like planets to apply the frequentist approach, but we could use our knowledge about the formation of solar systems to formulate a prior model. Then we could use any available observations or measurements from Jupiter to update this prior. We could do something!</p>
<ul class="simple">
<li><p>but I am getting ahead of myself, we must properly introduce the Bayesian approach to probability, by starting with Bayes‚Äô theorem.</p></li>
</ul>
</section>
<section id="bayes-theorem">
<h2>Bayes‚Äô Theorem</h2>
<p>Once again, here‚Äôs the multiplication rule,</p>
<div class="math notranslate nohighlight">
\[
P(B \cup A) = P(B,A) = P(A|B) \cdot P(B)
\]</div>
<p>and it follows that we can also express it as,</p>
<div class="math notranslate nohighlight">
\[
P(A \cup B) = P(A,B) = P(B|A) \cdot P(A)
\]</div>
<p>If follows that,</p>
<div class="math notranslate nohighlight">
\[
P(B \cup A) = P(A \cup B)
\]</div>
<p>therefore, we can substitute the right-hand sides of the multiplication rules above into this equality as,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) \cdot P(B) = P(B|A) \cdot P(A)
\]</div>
<p>this is Bayes‚Äô theorem. We can make a simple modification to get the popular form of Bayes‚Äô theorem,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\]</div>
<p>To better explain Bayes‚Äô theorem, we replace the <span class="math notranslate nohighlight">\(A\)</span> with ‚ÄúModel‚Äù and <span class="math notranslate nohighlight">\(B\)</span> with ‚ÄúNew Data‚Äù, where ‚ÄúNew Data‚Äù is the new data that we are updating with.</p>
<div class="math notranslate nohighlight">
\[
P(\text{Model}|\text{New Data}) = \frac{P(\text{New Data}|\text{Model}) \cdot P(\text{Model})}{P(\text{New Data})}
\]</div>
<p>now we can see Bayesian updating as taking a model and updating it with new data.</p>
<p>Now we make some observations about Bayes‚Äô theorem,</p>
<ol class="arabic simple">
<li><p>We are reversing conditional probabilities to get <span class="math notranslate nohighlight">\(P(A|B)\)</span> from <span class="math notranslate nohighlight">\(P(B|A)\)</span>, this often comes in handy because we readily have access to one but not the other.</p></li>
<li><p>The probabilities in Bayes‚Äô theorem are known as:</p></li>
</ol>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(B)\)</span> - evidence</p></li>
<li><p><span class="math notranslate nohighlight">\(P(A)\)</span> - prior</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|A)\)</span> - likelihood</p></li>
<li><p><span class="math notranslate nohighlight">\(P(A|B)\)</span> - posterior</p></li>
</ul>
<p>¬†¬†¬†¬†¬† we can substitute the probabilities with their labels,</p>
<div class="math notranslate nohighlight">
\[
\text{Posterior} = \frac{\text{Likelihood} \cdot \text{Prior}}{\text{Evidence}}
\]</div>
<ol class="arabic simple" start="3">
<li><p>Prior should have no information from likelihood.</p></li>
</ol>
<ul class="simple">
<li><p>the prior probability is estimated prior to the collection of the new information in the likelihood. If the prior and likelihood includes new information, then this is ‚Äúdouble accounting‚Äù of the new information!</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p>Evidence term is usually just a standardization to ensure probability closure.</p></li>
</ol>
<ul class="simple">
<li><p>evidence probability is often calculated with marginalization,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(B) = \int_{A} P(B|A) \cdot P(A) dA
\]</div>
<ul class="simple">
<li><p>for the example or two mutually exclusive, exhaustive outcomes, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(A^c\)</span>, then this marginalization can be applied to calculate <span class="math notranslate nohighlight">\(P(B)\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(B) = P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)
\]</div>
<ul class="simple">
<li><p>and by substitution we get this expanded but usually easier to calculate form of Bayes‚Äô theorem,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)}
\]</div>
<ul class="simple">
<li><p>we can generalize this marginalization for any number of discrete, mutually exclusive, exhaustive events as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(B) = \sum_{i=1}^{m} P(B|A_i) \cdot P(A_i), \quad \forall \quad i = 1,\ldots,m
\]</div>
<ul class="simple">
<li><p>in some cases, like Markov chain Monte Carlo with the Metropolis-Hastings algorithm, we eliminate the need for the evidence term because we calculate a probability ratio of the proposed step vs. the current state and the evidence probability cancels out.</p></li>
</ul>
<ol class="arabic simple" start="5">
<li><p>If the prior is na√Øve then the posterior is equal to the likelihood. This is logical, if we know nothing prior to collecting the new data then we completely rely on the likelihood probability.</p></li>
</ol>
<ul class="simple">
<li><p>a na√Øve prior is a maximum uncertainty prior, like the uniform probability with all outcomes equal probable</p></li>
<li><p>under the assumption of standard normal, global Gaussian distribution with a mean of 0.0 and standard deviation of 1.0, a na√Øve prior is the standard normal distribution</p></li>
</ul>
<ol class="arabic simple" start="6">
<li><p>If the likelihood probability is na√Øve then the posterior is equal to the prior. Once again, this is logical, if the new data provides no information, then we should continue to rely on the prior probability and the original model is not updated.</p></li>
</ol>
</section>
<section id="bayesian-probability-example-problems">
<h2>Bayesian Probability Example Problems</h2>
<p>An efficient way to improve your understanding of Bayesian probability is through solving illustrative problems. Here‚Äôs a great group of problems,</p>
<ul class="simple">
<li><p>you conduct a test and get a positive test result for something we can‚Äôt directly observe happening, but what is the probability of the thing actually happening given the positive test?</p></li>
</ul>
<p>Here‚Äôs some examples of tests that fit in this class of problems,</p>
<figure style="text-align: center;">
  <img src="../Images/a2624fe69f396e2d3677c62b7a46ec9a.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/tests.png"/>
  <figcaption style="text-align: center;"> Example cases of a tests, the underlying event is happening vs. a test that indicates the underlying event is happening. 
</figcaption>
</figure>
<p>and we can rewrite Bayes‚Äô theorem for the case of one of these tests,</p>
<figure style="text-align: center;">
  <img src="../Images/02aa6aa53eb33b77c1b4114e43091536.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/Bayes_tests.png"/>
  <figcaption style="text-align: center;"> Bayes' theorem rewritten for the case of tests. 
</figcaption>
</figure>
<p><strong>Bayesian Updating Example #1</strong> - prior information at a site suggests a deepwater channel reservoir exists at a given location with a probability of 0.6. We consider further investigation with a 3D seismic survey.</p>
<p>3D seismic survey will indicate a channelized reservoir:</p>
<p>‚Äê is present with 0.9 probability if it really is present</p>
<p>‚Äê is not present with a probability 0.7 if it really is not</p>
<p>We have our test, seismic survey, that will offer new data, and we have our prior information (prior to collecting the new seismic data). Now we define our events,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A\)</span> = the deepwater channel is present, then <span class="math notranslate nohighlight">\(P(A)\)</span> is the probability the thing is happening before collecting the new data</p></li>
<li><p><span class="math notranslate nohighlight">\(B\)</span> = new seismic shows a deepwater channel, then <span class="math notranslate nohighlight">\(P(B)\)</span> is the probability of a positive test for the thing happening</p></li>
</ul>
<p>It follows that the compliments are,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A^c\)</span> = the deepwater channel not present, then <span class="math notranslate nohighlight">\(P(A^c)\)</span> is the probability the thing is not happening before collecting the new data</p></li>
<li><p><span class="math notranslate nohighlight">\(B^c\)</span> = new seismic does not show a deepwater channel, then <span class="math notranslate nohighlight">\(P(B^c)\)</span> is the probability of a negative test for the thing happening</p></li>
</ul>
<p>Now we write out Bayes‚Äô theorem, the regular and expanded evidence term by marginalization,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)}
\]</div>
<p>What do we know? From the question above we have,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A) = 0.6\)</span> - ‚Äúprior information at a site suggests a deepwater channel reservoir exists at a given location with a probability of 0.6‚Äù</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|A) = 0.9\)</span> - ‚Äúis present with 0.9 probability if it really is present‚Äù</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B^c|A^c) = 0.7\)</span> - ‚Äúis not present with a probability 0.7 if it really is not‚Äù</p></li>
</ul>
<p>but we also need <span class="math notranslate nohighlight">\(P(A^c)\)</span> and <span class="math notranslate nohighlight">\(P(B|A^c)\)</span>. We calculate these from closure as,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A^c) = 1.0 - P(A) = 1.0 - 0.6 = 0.4\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|A^c) = 1.0 - P(B^c|A^c) = 1.0 - 0.7 = 0.3\)</span></p></li>
</ul>
<p>we have all the information that we need to substitute,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)} = \frac{0.9 \cdot 0.6}{0.9 \cdot 0.6 + 0.3 \cdot 0.4} = 0.82
\]</div>
<p>Given a positive test, seismic indicating a deepwater channel is present, we have posterior probability of 0.82.</p>
<p>Is seismic data useful? How do we access this?</p>
<ul class="simple">
<li><p>consider the change from prior probability, 0.6, to posterior probability, 0.82, showing a reduction in uncertainty. By integrating the value and potential loss of this decision it is possible to now assign the value of information for seismic data.</p></li>
<li><p>I leave decision analysis out of scope for this e-book, but it is a fascinating topic, and I recommend anyone involved in data science to learn at least the basics to ensure you add value by impacting the decision(s)!</p></li>
</ul>
<p>It is quite instructive to conduct a sensitivity on this example problem to assess the behavior of Bayesian updating. You could download and run my <a class="reference external" href="https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb">interactive Bayesian updating dashboard</a> to accomplish this.</p>
<ul class="simple">
<li><p>it is a custom dashboard to visualize Bayesian updating.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/cdc74f980ae59f1da85ef88b5b1e69d0.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/interactive_Bayesian.png"/>
  <figcaption style="text-align: center;"> My interactive Python dashboard demonstrating Bayesian updating. Note, \(H\) represents the thing is happening (\(A\) in our example), and \(+\) represents a positive test and \(-\) represents a negative test (\(B\) and \(B^c\) respectively in our example). 
</figcaption>
</figure>
<p>We have improved the test precision, probability of seismic detecting a channel given it is present, <span class="math notranslate nohighlight">\(P(B|A)\)</span> from 0.9 to 0.99.</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)} = \frac{0.99 \cdot 0.6}{0.99 \cdot 0.6 + 0.3 \cdot 0.4} = 0.83
\]</div>
<p>we have almost no change in the posterior probability. Now we have improved our ability to detect no channel given the channel is not present, <span class="math notranslate nohighlight">\(P(B|A^c)\)</span> from 0.3 to 0.03.</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)} = \frac{0.9 \cdot 0.6}{0.9 \cdot 0.6 + 0.03 \cdot 0.4} = 0.98
\]</div>
<p>we change the posterior probability from 0.82 to 0.98! What happened?</p>
<ul class="simple">
<li><p>our test was being impacted by a relatively high probability of false positive, seismic showing a channel when there is no channel present, <span class="math notranslate nohighlight">\(P(B|A^c) = 0.3\)</span> paired with a relatively high rate of no channel present, <span class="math notranslate nohighlight">\(P(A^c) = 0.4\)</span>.</p></li>
</ul>
<p>now we switch gears and look at another illustrative example of Bayesian updating with machines on a production line.</p>
<p><strong>Bayesian Updating Example #2</strong> - You have 3 machines making the same product (imagine a large production line). They have different volumes and errors.</p>
<ul class="simple">
<li><p>Note we assume the products are mutually exclusive (only come from a single machine), and exhaustive (all products come from one of the 3 machines).</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/2bbde9fa70803137b7d119b3a7e9636c.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/Bayesian_machines.png"/>
  <figcaption style="text-align: center;"> Machines 1, 2 and 3, produce the same product that are then mixed in the assembly line. They each have different production rates (percentage of total and error rates).
</figcaption>
</figure>
<p>We want to know given an error in an individual product, what is the probability that the product came from a specific machine? First, we define our variables,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y\)</span> = the product has an error</p></li>
<li><p><span class="math notranslate nohighlight">\(X_1, X_2, X_3\)</span> = the product came from machines 1, 2 or 3 respectively.</p></li>
</ul>
<p>our Bayesian formulation for probability of defective (product with error) product coming from a specific machine is,</p>
<div class="math notranslate nohighlight">
\[
P(X_i|Y) = \frac{P(Y|X_i) \cdot P(X_i)}{P(Y)} \quad \forall \quad i = 1, 2, 3
\]</div>
<p>we will need to calculate our evidence term, P(Y) to solve this for any product. Also, we only must do this once, the evidence term is the same for all products.</p>
<p>We will apply marginalization to accomplish this by substituting our variables into the previously introduced general equation as,</p>
<div class="math notranslate nohighlight">
\[
P(Y) = \sum_{i=1}^{m} P(Y|X_i) \cdot P(X_i), \quad \forall \quad i = 1,\ldots,3
\]</div>
<p>our specific form for our problem is,</p>
<div class="math notranslate nohighlight">
\[
P(Y) = P(Y|X_1) \cdot P(X_1) + P(Y|X_2) \cdot P(X_2) + P(Y|X_3) \cdot P(X_3) 
\]</div>
<p>we substitute the probabilities from the problem statement as,</p>
<div class="math notranslate nohighlight">
\[
P(Y) = 0.2 \cdot 0.05 + 0.3 \cdot 0.03 + 0.5 \cdot 0.01 = 0.024
\]</div>
<p>Given this evidence probability, now we can calculate the posterior probabilities of the product coming from each machine given the product is defective,</p>
<div class="math notranslate nohighlight">
\[
P(X_1|Y) = \frac{P(Y|X_1) \cdot P(X_1)}{P(Y)} = \frac{0.05 \cdot 0.2}{0.024} = 0.41
\]</div>
<div class="math notranslate nohighlight">
\[
P(X_2|Y) = \frac{P(Y|X_2) \cdot P(X_2)}{P(Y)} = \frac{0.03 \cdot 0.3}{0.024} = 0.38
\]</div>
<div class="math notranslate nohighlight">
\[
P(X_3|Y) = \frac{P(Y|X_3) \cdot P(X_3)}{P(Y)} = \frac{0.01 \cdot 0.5}{0.024} = 0.21
\]</div>
<p>Let us close the loop by checking closure, we expect these posterior probabilities to sum to 1.0 over all machines (once again assuming products are mutually exclusive, and exhaustive over the machines),</p>
<div class="math notranslate nohighlight">
\[
P(X_1|Y) + P(X_2|Y) + P(X_3|Y) = 0.41 + 0.38 + 0.21 = 1.0
\]</div>
<p>and we have closure.</p>
<p>These two examples are very helpful to understand Bayesian updating. In the classroom I get my students to calculate the additional posteriors such as the probability of the event happening given a negative test, etc.</p>
<ul class="simple">
<li><p>my second dashboard in this Jupyter notebook <a class="reference external" href="https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb">interactive Bayesian updating dashboard</a> includes these examples.</p></li>
<li><p>I invite you to play with Bayesian updating.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/25502cc12d722a9c207d481f3da67a20.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/interactive_Bayesian_all.png"/>
  <figcaption style="text-align: center;"> My more complete interactive Python dashboard demonstrating Bayesian updating for all possible posterior probabilities. Note, \(H\) represents the thing is happening \(A\) in our example, and \(+\) represents a positive test and \(-\) represents a negative test, \(B\) and \(B^c\) respectively in our example. 
</figcaption>
</figure>
</section>
<section id="bayesian-updating-with-gaussian-distributions">
<h2>Bayesian Updating with Gaussian Distributions</h2>
<p>Sivia (1996) provided an analytical approach for Bayesian updating under the assumption of a standard normal global distribution, Gaussian distributed with mean of 0.0 and variance of 1.0. We calculate,</p>
<ul class="simple">
<li><p>the mean of the posterior distribution from the prior and likelihood mean and variance.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mu_{\text{posterior}} = \frac{\mu_{\text{likelihood}} \cdot \sigma^2_{\text{prior}} + \mu_{\text{prior}} \cdot \sigma^2_{\text{likelihood}}}{\left[1.0 ‚àí \sigma^2_{\text{likelihood}} \right] \cdot \left[\sigma^2_{\text{prior}} ‚àí 1.0 \right]+1.0}
\]</div>
<ul class="simple">
<li><p>the variance of the posterior distribution from the prior and likelihood variances</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sigma^2_{\text{posterior}} = \frac{\sigma^2_{\text{prior}} \cdot \sigma^2_{\text{likelihood}}}{\left[1.0 ‚àí \sigma^2_{\text{likelihood}} \right] \cdot \left[\sigma^2_{\text{prior}} ‚àí 1.0 \right]+1.0}
\]</div>
<ul class="simple">
<li><p>consistent with the multivariate Gaussian distribution this posterior is homoscedastic, the prior or likelihood means are not in the equation for posterior variance.</p></li>
<li><p>my third dashboard in this Jupyter notebook <a class="reference external" href="https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb">interactive Bayesian updating dashboard</a> includes Bayesian updating with Gaussian distributions.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/1a20eb3b488e5034f7fbf4ed5cafb8b8.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/interactive_Bayesian_gaussian.png"/>
  <figcaption style="text-align: center;"> My interactive Python dashboard demonstrating Bayesian updating under the assumption of standard normal global distribution.
</figcaption>
</figure>
<p>This example helps my students understand the concept of Bayesian updating for building uncertainty distributions. Here are some things to try out,</p>
<ol class="arabic simple">
<li><p>a na√Øve prior or likelihood and observe the posterior is the same as the likelihood or prior, respectively.</p></li>
<li><p>a very certain, low variance prior. Start with a large prior variance and as you reduce the variance, the posterior is pulled towards the prior. The influence of prior and likelihood is proportional to their relative certainty.</p></li>
<li><p>a contradiction between prior and likelihood with very different means. You may be able to even introduce extrapolation, a low prior positive mean with higher positive likelihood mean can result in an even higher positive posterior mean.</p></li>
</ol>
</section>
<section id="comments">
<h2>Comments</h2>
<p>This was a basic treatment of probability. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a> and the YouTube lecture links at the start of this chapter with resource links in the videos‚Äô descriptions.</p>
<p>I hope this is helpful,</p>
<p><em>Michael</em></p>
</section>
<section id="about-the-author">
<h2>About the Author</h2>
<figure style="text-align: center;">
  <img src="../Images/eb709b2c0a0c715da01ae0165efdf3b2.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/intro/michael_pyrcz_officeshot_jacket.jpg"/>
  <figcaption style="text-align: center;"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.
</figcaption>
</figure>
<p>Michael Pyrcz is a professor in the <a class="reference external" href="https://cockrell.utexas.edu/faculty-directory/alphabetical/p">Cockrell School of Engineering</a>, and the <a class="reference external" href="https://www.jsg.utexas.edu/researcher/michael_pyrcz/">Jackson School of Geosciences</a>, at <a class="reference external" href="https://www.utexas.edu/">The University of Texas at Austin</a>, where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,</p>
<ul class="simple">
<li><p>the principal investigator of the <a class="reference external" href="https://fri.cns.utexas.edu/energy-analytics">Energy Analytics</a> freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin</p></li>
<li><p>an associate editor for <a class="reference external" href="https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board">Computers and Geosciences</a>, and a board member for <a class="reference external" href="https://link.springer.com/journal/11004/editorial-board">Mathematical Geosciences</a>, the International Association for Mathematical Geosciences.</p></li>
</ul>
<p>Michael has written over 70 <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en">peer-reviewed publications</a>, a <a class="reference external" href="https://pypi.org/project/geostatspy/">Python package</a> for spatial data analytics, co-authored a textbook on spatial data analytics, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a> and author of two recently released e-books, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy</a> and <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html">Applied Machine Learning in Python: a Hands-on Guide with Code</a>.</p>
<p>All of Michael‚Äôs university lectures are available on his <a class="reference external" href="https://www.youtube.com/@GeostatsGuyLectures">YouTube Channel</a> with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his <a class="reference external" href="https://github.com/GeostatsGuy">GitHub account</a>, to support any interested students and working professionals with evergreen content. To find out more about Michael‚Äôs work and shared educational resources visit his <span class="xref myst">Website</span>.</p>
</section>
<section id="want-to-work-together">
<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>I‚Äôm always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
</section>
&#13;

<h2>Motivation for Probability</h2>
<p>Why cover probability at the beginning of an e-book or course on machine learning?</p>
<ol class="arabic simple">
<li><p><strong>Model Formulation</strong> - many of our machine learning models are formulated with probability concepts, for example, naive Bayes classification is derived from Bayes‚Äô Theorem and Bayesian linear regression estimates the probability distributions for our model parameters.</p></li>
<li><p><strong>Data Cleaning and Preparation</strong> - is 90% of any machine learning workflow and we cannot complete these steps without understanding our data distributions and statistics and all of these are based on probability.</p></li>
<li><p><strong>Loss Functions and Optimization</strong> - many of our machine learning models are trained through optimization of a loss function that relies on probability for stochastic steps or even directly in the loss function as is the case for maximum likelihood estimation.</p></li>
<li><p><strong>Tuning Machine Learning Models</strong> - machine learning model tuning to reduce model overfit is based on the concept of expected model performance in the presence of various uncertainty sources, and probability is the language of statistical expectation and uncertainty.</p></li>
<li><p><strong>Machine Learning Model Choice</strong> - You will make choices between frequentist and Bayesian predictive machine learning models and their differences are the result of two distinct approaches to calculate probability and build models.</p></li>
<li><p><strong>Real World Applications</strong> - to make the best choices in machine learning workflows design and to use our results to support decisions we must integrate probability concepts Using our models in the real world.</p></li>
</ol>
<p>Let us build our machine learning skills on a solid foundation of probability!</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Probability</p>
<p>Probability is an essential prerequisite for machine learning.</p>
</div>
<p>Now we get started by defining probability and then we will be ready to talk about ways to calculate it.</p>
&#13;

<h2>What is Probability?</h2>
<p>To understand what is probability consider Kolmogorov‚Äôs 3 axioms for probabilities, i.e., the rules that any measure of probability must honor,</p>
<figure style="text-align: center;">
  <img src="../Images/53594a139b9058c07be33e4cff134ea4.png" style="display: block; margin: 0 auto; width: 30%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/kolmogorov.png"/>
  <figcaption style="text-align: center;"> Andrey Kolmogorov (1903 ‚Äì 1987), Soviet mathematician, photo taken 1972 and from image from https://culturemath.ens.fr/thematiques/biographie/life-and-work-kolmogorov).
</figcaption>
</figure>
<ol class="arabic simple">
<li><p><strong>non-negativity</strong> - probability of an event is a non-negative number</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
P(ùê¥) \ge 0
\]</div>
<p>¬†¬†¬†¬†¬† imagine negative probability!</p>
<ol class="arabic simple" start="2">
<li><p><strong>normalization</strong> - probability of the entire sample space is one (unity), also known as probability closure</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
P(\Omega) = 1
\]</div>
<p>¬†¬†¬†¬†¬† something happens!</p>
<ol class="arabic simple" start="3">
<li><p><strong>additivity</strong> - addition of probability of mutually exclusive events for unions</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
P\left(‚ãÉ_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i)
\]</div>
<p>¬†¬†¬†¬†¬† e.g., probability of <span class="math notranslate nohighlight">\(A_1\)</span> and <span class="math notranslate nohighlight">\(A_2\)</span> mutual exclusive events is, <span class="math notranslate nohighlight">\(Prob(A_1 + A_2) = Prob(A_1) + Prob(A_2)\)</span></p>
<ul class="simple">
<li><p>note our concise notation, <span class="math notranslate nohighlight">\(P(\cdot)\)</span>, for probability of event in the bracket occurring.</p></li>
</ul>
<p>This is a good place to start for valid probabilities, we will refine this later. Now we can ask,</p>
&#13;

<h2>How to Calculate Probability?</h2>
<p>There are 3 probability perspectives that can be applied to calculate probabilities,</p>
<ol class="arabic simple">
<li><p><strong>Probability by long-term frequencies</strong> (Frequentist Probability),</p></li>
</ol>
<ul class="simple">
<li><p>probability as ratio of outcomes from an experiment</p></li>
<li><p>requires repeated observations from a controlled experiment</p></li>
</ul>
<p>¬†¬†¬†¬†¬† for example, you flip a coin 100 times and count the number of outcomes with heads <span class="math notranslate nohighlight">\(n(\text{heads})\)</span> and then calculate this ratio,</p>
<div class="math notranslate nohighlight">
\[
P(\text{heads}) = \frac{n(\text{heads})}{n}
\]</div>
<p>¬†¬†¬†¬†¬† this is the frequentist approach to calculate probabilities. The experiment is the set of coin tosses.</p>
<ul class="simple">
<li><p>one issue with frequentist probabilities is, can we now use this probability of heads outside the experiment? For example,</p>
<ul>
<li><p>on another day?</p></li>
<li><p>a different person tossing the coin?</p></li>
<li><p>a different coin?</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Probability by physical tendencies or propensities</strong> (Engineering Probability),</p></li>
</ol>
<ul class="simple">
<li><p>probability calculated from knowledge about the system</p></li>
<li><p>we could know the probability of coin toss outcomes without the experiment</p></li>
</ul>
<p>¬†¬†¬†¬†¬† this is the engineering approach to probability, model the system and use this model, i.e., the physics of the system to calculate probability of outcomes</p>
<ul class="simple">
<li><p>did you know that there is a <span class="math notranslate nohighlight">\(\frac{1}{6,000}\)</span> probability of a coin landing and staying upright on its edge? This could  be lower depending of manner of tossing, coin thickness and characteristics of the surface.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Probability by Degrees of belief</strong> (Bayesian Probability),</p></li>
</ol>
<ul class="simple">
<li><p>first we specify the scientific concept of ‚Äúbelief‚Äù as your opinion based on all your knowledge and experience.</p></li>
<li><p>our model integrates our certainty about a result and data</p></li>
<li><p>this is very flexible, we can assign probability to any event, and includes a framework for updating with new information</p></li>
</ul>
<p>¬†¬†¬†¬†¬† if this is your approach, then you are using the Bayesian approach for probability. If not, before you dismiss this approach let me make a couple arguments,</p>
<ul class="simple">
<li><p>you may be bothered by this idea of belief, as it may seem subjective compared to the probabilities objectively measured from a frequentist‚Äôs experiment but,</p>
<ul>
<li><p>to use the frequentist probability you have to make the subjective decision to apply it outside the experiment, i.e., we need to go beyond a single coin!</p></li>
<li><p>the Bayesian probability approach includes objective probabilities from experiments, but it also allows for integration of our expert knowledge</p></li>
</ul>
</li>
</ul>
&#13;

<h2>A Warning about Calculating Probability</h2>
<p>Statistics can be misused or even abused, leading to flawed conclusions and poor decision-making. When probabilities are calculated improperly or misinterpreted, it can result in significant consequences. Here are a few examples of what can go wrong:</p>
<ol class="arabic simple">
<li><p><strong>Insufficient sampling</strong> - there are various rules of thumb about what constitutes a small sample size, i.e., too small for inference about the population parameters and too small to train a reliable prediction model. Some say 7, some say 30, indubitably there is a minimum sample size.</p></li>
</ol>
<ul class="simple">
<li><p>in general, our models will communicate increasing uncertainty as the number of data decreases, but at some point these models break!</p></li>
<li><p>consider bootstrap, yes as the number of samples decrease the uncertainty in the statistic reported by bootstrap increases, but this uncertainty distribution is centered on the sample statistic! In other words, we need enough samples to have a reasonable estimate of the uncertainty model‚Äôs expectation in the first place.</p></li>
<li><p>of course, there is minimum number of samples to complete a mathematical operation, for example principal components analysis can only calculate <span class="math notranslate nohighlight">\(n-1\)</span> principal components (where <span class="math notranslate nohighlight">\(n\)</span> is the number of samples) and we cannot fit a linear regression model to <span class="math notranslate nohighlight">\(n=1\)</span> samples. Best practice is to stay very far away (have many more samples) from any of these algorithm limitations.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Biased sampling</strong> - in general our probability calculations will not automatically debias the sample data. Any bias in the samples will pass bias through to the probabilities representing our uncertainty model.</p></li>
</ol>
<ul class="simple">
<li><p>for example, a biased sample mean will bias simple kriging estimates away from data. Geostatistical simulation reproduces the entire input feature distribution, so any bias in any part of the distribution will be passed to the spatial models.</p></li>
<li><p>also, there is bad data. When we use Bayesian methods and rely on expert experience, that is not a license to say anything and defend it as belief. On the contrary, we must rigorously document and defend all our choices</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Unskilled practice and a lack of rigor</strong> - there are mistakes that can be made with probabilities and some of them are shockingly common.</p></li>
</ol>
<ul class="simple">
<li><p>our first line of defense is to understand what our methods are doing under the hood! This will help us recognize logical inconsistencies as we build our workflows.</p></li>
<li><p>our second line of defense is to check every step in our workflows. Like an accountant we must close the loops with all our probability calculations, a process that accountants call reconciliation. Like a software engineer we must unit test every operation to ensure we don‚Äôt introduce errors as we update our probability workflows.</p></li>
<li><p>for example, if we perform primary feature cosimulation with the same random number seed as the secondary feature simulation we will introduce artificial correlation between the simulated primary and secondary features that will dramatically change the conditional and joint probabilities.</p></li>
<li><p>in another example, if we use information from the likelihood probabilities to inform the prior probabilities, we will significantly under-estimate the uncertainty. This error is a form of information leakage, descriptively we state it as ‚Äúdouble dipping‚Äù from the information.</p></li>
</ul>
<p>Please remember these warnings as you proceed below and onward as you build your own data science workflows.</p>
&#13;

<h2>Venn Diagrams</h2>
<p>Venn Diagrams are a tool to communicate probability.</p>
<ul class="simple">
<li><p>representing the possible events or outcomes (<span class="math notranslate nohighlight">\(A, B,\ldots\)</span>) of an experiment within the collection of all possible events or outcomes, the sample space (<span class="math notranslate nohighlight">\(\Omega\)</span>).</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/c212b1fc6b3c5da35b7df52769a0af42.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn.png"/>
  <figcaption style="text-align: center;"> Simple example Venn diagram.
</figcaption>
</figure>
<p>What do we learn from this Venn diagram?</p>
<ol class="arabic simple">
<li><p>the relative size of regions for each event within is the relative probability of occurrence, probability of <span class="math notranslate nohighlight">\(B\)</span> is greater than probability of <span class="math notranslate nohighlight">\(A\)</span></p></li>
<li><p>the overlap over events is the probability of joint occurrence, there is 0.0 probability of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> occurring together</p></li>
</ol>
<p>Let us include a more practical Venn diagram to ensure this concept is not too abstract. Here‚Äôs a Venn diagram from 3,000 core samples with interpreted facies</p>
<ul class="simple">
<li><p>the events are sandstone (Sm) and mudstone (Fm)</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/824ce0a24892a79f960068d34d629915.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_rock.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing the facies assignments for 3,000 core samples.
</figcaption>
</figure>
<p>What do we learn from this Venn diagram?</p>
<ul class="simple">
<li><p>mudstone is more likely than sandstone over these core samples</p></li>
<li><p>there are a lot of samples that are neither sandstone nor mudstone, the white space within <span class="math notranslate nohighlight">\(\Omega\)</span></p></li>
<li><p>there are samples that are interpreted as both sandstone and mudstone, i.e., interbedded sandstone-mudstone</p></li>
<li><p>do not forget to draw and label the <span class="math notranslate nohighlight">\(\Omega\)</span> box or we can‚Äôt understand the relative probability of the events</p></li>
<li><p>we can use any convenient shape to represent an event</p></li>
</ul>
<p>In summary, Venn diagrams are an excellent tool to visualize probability, so we will use them to visualize and teach probability here.</p>
&#13;

<h2>Frequentist Probability</h2>
<p>We now provide an extended definition for the frequentist approach for probability. A measure of the likelihood that an event will occur based on frequencies observed from an experiment.</p>
<ul class="simple">
<li><p>For random experiments and well-defined settings, we calculate probability as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A) = \lim_{n \to \infty} \frac{n(A)}{n}
\]</div>
<p>where:</p>
<p><span class="math notranslate nohighlight">\(n(A)\)</span> = number of times event <span class="math notranslate nohighlight">\(A\)</span> occurred
<span class="math notranslate nohighlight">\(n\)</span> = number of trails</p>
<ul class="simple">
<li><p>we use limit notation above to indicate sufficient sampling and that the solution converges and improves accuracy as we introduce more samples from our experiment</p></li>
</ul>
<p>For example,</p>
<ul class="simple">
<li><p>probability of drilling a dry hole, encountering sandstone, and exceeding a rock porosity of <span class="math notranslate nohighlight">\(15\%\)</span> at a location (<span class="math notranslate nohighlight">\(\bf{u}_{\alpha}\)</span>) based on historical results for drilling in this reservoir</p></li>
</ul>
<p>Now we walk-through various probability operations from the frequentist perspective, may I ask for patience from my Bayesian friends as we will later return to the Bayesian perspective for probability.</p>
&#13;

<h2>Probability Operations</h2>
<p><strong>Union of Events</strong> - for example, all outcomes in the sample space that belong to either event <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span></p>
<ul class="simple">
<li><p>for the union of events operator, we use the word ‚Äúor‚Äù and the mathematics symbol, <span class="math notranslate nohighlight">\(cup\)</span>, using set notation we state the samples in the union <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span> as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A \cup B = \{x: x \in A \text{ or } x \in B\}
\]</div>
<ul class="simple">
<li><p>and the probability notation for a union as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A \cup B)
\]</div>
<p>Here‚Äôs a Venn diagram illustrating the union <span class="math notranslate nohighlight">\(P(A \cup B)\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/01902ece9dac16a02139346ccccf2574.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_union.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing union probability operator for events \(A\) or \(B\). Note, the legend on the top right is included to clarify the plot but is not part of the Venn diagram.
</figcaption>
</figure>
<p><strong>Intersection of Events</strong> - for example, all outcomes in the sample space that belong to both events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span></p>
<ul class="simple">
<li><p>for the intersection of events operator, we use the word ‚Äúand‚Äù and the mathematics symbol <span class="math notranslate nohighlight">\(cap\)</span>, using set notation we state the samples in the intersection <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A \cap B = \{x: x \in A \text{ and } x \in B\}
\]</div>
<ul class="simple">
<li><p>and the probability notation for an intersection as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A \cap B) 
\]</div>
<ul class="simple">
<li><p>or with the common probability shorthand as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B)
\]</div>
<p>¬†¬†¬†¬†¬† we will call this a joint probability later. Here is a Venn diagram illustrating the intersection <span class="math notranslate nohighlight">\(P(A \cap B)\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/55a85b2ac3f3804377f80585821678e0.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_intersection.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing intersection probability operator for events \(A\) and \(B\). Note, the legend on the top right is included to clarify the plot but is not part of the Venn diagram.
</figcaption>
</figure>
<p><strong>Compliment of Events</strong> - for example, all outcomes in the sample space that do not belong to event <span class="math notranslate nohighlight">\(A\)</span></p>
<ul class="simple">
<li><p>for the compliment of event(s) operator we use the word ‚Äúnot‚Äù and the mathematics symbol <span class="math notranslate nohighlight">\(^c\)</span>, using set notation, we state the samples in the compliment of <span class="math notranslate nohighlight">\(A\)</span> as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A^c = \{x: x \notin A\}
\]</div>
<ul class="simple">
<li><p>and the probability notation for a compliment as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A^c) 
\]</div>
<p>Here‚Äôs a Venn diagram illustrating the compliment <span class="math notranslate nohighlight">\(P(A^c)\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/507be2ebc15778352071fdbdd52d2a3c.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_compliment.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing probability compliment operator for events \(A\). Note, the legend on the top right is included to clarify the plot but is not part of the Venn diagram.
</figcaption>
</figure>
<p><strong>Mutually Exclusive Events</strong> - for example, events do not intersect or do not have any common outcomes, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> do not occur at the same time.</p>
<ul class="simple">
<li><p>using set notation, we state events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are mutually exclusive as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A \cap B = \{x: x \in A \text{ and } x \in B \} = \emptyset
\]</div>
<ul class="simple">
<li><p>and the probability notation for mutually exclusive as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A \cap B) = 0.0
\]</div>
<ul class="simple">
<li><p>or with the common probability shorthand as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B) = 0.0
\]</div>
<p>Here‚Äôs a Venn diagram illustrating mutual exclusive events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/c212b1fc6b3c5da35b7df52769a0af42.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing events \(A\) and \(B\) as mutually exclusive.
</figcaption>
</figure>
<p><strong>Exhaustive, Mutually Exclusive Events</strong> the sequence of events whose union is equal to the sample space, all-possible events (<span class="math notranslate nohighlight">\(\Omega\)</span>) and there is no intersection between the events:</p>
<ul class="simple">
<li><p>using set notation, we state events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are exhaustive as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\{x: x \in A \text{ or } x \in B \} = \Omega
\]</div>
<ul class="simple">
<li><p>and events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are mutually exclusive as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\{x: x \in A \text{ and } x \in B \} = \emptyset
\]</div>
<ul class="simple">
<li><p>and the probability notation for exhaustive events as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A \cup B) = 1.0
\]</div>
<ul class="simple">
<li><p>and for mutually exhaustive events as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B) = 0.0
\]</div>
<p>Here‚Äôs a Venn diagram illustrating mutual exclusive, exhaustive events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/02c7a7dba945af2e442593dbf746edb5.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_mutual.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing events \(A\) and \(B\) as exhaustive, mutually exclusive.
</figcaption>
</figure>
<p><strong>Combinations of Operators</strong> - we can use these probability operators with any number of events to communicate complicated probability cases. For example, let us define these events,</p>
<ul class="simple">
<li><p>Event <span class="math notranslate nohighlight">\(A\)</span>: oil present (<span class="math notranslate nohighlight">\(A^c\)</span>: dry hole)</p></li>
<li><p>Event <span class="math notranslate nohighlight">\(B\)</span>: ùëÜùëö (<span class="math notranslate nohighlight">\(B^c\)</span>: ùêπùëö)</p></li>
<li><p>Event <span class="math notranslate nohighlight">\(C\)</span>: porosity ‚â• 15% (<span class="math notranslate nohighlight">\(C^c\)</span>: porosity &lt; 15%)</p></li>
</ul>
<p>What is the probability of dry hole with massive sandstone (ùëÜùëö) and porosity ‚â• 15%?</p>
<div class="math notranslate nohighlight">
\[
P(A^c \cap B \cap C) = \frac{\text{Area}(A^c \cap B \cap C)}{\text{Area}(\Omega)}
\]</div>
<p>Here‚Äôs a Venn diagram representing this case.</p>
<figure style="text-align: center;">
  <img src="../Images/fe9499a30db96f497a94f9d209947cec.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_complicated.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing a more complicated case with 3 events, \(A, B, C\), with compliment and intersection probability operators.
</figcaption>
</figure>
&#13;

<h2>Constraints on Probability</h2>
<p>Now that we have defined probability notation and probability operations, now we return the idea of permissible probability values expressed by Kolmogorov. We can now build on Kolmogorov‚Äôs probability axioms with this set of probability constraints,</p>
<p>Non-negativity, Normalization constraints include,</p>
<ul class="simple">
<li><p>Probability is bounded,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
0.0 \le P(A) \le 1.0
\]</div>
<p>¬†¬†¬†¬†¬† probability must be between 0.0 and 1.0 (including 0.0 and 1.0)</p>
<ul class="simple">
<li><p>Probability closure,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(\Omega) = 1.0
\]</div>
<p>¬†¬†¬†¬†¬† probability of any event is 1.0</p>
<div class="math notranslate nohighlight">
\[
P(A) + P(A^c) = 1.0 
\]</div>
<p>¬†¬†¬†¬†¬† probability of A or not A is 1.0</p>
<ul class="simple">
<li><p>Probability for null sets,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(\emptyset) = 0.0
\]</div>
<p>¬†¬†¬†¬†¬† probability of nothing happens is zero</p>
<p>Now we use these probability concepts and notation to append more essential complicated probability concepts.</p>
&#13;

<h2>Probability Addition Rule</h2>
<p>What is the probability for a union of events? Remember that union is an ‚Äúor‚Äù operation represented by the <span class="math notranslate nohighlight">\(\cup\)</span> notation. Consider,</p>
<div class="math notranslate nohighlight">
\[
P(A \cup B)
\]</div>
<p>inspection of the previously shown Venn diagram indicates that we can not calculate the union , <span class="math notranslate nohighlight">\(P(A \cup B)\)</span>, by just summing <span class="math notranslate nohighlight">\(P(A)\)</span> and <span class="math notranslate nohighlight">\(P(B)\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/55a85b2ac3f3804377f80585821678e0.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_intersection.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing events \(A\) and \(B\), showing the intersection of \(A\) and \(B\) that will be double counted if we calculate the union and \(A\) or \(B\) as the sum of \(A\) and \(B\). Note, the legend on the top right is included to clarify the plot but is not part of the Venn diagram.
</figcaption>
</figure>
<p>The sum of the probabilities <span class="math notranslate nohighlight">\(P(A)\)</span> and <span class="math notranslate nohighlight">\(P(B)\)</span> will double count the intersection, <span class="math notranslate nohighlight">\(P(A \cap B)\)</span>, so we must subtract it,</p>
<div class="math notranslate nohighlight">
\[
P(A \cup B) = P(A) + P(B) - P(A \cap B)
\]</div>
<p>Yes, there is a general expression for any number of events to calculate the probability of the union,</p>
<ul class="simple">
<li><p>I‚Äôm not going to include it here but suffice it to say that it is a book keeping nightmare given the combinatorial of intersections that can be counted too many times!</p></li>
</ul>
<p>There is a much simpler case. If the events are mutually exclusive then there is no intersection, <span class="math notranslate nohighlight">\(P(A,B) = 0.0\)</span>, and we can just sum the probabilities.</p>
<ul class="simple">
<li><p>for the general case of mutually exclusive for any number of events,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A_i \cap A_j = \emptyset, \quad \forall \quad i \ne j
\]</div>
<p>then we can write this general equation for the addition rule for any number of mutually exclusive events,</p>
<div class="math notranslate nohighlight">
\[
P\left( \bigcup_{i=1}^k A_i \right) = \sum_{i=1}^k P(A_i)
\]</div>
<p>Here‚Äôs a Venn diagram showing 4 mutual exclusive events, <span class="math notranslate nohighlight">\(A_1, A_2, A_3, A_4\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/aecd4b1580232c0f55e753bad39f1ba7.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_addition.png"/>
  <figcaption style="text-align: center;"> A Venn diagram representing 4 events, \(A_1, A_2, A_3, A_4\), that are all mutually exclusive. We can calculate the probability of the union of these events as the sum of probabilities of each.
</figcaption>
</figure>
&#13;

<h2>Conditional Probability</h2>
<p>What is the probability of an event given another event has occurred? To discuss this, let us get more specific, what is the probability of event <span class="math notranslate nohighlight">\(B\)</span> given event <span class="math notranslate nohighlight">\(A\)</span> has occurred?</p>
<ul class="simple">
<li><p>to express this we use the notation, <span class="math notranslate nohighlight">\(P(B|A)\)</span></p></li>
<li><p>we read this notation as, ‚Äúprobability of B given A‚Äù</p></li>
</ul>
<p>This is an example of a conditional probability. We calculate conditional probability with this equation,</p>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A \cap B)}{P(A)}
\]</div>
<p>This may seem complicated, but we can easily visualize and understand this equation from our Venn diagram.</p>
<figure style="text-align: center;">
  <img src="../Images/7e6c3488677335aca9cd1bdaaa70f6bb.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_conditional1.png"/>
  <figcaption style="text-align: center;"> To calculate conditional probability we "shrink our universe" to the given condition. For the probability of $B$ given $A$ we shrink our Venn diagram to \(A\).
</figcaption>
</figure>
<p>after we shrink our universe, our <span class="math notranslate nohighlight">\(\Omega\)</span> is only event <span class="math notranslate nohighlight">\(A\)</span>! Now we can easily see that the conditional probability is simply the probability of the intersection of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> divided by the probability of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/7b40d3731ffed4bc1939cfc97dfdce13.png" style="display: block; margin: 0 auto; width: 40%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_conditional2.png"/>
  <figcaption style="text-align: center;"> After we "shrink our universe" to the condition, we can see conditional probability equation clearly as the intersection of \(A\) and \(B\) divided by the probability of \(A\).
</figcaption>
</figure>
<p>Now we introduce a couple examples to test our knowledge of conditional probability,</p>
<p><strong>Conditional Probability Example #1</strong> - for this Venn diagram provide the conditional probabilities, <span class="math notranslate nohighlight">\(P(A|B)\)</span> and <span class="math notranslate nohighlight">\(P(B|A)\)</span>, in terms of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/5ca9cd3deee70b9c41f0865eb8f1bc9b.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_conditional_ex1.png"/>
  <figcaption style="text-align: center;"> Venn diagram to check your understanding of conditional probability.
</figcaption>
</figure>
<p>This is simple to solve, the answer is 0.0 for both conditional probability, because to calculate conditional probability the numerator is the probability, <span class="math notranslate nohighlight">\(P(A,B)\)</span>, and this is 0.0 if there is no overlap in the Venn diagram.</p>
<ul class="simple">
<li><p>also, since <span class="math notranslate nohighlight">\(P(B,A) = P(B,A)\)</span> the numerator is the same for both conditional probabilities,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B,A)}{P(B)} = \frac{0.0}{P(B)} = 0.0
\]</div>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A,B)}{P(A)} = \frac{0.0}{P(A)} = 0.0
\]</div>
<p>Now we attempt a more challenging example.</p>
<p><strong>Conditional Probability Example #2</strong> - for this Venn diagram provide the conditional probabilities, <span class="math notranslate nohighlight">\(P(A|B)\)</span> and <span class="math notranslate nohighlight">\(P(B|A)\)</span>, in terms of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/97292e6e32b997818928d4ab73818c14.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_conditional_ex2.png"/>
  <figcaption style="text-align: center;"> Venn diagram to check your understanding of conditional probability.
</figcaption>
</figure>
<p>This one is more interesting. Let us look at each conditional probability one at a time.</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B,A)}{P(B)}
\]</div>
<p>notice that <span class="math notranslate nohighlight">\(P(B,A) = P(B)\)</span> since all <span class="math notranslate nohighlight">\(B\)</span> is inside <span class="math notranslate nohighlight">\(A\)</span>, so we can substitute <span class="math notranslate nohighlight">\(P(B)\)</span> for <span class="math notranslate nohighlight">\(P(B,A)\)</span> above,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B)}{P(B)} = 1.0
\]</div>
<p>this makes sense, given <span class="math notranslate nohighlight">\(B\)</span> then <span class="math notranslate nohighlight">\(A\)</span> must occur, the conditional probability is 1.0. Now we calculate the other conditional probability,</p>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A,B)}{P(A)} = \frac{P(B,A)}{P(A)} 
\]</div>
<p>just a reminder that <span class="math notranslate nohighlight">\(P(B,A) = P(A,B)\)</span> so we can use our previous result and substitute, <span class="math notranslate nohighlight">\(P(B)\)</span> for <span class="math notranslate nohighlight">\(P(B,A)\)</span> above.</p>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A,B)}{P(A)} = \frac{P(B,A)}{P(A)} = \frac{P(B)}{P(A)}
\]</div>
<p>If you understand these two examples, then you have a good foundation in conditional probability,</p>
<ul class="simple">
<li><p>and I can challenge you with a more complicated without conditional probabilities and learn an interesting general case!</p></li>
<li><p>let us take this Venn diagram with 3 events (<span class="math notranslate nohighlight">\(A, B, C\)</span>) and all possible event intersections labeled.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/63615b4387d8a1e2080ddc3fc29f841c.png" style="display: block; margin: 0 auto; width: 50%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/venn_conditional3.png"/>
  <figcaption style="text-align: center;"> A more complicated case of conditional probability dealing with 3 events, \(A, B, C\).
</figcaption>
</figure>
<p>now we calculate the conditional probability,</p>
<div class="math notranslate nohighlight">
\[
P(C|B \cap A) = \frac{P(A \cap B \cap C}{P(A \cap B}
\]</div>
<p>Recalling the definition of conditional probability,</p>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A \cap B)}{P(A)}
\]</div>
<p>so we can reorder this to get,</p>
<div class="math notranslate nohighlight">
\[
\frac{P(A \cap B)} = P(B|A) \cdot {P(A)}
\]</div>
<p>spoiler alert, later we will define this at the multiplication rule, but for now we can substitute this as the denominator for the conditional probability, <span class="math notranslate nohighlight">\(P(C|B \cap A)\)</span>, to get,</p>
<div class="math notranslate nohighlight">
\[
P(C|B \cap A) = \frac{P(A \cap B \cap C}{P(B|A) \cdot P(A)}
\]</div>
<p>now we can reorder this as,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B \cap C) = P(C|B \cap A) \cdot P(B|A) \cdot P(A)
\]</div>
<p>Do you see the pattern? We can calculate high order probability intersections as a sequential set, product sum of marginal probability and then growing conditional probabilities. Let me state it with words,</p>
<ul class="simple">
<li><p>probability of <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(C\)</span> is the probability of <span class="math notranslate nohighlight">\(A\)</span> times the probability of <span class="math notranslate nohighlight">\(B\)</span> given <span class="math notranslate nohighlight">\(A\)</span>, times the probability of <span class="math notranslate nohighlight">\(C\)</span> given <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
</ul>
<p>In fact, we can generalize this for any number of events as,</p>
<div class="math notranslate nohighlight">
\[
P(A_1 \cap A_2 \cap \ldots \cap A_n) = P(A_n|A_{n-1} \cap \ldots \cap A_1) \cdot P(A_{n-1}|A_{n-2} \cap \ldots \cap A_1) \cdot \ldots \cdot P(A_2|A_1) \cdot P(A_1)
\]</div>
<p>or more compactly,</p>
<div class="math notranslate nohighlight">
\[
P(A_1, A_2, \ldots , A_n) = P(A_n|A_{n-1}, \ldots , A_1) \cdot P(A_{n-1}|A_{n-2}, \ldots , A_1) \cdot \ldots \cdot P(A_2|A_1) \cdot P(A_1)
\]</div>
<p>Are we just showing off now? No! This example teaches us a lot about how conditional probabilities work.</p>
<ul class="simple">
<li><p>also, this is exactly the concept used with sequential Gaussian simulation where we sample sequentially from conditional distributions instead of from the high order joint probability, an impossible task without the sequential approach!</p></li>
</ul>
<p>Now we are ready to summarize marginal, conditional and joint probabilities together.</p>
&#13;

<h2>Marginal, Conditional and Joint Probabilities</h2>
<p>Now we define marginal, conditional and joint probability, provide notation and discuss how to calculate them:</p>
<p><strong>Marginal Probability</strong> - probability of an event, irrespective of any other event,</p>
<div class="math notranslate nohighlight">
\[
P(A), P(B)
\]</div>
<ul class="simple">
<li><p>marginal probabilities are calculated as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A) = \frac{n(A)}{n(\Omega)}
\]</div>
<ul class="simple">
<li><p>marginal probabilities may be calculated from joint probabilities through the process of marginalization,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A) = \int_{-\infty}^{\infty} P(A,B) dB
\]</div>
<ul class="simple">
<li><p>where we integrate over all cases of the other events, <span class="math notranslate nohighlight">\(B\)</span>, to remove them.</p></li>
<li><p>given discrete cases of event <span class="math notranslate nohighlight">\(B\)</span> we can simply sum the probabilities over all cases of <span class="math notranslate nohighlight">\(B\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A) = \sum_{i=1}^{k_B} P(A,B)
\]</div>
<p><strong>Conditional Probability</strong> - probability of an event, given another event has already occurred,</p>
<div class="math notranslate nohighlight">
\[
P(A \text{ given } B), P(B \text{ given } A)
\]</div>
<ul class="simple">
<li><p>or with compaction probability notation,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A|B), P(B|A)
\]</div>
<ul class="simple">
<li><p>see the previous section for more information, but for symmetry we repeat the method to calculate conditional probability as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A,B)}{P(A)}
\]</div>
<ul class="simple">
<li><p>of course, as we saw in our conditional probability examples above, we cannot switch the order of the events,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(B|A) \ne P(A|B)
\]</div>
<p><strong>Joint Probability</strong> - probability of multiple events occurring together,</p>
<div class="math notranslate nohighlight">
\[
P(A \text{ and } B), P(B \text{ and } A)
\]</div>
<ul class="simple">
<li><p>or with compaction probability notation,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A \cap B), P(B \cap A)
\]</div>
<ul class="simple">
<li><p>or even more compact as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B), P(B,A)
\]</div>
<ul class="simple">
<li><p>of course, the ordering does not matter for joint probabilities,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B) = P(B,A)
\]</div>
<ul class="simple">
<li><p>we calculate joint probabilities as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B) = \frac{n(A,B)}{n(\Omega)}
\]</div>
<p>To clarify the concept of marginal, conditional and joint probability (and distributions), I have coded a set of interactive Python dashboards, <a class="reference external" href="https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Marginal_Joint_Conditional_Probability.ipynb">Interactive_Marginal_Joint_Conditional_Probability</a>,</p>
<ul class="simple">
<li><p>that provides a dataset and interactively calculates and compares marginal, conditional and joint probabilities and visualizes and compares the marginal and conditional distributions.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/ada0a5ff99a013044e2ec05654f7710a.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/interactive_joint.png"/>
  <figcaption style="text-align: center;"> One of the interactive Python dashboards from my series on marginal, conditional and joint probability and distributions.
</figcaption>
</figure>
&#13;

<h2>Probability Multiplication Rule</h2>
<p>As shown above, we can calculate the joint probability of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> as the product of the conditional probability of <span class="math notranslate nohighlight">\(B\)</span> given <span class="math notranslate nohighlight">\(A\)</span> with the marginal probability of <span class="math notranslate nohighlight">\(A\)</span>,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B) = P(A,B) = P(B|A) \cdot P(A)
\]</div>
<p>and of course, we can also state,</p>
<div class="math notranslate nohighlight">
\[
P(B \cap A) = P(B,A) = P(A|B) \cdot P(B)
\]</div>
<p>this is know as the multiplication rule, we use the multiplication rule to develop the definition of independence.</p>
&#13;

<h2>Independent Events</h2>
<p>Given the probability multiplication rule,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B) = P(A,B) = P(B|A) \cdot P(A)
\]</div>
<p>now we ask the question, what is the conditional probability <span class="math notranslate nohighlight">\(P(B|A)\)</span> if events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent? Given independence between <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> we would expect that knowing about <span class="math notranslate nohighlight">\(A\)</span> will not impact the outcome of <span class="math notranslate nohighlight">\(B\)</span>,</p>
<div class="math notranslate nohighlight">
\[
P(B|A) = P(B)
\]</div>
<p>if we substitute this into <span class="math notranslate nohighlight">\(P(A \cap B) = P(A,B) = P(B|A) \cdot P(A)\)</span> we get,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B) = P(B) \cdot P(A)
\]</div>
<p>by the same logic we can show,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = P(A)
\]</div>
<p>Now we can summarize this as, events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent if and only if the following relations are true,</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(P(A \cap B) = P(A) \cdot P(B)\)</span> - joint probability is the product of the marginal probabilities</p></li>
<li><p><span class="math notranslate nohighlight">\(P(A|B) = P(A)\)</span> - conditional is the marginal probability</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|A) = P(B)\)</span> - conditional probability is the marginal probability</p></li>
</ol>
<p>If any of these are violated, we can suspect that there exists some form of relationship.</p>
<ul class="simple">
<li><p>we leave the significance of this result outside the scope for this chapter on probability</p></li>
<li><p>certainly, if we can assume independence this simplifies our data science workflows</p></li>
</ul>
<p><strong>Independence Example #1</strong> - check independence for the following dataset.</p>
<figure style="text-align: center;">
  <img src="../Images/222dd21803448a6193e3a86b018195b1.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/ind_table.png"/>
  <figcaption style="text-align: center;"> If event \(A_1\) is Fm facies in Middle Unit (blue circles) and event \(A_2\) is Sm facies in Bottom Unit (red circles), and joint cases of event \(A_1\) and \(A_2\) (black rectangles for joint \(A_1\) and \(A_2\) occur together) are \(A_1\) and \(A_2\) independent events?
</figcaption>
</figure>
<p>To check for independence, we calculate the marginal and joint probabilities,</p>
<div class="math notranslate nohighlight">
\[
P(A_1) = \frac{5}{10} \quad P(A_2) = \frac{6}{10} \quad P(A_1,A_2) = \frac{2}{10}
\]</div>
<p>now we check one of the conditions for independence, as the joint probability as product of the marginal probabilities,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A_1 \cap A_2) = P(A_1) \cdot P(A_2)\)</span> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <span class="math notranslate nohighlight">\(0.2 \ne 0.5 \cdot 0.6\)</span>, <span class="math notranslate nohighlight">\(\therefore\)</span> we suspect a relationship between events <span class="math notranslate nohighlight">\(A_1\)</span> and <span class="math notranslate nohighlight">\(A_2\)</span></p></li>
</ul>
&#13;

<h2>Bayesian Probability</h2>
<p>Now we finally dive into the Bayesian approach to probability, a very flexible approach that can be applied to assign probability to anything, and includes a framework for updating with new information.</p>
<p>When I say that the Bayesian approach can assign probability to anything, I realize this needs some further explanation,</p>
<ul class="simple">
<li><p>of course, remember my statements in the ‚ÄúA Warning about Calculating Probability‚Äù section above, there are times when we do not have enough data, and there are workflows that are incorrect.</p></li>
</ul>
<p>I mean that any type of information can be encoded into the Bayesian framework, including belief</p>
<p>Let us discuss this supported by a great example from the introduction chapter of Sivia (1996), ‚ÄúWhat is the Mass of Jupyter?‚Äù.</p>
<figure style="text-align: center;">
  <img src="../Images/b1f7bb75c53f50d876649588c61eed7e.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/Jupiter.png"/>
  <figcaption style="text-align: center;"> Jupyter image from New Horizons Long Range Imager (LORRI), taken at 57 million km on January 2007. Image from https://en.wikipedia.org/wiki/Jupiter#/media/File:Jupiter_New_Horizons.jpg.
</figcaption>
</figure>
<p>What would be frequentist and Bayesian approaches to calculate the probabilities for the uncertainty model of the mass of Jupiter?</p>
<ul class="simple">
<li><p>Frequentist perspective - calculate the cumulative distribution function by measuring the mass of enough Jupiter-like planets from many star systems.</p></li>
<li><p>Bayesian - form a prior probability and update with any available information.</p></li>
</ul>
<p>Now to anyone saying, the frequentist approach is possible, remember the first exoplanets were discovered in 1995 by astronomers Michel Mayor and Didier Queloz.</p>
<ul class="simple">
<li><p>not coincidentally, the first discovered exoplanet was a Jupyter-mass hot, gas giant, as these are easier to see with the Doppler-based wobble method, because near-to-star gas giants have greater pull on their star resulting in a relatively  high amplitude and frequency that can be readily observed.</p></li>
<li><p>when Sivia was authoring his book, there were no known exoplanets.</p></li>
</ul>
<p>So, we didn‚Äôt have other Jupiter like planets to apply the frequentist approach, but we could use our knowledge about the formation of solar systems to formulate a prior model. Then we could use any available observations or measurements from Jupiter to update this prior. We could do something!</p>
<ul class="simple">
<li><p>but I am getting ahead of myself, we must properly introduce the Bayesian approach to probability, by starting with Bayes‚Äô theorem.</p></li>
</ul>
&#13;

<h2>Bayes‚Äô Theorem</h2>
<p>Once again, here‚Äôs the multiplication rule,</p>
<div class="math notranslate nohighlight">
\[
P(B \cup A) = P(B,A) = P(A|B) \cdot P(B)
\]</div>
<p>and it follows that we can also express it as,</p>
<div class="math notranslate nohighlight">
\[
P(A \cup B) = P(A,B) = P(B|A) \cdot P(A)
\]</div>
<p>If follows that,</p>
<div class="math notranslate nohighlight">
\[
P(B \cup A) = P(A \cup B)
\]</div>
<p>therefore, we can substitute the right-hand sides of the multiplication rules above into this equality as,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) \cdot P(B) = P(B|A) \cdot P(A)
\]</div>
<p>this is Bayes‚Äô theorem. We can make a simple modification to get the popular form of Bayes‚Äô theorem,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\]</div>
<p>To better explain Bayes‚Äô theorem, we replace the <span class="math notranslate nohighlight">\(A\)</span> with ‚ÄúModel‚Äù and <span class="math notranslate nohighlight">\(B\)</span> with ‚ÄúNew Data‚Äù, where ‚ÄúNew Data‚Äù is the new data that we are updating with.</p>
<div class="math notranslate nohighlight">
\[
P(\text{Model}|\text{New Data}) = \frac{P(\text{New Data}|\text{Model}) \cdot P(\text{Model})}{P(\text{New Data})}
\]</div>
<p>now we can see Bayesian updating as taking a model and updating it with new data.</p>
<p>Now we make some observations about Bayes‚Äô theorem,</p>
<ol class="arabic simple">
<li><p>We are reversing conditional probabilities to get <span class="math notranslate nohighlight">\(P(A|B)\)</span> from <span class="math notranslate nohighlight">\(P(B|A)\)</span>, this often comes in handy because we readily have access to one but not the other.</p></li>
<li><p>The probabilities in Bayes‚Äô theorem are known as:</p></li>
</ol>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(B)\)</span> - evidence</p></li>
<li><p><span class="math notranslate nohighlight">\(P(A)\)</span> - prior</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|A)\)</span> - likelihood</p></li>
<li><p><span class="math notranslate nohighlight">\(P(A|B)\)</span> - posterior</p></li>
</ul>
<p>¬†¬†¬†¬†¬† we can substitute the probabilities with their labels,</p>
<div class="math notranslate nohighlight">
\[
\text{Posterior} = \frac{\text{Likelihood} \cdot \text{Prior}}{\text{Evidence}}
\]</div>
<ol class="arabic simple" start="3">
<li><p>Prior should have no information from likelihood.</p></li>
</ol>
<ul class="simple">
<li><p>the prior probability is estimated prior to the collection of the new information in the likelihood. If the prior and likelihood includes new information, then this is ‚Äúdouble accounting‚Äù of the new information!</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p>Evidence term is usually just a standardization to ensure probability closure.</p></li>
</ol>
<ul class="simple">
<li><p>evidence probability is often calculated with marginalization,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(B) = \int_{A} P(B|A) \cdot P(A) dA
\]</div>
<ul class="simple">
<li><p>for the example or two mutually exclusive, exhaustive outcomes, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(A^c\)</span>, then this marginalization can be applied to calculate <span class="math notranslate nohighlight">\(P(B)\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(B) = P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)
\]</div>
<ul class="simple">
<li><p>and by substitution we get this expanded but usually easier to calculate form of Bayes‚Äô theorem,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)}
\]</div>
<ul class="simple">
<li><p>we can generalize this marginalization for any number of discrete, mutually exclusive, exhaustive events as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(B) = \sum_{i=1}^{m} P(B|A_i) \cdot P(A_i), \quad \forall \quad i = 1,\ldots,m
\]</div>
<ul class="simple">
<li><p>in some cases, like Markov chain Monte Carlo with the Metropolis-Hastings algorithm, we eliminate the need for the evidence term because we calculate a probability ratio of the proposed step vs. the current state and the evidence probability cancels out.</p></li>
</ul>
<ol class="arabic simple" start="5">
<li><p>If the prior is na√Øve then the posterior is equal to the likelihood. This is logical, if we know nothing prior to collecting the new data then we completely rely on the likelihood probability.</p></li>
</ol>
<ul class="simple">
<li><p>a na√Øve prior is a maximum uncertainty prior, like the uniform probability with all outcomes equal probable</p></li>
<li><p>under the assumption of standard normal, global Gaussian distribution with a mean of 0.0 and standard deviation of 1.0, a na√Øve prior is the standard normal distribution</p></li>
</ul>
<ol class="arabic simple" start="6">
<li><p>If the likelihood probability is na√Øve then the posterior is equal to the prior. Once again, this is logical, if the new data provides no information, then we should continue to rely on the prior probability and the original model is not updated.</p></li>
</ol>
&#13;

<h2>Bayesian Probability Example Problems</h2>
<p>An efficient way to improve your understanding of Bayesian probability is through solving illustrative problems. Here‚Äôs a great group of problems,</p>
<ul class="simple">
<li><p>you conduct a test and get a positive test result for something we can‚Äôt directly observe happening, but what is the probability of the thing actually happening given the positive test?</p></li>
</ul>
<p>Here‚Äôs some examples of tests that fit in this class of problems,</p>
<figure style="text-align: center;">
  <img src="../Images/a2624fe69f396e2d3677c62b7a46ec9a.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/tests.png"/>
  <figcaption style="text-align: center;"> Example cases of a tests, the underlying event is happening vs. a test that indicates the underlying event is happening. 
</figcaption>
</figure>
<p>and we can rewrite Bayes‚Äô theorem for the case of one of these tests,</p>
<figure style="text-align: center;">
  <img src="../Images/02aa6aa53eb33b77c1b4114e43091536.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/Bayes_tests.png"/>
  <figcaption style="text-align: center;"> Bayes' theorem rewritten for the case of tests. 
</figcaption>
</figure>
<p><strong>Bayesian Updating Example #1</strong> - prior information at a site suggests a deepwater channel reservoir exists at a given location with a probability of 0.6. We consider further investigation with a 3D seismic survey.</p>
<p>3D seismic survey will indicate a channelized reservoir:</p>
<p>‚Äê is present with 0.9 probability if it really is present</p>
<p>‚Äê is not present with a probability 0.7 if it really is not</p>
<p>We have our test, seismic survey, that will offer new data, and we have our prior information (prior to collecting the new seismic data). Now we define our events,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A\)</span> = the deepwater channel is present, then <span class="math notranslate nohighlight">\(P(A)\)</span> is the probability the thing is happening before collecting the new data</p></li>
<li><p><span class="math notranslate nohighlight">\(B\)</span> = new seismic shows a deepwater channel, then <span class="math notranslate nohighlight">\(P(B)\)</span> is the probability of a positive test for the thing happening</p></li>
</ul>
<p>It follows that the compliments are,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A^c\)</span> = the deepwater channel not present, then <span class="math notranslate nohighlight">\(P(A^c)\)</span> is the probability the thing is not happening before collecting the new data</p></li>
<li><p><span class="math notranslate nohighlight">\(B^c\)</span> = new seismic does not show a deepwater channel, then <span class="math notranslate nohighlight">\(P(B^c)\)</span> is the probability of a negative test for the thing happening</p></li>
</ul>
<p>Now we write out Bayes‚Äô theorem, the regular and expanded evidence term by marginalization,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)}
\]</div>
<p>What do we know? From the question above we have,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A) = 0.6\)</span> - ‚Äúprior information at a site suggests a deepwater channel reservoir exists at a given location with a probability of 0.6‚Äù</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|A) = 0.9\)</span> - ‚Äúis present with 0.9 probability if it really is present‚Äù</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B^c|A^c) = 0.7\)</span> - ‚Äúis not present with a probability 0.7 if it really is not‚Äù</p></li>
</ul>
<p>but we also need <span class="math notranslate nohighlight">\(P(A^c)\)</span> and <span class="math notranslate nohighlight">\(P(B|A^c)\)</span>. We calculate these from closure as,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A^c) = 1.0 - P(A) = 1.0 - 0.6 = 0.4\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|A^c) = 1.0 - P(B^c|A^c) = 1.0 - 0.7 = 0.3\)</span></p></li>
</ul>
<p>we have all the information that we need to substitute,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)} = \frac{0.9 \cdot 0.6}{0.9 \cdot 0.6 + 0.3 \cdot 0.4} = 0.82
\]</div>
<p>Given a positive test, seismic indicating a deepwater channel is present, we have posterior probability of 0.82.</p>
<p>Is seismic data useful? How do we access this?</p>
<ul class="simple">
<li><p>consider the change from prior probability, 0.6, to posterior probability, 0.82, showing a reduction in uncertainty. By integrating the value and potential loss of this decision it is possible to now assign the value of information for seismic data.</p></li>
<li><p>I leave decision analysis out of scope for this e-book, but it is a fascinating topic, and I recommend anyone involved in data science to learn at least the basics to ensure you add value by impacting the decision(s)!</p></li>
</ul>
<p>It is quite instructive to conduct a sensitivity on this example problem to assess the behavior of Bayesian updating. You could download and run my <a class="reference external" href="https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb">interactive Bayesian updating dashboard</a> to accomplish this.</p>
<ul class="simple">
<li><p>it is a custom dashboard to visualize Bayesian updating.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/cdc74f980ae59f1da85ef88b5b1e69d0.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/interactive_Bayesian.png"/>
  <figcaption style="text-align: center;"> My interactive Python dashboard demonstrating Bayesian updating. Note, \(H\) represents the thing is happening (\(A\) in our example), and \(+\) represents a positive test and \(-\) represents a negative test (\(B\) and \(B^c\) respectively in our example). 
</figcaption>
</figure>
<p>We have improved the test precision, probability of seismic detecting a channel given it is present, <span class="math notranslate nohighlight">\(P(B|A)\)</span> from 0.9 to 0.99.</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)} = \frac{0.99 \cdot 0.6}{0.99 \cdot 0.6 + 0.3 \cdot 0.4} = 0.83
\]</div>
<p>we have almost no change in the posterior probability. Now we have improved our ability to detect no channel given the channel is not present, <span class="math notranslate nohighlight">\(P(B|A^c)\)</span> from 0.3 to 0.03.</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)} = \frac{0.9 \cdot 0.6}{0.9 \cdot 0.6 + 0.03 \cdot 0.4} = 0.98
\]</div>
<p>we change the posterior probability from 0.82 to 0.98! What happened?</p>
<ul class="simple">
<li><p>our test was being impacted by a relatively high probability of false positive, seismic showing a channel when there is no channel present, <span class="math notranslate nohighlight">\(P(B|A^c) = 0.3\)</span> paired with a relatively high rate of no channel present, <span class="math notranslate nohighlight">\(P(A^c) = 0.4\)</span>.</p></li>
</ul>
<p>now we switch gears and look at another illustrative example of Bayesian updating with machines on a production line.</p>
<p><strong>Bayesian Updating Example #2</strong> - You have 3 machines making the same product (imagine a large production line). They have different volumes and errors.</p>
<ul class="simple">
<li><p>Note we assume the products are mutually exclusive (only come from a single machine), and exhaustive (all products come from one of the 3 machines).</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/2bbde9fa70803137b7d119b3a7e9636c.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/Bayesian_machines.png"/>
  <figcaption style="text-align: center;"> Machines 1, 2 and 3, produce the same product that are then mixed in the assembly line. They each have different production rates (percentage of total and error rates).
</figcaption>
</figure>
<p>We want to know given an error in an individual product, what is the probability that the product came from a specific machine? First, we define our variables,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y\)</span> = the product has an error</p></li>
<li><p><span class="math notranslate nohighlight">\(X_1, X_2, X_3\)</span> = the product came from machines 1, 2 or 3 respectively.</p></li>
</ul>
<p>our Bayesian formulation for probability of defective (product with error) product coming from a specific machine is,</p>
<div class="math notranslate nohighlight">
\[
P(X_i|Y) = \frac{P(Y|X_i) \cdot P(X_i)}{P(Y)} \quad \forall \quad i = 1, 2, 3
\]</div>
<p>we will need to calculate our evidence term, P(Y) to solve this for any product. Also, we only must do this once, the evidence term is the same for all products.</p>
<p>We will apply marginalization to accomplish this by substituting our variables into the previously introduced general equation as,</p>
<div class="math notranslate nohighlight">
\[
P(Y) = \sum_{i=1}^{m} P(Y|X_i) \cdot P(X_i), \quad \forall \quad i = 1,\ldots,3
\]</div>
<p>our specific form for our problem is,</p>
<div class="math notranslate nohighlight">
\[
P(Y) = P(Y|X_1) \cdot P(X_1) + P(Y|X_2) \cdot P(X_2) + P(Y|X_3) \cdot P(X_3) 
\]</div>
<p>we substitute the probabilities from the problem statement as,</p>
<div class="math notranslate nohighlight">
\[
P(Y) = 0.2 \cdot 0.05 + 0.3 \cdot 0.03 + 0.5 \cdot 0.01 = 0.024
\]</div>
<p>Given this evidence probability, now we can calculate the posterior probabilities of the product coming from each machine given the product is defective,</p>
<div class="math notranslate nohighlight">
\[
P(X_1|Y) = \frac{P(Y|X_1) \cdot P(X_1)}{P(Y)} = \frac{0.05 \cdot 0.2}{0.024} = 0.41
\]</div>
<div class="math notranslate nohighlight">
\[
P(X_2|Y) = \frac{P(Y|X_2) \cdot P(X_2)}{P(Y)} = \frac{0.03 \cdot 0.3}{0.024} = 0.38
\]</div>
<div class="math notranslate nohighlight">
\[
P(X_3|Y) = \frac{P(Y|X_3) \cdot P(X_3)}{P(Y)} = \frac{0.01 \cdot 0.5}{0.024} = 0.21
\]</div>
<p>Let us close the loop by checking closure, we expect these posterior probabilities to sum to 1.0 over all machines (once again assuming products are mutually exclusive, and exhaustive over the machines),</p>
<div class="math notranslate nohighlight">
\[
P(X_1|Y) + P(X_2|Y) + P(X_3|Y) = 0.41 + 0.38 + 0.21 = 1.0
\]</div>
<p>and we have closure.</p>
<p>These two examples are very helpful to understand Bayesian updating. In the classroom I get my students to calculate the additional posteriors such as the probability of the event happening given a negative test, etc.</p>
<ul class="simple">
<li><p>my second dashboard in this Jupyter notebook <a class="reference external" href="https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb">interactive Bayesian updating dashboard</a> includes these examples.</p></li>
<li><p>I invite you to play with Bayesian updating.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/25502cc12d722a9c207d481f3da67a20.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/interactive_Bayesian_all.png"/>
  <figcaption style="text-align: center;"> My more complete interactive Python dashboard demonstrating Bayesian updating for all possible posterior probabilities. Note, \(H\) represents the thing is happening \(A\) in our example, and \(+\) represents a positive test and \(-\) represents a negative test, \(B\) and \(B^c\) respectively in our example. 
</figcaption>
</figure>
&#13;

<h2>Bayesian Updating with Gaussian Distributions</h2>
<p>Sivia (1996) provided an analytical approach for Bayesian updating under the assumption of a standard normal global distribution, Gaussian distributed with mean of 0.0 and variance of 1.0. We calculate,</p>
<ul class="simple">
<li><p>the mean of the posterior distribution from the prior and likelihood mean and variance.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mu_{\text{posterior}} = \frac{\mu_{\text{likelihood}} \cdot \sigma^2_{\text{prior}} + \mu_{\text{prior}} \cdot \sigma^2_{\text{likelihood}}}{\left[1.0 ‚àí \sigma^2_{\text{likelihood}} \right] \cdot \left[\sigma^2_{\text{prior}} ‚àí 1.0 \right]+1.0}
\]</div>
<ul class="simple">
<li><p>the variance of the posterior distribution from the prior and likelihood variances</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sigma^2_{\text{posterior}} = \frac{\sigma^2_{\text{prior}} \cdot \sigma^2_{\text{likelihood}}}{\left[1.0 ‚àí \sigma^2_{\text{likelihood}} \right] \cdot \left[\sigma^2_{\text{prior}} ‚àí 1.0 \right]+1.0}
\]</div>
<ul class="simple">
<li><p>consistent with the multivariate Gaussian distribution this posterior is homoscedastic, the prior or likelihood means are not in the equation for posterior variance.</p></li>
<li><p>my third dashboard in this Jupyter notebook <a class="reference external" href="https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb">interactive Bayesian updating dashboard</a> includes Bayesian updating with Gaussian distributions.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/1a20eb3b488e5034f7fbf4ed5cafb8b8.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/probability/interactive_Bayesian_gaussian.png"/>
  <figcaption style="text-align: center;"> My interactive Python dashboard demonstrating Bayesian updating under the assumption of standard normal global distribution.
</figcaption>
</figure>
<p>This example helps my students understand the concept of Bayesian updating for building uncertainty distributions. Here are some things to try out,</p>
<ol class="arabic simple">
<li><p>a na√Øve prior or likelihood and observe the posterior is the same as the likelihood or prior, respectively.</p></li>
<li><p>a very certain, low variance prior. Start with a large prior variance and as you reduce the variance, the posterior is pulled towards the prior. The influence of prior and likelihood is proportional to their relative certainty.</p></li>
<li><p>a contradiction between prior and likelihood with very different means. You may be able to even introduce extrapolation, a low prior positive mean with higher positive likelihood mean can result in an even higher positive posterior mean.</p></li>
</ol>
&#13;

<h2>Comments</h2>
<p>This was a basic treatment of probability. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a> and the YouTube lecture links at the start of this chapter with resource links in the videos‚Äô descriptions.</p>
<p>I hope this is helpful,</p>
<p><em>Michael</em></p>
&#13;

<h2>About the Author</h2>
<figure style="text-align: center;">
  <img src="../Images/eb709b2c0a0c715da01ae0165efdf3b2.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/intro/michael_pyrcz_officeshot_jacket.jpg"/>
  <figcaption style="text-align: center;"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.
</figcaption>
</figure>
<p>Michael Pyrcz is a professor in the <a class="reference external" href="https://cockrell.utexas.edu/faculty-directory/alphabetical/p">Cockrell School of Engineering</a>, and the <a class="reference external" href="https://www.jsg.utexas.edu/researcher/michael_pyrcz/">Jackson School of Geosciences</a>, at <a class="reference external" href="https://www.utexas.edu/">The University of Texas at Austin</a>, where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,</p>
<ul class="simple">
<li><p>the principal investigator of the <a class="reference external" href="https://fri.cns.utexas.edu/energy-analytics">Energy Analytics</a> freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin</p></li>
<li><p>an associate editor for <a class="reference external" href="https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board">Computers and Geosciences</a>, and a board member for <a class="reference external" href="https://link.springer.com/journal/11004/editorial-board">Mathematical Geosciences</a>, the International Association for Mathematical Geosciences.</p></li>
</ul>
<p>Michael has written over 70 <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en">peer-reviewed publications</a>, a <a class="reference external" href="https://pypi.org/project/geostatspy/">Python package</a> for spatial data analytics, co-authored a textbook on spatial data analytics, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a> and author of two recently released e-books, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy</a> and <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html">Applied Machine Learning in Python: a Hands-on Guide with Code</a>.</p>
<p>All of Michael‚Äôs university lectures are available on his <a class="reference external" href="https://www.youtube.com/@GeostatsGuyLectures">YouTube Channel</a> with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his <a class="reference external" href="https://github.com/GeostatsGuy">GitHub account</a>, to support any interested students and working professionals with evergreen content. To find out more about Michael‚Äôs work and shared educational resources visit his <span class="xref myst">Website</span>.</p>
&#13;

<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>I‚Äôm always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
    
</body>
</html>