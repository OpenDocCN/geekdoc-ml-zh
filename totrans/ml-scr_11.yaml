- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://dafriedman97.github.io/mlbook/content/c2/code.html](https://dafriedman97.github.io/mlbook/content/c2/code.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This section shows how the linear regression extensions discussed in this chapter
    are typically fit in Python. First let’s import the [Boston housing](../appendix/data.html)
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Regularized Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both Ridge and Lasso regression can be easily fit using `scikit-learn`. A bare-bones
    implementation is provided below. Note that the regularization parameter `alpha`
    (which we called \(\lambda\)) is chosen arbitrarily.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In practice, however, we want to choose `alpha` through cross validation. This
    is easily implemented in `scikit-learn` by designating a set of `alpha` values
    to try and fitting the model with `RidgeCV` or `LassoCV`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can then see which values of `alpha` performed best with the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Bayesian Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also fit Bayesian regression using `scikit-learn` (though another popular
    package is `pymc3`). A very straightforward implementation is provided below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is not, however, identical to our construction in the previous section
    since it infers the \(\sigma^2\) and \(\tau\) parameters, rather than taking those
    as fixed inputs. More information can be found [here](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression).
    The hidden chunk below demonstrates a hacky solution for running Bayesian regression
    in `scikit-learn` using known values for \(\sigma^2\) and \(\tau\), though it
    is hard to imagine a practical reason to do so
  prefs: []
  type: TYPE_NORMAL
- en: By default, Bayesian regression in `scikit-learn` treats \(\alpha = \frac{1}{\sigma^2}\)
    and \(\lambda = \frac{1}{\tau}\) as random variables and assigns them the following
    prior distributions
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{aligned} \alpha &\sim \text{Gamma}(\alpha_1, \alpha_2)
    \\ \lambda &\sim \text{Gamma}(\lambda_1, \lambda_2). \end{aligned} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Note that \(E(\alpha) = \frac{\alpha_1}{\alpha_2}\) and \(E(\lambda) = \frac{\lambda_1}{\lambda_2}\).
    To *fix* \(\sigma^2\) and \(\tau\), we can provide an extremely strong prior on
    \(\alpha\) and \(\lambda\), guaranteeing that their estimates will be approximately
    equal to their expected value.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we want to use \(\sigma^2 = 11.8\) and \(\tau = 10\), or equivalently
    \(\alpha = \frac{1}{11.8}\), \(\lambda = \frac{1}{10}\). Then let
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{aligned} \alpha_1 &= 10000 \cdot \frac{1}{11.8}, \\ \alpha_2
    &= 10000, \\ \lambda_1 &= 10000 \cdot \frac{1}{10}, \\ \lambda_2 &= 10000. \end{aligned}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: This guarantees that \(\sigma^2\) and \(\tau\) will be approximately equal to
    their pre-determined values. This can be implemented in `scikit-learn` as follows
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Poisson Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GLMs are most commonly fit in Python through the `GLM` class from `statsmodels`.
    A simple Poisson regression example is given below.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the GLM concept section, a GLM is comprised of a random distribution
    and a link function. We identify the random distribution through the `family`
    argument to `GLM` (e.g. below, we specify the `Poisson` family). The default link
    function depends on the random distribution. By default, the Poisson model uses
    the link function
  prefs: []
  type: TYPE_NORMAL
- en: \[ \eta_n = g(\mu_n) = \log(\lambda_n), \]
  prefs: []
  type: TYPE_NORMAL
- en: which is what we use below. For more information on the possible distributions
    and link functions, check out the `statsmodels` GLM [docs](https://www.statsmodels.org/stable/glm.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Regularized Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both Ridge and Lasso regression can be easily fit using `scikit-learn`. A bare-bones
    implementation is provided below. Note that the regularization parameter `alpha`
    (which we called \(\lambda\)) is chosen arbitrarily.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In practice, however, we want to choose `alpha` through cross validation. This
    is easily implemented in `scikit-learn` by designating a set of `alpha` values
    to try and fitting the model with `RidgeCV` or `LassoCV`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can then see which values of `alpha` performed best with the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Bayesian Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also fit Bayesian regression using `scikit-learn` (though another popular
    package is `pymc3`). A very straightforward implementation is provided below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is not, however, identical to our construction in the previous section
    since it infers the \(\sigma^2\) and \(\tau\) parameters, rather than taking those
    as fixed inputs. More information can be found [here](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression).
    The hidden chunk below demonstrates a hacky solution for running Bayesian regression
    in `scikit-learn` using known values for \(\sigma^2\) and \(\tau\), though it
    is hard to imagine a practical reason to do so
  prefs: []
  type: TYPE_NORMAL
- en: By default, Bayesian regression in `scikit-learn` treats \(\alpha = \frac{1}{\sigma^2}\)
    and \(\lambda = \frac{1}{\tau}\) as random variables and assigns them the following
    prior distributions
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{aligned} \alpha &\sim \text{Gamma}(\alpha_1, \alpha_2)
    \\ \lambda &\sim \text{Gamma}(\lambda_1, \lambda_2). \end{aligned} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Note that \(E(\alpha) = \frac{\alpha_1}{\alpha_2}\) and \(E(\lambda) = \frac{\lambda_1}{\lambda_2}\).
    To *fix* \(\sigma^2\) and \(\tau\), we can provide an extremely strong prior on
    \(\alpha\) and \(\lambda\), guaranteeing that their estimates will be approximately
    equal to their expected value.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we want to use \(\sigma^2 = 11.8\) and \(\tau = 10\), or equivalently
    \(\alpha = \frac{1}{11.8}\), \(\lambda = \frac{1}{10}\). Then let
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{aligned} \alpha_1 &= 10000 \cdot \frac{1}{11.8}, \\ \alpha_2
    &= 10000, \\ \lambda_1 &= 10000 \cdot \frac{1}{10}, \\ \lambda_2 &= 10000. \end{aligned}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: This guarantees that \(\sigma^2\) and \(\tau\) will be approximately equal to
    their pre-determined values. This can be implemented in `scikit-learn` as follows
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Poisson Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GLMs are most commonly fit in Python through the `GLM` class from `statsmodels`.
    A simple Poisson regression example is given below.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the GLM concept section, a GLM is comprised of a random distribution
    and a link function. We identify the random distribution through the `family`
    argument to `GLM` (e.g. below, we specify the `Poisson` family). The default link
    function depends on the random distribution. By default, the Poisson model uses
    the link function
  prefs: []
  type: TYPE_NORMAL
- en: \[ \eta_n = g(\mu_n) = \log(\lambda_n), \]
  prefs: []
  type: TYPE_NORMAL
- en: which is what we use below. For more information on the possible distributions
    and link functions, check out the `statsmodels` GLM [docs](https://www.statsmodels.org/stable/glm.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
