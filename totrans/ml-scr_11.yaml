- en: Implementation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: 原文：[https://dafriedman97.github.io/mlbook/content/c2/code.html](https://dafriedman97.github.io/mlbook/content/c2/code.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://dafriedman97.github.io/mlbook/content/c2/code.html](https://dafriedman97.github.io/mlbook/content/c2/code.html)
- en: This section shows how the linear regression extensions discussed in this chapter
    are typically fit in Python. First let’s import the [Boston housing](../appendix/data.html)
    dataset.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了本章讨论的线性回归扩展在Python中的典型拟合方法。首先，让我们导入[Boston住房](../appendix/data.html)数据集。
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Regularized Regression
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化回归
- en: Both Ridge and Lasso regression can be easily fit using `scikit-learn`. A bare-bones
    implementation is provided below. Note that the regularization parameter `alpha`
    (which we called \(\lambda\)) is chosen arbitrarily.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Ridge回归和Lasso回归都可以很容易地使用`scikit-learn`进行拟合。下面提供了一个基本的实现示例。请注意，正则化参数`alpha`（我们称之为\(\lambda\)）是任意选择的。
- en: '[PRE1]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In practice, however, we want to choose `alpha` through cross validation. This
    is easily implemented in `scikit-learn` by designating a set of `alpha` values
    to try and fitting the model with `RidgeCV` or `LassoCV`.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，我们希望通过交叉验证来选择`alpha`。这通过指定一组要尝试的`alpha`值并在`RidgeCV`或`LassoCV`中拟合模型很容易地在`scikit-learn`中实现。
- en: '[PRE2]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can then see which values of `alpha` performed best with the following.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到以下哪些`alpha`值表现最好。
- en: '[PRE3]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Bayesian Regression
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯回归
- en: We can also fit Bayesian regression using `scikit-learn` (though another popular
    package is `pymc3`). A very straightforward implementation is provided below.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用`scikit-learn`（尽管另一个流行的包是`pymc3`）来拟合贝叶斯回归。下面提供了一个非常直接的实现示例。
- en: '[PRE5]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is not, however, identical to our construction in the previous section
    since it infers the \(\sigma^2\) and \(\tau\) parameters, rather than taking those
    as fixed inputs. More information can be found [here](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression).
    The hidden chunk below demonstrates a hacky solution for running Bayesian regression
    in `scikit-learn` using known values for \(\sigma^2\) and \(\tau\), though it
    is hard to imagine a practical reason to do so
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不等同于前一部分中的构造，因为它推断\(\sigma^2\)和\(\tau\)参数，而不是将它们作为固定输入。更多信息可以在[这里](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression)找到。下面的隐藏部分展示了在`scikit-learn`中使用已知的\(\sigma^2\)和\(\tau\)值运行贝叶斯回归的笨拙解决方案，尽管很难想象有实际的理由这样做。
- en: By default, Bayesian regression in `scikit-learn` treats \(\alpha = \frac{1}{\sigma^2}\)
    and \(\lambda = \frac{1}{\tau}\) as random variables and assigns them the following
    prior distributions
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`scikit-learn`中的贝叶斯回归将\(\alpha = \frac{1}{\sigma^2}\)和\(\lambda = \frac{1}{\tau}\)视为随机变量，并赋予它们以下先验分布
- en: \[\begin{split} \begin{aligned} \alpha &\sim \text{Gamma}(\alpha_1, \alpha_2)
    \\ \lambda &\sim \text{Gamma}(\lambda_1, \lambda_2). \end{aligned} \end{split}\]
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{aligned} \alpha &\sim \text{Gamma}(\alpha_1, \alpha_2)
    \\ \lambda &\sim \text{Gamma}(\lambda_1, \lambda_2). \end{aligned} \end{split}\]
- en: Note that \(E(\alpha) = \frac{\alpha_1}{\alpha_2}\) and \(E(\lambda) = \frac{\lambda_1}{\lambda_2}\).
    To *fix* \(\sigma^2\) and \(\tau\), we can provide an extremely strong prior on
    \(\alpha\) and \(\lambda\), guaranteeing that their estimates will be approximately
    equal to their expected value.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，\(E(\alpha) = \frac{\alpha_1}{\alpha_2}\)和\(E(\lambda) = \frac{\lambda_1}{\lambda_2}\)。为了*固定*
    \(\sigma^2\)和\(\tau\)，我们可以为\(\alpha\)和\(\lambda\)提供一个非常强的先验，从而保证它们的估计值将大约等于它们的期望值。
- en: Suppose we want to use \(\sigma^2 = 11.8\) and \(\tau = 10\), or equivalently
    \(\alpha = \frac{1}{11.8}\), \(\lambda = \frac{1}{10}\). Then let
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想使用\(\sigma^2 = 11.8\)和\(\tau = 10\)，或者等价地\(\alpha = \frac{1}{11.8}\)，\(\lambda
    = \frac{1}{10}\)。那么，让我们
- en: \[\begin{split} \begin{aligned} \alpha_1 &= 10000 \cdot \frac{1}{11.8}, \\ \alpha_2
    &= 10000, \\ \lambda_1 &= 10000 \cdot \frac{1}{10}, \\ \lambda_2 &= 10000. \end{aligned}
    \end{split}\]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{aligned} \alpha_1 &= 10000 \cdot \frac{1}{11.8}, \\ \alpha_2
    &= 10000, \\ \lambda_1 &= 10000 \cdot \frac{1}{10}, \\ \lambda_2 &= 10000. \end{aligned}
    \end{split}\]
- en: This guarantees that \(\sigma^2\) and \(\tau\) will be approximately equal to
    their pre-determined values. This can be implemented in `scikit-learn` as follows
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这保证了\(\sigma^2\)和\(\tau\)将大约等于它们预定的值。这可以在`scikit-learn`中如下实现
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Poisson Regression
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 泊松回归
- en: GLMs are most commonly fit in Python through the `GLM` class from `statsmodels`.
    A simple Poisson regression example is given below.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GLMs通常通过`statsmodels`中的`GLM`类在Python中进行拟合。下面提供了一个简单的泊松回归示例。
- en: As we saw in the GLM concept section, a GLM is comprised of a random distribution
    and a link function. We identify the random distribution through the `family`
    argument to `GLM` (e.g. below, we specify the `Poisson` family). The default link
    function depends on the random distribution. By default, the Poisson model uses
    the link function
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在GLM概念部分中看到的，GLM由一个随机分布和一个链接函数组成。我们通过`GLM`函数的`family`参数来识别随机分布（例如，下面我们指定了`Poisson`分布）。默认链接函数取决于随机分布。默认情况下，泊松模型使用以下链接函数
- en: \[ \eta_n = g(\mu_n) = \log(\lambda_n), \]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \eta_n = g(\mu_n) = \log(\lambda_n), \]
- en: which is what we use below. For more information on the possible distributions
    and link functions, check out the `statsmodels` GLM [docs](https://www.statsmodels.org/stable/glm.html).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们下面使用的。有关可能的分布和链接函数的更多信息，请查看`statsmodels` GLM [文档](https://www.statsmodels.org/stable/glm.html)。
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Regularized Regression
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化回归
- en: Both Ridge and Lasso regression can be easily fit using `scikit-learn`. A bare-bones
    implementation is provided below. Note that the regularization parameter `alpha`
    (which we called \(\lambda\)) is chosen arbitrarily.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Ridge和Lasso回归都可以很容易地使用`scikit-learn`进行拟合。下面提供了一个基本的实现。请注意，正则化参数`alpha`（我们称之为\(\lambda\)）是任意选择的。
- en: '[PRE8]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In practice, however, we want to choose `alpha` through cross validation. This
    is easily implemented in `scikit-learn` by designating a set of `alpha` values
    to try and fitting the model with `RidgeCV` or `LassoCV`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，我们希望通过交叉验证来选择`alpha`。这通过在`scikit-learn`中指定一组要尝试的`alpha`值并通过`RidgeCV`或`LassoCV`拟合模型来实现，这是很容易的。
- en: '[PRE9]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can then see which values of `alpha` performed best with the following.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以看到以下哪些`alpha`值表现最好。
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Bayesian Regression
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯回归
- en: We can also fit Bayesian regression using `scikit-learn` (though another popular
    package is `pymc3`). A very straightforward implementation is provided below.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用`scikit-learn`进行贝叶斯回归（尽管另一个流行的包是`pymc3`）。下面提供了一个非常直接的实现。
- en: '[PRE12]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is not, however, identical to our construction in the previous section
    since it infers the \(\sigma^2\) and \(\tau\) parameters, rather than taking those
    as fixed inputs. More information can be found [here](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression).
    The hidden chunk below demonstrates a hacky solution for running Bayesian regression
    in `scikit-learn` using known values for \(\sigma^2\) and \(\tau\), though it
    is hard to imagine a practical reason to do so
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不等同于上一节中的我们的构造，因为它推断出\(\sigma^2\)和\(\tau\)参数，而不是将它们作为固定输入。更多信息可以在[这里](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression)找到。下面的隐藏部分演示了一个在`scikit-learn`中使用已知的\(\sigma^2\)和\(\tau\)值进行贝叶斯回归的笨拙解决方案，尽管很难想象这样做有实际的理由。
- en: By default, Bayesian regression in `scikit-learn` treats \(\alpha = \frac{1}{\sigma^2}\)
    and \(\lambda = \frac{1}{\tau}\) as random variables and assigns them the following
    prior distributions
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`scikit-learn`中的贝叶斯回归将\(\alpha = \frac{1}{\sigma^2}\)和\(\lambda = \frac{1}{\tau}\)视为随机变量，并赋予它们以下先验分布
- en: \[\begin{split} \begin{aligned} \alpha &\sim \text{Gamma}(\alpha_1, \alpha_2)
    \\ \lambda &\sim \text{Gamma}(\lambda_1, \lambda_2). \end{aligned} \end{split}\]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{aligned} \alpha &\sim \text{Gamma}(\alpha_1, \alpha_2)
    \\ \lambda &\sim \text{Gamma}(\lambda_1, \lambda_2). \end{aligned} \end{split}\]
- en: Note that \(E(\alpha) = \frac{\alpha_1}{\alpha_2}\) and \(E(\lambda) = \frac{\lambda_1}{\lambda_2}\).
    To *fix* \(\sigma^2\) and \(\tau\), we can provide an extremely strong prior on
    \(\alpha\) and \(\lambda\), guaranteeing that their estimates will be approximately
    equal to their expected value.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，\(E(\alpha) = \frac{\alpha_1}{\alpha_2}\)和\(E(\lambda) = \frac{\lambda_1}{\lambda_2}\)。为了*固定*
    \(\sigma^2\)和\(\tau\)，我们可以为\(\alpha\)和\(\lambda\)提供一个非常强的先验，从而保证它们的估计值将大约等于它们的期望值。
- en: Suppose we want to use \(\sigma^2 = 11.8\) and \(\tau = 10\), or equivalently
    \(\alpha = \frac{1}{11.8}\), \(\lambda = \frac{1}{10}\). Then let
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想使用\(\sigma^2 = 11.8\)和\(\tau = 10\)，或者等价地\(\alpha = \frac{1}{11.8}\)，\(\lambda
    = \frac{1}{10}\)。那么让我们
- en: \[\begin{split} \begin{aligned} \alpha_1 &= 10000 \cdot \frac{1}{11.8}, \\ \alpha_2
    &= 10000, \\ \lambda_1 &= 10000 \cdot \frac{1}{10}, \\ \lambda_2 &= 10000. \end{aligned}
    \end{split}\]
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{aligned} \alpha_1 &= 10000 \cdot \frac{1}{11.8}, \\ \alpha_2
    &= 10000, \\ \lambda_1 &= 10000 \cdot \frac{1}{10}, \\ \lambda_2 &= 10000. \end{aligned}
    \end{split}\]
- en: This guarantees that \(\sigma^2\) and \(\tau\) will be approximately equal to
    their pre-determined values. This can be implemented in `scikit-learn` as follows
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这保证了\(\sigma^2\)和\(\tau\)将大约等于它们预定的值。这可以在`scikit-learn`中如下实现
- en: '[PRE13]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Poisson Regression
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 泊松回归
- en: GLMs are most commonly fit in Python through the `GLM` class from `statsmodels`.
    A simple Poisson regression example is given below.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: GLM通常在Python中通过`statsmodels`的`GLM`类进行拟合。下面给出了一个简单的泊松回归示例。
- en: As we saw in the GLM concept section, a GLM is comprised of a random distribution
    and a link function. We identify the random distribution through the `family`
    argument to `GLM` (e.g. below, we specify the `Poisson` family). The default link
    function depends on the random distribution. By default, the Poisson model uses
    the link function
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在GLM概念部分所看到的，GLM由一个随机分布和一个链接函数组成。我们通过`GLM`的`family`参数来识别随机分布（例如，下面我们指定了`Poisson`分布）。默认链接函数取决于随机分布。默认情况下，泊松模型使用链接函数
- en: \[ \eta_n = g(\mu_n) = \log(\lambda_n), \]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \eta_n = g(\mu_n) = \log(\lambda_n), \]
- en: which is what we use below. For more information on the possible distributions
    and link functions, check out the `statsmodels` GLM [docs](https://www.statsmodels.org/stable/glm.html).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们下面将要使用的。有关可能的分布和链接函数的更多信息，请查看`statsmodels` GLM [文档](https://www.statsmodels.org/stable/glm.html)。
- en: '[PRE14]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
