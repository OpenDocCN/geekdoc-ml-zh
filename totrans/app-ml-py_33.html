<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Autoencoder</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Autoencoder</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_autoencoder.html">https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_autoencoder.html</a></blockquote>

<p>Michael J. Pyrcz, Professor, The University of Texas at Austin</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
<p>Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with Code‚Äù.</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite this e-Book as:</p>
<p>Pyrcz, M.J., 2024, <em>Applied Machine Learning in Python: A Hands-on Guide with Code</em> [e-book]. Zenodo. doi:10.5281/zenodo.15169138 <a class="reference external" href="https://doi.org/10.5281/zenodo.15169138"><img alt="DOI" src="../Images/7e4ea662f44af1eae87e87ecbb962ff4.png" data-original-src="https://zenodo.org/badge/863274676.svg"/></a></p>
</div>
<p>The workflows in this book and more are available here:</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite the MachineLearningDemos GitHub Repository as:</p>
<p>Pyrcz, M.J., 2024, <em>MachineLearningDemos: Python Machine Learning Demonstration Workflows Repository</em> (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312. GitHub repository: <a class="github reference external" href="https://github.com/GeostatsGuy/MachineLearningDemos">GeostatsGuy/MachineLearningDemos</a> <a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.13835312"><img alt="DOI" src="../Images/4e3a59c17d684b06a170c4af84e0f631.png" data-original-src="https://zenodo.org/badge/862519860.svg"/></a></p>
</div>
<p>By Michael J. Pyrcz <br/>
¬© Copyright 2024.</p>
<p>This chapter is a tutorial for / demonstration of <strong>Autoencoders</strong>.</p>
<p><strong>YouTube Lecture</strong>: check out my lectures on:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://youtu.be/A9PiCMY_6nM?si=NxWSU_5RgQ4w55EL">Artificial Neural Networks</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/za2my_XDoOs?si=LeHU6p2_fc9dX4Yt">Convolutional Neural Networks</a></p></li>
</ul>
<p>These lectures are all part of my <a class="reference external" href="https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&amp;si=XonjO2wHdXffMpeI">Machine Learning Course</a> on YouTube with linked well-documented Python workflows and interactive dashboards. My goal is to share accessible, actionable, and repeatable educational content. If you want to know about my motivation, check out <a class="reference external" href="https://michaelpyrcz.com/my-story">Michael‚Äôs Story</a>.</p>
<section id="motivation">
<h2>Motivation</h2>
<p>Autoencoders are a very powerful, flexible deep learning approach for compressing information,</p>
<ul class="simple">
<li><p>mapping training data to a latent space</p></li>
<li><p>dimensionality reduction of high dimensional data to a much lower dimensionality</p></li>
<li><p>nonlinear, general approach</p></li>
</ul>
</section>
<section id="autoencoder-architecture">
<h2>Autoencoder Architecture</h2>
<p>Here‚Äôs our simple autoencoder,</p>
<figure style="text-align: center;">
  <img src="../Images/ed815fe23f4bd258b278f7aa6f0dd58e.png" style="display: block; margin: 0 auto; width: 90%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/autoencoder.png"/>
  <figcaption style="text-align: center;"> Simple demonstration autoencoder.
</figcaption>
</figure>
<p>This is literally the artificial neural network from the <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ANN.html">Artificial Neural Networks</a> mirrored.</p>
<p>I do not discuss the forward pass through the network, if you are unfamiliar with this process, for example,</p>
<ul class="simple">
<li><p>activation applied to the linear weighting plus bias in the nodes</p></li>
</ul>
<p>then please review the artificial neural network chapter.</p>
<p>I decided to use unique numerical indices for each node for concise notation for connection weights, for example <span class="math notranslate nohighlight">\(\lambda_{1,4}\)</span>, and biases, for example, <span class="math notranslate nohighlight">\(b_4\)</span>, <span class="math notranslate nohighlight">\(I\)</span> for input nodes, <span class="math notranslate nohighlight">\(L\)</span> for encoder hidden layer (‚Äòleft‚Äô), <span class="math notranslate nohighlight">\(M\)</span> for latent node (‚Äòmiddle‚Äô), <span class="math notranslate nohighlight">\(R\)</span> for decoder hidden layer (‚Äòright‚Äô) and finally <span class="math notranslate nohighlight">\(O\)</span> for output nodes.</p>
<p>The parts of the autoencoder are indicated below,</p>
<figure style="text-align: center;">
  <img src="../Images/652eb880f88b2d046adcc751aa2d62f6.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/autoencoderparts.png"/>
  <figcaption style="text-align: center;"> Simple demonstration autoencoder with parts labeled.
</figcaption>
</figure>
<p>The signal passed through the autoencoder and notation include,</p>
<ul class="simple">
<li><p><strong>Input</strong> ‚Äì training samples,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
z
\]</div>
<ul class="simple">
<li><p><strong>Encoder</strong> ‚Äì learned compression of the training samples to latent space,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
x = f_{\theta} (z)
\]</div>
<ul class="simple">
<li><p><strong>Latent Space</strong> ‚Äì bottleneck summarizes patterns in the training data,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
ùë•
\]</div>
<ul class="simple">
<li><p><strong>Decoder</strong> ‚Äì learned decompression of the latent space to reconstruction of the original training data,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{z} = g_{\phi} (x) = g_{\phi} (f_{\theta}(z) )
\]</div>
<ul class="simple">
<li><p>Reconstruction ‚Äì attempt to reproduce input,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{z} \sim z
\]</div>
</section>
<section id="training-model-parameters">
<h2>Training Model Parameters</h2>
<p>Training an autoencoder proceeds iteratively by these steps.</p>
<figure style="text-align: center;">
  <img src="../Images/c3a5bc8956f8ceda05ddf9b582cd141d.png" style="display: block; margin: 0 auto; width: 90%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ANN/training_cycle.png"/>
  <figcaption style="text-align: center;"> Training an artificial neural network proceeds iteratively by, 1. forward pass to make a prediction, 2. calculate the error derivative based on the prediction and truth over training data, 3. backpropagate the error derivative back through the artificial neural network to calculate the derivatives of the error over all the model weights and biases parameters, 4. update the model parameters based on the derivatives and learning rates, 5. repeat until convergence.
</figcaption>
</figure>
<p>Here‚Äôs some details on each step,</p>
<ol class="arabic simple" start="0">
<li><p><strong>Initializing the Model Parameters</strong> - initialize all model parameters with typically small (near zero) random values. Here‚Äôs a couple common methods,</p></li>
</ol>
<ul class="simple">
<li><p><strong>Xavier Weight Initialization</strong> - random realizations from uniform distributions specified by <span class="math notranslate nohighlight">\(U[\text{min}, \text{max}]\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}}, \frac{1}{\sqrt{p}} \right] (p^\ell)
\]</div>
<ul class="simple">
<li><p>where <span class="math notranslate nohighlight">\(F^{-1}_U\)</span> is the inverse of the CDF, <span class="math notranslate nohighlight">\(p\)</span> is the number of inputs, and <span class="math notranslate nohighlight">\(p^{\ell}\)</span> is a random cumulative probability value drawn from the uniform distribution, <span class="math notranslate nohighlight">\(U[0,1]\)</span>.</p></li>
<li><p><strong>Normalized Xavier Weight Initialization</strong> - random realizations from uniform distributions specified by <span class="math notranslate nohighlight">\(U[\text{min}, \text{max}]\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] (p^\ell)
\]</div>
<ul class="simple">
<li><p>where <span class="math notranslate nohighlight">\(F^{-1}_U\)</span> is the inverse of the CDF, <span class="math notranslate nohighlight">\(p\)</span> is the number of inputs, <span class="math notranslate nohighlight">\(k\)</span> is the number of outputs, and <span class="math notranslate nohighlight">\(p^{\ell}\)</span> is a random cumulative probability value drawn from the uniform distribution, <span class="math notranslate nohighlight">\(U[0,1]\)</span>.</p></li>
<li><p>For example, if we return to our first hidden layer node,</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/b2f8e46ea497049f4b95c03b8812eea7.png" style="display: block; margin: 0 auto; width: 30%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ANN/ann_walkthrough_hidden.png"/>
  <figcaption style="text-align: center;"> First hidden layer node with 3 inputs, and 1 output.
</figcaption>
</figure>
<ul class="simple">
<li><p>we have <span class="math notranslate nohighlight">\(p = 3\)</span> and <span class="math notranslate nohighlight">\(k = 1\)</span>, and we draw from the uniform distribution,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
U \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] = U \left[ \frac{-1}{\sqrt{3}+1}, \frac{1}{\sqrt{3}+1} \right]
\]</div>
<ol class="arabic simple">
<li><p><strong>Forward Pass</strong> - to pass a training sample, <span class="math notranslate nohighlight">\(z\)</span>, to calculate the reconstruction, $\hat{z}. Initial predictions will be random for the first iteration, but will improve.</p></li>
<li><p><strong>Calculate the Error Derivative</strong> - based on the miss match between the input training sample, <span class="math notranslate nohighlight">\(z\)</span>, and the reconstruction, <span class="math notranslate nohighlight">\(\hat{z}\)</span>.</p></li>
<li><p><strong>Backpropagate the Error Derivative</strong> - we shift back through the artificial neural network to calculate the derivatives of the error over all the model weights and biases parameters, to accomplish this we use the chain rule,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial x} f(g(h(x))) = \frac{\partial f}{\partial g} \cdot \frac{\partial g}{\partial h} \cdot \frac{\partial h}{\partial x}
\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Loop Over Batch and Average the Error Derivatives</strong> - go to step 1 for all training data in the batch and then calculate the average of the error derivatives, for example,</p></li>
<li><p><strong>Update the Model Parameters</strong> - based on the derivatives, \frac{\partial P}{\partial \lambda_{i,j}} and learning rates, <span class="math notranslate nohighlight">\(\eta\)</span>, like this,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\lambda_{1,4}^{\ell} = \lambda_{1,4}^{\ell-1} - \eta \cdot \frac{1}{B} \sum_{i=1}^{B} \frac{\partial \mathcal{L}^{(i)}}{\partial \lambda_{1,4}}
\]</div>
<ol class="arabic simple" start="5">
<li><p><strong>Repeat Until Convergence</strong> - return to step 1. until the error, <span class="math notranslate nohighlight">\(P\)</span>, is reduced to an acceptable level, i.e., model convergence is the condition to stop the iterations</p></li>
</ol>
</section>
<section id="autoencoder-loss">
<h2>Autoencoder Loss</h2>
<p>There is a loss and loss gradient at each output-input node pair. The error loss function,</p>
<figure style="text-align: center;">
  <img src="../Images/701ec6c7b420f85dae65e62285e83b13.png" style="display: block; margin: 0 auto; width: 45%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/error.png"/>
  <figcaption style="text-align: center;"> Autoencoder loss at each output node, the goal is for the output to match the input.
</figcaption>
</figure>
<p>We can generalize as,</p>
<div class="math notranslate nohighlight">
\[
L = \frac{1}{2} \sum_{i=1}^3 \left(O_{i+8} - I_i \right)^2
\]</div>
<p>Note, the irregular indexing is due to my choice to use a unique node index at each node.</p>
<p>Error derivative at each node is,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial O_9} = O_9 - I_1
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial O_{10}} = O_{10} - I_2
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial O_{11}} = O_{11} - I_3
\]</div>
</section>
<section id="autoencoder-backpropagation">
<h2>Autoencoder Backpropagation</h2>
<p>Let‚Äôs walk through the back propagation of our autoencoder, let‚Äôs start with a bias in the output node, <span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial b_{9}}\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/8a6b2383ff34c83e1de1a609373cc653.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backb9.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the bias, \(ùëè_9\), in the hidden decoder node, \(ùëÇ_9\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_9} 
= \frac{\partial O_{9_{\mathrm{in}}}}{\partial b_9} 
\cdot \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial O_9} 
= 1 \cdot 1 \cdot (O_9 - I_1)
\]</div>
<p>Let‚Äôs explain each part. We start with the output gradient <span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial O_9}\)</span> and step across the output node, <span class="math notranslate nohighlight">\(O_9\)</span>, since linear activation is applied in the output nodes,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial O_9}{\partial O_{9_{in}}} = 1.0
\]</div>
<p>Now we can calculate the derivative of the bias, <span class="math notranslate nohighlight">\(b_9\)</span>, with respect to the node input,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial 0_{9_{\mathrm{in}}}}{\partial b_9} 
= \frac{\partial}{\partial b_9} \left( \lambda_{7,9} R_7 + \lambda_{8,9} R_8 + b_9 \right) 
= 1
\]</div>
<p>Now we can proceed to the connection weight, ùúÜ_7,9.</p>
<figure style="text-align: center;">
  <img src="../Images/80eaca0166d0cf02f98e140c090fca18.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/back79.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the connection weight, \(\lambda_{7,9}\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \lambda_{7,9}} 
= \frac{\partial O_{9_{\mathrm{in}}}}{\partial \lambda_{7,9}} 
\cdot \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial O_9} 
= R_7 \cdot 1 \cdot (O_9 - I_1)
\]</div>
<p>Once again, since linear activation is applied in the output nodes,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial O_9}{\partial O_{9_{in}}} = 1.0
\]</div>
<p>and <span class="math notranslate nohighlight">\(\frac{\partial O^{\text{in}}_9}{\partial \lambda_{7,9}}\)</span> is simply the output from <span class="math notranslate nohighlight">\(ùëÖ_7\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial O^{\text{in}}_9}{\partial \lambda_{7,9}} 
= \frac{\partial}{\partial \lambda_{7,9}} \left( \lambda_{7,9} R_7 + \lambda_{8,9} R_8 + b_9 \right) 
= R_7
\]</div>
<p>Let‚Äôs continue past <span class="math notranslate nohighlight">\(\partial \lambda_{7,9}\)</span> to the output from our decoder hidden node, <span class="math notranslate nohighlight">\(ùëÖ_7\)</span></p>
<figure style="text-align: center;">
  <img src="../Images/1c85ce96ca6f0999b7bc167c32d65b89.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backr7.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the output of the decoder hidden layer node \(R_7\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial R_7} 
= \frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7} 
\cdot \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial O_9} 
+ \frac{\partial O_{10_{\mathrm{in}}}}{\partial R_7} 
\cdot \frac{\partial O_{10}}{\partial O_{10_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial O_{10}} 
+ \frac{\partial O_{11_{\mathrm{in}}}}{\partial R_7} 
\cdot \frac{\partial O_{11}}{\partial O_{11_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial O_{11}}
\]</div>
<p>that we can evaluate as,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial R_7} 
= \lambda_{7,9} \cdot 1 \cdot (O_9 - I_1) 
+ \lambda_{7,10} \cdot 1 \cdot (O_{10} - I_2) 
+ \lambda_{7,11} \cdot 1 \cdot (O_{11} - I_3)
\]</div>
<p>We add the derivatives from each connection.
Once again, since linear activation at <span class="math notranslate nohighlight">\(ùëÇ_{9}\)</span>, <span class="math notranslate nohighlight">\(ùëÇ_{10}\)</span>, and <span class="math notranslate nohighlight">\(ùëÇ_{11}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} = 1, \quad
\frac{\partial O_{10}}{\partial O_{10_{\mathrm{in}}}} = 1, \quad
\frac{\partial O_{11}}{\partial O_{11_{\mathrm{in}}}} = 1
\]</div>
<p>Also, along the connection, the derivative is simply the weight,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7} = \lambda_{7,9}, \quad
\frac{\partial O_{10_{\mathrm{in}}}}{\partial R_7} = \lambda_{7,10}, \quad
\frac{\partial O_{11_{\mathrm{in}}}}{\partial R_7} = \lambda_{7,11}
\]</div>
<p>for example we can demonstrate this for <span class="math notranslate nohighlight">\(\frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7}\)</span> as,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7} 
= \frac{\partial}{\partial R_7} \left( \lambda_{7,9} R_7 + \lambda_{8,9} R_8 + b_9 \right) 
= \lambda_{7,9}
\]</div>
<p>Let‚Äôs continue from the output from our decoder hidden layer node, <span class="math notranslate nohighlight">\(ùëÖ_7\)</span>,  to calculate the derivative of the bias in the node, <span class="math notranslate nohighlight">\(b_7\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/604e4fcf99d1c41dd899458f80a67179.png" style="display: block; margin: 0 auto; width: 65%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backb7.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the bias, $b_7$, in the hidden decoder node, $R_7$.
</figcaption>
</figure>
<p>From the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_7} 
= \frac{\partial R_{7_{\mathrm{in}}}}{\partial b_7} 
\cdot \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial R_7}
\]</div>
<p>Since sigmoid activation at <span class="math notranslate nohighlight">\(R_7\)</span>, to move across the node,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} 
= \sigma' (R_7) = R_7 (1 - R_7)
\]</div>
<p>and for the partial derivative of the node input given the bias,</p>
<div class="math notranslate nohighlight">
\[
\frac{R_{7_{\mathrm{in}}}}{\partial b_7} 
= \frac{\partial}{\partial b_7} \left( \lambda_{6,7} M_6 + b_7 \right) 
= 1
\]</div>
<p>So now we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_7} 
= 1 \cdot R_7 (1 - R_7) \cdot 
\overbrace{
\left[
\lambda_{7,9} \cdot 1 \cdot (O_9 - I_1) 
+ \lambda_{7,10} \cdot 1 \cdot (O_{10} - I_2) 
+ \lambda_{7,11} \cdot 1 \cdot (O_{11} - I_3)
\right]
}^{\frac{\partial L}{\partial R_7}}
\]</div>
<p>Now we can proceed to the connection weight, <span class="math notranslate nohighlight">\(\lambda_{6,7}\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/1559af01deb817828f382cd89480ff41.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/back67.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the connection weight, \(\lambda_{6,7}\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \lambda_{6,7}} 
= \frac{\partial R_{7_{\mathrm{in}}}}{\partial \lambda_{6,7}} 
\cdot \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial R_7}
\]</div>
<p>Once again, since sigmoid activation is applied in the hidden layer nodes,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial R_7}{\partial R_{7_{in}}} = 1.0
\]</div>
<p>and <span class="math notranslate nohighlight">\(\frac{\partial R_{7_{\mathrm{in}}}}{\partial \lambda_{6,7}}\)</span> is simply the output from <span class="math notranslate nohighlight">\(M_6\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial R_{7_{\mathrm{in}}}}{\partial \lambda_{6,7}} 
= \frac{\partial}{\partial \lambda_{6,7}} \left( \lambda_{6,7} M_6 + b_6 \right) 
= M_6
\]</div>
<p>So now we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_7} 
= M_6 \cdot R_7 (1 - R_7) \cdot 
\overbrace{
\left[
\lambda_{7,9} \cdot 1 \cdot (O_9 - I_1) 
+ \lambda_{7,10} \cdot 1 \cdot (O_{10} - I_2) 
+ \lambda_{7,11} \cdot 1 \cdot (O_{11} - I_3)
\right]
}^{\frac{\partial \mathcal{L}}{\partial R_7}}
\]</div>
<p>Let‚Äôs get continue to the output from our latent node, ùëÄ_6</p>
<figure style="text-align: center;">
  <img src="../Images/f4cc7dbc1493a36ab0eb828c1422d1f2.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backm6.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the output of the latent node, \(M_6\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial M_6} 
= \frac{\partial R_{7_{\mathrm{in}}}}{\partial M_6} 
\cdot \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial R_7} 
+ \frac{\partial R_{8_{\mathrm{in}}}}{\partial M_6} 
\cdot \frac{\partial R_8}{\partial R_{8_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial R_8}
\]</div>
<p>That we can resolve as,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial M_6} 
= \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} 
+ \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8}
\]</div>
<p>Once again, since sigmoid activation,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} = R_7 (1 - R_7), \quad
\frac{\partial R_8}{\partial R_{8_{\mathrm{in}}}} = R_8 (1 - R_8)
\]</div>
<p>and along the connections,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial R_{7_{\mathrm{in}}}}{\partial M_6} 
&amp;= \frac{\partial}{\partial M_6} \left( \lambda_{6,7} M_6 + b_7 \right) 
= \lambda_{6,7} \\
\frac{\partial R_{8_{\mathrm{in}}}}{\partial M_6} 
&amp;= \frac{\partial}{\partial M_6} \left( \lambda_{6,8} M_6 + b_8 \right) 
= \lambda_{6,8}
\end{aligned}
\end{split}\]</div>
<p>Let‚Äôs continue from the output from our latent node, <span class="math notranslate nohighlight">\(M_6\)</span>, to calculate the derivative of the bias in the node, <span class="math notranslate nohighlight">\(b_6\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/90618005b205c6c5ceb09965c36cf2e1.png" style="display: block; margin: 0 auto; width: 43%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backb6.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the bias, $b_6$, in the latent node, $M_6$. Note image shifted to make room.
</figcaption>
</figure>
<p>From the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_6} 
= \frac{\partial M_{6_{\mathrm{in}}}}{\partial b_6} 
\cdot \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial M_6}
\]</div>
<p>Since sigmoid activation at <span class="math notranslate nohighlight">\(M_6\)</span>, to move across the node,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} 
= \sigma' (M_6) = M_6 \cdot (1 - M_6)
\]</div>
<p>and for the partial derivative of the node input given the bias,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_{6_{\mathrm{in}}}}{\partial b_6} 
= \frac{\partial}{\partial b_6} \left( \lambda_{4,6} L_4 + b_6 \right) 
= 1
\]</div>
<p>So now we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_6} 
= 1 \cdot M_6 (1 - M_6) \cdot 
\overbrace{
  \left[
    \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} 
    + \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8}
  \right]
}^{\frac{\partial \mathcal{L}}{\partial M_6}}
\]</div>
<p>Now we can proceed to the connection weight, <span class="math notranslate nohighlight">\(\lambda_{4,6}\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/f5770d05672cfe3c14c6973f2775d2de.png" style="display: block; margin: 0 auto; width: 43%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/back46.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the connection weight, \(\lambda_{4,6}\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} 
= \frac{\partial M_{6_{\mathrm{in}}}}{\partial \lambda_{4,6}} 
\cdot \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial M_6}
\]</div>
<p>Once again, since sigmoid activation is applied in the hidden layer nodes,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_6}{\partial M_{6_{in}}} = M_6 \cdot (1 - M_6)
\]</div>
<p>and <span class="math notranslate nohighlight">\(\frac{\partial M_{6_{\mathrm{in}}}}{\partial \lambda_{4,6}}\)</span> is simply the output from <span class="math notranslate nohighlight">\(L_4\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_{6_{\mathrm{in}}}}{\partial \lambda_{4,6}} 
= \frac{\partial}{\partial \lambda_{4,6}} \left( \lambda_{4,6} L_4 + \lambda_{5,6} L_5 + b_6 \right) 
= L_4
\]</div>
<p>So now we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} 
= L_4 \cdot M_6 (1 - M_6) \cdot
\overbrace{
\left[
\lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7}
+ \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8}
\right]
}^{\frac{\partial \mathcal{L}}{\partial M_6}}
\]</div>
<p>Now we can proceed to the output of our encoder hidden layer node, <span class="math notranslate nohighlight">\(L_4\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/1e5148ec01b8276d13a3ac564a201ab3.png" style="display: block; margin: 0 auto; width: 67%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backl4.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the output of the encoder hidden node, \(ùêø_4\).
</figcaption>
</figure>
<p>By the chain rule we get this and evaluate it as,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial L_4} 
= \frac{\partial M_{6_{\mathrm{in}}}}{\partial L_4} 
\cdot \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial M_6} 
= \lambda_{4,6} \cdot M_6 (1 - M_6) \cdot \frac{\partial \mathcal{L}}{\partial M_6}
\]</div>
<p>Once again, since sigmoid activation is applied in the latent node,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} = M_6 (1 - M_6)
\]</div>
<p>and <span class="math notranslate nohighlight">\(\frac{\partial M_{6_{\mathrm{in}}}}{\partial L_4}\)</span> is simply the weight, <span class="math notranslate nohighlight">\(\lambda_{4,6}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_{6_{\mathrm{in}}}}{\partial L_4} 
= \frac{\partial}{\partial L_4} \left( \lambda_{4,6} L_4 + b_6 \right) 
= \lambda_{4,6}
\]</div>
<p>Let‚Äôs continue from the output from our encoder hidden layer node, <span class="math notranslate nohighlight">\(L_4\)</span>, to calculate the derivative of the bias in the node, <span class="math notranslate nohighlight">\(b_4\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/cf8f925e7a89e3d992b323edfd45034e.png" style="display: block; margin: 0 auto; width: 67%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backb4.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the bias, $b_4$, in the encoder hidden layer node, $L_4$.
</figcaption>
</figure>
<p>From the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_4} 
= \frac{\partial L_{4_{\mathrm{in}}}}{\partial b_4} 
\cdot \frac{\partial L_4}{\partial L_{4_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial L_4} 
= 1 \cdot L_4 (1 - L_4) \cdot \frac{\partial \mathcal{L}}{\partial L_4}
\]</div>
<p>Since sigmoid activation at <span class="math notranslate nohighlight">\(M_6\)</span>, to move across the node,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} 
= \sigma' (M_6) = M_6 \cdot (1 - M_6)
\]</div>
<p>and for the partial derivative of the node input given the bias,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_{6_{\mathrm{in}}}}{\partial b_6} 
= \frac{\partial}{\partial b_6} \left( \lambda_{4,6} L_4 + b_6 \right) 
= 1
\]</div>
<p>So now we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_6} 
= 1 \cdot M_6 (1 - M_6) \cdot 
\overbrace{
  \left[
    \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} 
    + \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8}
  \right]
}^{\frac{\partial \mathcal{L}}{\partial M_6}}
\]</div>
<p>And, finally we proceed to the connection weight, <span class="math notranslate nohighlight">\(\lambda_{1,4}\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/3623ed192b17eb44b8f6f8c59b1dc0d0.png" style="display: block; margin: 0 auto; width: 67%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/back14.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the connection weight, \(\lambda_{1,4}\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} 
= \frac{\partial L^{\text{in}}_4}{\partial \lambda_{1,4}} 
\cdot \frac{\partial L_4}{\partial L^{\text{in}}_4} 
\cdot \frac{\partial \mathcal{L}}{\partial L_4}
\]</div>
<p>Once again, since sigmoid activation is applied in the hidden layer nodes,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L_4}{\partial L_{4_{in}}} = L_4 \cdot (1 - L_4)
\]</div>
<p>and <span class="math notranslate nohighlight">\(\frac{\partial L^{\text{in}}_4}{\partial \lambda_{1,4}}\)</span> is simply the output from <span class="math notranslate nohighlight">\(I_1\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L^{\text{in}}_4}{\partial \lambda_{1,4}} 
= \frac{\partial}{\partial \lambda_{1,4}} \left( \lambda_{1,4} I_1 + \lambda_{2,4} I_2 + \lambda_{3,4} I_3 + b_4 \right) 
= I_1
\]</div>
<p>So now we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L}{\partial \lambda_{1,4}} 
= I_1 \cdot L_4 (1 - L_4) \cdot \underbrace{\left[ \lambda_{4,6} \cdot M_6 (1 - M_6) \cdot \frac{\partial L}{\partial M_6} \right]}_{\frac{\partial L}{\partial L_4}}
\]</div>
<p>Now we will build out this autoencoder from the ground up with only the NumPy python package for arrays and Python built-in data structure dictionaries.</p>
</section>
<section id="import-required-packages">
<h2>Import Required Packages</h2>
<p>We will also need some standard packages. These should have been installed with Anaconda 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">ignore_warnings</span> <span class="o">=</span> <span class="kc">True</span>                                        <span class="c1"># ignore warnings?</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span> <span class="n">AutoMinorLocator</span><span class="p">,</span> <span class="n">AutoLocator</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># set axes and grids in the background for all plots</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rankdata</span>                              <span class="c1"># to assist with plot label placement</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>             <span class="c1"># fit the relationship between latent and training data slope               </span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">13</span>                                                     <span class="c1"># random number seed</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">tab20</span>                                           <span class="c1"># default colormap</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># plot all grids below the plot elements</span>
<span class="k">if</span> <span class="n">ignore_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>                                   
    <span class="kn">import</span> <span class="nn">warnings</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available with the respective package docs.</p>
</section>
<section id="declare-functions">
<h2>Declare Functions</h2>
<p>Here‚Äôs the functions to train and visualize our autoencoder.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">add_grid</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks  </span>

<span class="k">def</span> <span class="nf">xavier</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>                                      <span class="c1"># Xavier initializer function</span>
    <span class="n">limit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">6</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_in</span> <span class="o">+</span> <span class="n">n_out</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>                                               <span class="c1"># sigmoid activation</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">():</span>                                  <span class="c1"># initialize all weights and biases and build dictionaries of both</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>                            
        <span class="s1">'w14'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w24'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w34'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w15'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w25'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w35'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w46'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="s1">'w56'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="s1">'w67'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w68'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w79'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="s1">'w89'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="s1">'w710'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="s1">'w810'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="s1">'w711'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="s1">'w811'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>                                                <span class="c1"># biases (one per neuron, excluding input)</span>
        <span class="s1">'b4'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b5'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b6'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b7'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b8'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b9'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b10'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b11'</span><span class="p">:</span> <span class="mf">0.0</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> 

<span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="n">input_vec</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">):</span>                 <span class="c1"># forward pass of the autoencoder</span>
    <span class="n">I1</span><span class="p">,</span> <span class="n">I2</span><span class="p">,</span> <span class="n">I3</span> <span class="o">=</span> <span class="n">input_vec</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>                               <span class="c1"># input nodes (I1, I2, I3)</span>
    <span class="n">z4</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w14'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I1</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w24'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I2</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w34'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I3</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b4'</span><span class="p">]</span> <span class="c1"># encoder</span>
    <span class="n">a4</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z4</span><span class="p">)</span>

    <span class="n">z5</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w15'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I1</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w25'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I2</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w35'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I3</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b5'</span><span class="p">]</span>
    <span class="n">a5</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z5</span><span class="p">)</span>

    <span class="n">z6</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w46'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a4</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w56'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a5</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b6'</span><span class="p">]</span> <span class="c1"># bottlekneck</span>
    <span class="n">a6</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z6</span><span class="p">)</span>

    <span class="n">z7</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w67'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a6</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b7'</span><span class="p">]</span>                   <span class="c1"># decoder</span>
    <span class="n">a7</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z7</span><span class="p">)</span>

    <span class="n">z8</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w68'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a6</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b8'</span><span class="p">]</span>
    <span class="n">a8</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z8</span><span class="p">)</span>

    <span class="n">z9</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w79'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a7</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w89'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a8</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b9'</span><span class="p">]</span>
    <span class="n">a9</span> <span class="o">=</span> <span class="n">z9</span>  

    <span class="n">z10</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w710'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a7</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w810'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a8</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b10'</span><span class="p">]</span>
    <span class="n">a10</span> <span class="o">=</span> <span class="n">z10</span>  <span class="c1"># linear</span>

    <span class="n">z11</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w711'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a7</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w811'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a8</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b11'</span><span class="p">]</span>
    <span class="n">a11</span> <span class="o">=</span> <span class="n">z11</span>  <span class="c1"># linear</span>

    <span class="k">return</span> <span class="p">{</span>                                                  <span class="c1"># return all activations as a dictionary</span>
        <span class="s1">'I1'</span><span class="p">:</span> <span class="n">I1</span><span class="p">,</span> <span class="s1">'I2'</span><span class="p">:</span> <span class="n">I2</span><span class="p">,</span> <span class="s1">'I3'</span><span class="p">:</span> <span class="n">I3</span><span class="p">,</span>
        <span class="s1">'L4'</span><span class="p">:</span> <span class="n">a4</span><span class="p">,</span> <span class="s1">'L5'</span><span class="p">:</span> <span class="n">a5</span><span class="p">,</span>
        <span class="s1">'M6'</span><span class="p">:</span> <span class="n">a6</span><span class="p">,</span>
        <span class="s1">'R7'</span><span class="p">:</span> <span class="n">a7</span><span class="p">,</span> <span class="s1">'R8'</span><span class="p">:</span> <span class="n">a8</span><span class="p">,</span>
        <span class="s1">'O9'</span><span class="p">:</span> <span class="n">a9</span><span class="p">,</span> <span class="s1">'O10'</span><span class="p">:</span> <span class="n">a10</span><span class="p">,</span> <span class="s1">'O11'</span><span class="p">:</span> <span class="n">a11</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">mse_loss_and_derivative</span><span class="p">(</span><span class="n">output_vec</span><span class="p">,</span> <span class="n">input_vec</span><span class="p">):</span>           <span class="c1"># MSE loss and error derivative given output and input</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">output_vec</span> <span class="o">-</span> <span class="n">input_vec</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">diff</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">dloss_dout</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span>  <span class="c1"># shape (3,1)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dloss_dout</span>

<span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>                                    <span class="c1"># derivative of sigmoid activation</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">backpropagate</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">dloss_dout</span><span class="p">):</span>  <span class="c1"># backpropagate the error derivatives</span>
    <span class="n">I1</span><span class="p">,</span> <span class="n">I2</span><span class="p">,</span> <span class="n">I3</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'I1'</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'I2'</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'I3'</span><span class="p">]</span>
    <span class="n">a4</span><span class="p">,</span> <span class="n">a5</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'L4'</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'L5'</span><span class="p">]</span>
    <span class="n">a6</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'M6'</span><span class="p">]</span>
    <span class="n">a7</span><span class="p">,</span> <span class="n">a8</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'R7'</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'R8'</span><span class="p">]</span>
    <span class="n">O9</span><span class="p">,</span> <span class="n">O10</span><span class="p">,</span> <span class="n">O11</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'O9'</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'O10'</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'O11'</span><span class="p">]</span>

    <span class="n">delta9</span> <span class="o">=</span> <span class="n">dloss_dout</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>                                 <span class="c1"># error terms (delta) for output nodes = dLoss/dOutput</span>
    <span class="n">delta10</span> <span class="o">=</span> <span class="n">dloss_dout</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">delta11</span> <span class="o">=</span> <span class="n">dloss_dout</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="n">grad_weights</span> <span class="o">=</span> <span class="p">{}</span>                                         <span class="c1"># gradients for weights from R7, R8 to O9, O10, O11</span>
    <span class="n">grad_biases</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w79'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta9</span> <span class="o">*</span> <span class="n">a7</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w89'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta9</span> <span class="o">*</span> <span class="n">a8</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w710'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta10</span> <span class="o">*</span> <span class="n">a7</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w810'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta10</span> <span class="o">*</span> <span class="n">a8</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w711'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta11</span> <span class="o">*</span> <span class="n">a7</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w811'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta11</span> <span class="o">*</span> <span class="n">a8</span>

    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b9'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta9</span>
    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b10'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta10</span>
    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b11'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta11</span>

    <span class="n">delta_r7</span> <span class="o">=</span> <span class="p">(</span><span class="n">delta9</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w79'</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta10</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w710'</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta11</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w711'</span><span class="p">])</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">a7</span><span class="p">)</span> <span class="c1"># gradients for R7 and R8</span>
    <span class="n">delta_r8</span> <span class="o">=</span> <span class="p">(</span><span class="n">delta9</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w89'</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta10</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w810'</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta11</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w811'</span><span class="p">])</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">a8</span><span class="p">)</span>

    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w67'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_r7</span> <span class="o">*</span> <span class="n">a6</span>                       <span class="c1"># gradients for weights from M6 to R7, R8</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w68'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_r8</span> <span class="o">*</span> <span class="n">a6</span>

    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b7'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_r7</span>
    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b8'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_r8</span>

    <span class="n">delta_m6</span> <span class="o">=</span> <span class="p">(</span><span class="n">delta_r7</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w67'</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta_r8</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w68'</span><span class="p">])</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">a6</span><span class="p">)</span> <span class="c1"># backpropagate delta to M6 (sigmoid)</span>

    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w46'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_m6</span> <span class="o">*</span> <span class="n">a4</span>                       <span class="c1"># gradients for weights from L4, L5 to M6</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w56'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_m6</span> <span class="o">*</span> <span class="n">a5</span>

    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b6'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_m6</span>

    <span class="n">delta_l4</span> <span class="o">=</span> <span class="n">delta_m6</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w46'</span><span class="p">]</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">a4</span><span class="p">)</span> <span class="c1"># backpropagate delta to L4, L5 (sigmoid)</span>
    <span class="n">delta_l5</span> <span class="o">=</span> <span class="n">delta_m6</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w56'</span><span class="p">]</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">a5</span><span class="p">)</span>

    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w14'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l4</span> <span class="o">*</span> <span class="n">I1</span>                       <span class="c1"># gradients for weights from I1, I2, I3 to L4</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w24'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l4</span> <span class="o">*</span> <span class="n">I2</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w34'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l4</span> <span class="o">*</span> <span class="n">I3</span>

    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b4'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l4</span>

    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w15'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l5</span> <span class="o">*</span> <span class="n">I1</span>                       <span class="c1"># gradients for weights from I1, I2, I3 to L5</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w25'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l5</span> <span class="o">*</span> <span class="n">I2</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w35'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l5</span> <span class="o">*</span> <span class="n">I3</span>

    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b5'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l5</span>
    <span class="k">return</span> <span class="n">grad_weights</span><span class="p">,</span> <span class="n">grad_biases</span>

<span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">grad_weights</span><span class="p">,</span> <span class="n">grad_biases</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span> <span class="c1"># update the weights and biased by derivatives and learning rate</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">grad_weights</span><span class="p">:</span>                                  <span class="c1"># update weights</span>
        <span class="n">weights</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_weights</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">grad_biases</span><span class="p">:</span>                                   <span class="c1"># update biases</span>
        <span class="n">biases</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_biases</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualize-the-autoencoder-network">
<h2>Visualize the Autoencoder Network</h2>
<p>Here we specify the autoencoder labels, positions, connections and colors and then plot the autoencoder.</p>
<ul class="simple">
<li><p>while this code is general, the actual autoencoder codes are not generalized to work with other architectures, for example changing the depth or width of the network</p></li>
<li><p>change the display parameters but do not the autoencoder architecture</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">positions</span> <span class="o">=</span> <span class="p">{</span>                                                 <span class="c1"># node positions</span>
    <span class="s1">'I1'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s1">'I2'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">'I3'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="s1">'L4'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="s1">'L5'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
    <span class="s1">'M6'</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="s1">'R7'</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="s1">'R8'</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
    <span class="s1">'O9'</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s1">'O10'</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">'O11'</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">node_colors</span> <span class="o">=</span> <span class="p">{</span>                                               <span class="c1"># node colors</span>
    <span class="s1">'I1'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span> <span class="s1">'I2'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span> <span class="s1">'I3'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span>
    <span class="s1">'L4'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span> <span class="s1">'L5'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span>
    <span class="s1">'M6'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span>
    <span class="s1">'R7'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span> <span class="s1">'R8'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span>
    <span class="s1">'O9'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span> <span class="s1">'O10'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span> <span class="s1">'O11'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">edges</span> <span class="o">=</span> <span class="p">[</span>                                                     <span class="c1"># edges and weight labels</span>
    <span class="p">(</span><span class="s1">'I1'</span><span class="p">,</span> <span class="s1">'L4'</span><span class="p">,</span> <span class="s1">'lightcoral'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'I2'</span><span class="p">,</span> <span class="s1">'L4'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'I3'</span><span class="p">,</span> <span class="s1">'L4'</span><span class="p">,</span> <span class="s1">'darkred'</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'I1'</span><span class="p">,</span> <span class="s1">'L5'</span><span class="p">,</span> <span class="s1">'dodgerblue'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'I2'</span><span class="p">,</span> <span class="s1">'L5'</span><span class="p">,</span> <span class="s1">'blue'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'I3'</span><span class="p">,</span> <span class="s1">'L5'</span><span class="p">,</span> <span class="s1">'darkblue'</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'L4'</span><span class="p">,</span> <span class="s1">'M6'</span><span class="p">,</span> <span class="s1">'orange'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'L5'</span><span class="p">,</span> <span class="s1">'M6'</span><span class="p">,</span> <span class="s1">'darkorange'</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'M6'</span><span class="p">,</span> <span class="s1">'R7'</span><span class="p">,</span> <span class="s1">'orange'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'M6'</span><span class="p">,</span> <span class="s1">'R8'</span><span class="p">,</span> <span class="s1">'darkorange'</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'R7'</span><span class="p">,</span> <span class="s1">'O9'</span><span class="p">,</span> <span class="s1">'lightcoral'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'R7'</span><span class="p">,</span> <span class="s1">'O10'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'R7'</span><span class="p">,</span> <span class="s1">'O11'</span><span class="p">,</span> <span class="s1">'darkred'</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'R8'</span><span class="p">,</span> <span class="s1">'O9'</span><span class="p">,</span> <span class="s1">'dodgerblue'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'R8'</span><span class="p">,</span> <span class="s1">'O10'</span><span class="p">,</span> <span class="s1">'blue'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'R8'</span><span class="p">,</span> <span class="s1">'O11'</span><span class="p">,</span> <span class="s1">'darkblue'</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">weight_labels</span> <span class="o">=</span> <span class="p">{</span> <span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">,):</span> <span class="sa">f</span><span class="s2">"$</span><span class="se">\\</span><span class="s2">lambda_</span><span class="se">{{</span><span class="si">{</span><span class="n">src</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}{</span><span class="n">dst</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="si">}</span><span class="se">}}</span><span class="s2">$"</span> <span class="k">for</span> <span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span> <span class="ow">in</span> <span class="n">edges</span> <span class="p">}</span>

<span class="n">bias_offsets</span> <span class="o">=</span> <span class="p">{</span>                                              <span class="c1"># bias vector offsets</span>
    <span class="s1">'L4'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">),</span> <span class="s1">'L5'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">),</span>
    <span class="s1">'M6'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">),</span>
    <span class="s1">'R7'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">),</span> <span class="s1">'R8'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">),</span>
    <span class="s1">'O9'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">),</span> <span class="s1">'O10'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">),</span> <span class="s1">'O11'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">bias_labels</span> <span class="o">=</span> <span class="p">{</span> <span class="n">node</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"$b_</span><span class="se">{{</span><span class="si">{</span><span class="n">node</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="si">}</span><span class="se">}}</span><span class="s2">$"</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">bias_offsets</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="p">}</span>
<span class="c1"># Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">custom_weight_offsets</span> <span class="o">=</span> <span class="p">{</span>                                     <span class="c1"># custom label offsets for select overlapping weights</span>
    <span class="p">(</span><span class="s1">'I2'</span><span class="p">,</span> <span class="s1">'L4'</span><span class="p">):</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.20</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'I2'</span><span class="p">,</span> <span class="s1">'L5'</span><span class="p">):</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'R8'</span><span class="p">,</span> <span class="s1">'O9'</span><span class="p">):</span> <span class="p">(</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'R8'</span><span class="p">,</span> <span class="s1">'O10'</span><span class="p">):</span> <span class="p">(</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.16</span><span class="p">),</span>
<span class="p">}</span>

<span class="k">for</span> <span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span> <span class="ow">in</span> <span class="n">edges</span><span class="p">:</span>                               <span class="c1"># plot edges and weight labels</span>
    <span class="n">x0</span><span class="p">,</span> <span class="n">y0</span> <span class="o">=</span> <span class="n">positions</span><span class="p">[</span><span class="n">src</span><span class="p">]</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span> <span class="o">=</span> <span class="n">positions</span><span class="p">[</span><span class="n">dst</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">],</span> <span class="p">[</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">xm</span><span class="p">,</span> <span class="n">ym</span> <span class="o">=</span> <span class="p">(</span><span class="n">x0</span> <span class="o">+</span> <span class="n">x1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">y0</span> <span class="o">+</span> <span class="n">y1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span> <span class="o">=</span> <span class="n">custom_weight_offsets</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">xm</span> <span class="o">+</span> <span class="n">dx</span><span class="p">,</span> <span class="n">ym</span> <span class="o">+</span> <span class="n">dy</span><span class="p">,</span> <span class="n">weight_labels</span><span class="p">[(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">)],</span>
            <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="n">positions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>                        <span class="c1"># white back circles</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="n">positions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>                        <span class="c1"># node circles and labels</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">node_colors</span><span class="p">[</span><span class="n">node</span><span class="p">],</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span> <span class="ow">in</span> <span class="n">bias_offsets</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>                   <span class="c1"># bias arrows and tighter label placement</span>
    <span class="n">nx</span><span class="p">,</span> <span class="n">ny</span> <span class="o">=</span> <span class="n">positions</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
    <span class="n">bx</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="n">nx</span> <span class="o">+</span> <span class="n">dx</span><span class="p">,</span> <span class="n">ny</span> <span class="o">+</span> <span class="n">dy</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">),</span>
                <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-&gt;"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">,</span> <span class="n">bias_labels</span><span class="p">[</span><span class="n">node</span><span class="p">],</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'right'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'bottom'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Final formatting</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/333249f6a43bbad84e15a2423db3b9cc8670650c55532adfe9fea6ac7c992872.png" src="../Images/330a264f2ed0fefaff128fb34a83b1e7.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/333249f6a43bbad84e15a2423db3b9cc8670650c55532adfe9fea6ac7c992872.png"/>
</div>
</div>
</section>
<section id="make-an-interesting-synthetic-dataset">
<h2>Make an Interesting Synthetic Dataset</h2>
<p>Generate a stochastic dataset of 1D length of 3 vectors with a pattern that can be summarized by our autoencoder.</p>
<ul class="simple">
<li><p>if we generate random 1D vectors of length 3 our autoencoder would not be able to summarize, i.e., it is not possible to compress the information from the original 3 values</p></li>
<li><p>we must include a pattern that can be learned by the autoencoder to observe dimensionality reduction through the latent node with good data reconstruction</p></li>
</ul>
<p>To do this, I have calculate dataset as a hybrid model, linear + small random residual. The data generation steps include,</p>
<ol class="arabic simple">
<li><p>draw a random slope <span class="math notranslate nohighlight">\(\sim N\left[-2.0, 2.0 \right]\)</span></p></li>
<li><p>calculate 3 points at locations <span class="math notranslate nohighlight">\(\left[-1, 0, 1 \right]\)</span>, <span class="math notranslate nohighlight">\(f(\left[-1, 0, 1 \right])\)</span></p></li>
<li><p>add random, independent residual to each location, <span class="math notranslate nohighlight">\(f(\left[-1, 0, 1 \right]) + N\left[0.0,\sigma \right]\)</span>, where sigma is the residual standard deviation</p></li>
</ol>
<p>Note, the slope is retained as a label that will be compared to the latent node, <span class="math notranslate nohighlight">\(M_6\)</span> output to check, what has our autoencoder has learned?</p>
<ul class="simple">
<li><p>our hypothesis is that the autoencoder will learn a value that directly maps to slope to describe this dataset.</p></li>
<li><p>note, while this label is used to demonstrate the ability of the autoencoder to learn, it is not used to train the model!</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>                                 <span class="c1"># set random seed</span>
<span class="n">nbatch</span> <span class="o">=</span> <span class="mi">12</span><span class="p">;</span> <span class="n">nnodes</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.1</span>                          <span class="c1"># set number of data (total number of data), number of nodes (must be 3), error st.dev.</span>
<span class="n">ymat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nbatch</span><span class="p">);</span> <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">nnodes</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="n">Xmat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">nbatch</span><span class="p">,</span><span class="n">nnodes</span><span class="p">])</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ibatch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nbatch</span><span class="p">):</span>                                <span class="c1"># loop over synthetic data</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">)</span>
    <span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">2.0</span><span class="p">)</span><span class="o">*</span><span class="n">m</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">nnodes</span><span class="p">)</span>
    <span class="n">ymat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="n">rank</span> <span class="o">=</span> <span class="n">rankdata</span><span class="p">(</span><span class="n">Xmat</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>                                   <span class="c1"># rank data to improve (alternate) adjacent labels' locations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot the synthetic data</span>
<span class="k">for</span> <span class="n">ibatch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nbatch</span><span class="p">):</span>                                
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="n">x</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">ibatch</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="n">x</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">ibatch</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">custom_positions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">3.2</span><span class="p">]</span>
    <span class="n">custom_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'I1'</span><span class="p">,</span><span class="s1">'I2'</span><span class="p">,</span><span class="s1">'I3'</span><span class="p">,</span><span class="s1">'Y'</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">rank</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ymat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="mi">2</span><span class="p">),[</span><span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mf">3.18</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ymat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="mi">2</span><span class="p">),[</span><span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mf">3.25</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">ibatch</span><span class="o">+</span><span class="mi">1</span><span class="p">,[</span><span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="mf">0.9</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">custom_positions</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">custom_labels</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.4</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Input Nodes'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'z'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Synthetic 1D Data and Labels'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Data Index: '</span><span class="p">,[</span><span class="o">-</span><span class="mf">1.4</span><span class="p">,</span><span class="mf">0.9</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/39557c0e268bdf9e355e0769aff4633ec5601e1ef244d68560aa0a4c22ac5f3f.png" src="../Images/9ce15f05dfa887ce6ee1f02619cb004d.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/39557c0e268bdf9e355e0769aff4633ec5601e1ef244d68560aa0a4c22ac5f3f.png"/>
</div>
</div>
</section>
<section id="train-the-autoencoder">
<h2>Train the Autoencoder</h2>
<p>We have previously defined all the basic functions for our autoencoder so we can put together our autoencoder training steps with the following functions,</p>
<ol class="arabic simple">
<li><p><strong>initialize_parameters</strong> - initialize the weights and bias</p></li>
<li><p><strong>forward_pass</strong> - forward pass through our autoencoder to calculate node outputs and data reconstruction</p></li>
<li><p><strong>mse_loss_and_derivative</strong> - calculate the L2 loss and associated error derivative for each output node from training data and reconstruction</p></li>
<li><p><strong>backpropagate</strong> - backpropagate the error derivative through the network based on error derivative and node outputs and then average the gradients at each weight and bias over the batch</p></li>
<li><p><strong>update_parameters</strong> - update the weights and biases with the average gradient over the batch and the learning rate</p></li>
<li><p>go to 2 until convergence, in the case a set number of training epochs</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">epochs</span> <span class="o">=</span> <span class="mi">10000</span>                                                <span class="c1"># set hyperparameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">nbatch</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">13</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">output_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="mi">3</span><span class="p">));</span> <span class="n">loss_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">epochs</span><span class="p">));</span> <span class="n">M6_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span><span class="n">epochs</span><span class="p">))</span>

<span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">()</span>                     <span class="c1"># initialize weights and biases</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">sum_grad_w</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">weights</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>               <span class="c1"># initialize zero dictionary to average backpropogated gradients</span>
    <span class="n">sum_grad_b</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">biases</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">idata</span><span class="p">,</span><span class="n">input_vec</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">input_vec</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span> <span class="c1"># forward pass</span>
        <span class="n">M6_mat</span><span class="p">[</span><span class="n">idata</span><span class="p">,</span><span class="n">epoch</span><span class="p">]</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'M6'</span><span class="p">]</span>
        <span class="n">output_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">activations</span><span class="p">[</span><span class="s1">'O9'</span><span class="p">]],</span> <span class="p">[</span><span class="n">activations</span><span class="p">[</span><span class="s1">'O10'</span><span class="p">]],</span> <span class="p">[</span><span class="n">activations</span><span class="p">[</span><span class="s1">'O11'</span><span class="p">]]])</span>
        <span class="n">output_mat</span><span class="p">[</span><span class="n">idata</span><span class="p">,</span><span class="n">epoch</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">output_vec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">dloss_dout</span> <span class="o">=</span> <span class="n">mse_loss_and_derivative</span><span class="p">(</span><span class="n">output_vec</span><span class="p">,</span> <span class="n">input_vec</span><span class="p">)</span> <span class="c1"># compute loss and derivative</span>
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span>
        <span class="n">grad_w</span><span class="p">,</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="n">backpropagate</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">dloss_dout</span><span class="p">)</span> <span class="c1"># backpropagation the derivative</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">grad_w</span><span class="p">:</span>                                      <span class="c1"># accumulate gradients</span>
            <span class="n">sum_grad_w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">grad_w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">grad_b</span><span class="p">:</span>
            <span class="n">sum_grad_b</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">grad_b</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="n">avg_grad_w</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="o">/</span> <span class="n">batch_size</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sum_grad_w</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span> <span class="c1"># average gradients over batch</span>
    <span class="n">avg_grad_b</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="o">/</span> <span class="n">batch_size</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sum_grad_b</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">epoch_loss</span> <span class="o">/=</span> <span class="n">batch_size</span>
    <span class="n">loss_mat</span><span class="p">[</span><span class="n">epoch</span><span class="p">]</span> <span class="o">=</span> <span class="n">epoch_loss</span>
    <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">avg_grad_w</span><span class="p">,</span> <span class="n">avg_grad_b</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span> <span class="c1"># update parameters</span>
    <span class="c1"># if epoch % 500 == 0:                                    # print loss every 100 training epochs</span>
    <span class="c1">#     print(f"Epoch {epoch}, Loss: {epoch_loss:.6f}")</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot training error vs. training epoch</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">loss_mat</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">'MSE'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="n">epoch</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Epochs'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">'Mean Square Error (L2 loss)'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Autoencoder Average Batch L2 Loss vs. Training Epoch'</span><span class="p">)</span>
<span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">'linear'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b4374d79f0f8d85887bb5f6075aa68f024e9e33bc189c7492047de36822bcb2a.png" src="../Images/3ab1c8fef6098b7c75943615555e53e5.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/b4374d79f0f8d85887bb5f6075aa68f024e9e33bc189c7492047de36822bcb2a.png"/>
</div>
</div>
<p>The average L2 loss vs. training epoch curve looks very good.</p>
<ul class="simple">
<li><p>we are seeing a pause in learning and then suddenly a fast reduction in training error and then slow convergence</p></li>
<li><p>I stopped at 10,000 epochs for efficiency</p></li>
</ul>
</section>
<section id="evaluating-our-autoencoder-network">
<h2>Evaluating Our Autoencoder Network</h2>
<p>Let‚Äôs look at the output from the latent node at the network bottleneck, i.e., the output of node M6.</p>
<ul class="simple">
<li><p>notice above that we recorded the M6 output (called node activation) for all training epochs and for all data.</p></li>
<li><p>let‚Äôs look at the final trained network, the last epoch, and loop over all data</p></li>
</ul>
<p>Here‚Äôs a plot of final epoch M6 output vs. the sample slopes,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">linear_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ymat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">M6_mat</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># fit linear model to regress latent on training data slope</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot latent vs. training data slope</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mi">100</span><span class="p">),</span><span class="n">linear_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ibatch</span><span class="p">,</span><span class="n">input_vec</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>                      <span class="c1"># plot and label training data</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ymat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="n">M6_mat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">ibatch</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">ibatch</span><span class="o">+</span><span class="mi">1</span><span class="p">,[</span><span class="n">ymat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span><span class="n">M6_mat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">0.01</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'M6 Output'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">'Sample Slope, $m_i$'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Latent Node Output vs. Sample Slope'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.4</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d0ab2f0cf0b3c073fe93da6aa44c8e663feb48b72910cb0e72f2f5ec9416f426.png" src="../Images/7815f0d074113f20a6f77a446f1f83d2.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/d0ab2f0cf0b3c073fe93da6aa44c8e663feb48b72910cb0e72f2f5ec9416f426.png"/>
</div>
</div>
<p>As hypothesized, there is a good relationship between the output of our latent node at the network bottleneck and the slope of the samples used to generate the data!</p>
<ul class="simple">
<li><p>our autoencoder has learned 1 value to represent the vectors of 3 values in the dataset!</p></li>
<li><p>this is a great demonstration of information compression, 3:1!</p></li>
</ul>
</section>
<section id="check-training-data-reconstruction">
<h2>Check Training Data Reconstruction</h2>
<p>Let‚Äôs visualize the reconstructed 1D data, encoded and then decoded with out autoencoder network.</p>
<ul class="simple">
<li><p>for all training data, I include the original data and the reconstructed data, i.e., data encoded and decoded by our trained autoencoder</p></li>
<li><p>for each data training sample, I include the sample slope for interest, but this label are not used in the in the training, nor with the encoder or decoder</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">for</span> <span class="n">idata</span><span class="p">,</span><span class="n">input_vec</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>                       <span class="c1"># plot training data and reconstructions             </span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">idata</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xmat</span><span class="p">[</span><span class="n">idata</span><span class="p">],</span><span class="n">x</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="o">+</span><span class="mi">2</span><span class="p">)),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xmat</span><span class="p">[</span><span class="n">idata</span><span class="p">],</span><span class="n">x</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="o">+</span><span class="mi">2</span><span class="p">)),</span><span class="n">label</span><span class="o">=</span><span class="s1">'data'</span><span class="p">)</span>
    <span class="n">custom_positions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">3.2</span><span class="p">]</span>
    <span class="n">custom_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'I1'</span><span class="p">,</span><span class="s1">'I2'</span><span class="p">,</span><span class="s1">'I3'</span><span class="p">,</span><span class="s1">'Y'</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ymat</span><span class="p">[</span><span class="n">idata</span><span class="p">],</span><span class="mi">2</span><span class="p">),[</span><span class="n">Xmat</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mf">3.25</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>  
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">output_mat</span><span class="p">[</span><span class="n">idata</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span><span class="n">x</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="o">+</span><span class="mi">2</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">output_mat</span><span class="p">[</span><span class="n">idata</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span><span class="n">x</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="o">+</span><span class="mi">2</span><span class="p">)),</span><span class="n">label</span><span class="o">=</span><span class="s1">'reconstruction'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">custom_positions</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">custom_labels</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'index'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'z'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Synthetic 1D Training Data #'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">idata</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">4.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fe7102239237e5dec48032be77d18524b61a306c2238811815f52c81fcbd5955.png" src="../Images/7203aaa35d3b4fe06560d8885fa0bc78.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/fe7102239237e5dec48032be77d18524b61a306c2238811815f52c81fcbd5955.png"/>
</div>
</div>
<p>The training data reconstruction is quite good!</p>
<ul class="simple">
<li><p>our autoencoder has learned to encode and decode the training data</p></li>
<li><p>demonstrating good dimensionality reduction from 3 to 1!</p></li>
</ul>
</section>
<section id="check-testing-data-reconstruction">
<h2>Check Testing Data Reconstruction</h2>
<p>Let‚Äôs generate additional data and test the reconstruction.</p>
<ul class="simple">
<li><p>check the performance of our training autoencoder with data not used to train the autoencoder, known as model generalization</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span><span class="o">+</span><span class="mi">7</span><span class="p">)</span>
<span class="n">nbatch_test</span> <span class="o">=</span> <span class="mi">12</span><span class="p">;</span> <span class="n">nnodes</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">ymat_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nbatch</span><span class="p">);</span> <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">nnodes</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="n">Xmat_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">nbatch</span><span class="p">,</span><span class="n">nnodes</span><span class="p">])</span>
<span class="n">data_test</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ibatch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nbatch</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">)</span>
    <span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">2.0</span><span class="p">)</span><span class="o">*</span><span class="n">m</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">nnodes</span><span class="p">)</span>
    <span class="n">ymat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">data_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="n">rank</span> <span class="o">=</span> <span class="n">rankdata</span><span class="p">(</span><span class="n">Xmat_test</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ibatch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nbatch_test</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="n">x</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">ibatch</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="n">x</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">ibatch</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">custom_positions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">3.2</span><span class="p">]</span>
    <span class="n">custom_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'I1'</span><span class="p">,</span><span class="s1">'I2'</span><span class="p">,</span><span class="s1">'I3'</span><span class="p">,</span><span class="s1">'Y'</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">rank</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ymat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="mi">2</span><span class="p">),[</span><span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mf">3.18</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ymat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="mi">2</span><span class="p">),[</span><span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mf">3.25</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">ibatch</span><span class="o">+</span><span class="mi">13</span><span class="p">,[</span><span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="mf">0.9</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">custom_positions</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">custom_labels</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.4</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Input Nodes'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'z'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Synthetic 1D Data and Labels'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Test Data Index: '</span><span class="p">,[</span><span class="o">-</span><span class="mf">1.45</span><span class="p">,</span><span class="mf">0.9</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/422b0a32f31f3498764ada7f7ec63e50a53a411b5d12e6c718a4288f371d62e8.png" src="../Images/28e4aff8d55696fd326194c3a79007d1.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/422b0a32f31f3498764ada7f7ec63e50a53a411b5d12e6c718a4288f371d62e8.png"/>
</div>
</div>
<p>Apply trained autoencoder to reconstruct test data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">output_vec_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data_test</span><span class="p">),</span><span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idata_test</span><span class="p">,</span><span class="n">input_vec_test</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_test</span><span class="p">):</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">input_vec_test</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span>                                                    <span class="c1"># forward pass</span>
    <span class="n">output_vec_test</span><span class="p">[</span><span class="n">idata_test</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">activations</span><span class="p">[</span><span class="s1">'O9'</span><span class="p">]],</span> <span class="p">[</span><span class="n">activations</span><span class="p">[</span><span class="s1">'O10'</span><span class="p">]],</span> <span class="p">[</span><span class="n">activations</span><span class="p">[</span><span class="s1">'O11'</span><span class="p">]]])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now visualizated the test data reconstructions,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">for</span> <span class="n">idata</span><span class="p">,</span><span class="n">input_vec_test</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_test</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">idata</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">input_vec_test</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_vec_test</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">label</span><span class="o">=</span><span class="s1">'data'</span><span class="p">)</span>
    <span class="n">custom_positions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">3.2</span><span class="p">]</span>
    <span class="n">custom_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'I1'</span><span class="p">,</span><span class="s1">'I2'</span><span class="p">,</span><span class="s1">'I3'</span><span class="p">,</span><span class="s1">'Y'</span><span class="p">]</span>
    <span class="c1"># plt.annotate(np.round(ymat[idata],2),[Xmat[idata][-1],3.25],size=8,color='black',ha='center')  </span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">output_vec_test</span><span class="p">[</span><span class="n">idata</span><span class="p">,:],</span><span class="n">x</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">output_vec_test</span><span class="p">[</span><span class="n">idata</span><span class="p">,:],</span><span class="n">x</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">label</span><span class="o">=</span><span class="s1">'reconstruction'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">custom_positions</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">custom_labels</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'index'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'z'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Synthetic 1D Test Image #'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">idata</span><span class="o">+</span><span class="mi">13</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">4.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6d4114b87e54f6dcb8a4cbefcdd99015b46e78ceca3a5d93daa4ff900d2cdf08.png" src="../Images/abda6b15ea724a35119dc1c5cf9554e9.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/6d4114b87e54f6dcb8a4cbefcdd99015b46e78ceca3a5d93daa4ff900d2cdf08.png"/>
</div>
</div>
<p>Our trained autoencoder seems to have generalized well with very good performance reconstructing training and also the withheld testing cases.</p>
<ul class="simple">
<li><p>For a more complete workflow we would evaluate training and testing error in parallel over training epochs to check for model overfit.</p></li>
<li><p>I separated these components for brevity and clarity in the demonstration</p></li>
</ul>
</section>
<section id="comments">
<h2>Comments</h2>
<p>This was a basic treatment of autoencoder deep learning networks. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a> and the YouTube lecture links at the start of this chapter with resource links in the videos‚Äô descriptions.</p>
<p>I hope this is helpful,</p>
<p><em>Michael</em></p>
</section>
<section id="about-the-author">
<h2>About the Author</h2>
<figure style="text-align: center;">
  <img src="../Images/eb709b2c0a0c715da01ae0165efdf3b2.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/intro/michael_pyrcz_officeshot_jacket.jpg"/>
  <figcaption style="text-align: center;"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.
</figcaption>
</figure>
<p>Michael Pyrcz is a professor in the <a class="reference external" href="https://cockrell.utexas.edu/faculty-directory/alphabetical/p">Cockrell School of Engineering</a>, and the <a class="reference external" href="https://www.jsg.utexas.edu/researcher/michael_pyrcz/">Jackson School of Geosciences</a>, at <a class="reference external" href="https://www.utexas.edu/">The University of Texas at Austin</a>, where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,</p>
<ul class="simple">
<li><p>the principal investigator of the <a class="reference external" href="https://fri.cns.utexas.edu/energy-analytics">Energy Analytics</a> freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin</p></li>
<li><p>an associate editor for <a class="reference external" href="https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board">Computers and Geosciences</a>, and a board member for <a class="reference external" href="https://link.springer.com/journal/11004/editorial-board">Mathematical Geosciences</a>, the International Association for Mathematical Geosciences.</p></li>
</ul>
<p>Michael has written over 70 <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en">peer-reviewed publications</a>, a <a class="reference external" href="https://pypi.org/project/geostatspy/">Python package</a> for spatial data analytics, co-authored a textbook on spatial data analytics, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a> and author of two recently released e-books, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy</a> and <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html">Applied Machine Learning in Python: a Hands-on Guide with Code</a>.</p>
<p>All of Michael‚Äôs university lectures are available on his <a class="reference external" href="https://www.youtube.com/@GeostatsGuyLectures">YouTube Channel</a> with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his <a class="reference external" href="https://github.com/GeostatsGuy">GitHub account</a>, to support any interested students and working professionals with evergreen content. To find out more about Michael‚Äôs work and shared educational resources visit his <span class="xref myst">Website</span>.</p>
</section>
<section id="want-to-work-together">
<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>I‚Äôm always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
</section>
&#13;

<h2>Motivation</h2>
<p>Autoencoders are a very powerful, flexible deep learning approach for compressing information,</p>
<ul class="simple">
<li><p>mapping training data to a latent space</p></li>
<li><p>dimensionality reduction of high dimensional data to a much lower dimensionality</p></li>
<li><p>nonlinear, general approach</p></li>
</ul>
&#13;

<h2>Autoencoder Architecture</h2>
<p>Here‚Äôs our simple autoencoder,</p>
<figure style="text-align: center;">
  <img src="../Images/ed815fe23f4bd258b278f7aa6f0dd58e.png" style="display: block; margin: 0 auto; width: 90%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/autoencoder.png"/>
  <figcaption style="text-align: center;"> Simple demonstration autoencoder.
</figcaption>
</figure>
<p>This is literally the artificial neural network from the <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ANN.html">Artificial Neural Networks</a> mirrored.</p>
<p>I do not discuss the forward pass through the network, if you are unfamiliar with this process, for example,</p>
<ul class="simple">
<li><p>activation applied to the linear weighting plus bias in the nodes</p></li>
</ul>
<p>then please review the artificial neural network chapter.</p>
<p>I decided to use unique numerical indices for each node for concise notation for connection weights, for example <span class="math notranslate nohighlight">\(\lambda_{1,4}\)</span>, and biases, for example, <span class="math notranslate nohighlight">\(b_4\)</span>, <span class="math notranslate nohighlight">\(I\)</span> for input nodes, <span class="math notranslate nohighlight">\(L\)</span> for encoder hidden layer (‚Äòleft‚Äô), <span class="math notranslate nohighlight">\(M\)</span> for latent node (‚Äòmiddle‚Äô), <span class="math notranslate nohighlight">\(R\)</span> for decoder hidden layer (‚Äòright‚Äô) and finally <span class="math notranslate nohighlight">\(O\)</span> for output nodes.</p>
<p>The parts of the autoencoder are indicated below,</p>
<figure style="text-align: center;">
  <img src="../Images/652eb880f88b2d046adcc751aa2d62f6.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/autoencoderparts.png"/>
  <figcaption style="text-align: center;"> Simple demonstration autoencoder with parts labeled.
</figcaption>
</figure>
<p>The signal passed through the autoencoder and notation include,</p>
<ul class="simple">
<li><p><strong>Input</strong> ‚Äì training samples,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
z
\]</div>
<ul class="simple">
<li><p><strong>Encoder</strong> ‚Äì learned compression of the training samples to latent space,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
x = f_{\theta} (z)
\]</div>
<ul class="simple">
<li><p><strong>Latent Space</strong> ‚Äì bottleneck summarizes patterns in the training data,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
ùë•
\]</div>
<ul class="simple">
<li><p><strong>Decoder</strong> ‚Äì learned decompression of the latent space to reconstruction of the original training data,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{z} = g_{\phi} (x) = g_{\phi} (f_{\theta}(z) )
\]</div>
<ul class="simple">
<li><p>Reconstruction ‚Äì attempt to reproduce input,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{z} \sim z
\]</div>
&#13;

<h2>Training Model Parameters</h2>
<p>Training an autoencoder proceeds iteratively by these steps.</p>
<figure style="text-align: center;">
  <img src="../Images/c3a5bc8956f8ceda05ddf9b582cd141d.png" style="display: block; margin: 0 auto; width: 90%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ANN/training_cycle.png"/>
  <figcaption style="text-align: center;"> Training an artificial neural network proceeds iteratively by, 1. forward pass to make a prediction, 2. calculate the error derivative based on the prediction and truth over training data, 3. backpropagate the error derivative back through the artificial neural network to calculate the derivatives of the error over all the model weights and biases parameters, 4. update the model parameters based on the derivatives and learning rates, 5. repeat until convergence.
</figcaption>
</figure>
<p>Here‚Äôs some details on each step,</p>
<ol class="arabic simple" start="0">
<li><p><strong>Initializing the Model Parameters</strong> - initialize all model parameters with typically small (near zero) random values. Here‚Äôs a couple common methods,</p></li>
</ol>
<ul class="simple">
<li><p><strong>Xavier Weight Initialization</strong> - random realizations from uniform distributions specified by <span class="math notranslate nohighlight">\(U[\text{min}, \text{max}]\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}}, \frac{1}{\sqrt{p}} \right] (p^\ell)
\]</div>
<ul class="simple">
<li><p>where <span class="math notranslate nohighlight">\(F^{-1}_U\)</span> is the inverse of the CDF, <span class="math notranslate nohighlight">\(p\)</span> is the number of inputs, and <span class="math notranslate nohighlight">\(p^{\ell}\)</span> is a random cumulative probability value drawn from the uniform distribution, <span class="math notranslate nohighlight">\(U[0,1]\)</span>.</p></li>
<li><p><strong>Normalized Xavier Weight Initialization</strong> - random realizations from uniform distributions specified by <span class="math notranslate nohighlight">\(U[\text{min}, \text{max}]\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] (p^\ell)
\]</div>
<ul class="simple">
<li><p>where <span class="math notranslate nohighlight">\(F^{-1}_U\)</span> is the inverse of the CDF, <span class="math notranslate nohighlight">\(p\)</span> is the number of inputs, <span class="math notranslate nohighlight">\(k\)</span> is the number of outputs, and <span class="math notranslate nohighlight">\(p^{\ell}\)</span> is a random cumulative probability value drawn from the uniform distribution, <span class="math notranslate nohighlight">\(U[0,1]\)</span>.</p></li>
<li><p>For example, if we return to our first hidden layer node,</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/b2f8e46ea497049f4b95c03b8812eea7.png" style="display: block; margin: 0 auto; width: 30%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/ANN/ann_walkthrough_hidden.png"/>
  <figcaption style="text-align: center;"> First hidden layer node with 3 inputs, and 1 output.
</figcaption>
</figure>
<ul class="simple">
<li><p>we have <span class="math notranslate nohighlight">\(p = 3\)</span> and <span class="math notranslate nohighlight">\(k = 1\)</span>, and we draw from the uniform distribution,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
U \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] = U \left[ \frac{-1}{\sqrt{3}+1}, \frac{1}{\sqrt{3}+1} \right]
\]</div>
<ol class="arabic simple">
<li><p><strong>Forward Pass</strong> - to pass a training sample, <span class="math notranslate nohighlight">\(z\)</span>, to calculate the reconstruction, $\hat{z}. Initial predictions will be random for the first iteration, but will improve.</p></li>
<li><p><strong>Calculate the Error Derivative</strong> - based on the miss match between the input training sample, <span class="math notranslate nohighlight">\(z\)</span>, and the reconstruction, <span class="math notranslate nohighlight">\(\hat{z}\)</span>.</p></li>
<li><p><strong>Backpropagate the Error Derivative</strong> - we shift back through the artificial neural network to calculate the derivatives of the error over all the model weights and biases parameters, to accomplish this we use the chain rule,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial x} f(g(h(x))) = \frac{\partial f}{\partial g} \cdot \frac{\partial g}{\partial h} \cdot \frac{\partial h}{\partial x}
\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Loop Over Batch and Average the Error Derivatives</strong> - go to step 1 for all training data in the batch and then calculate the average of the error derivatives, for example,</p></li>
<li><p><strong>Update the Model Parameters</strong> - based on the derivatives, \frac{\partial P}{\partial \lambda_{i,j}} and learning rates, <span class="math notranslate nohighlight">\(\eta\)</span>, like this,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\lambda_{1,4}^{\ell} = \lambda_{1,4}^{\ell-1} - \eta \cdot \frac{1}{B} \sum_{i=1}^{B} \frac{\partial \mathcal{L}^{(i)}}{\partial \lambda_{1,4}}
\]</div>
<ol class="arabic simple" start="5">
<li><p><strong>Repeat Until Convergence</strong> - return to step 1. until the error, <span class="math notranslate nohighlight">\(P\)</span>, is reduced to an acceptable level, i.e., model convergence is the condition to stop the iterations</p></li>
</ol>
&#13;

<h2>Autoencoder Loss</h2>
<p>There is a loss and loss gradient at each output-input node pair. The error loss function,</p>
<figure style="text-align: center;">
  <img src="../Images/701ec6c7b420f85dae65e62285e83b13.png" style="display: block; margin: 0 auto; width: 45%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/error.png"/>
  <figcaption style="text-align: center;"> Autoencoder loss at each output node, the goal is for the output to match the input.
</figcaption>
</figure>
<p>We can generalize as,</p>
<div class="math notranslate nohighlight">
\[
L = \frac{1}{2} \sum_{i=1}^3 \left(O_{i+8} - I_i \right)^2
\]</div>
<p>Note, the irregular indexing is due to my choice to use a unique node index at each node.</p>
<p>Error derivative at each node is,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial O_9} = O_9 - I_1
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial O_{10}} = O_{10} - I_2
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial O_{11}} = O_{11} - I_3
\]</div>
&#13;

<h2>Autoencoder Backpropagation</h2>
<p>Let‚Äôs walk through the back propagation of our autoencoder, let‚Äôs start with a bias in the output node, <span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial b_{9}}\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/8a6b2383ff34c83e1de1a609373cc653.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backb9.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the bias, \(ùëè_9\), in the hidden decoder node, \(ùëÇ_9\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_9} 
= \frac{\partial O_{9_{\mathrm{in}}}}{\partial b_9} 
\cdot \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial O_9} 
= 1 \cdot 1 \cdot (O_9 - I_1)
\]</div>
<p>Let‚Äôs explain each part. We start with the output gradient <span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial O_9}\)</span> and step across the output node, <span class="math notranslate nohighlight">\(O_9\)</span>, since linear activation is applied in the output nodes,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial O_9}{\partial O_{9_{in}}} = 1.0
\]</div>
<p>Now we can calculate the derivative of the bias, <span class="math notranslate nohighlight">\(b_9\)</span>, with respect to the node input,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial 0_{9_{\mathrm{in}}}}{\partial b_9} 
= \frac{\partial}{\partial b_9} \left( \lambda_{7,9} R_7 + \lambda_{8,9} R_8 + b_9 \right) 
= 1
\]</div>
<p>Now we can proceed to the connection weight, ùúÜ_7,9.</p>
<figure style="text-align: center;">
  <img src="../Images/80eaca0166d0cf02f98e140c090fca18.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/back79.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the connection weight, \(\lambda_{7,9}\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \lambda_{7,9}} 
= \frac{\partial O_{9_{\mathrm{in}}}}{\partial \lambda_{7,9}} 
\cdot \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial O_9} 
= R_7 \cdot 1 \cdot (O_9 - I_1)
\]</div>
<p>Once again, since linear activation is applied in the output nodes,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial O_9}{\partial O_{9_{in}}} = 1.0
\]</div>
<p>and <span class="math notranslate nohighlight">\(\frac{\partial O^{\text{in}}_9}{\partial \lambda_{7,9}}\)</span> is simply the output from <span class="math notranslate nohighlight">\(ùëÖ_7\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial O^{\text{in}}_9}{\partial \lambda_{7,9}} 
= \frac{\partial}{\partial \lambda_{7,9}} \left( \lambda_{7,9} R_7 + \lambda_{8,9} R_8 + b_9 \right) 
= R_7
\]</div>
<p>Let‚Äôs continue past <span class="math notranslate nohighlight">\(\partial \lambda_{7,9}\)</span> to the output from our decoder hidden node, <span class="math notranslate nohighlight">\(ùëÖ_7\)</span></p>
<figure style="text-align: center;">
  <img src="../Images/1c85ce96ca6f0999b7bc167c32d65b89.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backr7.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the output of the decoder hidden layer node \(R_7\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial R_7} 
= \frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7} 
\cdot \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial O_9} 
+ \frac{\partial O_{10_{\mathrm{in}}}}{\partial R_7} 
\cdot \frac{\partial O_{10}}{\partial O_{10_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial O_{10}} 
+ \frac{\partial O_{11_{\mathrm{in}}}}{\partial R_7} 
\cdot \frac{\partial O_{11}}{\partial O_{11_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial O_{11}}
\]</div>
<p>that we can evaluate as,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial R_7} 
= \lambda_{7,9} \cdot 1 \cdot (O_9 - I_1) 
+ \lambda_{7,10} \cdot 1 \cdot (O_{10} - I_2) 
+ \lambda_{7,11} \cdot 1 \cdot (O_{11} - I_3)
\]</div>
<p>We add the derivatives from each connection.
Once again, since linear activation at <span class="math notranslate nohighlight">\(ùëÇ_{9}\)</span>, <span class="math notranslate nohighlight">\(ùëÇ_{10}\)</span>, and <span class="math notranslate nohighlight">\(ùëÇ_{11}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} = 1, \quad
\frac{\partial O_{10}}{\partial O_{10_{\mathrm{in}}}} = 1, \quad
\frac{\partial O_{11}}{\partial O_{11_{\mathrm{in}}}} = 1
\]</div>
<p>Also, along the connection, the derivative is simply the weight,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7} = \lambda_{7,9}, \quad
\frac{\partial O_{10_{\mathrm{in}}}}{\partial R_7} = \lambda_{7,10}, \quad
\frac{\partial O_{11_{\mathrm{in}}}}{\partial R_7} = \lambda_{7,11}
\]</div>
<p>for example we can demonstrate this for <span class="math notranslate nohighlight">\(\frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7}\)</span> as,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7} 
= \frac{\partial}{\partial R_7} \left( \lambda_{7,9} R_7 + \lambda_{8,9} R_8 + b_9 \right) 
= \lambda_{7,9}
\]</div>
<p>Let‚Äôs continue from the output from our decoder hidden layer node, <span class="math notranslate nohighlight">\(ùëÖ_7\)</span>,  to calculate the derivative of the bias in the node, <span class="math notranslate nohighlight">\(b_7\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/604e4fcf99d1c41dd899458f80a67179.png" style="display: block; margin: 0 auto; width: 65%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backb7.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the bias, $b_7$, in the hidden decoder node, $R_7$.
</figcaption>
</figure>
<p>From the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_7} 
= \frac{\partial R_{7_{\mathrm{in}}}}{\partial b_7} 
\cdot \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial R_7}
\]</div>
<p>Since sigmoid activation at <span class="math notranslate nohighlight">\(R_7\)</span>, to move across the node,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} 
= \sigma' (R_7) = R_7 (1 - R_7)
\]</div>
<p>and for the partial derivative of the node input given the bias,</p>
<div class="math notranslate nohighlight">
\[
\frac{R_{7_{\mathrm{in}}}}{\partial b_7} 
= \frac{\partial}{\partial b_7} \left( \lambda_{6,7} M_6 + b_7 \right) 
= 1
\]</div>
<p>So now we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_7} 
= 1 \cdot R_7 (1 - R_7) \cdot 
\overbrace{
\left[
\lambda_{7,9} \cdot 1 \cdot (O_9 - I_1) 
+ \lambda_{7,10} \cdot 1 \cdot (O_{10} - I_2) 
+ \lambda_{7,11} \cdot 1 \cdot (O_{11} - I_3)
\right]
}^{\frac{\partial L}{\partial R_7}}
\]</div>
<p>Now we can proceed to the connection weight, <span class="math notranslate nohighlight">\(\lambda_{6,7}\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/1559af01deb817828f382cd89480ff41.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/back67.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the connection weight, \(\lambda_{6,7}\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \lambda_{6,7}} 
= \frac{\partial R_{7_{\mathrm{in}}}}{\partial \lambda_{6,7}} 
\cdot \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial R_7}
\]</div>
<p>Once again, since sigmoid activation is applied in the hidden layer nodes,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial R_7}{\partial R_{7_{in}}} = 1.0
\]</div>
<p>and <span class="math notranslate nohighlight">\(\frac{\partial R_{7_{\mathrm{in}}}}{\partial \lambda_{6,7}}\)</span> is simply the output from <span class="math notranslate nohighlight">\(M_6\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial R_{7_{\mathrm{in}}}}{\partial \lambda_{6,7}} 
= \frac{\partial}{\partial \lambda_{6,7}} \left( \lambda_{6,7} M_6 + b_6 \right) 
= M_6
\]</div>
<p>So now we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_7} 
= M_6 \cdot R_7 (1 - R_7) \cdot 
\overbrace{
\left[
\lambda_{7,9} \cdot 1 \cdot (O_9 - I_1) 
+ \lambda_{7,10} \cdot 1 \cdot (O_{10} - I_2) 
+ \lambda_{7,11} \cdot 1 \cdot (O_{11} - I_3)
\right]
}^{\frac{\partial \mathcal{L}}{\partial R_7}}
\]</div>
<p>Let‚Äôs get continue to the output from our latent node, ùëÄ_6</p>
<figure style="text-align: center;">
  <img src="../Images/f4cc7dbc1493a36ab0eb828c1422d1f2.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backm6.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the output of the latent node, \(M_6\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial M_6} 
= \frac{\partial R_{7_{\mathrm{in}}}}{\partial M_6} 
\cdot \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial R_7} 
+ \frac{\partial R_{8_{\mathrm{in}}}}{\partial M_6} 
\cdot \frac{\partial R_8}{\partial R_{8_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial R_8}
\]</div>
<p>That we can resolve as,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial M_6} 
= \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} 
+ \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8}
\]</div>
<p>Once again, since sigmoid activation,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} = R_7 (1 - R_7), \quad
\frac{\partial R_8}{\partial R_{8_{\mathrm{in}}}} = R_8 (1 - R_8)
\]</div>
<p>and along the connections,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial R_{7_{\mathrm{in}}}}{\partial M_6} 
&amp;= \frac{\partial}{\partial M_6} \left( \lambda_{6,7} M_6 + b_7 \right) 
= \lambda_{6,7} \\
\frac{\partial R_{8_{\mathrm{in}}}}{\partial M_6} 
&amp;= \frac{\partial}{\partial M_6} \left( \lambda_{6,8} M_6 + b_8 \right) 
= \lambda_{6,8}
\end{aligned}
\end{split}\]</div>
<p>Let‚Äôs continue from the output from our latent node, <span class="math notranslate nohighlight">\(M_6\)</span>, to calculate the derivative of the bias in the node, <span class="math notranslate nohighlight">\(b_6\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/90618005b205c6c5ceb09965c36cf2e1.png" style="display: block; margin: 0 auto; width: 43%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backb6.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the bias, $b_6$, in the latent node, $M_6$. Note image shifted to make room.
</figcaption>
</figure>
<p>From the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_6} 
= \frac{\partial M_{6_{\mathrm{in}}}}{\partial b_6} 
\cdot \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial M_6}
\]</div>
<p>Since sigmoid activation at <span class="math notranslate nohighlight">\(M_6\)</span>, to move across the node,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} 
= \sigma' (M_6) = M_6 \cdot (1 - M_6)
\]</div>
<p>and for the partial derivative of the node input given the bias,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_{6_{\mathrm{in}}}}{\partial b_6} 
= \frac{\partial}{\partial b_6} \left( \lambda_{4,6} L_4 + b_6 \right) 
= 1
\]</div>
<p>So now we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_6} 
= 1 \cdot M_6 (1 - M_6) \cdot 
\overbrace{
  \left[
    \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} 
    + \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8}
  \right]
}^{\frac{\partial \mathcal{L}}{\partial M_6}}
\]</div>
<p>Now we can proceed to the connection weight, <span class="math notranslate nohighlight">\(\lambda_{4,6}\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/f5770d05672cfe3c14c6973f2775d2de.png" style="display: block; margin: 0 auto; width: 43%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/back46.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the connection weight, \(\lambda_{4,6}\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} 
= \frac{\partial M_{6_{\mathrm{in}}}}{\partial \lambda_{4,6}} 
\cdot \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial M_6}
\]</div>
<p>Once again, since sigmoid activation is applied in the hidden layer nodes,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_6}{\partial M_{6_{in}}} = M_6 \cdot (1 - M_6)
\]</div>
<p>and <span class="math notranslate nohighlight">\(\frac{\partial M_{6_{\mathrm{in}}}}{\partial \lambda_{4,6}}\)</span> is simply the output from <span class="math notranslate nohighlight">\(L_4\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_{6_{\mathrm{in}}}}{\partial \lambda_{4,6}} 
= \frac{\partial}{\partial \lambda_{4,6}} \left( \lambda_{4,6} L_4 + \lambda_{5,6} L_5 + b_6 \right) 
= L_4
\]</div>
<p>So now we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} 
= L_4 \cdot M_6 (1 - M_6) \cdot
\overbrace{
\left[
\lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7}
+ \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8}
\right]
}^{\frac{\partial \mathcal{L}}{\partial M_6}}
\]</div>
<p>Now we can proceed to the output of our encoder hidden layer node, <span class="math notranslate nohighlight">\(L_4\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/1e5148ec01b8276d13a3ac564a201ab3.png" style="display: block; margin: 0 auto; width: 67%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backl4.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the output of the encoder hidden node, \(ùêø_4\).
</figcaption>
</figure>
<p>By the chain rule we get this and evaluate it as,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial L_4} 
= \frac{\partial M_{6_{\mathrm{in}}}}{\partial L_4} 
\cdot \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial M_6} 
= \lambda_{4,6} \cdot M_6 (1 - M_6) \cdot \frac{\partial \mathcal{L}}{\partial M_6}
\]</div>
<p>Once again, since sigmoid activation is applied in the latent node,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} = M_6 (1 - M_6)
\]</div>
<p>and <span class="math notranslate nohighlight">\(\frac{\partial M_{6_{\mathrm{in}}}}{\partial L_4}\)</span> is simply the weight, <span class="math notranslate nohighlight">\(\lambda_{4,6}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_{6_{\mathrm{in}}}}{\partial L_4} 
= \frac{\partial}{\partial L_4} \left( \lambda_{4,6} L_4 + b_6 \right) 
= \lambda_{4,6}
\]</div>
<p>Let‚Äôs continue from the output from our encoder hidden layer node, <span class="math notranslate nohighlight">\(L_4\)</span>, to calculate the derivative of the bias in the node, <span class="math notranslate nohighlight">\(b_4\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/cf8f925e7a89e3d992b323edfd45034e.png" style="display: block; margin: 0 auto; width: 67%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/backb4.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the bias, $b_4$, in the encoder hidden layer node, $L_4$.
</figcaption>
</figure>
<p>From the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_4} 
= \frac{\partial L_{4_{\mathrm{in}}}}{\partial b_4} 
\cdot \frac{\partial L_4}{\partial L_{4_{\mathrm{in}}}} 
\cdot \frac{\partial \mathcal{L}}{\partial L_4} 
= 1 \cdot L_4 (1 - L_4) \cdot \frac{\partial \mathcal{L}}{\partial L_4}
\]</div>
<p>Since sigmoid activation at <span class="math notranslate nohighlight">\(M_6\)</span>, to move across the node,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} 
= \sigma' (M_6) = M_6 \cdot (1 - M_6)
\]</div>
<p>and for the partial derivative of the node input given the bias,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial M_{6_{\mathrm{in}}}}{\partial b_6} 
= \frac{\partial}{\partial b_6} \left( \lambda_{4,6} L_4 + b_6 \right) 
= 1
\]</div>
<p>So now we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial b_6} 
= 1 \cdot M_6 (1 - M_6) \cdot 
\overbrace{
  \left[
    \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} 
    + \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8}
  \right]
}^{\frac{\partial \mathcal{L}}{\partial M_6}}
\]</div>
<p>And, finally we proceed to the connection weight, <span class="math notranslate nohighlight">\(\lambda_{1,4}\)</span>.</p>
<figure style="text-align: center;">
  <img src="../Images/3623ed192b17eb44b8f6f8c59b1dc0d0.png" style="display: block; margin: 0 auto; width: 67%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/autoencoder/back14.png"/>
  <figcaption style="text-align: center;"> Backpropagation to the connection weight, \(\lambda_{1,4}\).
</figcaption>
</figure>
<p>By the chain rule we get,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} 
= \frac{\partial L^{\text{in}}_4}{\partial \lambda_{1,4}} 
\cdot \frac{\partial L_4}{\partial L^{\text{in}}_4} 
\cdot \frac{\partial \mathcal{L}}{\partial L_4}
\]</div>
<p>Once again, since sigmoid activation is applied in the hidden layer nodes,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L_4}{\partial L_{4_{in}}} = L_4 \cdot (1 - L_4)
\]</div>
<p>and <span class="math notranslate nohighlight">\(\frac{\partial L^{\text{in}}_4}{\partial \lambda_{1,4}}\)</span> is simply the output from <span class="math notranslate nohighlight">\(I_1\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L^{\text{in}}_4}{\partial \lambda_{1,4}} 
= \frac{\partial}{\partial \lambda_{1,4}} \left( \lambda_{1,4} I_1 + \lambda_{2,4} I_2 + \lambda_{3,4} I_3 + b_4 \right) 
= I_1
\]</div>
<p>So now we have,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L}{\partial \lambda_{1,4}} 
= I_1 \cdot L_4 (1 - L_4) \cdot \underbrace{\left[ \lambda_{4,6} \cdot M_6 (1 - M_6) \cdot \frac{\partial L}{\partial M_6} \right]}_{\frac{\partial L}{\partial L_4}}
\]</div>
<p>Now we will build out this autoencoder from the ground up with only the NumPy python package for arrays and Python built-in data structure dictionaries.</p>
&#13;

<h2>Import Required Packages</h2>
<p>We will also need some standard packages. These should have been installed with Anaconda 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">ignore_warnings</span> <span class="o">=</span> <span class="kc">True</span>                                        <span class="c1"># ignore warnings?</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span> <span class="n">AutoMinorLocator</span><span class="p">,</span> <span class="n">AutoLocator</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># set axes and grids in the background for all plots</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rankdata</span>                              <span class="c1"># to assist with plot label placement</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>             <span class="c1"># fit the relationship between latent and training data slope               </span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">13</span>                                                     <span class="c1"># random number seed</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">tab20</span>                                           <span class="c1"># default colormap</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># plot all grids below the plot elements</span>
<span class="k">if</span> <span class="n">ignore_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>                                   
    <span class="kn">import</span> <span class="nn">warnings</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available with the respective package docs.</p>
&#13;

<h2>Declare Functions</h2>
<p>Here‚Äôs the functions to train and visualize our autoencoder.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">add_grid</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks  </span>

<span class="k">def</span> <span class="nf">xavier</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>                                      <span class="c1"># Xavier initializer function</span>
    <span class="n">limit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">6</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_in</span> <span class="o">+</span> <span class="n">n_out</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>                                               <span class="c1"># sigmoid activation</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">():</span>                                  <span class="c1"># initialize all weights and biases and build dictionaries of both</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>                            
        <span class="s1">'w14'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w24'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w34'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w15'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w25'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w35'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w46'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="s1">'w56'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="s1">'w67'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w68'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s1">'w79'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="s1">'w89'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="s1">'w710'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="s1">'w810'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="s1">'w711'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="s1">'w811'</span><span class="p">:</span> <span class="n">xavier</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>                                                <span class="c1"># biases (one per neuron, excluding input)</span>
        <span class="s1">'b4'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b5'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b6'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b7'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b8'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b9'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b10'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">'b11'</span><span class="p">:</span> <span class="mf">0.0</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> 

<span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="n">input_vec</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">):</span>                 <span class="c1"># forward pass of the autoencoder</span>
    <span class="n">I1</span><span class="p">,</span> <span class="n">I2</span><span class="p">,</span> <span class="n">I3</span> <span class="o">=</span> <span class="n">input_vec</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>                               <span class="c1"># input nodes (I1, I2, I3)</span>
    <span class="n">z4</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w14'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I1</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w24'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I2</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w34'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I3</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b4'</span><span class="p">]</span> <span class="c1"># encoder</span>
    <span class="n">a4</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z4</span><span class="p">)</span>

    <span class="n">z5</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w15'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I1</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w25'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I2</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w35'</span><span class="p">]</span> <span class="o">*</span> <span class="n">I3</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b5'</span><span class="p">]</span>
    <span class="n">a5</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z5</span><span class="p">)</span>

    <span class="n">z6</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w46'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a4</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w56'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a5</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b6'</span><span class="p">]</span> <span class="c1"># bottlekneck</span>
    <span class="n">a6</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z6</span><span class="p">)</span>

    <span class="n">z7</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w67'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a6</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b7'</span><span class="p">]</span>                   <span class="c1"># decoder</span>
    <span class="n">a7</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z7</span><span class="p">)</span>

    <span class="n">z8</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w68'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a6</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b8'</span><span class="p">]</span>
    <span class="n">a8</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z8</span><span class="p">)</span>

    <span class="n">z9</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w79'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a7</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w89'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a8</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b9'</span><span class="p">]</span>
    <span class="n">a9</span> <span class="o">=</span> <span class="n">z9</span>  

    <span class="n">z10</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w710'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a7</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w810'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a8</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b10'</span><span class="p">]</span>
    <span class="n">a10</span> <span class="o">=</span> <span class="n">z10</span>  <span class="c1"># linear</span>

    <span class="n">z11</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w711'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a7</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w811'</span><span class="p">]</span> <span class="o">*</span> <span class="n">a8</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="s1">'b11'</span><span class="p">]</span>
    <span class="n">a11</span> <span class="o">=</span> <span class="n">z11</span>  <span class="c1"># linear</span>

    <span class="k">return</span> <span class="p">{</span>                                                  <span class="c1"># return all activations as a dictionary</span>
        <span class="s1">'I1'</span><span class="p">:</span> <span class="n">I1</span><span class="p">,</span> <span class="s1">'I2'</span><span class="p">:</span> <span class="n">I2</span><span class="p">,</span> <span class="s1">'I3'</span><span class="p">:</span> <span class="n">I3</span><span class="p">,</span>
        <span class="s1">'L4'</span><span class="p">:</span> <span class="n">a4</span><span class="p">,</span> <span class="s1">'L5'</span><span class="p">:</span> <span class="n">a5</span><span class="p">,</span>
        <span class="s1">'M6'</span><span class="p">:</span> <span class="n">a6</span><span class="p">,</span>
        <span class="s1">'R7'</span><span class="p">:</span> <span class="n">a7</span><span class="p">,</span> <span class="s1">'R8'</span><span class="p">:</span> <span class="n">a8</span><span class="p">,</span>
        <span class="s1">'O9'</span><span class="p">:</span> <span class="n">a9</span><span class="p">,</span> <span class="s1">'O10'</span><span class="p">:</span> <span class="n">a10</span><span class="p">,</span> <span class="s1">'O11'</span><span class="p">:</span> <span class="n">a11</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">mse_loss_and_derivative</span><span class="p">(</span><span class="n">output_vec</span><span class="p">,</span> <span class="n">input_vec</span><span class="p">):</span>           <span class="c1"># MSE loss and error derivative given output and input</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">output_vec</span> <span class="o">-</span> <span class="n">input_vec</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">diff</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">dloss_dout</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span>  <span class="c1"># shape (3,1)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dloss_dout</span>

<span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>                                    <span class="c1"># derivative of sigmoid activation</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">backpropagate</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">dloss_dout</span><span class="p">):</span>  <span class="c1"># backpropagate the error derivatives</span>
    <span class="n">I1</span><span class="p">,</span> <span class="n">I2</span><span class="p">,</span> <span class="n">I3</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'I1'</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'I2'</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'I3'</span><span class="p">]</span>
    <span class="n">a4</span><span class="p">,</span> <span class="n">a5</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'L4'</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'L5'</span><span class="p">]</span>
    <span class="n">a6</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'M6'</span><span class="p">]</span>
    <span class="n">a7</span><span class="p">,</span> <span class="n">a8</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'R7'</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'R8'</span><span class="p">]</span>
    <span class="n">O9</span><span class="p">,</span> <span class="n">O10</span><span class="p">,</span> <span class="n">O11</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'O9'</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'O10'</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'O11'</span><span class="p">]</span>

    <span class="n">delta9</span> <span class="o">=</span> <span class="n">dloss_dout</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>                                 <span class="c1"># error terms (delta) for output nodes = dLoss/dOutput</span>
    <span class="n">delta10</span> <span class="o">=</span> <span class="n">dloss_dout</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">delta11</span> <span class="o">=</span> <span class="n">dloss_dout</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="n">grad_weights</span> <span class="o">=</span> <span class="p">{}</span>                                         <span class="c1"># gradients for weights from R7, R8 to O9, O10, O11</span>
    <span class="n">grad_biases</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w79'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta9</span> <span class="o">*</span> <span class="n">a7</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w89'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta9</span> <span class="o">*</span> <span class="n">a8</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w710'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta10</span> <span class="o">*</span> <span class="n">a7</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w810'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta10</span> <span class="o">*</span> <span class="n">a8</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w711'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta11</span> <span class="o">*</span> <span class="n">a7</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w811'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta11</span> <span class="o">*</span> <span class="n">a8</span>

    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b9'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta9</span>
    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b10'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta10</span>
    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b11'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta11</span>

    <span class="n">delta_r7</span> <span class="o">=</span> <span class="p">(</span><span class="n">delta9</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w79'</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta10</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w710'</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta11</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w711'</span><span class="p">])</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">a7</span><span class="p">)</span> <span class="c1"># gradients for R7 and R8</span>
    <span class="n">delta_r8</span> <span class="o">=</span> <span class="p">(</span><span class="n">delta9</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w89'</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta10</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w810'</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta11</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w811'</span><span class="p">])</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">a8</span><span class="p">)</span>

    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w67'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_r7</span> <span class="o">*</span> <span class="n">a6</span>                       <span class="c1"># gradients for weights from M6 to R7, R8</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w68'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_r8</span> <span class="o">*</span> <span class="n">a6</span>

    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b7'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_r7</span>
    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b8'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_r8</span>

    <span class="n">delta_m6</span> <span class="o">=</span> <span class="p">(</span><span class="n">delta_r7</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w67'</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta_r8</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w68'</span><span class="p">])</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">a6</span><span class="p">)</span> <span class="c1"># backpropagate delta to M6 (sigmoid)</span>

    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w46'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_m6</span> <span class="o">*</span> <span class="n">a4</span>                       <span class="c1"># gradients for weights from L4, L5 to M6</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w56'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_m6</span> <span class="o">*</span> <span class="n">a5</span>

    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b6'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_m6</span>

    <span class="n">delta_l4</span> <span class="o">=</span> <span class="n">delta_m6</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w46'</span><span class="p">]</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">a4</span><span class="p">)</span> <span class="c1"># backpropagate delta to L4, L5 (sigmoid)</span>
    <span class="n">delta_l5</span> <span class="o">=</span> <span class="n">delta_m6</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s1">'w56'</span><span class="p">]</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">a5</span><span class="p">)</span>

    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w14'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l4</span> <span class="o">*</span> <span class="n">I1</span>                       <span class="c1"># gradients for weights from I1, I2, I3 to L4</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w24'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l4</span> <span class="o">*</span> <span class="n">I2</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w34'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l4</span> <span class="o">*</span> <span class="n">I3</span>

    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b4'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l4</span>

    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w15'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l5</span> <span class="o">*</span> <span class="n">I1</span>                       <span class="c1"># gradients for weights from I1, I2, I3 to L5</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w25'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l5</span> <span class="o">*</span> <span class="n">I2</span>
    <span class="n">grad_weights</span><span class="p">[</span><span class="s1">'w35'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l5</span> <span class="o">*</span> <span class="n">I3</span>

    <span class="n">grad_biases</span><span class="p">[</span><span class="s1">'b5'</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_l5</span>
    <span class="k">return</span> <span class="n">grad_weights</span><span class="p">,</span> <span class="n">grad_biases</span>

<span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">grad_weights</span><span class="p">,</span> <span class="n">grad_biases</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span> <span class="c1"># update the weights and biased by derivatives and learning rate</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">grad_weights</span><span class="p">:</span>                                  <span class="c1"># update weights</span>
        <span class="n">weights</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_weights</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">grad_biases</span><span class="p">:</span>                                   <span class="c1"># update biases</span>
        <span class="n">biases</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_biases</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Visualize the Autoencoder Network</h2>
<p>Here we specify the autoencoder labels, positions, connections and colors and then plot the autoencoder.</p>
<ul class="simple">
<li><p>while this code is general, the actual autoencoder codes are not generalized to work with other architectures, for example changing the depth or width of the network</p></li>
<li><p>change the display parameters but do not the autoencoder architecture</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">positions</span> <span class="o">=</span> <span class="p">{</span>                                                 <span class="c1"># node positions</span>
    <span class="s1">'I1'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s1">'I2'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">'I3'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="s1">'L4'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="s1">'L5'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
    <span class="s1">'M6'</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="s1">'R7'</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="s1">'R8'</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
    <span class="s1">'O9'</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s1">'O10'</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">'O11'</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">node_colors</span> <span class="o">=</span> <span class="p">{</span>                                               <span class="c1"># node colors</span>
    <span class="s1">'I1'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span> <span class="s1">'I2'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span> <span class="s1">'I3'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span>
    <span class="s1">'L4'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span> <span class="s1">'L5'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span>
    <span class="s1">'M6'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span>
    <span class="s1">'R7'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span> <span class="s1">'R8'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span>
    <span class="s1">'O9'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span> <span class="s1">'O10'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span> <span class="s1">'O11'</span><span class="p">:</span> <span class="s1">'white'</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">edges</span> <span class="o">=</span> <span class="p">[</span>                                                     <span class="c1"># edges and weight labels</span>
    <span class="p">(</span><span class="s1">'I1'</span><span class="p">,</span> <span class="s1">'L4'</span><span class="p">,</span> <span class="s1">'lightcoral'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'I2'</span><span class="p">,</span> <span class="s1">'L4'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'I3'</span><span class="p">,</span> <span class="s1">'L4'</span><span class="p">,</span> <span class="s1">'darkred'</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'I1'</span><span class="p">,</span> <span class="s1">'L5'</span><span class="p">,</span> <span class="s1">'dodgerblue'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'I2'</span><span class="p">,</span> <span class="s1">'L5'</span><span class="p">,</span> <span class="s1">'blue'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'I3'</span><span class="p">,</span> <span class="s1">'L5'</span><span class="p">,</span> <span class="s1">'darkblue'</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'L4'</span><span class="p">,</span> <span class="s1">'M6'</span><span class="p">,</span> <span class="s1">'orange'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'L5'</span><span class="p">,</span> <span class="s1">'M6'</span><span class="p">,</span> <span class="s1">'darkorange'</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'M6'</span><span class="p">,</span> <span class="s1">'R7'</span><span class="p">,</span> <span class="s1">'orange'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'M6'</span><span class="p">,</span> <span class="s1">'R8'</span><span class="p">,</span> <span class="s1">'darkorange'</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'R7'</span><span class="p">,</span> <span class="s1">'O9'</span><span class="p">,</span> <span class="s1">'lightcoral'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'R7'</span><span class="p">,</span> <span class="s1">'O10'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'R7'</span><span class="p">,</span> <span class="s1">'O11'</span><span class="p">,</span> <span class="s1">'darkred'</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'R8'</span><span class="p">,</span> <span class="s1">'O9'</span><span class="p">,</span> <span class="s1">'dodgerblue'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'R8'</span><span class="p">,</span> <span class="s1">'O10'</span><span class="p">,</span> <span class="s1">'blue'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'R8'</span><span class="p">,</span> <span class="s1">'O11'</span><span class="p">,</span> <span class="s1">'darkblue'</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">weight_labels</span> <span class="o">=</span> <span class="p">{</span> <span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">,):</span> <span class="sa">f</span><span class="s2">"$</span><span class="se">\\</span><span class="s2">lambda_</span><span class="se">{{</span><span class="si">{</span><span class="n">src</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}{</span><span class="n">dst</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="si">}</span><span class="se">}}</span><span class="s2">$"</span> <span class="k">for</span> <span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span> <span class="ow">in</span> <span class="n">edges</span> <span class="p">}</span>

<span class="n">bias_offsets</span> <span class="o">=</span> <span class="p">{</span>                                              <span class="c1"># bias vector offsets</span>
    <span class="s1">'L4'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">),</span> <span class="s1">'L5'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">),</span>
    <span class="s1">'M6'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">),</span>
    <span class="s1">'R7'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">),</span> <span class="s1">'R8'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">),</span>
    <span class="s1">'O9'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">),</span> <span class="s1">'O10'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">),</span> <span class="s1">'O11'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">bias_labels</span> <span class="o">=</span> <span class="p">{</span> <span class="n">node</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"$b_</span><span class="se">{{</span><span class="si">{</span><span class="n">node</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="si">}</span><span class="se">}}</span><span class="s2">$"</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">bias_offsets</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="p">}</span>
<span class="c1"># Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">custom_weight_offsets</span> <span class="o">=</span> <span class="p">{</span>                                     <span class="c1"># custom label offsets for select overlapping weights</span>
    <span class="p">(</span><span class="s1">'I2'</span><span class="p">,</span> <span class="s1">'L4'</span><span class="p">):</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.20</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'I2'</span><span class="p">,</span> <span class="s1">'L5'</span><span class="p">):</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'R8'</span><span class="p">,</span> <span class="s1">'O9'</span><span class="p">):</span> <span class="p">(</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">'R8'</span><span class="p">,</span> <span class="s1">'O10'</span><span class="p">):</span> <span class="p">(</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.16</span><span class="p">),</span>
<span class="p">}</span>

<span class="k">for</span> <span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span> <span class="ow">in</span> <span class="n">edges</span><span class="p">:</span>                               <span class="c1"># plot edges and weight labels</span>
    <span class="n">x0</span><span class="p">,</span> <span class="n">y0</span> <span class="o">=</span> <span class="n">positions</span><span class="p">[</span><span class="n">src</span><span class="p">]</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span> <span class="o">=</span> <span class="n">positions</span><span class="p">[</span><span class="n">dst</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">],</span> <span class="p">[</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">xm</span><span class="p">,</span> <span class="n">ym</span> <span class="o">=</span> <span class="p">(</span><span class="n">x0</span> <span class="o">+</span> <span class="n">x1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">y0</span> <span class="o">+</span> <span class="n">y1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span> <span class="o">=</span> <span class="n">custom_weight_offsets</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">xm</span> <span class="o">+</span> <span class="n">dx</span><span class="p">,</span> <span class="n">ym</span> <span class="o">+</span> <span class="n">dy</span><span class="p">,</span> <span class="n">weight_labels</span><span class="p">[(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">)],</span>
            <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="n">positions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>                        <span class="c1"># white back circles</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="n">positions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>                        <span class="c1"># node circles and labels</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">node_colors</span><span class="p">[</span><span class="n">node</span><span class="p">],</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span> <span class="ow">in</span> <span class="n">bias_offsets</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>                   <span class="c1"># bias arrows and tighter label placement</span>
    <span class="n">nx</span><span class="p">,</span> <span class="n">ny</span> <span class="o">=</span> <span class="n">positions</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
    <span class="n">bx</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="n">nx</span> <span class="o">+</span> <span class="n">dx</span><span class="p">,</span> <span class="n">ny</span> <span class="o">+</span> <span class="n">dy</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">),</span>
                <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">"-&gt;"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">,</span> <span class="n">bias_labels</span><span class="p">[</span><span class="n">node</span><span class="p">],</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'right'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'bottom'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Final formatting</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/333249f6a43bbad84e15a2423db3b9cc8670650c55532adfe9fea6ac7c992872.png" src="../Images/330a264f2ed0fefaff128fb34a83b1e7.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/333249f6a43bbad84e15a2423db3b9cc8670650c55532adfe9fea6ac7c992872.png"/>
</div>
</div>
&#13;

<h2>Make an Interesting Synthetic Dataset</h2>
<p>Generate a stochastic dataset of 1D length of 3 vectors with a pattern that can be summarized by our autoencoder.</p>
<ul class="simple">
<li><p>if we generate random 1D vectors of length 3 our autoencoder would not be able to summarize, i.e., it is not possible to compress the information from the original 3 values</p></li>
<li><p>we must include a pattern that can be learned by the autoencoder to observe dimensionality reduction through the latent node with good data reconstruction</p></li>
</ul>
<p>To do this, I have calculate dataset as a hybrid model, linear + small random residual. The data generation steps include,</p>
<ol class="arabic simple">
<li><p>draw a random slope <span class="math notranslate nohighlight">\(\sim N\left[-2.0, 2.0 \right]\)</span></p></li>
<li><p>calculate 3 points at locations <span class="math notranslate nohighlight">\(\left[-1, 0, 1 \right]\)</span>, <span class="math notranslate nohighlight">\(f(\left[-1, 0, 1 \right])\)</span></p></li>
<li><p>add random, independent residual to each location, <span class="math notranslate nohighlight">\(f(\left[-1, 0, 1 \right]) + N\left[0.0,\sigma \right]\)</span>, where sigma is the residual standard deviation</p></li>
</ol>
<p>Note, the slope is retained as a label that will be compared to the latent node, <span class="math notranslate nohighlight">\(M_6\)</span> output to check, what has our autoencoder has learned?</p>
<ul class="simple">
<li><p>our hypothesis is that the autoencoder will learn a value that directly maps to slope to describe this dataset.</p></li>
<li><p>note, while this label is used to demonstrate the ability of the autoencoder to learn, it is not used to train the model!</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>                                 <span class="c1"># set random seed</span>
<span class="n">nbatch</span> <span class="o">=</span> <span class="mi">12</span><span class="p">;</span> <span class="n">nnodes</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.1</span>                          <span class="c1"># set number of data (total number of data), number of nodes (must be 3), error st.dev.</span>
<span class="n">ymat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nbatch</span><span class="p">);</span> <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">nnodes</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="n">Xmat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">nbatch</span><span class="p">,</span><span class="n">nnodes</span><span class="p">])</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ibatch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nbatch</span><span class="p">):</span>                                <span class="c1"># loop over synthetic data</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">)</span>
    <span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">2.0</span><span class="p">)</span><span class="o">*</span><span class="n">m</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">nnodes</span><span class="p">)</span>
    <span class="n">ymat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="n">rank</span> <span class="o">=</span> <span class="n">rankdata</span><span class="p">(</span><span class="n">Xmat</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>                                   <span class="c1"># rank data to improve (alternate) adjacent labels' locations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot the synthetic data</span>
<span class="k">for</span> <span class="n">ibatch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nbatch</span><span class="p">):</span>                                
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="n">x</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">ibatch</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="n">x</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">ibatch</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">custom_positions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">3.2</span><span class="p">]</span>
    <span class="n">custom_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'I1'</span><span class="p">,</span><span class="s1">'I2'</span><span class="p">,</span><span class="s1">'I3'</span><span class="p">,</span><span class="s1">'Y'</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">rank</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ymat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="mi">2</span><span class="p">),[</span><span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mf">3.18</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ymat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="mi">2</span><span class="p">),[</span><span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mf">3.25</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">ibatch</span><span class="o">+</span><span class="mi">1</span><span class="p">,[</span><span class="n">Xmat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="mf">0.9</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">custom_positions</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">custom_labels</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.4</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Input Nodes'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'z'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Synthetic 1D Data and Labels'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Data Index: '</span><span class="p">,[</span><span class="o">-</span><span class="mf">1.4</span><span class="p">,</span><span class="mf">0.9</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/39557c0e268bdf9e355e0769aff4633ec5601e1ef244d68560aa0a4c22ac5f3f.png" src="../Images/9ce15f05dfa887ce6ee1f02619cb004d.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/39557c0e268bdf9e355e0769aff4633ec5601e1ef244d68560aa0a4c22ac5f3f.png"/>
</div>
</div>
&#13;

<h2>Train the Autoencoder</h2>
<p>We have previously defined all the basic functions for our autoencoder so we can put together our autoencoder training steps with the following functions,</p>
<ol class="arabic simple">
<li><p><strong>initialize_parameters</strong> - initialize the weights and bias</p></li>
<li><p><strong>forward_pass</strong> - forward pass through our autoencoder to calculate node outputs and data reconstruction</p></li>
<li><p><strong>mse_loss_and_derivative</strong> - calculate the L2 loss and associated error derivative for each output node from training data and reconstruction</p></li>
<li><p><strong>backpropagate</strong> - backpropagate the error derivative through the network based on error derivative and node outputs and then average the gradients at each weight and bias over the batch</p></li>
<li><p><strong>update_parameters</strong> - update the weights and biases with the average gradient over the batch and the learning rate</p></li>
<li><p>go to 2 until convergence, in the case a set number of training epochs</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">epochs</span> <span class="o">=</span> <span class="mi">10000</span>                                                <span class="c1"># set hyperparameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">nbatch</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">13</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">output_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="mi">3</span><span class="p">));</span> <span class="n">loss_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">epochs</span><span class="p">));</span> <span class="n">M6_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span><span class="n">epochs</span><span class="p">))</span>

<span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">()</span>                     <span class="c1"># initialize weights and biases</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">sum_grad_w</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">weights</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>               <span class="c1"># initialize zero dictionary to average backpropogated gradients</span>
    <span class="n">sum_grad_b</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">biases</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">idata</span><span class="p">,</span><span class="n">input_vec</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">input_vec</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span> <span class="c1"># forward pass</span>
        <span class="n">M6_mat</span><span class="p">[</span><span class="n">idata</span><span class="p">,</span><span class="n">epoch</span><span class="p">]</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">'M6'</span><span class="p">]</span>
        <span class="n">output_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">activations</span><span class="p">[</span><span class="s1">'O9'</span><span class="p">]],</span> <span class="p">[</span><span class="n">activations</span><span class="p">[</span><span class="s1">'O10'</span><span class="p">]],</span> <span class="p">[</span><span class="n">activations</span><span class="p">[</span><span class="s1">'O11'</span><span class="p">]]])</span>
        <span class="n">output_mat</span><span class="p">[</span><span class="n">idata</span><span class="p">,</span><span class="n">epoch</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">output_vec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">dloss_dout</span> <span class="o">=</span> <span class="n">mse_loss_and_derivative</span><span class="p">(</span><span class="n">output_vec</span><span class="p">,</span> <span class="n">input_vec</span><span class="p">)</span> <span class="c1"># compute loss and derivative</span>
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span>
        <span class="n">grad_w</span><span class="p">,</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="n">backpropagate</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">dloss_dout</span><span class="p">)</span> <span class="c1"># backpropagation the derivative</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">grad_w</span><span class="p">:</span>                                      <span class="c1"># accumulate gradients</span>
            <span class="n">sum_grad_w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">grad_w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">grad_b</span><span class="p">:</span>
            <span class="n">sum_grad_b</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">grad_b</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="n">avg_grad_w</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="o">/</span> <span class="n">batch_size</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sum_grad_w</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span> <span class="c1"># average gradients over batch</span>
    <span class="n">avg_grad_b</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="o">/</span> <span class="n">batch_size</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sum_grad_b</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">epoch_loss</span> <span class="o">/=</span> <span class="n">batch_size</span>
    <span class="n">loss_mat</span><span class="p">[</span><span class="n">epoch</span><span class="p">]</span> <span class="o">=</span> <span class="n">epoch_loss</span>
    <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">avg_grad_w</span><span class="p">,</span> <span class="n">avg_grad_b</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span> <span class="c1"># update parameters</span>
    <span class="c1"># if epoch % 500 == 0:                                    # print loss every 100 training epochs</span>
    <span class="c1">#     print(f"Epoch {epoch}, Loss: {epoch_loss:.6f}")</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot training error vs. training epoch</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">loss_mat</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">'MSE'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="n">epoch</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Epochs'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">'Mean Square Error (L2 loss)'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Autoencoder Average Batch L2 Loss vs. Training Epoch'</span><span class="p">)</span>
<span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">'linear'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b4374d79f0f8d85887bb5f6075aa68f024e9e33bc189c7492047de36822bcb2a.png" src="../Images/3ab1c8fef6098b7c75943615555e53e5.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/b4374d79f0f8d85887bb5f6075aa68f024e9e33bc189c7492047de36822bcb2a.png"/>
</div>
</div>
<p>The average L2 loss vs. training epoch curve looks very good.</p>
<ul class="simple">
<li><p>we are seeing a pause in learning and then suddenly a fast reduction in training error and then slow convergence</p></li>
<li><p>I stopped at 10,000 epochs for efficiency</p></li>
</ul>
&#13;

<h2>Evaluating Our Autoencoder Network</h2>
<p>Let‚Äôs look at the output from the latent node at the network bottleneck, i.e., the output of node M6.</p>
<ul class="simple">
<li><p>notice above that we recorded the M6 output (called node activation) for all training epochs and for all data.</p></li>
<li><p>let‚Äôs look at the final trained network, the last epoch, and loop over all data</p></li>
</ul>
<p>Here‚Äôs a plot of final epoch M6 output vs. the sample slopes,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">linear_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ymat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">M6_mat</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># fit linear model to regress latent on training data slope</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># plot latent vs. training data slope</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mi">100</span><span class="p">),</span><span class="n">linear_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ibatch</span><span class="p">,</span><span class="n">input_vec</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>                      <span class="c1"># plot and label training data</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ymat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="n">M6_mat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">ibatch</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">ibatch</span><span class="o">+</span><span class="mi">1</span><span class="p">,[</span><span class="n">ymat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span><span class="n">M6_mat</span><span class="p">[</span><span class="n">ibatch</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">0.01</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'M6 Output'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">'Sample Slope, $m_i$'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Latent Node Output vs. Sample Slope'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.4</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d0ab2f0cf0b3c073fe93da6aa44c8e663feb48b72910cb0e72f2f5ec9416f426.png" src="../Images/7815f0d074113f20a6f77a446f1f83d2.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/d0ab2f0cf0b3c073fe93da6aa44c8e663feb48b72910cb0e72f2f5ec9416f426.png"/>
</div>
</div>
<p>As hypothesized, there is a good relationship between the output of our latent node at the network bottleneck and the slope of the samples used to generate the data!</p>
<ul class="simple">
<li><p>our autoencoder has learned 1 value to represent the vectors of 3 values in the dataset!</p></li>
<li><p>this is a great demonstration of information compression, 3:1!</p></li>
</ul>
&#13;

<h2>Check Training Data Reconstruction</h2>
<p>Let‚Äôs visualize the reconstructed 1D data, encoded and then decoded with out autoencoder network.</p>
<ul class="simple">
<li><p>for all training data, I include the original data and the reconstructed data, i.e., data encoded and decoded by our trained autoencoder</p></li>
<li><p>for each data training sample, I include the sample slope for interest, but this label are not used in the in the training, nor with the encoder or decoder</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">for</span> <span class="n">idata</span><span class="p">,</span><span class="n">input_vec</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>                       <span class="c1"># plot training data and reconstructions             </span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">idata</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xmat</span><span class="p">[</span><span class="n">idata</span><span class="p">],</span><span class="n">x</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="o">+</span><span class="mi">2</span><span class="p">)),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xmat</span><span class="p">[</span><span class="n">idata</span><span class="p">],</span><span class="n">x</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="o">+</span><span class="mi">2</span><span class="p">)),</span><span class="n">label</span><span class="o">=</span><span class="s1">'data'</span><span class="p">)</span>
    <span class="n">custom_positions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">3.2</span><span class="p">]</span>
    <span class="n">custom_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'I1'</span><span class="p">,</span><span class="s1">'I2'</span><span class="p">,</span><span class="s1">'I3'</span><span class="p">,</span><span class="s1">'Y'</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ymat</span><span class="p">[</span><span class="n">idata</span><span class="p">],</span><span class="mi">2</span><span class="p">),[</span><span class="n">Xmat</span><span class="p">[</span><span class="n">idata</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mf">3.25</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>  
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">output_mat</span><span class="p">[</span><span class="n">idata</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span><span class="n">x</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="o">+</span><span class="mi">2</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">output_mat</span><span class="p">[</span><span class="n">idata</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span><span class="n">x</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="o">+</span><span class="mi">2</span><span class="p">)),</span><span class="n">label</span><span class="o">=</span><span class="s1">'reconstruction'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">custom_positions</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">custom_labels</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'index'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'z'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Synthetic 1D Training Data #'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">idata</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">4.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fe7102239237e5dec48032be77d18524b61a306c2238811815f52c81fcbd5955.png" src="../Images/7203aaa35d3b4fe06560d8885fa0bc78.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/fe7102239237e5dec48032be77d18524b61a306c2238811815f52c81fcbd5955.png"/>
</div>
</div>
<p>The training data reconstruction is quite good!</p>
<ul class="simple">
<li><p>our autoencoder has learned to encode and decode the training data</p></li>
<li><p>demonstrating good dimensionality reduction from 3 to 1!</p></li>
</ul>
&#13;

<h2>Check Testing Data Reconstruction</h2>
<p>Let‚Äôs generate additional data and test the reconstruction.</p>
<ul class="simple">
<li><p>check the performance of our training autoencoder with data not used to train the autoencoder, known as model generalization</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span><span class="o">+</span><span class="mi">7</span><span class="p">)</span>
<span class="n">nbatch_test</span> <span class="o">=</span> <span class="mi">12</span><span class="p">;</span> <span class="n">nnodes</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">ymat_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nbatch</span><span class="p">);</span> <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">nnodes</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="n">Xmat_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">nbatch</span><span class="p">,</span><span class="n">nnodes</span><span class="p">])</span>
<span class="n">data_test</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ibatch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nbatch</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">)</span>
    <span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">2.0</span><span class="p">)</span><span class="o">*</span><span class="n">m</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">nnodes</span><span class="p">)</span>
    <span class="n">ymat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">data_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="n">rank</span> <span class="o">=</span> <span class="n">rankdata</span><span class="p">(</span><span class="n">Xmat_test</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ibatch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nbatch_test</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="n">x</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">ibatch</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="n">x</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">ibatch</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">custom_positions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">3.2</span><span class="p">]</span>
    <span class="n">custom_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'I1'</span><span class="p">,</span><span class="s1">'I2'</span><span class="p">,</span><span class="s1">'I3'</span><span class="p">,</span><span class="s1">'Y'</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">rank</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ymat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="mi">2</span><span class="p">),[</span><span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mf">3.18</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ymat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">],</span><span class="mi">2</span><span class="p">),[</span><span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mf">3.25</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">ibatch</span><span class="o">+</span><span class="mi">13</span><span class="p">,[</span><span class="n">Xmat_test</span><span class="p">[</span><span class="n">ibatch</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="mf">0.9</span><span class="p">],</span><span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">custom_positions</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">custom_labels</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.4</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Input Nodes'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'z'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Synthetic 1D Data and Labels'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Test Data Index: '</span><span class="p">,[</span><span class="o">-</span><span class="mf">1.45</span><span class="p">,</span><span class="mf">0.9</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/422b0a32f31f3498764ada7f7ec63e50a53a411b5d12e6c718a4288f371d62e8.png" src="../Images/28e4aff8d55696fd326194c3a79007d1.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/422b0a32f31f3498764ada7f7ec63e50a53a411b5d12e6c718a4288f371d62e8.png"/>
</div>
</div>
<p>Apply trained autoencoder to reconstruct test data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">output_vec_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data_test</span><span class="p">),</span><span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idata_test</span><span class="p">,</span><span class="n">input_vec_test</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_test</span><span class="p">):</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">input_vec_test</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span>                                                    <span class="c1"># forward pass</span>
    <span class="n">output_vec_test</span><span class="p">[</span><span class="n">idata_test</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">activations</span><span class="p">[</span><span class="s1">'O9'</span><span class="p">]],</span> <span class="p">[</span><span class="n">activations</span><span class="p">[</span><span class="s1">'O10'</span><span class="p">]],</span> <span class="p">[</span><span class="n">activations</span><span class="p">[</span><span class="s1">'O11'</span><span class="p">]]])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now visualizated the test data reconstructions,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">for</span> <span class="n">idata</span><span class="p">,</span><span class="n">input_vec_test</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_test</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">idata</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">input_vec_test</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_vec_test</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">label</span><span class="o">=</span><span class="s1">'data'</span><span class="p">)</span>
    <span class="n">custom_positions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">3.2</span><span class="p">]</span>
    <span class="n">custom_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'I1'</span><span class="p">,</span><span class="s1">'I2'</span><span class="p">,</span><span class="s1">'I3'</span><span class="p">,</span><span class="s1">'Y'</span><span class="p">]</span>
    <span class="c1"># plt.annotate(np.round(ymat[idata],2),[Xmat[idata][-1],3.25],size=8,color='black',ha='center')  </span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">output_vec_test</span><span class="p">[</span><span class="n">idata</span><span class="p">,:],</span><span class="n">x</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">output_vec_test</span><span class="p">[</span><span class="n">idata</span><span class="p">,:],</span><span class="n">x</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idata</span><span class="o">/</span><span class="p">(</span><span class="n">nbatch</span><span class="p">)),</span><span class="n">label</span><span class="o">=</span><span class="s1">'reconstruction'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">custom_positions</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">custom_labels</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'index'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'z'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Synthetic 1D Test Image #'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">idata</span><span class="o">+</span><span class="mi">13</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">4.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6d4114b87e54f6dcb8a4cbefcdd99015b46e78ceca3a5d93daa4ff900d2cdf08.png" src="../Images/abda6b15ea724a35119dc1c5cf9554e9.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/6d4114b87e54f6dcb8a4cbefcdd99015b46e78ceca3a5d93daa4ff900d2cdf08.png"/>
</div>
</div>
<p>Our trained autoencoder seems to have generalized well with very good performance reconstructing training and also the withheld testing cases.</p>
<ul class="simple">
<li><p>For a more complete workflow we would evaluate training and testing error in parallel over training epochs to check for model overfit.</p></li>
<li><p>I separated these components for brevity and clarity in the demonstration</p></li>
</ul>
&#13;

<h2>Comments</h2>
<p>This was a basic treatment of autoencoder deep learning networks. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a> and the YouTube lecture links at the start of this chapter with resource links in the videos‚Äô descriptions.</p>
<p>I hope this is helpful,</p>
<p><em>Michael</em></p>
&#13;

<h2>About the Author</h2>
<figure style="text-align: center;">
  <img src="../Images/eb709b2c0a0c715da01ae0165efdf3b2.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/intro/michael_pyrcz_officeshot_jacket.jpg"/>
  <figcaption style="text-align: center;"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.
</figcaption>
</figure>
<p>Michael Pyrcz is a professor in the <a class="reference external" href="https://cockrell.utexas.edu/faculty-directory/alphabetical/p">Cockrell School of Engineering</a>, and the <a class="reference external" href="https://www.jsg.utexas.edu/researcher/michael_pyrcz/">Jackson School of Geosciences</a>, at <a class="reference external" href="https://www.utexas.edu/">The University of Texas at Austin</a>, where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,</p>
<ul class="simple">
<li><p>the principal investigator of the <a class="reference external" href="https://fri.cns.utexas.edu/energy-analytics">Energy Analytics</a> freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin</p></li>
<li><p>an associate editor for <a class="reference external" href="https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board">Computers and Geosciences</a>, and a board member for <a class="reference external" href="https://link.springer.com/journal/11004/editorial-board">Mathematical Geosciences</a>, the International Association for Mathematical Geosciences.</p></li>
</ul>
<p>Michael has written over 70 <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en">peer-reviewed publications</a>, a <a class="reference external" href="https://pypi.org/project/geostatspy/">Python package</a> for spatial data analytics, co-authored a textbook on spatial data analytics, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a> and author of two recently released e-books, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy</a> and <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html">Applied Machine Learning in Python: a Hands-on Guide with Code</a>.</p>
<p>All of Michael‚Äôs university lectures are available on his <a class="reference external" href="https://www.youtube.com/@GeostatsGuyLectures">YouTube Channel</a> with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his <a class="reference external" href="https://github.com/GeostatsGuy">GitHub account</a>, to support any interested students and working professionals with evergreen content. To find out more about Michael‚Äôs work and shared educational resources visit his <span class="xref myst">Website</span>.</p>
&#13;

<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>I‚Äôm always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
    
</body>
</html>