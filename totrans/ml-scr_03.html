<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Conventions and Notation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Conventions and Notation</h1>
<blockquote>原文：<a href="https://dafriedman97.github.io/mlbook/content/conventions_notation.html">https://dafriedman97.github.io/mlbook/content/conventions_notation.html</a></blockquote>

<p>The following terminology will be used throughout the book.</p>
<ul class="simple">
<li><p>Variables can be split into two types: the variables we intend to model are referred to as <strong>target</strong> or <strong>output</strong> variables, while the variables we use to model the target variables are referred to as <strong>predictors</strong>, <strong>features</strong>, or <strong>input</strong> variables. These are also known as the <em>dependent</em> and <em>independent</em> variables, respectively.</p></li>
<li><p>An <strong>observation</strong> is a single collection of predictors and target variables. Multiple observations with the same variables are combined to form a <strong>dataset</strong>.</p></li>
<li><p>A <strong>training</strong> dataset is one used to build a machine learning model. A <strong>validation</strong> dataset is one used to compare multiple models built on the same training dataset with different parameters. A <strong>testing</strong> dataset is one used to evaluate a final model.</p></li>
<li><p>Variables, whether predictors or targets, may be <strong>quantitative</strong> or <strong>categorical</strong>. Quantitative variables follow a continuous or near-continuous scale (such as height in inches or income in dollars). Categorical variables fall in one of a discrete set of groups (such as nation of birth or species type). While the values of categorical variables may follow some natural order (such as shirt size), this is not assumed.</p></li>
<li><p>Modeling tasks are referred to as <strong>regression</strong> if the target is quantitative and <strong>classification</strong> if the target is categorical. Note that regression does not necessarily refer to ordinary least squares (OLS) linear regression.</p></li>
</ul>
<p>Unless indicated otherwise, the following conventions are used to represent data and datasets.</p>
<ul>
<li><p>Training datasets are assumed to have <span class="math notranslate nohighlight">\(N\)</span> observations and <span class="math notranslate nohighlight">\(D\)</span> predictors.</p></li>
<li><p>The vector of features for the <span class="math notranslate nohighlight">\(n^\text{th}\)</span> observation is given by <span class="math notranslate nohighlight">\(\bx_n\)</span>. Note that <span class="math notranslate nohighlight">\(\bx_n\)</span> might include functions of the original predictors through feature engineering. When the target variable is single-dimensional (i.e. there is only one target variable per observation), it is given by <span class="math notranslate nohighlight">\(y_n\)</span>; when there are multiple target variables per observation, the vector of targets is given by <span class="math notranslate nohighlight">\(\by_n\)</span>.</p></li>
<li><p>The entire collection of input and output data is often represented with <span class="math notranslate nohighlight">\(\{\bx_n, y_n\}_{n = 1}^N\)</span>, which implies observation <span class="math notranslate nohighlight">\(n\)</span> has a multi-dimensional predictor vector <span class="math notranslate nohighlight">\(\bx_n\)</span> and a target variable <span class="math notranslate nohighlight">\(y_n\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>.</p></li>
<li><p>Many models, such as ordinary linear regression, append an intercept term to the predictor vector. When this is the case, <span class="math notranslate nohighlight">\(\bx_n\)</span> will be defined as</p>
<div class="math notranslate nohighlight">
\[
  \bx_n = \begin{pmatrix} 1 &amp; x_{n1} &amp; x_{n2} &amp; ... &amp; x_{nD} \end{pmatrix}.
  \]</div>
</li>
<li><p><em>Feature matrices</em> or <em>data frames</em> are created by concatenating feature vectors across observations. Within a matrix, feature vectors are row vectors, with <span class="math notranslate nohighlight">\(\bx_n\)</span> representing the matrix’s <span class="math notranslate nohighlight">\(n^\text{th}\)</span> row. These matrices are then given by <span class="math notranslate nohighlight">\(\bX\)</span>. If a leading 1 is appended to each <span class="math notranslate nohighlight">\(\bx_n\)</span>, the first column of the corresponding feature matrix <span class="math notranslate nohighlight">\(\bX\)</span> will consist of only 1s.</p></li>
</ul>
<p>Finally, the following mathematical and notational conventions are used.</p>
<ul class="simple">
<li><p>Scalar values will be non-boldface and lowercase, random variables will be non-boldface and uppercase, vectors will be bold and lowercase, and matrices will be bold and uppercase. E.g. <span class="math notranslate nohighlight">\(b\)</span> is a scalar, <span class="math notranslate nohighlight">\(B\)</span> a random variable, <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> a vector, and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> a matrix.</p></li>
<li><p>Unless indicated otherwise, all vectors are assumed to be column vectors. Since feature vectors (such as <span class="math notranslate nohighlight">\(\bx_n\)</span> and <span class="math notranslate nohighlight">\(\bphi_n\)</span> above) are entered into data frames as rows, they will sometimes be treated as row vectors, even outside of data frames.</p></li>
<li><p>Matrix or vector derivatives, covered in the <a class="reference internal" href="appendix/math.html"><span class="doc">math appendix</span></a>, will use the numerator <a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions">layout convention</a>. Let <span class="math notranslate nohighlight">\(\by \in \R^a\)</span> and <span class="math notranslate nohighlight">\(\bx \in \R^b\)</span>; under this convention, the derivative <span class="math notranslate nohighlight">\(\partial\by/\partial\bx\)</span> is written as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
  \dadb{\by}{\bx} = \begin{pmatrix}
  \dadb{y_1}{x_1} &amp; ... &amp; \dadb{y_1}{x_b} \\
  \dadb{y_2}{x_1} &amp;  ... &amp; \dadb{y_2}{x_b} \\
  &amp;  ... &amp; \\
  \dadb{y_a}{x_1} &amp; ... &amp; \dadb{y_a}{x_b} \\
  \end{pmatrix}.
\end{split}\]</div>
<ul class="simple">
<li><p>The likelihood of a parameter <span class="math notranslate nohighlight">\(\theta\)</span> given data <span class="math notranslate nohighlight">\(\{x_n\}_{n = 1}^N\)</span> is represented by <span class="math notranslate nohighlight">\(\mathcal{L}\left(\theta; \{x_n\}_{n = 1}^N\right)\)</span>. If we are considering the data to be random (i.e. not yet observed), it will be written as <span class="math notranslate nohighlight">\(\{X_n\}_{n = 1}^N\)</span>. If the data in consideration is obvious, we may write the likelihood as just <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>.</p></li>
</ul>
    
</body>
</html>