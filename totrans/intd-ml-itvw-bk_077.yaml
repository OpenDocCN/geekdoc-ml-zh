- en: 5.1.4 Calculus and convex optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/5.1.4-calculus-and-convex-optimization.html](https://huyenchip.com/ml-interviews-book/contents/5.1.4-calculus-and-convex-optimization.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*If some characters seem to be missing, it''s because MathJax is not loaded
    correctly. Refreshing the page should fix it.*'
  prefs: []
  type: TYPE_NORMAL
- en: Differentiable functions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What does it mean when a function is differentiable?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Give an example of when a function doesn’t have a derivative at a point.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Give an example of non-differentiable functions that are frequently used
    in machine learning. How do we do backpropagation if those functions aren’t differentiable?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Convexity
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What does it mean for a function to be convex or concave? Draw it.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Why is convexity desirable in an optimization problem?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Show that the cross-entropy loss function is convex.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Given a logistic discriminant classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'where the sigmoid function is given by:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The logistic loss for a training sample  with class label  is given by:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Show that .
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that .
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that  is convex.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Most ML algorithms we use nowadays use first-order derivatives (gradients) to
    construct the next training iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] How can we use second-order derivatives for training models?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Pros and cons of second-order optimization.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Why don’t we see more second-order optimization in practice?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] How can we use the Hessian (second derivative matrix) to test for critical
    points?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Jensen’s inequality forms the basis for many algorithms for probabilistic
    inference, including Expectation-Maximization and variational inference.. Explain
    what Jensen’s inequality is.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Explain the chain rule.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Let ,  in which  is a one-hot vector. Take the derivative of  with respect
    to .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Given the function  with the constraint . Find the function’s maximum and
    minimum values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On convex optimization
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Convex optimization is important because it's the only type of optimization
    that we more or less understand. Some might argue that since many of the common
    objective functions in deep learning aren't convex, we don't need to know about
    convex optimization. However, even when the functions aren't convex, analyzing
    them as if they were convex often gives us meaningful bounds. If an algorithm
    doesn't work assuming that a loss function is convex, it definitely doesn't work
    when the loss function is non-convex.
  prefs: []
  type: TYPE_NORMAL
- en: Convexity is the exception, not the rule. If you're asked whether a function
    is convex and it isn't already in the list of commonly known convex functions,
    there's a good chance that it isn't convex. If you want to learn about convex
    optimization, check out [Stephen Boyd's textbook](http://cs229.stanford.edu/section/cs229-cvxopt.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On Hessian matrix
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Hessian matrix or Hessian is a square matrix of second-order partial derivatives
    of a scalar-valued function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a function . If all second partial derivatives of f exist and are continuous
    over the domain of the function, then the Hessian matrix H of f is a square nn
    matrix such that: .'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hessian matrix](../Images/b8a01737fcf984b213c641a1a67c9bde.png "image_tooltip")'
  prefs: []
  type: TYPE_IMG
- en: The Hessian is used for large-scale optimization problems within Newton-type
    methods and quasi-Newton methods. It is also commonly used for expressing image
    processing operators in image processing and computer vision for tasks such as
    blob detection and multi-scale signal representation.
  prefs: []
  type: TYPE_NORMAL
