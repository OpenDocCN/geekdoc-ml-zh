- en: 5.1.4 Calculus and convex optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.1.4 微积分与凸优化
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/5.1.4-calculus-and-convex-optimization.html](https://huyenchip.com/ml-interviews-book/contents/5.1.4-calculus-and-convex-optimization.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huyenchip.com/ml-interviews-book/contents/5.1.4-calculus-and-convex-optimization.html](https://huyenchip.com/ml-interviews-book/contents/5.1.4-calculus-and-convex-optimization.html)
- en: '*If some characters seem to be missing, it''s because MathJax is not loaded
    correctly. Refreshing the page should fix it.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某些字符似乎缺失，那是因为MathJax没有正确加载。刷新页面应该可以解决这个问题。
- en: Differentiable functions
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可导函数
- en: '[E] What does it mean when a function is differentiable?'
  id: totrans-4
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 函数可导意味着什么？'
- en: '[E] Give an example of when a function doesn’t have a derivative at a point.'
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 给出一个函数在某点没有导数的例子。'
- en: '[M] Give an example of non-differentiable functions that are frequently used
    in machine learning. How do we do backpropagation if those functions aren’t differentiable?'
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 给出一个在机器学习中经常使用的非可导函数的例子。如果这些函数不可导，我们如何进行反向传播？'
- en: Convexity
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 凸性
- en: '[E] What does it mean for a function to be convex or concave? Draw it.'
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 函数凸或凹意味着什么？画出来。'
- en: '[E] Why is convexity desirable in an optimization problem?'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 为什么凸性在优化问题中是可取的？'
- en: '[M] Show that the cross-entropy loss function is convex.'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 证明交叉熵损失函数是凸的。'
- en: 'Given a logistic discriminant classifier:'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个逻辑判别分类器：
- en: 'where the sigmoid function is given by:'
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中sigmoid函数由以下公式给出：
- en: 'The logistic loss for a training sample  with class label  is given by:'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于具有类别标签  的训练样本，逻辑损失由以下公式给出：
- en: Show that .
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 证明 。
- en: Show that .
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 证明 。
- en: Show that  is convex.
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 证明  是凸的。
- en: Most ML algorithms we use nowadays use first-order derivatives (gradients) to
    construct the next training iteration.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在使用的多数机器学习算法都使用一阶导数（梯度）来构建下一个训练迭代。
- en: '[E] How can we use second-order derivatives for training models?'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 我们如何使用二阶导数进行模型训练？'
- en: '[M] Pros and cons of second-order optimization.'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 二阶优化优缺点。'
- en: '[M] Why don’t we see more second-order optimization in practice?'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 为什么我们在实践中看不到更多的二阶优化？'
- en: '[M] How can we use the Hessian (second derivative matrix) to test for critical
    points?'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 我们如何使用Hessian（二阶导数矩阵）来测试临界点？'
- en: '[E] Jensen’s inequality forms the basis for many algorithms for probabilistic
    inference, including Expectation-Maximization and variational inference.. Explain
    what Jensen’s inequality is.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] Jensen不等式是许多概率推理算法的基础，包括期望最大化（Expectation-Maximization）和变分推理（variational
    inference）。解释Jensen不等式是什么。'
- en: '[E] Explain the chain rule.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 解释链式法则。'
- en: '[M] Let ,  in which  is a one-hot vector. Take the derivative of  with respect
    to .'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 设 ，其中  是一个one-hot向量。求  关于  的导数。'
- en: '[M] Given the function  with the constraint . Find the function’s maximum and
    minimum values.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 给定函数  和约束条件 。求函数的最大值和最小值。'
- en: '* * *'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On convex optimization
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于凸优化
- en: Convex optimization is important because it's the only type of optimization
    that we more or less understand. Some might argue that since many of the common
    objective functions in deep learning aren't convex, we don't need to know about
    convex optimization. However, even when the functions aren't convex, analyzing
    them as if they were convex often gives us meaningful bounds. If an algorithm
    doesn't work assuming that a loss function is convex, it definitely doesn't work
    when the loss function is non-convex.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 凸优化很重要，因为它是我们或多或少理解的唯一一种优化类型。有些人可能会争论，由于深度学习中许多常见的目标函数不是凸函数，所以我们不需要了解凸优化。然而，即使函数不是凸函数，将它们视为凸函数进行分析通常也会给我们提供有意义的界限。如果一个算法假设损失函数是凸的而无法工作，那么当损失函数是非凸的时，它肯定也无法工作。
- en: Convexity is the exception, not the rule. If you're asked whether a function
    is convex and it isn't already in the list of commonly known convex functions,
    there's a good chance that it isn't convex. If you want to learn about convex
    optimization, check out [Stephen Boyd's textbook](http://cs229.stanford.edu/section/cs229-cvxopt.pdf).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 凸性是例外，而不是规则。如果你被问到一个函数是否是凸的，而它还没有在已知凸函数的列表中，那么它很可能不是凸的。如果你想了解凸优化，可以查看[Stephen
    Boyd的教科书](http://cs229.stanford.edu/section/cs229-cvxopt.pdf)。
- en: '* * *'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On Hessian matrix
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于Hessian矩阵
- en: The Hessian matrix or Hessian is a square matrix of second-order partial derivatives
    of a scalar-valued function.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian矩阵或Hessian是一个标量值函数的二阶偏导数的方阵。
- en: 'Given a function . If all second partial derivatives of f exist and are continuous
    over the domain of the function, then the Hessian matrix H of f is a square nn
    matrix such that: .'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个函数 . 如果函数的所有二阶偏导数在该函数的定义域内存在且连续，那么函数 f 的 Hessian 矩阵 H 是一个 n×n 的方阵，满足以下条件：.
- en: '![Hessian matrix](../Images/b8a01737fcf984b213c641a1a67c9bde.png "image_tooltip")'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Hessian 矩阵](../Images/b8a01737fcf984b213c641a1a67c9bde.png "image_tooltip")'
- en: The Hessian is used for large-scale optimization problems within Newton-type
    methods and quasi-Newton methods. It is also commonly used for expressing image
    processing operators in image processing and computer vision for tasks such as
    blob detection and multi-scale signal representation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian 矩阵在 Newton 类方法和准 Newton 方法中用于大规模优化问题。它也常用于在图像处理和计算机视觉中表达图像处理算子，例如用于目标检测和多尺度信号表示。
