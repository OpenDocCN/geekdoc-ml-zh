- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://dafriedman97.github.io/mlbook/content/c6/code.html](https://dafriedman97.github.io/mlbook/content/c6/code.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This section demonstrates how to fit bagging, random forest, and boosting models
    using `scikit-learn`. We will again use the [penguins](../appendix/data.html)
    dataset for classification and the [tips](../appendix/data.html) dataset for regression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 1\. Bagging and Random Forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that bagging and random forests can handle both classification and regression
    tasks. For this example we will do classification on the `penguins` dataset. Recall
    that `scikit-learn` trees do not currently support categorical predictors, so
    we must first convert those to dummy variables
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Bagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A simple bagging classifier is fit below. The most important arguments are `n_estimators`
    and `base_estimator`, which determine the number and type of weak learners the
    bagging model should use. The default `base_estimator` is a decision tree, though
    this can be changed as in the second example below, which uses Naive Bayes estimators.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Random Forests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An example of a random forest in `scikit-learn` is given below. The most important
    arguments to the random forest are the number of estimators (decision trees),
    `max_features` (the number of predictors to consider at each split), and any chosen
    parameters for the decision trees (such as the maximum depth). Guidelines for
    setting each of these parameters are given below.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: In general, the more base estimators the better, though there
    are diminishing marginal returns. While increasing the number of base estimators
    does not risk overfitting, it eventually provides no benefit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: This argument is set by default to the square root of the number
    of total features (which is made explicit in the example below). If this value
    equals the number of total features, we are left with a bagging model. Lowering
    this value lowers the amount of correlation between trees but also prevents the
    base estimators from learning potentially valuable information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decision tree parameters: These parameters are generally left untouched. This
    allows the individual decision trees to grow deep, increasing variance but decreasing
    bias. The variance is then decreased by the ensemble of individual trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the `AdaBoostClassifier` from `scikit-learn` uses a slightly different
    algorithm than the one introduced in the [concept section](s1/boosting.html) though
    results should be similar. The `AdaBoostRegressor` class in `scikit-learn` uses
    the same algorithm we introduced: *AdaBoost.R2*'
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `AdaBoostClassifier` in `scikit-learn` is actually able to handle multiclass
    target variables, but for consistency, let’s use the same binary target we did
    in our AdaBoost construction: whether the penguin’s species is *Adelie*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can then fit the classifier with the `AdaBoostClassifier` class as below.
    Again, we first convert categorical predictors to dummy variables. The classifier
    will by default use 50 decision trees, each with a max depth of 1, for the weak
    learners.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: A different weak learner can easily be used in place of a decision tree. The
    below shows an example using logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: AdaBoost Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AdaBoost regression is implemented almost identically in `scikit-learn`. An
    example with the `tips` dataset is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/code_24_0.png](../Images/ef429046407aca3429fd95a23b95bab9.png)'
  prefs: []
  type: TYPE_IMG
- en: 1\. Bagging and Random Forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that bagging and random forests can handle both classification and regression
    tasks. For this example we will do classification on the `penguins` dataset. Recall
    that `scikit-learn` trees do not currently support categorical predictors, so
    we must first convert those to dummy variables
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Bagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A simple bagging classifier is fit below. The most important arguments are `n_estimators`
    and `base_estimator`, which determine the number and type of weak learners the
    bagging model should use. The default `base_estimator` is a decision tree, though
    this can be changed as in the second example below, which uses Naive Bayes estimators.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Random Forests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An example of a random forest in `scikit-learn` is given below. The most important
    arguments to the random forest are the number of estimators (decision trees),
    `max_features` (the number of predictors to consider at each split), and any chosen
    parameters for the decision trees (such as the maximum depth). Guidelines for
    setting each of these parameters are given below.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: In general, the more base estimators the better, though there
    are diminishing marginal returns. While increasing the number of base estimators
    does not risk overfitting, it eventually provides no benefit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: This argument is set by default to the square root of the number
    of total features (which is made explicit in the example below). If this value
    equals the number of total features, we are left with a bagging model. Lowering
    this value lowers the amount of correlation between trees but also prevents the
    base estimators from learning potentially valuable information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decision tree parameters: These parameters are generally left untouched. This
    allows the individual decision trees to grow deep, increasing variance but decreasing
    bias. The variance is then decreased by the ensemble of individual trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Bagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A simple bagging classifier is fit below. The most important arguments are `n_estimators`
    and `base_estimator`, which determine the number and type of weak learners the
    bagging model should use. The default `base_estimator` is a decision tree, though
    this can be changed as in the second example below, which uses Naive Bayes estimators.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Random Forests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An example of a random forest in `scikit-learn` is given below. The most important
    arguments to the random forest are the number of estimators (decision trees),
    `max_features` (the number of predictors to consider at each split), and any chosen
    parameters for the decision trees (such as the maximum depth). Guidelines for
    setting each of these parameters are given below.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: In general, the more base estimators the better, though there
    are diminishing marginal returns. While increasing the number of base estimators
    does not risk overfitting, it eventually provides no benefit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: This argument is set by default to the square root of the number
    of total features (which is made explicit in the example below). If this value
    equals the number of total features, we are left with a bagging model. Lowering
    this value lowers the amount of correlation between trees but also prevents the
    base estimators from learning potentially valuable information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decision tree parameters: These parameters are generally left untouched. This
    allows the individual decision trees to grow deep, increasing variance but decreasing
    bias. The variance is then decreased by the ensemble of individual trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the `AdaBoostClassifier` from `scikit-learn` uses a slightly different
    algorithm than the one introduced in the [concept section](s1/boosting.html) though
    results should be similar. The `AdaBoostRegressor` class in `scikit-learn` uses
    the same algorithm we introduced: *AdaBoost.R2*'
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `AdaBoostClassifier` in `scikit-learn` is actually able to handle multiclass
    target variables, but for consistency, let’s use the same binary target we did
    in our AdaBoost construction: whether the penguin’s species is *Adelie*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We can then fit the classifier with the `AdaBoostClassifier` class as below.
    Again, we first convert categorical predictors to dummy variables. The classifier
    will by default use 50 decision trees, each with a max depth of 1, for the weak
    learners.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: A different weak learner can easily be used in place of a decision tree. The
    below shows an example using logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: AdaBoost Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AdaBoost regression is implemented almost identically in `scikit-learn`. An
    example with the `tips` dataset is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/code_24_0.png](../Images/ef429046407aca3429fd95a23b95bab9.png)'
  prefs: []
  type: TYPE_IMG
- en: AdaBoost Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `AdaBoostClassifier` in `scikit-learn` is actually able to handle multiclass
    target variables, but for consistency, let’s use the same binary target we did
    in our AdaBoost construction: whether the penguin’s species is *Adelie*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We can then fit the classifier with the `AdaBoostClassifier` class as below.
    Again, we first convert categorical predictors to dummy variables. The classifier
    will by default use 50 decision trees, each with a max depth of 1, for the weak
    learners.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: A different weak learner can easily be used in place of a decision tree. The
    below shows an example using logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: AdaBoost Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AdaBoost regression is implemented almost identically in `scikit-learn`. An
    example with the `tips` dataset is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/code_24_0.png](../Images/ef429046407aca3429fd95a23b95bab9.png)'
  prefs: []
  type: TYPE_IMG
