["```py\nignore_warnings = True                                        # ignore warnings?\nimport os                                                     # to set current working directory \nimport pandas as pd                                           # DataFrames and plotting\nimport numpy as np                                            # arrays and matrix math\nimport matplotlib.pyplot as plt                               # plotting\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import train_test_split          # random train and test data split\nfrom sklearn import tree                                      # tree program from scikit learn (package for machine learning)\nplt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\nif ignore_warnings == True:                                   \n    import warnings\n    warnings.filterwarnings('ignore')\ncmap = plt.cm.inferno                                         # color map\nseed = 42                                                     # random number seed \n```", "```py\nnp.random.seed(seed)                                        # scikit-learn uses the NumPy seed \n```", "```py\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n```", "```py\n# not needed currently\n#os.chdir(\"c:/PGE383\")                                       # set the working directory \n```", "```py\nmy_data = pd.read_csv(\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/1D_Porosity.csv\") # load the comma delimited data file\n#my_data = pd.read_csv(\"1D_Porosity.csv\")                     # load the comma delimited data file locally\nX = pd.DataFrame(data = my_data.loc[:,'Depth'])               # ensure X and y features are DataFrames\ny = pd.DataFrame(data = my_data.loc[:,'Nporosity'])\nX.head(n=2); y.head(n=2)                                      # preview the predictor feature\nprint('Loaded ' + str(len(my_data)) + ' samples, with features = ' + str(my_data.columns.values) + '.') \n```", "```py\nLoaded 40 samples, with features = ['Depth' 'Nporosity']. \n```", "```py\nplt.scatter(X,y,color='red',s=30,edgecolor='black',alpha=1.0); plt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity'); plt.title('All Data'); \nplt.xlim([0,10.25]); plt.ylim([-2.5,2.5]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); \n```", "```py\nerror_stdev = 0.0                                           # standard deviation of additional random error \n```", "```py\nerror_stdev = 0.0                                             # standard deviation of additional random error\ny_orig = y.copy(deep = True)                                  # make a deep copy of original dataset\ny['Nporosity'] = y['Nporosity'] + np.random.normal(loc = 0, scale = error_stdev, size = y.shape[0])\nplt.scatter(X,y,color='blue',s=30,edgecolor='black',alpha=0.2,label='with added noise'); plt.xlabel('Depth (m)'); \nplt.ylabel('Standardized Porosity'); plt.title('All Data, Original and with Added Noise'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5])\nplt.scatter(X,y_orig,color='red',s=30,edgecolor='black',alpha=0.2,label='original data'); plt.xlabel('Depth (m)'); \nplt.ylabel('Standardized Porosity'); plt.legend(loc='upper left')\nplt.xlim([0,10.25]); plt.ylim([-2.5,2.5]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=seed) # train and test split\nplt.scatter(X_train,y_train,color='green',s=30,edgecolor='black',alpha=1.0,label='train'); plt.xlabel('Depth (m)') \nplt.ylabel('Standardized Porosity'); plt.title('Train and Test Data'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5]) \nplt.scatter(X_test,y_test,color='white',s=30,edgecolor='black',alpha=1.0,label='test'); plt.xlabel('Depth (m)')\nplt.ylabel('Standardized Porosity');plt.legend(loc='upper right'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); \n```", "```py\nmax_leaf_nodes = 6                                            # set the hyperparameter\nour_tree = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes) # instantiate the model\nour_tree = our_tree.fit(X_train, y_train)                     # fit the model to the trainin data \n```", "```py\nitest = 3; depth = X_test['Depth'].values[itest]            # set the predictor value for our prediction\nspor = our_tree.predict([[depth]])                          # predict with our trained model\nplt.scatter(X_train,y_train,color='green',s=30,edgecolor='black',alpha=1.0,label='train'); plt.xlabel('Depth (m)'); \nplt.ylabel('Standardized Porosity'); plt.title('Train and Test Data and Model Prediction'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5]) \nplt.scatter(X_test,y_test,color='white',s=30,edgecolor='black',alpha=1.0,label='test'); plt.xlabel('Depth (m)')\nplt.ylabel('Standardized Porosity');plt.legend(loc='upper right')\nplt.scatter(depth,spor,color='red',s=30,marker='s',edgecolor='black',alpha=1.0,lw=2,label='predict'); plt.xlabel('Depth (m)') \nplt.ylabel('Standardized Porosity');plt.legend(loc='upper right'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); add_grid() \n```", "```py\ndepths = np.linspace(0,10.25,1000)                          # set the predictor values for our prediction\nspors = our_tree.predict(depths.reshape(-1, 1))             # predict with our trained model\nplt.scatter(X_train,y_train,color='green',s=30,edgecolor='black',alpha=1.0,label='train'); plt.xlabel('Depth (m)') \nplt.ylabel('Standardized Porosity'); plt.title('Train and Test Data and Model Predictions'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5])\nplt.scatter(X_test,y_test,color='white',s=30,edgecolor='black',alpha=1.0,label='test'); plt.xlabel('Depth (m)') \nplt.ylabel('Standardized Porosity');plt.legend(loc='upper right')\nplt.plot(depths,spors,color='red',alpha=1.0,label='predict',linestyle='--',lw=2,zorder=-1); plt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity')\nplt.xlim([0,10.25]); plt.ylim([-2.5,2.5]); add_grid()\nplt.legend(loc='upper right'); plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); \n```", "```py\nmax_leaf_nodes = 10                                         # set the hyperparameter\nour_tree = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes) # instantiate the model\nour_tree = our_tree.fit(X_train, y_train)                   # fit the model to the trainin data\nspors = our_tree.predict(depths.reshape(-1, 1))             # predict with our trained model\nplt.scatter(X_train,y_train,color='green',s=30,edgecolor='black',alpha=1.0,label='train'); \nplt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5]) \nplt.scatter(X_test,y_test,color='white',s=30,edgecolor='black',alpha=1.0,label='test'); plt.xlabel('Depth (m)')\nplt.ylabel('Standardized Porosity');plt.legend(loc='upper right')\nplt.plot(depths,spors,color='red',alpha=1.0,label='predict',linestyle='--',lw=2); plt.xlabel('Depth (m)'); \nplt.ylabel('Standardized Porosity');plt.legend(loc='upper right'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); \n```", "```py\nleaf_nodes = 7                                              # set the hyperparameter\nour_tree = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes) # instantiate the model\nour_tree = our_tree.fit(X_train, y_train)                   # fit the model to the trainin data\ny_predict = our_tree.predict(X_test)                        # predict at the testing data locations\nmse = metrics.mean_squared_error(y_test,y_predict)\nprint('Testing MSE is ' + str(round(mse,2)) + '.') \n```", "```py\nTesting MSE is 0.67. \n```", "```py\nnodes = []; scores = []; max_leaf_nodes = 30\nfor i, nnode in enumerate(np.arange(2,max_leaf_nodes,1)):\n    our_tree = tree.DecisionTreeRegressor(max_leaf_nodes = nnode).fit(X_train, y_train) # instantiate / fit\n    y_predict = our_tree.predict(X_test)                    # predict at the testing data locations\n    nodes.append(nnode); scores.append(metrics.mean_squared_error(y_test,y_predict))\nplt.scatter(nodes,scores,color='red',s=30,edgecolor='black',alpha=1.0,label='train'); \nplt.plot(nodes,scores,color='red',alpha=1.0,linestyle='--',zorder=-1); plt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity') \nplt.xlim([1,max_leaf_nodes]); plt.xlabel('Number of Leaf Nodes'); plt.ylabel('Testing Mean Square Error')\nplt.xlim([0,max_leaf_nodes]); plt.ylim([0.0,1.0]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2) \n```", "```py\nfrom sklearn.model_selection import cross_val_score as kfold       # cross validation methods\nnodes = []; scores = []; max_leaf_nodes = 30\nfor i, nnode in enumerate(np.arange(2,max_leaf_nodes,1,dtype = int)):\n    our_tree = tree.DecisionTreeRegressor(max_leaf_nodes = nnode) # instantiate / fit\n    nodes.append(nnode); \n    scores.append(abs(kfold(estimator=our_tree,X=X,y=y,cv=4,n_jobs=4,scoring = \"neg_mean_squared_error\").mean()))\nplt.scatter(nodes,scores,color='red',s=30,edgecolor='black',alpha=1.0,label='train')\nplt.plot(nodes,scores,color='red',alpha=1.0,linestyle='--'); plt.xlabel('Depth (m)') \nplt.ylabel('Standardized Porosity'); plt.xlim([1,max_leaf_nodes]); plt.xlabel('Number of Leaf Nodes'); plt.ylabel('Testing Mean Square Error')\nplt.xlim([0,max_leaf_nodes]); plt.ylim([0.0,4.0]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2) \n```", "```py\ntuned_nodes = nodes[np.argmin(scores)]\nour_tuned_tree = tree.DecisionTreeRegressor(max_leaf_nodes = tuned_nodes).fit(X, y) # instantiate / fit\ntuned_spors = our_tuned_tree.predict(depths.reshape(-1, 1))             # predict with our trained model\nplt.scatter(X_train,y_train,color='green',s=30,edgecolor='black',alpha=1.0,label='train')\nplt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity')\nplt.title('Train and Test Data and Tuned Model with ' + str(tuned_nodes) + ' Leaf Nodes')\nplt.xlim([0,10.25]); plt.ylim([-2.5,2.5])\nplt.scatter(X_test,y_test,color='white',s=30,edgecolor='black',alpha=1.0,label='test'); \nplt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity');plt.legend(loc='upper right')\nplt.plot(depths,tuned_spors,color='red',alpha=1.0,label='tuned model',linestyle='--',lw=2,zorder=-1); \nplt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity');plt.legend(loc='upper right'); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); \n```", "```py\nfrom sklearn.pipeline import Pipeline                       # machine learning modeling pipeline\npipe = Pipeline([                                           # the machine learning workflow as a pipeline object\n    ('tree', tree.DecisionTreeRegressor())\n])\n\nparams = {                                                  # the machine learning workflow method's parameters\n    'tree__max_leaf_nodes': np.arange(2,max_leaf_nodes,1,dtype = int)\n}\n\nfrom sklearn.model_selection import GridSearchCV            # model hyperparameter grid search\ngrid_cv_tuned = GridSearchCV(pipe, params, scoring = 'neg_mean_squared_error', # grid search cross validation \n                             cv=4,refit = True);\n\ngrid_cv_tuned.fit(X,y);                                      # fit model with tuned hyperparameters\nprint('Tuned maximum number of leaf nodes = ' + str(grid_cv_tuned.best_params_['tree__max_leaf_nodes']) + '.')\ndepth = 2.5\nprint('Prediction at depth = ' + str(depth) + ' m is ' + str(np.round(grid_cv_tuned.predict([[depth]]),2))) # predict with our trained model) + '.') \n```", "```py\nTuned maximum number of leaf nodes = 14.\nPrediction at depth = 2.5 m is [-0.97] \n```", "```py\nignore_warnings = True                                        # ignore warnings?\nimport os                                                     # to set current working directory \nimport pandas as pd                                           # DataFrames and plotting\nimport numpy as np                                            # arrays and matrix math\nimport matplotlib.pyplot as plt                               # plotting\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import train_test_split          # random train and test data split\nfrom sklearn import tree                                      # tree program from scikit learn (package for machine learning)\nplt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\nif ignore_warnings == True:                                   \n    import warnings\n    warnings.filterwarnings('ignore')\ncmap = plt.cm.inferno                                         # color map\nseed = 42                                                     # random number seed \n```", "```py\nnp.random.seed(seed)                                        # scikit-learn uses the NumPy seed \n```", "```py\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n```", "```py\n# not needed currently\n#os.chdir(\"c:/PGE383\")                                       # set the working directory \n```", "```py\nmy_data = pd.read_csv(\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/1D_Porosity.csv\") # load the comma delimited data file\n#my_data = pd.read_csv(\"1D_Porosity.csv\")                     # load the comma delimited data file locally\nX = pd.DataFrame(data = my_data.loc[:,'Depth'])               # ensure X and y features are DataFrames\ny = pd.DataFrame(data = my_data.loc[:,'Nporosity'])\nX.head(n=2); y.head(n=2)                                      # preview the predictor feature\nprint('Loaded ' + str(len(my_data)) + ' samples, with features = ' + str(my_data.columns.values) + '.') \n```", "```py\nLoaded 40 samples, with features = ['Depth' 'Nporosity']. \n```", "```py\nplt.scatter(X,y,color='red',s=30,edgecolor='black',alpha=1.0); plt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity'); plt.title('All Data'); \nplt.xlim([0,10.25]); plt.ylim([-2.5,2.5]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); \n```", "```py\nerror_stdev = 0.0                                           # standard deviation of additional random error \n```", "```py\nerror_stdev = 0.0                                             # standard deviation of additional random error\ny_orig = y.copy(deep = True)                                  # make a deep copy of original dataset\ny['Nporosity'] = y['Nporosity'] + np.random.normal(loc = 0, scale = error_stdev, size = y.shape[0])\nplt.scatter(X,y,color='blue',s=30,edgecolor='black',alpha=0.2,label='with added noise'); plt.xlabel('Depth (m)'); \nplt.ylabel('Standardized Porosity'); plt.title('All Data, Original and with Added Noise'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5])\nplt.scatter(X,y_orig,color='red',s=30,edgecolor='black',alpha=0.2,label='original data'); plt.xlabel('Depth (m)'); \nplt.ylabel('Standardized Porosity'); plt.legend(loc='upper left')\nplt.xlim([0,10.25]); plt.ylim([-2.5,2.5]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=seed) # train and test split\nplt.scatter(X_train,y_train,color='green',s=30,edgecolor='black',alpha=1.0,label='train'); plt.xlabel('Depth (m)') \nplt.ylabel('Standardized Porosity'); plt.title('Train and Test Data'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5]) \nplt.scatter(X_test,y_test,color='white',s=30,edgecolor='black',alpha=1.0,label='test'); plt.xlabel('Depth (m)')\nplt.ylabel('Standardized Porosity');plt.legend(loc='upper right'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); \n```", "```py\nmax_leaf_nodes = 6                                            # set the hyperparameter\nour_tree = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes) # instantiate the model\nour_tree = our_tree.fit(X_train, y_train)                     # fit the model to the trainin data \n```", "```py\nitest = 3; depth = X_test['Depth'].values[itest]            # set the predictor value for our prediction\nspor = our_tree.predict([[depth]])                          # predict with our trained model\nplt.scatter(X_train,y_train,color='green',s=30,edgecolor='black',alpha=1.0,label='train'); plt.xlabel('Depth (m)'); \nplt.ylabel('Standardized Porosity'); plt.title('Train and Test Data and Model Prediction'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5]) \nplt.scatter(X_test,y_test,color='white',s=30,edgecolor='black',alpha=1.0,label='test'); plt.xlabel('Depth (m)')\nplt.ylabel('Standardized Porosity');plt.legend(loc='upper right')\nplt.scatter(depth,spor,color='red',s=30,marker='s',edgecolor='black',alpha=1.0,lw=2,label='predict'); plt.xlabel('Depth (m)') \nplt.ylabel('Standardized Porosity');plt.legend(loc='upper right'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); add_grid() \n```", "```py\ndepths = np.linspace(0,10.25,1000)                          # set the predictor values for our prediction\nspors = our_tree.predict(depths.reshape(-1, 1))             # predict with our trained model\nplt.scatter(X_train,y_train,color='green',s=30,edgecolor='black',alpha=1.0,label='train'); plt.xlabel('Depth (m)') \nplt.ylabel('Standardized Porosity'); plt.title('Train and Test Data and Model Predictions'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5])\nplt.scatter(X_test,y_test,color='white',s=30,edgecolor='black',alpha=1.0,label='test'); plt.xlabel('Depth (m)') \nplt.ylabel('Standardized Porosity');plt.legend(loc='upper right')\nplt.plot(depths,spors,color='red',alpha=1.0,label='predict',linestyle='--',lw=2,zorder=-1); plt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity')\nplt.xlim([0,10.25]); plt.ylim([-2.5,2.5]); add_grid()\nplt.legend(loc='upper right'); plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); \n```", "```py\nmax_leaf_nodes = 10                                         # set the hyperparameter\nour_tree = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes) # instantiate the model\nour_tree = our_tree.fit(X_train, y_train)                   # fit the model to the trainin data\nspors = our_tree.predict(depths.reshape(-1, 1))             # predict with our trained model\nplt.scatter(X_train,y_train,color='green',s=30,edgecolor='black',alpha=1.0,label='train'); \nplt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5]) \nplt.scatter(X_test,y_test,color='white',s=30,edgecolor='black',alpha=1.0,label='test'); plt.xlabel('Depth (m)')\nplt.ylabel('Standardized Porosity');plt.legend(loc='upper right')\nplt.plot(depths,spors,color='red',alpha=1.0,label='predict',linestyle='--',lw=2); plt.xlabel('Depth (m)'); \nplt.ylabel('Standardized Porosity');plt.legend(loc='upper right'); plt.xlim([0,10.25]); plt.ylim([-2.5,2.5]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); \n```", "```py\nleaf_nodes = 7                                              # set the hyperparameter\nour_tree = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes) # instantiate the model\nour_tree = our_tree.fit(X_train, y_train)                   # fit the model to the trainin data\ny_predict = our_tree.predict(X_test)                        # predict at the testing data locations\nmse = metrics.mean_squared_error(y_test,y_predict)\nprint('Testing MSE is ' + str(round(mse,2)) + '.') \n```", "```py\nTesting MSE is 0.67. \n```", "```py\nnodes = []; scores = []; max_leaf_nodes = 30\nfor i, nnode in enumerate(np.arange(2,max_leaf_nodes,1)):\n    our_tree = tree.DecisionTreeRegressor(max_leaf_nodes = nnode).fit(X_train, y_train) # instantiate / fit\n    y_predict = our_tree.predict(X_test)                    # predict at the testing data locations\n    nodes.append(nnode); scores.append(metrics.mean_squared_error(y_test,y_predict))\nplt.scatter(nodes,scores,color='red',s=30,edgecolor='black',alpha=1.0,label='train'); \nplt.plot(nodes,scores,color='red',alpha=1.0,linestyle='--',zorder=-1); plt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity') \nplt.xlim([1,max_leaf_nodes]); plt.xlabel('Number of Leaf Nodes'); plt.ylabel('Testing Mean Square Error')\nplt.xlim([0,max_leaf_nodes]); plt.ylim([0.0,1.0]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2) \n```", "```py\nfrom sklearn.model_selection import cross_val_score as kfold       # cross validation methods\nnodes = []; scores = []; max_leaf_nodes = 30\nfor i, nnode in enumerate(np.arange(2,max_leaf_nodes,1,dtype = int)):\n    our_tree = tree.DecisionTreeRegressor(max_leaf_nodes = nnode) # instantiate / fit\n    nodes.append(nnode); \n    scores.append(abs(kfold(estimator=our_tree,X=X,y=y,cv=4,n_jobs=4,scoring = \"neg_mean_squared_error\").mean()))\nplt.scatter(nodes,scores,color='red',s=30,edgecolor='black',alpha=1.0,label='train')\nplt.plot(nodes,scores,color='red',alpha=1.0,linestyle='--'); plt.xlabel('Depth (m)') \nplt.ylabel('Standardized Porosity'); plt.xlim([1,max_leaf_nodes]); plt.xlabel('Number of Leaf Nodes'); plt.ylabel('Testing Mean Square Error')\nplt.xlim([0,max_leaf_nodes]); plt.ylim([0.0,4.0]); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2) \n```", "```py\ntuned_nodes = nodes[np.argmin(scores)]\nour_tuned_tree = tree.DecisionTreeRegressor(max_leaf_nodes = tuned_nodes).fit(X, y) # instantiate / fit\ntuned_spors = our_tuned_tree.predict(depths.reshape(-1, 1))             # predict with our trained model\nplt.scatter(X_train,y_train,color='green',s=30,edgecolor='black',alpha=1.0,label='train')\nplt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity')\nplt.title('Train and Test Data and Tuned Model with ' + str(tuned_nodes) + ' Leaf Nodes')\nplt.xlim([0,10.25]); plt.ylim([-2.5,2.5])\nplt.scatter(X_test,y_test,color='white',s=30,edgecolor='black',alpha=1.0,label='test'); \nplt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity');plt.legend(loc='upper right')\nplt.plot(depths,tuned_spors,color='red',alpha=1.0,label='tuned model',linestyle='--',lw=2,zorder=-1); \nplt.xlabel('Depth (m)'); plt.ylabel('Standardized Porosity');plt.legend(loc='upper right'); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.7, wspace=0.2, hspace=0.2); \n```", "```py\nfrom sklearn.pipeline import Pipeline                       # machine learning modeling pipeline\npipe = Pipeline([                                           # the machine learning workflow as a pipeline object\n    ('tree', tree.DecisionTreeRegressor())\n])\n\nparams = {                                                  # the machine learning workflow method's parameters\n    'tree__max_leaf_nodes': np.arange(2,max_leaf_nodes,1,dtype = int)\n}\n\nfrom sklearn.model_selection import GridSearchCV            # model hyperparameter grid search\ngrid_cv_tuned = GridSearchCV(pipe, params, scoring = 'neg_mean_squared_error', # grid search cross validation \n                             cv=4,refit = True);\n\ngrid_cv_tuned.fit(X,y);                                      # fit model with tuned hyperparameters\nprint('Tuned maximum number of leaf nodes = ' + str(grid_cv_tuned.best_params_['tree__max_leaf_nodes']) + '.')\ndepth = 2.5\nprint('Prediction at depth = ' + str(depth) + ' m is ' + str(np.round(grid_cv_tuned.predict([[depth]]),2))) # predict with our trained model) + '.') \n```", "```py\nTuned maximum number of leaf nodes = 14.\nPrediction at depth = 2.5 m is [-0.97] \n```"]