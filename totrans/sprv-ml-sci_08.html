<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>4  Justification to Use Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>4  Justification to Use Machine Learning</h1>
<blockquote>原文：<a href="https://ml-science-book.com/justification.html">https://ml-science-book.com/justification.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part-one.html">Justifying Machine Learning For Science</a></li><li class="breadcrumb-item"><a href="./justification.html"><span class="chapter-number">4</span>  <span class="chapter-title">Justification to Use Machine Learning</span></a></li></ol></nav>
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>We have demonstrated that prediction is an essential goal in science. Great! But how does machine learning help scientists make predictions? And, what value does it add to existing predictive modeling approaches? What are good reasons for using machine learning in science?</p>
<p>In this chapter, we justify the use of machine learning in science both epistemically and pragmatically.</p>
<div class="raven-box">
<p>With the blessing of the Elder Council, tornado prediction became the first machine learning project in Raven Science. Rattle teamed up with the tornado expert Krarah. Krarah was responsible for the data, and Rattle for the training and modeling. Within one month, they had created a model that outperformed all previous tornado prediction models. Eureka! A new paradigm was born.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../Images/ffcd19acdb07f5d042720d6d89c2ba2c.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%" data-original-src="https://ml-science-book.com/images/raven-justification.jpg"/></p>
</figure>
</div>
</div>
<section id="machine-learning-has-a-clear-notion-of-success" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="machine-learning-has-a-clear-notion-of-success"><span class="header-section-number">4.1</span> Machine Learning has a clear notion of success</h2>
<p>In science, the quality of models is often assessed in a lengthy and non-transparent manner. Social aspects such as the reputation of the researchers who developed the model, the opinion of other scientific authorities, or the amount of funding for the research program can play as important a role in model assessment as the predictive capacity of the model <span class="citation" data-cites="kuhn1997structure"><a href="references.html#ref-kuhn1997structure" role="doc-biblioref">[1]</a></span>.</p>
<p>In comparison, machine learning offers a very transparent notion to assess model quality: a good model is one that has a low average prediction error on an unseen test dataset. The idea behind this assessment is that a model with a low empirical error on a test set is expected to make few errors on the task to be solved. This inference from the test to the so-called <em>generalization error</em> is even demonstrably correct if the test data is a random and representative data sample of the task.</p>
<p>Furthermore, not only is the goal of modeling very transparent, but the feedback that researchers receive is also quick to obtain and comes in quantified form. In this way, scientists can check their modeling intuitions throughout the modeling process on an almost purely empirical basis and even compare their models with entirely different modeling schools.</p>
<p>This transparency about what makes a good model enables scientists to work towards a common goal. We discuss the concept of generalization in <a href="generalization.html" class="quarto-xref"><span>Chapter 7</span></a>.</p>
</section>
<section id="machine-learning-adapts-the-model-to-the-world" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="machine-learning-adapts-the-model-to-the-world"><span class="header-section-number">4.2</span> Machine learning adapts the model to the world</h2>
<p>Most approaches where mathematical models are fit to data, for example, statistical models or simulations, come with a constrained functional form. Like assuming a linear, exponential, inverse, or additive relationship between variables. Such constraints usually reflect the domain knowledge of the researcher. For example, if you know that a certain fertilizer is less effective in dry climates, you may add an interaction term.</p>
<p>While incorporating domain knowledge sounds smart, it can be dangerous: Your modeling success will depend on the reliability of your domain knowledge and your skill in encoding it. A researcher who only knows linear models will assume linearity in all contexts, whether it is a conscious choice or not. Misspecified models will lead to wrong conclusions and therefore to bad science <span class="citation" data-cites="dennis2019errors"><a href="references.html#ref-dennis2019errors" role="doc-biblioref">[2]</a></span>.</p>
<p>In machine learning, the model classes – like neural networks or random forests – are more flexible and powerful. They can, in principle, approximate any function.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> You don’t need to know that there is an interaction between humidity and fertilizer effectiveness to build a good model. The algorithm will learn this interaction from the data. There is a shift in burden, the modeling success rests not on your domain knowledge anymore but on the size and quality of your data. Equipped with enough data, even non-domain experts can find highly predictive models. Indeed, there is a trade-off: with lots of data, domain knowledge is secondary; with little data, on the other hand, domain knowledge is key.</p>
<p>We will discuss the role of domain knowledge and how to incorporate it in machine learning modeling in <a href="domain.html" class="quarto-xref"><span>Chapter 8</span></a>.</p>
</section>
<section id="machine-learning-handles-various-data-structures" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="machine-learning-handles-various-data-structures"><span class="header-section-number">4.3</span> Machine learning handles various data structures</h2>
<p>The more complex the data structures become, the more difficult it becomes to make assumptions and follow a traditional modeling approach. This becomes evident if we leave the realm of tabular data and consider images, text, and geospatial data. This is where machine learning can shine. Machine learning successfully handles various data types, such as tabular <span class="citation" data-cites="gao2020machine"><a href="references.html#ref-gao2020machine" role="doc-biblioref">[5]</a></span>, image <span class="citation" data-cites="erickson2017machine"><a href="references.html#ref-erickson2017machine" role="doc-biblioref">[6]</a></span>, geospatial <span class="citation" data-cites="ren2021deep"><a href="references.html#ref-ren2021deep" role="doc-biblioref">[7]</a></span>, sound <span class="citation" data-cites="oikarinen2019deep"><a href="references.html#ref-oikarinen2019deep" role="doc-biblioref">[8]</a></span>, graphs <span class="citation" data-cites="hu2020open"><a href="references.html#ref-hu2020open" role="doc-biblioref">[9]</a></span>, or text data <span class="citation" data-cites="neethu2013sentiment"><a href="references.html#ref-neethu2013sentiment" role="doc-biblioref">[10]</a></span>. There is often lots of data, but it’s not always in perfect shape:</p>
<ul>
<li>Most data wasn’t produced by controlled experiments but comes from the messy world.</li>
<li>Data is often high-dimensional without indication of what’s relevant and what’s not.</li>
<li>Data can come in various forms (text, images, audio, etc.) and is not restricted to tables with clearly interpretable columns.</li>
</ul>
<p>Unlike approaches such as classic statistical modeling, machine learning can deal with these problems. All you need is data! Well, we exaggerate a bit here, but you get the idea. Dealing with diverse data is not merely practical but also epistemic. If diverse data sources point to similar insights, this strongly supports the validity of these insights <span class="citation" data-cites="earman1992bayes"><a href="references.html#ref-earman1992bayes" role="doc-biblioref">[11]</a></span>.</p>
</section>
<section id="machine-learning-allows-you-to-work-on-new-questions" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="machine-learning-allows-you-to-work-on-new-questions"><span class="header-section-number">4.4</span> Machine learning allows you to work on new questions</h2>
<p>Machine learning has enabled scientists to leverage data to solve specific problems that they hadn’t been able to “solve” before using other approaches:</p>
<ul>
<li><strong>Translating texts:</strong> Machine translation is older than machine learning, but, let’s be honest, it was too bad to be useful. Before machine learning was mature enough and embraced, rule-based systems and statistical machine translation were used.</li>
<li><strong>Diagnosis of diseases based on X-ray images:</strong> Before machine learning, techniques like template matching and rule-based algorithms were used. Only machine learning made it possible to make huge progress in this task.</li>
<li><strong>Drug discovery:</strong> Traditionally, drug discovery involved a lot of trial and error. Machine learning has made it possible to analyze huge amounts of data and predict how different drugs might interact with targets in the body. In the future, this may greatly accelerate the process of drug discovery.</li>
</ul>
<p>For such research questions, machine learning is finally offering routes toward studying them systematically, built on empirical data. Paraphrasing Wittgenstein – the limits of my modeling are the limits of my world. This implies that extending the limits of modeling with machine learning extends the realm of phenomena scientists can investigate.</p>
</section>
<section id="dont-undervalue-machine-learning-because-it-is-new" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="dont-undervalue-machine-learning-because-it-is-new"><span class="header-section-number">4.5</span> Don’t undervalue machine learning because it is new</h2>
<p>Compared to other modeling approaches, machine learning has a short history. Differential equations or linear models, for instance, have existed for hundreds of years. Historically, new modeling approaches like machine learning always had a hard time establishing themselves:</p>
<ul>
<li>Einstein’s theory of relativity relied on non-Euclidean geometry – an unusual branch of mathematics for these endeavors – which was one of the reasons why he was met with skepticism.</li>
<li>Probabilistic models were first completely rejected in linguistics due to the dominance of logical methods. Today, they are well-established.</li>
<li>In econometrics, graphical causal models have a hard time asserting themselves against the prevailing framework of potential outcomes.</li>
</ul>
<p>The novelty of an approach should neither be taken as an argument in favor nor against using a certain modeling approach. The essential question is: Can the tool help answer the question you have set out to address?</p>
<!--
## To predict means to mimic reality

Machine learning models are trained to model the relationship between the features and the target. This training grounds the model in "reality", which of course depends on how you represent reality with your choice of data and so on. It's an informational abstraction of reality. The reality and abstraction of reality is an important distinction since the quality and direction of abstraction determines what you can later do with the model. During training, the model will faithfully adapt to the correlations in the data.

If you predict tornadoes but also use temporal data after the tornado happened, it's in a sense not a good match with reality since time flows backward in your model. However, it's not modeling reality, but an informational abstraction of reality based on how you represent it.
-->
</section>
<section id="further-justifications-for-machine-learning" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="further-justifications-for-machine-learning"><span class="header-section-number">4.6</span> Further justifications for machine learning</h2>
<p>There are further justifications for using machine learning in science:</p>
<ul>
<li><strong>Time efficiency:</strong> Accurate machine learning models can often be built time-efficiently by researchers. There is less pre-processing needed compared to other approaches and large parts of the model selection and training process can be automated. Also, there is great support for machine learning packages in popular programming languages like Python (e.g., scikit-learn, PyTorch, or TensorFlow) or R (e.g., mlr3 or tidymodels).</li>
<li><strong>Computational efficiency:</strong> In some situations, machine learning can be computationally cheaper than traditional modeling. Machine learning models can, for example, be used as fast surrogate models for simulations from physics or material science that are computationally very intense <span class="citation" data-cites="toledo2021deep"><a href="references.html#ref-toledo2021deep" role="doc-biblioref">[12]</a></span>. Machine learning weather prediction can run on your laptop, unlike numerical methods that demand the world’s biggest computing clusters <span class="citation" data-cites="lam2023learning"><a href="references.html#ref-lam2023learning" role="doc-biblioref">[13]</a></span>.</li>
<li><strong>Basis for theory:</strong> Machine learning may support you with the knowledge needed to build classical theory-based models. They may show which features contain predictive information and how features interact.</li>
<li><strong>Effective for operationalized goals:</strong> Machine learning is ideal if the aim can be easily encoded in a single metric. If you are confident in your metric, machine learning will provide you with the means to optimize for it. <!--- **Theoretical underpinnings:** Theory starts to slowly catch up to practical success. For some methods, there are learning guarantees [@vapnik1999overview;@bishop2006pattern] and even for deep learning techniques researchers increasingly have formal intuitions about how they work [@belkin2021fit].--></li>
</ul>


<div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-kuhn1997structure" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">T. S. Kuhn, <em>The structure of scientific revolutions</em>, vol. 962. University of Chicago press Chicago, 1997.</div>
</div>
<div id="ref-dennis2019errors" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">B. Dennis, J. M. Ponciano, M. L. Taper, and S. R. Lele, <span>“Errors in statistical inference under model misspecification: Evidence, hypothesis testing, and AIC,”</span> <em>Frontiers in Ecology and Evolution</em>, vol. 7, p. 372, 2019, doi: <a href="https://doi.org/10.3389/fevo.2019.00372">10.3389/fevo.2019.00372</a>.</div>
</div>
<div id="ref-hornik1991approximation" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">K. Hornik, <span>“Approximation capabilities of multilayer feedforward networks,”</span> <em>Neural networks</em>, vol. 4, no. 2, pp. 251–257, 1991, doi: <a href="https://doi.org/10.1016/0893-6080(91)90009-T">10.1016/0893-6080(91)90009-T</a>.</div>
</div>
<div id="ref-lu2020universal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">Y. Lu and J. Lu, <span>“A universal approximation theorem of deep neural networks for expressing probability distributions,”</span> <em>Advances in neural information processing systems</em>, vol. 33, pp. 3094–3105, 2020, doi: <a href="https://doi.org/10.5555/3495724.3495984">10.5555/3495724.3495984</a>.</div>
</div>
<div id="ref-gao2020machine" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">Y. Gao <em>et al.</em>, <span>“Machine learning based early warning system enables accurate mortality risk prediction for COVID-19,”</span> <em>Nature communications</em>, vol. 11, no. 1, p. 5033, 2020, doi: <a href="https://doi.org/10.5281/zenodo.3991113">10.5281/zenodo.3991113</a>.</div>
</div>
<div id="ref-erickson2017machine" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">B. J. Erickson, P. Korfiatis, Z. Akkus, and T. L. Kline, <span>“Machine learning for medical imaging,”</span> <em>Radiographics</em>, vol. 37, no. 2, pp. 505–515, 2017, doi: <a href="https://doi.org/10.1148/rg.2017160130">10.1148/rg.2017160130</a>.</div>
</div>
<div id="ref-ren2021deep" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">X. Ren <em>et al.</em>, <span>“Deep learning-based weather prediction: A survey,”</span> <em>Big Data Research</em>, vol. 23, p. 100178, 2021, doi: <a href="https://doi.org/10.1016/j.bdr.2020.100178">10.1016/j.bdr.2020.100178</a>.</div>
</div>
<div id="ref-oikarinen2019deep" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">T. Oikarinen <em>et al.</em>, <span>“Deep convolutional network for animal sound classification and source attribution using dual audio recordings,”</span> <em>The Journal of the Acoustical Society of America</em>, vol. 145, no. 2, pp. 654–662, 2019, doi: <a href="https://doi.org/10.1121/1.5097583">10.1121/1.5097583</a>.</div>
</div>
<div id="ref-hu2020open" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">W. Hu <em>et al.</em>, <span>“Open graph benchmark: Datasets for machine learning on graphs,”</span> <em>Advances in neural information processing systems</em>, vol. 33, pp. 22118–22133, 2020, doi: <a href="https://doi.org/doi/10.5555/3495724.3497579">doi/10.5555/3495724.3497579</a>.</div>
</div>
<div id="ref-neethu2013sentiment" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">M. Neethu and R. Rajasree, <span>“Sentiment analysis in twitter using machine learning techniques,”</span> in <em>2013 fourth international conference on computing, communications and networking technologies (ICCCNT)</em>, IEEE, 2013, pp. 1–5. doi: <a href="https://doi.org/10.1109/ICCCNT.2013.6726818">10.1109/ICCCNT.2013.6726818</a>.</div>
</div>
<div id="ref-earman1992bayes" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">J. Earman, <span>“Bayes or bust? A critical examination of bayesian confirmation theory.”</span> MIT Press, 1992.</div>
</div>
<div id="ref-toledo2021deep" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">J. Q. Toledo-Marı́n, G. Fox, J. P. Sluka, and J. A. Glazier, <span>“Deep learning approaches to surrogates for solving the diffusion equation for mechanistic real-world simulations,”</span> <em>Frontiers in Physiology</em>, vol. 12, p. 667828, 2021, doi: <a href="https://doi.org/10.3389/fphys.2021.667828">10.3389/fphys.2021.667828</a>.</div>
</div>
<div id="ref-lam2023learning" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">R. Lam <em>et al.</em>, <span>“Learning skillful medium-range global weather forecasting,”</span> <em>Science</em>, p. eadi2336, 2023, doi: <a href="https://doi.org/10.1126/science.adi2336">10.1126/science.adi2336</a>.</div>
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr/>
<ol>
<li id="fn1"><p>Many hypotheses classes in machine learning are universal approximators of some form; that is, they lie dense within the space of (continuous/step) functions <span class="citation" data-cites="hornik1991approximation lu2020universal"><a href="references.html#ref-hornik1991approximation" role="doc-biblioref">[3]</a>, <a href="references.html#ref-lu2020universal" role="doc-biblioref">[4]</a></span>. While this is not a super special property, e.g., also polynomials lie dense in the space of continuous functions, it at least expresses that there is in principle a model that approximates any possible relationship well. This is different compared to classical statistical models that rely on small model classes; even the best model within such a class may be overall a bad model to capture the true relationship between predictors and target.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

    
</body>
</html>