- en: Concept
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概念
- en: 原文：[https://dafriedman97.github.io/mlbook/content/c7/concept.html](https://dafriedman97.github.io/mlbook/content/c7/concept.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://dafriedman97.github.io/mlbook/content/c7/concept.html](https://dafriedman97.github.io/mlbook/content/c7/concept.html)
- en: '\[ \newcommand{\sumN}{\sum_{n = 1}^N} \newcommand{\sumn}{\sum_n} \newcommand{\prodN}{\prod_{n
    = 1}^N} \newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}} \newcommand{\bx}{\mathbf{x}}
    \newcommand{\bz}{\mathbf{z}} \newcommand{\bw}{\mathbf{w}} \newcommand{\bbeta}{\boldsymbol{\beta}}
    \newcommand{\btheta}{\boldsymbol{\theta}} \newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
    \newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}} \newcommand{\bSigma}{\boldsymbol{\Sigma}}
    \newcommand{\bT}{\mathbf{T}} \newcommand{\dadb}[2]{\frac{\partial #1}{\partial
    #2}} \newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}} \newcommand{\super}[2]{#1^{(#2)}}
    \newcommand{\superb}[2]{\mathbf{#1}^{(#2)}} \]'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \newcommand{\sumN}{\sum_{n = 1}^N} \newcommand{\sumn}{\sum_n} \newcommand{\prodN}{\prod_{n
    = 1}^N} \newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}} \newcommand{\bx}{\mathbf{x}}
    \newcommand{\bz}{\mathbf{z}} \newcommand{\bw}{\mathbf{w}} \newcommand{\bbeta}{\boldsymbol{\beta}}
    \newcommand{\btheta}{\boldsymbol{\theta}} \newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
    \newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}} \newcommand{\bSigma}{\boldsymbol{\Sigma}}
    \newcommand{\bT}{\mathbf{T}} \newcommand{\dadb}[2]{\frac{\partial #1}{\partial
    #2}} \newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}} \newcommand{\super}[2]{#1^{(#2)}}
    \newcommand{\superb}[2]{\mathbf{#1}^{(#2)}} \]'
- en: The neural network is a highly powerful and versatile class of models that has
    become quite a hot topic in machine learning. While the neural network’s ability
    to often outperform other popular model classes has earned it a reputation for
    being a near-magical black box algorithm, networks are not terribly complex or
    mysterious. Rather, by optimizing a highly-parametric and nonlinear structure,
    neural networks are flexible enough to model subtle relationships that other models
    may struggle to detect.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一种高度强大且通用的模型类别，已成为机器学习中的一个热门话题。虽然神经网络经常优于其他流行的模型类别，使其获得了近乎魔法的黑盒算法声誉，但网络并不特别复杂或神秘。相反，通过优化高度参数化和非线性结构，神经网络足够灵活，可以模拟其他模型可能难以检测到的微妙关系。
- en: Note
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Neural networks come in a variety of forms intended to accomplish a variety
    of tasks. Recurrent neural networks, for instance, are designed to model time
    series data, and convolutional neural networks are designed to model image data.
    In this chapter, we only cover feed-forward neural networks (FFNNs). FFNNs can
    be used for regression or classification tasks and serve as a natural introduction
    to other forms of neural networks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有多种形式，旨在完成各种任务。例如，循环神经网络被设计用来模拟时间序列数据，而卷积神经网络被设计用来模拟图像数据。在本章中，我们只介绍前馈神经网络（FFNNs）。FFNNs可用于回归或分类任务，并作为其他形式神经网络的自然引入。
- en: This section is organized as follows.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本节组织如下。
- en: Model Structure
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型结构
- en: An Overview
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 概述
- en: Communication between Layers
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层之间的通信
- en: Activation Functions
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Optimization
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化
- en: Back Propagation
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播
- en: Calculating Gradients
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度
- en: Combining Results with the Chain Rule
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用链式法则组合结果
- en: Combining Observations
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组合观测值
- en: A New Representation
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新的表示
- en: Gradients
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度
- en: 1\. Model Structure
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 模型结构
- en: Throughout this chapter, suppose we have training data \(\{\bx_n, \by_n\}_{n
    = 1}^N\) with \(\bx_n \in \R^{D_x}\)—which does *not* include an intercept term—and
    \(\by_n \in \R^{D_y}\) for \(n = 1, 2, \dots, N\). In other words, for each observation
    we have \(D_x\) predictors and \(D_y\) target variables. In this chapter, these
    will primarily be referred to as the *input* and *output* variables, respectively.
    Note that unlike in previous chapters, we might now have a *vector* of target
    variables rather than a single value. If there is only one target variable per
    observation (i.e. \(D_y = 1\)), we will write it as \(y_n\) rather than \(\by_n\).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的整个过程中，假设我们拥有训练数据 \(\{\bx_n, \by_n\}_{n = 1}^N\)，其中 \(\bx_n \in \R^{D_x}\)（不包括截距项）且
    \(\by_n \in \R^{D_y}\) 对于 \(n = 1, 2, \dots, N\)。换句话说，对于每个观测值，我们有 \(D_x\) 个预测变量和
    \(D_y\) 个目标变量。在本章中，这些将主要被称为*输入*和*输出*变量，分别。请注意，与前面的章节不同，我们可能现在有一个*向量*的目标变量而不是单个值。如果每个观测值只有一个目标变量（即
    \(D_y = 1\)），我们将将其写作 \(y_n\) 而不是 \(\by_n\)。
- en: 1.1 An Overview
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 概述
- en: The diagram below is a helpful representation of a basic neural network. Neural
    networks operate in layers. The network starts of with an *input layer*, consisting
    of the vector of predictors for a single observation. This is shown by \(x_0,
    \dots, x_3\) in the diagram (indicating that \(D_x = 4\) here). The network then
    passes through one or more *hidden layers*. The first hidden layer is a function
    of the input layer and each following hidden layer is a function of the last.
    (We will discuss these functions in more detail later). The network below has
    two hidden layers. Finally, the network passes from the last hidden layer into
    an *output layer*, representing the target variable or variables. In the network
    below, the target variable is two-dimensional (i.e. \(D_y = 2\)), so the layer
    is represented by the values \(y_0\) and \(y_1\).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图是表示基本神经网络的有助于理解的表现形式。神经网络在层中操作。网络从*输入层*开始，由单个观察值的预测因子向量组成。这在图中由 \(x_0, \dots,
    x_3\) 表示（表明这里 \(D_x = 4\)）。然后网络通过一个或多个*隐藏层*。第一隐藏层是输入层的函数，每个后续隐藏层是前一个隐藏层的函数。（我们将在稍后更详细地讨论这些函数）。下面的网络有两个隐藏层。最后，网络从最后一个隐藏层进入*输出层*，表示目标变量或变量。在下面的网络中，目标变量是二维的（即
    \(D_y = 2\)），所以该层由 \(y_0\) 和 \(y_1\) 的值表示。
- en: Note
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Diagrams like the one below are commonly used to represent neural networks.
    Note that these diagrams show only a single observation at a time. For instance,
    \(x_0, \dots x_3\) represent four predictors within one observation, rather than
    four different observations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图所示的图表通常用于表示神经网络。请注意，这些图表一次只显示一个观察值。例如，\(x_0, \dots x_3\) 代表一个观察值内的四个预测因子，而不是四个不同的观察值。
- en: '![](../Images/58b930168130f70132b7bfa0a2bc80f4.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/58b930168130f70132b7bfa0a2bc80f4.png)'
- en: Each layer in a neural network consists of *neurons*, represented by the circles
    in the diagram above. Neurons are simply scalar values. In the *input layer*,
    each neuron represents a single predictor. In the above diagram, the input layer
    has four neurons, labeled \(x_0\) through \(x_3\), each representing a single
    predictor. The neurons in the input layer then determine the neurons in the first
    hidden layer, labeled \(\super{z}{1}_0\) through \(\super{z}{1}_2\). We will discuss
    *how* shortly, but for now simply note the lines running from the input layer’s
    neurons to the first hidden layer’s neurons in the diagram above. Once the neurons
    in the first hidden layer are set, they become predictors for the next layer,
    acting just as the input layer did. When the neurons in the final hidden layer
    are fixed, they act as predictors for the output layer.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的每一层都由*神经元*组成，如图上所示的圆圈。神经元仅仅是标量值。在*输入层*中，每个神经元代表一个单独的预测因子。在上面的图中，输入层有四个神经元，分别标记为
    \(x_0\) 到 \(x_3\)，每个代表一个单独的预测因子。输入层中的神经元随后确定第一隐藏层中的神经元，分别标记为 \(\super{z}{1}_0\)
    到 \(\super{z}{1}_2\)。我们将在稍后讨论*如何*，但现在只需注意上图从输入层神经元到第一隐藏层神经元的线条。一旦第一隐藏层的神经元被设置，它们就成为了下一层的预测因子，作用就像输入层一样。当最终隐藏层的神经元被固定时，它们作为输出层的预测因子。
- en: One natural question is how many layers our neural network should contain. There
    is no single answer to this question, as the number of layers is chosen by the
    modeler. Any true neural network will have an input layer, an output layer, and
    at least one hidden layer. The network above has two hidden layers. Note that
    the superscript indicates the hidden layer number, e.g. \(z_{0}^{(1)}\) through
    \(z_2^{(1)}\) are in the first hidden layer and \(z_{0}^{(2)}\) through \(z_2^{(2)}\)
    are in the second hidden layer. We could also consider the input layer as an exogenous
    “hidden layer” and represent it with \(z_{0}^{(0)}\) through \(z_3^{(0)}\).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的问题是我们的人工神经网络应该包含多少层。这个问题没有单一的答案，因为层数是由模型选择决定的。任何真正的神经网络都将包含一个输入层、一个输出层和至少一个隐藏层。上面的网络有两个隐藏层。请注意，上标表示隐藏层的编号，例如
    \(z_{0}^{(1)}\) 到 \(z_2^{(1)}\) 在第一隐藏层，\(z_{0}^{(2)}\) 到 \(z_2^{(2)}\) 在第二隐藏层。我们也可以考虑输入层作为一个外生的“隐藏层”，并用
    \(z_{0}^{(0)}\) 到 \(z_3^{(0)}\) 来表示它。
- en: Another natural question is how many neurons each layer should contain. This
    is in part chosen by the modeler and in part predetermined. If our predictor vectors
    are of length \(D\), the input layer must have \(D\) neurons. Similarly, the output
    layer must have as many neurons as there are target variables. If, for instance,
    our model attempts to predict a store’s revenue and its costs (two targets) in
    a given month, our output layer must have two neurons. The sizes of the hidden
    layers, however, are chosen by the modeler. Too few neurons may cause underfitting
    by preventing the network from picking up on important patterns in the data while
    too many neurons may cause overfitting, allowing the network to fit parameters
    that match the training data exactly.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个自然的问题是每一层应该包含多少个神经元。这在一定程度上是由模型器选择的，在某种程度上是预先确定的。如果我们的预测向量长度为 \(D\)，则输入层必须具有
    \(D\) 个神经元。同样，输出层必须具有与目标变量数量相等的神经元。例如，如果我们的模型试图预测某个月份商店的收入和成本（两个目标），则输出层必须有两个神经元。然而，隐藏层的大小是由模型器选择的。神经元太少可能会导致欠拟合，因为网络无法从数据中捕捉到重要的模式，而神经元太多可能会导致过拟合，允许网络拟合与训练数据完全匹配的参数。
- en: 1.2 Communication between Layers
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 层之间的通信
- en: Let’s now turn to the process through which one layer communicates with the
    next. In this section, let \(\bz^{(a)}\) and \(\super{\bz}{b}\) represent the
    vector of neurons in any two consecutive layers. For instance, \(\super{\bz}{a}\)
    might be an input layer and \(\super{\bz}{b}\) the first hidden layer or \(\super{\bz}{a}\)
    might be a hidden layer and \(\super{\bz}{b}\) the following hidden layer. Suppose
    \(\super{\bz}{a} \in \R^{D_a}\) and \(\super{\bz}{b} \in \R^{D_b}\).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探讨一层如何与下一层进行通信的过程。在本节中，让我们用 \(\bz^{(a)}\) 和 \(\super{\bz}{b}\) 来表示任意两个连续层中的神经元向量。例如，\(\super{\bz}{a}\)
    可能是一个输入层，而 \(\super{\bz}{b}\) 是第一个隐藏层，或者 \(\super{\bz}{a}\) 可能是一个隐藏层，而 \(\super{\bz}{b}\)
    是后续的隐藏层。假设 \(\super{\bz}{a} \in \R^{D_a}\) 和 \(\super{\bz}{b} \in \R^{D_b}\)。
- en: 'In a feed-forward neural network, each neuron in \(\super{\bz}{b}\) is a function
    of every neuron in \(\super{\bz}{a}\). This function occurs in two stages: first
    a linear mapping of \(\super{\bz}{a}\) onto one dimension, then a nonlinear function
    called an *activation function*. Let’s look at a single neuron within \(\super{\bz}{b}\),
    \(\super{z}{b}_i\). The transformation from \(\super{\bz}{a}\) to \(\super{z}{b}_i\)
    takes the form'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在前馈神经网络中，\(\super{\bz}{b}\) 中的每个神经元都是 \(\super{\bz}{a}\) 中每个神经元的函数。这个函数发生在两个阶段：首先是将
    \(\super{\bz}{a}\) 线性映射到一维，然后是一个称为 *激活函数* 的非线性函数。让我们看看 \(\super{\bz}{b}\) 中的一个单个神经元，\(\super{z}{b}_i\)。从
    \(\super{\bz}{a}\) 到 \(\super{z}{b}_i\) 的转换形式如下
- en: \[\begin{split} \begin{align*} \super{h}{b}_i &= \bw_i^\top\super{\bz}{a} +
    c_i \\ \super{z}{b}_i &= f(\super{h}{b}_i), \end{align*} \end{split}\]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{h}{b}_i &= \bw_i^\top\super{\bz}{a} +
    c_i \\ \super{z}{b}_i &= f(\super{h}{b}_i), \end{align*} \end{split}\]
- en: where \(\bw_i \in \R^{D_a}\) is a vector of weights, \(c_i\) is a constant intercept
    term, and \(f()\) is an activation function. Note that \(\bw_i\) and \(c_i\) are
    specific to the \(i^\text{th}\) neuron in \(\super{\bz}{b}\) while \(f()\) is
    typically common among all neurons in \(\super{\bz}{b}\). We can also write the
    function relating the two layers in matrix form, as below.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\bw_i \in \R^{D_a}\) 是权重向量，\(c_i\) 是常数截距项，\(f()\) 是激活函数。请注意，\(\bw_i\) 和
    \(c_i\) 是 \(\super{\bz}{b}\) 中第 \(i^\text{th}\) 个神经元的特定值，而 \(f()\) 通常在 \(\super{\bz}{b}\)
    中所有神经元中是通用的。我们还可以将两个层之间的函数关系写成矩阵形式，如下所示。
- en: \[\begin{split} \begin{align*} \super{\mathbf{h}}{b} &= \mathbf{W}\super{\bz}{a}
    + \mathbf{c} \\\ \super{\mathbf{z}}{b} &= f(\super{\mathbf{h}}{b}), \end{align*}
    \end{split}\]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\mathbf{h}}{b} &= \mathbf{W}\super{\bz}{a}
    + \mathbf{c} \\\ \super{\mathbf{z}}{b} &= f(\super{\mathbf{h}}{b}), \end{align*}
    \end{split}\]
- en: where \(\mathbf{W} \in \R^{D_b \times D_a}\), \(\mathbf{c} \in \R^{D_b}\) and
    \(f()\) is applied element-wise.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{W} \in \R^{D_b \times D_a}\)，\(\mathbf{c} \in \R^{D_b}\) 且 \(f()\)
    是逐元素应用。
- en: Note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that we haven’t yet discussed *how* \(\mathbf{W}\), \(\mathbf{c}\) or \(f()\)
    are determined. For now, consider these all to be fixed and focus on the structure
    of a network. *How* we determine these values is discussed in the optimization
    section below.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们还没有讨论 \(\mathbf{W}\)，\(\mathbf{c}\) 或 \(f()\) 如何确定。现在，让我们考虑这些都被视为固定的，并专注于网络的结构。我们如何确定这些值将在下面的优化部分讨论。
- en: Once \(\super{\bz}{b}\) is fixed, we use the same process to create the next
    layer, \(\super{\bz}{c}\). When discussing many layers at a time, it is helpful
    to add superscripts to \(\mathbf{W}, \mathbf{c}\), and \(f()\) to indicate the
    layer. We can write the transmission of \(\super{\bz}{a}\) to \(\super{\bz}{b}\)
    followed by \(\super{\bz}{b}\) to \(\super{\bz}{c}\) as
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 \(\super{\bz}{b}\) 被固定，我们就使用相同的过程来创建下一层，\(\super{\bz}{c}\)。在同时讨论许多层时，给 \(\mathbf{W},
    \mathbf{c}\) 和 \(f()\) 添加上标以指示层是有帮助的。我们可以将 \(\super{\bz}{a}\) 到 \(\super{\bz}{b}\)
    的传输以及 \(\super{\bz}{b}\) 到 \(\super{\bz}{c}\) 的传输写为
- en: \[\begin{split} \begin{align*} \super{\bz}{b} &= \super{f}{b}\left(\super{\mathbf{W}}{b}\super{\bz}{a}
    + \super{\mathbf{c}}{b} \right) \\ \super{\bz}{c} &= \super{f}{c}\left(\super{\mathbf{W}}{c}\super{\bz}{b}
    + \super{\mathbf{c}}{c} \right). \\ \end{align*} \end{split}\]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\bz}{b} &= \super{f}{b}\left(\super{\mathbf{W}}{b}\super{\bz}{a}
    + \super{\mathbf{c}}{b} \right) \\ \super{\bz}{c} &= \super{f}{c}\left(\super{\mathbf{W}}{c}\super{\bz}{b}
    + \super{\mathbf{c}}{c} \right). \\ \end{align*} \end{split}\]
- en: A more mathematical representation of a neural network is given below. The network
    starts with a vector of predictors \(\bx\). This vector is then multiplied by
    \(\super{\mathbf{W}}{1}\) and added to \(\super{\mathbf{c}}{1}\), which sums to
    \(\super{\mathbf{h}}{1}\). We then apply an activation \(\super{f}{1}\) to \(\super{\mathbf{h}}{1}\),
    which results in our single hidden layer, \(\super{\mathbf{z}}{1}\). The same
    process is then applied to \(\super{\bz}{1}\), which results in our output vector,
    \(\by\).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个神经网络的更数学化的表示。网络从预测器的向量 \(\bx\) 开始。然后这个向量被乘以 \(\super{\mathbf{W}}{1}\) 并加上
    \(\super{\mathbf{c}}{1}\)，总和为 \(\super{\mathbf{h}}{1}\)。然后我们对 \(\super{\mathbf{h}}{1}\)
    应用激活 \(\super{f}{1}\)，从而得到我们的单个隐藏层，\(\super{\mathbf{z}}{1}\)。然后以相同的过程应用于 \(\super{\bz}{1}\)，从而得到我们的输出向量，\(\by\)。
- en: '![](../Images/ddd4201d5226ba07c2ffe7c422934b04.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddd4201d5226ba07c2ffe7c422934b04.png)'
- en: 1.3 Activation Functions
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 激活函数
- en: As we have seen, we create a neuron in one layer by taking a linear mapping
    of the neurons in the previous layer and then applying some *activation function*.
    What exactly is this activation function? An activation function is a (typically)
    nonlinear function that allows the network to learn complex relationships between
    the predictor(s) and the target variable(s).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们通过将前一层神经元的线性映射应用到一个神经元上，然后应用某种*激活函数*来创建一个层中的神经元。这个激活函数究竟是什么？激活函数是一个（通常是）非线性函数，它允许网络学习预测器（们）和目标变量（们）之间的复杂关系。
- en: Suppose, for instance, the relationship between a target variable \(y_n\) and
    a predictor \(x_n\) is given by
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设目标变量 \(y_n\) 和预测器 \(x_n\) 之间的关系由以下公式给出
- en: \[ y_n = |x_n| + \epsilon_n, \]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y_n = |x_n| + \epsilon_n, \]
- en: where \(\epsilon_n\) is a noise term. Despite its simplicity, this relationship
    cannot be accurately fit by a linear model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\epsilon_n\) 是一个噪声项。尽管它很简单，但这种关系不能被线性模型准确拟合。
- en: '![](../Images/ec9dd141265cd1ab9d61d344f2d31699.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec9dd141265cd1ab9d61d344f2d31699.png)'
- en: Ideally, we would apply some function to the predictor and use a different model
    depending on the result of this function. In the case above, \(x_n > 0\) would
    “activate” the model \(y_n \approx x_n\), and \(x_n \leq 0\) would “activate”
    the model \(y_n \approx -x_n\). Hence the name “activation function”.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们会将某个函数应用于预测器，并根据这个函数的结果使用不同的模型。在上面的例子中，如果 \(x_n > 0\)，则会“激活”模型 \(y_n
    \approx x_n\)，而如果 \(x_n \leq 0\)，则会“激活”模型 \(y_n \approx -x_n\)。因此得名“激活函数”。
- en: 'There are many commonly used activation functions, and deciding which function
    to use is a major consideration in modeling a neural network. Here we will limit
    our discussion to two of the most common functions: the ReLU (Rectified Linear
    Unit) and sigmoid functions. The linear activation function (which is really the
    absence of an activation function) is also discussed.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多常用的激活函数，决定使用哪个函数是构建神经网络时的一个主要考虑因素。在这里，我们将限制我们的讨论范围到两种最常见的函数：ReLU（线性整流单元）和sigmoid函数。线性激活函数（实际上是没有激活函数）也被讨论。
- en: ReLU
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReLU
- en: '![](../Images/a2c490c62bae3864640ecdbee382bd4a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2c490c62bae3864640ecdbee382bd4a.png)'
- en: ReLU is a simple yet extremely common activation function. It is defined as
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU是一种简单但极其常见的激活函数。它被定义为
- en: \[ f(x) = \text{max}(x, 0). \]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = \text{max}(x, 0). \]
- en: How can such a simple function benefit a neural network? ReLU acts like a switch,
    selectively turning channels on and off. Consider fitting a neural network to
    the dataset above generated with \(y_n = |x_n| + \epsilon_n\). Let’s use a very
    simple network represented by the diagram below. This network has one predictor,
    a single hidden layer with two neurons, and one output variable.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一个简单的函数如何使神经网络受益？ReLU就像一个开关，选择性地打开和关闭通道。考虑将神经网络拟合到上面生成的数据集，其中 \(y_n = |x_n|
    + \epsilon_n\)。让我们使用以下图表表示的非常简单的网络。这个网络有一个预测器，一个包含两个神经元的单个隐藏层和一个输出变量。
- en: '![](../Images/7956d5c1c598913c656afdc0767e7ee1.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7956d5c1c598913c656afdc0767e7ee1.png)'
- en: 'Now let’s say we decide to use \(f(\bx) = \text{ReLU}(\bx)\) and we land on
    the following parameters:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们决定使用 \(f(\bx) = \text{ReLU}(\bx)\)，并得到以下参数：
- en: \[\begin{split} \super{\mathbf{W}}{1} = \begin{pmatrix} 1 \\ -1 \end{pmatrix},
    \hspace{1mm} \super{\mathbf{c}}{1} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \hspace{1mm}
    \super{\mathbf{W}}{2} = \begin{pmatrix} 1 & 1 \end{pmatrix}, \hspace{1mm} \mathbf{c}^{(2)}
    = 0. \end{split}\]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \super{\mathbf{W}}{1} = \begin{pmatrix} 1 \\ -1 \end{pmatrix},
    \hspace{1mm} \super{\mathbf{c}}{1} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \hspace{1mm}
    \super{\mathbf{W}}{2} = \begin{pmatrix} 1 & 1 \end{pmatrix}, \hspace{1mm} \mathbf{c}^{(2)}
    = 0. \end{split}\]
- en: This is equivalent to the following complete model
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这等价于以下完整模型
- en: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    1 \\ -1 \end{pmatrix} x \right) \\ y &= \begin{pmatrix} 1 & 1 \end{pmatrix} \super{\bz}{1}.
    \end{align*} \end{split}\]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    1 \\ -1 \end{pmatrix} x \right) \\ y &= \begin{pmatrix} 1 & 1 \end{pmatrix} \super{\bz}{1}.
    \end{align*} \end{split}\]
- en: Will this model be able to fit our dataset? Suppose \(x_n = c\) for some *positive*
    constant \(c\). We will then get
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型能否拟合我们的数据集？假设 \(x_n = c\) 对于某个正的常数 \(c\)。那么我们将得到
- en: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    c \\ -c \end{pmatrix} \right) = \begin{pmatrix} c \\ 0 \end{pmatrix} \\ y &= \begin{pmatrix}
    1 & 1 \end{pmatrix} \begin{pmatrix} c \\ 0 \end{pmatrix} = c. \end{align*} \end{split}\]
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    c \\ -c \end{pmatrix} \right) = \begin{pmatrix} c \\ 0 \end{pmatrix} \\ y &= \begin{pmatrix}
    1 & 1 \end{pmatrix} \begin{pmatrix} c \\ 0 \end{pmatrix} = c. \end{align*} \end{split}\]
- en: 'So we will predict \(y_n = |x_n| = c\), a sensible result! Similarly, if \(x_n
    = -c\), we would again obtain the valid prediction \(y_n = |x_n| = c\). ReLU is
    able to achieve this result by activating a different channel depending on the
    value of \(x_n\): if \(x_n\) is greater than 0, it activates \(y_n = x_n\), and
    if \(x_n\) is less than 0, it activates \(y_n = -x_n\).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将预测 \(y_n = |x_n| = c\)，这是一个合理的预测结果！同样，如果 \(x_n = -c\)，我们也会得到有效的预测 \(y_n
    = |x_n| = c\)。ReLU能够通过根据 \(x_n\) 的值激活不同的通道来实现这一结果：如果 \(x_n\) 大于0，它激活 \(y_n = x_n\)；如果
    \(x_n\) 小于0，它激活 \(y_n = -x_n\)。
- en: 'As we will see in the next section, fitting a neural network consists of taking
    gradients of our activation functions. Fortunately ReLU has a straightforward
    derivative:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在下一节中看到的，拟合神经网络包括计算激活函数的梯度。幸运的是，ReLU有一个直接的导数：
- en: \[\begin{split} \frac{\partial}{\partial x} \text{ReLU}(x) = \begin{cases} 1,
    & x > 0 \\ 0, & x \leq 0\. \end{cases} \end{split}\]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \frac{\partial}{\partial x} \text{ReLU}(x) = \begin{cases} 1,
    & x > 0 \\ 0, & x \leq 0\. \end{cases} \end{split}\]
- en: Note that this derivative is not technically defined at 0\. In practice, it
    is very unlikely that we will be applying an activation function to 0 *exactly*,
    though in that case the convention is to set its derivative equal to 0.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个导数在0处技术上没有定义。在实践中，我们很少会精确地将激活函数应用于0，尽管在这种情况下，惯例是将其导数设为0。
- en: Sigmoid
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Sigmoid
- en: '![](../Images/f99d9f109382dc57dff15439e009ffe8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f99d9f109382dc57dff15439e009ffe8.png)'
- en: A second common activation function is the *logistic sigmoid function*, often
    referred to as just *the sigmoid function*. This function was introduced in [chapter
    3](../c3/s1/logistic_regression.html) in the context of the logistic regression.
    The sigmoid function is defined as
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个常见的激活函数是 *逻辑sigmoid函数*，通常简称为 *sigmoid函数*。这个函数在 [第3章](../c3/s1/logistic_regression.html)
    的逻辑回归背景下被引入。sigmoid函数定义为
- en: \[ \sigma(x) = \frac{1}{1 + \exp(-x)}. \]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma(x) = \frac{1}{1 + \exp(-x)}. \]
- en: Note that the sigmoid function takes any real value and returns a value between
    0 and 1\. As a result, the sigmoid function is commonly applied to the last hidden
    layer in a network in order to return a probability estimate in the output layer.
    This makes it common in classification problems.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到sigmoid函数接受任何实数值并返回介于0和1之间的值。因此，sigmoid函数通常应用于网络的最后一隐藏层，以便在输出层返回概率估计。这使得它在分类问题中很常见。
- en: As we saw in chapter 3, a convenient fact about the sigmoid function is that
    we can express its derivative in terms of itself.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第3章中看到的，sigmoid函数的一个方便的事实是，我们可以用自身来表示其导数。
- en: \[ \dadb{\sigma(x)}{x} = \frac{\exp(-x)}{\left( 1 + \exp(-x) \right)^2} = \frac{1}{1
    + \exp(-x)} \cdot \frac{\exp(-x)}{1 + \exp(-x)} = \sigma(x)\left(1 - \sigma(x)\right).
    \]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\sigma(x)}{x} = \frac{\exp(-x)}{\left( 1 + \exp(-x) \right)^2} = \frac{1}{1
    + \exp(-x)} \cdot \frac{\exp(-x)}{1 + \exp(-x)} = \sigma(x)\left(1 - \sigma(x)\right).
    \]
- en: The Linear Activation Function
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线性激活函数
- en: Another possible activation function is the “linear” activation function, which
    is the same as skipping the activation function altogether. The linear activation
    function simply returns its input. It is defined with
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能的激活函数是“线性”激活函数，它与完全不使用激活函数相同。线性激活函数简单地返回其输入。它定义为
- en: \[ f(x) = x, \]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = x, \]
- en: and has derivative
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 并且有导数
- en: \[ \dadb{f(x)}{x} = 1\. \]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{f(x)}{x} = 1\. \]
- en: The linear activation function is often used before the last layer in a neural
    network for regression. Rather than constraining the fitted values to be in some
    range or setting half of them equal to 0, we want to leave them as they are.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 线性激活函数通常用于神经网络回归的最后一层之前。我们不想将拟合值约束在某个范围内或将其一半设置为 0，而是希望保持它们原样。
- en: 2\. Optimization
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 优化
- en: We have now seen that a neural network operates through a series of linear mappings
    and activation functions. The linear mapping for layer \(\ell\) is determined
    by the parameters in \(\super{\mathbf{W}}{\ell}\) and \(\super{\mathbf{c}}{\ell}\),
    also called the *weights*. This section discusses the process through which the
    weights in a neural network are fit, called *back propagation*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到，神经网络通过一系列线性映射和激活函数运行。层 \(\ell\) 的线性映射由 \(\super{\mathbf{W}}{\ell}\)
    和 \(\super{\mathbf{c}}{\ell}\) 中的参数确定，也称为*权重*。本节讨论了神经网络中权重拟合的过程，称为*反向传播*。
- en: The rest of this page requires a good amount of matrix differentiation, which
    is introduced in the [math appendix](../appendix/math.html). Note that we use
    the “numerator layout,” meaning for \(\by \in \R^m\) and \(\bx \in \R^n\), we
    write \(\partial\by/\partial\bx\) as
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本页的其余部分需要大量的矩阵微分，这在[数学附录](../appendix/math.html)中介绍。请注意，我们使用“分子布局”，这意味着对于 \(\by
    \in \R^m\) 和 \(\bx \in \R^n\)，我们写 \(\partial\by/\partial\bx\) 为
- en: \[\begin{split} \dadb{\by}{\bx} = \begin{bmatrix} \dadb{y_1}{x_1} & \dots &
    \dadb{y_1}{x_n} \\ & \dots & \\ \dadb{y_m}{x_1} & \dots & \dadb{y_m}{x_n} \end{bmatrix}
    \in \R^{m \times n}. \end{split}\]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\by}{\bx} = \begin{bmatrix} \dadb{y_1}{x_1} & \dots &
    \dadb{y_1}{x_n} \\ & \dots & \\ \dadb{y_m}{x_1} & \dots & \dadb{y_m}{x_n} \end{bmatrix}
    \in \R^{m \times n}. \end{split}\]
- en: 2.1 Back Propagation
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 反向传播
- en: Suppose we choose some loss function \(\mathcal{L}\) for our network to minimize.
    Note that because our target variable is multi-dimensional, \(\boldsymbol{\mathcal{L}}\)
    function will be a vector of losses (e.g. the loss for the first target, the loss
    for the second target, etc.). To find the network’s optimal weights, we can conduct
    gradient descent, repeatedly taking the derivative of our loss function with respect
    to each weight and adjusting accordingly. As we will see, this involves finding
    the gradient of the network’s final weights, then using the chain rule to find
    the gradient of the weights that came earlier. In this process, we move backward
    through the network, and hence the name “back propagation.”
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们为我们的网络选择一些损失函数 \(\mathcal{L}\) 以最小化。请注意，因为我们的目标变量是多维的，\(\boldsymbol{\mathcal{L}}\)
    函数将是一个损失向量（例如，第一个目标的损失，第二个目标的损失等）。为了找到网络的优化权重，我们可以进行梯度下降，反复对每个权重求损失函数的导数并相应调整。正如我们将看到的，这涉及到找到网络最终权重的梯度，然后使用链式法则找到先前权重的梯度。在这个过程中，我们沿着网络向后移动，因此得名“反向传播”。
- en: '![](../Images/1227d5a61b04d2213382374c634abd8a.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1227d5a61b04d2213382374c634abd8a.png)'
- en: Consider conducting gradient descent for the network above. Write the loss function
    as \(\mathcal{L}(\hat{\by})\), where \(\hat{\by}\) is the network’s output. Let’s
    start by writing out the derivative of \(\mathcal{L}\) with respect to \(\super{\mathbf{W}}{L}\),
    the final matrix of weights in our network. We can do this with the chain rule,
    as below.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑对上述网络进行梯度下降。将损失函数写成 \(\mathcal{L}(\hat{\by})\)，其中 \(\hat{\by}\) 是网络的输出。让我们首先写出
    \(\mathcal{L}\) 关于 \(\super{\mathbf{W}}{L}\)，我们网络中的最终权重矩阵的导数。我们可以使用链式法则来完成这个操作，如下所示。
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
- en: The gradient of \(\super{\mathbf{c}}{L}\) is equivalent. The math behind these
    calculations is covered in the following section. Next, we want to find the gradient
    of \(\super{\mathbf{W}}{L-1}\), shown below.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: \(\super{\mathbf{c}}{L}\) 的梯度是等价的。这些计算背后的数学将在下一节中介绍。接下来，我们想要找到 \(\super{\mathbf{W}}{L-1}\)
    的梯度，如下所示。
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L-1}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}
    \cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}} \cdot \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{z}}{L-1}}
    \cdot \dadb{\super{\mathbf{z}}{L-1}}{\super{\mathbf{h}}{L-1}} \cdot \dadb{\super{\mathbf{h}}{L-1}}{\super{\mathbf{W}}{L-1}}
    \]
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L-1}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}
    \cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}} \cdot \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{z}}{L-1}}
    \cdot \dadb{\super{\mathbf{z}}{L-1}}{\super{\mathbf{h}}{L-1}} \cdot \dadb{\super{\mathbf{h}}{L-1}}{\super{\mathbf{W}}{L-1}}
    \]
- en: 'This expression is pretty ugly, but there is a shortcut. This gradient and
    the gradient of \(\super{\mathbf{W}}{L}\) share the first two terms, which represent
    the gradient of \(\super{\mathbf{h}}{L}\). To save time (both in writing out the
    gradients and in calculating them in practice), we can record this gradient, \(\nabla
    \super{\mathbf{h}}{L}\), and apply it where necessary. We can do the same with
    \(\nabla \mathbf{h}^{(L-1)}\), which simplifies the gradient of \(\mathbf{W}^{(L-2)}\):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式看起来很丑陋，但有一个捷径。这个梯度与 \(\super{\mathbf{W}}{L}\) 的梯度共享前两个项，它们代表 \(\super{\mathbf{h}}{L}\)
    的梯度。为了节省时间（在写出梯度以及在实践中计算它们时），我们可以记录这个梯度，\(\nabla \super{\mathbf{h}}{L}\)，并在需要的地方应用它。我们也可以对
    \(\nabla \mathbf{h}^{(L-1)}\) 做同样的事情，这简化了 \(\mathbf{W}^{(L-2)}\) 的梯度：
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L-2}} = \nabla \super{\mathbf{h}}{L-1}
    \cdot \dadb{\super{\mathbf{h}}{L-1}}{\super{\mathbf{z}}{L-2}} \cdot \dadb{\super{\mathbf{z}}{L-2}}{\super{\mathbf{h}}{L-2}}
    \cdot \dadb{\super{\mathbf{h}}{L-2}}{\super{\mathbf{W}}{L-2}}. \]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L-2}} = \nabla \super{\mathbf{h}}{L-1}
    \cdot \dadb{\super{\mathbf{h}}{L-1}}{\super{\mathbf{z}}{L-2}} \cdot \dadb{\super{\mathbf{z}}{L-2}}{\super{\mathbf{h}}{L-2}}
    \cdot \dadb{\super{\mathbf{h}}{L-2}}{\super{\mathbf{W}}{L-2}}. \]
- en: We continue this same process until we reach the first set of weights.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续这个过程，直到达到第一组权重。
- en: We’ve now seen intuitively how to find the gradients for our network’s many
    weights. To conduct back propagation, we simply use these gradients to run gradient
    descent. Next, let’s see how to actually calculate these gradients.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经直观地看到了如何找到我们网络中许多权重的梯度。为了进行反向传播，我们只需使用这些梯度来运行梯度下降。接下来，让我们看看如何实际计算这些梯度。
- en: 2.2 Calculating Gradients
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 梯度计算
- en: 'In this section we will derive the gradients used in back propagation. For
    each iteration in this process we need to know the derivative of our loss function
    with respect to each weight in the entire network. For the network shown above,
    this requires calculating the following gradients:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将推导出反向传播中使用的梯度。对于这个过程中的每一次迭代，我们需要知道我们的损失函数相对于整个网络中每个权重的导数。对于上面显示的网络，这需要计算以下梯度：
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{1}}, \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{1}},
    \dots, \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{L}}, \text{ and } \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{L}}.
    \]
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{1}}, \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{1}},
    \dots, \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{L}}, \text{ 和 } \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{L}}.
    \]
- en: Since we will find these with the chain rule, we will need to calculate other
    gradients along the way. All the necessary gradients are derived below. Note that
    the following sub-sections cover the stages within a single layer of a network
    in reverse order (as back propagation does).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用链式法则来找到这些梯度，因此我们需要在途中计算其他梯度。所有必要的梯度都在下面推导出来。注意，以下子节按反向顺序（正如反向传播所做的那样）覆盖了网络单层中的各个阶段。
- en: Note
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the rest of this section considers only one observation at a time.
    The vector \(\by\), for instance, refers to the output variables for a single
    observation, rather than a vector of 1-dimensional output variables for several
    observations. Similarly, \(\partial \mathcal{L}(\hat{\by})/\partial\hat{\by}\)
    refers to the derivative of the loss with respect to a single observation’s output.
    The final section discusses how to combine the derivatives from multiple observations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，本节的其余部分只考虑一次一个观测值。例如，向量\(\by\)指的是单个观测值的输出变量，而不是多个观测值的1维输出变量的向量。同样，\(\partial
    \mathcal{L}(\hat{\by})/\partial\hat{\by}\)指的是相对于单个观测值输出的导数。最后一节讨论了如何将多个观测值的导数组合起来。
- en: For the following, let there be \(L\) layers in total. Also let layer \(\ell\)
    have size \(D_\ell\), except the input and output layers which have sizes \(D_x\)
    and \(D_y\), respectively.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下内容，假设总共有\(L\)层。也假设层\(\ell\)的大小为\(D_\ell\)，除了输入层和输出层，它们的大小分别为\(D_x\)和\(D_y\)。
- en: 2.2.1 Loss Functions and their Gradients
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 损失函数及其梯度
- en: '![](../Images/6e20022b03b555510592958ec46ee03d.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e20022b03b555510592958ec46ee03d.png)'
- en: 'Back propagation begins where the network ends: the loss function \(\mathcal{L}(\hat{\by})\).
    Let’s start by introducing some common loss functions and their derivatives with
    respect to our predictions, \(\hat{\by}\). Later, using the chain rule, we will
    use these derivatives to calculate the derivatives with respect to our network’s
    weights.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播从网络结束的地方开始：损失函数\(\mathcal{L}(\hat{\by})\)。让我们先介绍一些常见的损失函数及其相对于我们预测的导数，\(\hat{\by}\)。稍后，我们将使用链式法则，使用这些导数来计算相对于我们网络权重的导数。
- en: A common loss function for quantitative output variables is the residual sum
    of squares. For a single observation, the loss is
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定量输出变量，一个常见的损失函数是残差平方和。对于单个观测值，损失是
- en: \[ \mathcal{L}_{RSS}(\hat{\by}) = (\by - \hat{\by})^2. \]
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}_{RSS}(\hat{\by}) = (\by - \hat{\by})^2. \]
- en: Note
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the loss is a function of both our predictions (\(\hat{\by}\)) and
    the true targets (\(\by\)). However, since the true targets are fixed, we can
    only manipulate \(\hat{\by}\), so we write the loss as only a function of \(\hat{\by}\).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，损失是我们的预测(\(\hat{\by}\))和真实目标(\(\by\))的函数。然而，由于真实目标是固定的，我们只能操作\(\hat{\by}\)，因此我们将损失写成仅是\(\hat{\by}\)的函数。
- en: Note that we have a vector of losses because there are multiple output variables
    and we consider the loss for each variable independently. Now for the first step
    in back propagation, we calculate the derivative of this loss with respect to
    \(\hat{\by}\), which is simply given by
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们有一个损失向量，因为存在多个输出变量，并且我们独立地考虑每个变量的损失。现在，对于反向传播的第一步，我们计算这个损失相对于\(\hat{\by}\)的导数，它简单地给出
- en: \[ \dadb{\mathcal{L}_{RSS}(\hat{\by})}{\hat{\by}} = -2(\by - \hat{\by})^\top
    \in \R^{1 \times D_y}. \]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}_{RSS}(\hat{\by})}{\hat{\by}} = -2(\by - \hat{\by})^\top
    \in \R^{1 \times D_y}. \]
- en: Since we are using the numerator layout convention, this derivative is a length-\(D_y\)
    row vector, or equivalently a \(1\) by \(D_y\) matrix.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是分子布局约定，这个导数是一个长度为\(D_y\)的行向量，或者等价地，一个\(1\)乘以\(D_y\)的矩阵。
- en: For binary classification problems, a common loss function is the log loss or
    cross entropy, given by
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类问题，一个常见的损失函数是对数损失或交叉熵，给出
- en: \[ \mathcal{L}_{Log}(\hat{\by}) = -\Big(\by\log \hat{\by}+(1-\by)\log(1-\hat{\by})\Big),
    \]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}_{Log}(\hat{\by}) = -\Big(\by\log \hat{\by}+(1-\by)\log(1-\hat{\by})\Big),
    \]
- en: where the \(i^\text{th}\) entry in \(\hat{\by}\) gives the estimated probability
    that the \(i^\text{th}\) output variable equals 1\. The derivative of this loss
    function with respect to \(\hat{\by}\) is given by
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中\(\hat{\by}\)中的第\(i^\text{th}\)个条目给出了第\(i^\text{th}\)个输出变量等于1的估计概率。这个损失函数相对于\(\hat{\by}\)的导数由以下给出
- en: \[ \begin{align*} \dadb{\mathcal{L}_{Log}(\hat{\by})}{\hat{\by}} &= \left(-\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top\in \R^{1 \times D_y}. \end{align*} \]
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\mathcal{L}_{Log}(\hat{\by})}{\hat{\by}} &= \left(-\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top\in \R^{1 \times D_y}. \end{align*} \]
- en: Once we calculate \(\partial \mathcal{L}(\hat{\by})/\partial\hat{\by}\), we
    can move further back into the network. Since \(\hat{\by}\) is the result of an
    activation function, the next step in back propagation is to calculate the derivative
    of our activation functions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算出\(\partial \mathcal{L}(\hat{\by})/\partial\hat{\by}\)，我们就可以进一步向网络的反方向移动。由于\(\hat{\by}\)是激活函数的结果，反向传播的下一步是计算我们激活函数的导数。
- en: 2.2.2 Gradients of the Activation Functions
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 激活函数的梯度
- en: '![](../Images/73cd03e425f8718e9a2e8b211cbb8b9c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73cd03e425f8718e9a2e8b211cbb8b9c.png)'
- en: Recall that \(\superb{z}{\ell}\), the output layer of \(\ell\), is the result
    of an activation function applied to a linear mapping \(\superb{h}{\ell}\). This
    includes the output of the final layer, \(\mathbf{\hat{y}}\), which we can also
    write as \(\superb{z}{L}\).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，\(\superb{z}{\ell}\)，层 \(\ell\) 的输出层，是通过对线性映射 \(\superb{h}{\ell}\) 应用激活函数得到的结果。这包括最终层的输出，\(\mathbf{\hat{y}}\)，我们也可以将其写作
    \(\superb{z}{L}\)。
- en: ReLU
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ReLU
- en: Suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) where
    \(\super{f}{\ell}\) is the ReLU function. We are interested in \(\partial \superb{z}{\ell}/\partial
    \superb{h}{\ell}\). For \(i \neq j\), we have
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) 其中 \(\super{f}{\ell}\)
    是 ReLU 函数。我们感兴趣的是 \(\partial \superb{z}{\ell}/\partial \superb{h}{\ell}\)。对于 \(i
    \neq j\)，我们有
- en: \[ \frac{\partial \super{z}{\ell}_i}{\partial \super{h}{\ell}_j} = 0, \]
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \super{z}{\ell}_i}{\partial \super{h}{\ell}_j} = 0, \]
- en: since \(\super{z}{\ell}_i\) is not a function of \(\super{h}{\ell}_j\). Then
    using the ReLU derivative, we have
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 \(\super{z}{\ell}_i\) 不是 \(\super{h}{\ell}_j\) 的函数。然后使用 ReLU 导数，我们有
- en: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \begin{cases}
    1, & \super{h}{\ell}_i > 0 \\ 0, & \super{h}{\ell}_i \leq 0\. \end{cases} \end{split}\]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \begin{cases}
    1, & \super{h}{\ell}_i > 0 \\ 0, & \super{h}{\ell}_i \leq 0\. \end{cases} \end{split}\]
- en: We can then compactly write the entire derivative as
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简洁地写出整个导数
- en: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = \text{diag}(\superb{h}{\ell}
    > 0) \in \R^{D_\ell \times D_\ell}. \]
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = \text{diag}(\superb{h}{\ell}
    > 0) \in \R^{D_\ell \times D_\ell}. \]
- en: Sigmoid
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Now suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)
    where \(\super{f}{\ell}\) is the sigmoid function. Again, the partial derivative
    \(\partial \super{z}{\ell}_i/\partial \super{h}{\ell}_j\) is 0 for \(i \neq j\).
    By the sigmoid derivative, we have
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们有 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) 其中 \(\super{f}{\ell}\)
    是 sigmoid 函数。再次，偏导数 \(\partial \super{z}{\ell}_i/\partial \super{h}{\ell}_j\)
    对于 \(i \neq j\) 是 0。通过 sigmoid 导数，我们有
- en: \[ \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \sigma(\super{h}{\ell}_i)(1-\sigma(\super{h}{\ell}_i)).
    \]
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \sigma(\super{h}{\ell}_i)(1-\sigma(\super{h}{\ell}_i)).
    \]
- en: We can again write the entire result compactly as
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次简洁地写出整个结果
- en: \[ \begin{align*} \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} &= \text{diag}\left(\sigma(\superb{h}{\ell})(1-\sigma(\superb{h}{\ell})\right)
    \in \R^{D_\ell \times D_\ell}. \end{align*} \]
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} &= \text{diag}\left(\sigma(\superb{h}{\ell})(1-\sigma(\superb{h}{\ell})\right)
    \in \R^{D_\ell \times D_\ell}. \end{align*} \]
- en: Linear
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 线性
- en: Finally, suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)
    where \(\super{f}{\ell}\) is the linear function. We then have
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，假设我们有 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) 其中 \(\super{f}{\ell}\)
    是线性函数。然后我们有
- en: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_j} = \begin{cases}
    1 , & i = j\\ 0, & i \neq j. \end{cases} \end{split}\]
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_j} = \begin{cases}
    1 , & i = j\\ 0, & i \neq j. \end{cases} \end{split}\]
- en: The entire gradient is then simply
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 整个梯度可以简单地表示为
- en: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = I_{D_{\ell}} \in \R^{D_\ell \times
    D_\ell}. \]
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = I_{D_{\ell}} \in \R^{D_\ell \times
    D_\ell}. \]
- en: 2.2.3 Gradients of the Weights
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 权重的梯度
- en: '![](../Images/cff599cc6963d9f0e1254e3990346bc0.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cff599cc6963d9f0e1254e3990346bc0.png)'
- en: We are now finally able to calculate the gradients of our weights. Specifically,
    we will calculate \(\partial \superb{h}{\ell}/ \partial \superb{c}{\ell}\) and
    \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\) which, when combined
    with our previous results through the chain rule, will allow us to obtain the
    derivative of the loss function with respect the layer \(\ell\) weights.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在终于能够计算我们权重的梯度了。具体来说，我们将计算 \(\partial \superb{h}{\ell}/ \partial \superb{c}{\ell}\)
    和 \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\)，结合我们之前通过链式法则得到的结果，这将使我们能够获得损失函数相对于层
    \(\ell\) 权重的导数。
- en: Recall that we obtain \(\superb{h}{\ell}\) through
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们通过
- en: \[ \superb{h}{\ell} = \superb{W}{\ell}\superb{z}{\ell-1} + \superb{c}{\ell},
    \]
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \superb{h}{\ell} = \superb{W}{\ell}\superb{z}{\ell-1} + \superb{c}{\ell},
    \]
- en: giving us the simple derivative
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 给出简单的导数
- en: \[ \dadb{\super{\mathbf{h}}{\ell}}{\superb{c}{\ell}} = I_{D_\ell} \in \R^{D_\ell
    \times D_\ell}. \]
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\super{\mathbf{h}}{\ell}}{\superb{c}{\ell}} = I_{D_\ell} \in \R^{D_\ell
    \times D_\ell}. \]
- en: The derivative \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\) is more
    complicated. Since we are taking the derivative of a vector with respect to a
    matrix, our result will be a tensor. The shape of this tensor will be \(D_\ell
    \times (D_\ell \times D_{\ell - 1})\) since \(\superb{h}{\ell} \in R^{D_\ell}\)
    and \(\superb{W}{\ell} \in \R^{D_\ell \times D_{\ell-1}}\). The first element
    of this tensor is given by \(\partial \super{h}{\ell}_1/ \partial \superb{W}{\ell}\).
    Using the expression for \(\superb{h}{\ell}\) above, we see that this is a matrix
    with \((\superb{z}{\ell - 1})^\top\) in the first row and 0s everywhere else.
    More generally, the \(i^\text{th}\) entry in this derivative will have all 0s
    except \((\superb{z}{\ell - 1})^\top\) in its \(i^\text{th}\) row. This is represented
    below.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 导数\(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\)更复杂。由于我们是对一个向量相对于一个矩阵求导，我们的结果将是一个张量。这个张量的形状将是\(D_\ell
    \times (D_\ell \times D_{\ell - 1})\)，因为\(\superb{h}{\ell} \in R^{D_\ell}\)且\(\superb{W}{\ell}
    \in \R^{D_\ell \times D_{\ell-1}}\)。这个张量的第一个元素由\(\partial \super{h}{\ell}_1/ \partial
    \superb{W}{\ell}\)给出。使用上面给出的\(\superb{h}{\ell}\)的表达式，我们看到这是一个矩阵，第一行有\((\superb{z}{\ell
    - 1})^\top\)，其他地方都是0。更一般地，这个导数的第\(i^\text{th}\)项将除了其第\(i^\text{th}\)行有\((\superb{z}{\ell
    - 1})^\top\)外，其他都是0。这如下所示。
- en: \[\begin{split} \begin{align*} \dadb{\superb{h}{\ell}}{\superb{W}{\ell}} &=
    \begin{bmatrix} \dadb{\super{h}{\ell}_1}{\superb{W}{\ell}} \\ \\ \dots \\ \\ \dadb{\super{h}{\ell}_{n_\ell}}{\superb{W}{\ell}}
    \end{bmatrix} = \begin{bmatrix} \begin{bmatrix} \superb{z}{\ell - 1})^\top \\
    ... \\ \mathbf{0}^\top \end{bmatrix}\\ \dots \\ \begin{bmatrix} \mathbf{0}^\top
    \\ \dots \\ (\superb{z}{\ell - 1})^\top\end{bmatrix}\end{bmatrix} \in \R^{D_\ell
    \times (D_\ell \times D_{\ell - 1})}. \end{align*} \end{split}\]
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \dadb{\superb{h}{\ell}}{\superb{W}{\ell}} &=
    \begin{bmatrix} \dadb{\super{h}{\ell}_1}{\superb{W}{\ell}} \\ \\ \dots \\ \\ \dadb{\super{h}{\ell}_{n_\ell}}{\superb{W}{\ell}}
    \end{bmatrix} = \begin{bmatrix} \begin{bmatrix} \superb{z}{\ell - 1})^\top \\
    ... \\ \mathbf{0}^\top \end{bmatrix}\\ \dots \\ \begin{bmatrix} \mathbf{0}^\top
    \\ \dots \\ (\superb{z}{\ell - 1})^\top\end{bmatrix}\end{bmatrix} \in \R^{D_\ell
    \times (D_\ell \times D_{\ell - 1})}. \end{align*} \end{split}\]
- en: 2.2.4 One Last Gradient
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 最后一个梯度
- en: We now have all the results necessary to calculate the derivative of the loss
    function with respect to the weights in the *final* layer. For instance, we can
    evaluate
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了计算相对于最终层权重的损失函数导数的所有必要结果。例如，我们可以评估
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
- en: 'using the results from sections 2.1, 2.2, and 2.3\. However, to obtain the
    derivative of the loss function with respect to weights in the *previous* layers,
    we need one more derivative: the derivative of \(\superb{h}{\ell}\), the linear
    mapping in layer \(\ell\), with respect to \(\superb{z}{\ell - 1}\), the output
    of the previous layer. Fortunately, this derivative is simple:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用2.1、2.2和2.3节的结果。然而，为了获得相对于前一层权重的损失函数的导数，我们需要一个额外的导数：\(\superb{h}{\ell}\)，即层\(\ell\)中的线性映射相对于\(\superb{z}{\ell
    - 1}\)，即前一层输出的导数。幸运的是，这个导数很简单：
- en: \[ \dadb{\superb{h}{\ell}}{\superb{z}{\ell - 1}} = {\superb{W}{\ell}}. \]
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\superb{h}{\ell}}{\superb{z}{\ell - 1}} = {\superb{W}{\ell}}. \]
- en: Now that we have \(\partial \superb{h}{\ell}/\partial \superb{z}{\ell - 1}\),
    we reuse the results from sections 2.2 and 2.3 to calculate \(\partial \superb{z}{\ell
    - 1}/\partial \superb{h}{\ell - 1}\) and \(\partial \superb{h}{\ell - 1}/ \partial
    \superb{W}{\ell - 1}\) (respectively); this gives us all the necessary results
    to compute the gradient of the weights in the previous layer. We then rinse, lather,
    and repeat with layer \(\ell - 2\) through the first layer.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了\(\partial \superb{h}{\ell}/\partial \superb{z}{\ell - 1}\)，我们重用2.2和2.3节的结果来计算\(\partial
    \superb{z}{\ell - 1}/\partial \superb{h}{\ell - 1}\)和\(\partial \superb{h}{\ell
    - 1}/ \partial \superb{W}{\ell - 1}\)（分别）；这为我们计算前一层权重梯度提供了所有必要的结果。然后我们用层\(\ell
    - 2\)到第一层重复这个过程。
- en: 2.3 Combining Results with the Chain Rule
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 使用链式法则组合结果
- en: '![](../Images/ddd4201d5226ba07c2ffe7c422934b04.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddd4201d5226ba07c2ffe7c422934b04.png)'
- en: We’ve seen lots of individual derivatives. Ultimately, we really care about
    the derivatives of the loss function with respect to the network’s weights. Let’s
    review by calculating the derivatives of the loss function with respect to the
    weights in the final layer for the familiar network above. Suppose \(\super{f}{2}\)
    is the Sigmoid function and we use the log loss. For \(\superb{W}{2}\) we get
    the following.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了许多单个导数。最终，我们真正关心的是损失函数相对于网络权重的导数。让我们通过计算损失函数相对于上述熟悉网络中最终层权重的导数来回顾一下。假设
    \(\super{f}{2}\) 是 Sigmoid 函数，我们使用对数损失。对于 \(\superb{W}{2}\)，我们得到以下结果。
- en: \[\begin{split} \begin{align*} \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{2}}
    &= \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{2}}\cdot
    \dadb{\super{\mathbf{h}}{2}}{\superb{W}{2}} \\ &=-\left(\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top \cdot \text{diag}\left(\sigma(\superb{h}{2})(1-
    \sigma(\superb{h}{2}))\right)\cdot \mathbf{T} \\ &= -\begin{bmatrix} (\frac{y_1}{\hat{y}_1}
    + \frac{1-y_1}{1-\hat{y}_1})\cdot \sigma(\super{h}{2}_1)(1-\sigma(\super{h}{2}_1))\cdot
    \superb{z}{1} \\ \dots \\ (\frac{y_{n_2}}{\hat{y}_{n_2}} + \frac{1-y_{n_2}}{1-\hat{y}_{n_2}})\cdot
    \sigma(\super{h}{2}_{n_2})(1-\sigma(\super{h}{2}_{n_2}))\cdot \superb{z}{1} \end{bmatrix}
    \in \R^{n_2 \times n_1}, \end{align*} \end{split}\]
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{2}}
    &= \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{2}}\cdot
    \dadb{\super{\mathbf{h}}{2}}{\superb{W}{2}} \\ &=-\left(\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top \cdot \text{diag}\left(\sigma(\superb{h}{2})(1-
    \sigma(\superb{h}{2}))\right)\cdot \mathbf{T} \\ &= -\begin{bmatrix} (\frac{y_1}{\hat{y}_1}
    + \frac{1-y_1}{1-\hat{y}_1})\cdot \sigma(\super{h}{2}_1)(1-\sigma(\super{h}{2}_1))\cdot
    \superb{z}{1} \\ \dots \\ (\frac{y_{n_2}}{\hat{y}_{n_2}} + \frac{1-y_{n_2}}{1-\hat{y}_{n_2}})\cdot
    \sigma(\super{h}{2}_{n_2})(1-\sigma(\super{h}{2}_{n_2}))\cdot \superb{z}{1} \end{bmatrix}
    \in \R^{n_2 \times n_1}, \end{align*} \end{split}\]
- en: where \(\mathbf{T}\) is the tensor derivative discussed in section 2.2.3.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{T}\) 是第 2.2.3 节中讨论的张量导数。
- en: For \(\superb{c}{2}\), we get
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(\superb{c}{2}\)，我们得到
- en: \[\begin{split} \begin{align*} \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{2}}
    &= \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{2}}\cdot
    \dadb{\super{\mathbf{h}}{2}}{\superb{c}{2}} \\ &=-\left(\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top \cdot \text{diag}\left(\sigma(\superb{h}{2})(1-
    \sigma(\superb{h}{2}))\right)\cdot I_{n_2} \\ &= -\begin{bmatrix} (\frac{y_1}{\hat{y}_1}
    + \frac{1-y_1}{1-\hat{y}_1})\cdot \sigma(\super{h}{2}_1)(1-\sigma(\super{h}{2}_1))
    \\ \dots \\ (\frac{y_{n_2}}{\hat{y}_{n_2}} + \frac{1-y_{n_2}}{1-\hat{y}_{n_2}})\cdot
    \sigma(\super{h}{2}_{n_2})(1-\sigma(\super{h}{2}_{n_2})) \end{bmatrix} \in \R^{n_2}.
    \end{align*} \end{split}\]
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{2}}
    &= \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{2}}\cdot
    \dadb{\super{\mathbf{h}}{2}}{\superb{c}{2}} \\ &=-\left(\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top \cdot \text{diag}\left(\sigma(\superb{h}{2})(1-
    \sigma(\superb{h}{2}))\right)\cdot I_{n_2} \\ &= -\begin{bmatrix} (\frac{y_1}{\hat{y}_1}
    + \frac{1-y_1}{1-\hat{y}_1})\cdot \sigma(\super{h}{2}_1)(1-\sigma(\super{h}{2}_1))
    \\ \dots \\ (\frac{y_{n_2}}{\hat{y}_{n_2}} + \frac{1-y_{n_2}}{1-\hat{y}_{n_2}})\cdot
    \sigma(\super{h}{2}_{n_2})(1-\sigma(\super{h}{2}_{n_2})) \end{bmatrix} \in \R^{n_2}.
    \end{align*} \end{split}\]
- en: 3\. Combining Observations
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3. 结合观察
- en: 'So far, we’ve only considered the derivative of the loss function for a *single*
    observation. When training a network, we will of course want to consider the entire
    dataset. One way to do so is to simply add the derivatives of the loss function
    with respect to the weights across observations. Since the loss over the dataset
    is the sum of the individual observation losses and the derivative of a sum is
    the sum of the derivatives, we can simply add the results above. For instance,
    to find the derivative of the loss with respect to the final matrix of weights
    \(\superb{W}{L}\), we could loop through observations and sum the individual derivatives:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了单个观察的损失函数的导数。在训练网络时，我们当然希望考虑整个数据集。这样做的一种方法是将观察到的损失函数相对于权重的导数简单相加。由于数据集上的损失是单个观察损失的总和，而求和的导数是导数的和，因此我们可以简单地将上述结果相加。例如，为了找到相对于最终权重矩阵
    \(\superb{W}{L}\) 的损失函数的导数，我们可以遍历观察并求和单个导数：
- en: \[ \dadb{\mathcal{L}(\{\hat{\by}_n\}_{n = 1}^N))}{\superb{W}{L}} = \dadb{\sumN
    \mathcal{L}(\hat{\by}_n)}{\superb{W} {L}} = \sumN \dadb{\mathcal{L}(\hat{\by}_n)}{\superb{W}{L}}.
    \]
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\{\hat{\by}_n\}_{n = 1}^N))}{\superb{W}{L}} = \dadb{\sumN
    \mathcal{L}(\hat{\by}_n)}{\superb{W} {L}} = \sumN \dadb{\mathcal{L}(\hat{\by}_n)}{\superb{W}{L}}.
    \]
- en: While straightforward, this approach is computationally inefficient. The rest
    of this section outlines a more complicated but *much* faster method.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法简单直接，但在计算上效率不高。本节其余部分概述了一种更复杂但*快得多*的方法。
- en: 3.1 A New Representation
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1 新的表示
- en: So far, we’ve treated our predictors and outputs as vectors. The network starts
    with \(\bx\) and outputs \(\superb{z}{1}\). Then it predicts with \(\superb{z}{1}\)
    and outputs \(\superb{z}{2}\). It repeats this process until \(\superb{z}{L-1}\)
    outputs \(\mathbf{\hat{y}}\). To incorporate multiple observations, we can turn
    these vectors into matrices. Again suppose our dataset consists of \(N\) observations
    with \(\bx_n \in \R^{D_x}\) and \(\by_n \in \R^{D_y}\). We start with \(\bX \in
    \R^{N \times D_x}\), whose \(n^\text{th}\) row is \(\bx_n\). Note that in \(\bX\),
    \(\bx_n\) is a row vector; to keep consistent with our previous sections, we want
    it to be a column vector. So, we’ll work with \(\bX^\top\) rather than \(\bX\).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直将预测器和输出视为向量。网络从 \(\bx\) 开始，输出 \(\superb{z}{1}\)。然后它使用 \(\superb{z}{1}\)
    进行预测并输出 \(\superb{z}{2}\)。它重复这个过程，直到 \(\superb{z}{L-1}\) 输出 \(\mathbf{\hat{y}}\)。为了包含多个观测值，我们可以将这些向量转换为矩阵。再次假设我们的数据集由
    \(N\) 个观测值组成，其中 \(\bx_n \in \R^{D_x}\) 和 \(\by_n \in \R^{D_y}\)。我们开始于 \(\bX \in
    \R^{N \times D_x}\)，其第 \(n\) 行是 \(\bx_n\)。请注意，在 \(\bX\) 中，\(\bx_n\) 是一个行向量；为了与前面的章节保持一致，我们希望它是一个列向量。因此，我们将使用
    \(\bX^\top\) 而不是 \(\bX\)。
- en: '![](../Images/741660f5553d62898ffec20314efc58c.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/741660f5553d62898ffec20314efc58c.png)'
- en: Rather than feeding each observation through the network at once, we will feed
    all observations together and give each observation its own column. Each column
    in \(\bX^\top\) represents an observation’s predictors. We then multiply this
    matrix by \(\superb{W}{1}\) and add \(\superb{c}{1}\) *element-wise* to get \(\superb{H}{1}\).
    Each column in \(\superb{H}{1}\) represents a vector of linear combinations of
    the corresponding column in \(\bX^\top\). We then pass \(\superb{H}{1}\) through
    an activation function to obtain \(\superb{Z}{1}\). Similarly, each column in
    \(\superb{Z}{1}\) represents the output vector for the corresponding observation
    in \(\bX^\top\). We then repeat, with \(\superb{Z}{1}\) acting as the matrix of
    predictors for the next layer. Ultimately, we will obtain a matrix \(\hat{\mathbf{Y}}^\top
    \in \R^{D_y \times N}\) whose \(n^\text{th}\) column represents the vector of
    fitted values for the \(n^\text{th}\) observation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会一次将每个观测值通过网络，而是将所有观测值一起输入，并为每个观测值分配其自己的列。在 \(\bX^\top\) 中的每一列代表一个观测值的预测器。然后我们乘以
    \(\superb{W}{1}\) 并 *逐元素* 加上 \(\superb{c}{1}\) 以得到 \(\superb{H}{1}\)。在 \(\superb{H}{1}\)
    中的每一列代表 \(\bX^\top\) 中对应列的线性组合向量。然后我们通过激活函数传递 \(\superb{H}{1}\) 以获得 \(\superb{Z}{1}\)。同样，在
    \(\superb{Z}{1}\) 中的每一列代表 \(\bX^\top\) 中对应观测值的输出向量。然后我们重复这个过程，其中 \(\superb{Z}{1}\)
    作为下一层的预测器矩阵。最终，我们将获得一个矩阵 \(\hat{\mathbf{Y}}^\top \in \R^{D_y \times N}\)，其第 \(n\)
    列代表第 \(n\) 个观测值的拟合值向量。
- en: 3.2 Gradients
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2 梯度
- en: While this new representation is more efficient, it also makes the gradients
    more complicated since we are taking derivatives with respect to matrices rather
    than vectors. Ordinarily, the derivative of one matrix with respect to another
    would be a four-dimensional tensor. Luckily, there’s a shortcut.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种新的表示方式更高效，但它也使得梯度更加复杂，因为我们是在矩阵而不是向量上求导。通常，一个矩阵相对于另一个矩阵的导数将是一个四维张量。幸运的是，有一个捷径。
- en: For each parameter \(\theta\) in our network, we will find its gradient by asking
    “which parameters does \(\theta\) affect in the next layer”. Supposing the answer
    is some set \(\{\psi_1, \psi_2, \dots, \psi_n\},\) we will calculate the derivative
    of the loss function with respect to \(\theta\) as
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们网络中的每个参数 \(\theta\)，我们将通过询问“\(\theta\) 影响下一层的哪些参数”来找到其梯度。假设答案是某个集合 \(\{\psi_1,
    \psi_2, \dots, \psi_n\}\)，我们将计算损失函数相对于 \(\theta\) 的导数，如下所示
- en: \[ \dadb{\mathcal{L}}{\theta} = \sum_{i = 1}^n \dadb{L}{\psi_i}\cdot \dadb{\psi_i}{\theta}.
    \]
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\theta} = \sum_{i = 1}^n \dadb{L}{\psi_i}\cdot \dadb{\psi_i}{\theta}.
    \]
- en: '![](../Images/93567a4ed40bb43e9a9bdb2977ce6007.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93567a4ed40bb43e9a9bdb2977ce6007.png)'
- en: Recall that our loss function is a vector \(\bf{\mathcal{L}}\) of size \(D_y\)
    since we have \(D_y\) output variables. This loss vector is a *row-wise* function
    of the prediction matrix, \(\hat{\mathbf{Y}}^\top\), meaning the \(d^\text{th}\)
    entry in \(\mathbf{\mathcal{L}}\) is a function of only the \(d^\text{th}\) row
    of \(\hat{\mathbf{Y}}^\top\) (which represents the fitted values for the \(d^\text{th}\)
    output variable across observations). For the \((i, d)^\text{th}\) entry in \(\hat{\mathbf{Y}}^\top\),
    then, we only need to consider the derivative of the \(d^\text{th}\) entry in
    \(\mathbf{\mathcal{L}}\)—the derivative of any other entry in \(\mathcal{L}\)
    with respect \(\hat{\mathbf{Y}}^\top_{i, d}\) is 0\. We can then use the following
    gradient in place of a four-dimensional tensor.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的损失函数 \(\bf{\mathcal{L}}\) 是一个大小为 \(D_y\) 的向量，因为我们有 \(D_y\) 个输出变量。这个损失向量是预测矩阵
    \(\hat{\mathbf{Y}}^\top\) 的 *行向量* 函数，这意味着 \(\mathbf{\mathcal{L}}\) 中的第 \(d^\text{th}\)
    个元素仅是 \(\hat{\mathbf{Y}}^\top\) 的第 \(d^\text{th}\) 行的函数（这代表了第 \(d^\text{th}\)
    个输出变量在观察中的拟合值）。对于 \(\hat{\mathbf{Y}}^\top\) 中的 \((i, d)^\text{th}\) 个元素，我们只需要考虑
    \(\mathbf{\mathcal{L}}\) 中第 \(d^\text{th}\) 个元素的导数——\(\mathcal{L}\) 中任何其他元素相对于
    \(\hat{\mathbf{Y}}^\top_{i, d}\) 的导数是 0。然后我们可以使用以下梯度来代替四维张量。
- en: \[\begin{split} \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top} = \begin{bmatrix}
    \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,1}} & ... & \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,N}}
    \\ & ... & \\ \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,1}} & ... &
    \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,N}}\end{bmatrix} \end{split}\]
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top} = \begin{bmatrix}
    \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,1}} & ... & \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,N}}
    \\ & ... & \\ \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,1}} & ... &
    \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,N}}\end{bmatrix} \end{split}\]
- en: Next, we consider the derivative of \(\mathbf{\hat{Y}}^\top\) with respect to
    \(\superb{H}{L}\). Note that \(\mathbf{\hat{Y}}^\top\) is an *element-wise* function
    of \(\superb{H}{L}\). This means we only need to consider the gradient of each
    element in the former with respect to its corresponding element in the latter.
    This gives us
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们考虑 \(\mathbf{\hat{Y}}^\top\) 对 \(\superb{H}{L}\) 的导数。注意 \(\mathbf{\hat{Y}}^\top\)
    是 \(\superb{H}{L}\) 的 *逐元素* 函数。这意味着我们只需要考虑前者中每个元素相对于后者中相应元素的梯度。这给我们
- en: \[\begin{split} \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}} = \begin{bmatrix}
    \dadb{\mathbf{\hat{Y}}^\top_{1,1}}{\superb{H}{L}_{1,1}} & ... & \dadb{\mathbf{\hat{Y}}^\top_{1,N}}{\superb{H}{L}_{1,N}}
    \\ & ... & \\ \dadb{\mathbf{\hat{Y}}^\top_{D_y,1}}{\superb{H}{1}_{D_y,1}} & ...
    & \dadb{\mathbf{\hat{Y}}^\top_{D_y,N}}{\superb{L}{L}_{D_y,N}} \end{bmatrix}. \end{split}\]
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}} = \begin{bmatrix}
    \dadb{\mathbf{\hat{Y}}^\top_{1,1}}{\superb{H}{L}_{1,1}} & ... & \dadb{\mathbf{\hat{Y}}^\top_{1,N}}{\superb{H}{L}_{1,N}}
    \\ & ... & \\ \dadb{\mathbf{\hat{Y}}^\top_{D_y,1}}{\superb{H}{1}_{D_y,1}} & ...
    & \dadb{\mathbf{\hat{Y}}^\top_{D_y,N}}{\superb{L}{L}_{D_y,N}} \end{bmatrix}. \end{split}\]
- en: Now let’s use the shortcut described above. Since each element in \(\superb{H}{L}\)
    only affects the corresponding element in \(\mathbf{\hat{Y}}^\top\), we calculate
    \(\partial \mathcal{L}/\partial \superb{H}{L}\) by multiplying the two gradients
    above *element-wise*. I.e.,
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用上面描述的快捷方式。由于 \(\superb{H}{L}\) 中的每个元素只影响 \(\mathbf{\hat{Y}}^\top\) 中的相应元素，我们通过逐元素乘以上述两个梯度来计算
    \(\partial \mathcal{L}/\partial \superb{H}{L}\)。即，
- en: \[ \dadb{\mathcal{L}}{\superb{H}{L}} = \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top}
    \circ \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}}, \]
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\superb{H}{L}} = \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top}
    \circ \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}}, \]
- en: where \(\circ\) is the element-wise multiplication operator, also known as the
    *Hadamard product*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\circ\) 是逐元素乘法运算符，也称为 *Hadamard 积*。
- en: Next up is \(\superb{c}{L}\). Whereas each element in \(\superb{H}{L}\) affected
    only one element in \(\mathbf{\hat{Y}}^\top\), each element in \(\superb{c}{L}\)
    affects \(N\) elements in \(\superb{H}{L}\)—every element in its corresponding
    row. Consider the first entry in \(\superb{c}{L}\). Since this entry affects each
    entry in the first row of \(\superb{H}{L}\), the chain rule gives us
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 \(\superb{c}{L}\)。在 \(\superb{H}{L}\) 中，每个元素只影响 \(\mathbf{\hat{Y}}^\top\)
    中的一个元素，而在 \(\superb{c}{L}\) 中，每个元素影响 \(\superb{H}{L}\) 中的 \(N\) 个元素——其对应行的每个元素。考虑
    \(\superb{c}{L}\) 中的第一个元素。由于这个元素影响 \(\superb{H}{L}\) 第一行的每个元素，链式法则给出
- en: \[ \dadb{\mathcal{L}}{\super{c}{L}_1} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{1,n}}\cdot\dadb{\superb{H}{L}_{1,n}}{\super{c}{L}_1}.
    \]
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\super{c}{L}_1} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{1,n}}\cdot\dadb{\superb{H}{L}_{1,n}}{\super{c}{L}_1}.
    \]
- en: Fortunately \(\partial \superb{H}{L}_{1,n}/\partial \super{c}{L}_1\) is just
    1 since \(\super{c}{L}_1\) is an intercept term. This implies that the derivative
    of the loss function with respect to \(\superb{c}{1}\) is just the row sum of
    \(\partial\mathcal{L}/\partial \superb{H}{L}\), or
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，\(\partial \superb{H}{L}_{1,n}/\partial \super{c}{L}_1\) 等于 1，因为 \(\super{c}{L}_1\)
    是一个截距项。这意味着损失函数相对于 \(\superb{c}{1}\) 的导数仅仅是 \(\partial\mathcal{L}/\partial \superb{H}{L}\)
    的行和，或者说
- en: \[ \dadb{\mathcal{L}}{\super{c}{L}_i} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,n}}.
    \]
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\super{c}{L}_i} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,n}}.
    \]
- en: Next, we have \(\superb{W}{L}\). Using our shortcut, we ask “which values does
    the \((i, j)^\text{th}\) entry in \(\superb{W}{L}\) affect?” Since \(\superb{H}{L}
    = \superb{W}{L}\superb{Z}{L-1}\), we have that
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有 \(\superb{W}{L}\)。使用我们的快捷方式，我们询问“\(\superb{W}{L}\) 的 \((i, j)^\text{th}\)
    个条目影响哪些值？”由于 \(\superb{H}{L} = \superb{W}{L}\superb{Z}{L-1}\)，我们有
- en: \[ \superb{H}{L}_{i,n} = \superb{W}{L}_{i, j} \superb{Z}{L-1}_{j,n} \hspace{1mm}
    \forall \hspace{1mm} n \in \{1, \dots, N\}. \]
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \superb{H}{L}_{i,n} = \superb{W}{L}_{i, j} \superb{Z}{L-1}_{j,n} \hspace{1mm}
    \forall \hspace{1mm} n \in \{1, \dots, N\}. \]
- en: This tells us that \(\superb{W}{L}_{i,j}\) affects each entry in the \(i^\text{th}\)
    row of \(\superb{H}{L}\) and gives us the derivative \(\partial{\superb{H}{L}_{i,
    n}}/\partial \superb{W}{L}_{i, j} = \superb{Z}{L-1}_{j, n}.\) Therefore,
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们 \(\superb{W}{L}_{i,j}\) 影响到 \(\superb{H}{L}\) 的第 \(i\) 行的每个条目，并给出导数 \(\partial{\superb{H}{L}_{i,
    n}}/\partial \superb{W}{L}_{i, j} = \superb{Z}{L-1}_{j, n}.\) 因此，
- en: \[ \dadb{\mathcal{L}}{\superb{W}{L}_{i, j}} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,
    n}}\cdot\dadb{\superb{H}{L}_{i, n}}{\superb{W}{L}_{i, j}} = \sumN (\nabla \superb{H}{L})_{i,
    n}\cdot{\superb{Z}{L-1}_{j,n}}, \]
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\superb{W}{L}_{i, j}} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,
    n}}\cdot\dadb{\superb{H}{L}_{i, n}}{\superb{W}{L}_{i, j}} = \sumN (\nabla \superb{H}{L})_{i,
    n}\cdot{\superb{Z}{L-1}_{j,n}}, \]
- en: where \(\nabla \superb{H}{L}\) is the matrix representing \(\partial \mathcal{L}/\partial\superb{H}{L}\).
    This can be computed for each element in \(\superb{W}{L}\) using a tensor dot
    product, which will be covered in the construction section.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，\(\nabla \superb{H}{L}\) 是表示 \(\partial \mathcal{L}/\partial\superb{H}{L}\)
    的矩阵。这可以通过使用张量点积为 \(\superb{W}{L}\) 中的每个元素计算，这将在构建部分中介绍。
- en: Finally, we have \(\superb{Z}{L-1}\). This case is symmetric to \(\superb{W}{L}\),
    and the same approach gives us the result
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到 \(\superb{Z}{L-1}\)。这种情况与 \(\superb{W}{L}\) 对称，采用相同的方法可以得到结果
- en: \[ \dadb{\mathcal{L}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R \dadb{\mathcal{L}}{\superb{H}{L}_{r,n}}\cdot\dadb{\superb{H}{L}_{r,
    n}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R {(\nabla \superb{H}{L})}_{r, n}\cdot{\superb{W}{L}_{r,i}}.
    \]
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R \dadb{\mathcal{L}}{\superb{H}{L}_{r,n}}\cdot\dadb{\superb{H}{L}_{r,
    n}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R {(\nabla \superb{H}{L})}_{r, n}\cdot{\superb{W}{L}_{r,i}}.
    \]
- en: Again, the derivative for all of \(\superb{Z}{L-1}\) can be calculated at once
    using a tensor dot product.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，所有 \(\superb{Z}{L-1}\) 的导数可以使用张量点积一次计算出来。
- en: 1\. Model Structure
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 模型结构
- en: Throughout this chapter, suppose we have training data \(\{\bx_n, \by_n\}_{n
    = 1}^N\) with \(\bx_n \in \R^{D_x}\)—which does *not* include an intercept term—and
    \(\by_n \in \R^{D_y}\) for \(n = 1, 2, \dots, N\). In other words, for each observation
    we have \(D_x\) predictors and \(D_y\) target variables. In this chapter, these
    will primarily be referred to as the *input* and *output* variables, respectively.
    Note that unlike in previous chapters, we might now have a *vector* of target
    variables rather than a single value. If there is only one target variable per
    observation (i.e. \(D_y = 1\)), we will write it as \(y_n\) rather than \(\by_n\).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，假设我们拥有训练数据 \(\{\bx_n, \by_n\}_{n = 1}^N\)，其中 \(\bx_n \in \R^{D_x}\)——这*不*包括截距项，并且
    \(\by_n \in \R^{D_y}\) 对于 \(n = 1, 2, \dots, N\)。换句话说，对于每个观测值，我们有 \(D_x\) 个预测变量和
    \(D_y\) 个目标变量。在本章中，这些将主要被称为*输入*和*输出*变量，分别。请注意，与前面的章节不同，我们可能现在有一个*向量*的目标变量而不是单个值。如果每个观测值只有一个目标变量（即
    \(D_y = 1\)），我们将将其写作 \(y_n\) 而不是 \(\by_n\)。
- en: 1.1 An Overview
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 概述
- en: The diagram below is a helpful representation of a basic neural network. Neural
    networks operate in layers. The network starts of with an *input layer*, consisting
    of the vector of predictors for a single observation. This is shown by \(x_0,
    \dots, x_3\) in the diagram (indicating that \(D_x = 4\) here). The network then
    passes through one or more *hidden layers*. The first hidden layer is a function
    of the input layer and each following hidden layer is a function of the last.
    (We will discuss these functions in more detail later). The network below has
    two hidden layers. Finally, the network passes from the last hidden layer into
    an *output layer*, representing the target variable or variables. In the network
    below, the target variable is two-dimensional (i.e. \(D_y = 2\)), so the layer
    is represented by the values \(y_0\) and \(y_1\).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图是表示基本神经网络的有助于理解的表达方式。神经网络在层中操作。网络从*输入层*开始，由单个观察的预测因子向量组成。这在图中由 \(x_0, \dots,
    x_3\) 表示（表示这里 \(D_x = 4\)）。然后网络通过一个或多个*隐藏层*。第一隐藏层是输入层的函数，每个后续的隐藏层是前一个隐藏层的函数。（我们将在稍后详细讨论这些函数）。下面的网络有两个隐藏层。最后，网络从最后一个隐藏层进入*输出层*，表示目标变量或变量。在下面的网络中，目标变量是二维的（即
    \(D_y = 2\)），所以该层由 \(y_0\) 和 \(y_1\) 的值表示。
- en: Note
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Diagrams like the one below are commonly used to represent neural networks.
    Note that these diagrams show only a single observation at a time. For instance,
    \(x_0, \dots x_3\) represent four predictors within one observation, rather than
    four different observations.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 像下面的图这样的图通常用来表示神经网络。请注意，这些图一次只显示一个观察。例如，\(x_0, \dots x_3\) 代表一个观察中的四个预测因子，而不是四个不同的观察。
- en: '![](../Images/58b930168130f70132b7bfa0a2bc80f4.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/58b930168130f70132b7bfa0a2bc80f4.png)'
- en: Each layer in a neural network consists of *neurons*, represented by the circles
    in the diagram above. Neurons are simply scalar values. In the *input layer*,
    each neuron represents a single predictor. In the above diagram, the input layer
    has four neurons, labeled \(x_0\) through \(x_3\), each representing a single
    predictor. The neurons in the input layer then determine the neurons in the first
    hidden layer, labeled \(\super{z}{1}_0\) through \(\super{z}{1}_2\). We will discuss
    *how* shortly, but for now simply note the lines running from the input layer’s
    neurons to the first hidden layer’s neurons in the diagram above. Once the neurons
    in the first hidden layer are set, they become predictors for the next layer,
    acting just as the input layer did. When the neurons in the final hidden layer
    are fixed, they act as predictors for the output layer.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的每一层都由*神经元*组成，如图上所示的圆圈。神经元仅仅是标量值。在*输入层*中，每个神经元代表一个单独的预测因子。在上面的图中，输入层有四个神经元，分别标记为
    \(x_0\) 到 \(x_3\)，每个代表一个单独的预测因子。输入层的神经元随后决定了第一隐藏层的神经元，分别标记为 \(\super{z}{1}_0\)
    到 \(\super{z}{1}_2\)。我们将在稍后讨论*如何*，但现在只需注意上图上从输入层的神经元到第一隐藏层神经元的线条。一旦第一隐藏层的神经元被设定，它们就成为了下一层的预测因子，作用就像输入层一样。当最终隐藏层的神经元被固定时，它们作为输出层的预测因子。
- en: One natural question is how many layers our neural network should contain. There
    is no single answer to this question, as the number of layers is chosen by the
    modeler. Any true neural network will have an input layer, an output layer, and
    at least one hidden layer. The network above has two hidden layers. Note that
    the superscript indicates the hidden layer number, e.g. \(z_{0}^{(1)}\) through
    \(z_2^{(1)}\) are in the first hidden layer and \(z_{0}^{(2)}\) through \(z_2^{(2)}\)
    are in the second hidden layer. We could also consider the input layer as an exogenous
    “hidden layer” and represent it with \(z_{0}^{(0)}\) through \(z_3^{(0)}\).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的问题是我们的人工神经网络应该包含多少层。这个问题没有单一的答案，因为层数是由模型选择决定的。任何真正的神经网络都将包含一个输入层、一个输出层和至少一个隐藏层。上面的网络有两个隐藏层。请注意，上标表示隐藏层的编号，例如
    \(z_{0}^{(1)}\) 到 \(z_2^{(1)}\) 在第一隐藏层，而 \(z_{0}^{(2)}\) 到 \(z_2^{(2)}\) 在第二隐藏层。我们也可以将输入层视为一个外生的“隐藏层”，并用
    \(z_{0}^{(0)}\) 到 \(z_3^{(0)}\) 来表示它。
- en: Another natural question is how many neurons each layer should contain. This
    is in part chosen by the modeler and in part predetermined. If our predictor vectors
    are of length \(D\), the input layer must have \(D\) neurons. Similarly, the output
    layer must have as many neurons as there are target variables. If, for instance,
    our model attempts to predict a store’s revenue and its costs (two targets) in
    a given month, our output layer must have two neurons. The sizes of the hidden
    layers, however, are chosen by the modeler. Too few neurons may cause underfitting
    by preventing the network from picking up on important patterns in the data while
    too many neurons may cause overfitting, allowing the network to fit parameters
    that match the training data exactly.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个自然的问题是每一层应该包含多少个神经元。这在一定程度上由模型设计者选择，在某种程度上是预先确定的。如果我们的预测向量长度为 \(D\)，则输入层必须包含
    \(D\) 个神经元。同样，输出层必须包含与目标变量数量相等的神经元。例如，如果我们的模型试图预测某个月份商店的收入和成本（两个目标），则输出层必须有两个神经元。然而，隐藏层的尺寸由模型设计者选择。神经元太少可能会导致欠拟合，因为网络无法从数据中捕捉到重要的模式，而神经元太多可能会导致过拟合，允许网络拟合与训练数据完全匹配的参数。
- en: 1.2 Communication between Layers
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 层之间的通信
- en: Let’s now turn to the process through which one layer communicates with the
    next. In this section, let \(\bz^{(a)}\) and \(\super{\bz}{b}\) represent the
    vector of neurons in any two consecutive layers. For instance, \(\super{\bz}{a}\)
    might be an input layer and \(\super{\bz}{b}\) the first hidden layer or \(\super{\bz}{a}\)
    might be a hidden layer and \(\super{\bz}{b}\) the following hidden layer. Suppose
    \(\super{\bz}{a} \in \R^{D_a}\) and \(\super{\bz}{b} \in \R^{D_b}\).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向一层与下一层之间通信的过程。在本节中，让 \(\bz^{(a)}\) 和 \(\super{\bz}{b}\) 代表任意两个连续层中的神经元向量。例如，\(\super{\bz}{a}\)
    可能是一个输入层，而 \(\super{\bz}{b}\) 是第一个隐藏层，或者 \(\super{\bz}{a}\) 可能是一个隐藏层，而 \(\super{\bz}{b}\)
    是随后的隐藏层。假设 \(\super{\bz}{a} \in \R^{D_a}\) 和 \(\super{\bz}{b} \in \R^{D_b}\)。
- en: 'In a feed-forward neural network, each neuron in \(\super{\bz}{b}\) is a function
    of every neuron in \(\super{\bz}{a}\). This function occurs in two stages: first
    a linear mapping of \(\super{\bz}{a}\) onto one dimension, then a nonlinear function
    called an *activation function*. Let’s look at a single neuron within \(\super{\bz}{b}\),
    \(\super{z}{b}_i\). The transformation from \(\super{\bz}{a}\) to \(\super{z}{b}_i\)
    takes the form'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在前馈神经网络中，\(\super{\bz}{b}\) 中的每个神经元都是 \(\super{\bz}{a}\) 中每个神经元的函数。这个函数分为两个阶段：首先将
    \(\super{\bz}{a}\) 线性映射到一维，然后是一个称为 *激活函数* 的非线性函数。让我们看看 \(\super{\bz}{b}\) 中的一个单个神经元，\(\super{z}{b}_i\)。从
    \(\super{\bz}{a}\) 到 \(\super{z}{b}_i\) 的转换形式如下
- en: \[\begin{split} \begin{align*} \super{h}{b}_i &= \bw_i^\top\super{\bz}{a} +
    c_i \\ \super{z}{b}_i &= f(\super{h}{b}_i), \end{align*} \end{split}\]
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{h}{b}_i &= \bw_i^\top\super{\bz}{a} +
    c_i \\ \super{z}{b}_i &= f(\super{h}{b}_i), \end{align*} \end{split}\]
- en: where \(\bw_i \in \R^{D_a}\) is a vector of weights, \(c_i\) is a constant intercept
    term, and \(f()\) is an activation function. Note that \(\bw_i\) and \(c_i\) are
    specific to the \(i^\text{th}\) neuron in \(\super{\bz}{b}\) while \(f()\) is
    typically common among all neurons in \(\super{\bz}{b}\). We can also write the
    function relating the two layers in matrix form, as below.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\bw_i \in \R^{D_a}\) 是权重向量，\(c_i\) 是一个常数截距项，\(f()\) 是一个激活函数。请注意，\(\bw_i\)
    和 \(c_i\) 是 \(\super{\bz}{b}\) 中第 \(i^\text{th}\) 个神经元的特定值，而 \(f()\) 通常在 \(\super{\bz}{b}\)
    中所有神经元中是通用的。我们还可以用矩阵形式写出两个层之间关系的函数，如下所示。
- en: \[\begin{split} \begin{align*} \super{\mathbf{h}}{b} &= \mathbf{W}\super{\bz}{a}
    + \mathbf{c} \\\ \super{\mathbf{z}}{b} &= f(\super{\mathbf{h}}{b}), \end{align*}
    \end{split}\]
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\mathbf{h}}{b} &= \mathbf{W}\super{\bz}{a}
    + \mathbf{c} \\\ \super{\mathbf{z}}{b} &= f(\super{\mathbf{h}}{b}), \end{align*}
    \end{split}\]
- en: where \(\mathbf{W} \in \R^{D_b \times D_a}\), \(\mathbf{c} \in \R^{D_b}\) and
    \(f()\) is applied element-wise.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{W} \in \R^{D_b \times D_a}\)，\(\mathbf{c} \in \R^{D_b}\) 且 \(f()\)
    是逐元素应用。
- en: Note
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that we haven’t yet discussed *how* \(\mathbf{W}\), \(\mathbf{c}\) or \(f()\)
    are determined. For now, consider these all to be fixed and focus on the structure
    of a network. *How* we determine these values is discussed in the optimization
    section below.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们尚未讨论 \(\mathbf{W}\)，\(\mathbf{c}\) 或 \(f()\) 的确定方法。现在，让我们假设这些都被固定，并关注网络的结构。我们如何确定这些值将在下面的优化部分讨论。
- en: Once \(\super{\bz}{b}\) is fixed, we use the same process to create the next
    layer, \(\super{\bz}{c}\). When discussing many layers at a time, it is helpful
    to add superscripts to \(\mathbf{W}, \mathbf{c}\), and \(f()\) to indicate the
    layer. We can write the transmission of \(\super{\bz}{a}\) to \(\super{\bz}{b}\)
    followed by \(\super{\bz}{b}\) to \(\super{\bz}{c}\) as
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 \(\super{\bz}{b}\) 被固定，我们就使用相同的过程来创建下一层，\(\super{\bz}{c}\)。当我们同时讨论许多层时，给
    \(\mathbf{W}, \mathbf{c}\) 和 \(f()\) 添加上标是有帮助的，以表示层。我们可以将 \(\super{\bz}{a}\) 到
    \(\super{\bz}{b}\) 的传输以及随后 \(\super{\bz}{b}\) 到 \(\super{\bz}{c}\) 的传输写成
- en: \[\begin{split} \begin{align*} \super{\bz}{b} &= \super{f}{b}\left(\super{\mathbf{W}}{b}\super{\bz}{a}
    + \super{\mathbf{c}}{b} \right) \\ \super{\bz}{c} &= \super{f}{c}\left(\super{\mathbf{W}}{c}\super{\bz}{b}
    + \super{\mathbf{c}}{c} \right). \\ \end{align*} \end{split}\]
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\bz}{b} &= \super{f}{b}\left(\super{\mathbf{W}}{b}\super{\bz}{a}
    + \super{\mathbf{c}}{b} \right) \\ \super{\bz}{c} &= \super{f}{c}\left(\super{\mathbf{W}}{c}\super{\bz}{b}
    + \super{\mathbf{c}}{c} \right). \\ \end{align*} \end{split}\]
- en: A more mathematical representation of a neural network is given below. The network
    starts with a vector of predictors \(\bx\). This vector is then multiplied by
    \(\super{\mathbf{W}}{1}\) and added to \(\super{\mathbf{c}}{1}\), which sums to
    \(\super{\mathbf{h}}{1}\). We then apply an activation \(\super{f}{1}\) to \(\super{\mathbf{h}}{1}\),
    which results in our single hidden layer, \(\super{\mathbf{z}}{1}\). The same
    process is then applied to \(\super{\bz}{1}\), which results in our output vector,
    \(\by\).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 下面给出了神经网络的更数学化的表示。网络从预测器的向量 \(\bx\) 开始。然后这个向量被乘以 \(\super{\mathbf{W}}{1}\) 并加上
    \(\super{\mathbf{c}}{1}\)，总和为 \(\super{\mathbf{h}}{1}\)。然后我们对 \(\super{\mathbf{h}}{1}\)
    应用激活 \(\super{f}{1}\)，从而得到我们的单个隐藏层，\(\super{\mathbf{z}}{1}\)。然后对 \(\super{\bz}{1}\)
    应用相同的过程，从而得到我们的输出向量，\(\by\)。
- en: '![](../Images/ddd4201d5226ba07c2ffe7c422934b04.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddd4201d5226ba07c2ffe7c422934b04.png)'
- en: 1.3 Activation Functions
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 激活函数
- en: As we have seen, we create a neuron in one layer by taking a linear mapping
    of the neurons in the previous layer and then applying some *activation function*.
    What exactly is this activation function? An activation function is a (typically)
    nonlinear function that allows the network to learn complex relationships between
    the predictor(s) and the target variable(s).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，我们通过将前一层神经元的线性映射并应用某种**激活函数**来创建一个单层神经元。那么这个激活函数究竟是什么呢？激活函数是一个（通常是）非线性函数，它允许网络学习预测器（们）和目标变量（们）之间的复杂关系。
- en: Suppose, for instance, the relationship between a target variable \(y_n\) and
    a predictor \(x_n\) is given by
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，例如，目标变量 \(y_n\) 和预测器 \(x_n\) 之间的关系由以下公式给出
- en: \[ y_n = |x_n| + \epsilon_n, \]
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y_n = |x_n| + \epsilon_n, \]
- en: where \(\epsilon_n\) is a noise term. Despite its simplicity, this relationship
    cannot be accurately fit by a linear model.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\epsilon_n\) 是一个噪声项。尽管这个关系很简单，但它不能被线性模型准确拟合。
- en: '![](../Images/ec9dd141265cd1ab9d61d344f2d31699.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec9dd141265cd1ab9d61d344f2d31699.png)'
- en: Ideally, we would apply some function to the predictor and use a different model
    depending on the result of this function. In the case above, \(x_n > 0\) would
    “activate” the model \(y_n \approx x_n\), and \(x_n \leq 0\) would “activate”
    the model \(y_n \approx -x_n\). Hence the name “activation function”.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们会将某个函数应用于预测器，并根据这个函数的结果使用不同的模型。在上面的例子中，如果 \(x_n > 0\)，则会“激活”模型 \(y_n
    \approx x_n\)，而如果 \(x_n \leq 0\)，则会“激活”模型 \(y_n \approx -x_n\)。因此得名“激活函数”。
- en: 'There are many commonly used activation functions, and deciding which function
    to use is a major consideration in modeling a neural network. Here we will limit
    our discussion to two of the most common functions: the ReLU (Rectified Linear
    Unit) and sigmoid functions. The linear activation function (which is really the
    absence of an activation function) is also discussed.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多常用的激活函数，决定使用哪个函数是构建神经网络时的一个重要考虑因素。在这里，我们将仅讨论两种最常见的函数：ReLU（修正线性单元）和sigmoid函数。线性激活函数（实际上是没有激活函数）也被讨论。
- en: ReLU
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReLU
- en: '![](../Images/a2c490c62bae3864640ecdbee382bd4a.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2c490c62bae3864640ecdbee382bd4a.png)'
- en: ReLU is a simple yet extremely common activation function. It is defined as
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 是一个简单但极其常见的激活函数。它定义为
- en: \[ f(x) = \text{max}(x, 0). \]
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = \text{max}(x, 0). \]
- en: How can such a simple function benefit a neural network? ReLU acts like a switch,
    selectively turning channels on and off. Consider fitting a neural network to
    the dataset above generated with \(y_n = |x_n| + \epsilon_n\). Let’s use a very
    simple network represented by the diagram below. This network has one predictor,
    a single hidden layer with two neurons, and one output variable.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一个简单的函数如何使神经网络受益？ReLU就像一个开关，选择性地打开和关闭通道。考虑将一个神经网络拟合到上面生成的数据集，其中 \(y_n = |x_n|
    + \epsilon_n\)。让我们使用以下图表表示的非常简单的网络。这个网络有一个预测器，一个包含两个神经元的单个隐藏层和一个输出变量。
- en: '![](../Images/7956d5c1c598913c656afdc0767e7ee1.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7956d5c1c598913c656afdc0767e7ee1.png)'
- en: 'Now let’s say we decide to use \(f(\bx) = \text{ReLU}(\bx)\) and we land on
    the following parameters:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们决定使用 \(f(\bx) = \text{ReLU}(\bx)\) 并得到以下参数：
- en: \[\begin{split} \super{\mathbf{W}}{1} = \begin{pmatrix} 1 \\ -1 \end{pmatrix},
    \hspace{1mm} \super{\mathbf{c}}{1} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \hspace{1mm}
    \super{\mathbf{W}}{2} = \begin{pmatrix} 1 & 1 \end{pmatrix}, \hspace{1mm} \mathbf{c}^{(2)}
    = 0. \end{split}\]
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \super{\mathbf{W}}{1} = \begin{pmatrix} 1 \\ -1 \end{pmatrix},
    \hspace{1mm} \super{\mathbf{c}}{1} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \hspace{1mm}
    \super{\mathbf{W}}{2} = \begin{pmatrix} 1 & 1 \end{pmatrix}, \hspace{1mm} \mathbf{c}^{(2)}
    = 0. \end{split}\]
- en: This is equivalent to the following complete model
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于以下完整模型
- en: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    1 \\ -1 \end{pmatrix} x \right) \\ y &= \begin{pmatrix} 1 & 1 \end{pmatrix} \super{\bz}{1}.
    \end{align*} \end{split}\]
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    1 \\ -1 \end{pmatrix} x \right) \\ y &= \begin{pmatrix} 1 & 1 \end{pmatrix} \super{\bz}{1}.
    \end{align*} \end{split}\]
- en: Will this model be able to fit our dataset? Suppose \(x_n = c\) for some *positive*
    constant \(c\). We will then get
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型能否拟合我们的数据集？假设 \(x_n = c\) 对于某个正的常数 \(c\)。那么我们将得到
- en: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    c \\ -c \end{pmatrix} \right) = \begin{pmatrix} c \\ 0 \end{pmatrix} \\ y &= \begin{pmatrix}
    1 & 1 \end{pmatrix} \begin{pmatrix} c \\ 0 \end{pmatrix} = c. \end{align*} \end{split}\]
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    c \\ -c \end{pmatrix} \right) = \begin{pmatrix} c \\ 0 \end{pmatrix} \\ y &= \begin{pmatrix}
    1 & 1 \end{pmatrix} \begin{pmatrix} c \\ 0 \end{pmatrix} = c. \end{align*} \end{split}\]
- en: 'So we will predict \(y_n = |x_n| = c\), a sensible result! Similarly, if \(x_n
    = -c\), we would again obtain the valid prediction \(y_n = |x_n| = c\). ReLU is
    able to achieve this result by activating a different channel depending on the
    value of \(x_n\): if \(x_n\) is greater than 0, it activates \(y_n = x_n\), and
    if \(x_n\) is less than 0, it activates \(y_n = -x_n\).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将预测 \(y_n = |x_n| = c\)，这是一个合理的预测结果！同样地，如果 \(x_n = -c\)，我们也会得到有效的预测 \(y_n
    = |x_n| = c\)。ReLU能够通过根据 \(x_n\) 的值激活不同的通道来实现这个结果：如果 \(x_n\) 大于 0，它激活 \(y_n =
    x_n\)；如果 \(x_n\) 小于 0，它激活 \(y_n = -x_n\)。
- en: 'As we will see in the next section, fitting a neural network consists of taking
    gradients of our activation functions. Fortunately ReLU has a straightforward
    derivative:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们将在下一节中看到的，拟合神经网络包括计算激活函数的梯度。幸运的是，ReLU有一个直接的导数：
- en: \[\begin{split} \frac{\partial}{\partial x} \text{ReLU}(x) = \begin{cases} 1,
    & x > 0 \\ 0, & x \leq 0\. \end{cases} \end{split}\]
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \frac{\partial}{\partial x} \text{ReLU}(x) = \begin{cases} 1,
    & x > 0 \\ 0, & x \leq 0\. \end{cases} \end{split}\]
- en: Note that this derivative is not technically defined at 0\. In practice, it
    is very unlikely that we will be applying an activation function to 0 *exactly*,
    though in that case the convention is to set its derivative equal to 0.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个导数在 0 处技术上是没有定义的。在实践中，我们很少会精确地将激活函数应用于 0，尽管在这种情况下，惯例是将它的导数设为 0。
- en: Sigmoid
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Sigmoid
- en: '![](../Images/f99d9f109382dc57dff15439e009ffe8.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f99d9f109382dc57dff15439e009ffe8.png)'
- en: A second common activation function is the *logistic sigmoid function*, often
    referred to as just *the sigmoid function*. This function was introduced in [chapter
    3](../c3/s1/logistic_regression.html) in the context of the logistic regression.
    The sigmoid function is defined as
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种常见的激活函数是 *对数逻辑sigmoid函数*，通常简称为 *sigmoid函数*。这个函数在[第3章](../c3/s1/logistic_regression.html)中，在逻辑回归的背景下被引入。sigmoid函数定义为
- en: \[ \sigma(x) = \frac{1}{1 + \exp(-x)}. \]
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma(x) = \frac{1}{1 + \exp(-x)}. \]
- en: Note that the sigmoid function takes any real value and returns a value between
    0 and 1\. As a result, the sigmoid function is commonly applied to the last hidden
    layer in a network in order to return a probability estimate in the output layer.
    This makes it common in classification problems.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，sigmoid函数接受任何实数值并返回介于0和1之间的值。因此，sigmoid函数通常应用于网络的最后一隐藏层，以便在输出层返回概率估计。这使得它在分类问题中很常见。
- en: As we saw in chapter 3, a convenient fact about the sigmoid function is that
    we can express its derivative in terms of itself.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第3章中看到的，sigmoid函数的一个方便的事实是，我们可以用自身来表示其导数。
- en: \[ \dadb{\sigma(x)}{x} = \frac{\exp(-x)}{\left( 1 + \exp(-x) \right)^2} = \frac{1}{1
    + \exp(-x)} \cdot \frac{\exp(-x)}{1 + \exp(-x)} = \sigma(x)\left(1 - \sigma(x)\right).
    \]
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\sigma(x)}{x} = \frac{\exp(-x)}{\left( 1 + \exp(-x) \right)^2} = \frac{1}{1
    + \exp(-x)} \cdot \frac{\exp(-x)}{1 + \exp(-x)} = \sigma(x)\left(1 - \sigma(x)\right).
    \]
- en: The Linear Activation Function
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线性激活函数
- en: Another possible activation function is the “linear” activation function, which
    is the same as skipping the activation function altogether. The linear activation
    function simply returns its input. It is defined with
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能的激活函数是“线性”激活函数，它等同于完全跳过激活函数。线性激活函数简单地返回其输入。它定义为
- en: \[ f(x) = x, \]
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = x, \]
- en: and has derivative
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 并且有导数
- en: \[ \dadb{f(x)}{x} = 1\. \]
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{f(x)}{x} = 1\. \]
- en: The linear activation function is often used before the last layer in a neural
    network for regression. Rather than constraining the fitted values to be in some
    range or setting half of them equal to 0, we want to leave them as they are.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 线性激活函数通常在神经网络回归的最后一层之前使用。我们不想将拟合值限制在某个范围内或设置其中一半等于0，而是希望保持它们原样。
- en: 1.1 An Overview
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 概述
- en: The diagram below is a helpful representation of a basic neural network. Neural
    networks operate in layers. The network starts of with an *input layer*, consisting
    of the vector of predictors for a single observation. This is shown by \(x_0,
    \dots, x_3\) in the diagram (indicating that \(D_x = 4\) here). The network then
    passes through one or more *hidden layers*. The first hidden layer is a function
    of the input layer and each following hidden layer is a function of the last.
    (We will discuss these functions in more detail later). The network below has
    two hidden layers. Finally, the network passes from the last hidden layer into
    an *output layer*, representing the target variable or variables. In the network
    below, the target variable is two-dimensional (i.e. \(D_y = 2\)), so the layer
    is represented by the values \(y_0\) and \(y_1\).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示有助于理解基本神经网络。神经网络在层中操作。网络从*输入层*开始，该层由单个观察值的预测向量组成。图中用 \(x_0, \dots, x_3\)
    表示（表示此处 \(D_x = 4\)）。然后网络通过一个或多个*隐藏层*。第一个隐藏层是输入层的函数，每个后续的隐藏层是前一个隐藏层的函数。（我们将在后面更详细地讨论这些函数）。下面的网络有两个隐藏层。最后，网络从最后一个隐藏层传递到*输出层*，表示目标变量或变量。在下面的网络中，目标变量是二维的（即
    \(D_y = 2\)），因此该层由 \(y_0\) 和 \(y_1\) 的值表示。
- en: Note
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Diagrams like the one below are commonly used to represent neural networks.
    Note that these diagrams show only a single observation at a time. For instance,
    \(x_0, \dots x_3\) represent four predictors within one observation, rather than
    four different observations.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 像下面这样的图示通常用来表示神经网络。请注意，这些图示一次只显示一个观察值。例如，\(x_0, \dots x_3\) 代表一个观察值内的四个预测因子，而不是四个不同的观察值。
- en: '![](../Images/58b930168130f70132b7bfa0a2bc80f4.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58b930168130f70132b7bfa0a2bc80f4.png)'
- en: Each layer in a neural network consists of *neurons*, represented by the circles
    in the diagram above. Neurons are simply scalar values. In the *input layer*,
    each neuron represents a single predictor. In the above diagram, the input layer
    has four neurons, labeled \(x_0\) through \(x_3\), each representing a single
    predictor. The neurons in the input layer then determine the neurons in the first
    hidden layer, labeled \(\super{z}{1}_0\) through \(\super{z}{1}_2\). We will discuss
    *how* shortly, but for now simply note the lines running from the input layer’s
    neurons to the first hidden layer’s neurons in the diagram above. Once the neurons
    in the first hidden layer are set, they become predictors for the next layer,
    acting just as the input layer did. When the neurons in the final hidden layer
    are fixed, they act as predictors for the output layer.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的每一层都由 *神经元* 组成，在上述图中由圆圈表示。神经元仅仅是标量值。在 *输入层* 中，每个神经元代表一个单独的预测因子。在上面的图中，输入层有四个神经元，标记为
    \(x_0\) 到 \(x_3\)，每个代表一个单独的预测因子。输入层中的神经元随后决定了第一隐藏层中的神经元，标记为 \(\super{z}{1}_0\)
    到 \(\super{z}{1}_2\)。我们将在稍后讨论 *如何*，但现在只需注意上图从输入层神经元到第一隐藏层神经元的线条。一旦第一隐藏层的神经元被设置，它们就成为了下一层的预测因子，就像输入层一样起作用。当最终隐藏层的神经元被固定时，它们作为输出层的预测因子起作用。
- en: One natural question is how many layers our neural network should contain. There
    is no single answer to this question, as the number of layers is chosen by the
    modeler. Any true neural network will have an input layer, an output layer, and
    at least one hidden layer. The network above has two hidden layers. Note that
    the superscript indicates the hidden layer number, e.g. \(z_{0}^{(1)}\) through
    \(z_2^{(1)}\) are in the first hidden layer and \(z_{0}^{(2)}\) through \(z_2^{(2)}\)
    are in the second hidden layer. We could also consider the input layer as an exogenous
    “hidden layer” and represent it with \(z_{0}^{(0)}\) through \(z_3^{(0)}\).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的问题是我们神经网络应该包含多少层。对于这个问题没有唯一的答案，因为层数是由模型设计者选择的。任何真正的神经网络都将包含一个输入层、一个输出层以及至少一个隐藏层。上面的网络有两个隐藏层。请注意，上标表示隐藏层的编号，例如
    \(z_{0}^{(1)}\) 到 \(z_2^{(1)}\) 都位于第一个隐藏层，而 \(z_{0}^{(2)}\) 到 \(z_2^{(2)}\) 都位于第二个隐藏层。我们也可以将输入层视为一个外生的“隐藏层”，并用
    \(z_{0}^{(0)}\) 到 \(z_3^{(0)}\) 来表示它。
- en: Another natural question is how many neurons each layer should contain. This
    is in part chosen by the modeler and in part predetermined. If our predictor vectors
    are of length \(D\), the input layer must have \(D\) neurons. Similarly, the output
    layer must have as many neurons as there are target variables. If, for instance,
    our model attempts to predict a store’s revenue and its costs (two targets) in
    a given month, our output layer must have two neurons. The sizes of the hidden
    layers, however, are chosen by the modeler. Too few neurons may cause underfitting
    by preventing the network from picking up on important patterns in the data while
    too many neurons may cause overfitting, allowing the network to fit parameters
    that match the training data exactly.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个自然的问题是每一层应该包含多少个神经元。这在一定程度上是由模型设计者选择的，在某种程度上是预先确定的。如果我们的预测向量长度为 \(D\)，则输入层必须包含
    \(D\) 个神经元。同样，输出层必须包含与目标变量数量相等的神经元。例如，如果我们的模型试图预测某个月份商店的收入和成本（两个目标），则输出层必须有两个神经元。然而，隐藏层的大小是由模型设计者选择的。神经元太少可能会导致欠拟合，因为网络无法从数据中捕捉到重要的模式，而神经元太多可能会导致过拟合，使得网络能够拟合与训练数据完全匹配的参数。
- en: 1.2 Communication between Layers
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 层之间的通信
- en: Let’s now turn to the process through which one layer communicates with the
    next. In this section, let \(\bz^{(a)}\) and \(\super{\bz}{b}\) represent the
    vector of neurons in any two consecutive layers. For instance, \(\super{\bz}{a}\)
    might be an input layer and \(\super{\bz}{b}\) the first hidden layer or \(\super{\bz}{a}\)
    might be a hidden layer and \(\super{\bz}{b}\) the following hidden layer. Suppose
    \(\super{\bz}{a} \in \R^{D_a}\) and \(\super{\bz}{b} \in \R^{D_b}\).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向一个层与下一层之间通信的过程。在本节中，让 \(\bz^{(a)}\) 和 \(\super{\bz}{b}\) 代表任意两个连续层中的神经元向量。例如，\(\super{\bz}{a}\)
    可能是一个输入层，而 \(\super{\bz}{b}\) 是第一个隐藏层，或者 \(\super{\bz}{a}\) 可能是一个隐藏层，而 \(\super{\bz}{b}\)
    是随后的隐藏层。假设 \(\super{\bz}{a} \in \R^{D_a}\) 和 \(\super{\bz}{b} \in \R^{D_b}\)。
- en: 'In a feed-forward neural network, each neuron in \(\super{\bz}{b}\) is a function
    of every neuron in \(\super{\bz}{a}\). This function occurs in two stages: first
    a linear mapping of \(\super{\bz}{a}\) onto one dimension, then a nonlinear function
    called an *activation function*. Let’s look at a single neuron within \(\super{\bz}{b}\),
    \(\super{z}{b}_i\). The transformation from \(\super{\bz}{a}\) to \(\super{z}{b}_i\)
    takes the form'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在前馈神经网络中，\(\super{\bz}{b}\) 中的每个神经元都是 \(\super{\bz}{a}\) 中每个神经元的函数。这个函数发生在两个阶段：首先是将
    \(\super{\bz}{a}\) 线性映射到一维，然后是一个称为 *激活函数* 的非线性函数。让我们看看 \(\super{\bz}{b}\) 中的一个单个神经元，\(\super{z}{b}_i\)。从
    \(\super{\bz}{a}\) 到 \(\super{z}{b}_i\) 的转换形式为
- en: \[\begin{split} \begin{align*} \super{h}{b}_i &= \bw_i^\top\super{\bz}{a} +
    c_i \\ \super{z}{b}_i &= f(\super{h}{b}_i), \end{align*} \end{split}\]
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{h}{b}_i &= \bw_i^\top\super{\bz}{a} +
    c_i \\ \super{z}{b}_i &= f(\super{h}{b}_i), \end{align*} \end{split}\]
- en: where \(\bw_i \in \R^{D_a}\) is a vector of weights, \(c_i\) is a constant intercept
    term, and \(f()\) is an activation function. Note that \(\bw_i\) and \(c_i\) are
    specific to the \(i^\text{th}\) neuron in \(\super{\bz}{b}\) while \(f()\) is
    typically common among all neurons in \(\super{\bz}{b}\). We can also write the
    function relating the two layers in matrix form, as below.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\bw_i \in \R^{D_a}\) 是权重向量，\(c_i\) 是常数截距项，\(f()\) 是激活函数。注意，\(\bw_i\) 和
    \(c_i\) 对 \(\super{\bz}{b}\) 中的第 \(i^\text{th}\) 个神经元是特定的，而 \(f()\) 通常对所有 \(\super{\bz}{b}\)
    中的神经元是通用的。我们还可以将连接两层的函数写成矩阵形式，如下所示。
- en: \[\begin{split} \begin{align*} \super{\mathbf{h}}{b} &= \mathbf{W}\super{\bz}{a}
    + \mathbf{c} \\\ \super{\mathbf{z}}{b} &= f(\super{\mathbf{h}}{b}), \end{align*}
    \end{split}\]
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\mathbf{h}}{b} &= \mathbf{W}\super{\bz}{a}
    + \mathbf{c} \\\ \super{\mathbf{z}}{b} &= f(\super{\mathbf{h}}{b}), \end{align*}
    \end{split}\]
- en: where \(\mathbf{W} \in \R^{D_b \times D_a}\), \(\mathbf{c} \in \R^{D_b}\) and
    \(f()\) is applied element-wise.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{W} \in \R^{D_b \times D_a}\), \(\mathbf{c} \in \R^{D_b}\) 且 \(f()\)
    是逐元素应用。
- en: Note
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that we haven’t yet discussed *how* \(\mathbf{W}\), \(\mathbf{c}\) or \(f()\)
    are determined. For now, consider these all to be fixed and focus on the structure
    of a network. *How* we determine these values is discussed in the optimization
    section below.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们尚未讨论 \(\mathbf{W}\), \(\mathbf{c}\) 或 \(f()\) 的确定方法。目前，将这些都视为固定，并关注网络的结构。这些值的确定方法将在下面的优化部分讨论。
- en: Once \(\super{\bz}{b}\) is fixed, we use the same process to create the next
    layer, \(\super{\bz}{c}\). When discussing many layers at a time, it is helpful
    to add superscripts to \(\mathbf{W}, \mathbf{c}\), and \(f()\) to indicate the
    layer. We can write the transmission of \(\super{\bz}{a}\) to \(\super{\bz}{b}\)
    followed by \(\super{\bz}{b}\) to \(\super{\bz}{c}\) as
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 \(\super{\bz}{b}\) 被固定，我们就使用相同的过程来创建下一层，\(\super{\bz}{c}\)。在同时讨论多层时，给 \(\mathbf{W},
    \mathbf{c}\) 和 \(f()\) 添加上标以指示层是有帮助的。我们可以将 \(\super{\bz}{a}\) 到 \(\super{\bz}{b}\)
    的传输，随后 \(\super{\bz}{b}\) 到 \(\super{\bz}{c}\) 的传输写为
- en: \[\begin{split} \begin{align*} \super{\bz}{b} &= \super{f}{b}\left(\super{\mathbf{W}}{b}\super{\bz}{a}
    + \super{\mathbf{c}}{b} \right) \\ \super{\bz}{c} &= \super{f}{c}\left(\super{\mathbf{W}}{c}\super{\bz}{b}
    + \super{\mathbf{c}}{c} \right). \\ \end{align*} \end{split}\]
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\bz}{b} &= \super{f}{b}\left(\super{\mathbf{W}}{b}\super{\bz}{a}
    + \super{\mathbf{c}}{b} \right) \\ \super{\bz}{c} &= \super{f}{c}\left(\super{\mathbf{W}}{c}\super{\bz}{b}
    + \super{\mathbf{c}}{c} \right). \\ \end{align*} \end{split}\]
- en: A more mathematical representation of a neural network is given below. The network
    starts with a vector of predictors \(\bx\). This vector is then multiplied by
    \(\super{\mathbf{W}}{1}\) and added to \(\super{\mathbf{c}}{1}\), which sums to
    \(\super{\mathbf{h}}{1}\). We then apply an activation \(\super{f}{1}\) to \(\super{\mathbf{h}}{1}\),
    which results in our single hidden layer, \(\super{\mathbf{z}}{1}\). The same
    process is then applied to \(\super{\bz}{1}\), which results in our output vector,
    \(\by\).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以下给出了神经网络的一个更数学化的表示。网络从预测因子向量 \(\bx\) 开始。然后这个向量被乘以 \(\super{\mathbf{W}}{1}\)
    并加上 \(\super{\mathbf{c}}{1}\)，总和为 \(\super{\mathbf{h}}{1}\)。然后我们对 \(\super{\mathbf{h}}{1}\)
    应用激活函数 \(\super{f}{1}\)，从而得到我们的单个隐藏层 \(\super{\mathbf{z}}{1}\)。然后将相同的过程应用于 \(\super{\bz}{1}\)，从而得到我们的输出向量
    \(\by\)。
- en: '![](../Images/ddd4201d5226ba07c2ffe7c422934b04.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddd4201d5226ba07c2ffe7c422934b04.png)'
- en: 1.3 Activation Functions
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 激活函数
- en: As we have seen, we create a neuron in one layer by taking a linear mapping
    of the neurons in the previous layer and then applying some *activation function*.
    What exactly is this activation function? An activation function is a (typically)
    nonlinear function that allows the network to learn complex relationships between
    the predictor(s) and the target variable(s).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们通过将前一层神经元的线性映射然后应用某种 *激活函数* 来创建一个层的神经元。这个激活函数究竟是什么？激活函数是一个（通常是）非线性函数，它允许网络学习预测变量和目标变量之间的复杂关系。
- en: Suppose, for instance, the relationship between a target variable \(y_n\) and
    a predictor \(x_n\) is given by
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，例如，目标变量 \(y_n\) 和预测变量 \(x_n\) 之间的关系由以下给出
- en: \[ y_n = |x_n| + \epsilon_n, \]
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y_n = |x_n| + \epsilon_n, \]
- en: where \(\epsilon_n\) is a noise term. Despite its simplicity, this relationship
    cannot be accurately fit by a linear model.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\epsilon_n\) 是一个噪声项。尽管这个关系很简单，但它不能被线性模型精确地拟合。
- en: '![](../Images/ec9dd141265cd1ab9d61d344f2d31699.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec9dd141265cd1ab9d61d344f2d31699.png)'
- en: Ideally, we would apply some function to the predictor and use a different model
    depending on the result of this function. In the case above, \(x_n > 0\) would
    “activate” the model \(y_n \approx x_n\), and \(x_n \leq 0\) would “activate”
    the model \(y_n \approx -x_n\). Hence the name “activation function”.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们会应用某个函数到预测变量上，并根据这个函数的结果使用不同的模型。在上面的例子中，\(x_n > 0\) 会“激活”模型 \(y_n \approx
    x_n\)，而 \(x_n \leq 0\) 会“激活”模型 \(y_n \approx -x_n\)。因此得名“激活函数”。
- en: 'There are many commonly used activation functions, and deciding which function
    to use is a major consideration in modeling a neural network. Here we will limit
    our discussion to two of the most common functions: the ReLU (Rectified Linear
    Unit) and sigmoid functions. The linear activation function (which is really the
    absence of an activation function) is also discussed.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的激活函数有很多，决定使用哪个函数是构建神经网络时的重要考虑因素。在这里，我们将限制我们的讨论范围到两种最常见的函数：ReLU（修正线性单元）和sigmoid函数。线性激活函数（实际上是没有激活函数）也被讨论了。
- en: ReLU
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReLU
- en: '![](../Images/a2c490c62bae3864640ecdbee382bd4a.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2c490c62bae3864640ecdbee382bd4a.png)'
- en: ReLU is a simple yet extremely common activation function. It is defined as
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 是一种简单但极其常见的激活函数。它被定义为
- en: \[ f(x) = \text{max}(x, 0). \]
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = \text{max}(x, 0). \]
- en: How can such a simple function benefit a neural network? ReLU acts like a switch,
    selectively turning channels on and off. Consider fitting a neural network to
    the dataset above generated with \(y_n = |x_n| + \epsilon_n\). Let’s use a very
    simple network represented by the diagram below. This network has one predictor,
    a single hidden layer with two neurons, and one output variable.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一个简单的函数如何使神经网络受益？ReLU 就像是一个开关，选择性地打开和关闭通道。考虑将神经网络拟合到由 \(y_n = |x_n| + \epsilon_n\)
    生成的上述数据集。让我们使用以下图表表示的非常简单的网络。这个网络有一个预测变量，一个包含两个神经元的单个隐藏层和一个输出变量。
- en: '![](../Images/7956d5c1c598913c656afdc0767e7ee1.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7956d5c1c598913c656afdc0767e7ee1.png)'
- en: 'Now let’s say we decide to use \(f(\bx) = \text{ReLU}(\bx)\) and we land on
    the following parameters:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们假设我们决定使用 \(f(\bx) = \text{ReLU}(\bx)\) 并得到以下参数：
- en: \[\begin{split} \super{\mathbf{W}}{1} = \begin{pmatrix} 1 \\ -1 \end{pmatrix},
    \hspace{1mm} \super{\mathbf{c}}{1} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \hspace{1mm}
    \super{\mathbf{W}}{2} = \begin{pmatrix} 1 & 1 \end{pmatrix}, \hspace{1mm} \mathbf{c}^{(2)}
    = 0. \end{split}\]
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \super{\mathbf{W}}{1} = \begin{pmatrix} 1 \\ -1 \end{pmatrix},
    \hspace{1mm} \super{\mathbf{c}}{1} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \hspace{1mm}
    \super{\mathbf{W}}{2} = \begin{pmatrix} 1 & 1 \end{pmatrix}, \hspace{1mm} \mathbf{c}^{(2)}
    = 0. \end{split}\]
- en: This is equivalent to the following complete model
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于以下完整的模型
- en: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    1 \\ -1 \end{pmatrix} x \right) \\ y &= \begin{pmatrix} 1 & 1 \end{pmatrix} \super{\bz}{1}.
    \end{align*} \end{split}\]
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    1 \\ -1 \end{pmatrix} x \right) \\ y &= \begin{pmatrix} 1 & 1 \end{pmatrix} \super{\bz}{1}.
    \end{align*} \end{split}\]
- en: Will this model be able to fit our dataset? Suppose \(x_n = c\) for some *positive*
    constant \(c\). We will then get
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型能否拟合我们的数据集？假设 \(x_n = c\) 对于某个正的常数 \(c\)。然后我们将得到
- en: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    c \\ -c \end{pmatrix} \right) = \begin{pmatrix} c \\ 0 \end{pmatrix} \\ y &= \begin{pmatrix}
    1 & 1 \end{pmatrix} \begin{pmatrix} c \\ 0 \end{pmatrix} = c. \end{align*} \end{split}\]
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    c \\ -c \end{pmatrix} \right) = \begin{pmatrix} c \\ 0 \end{pmatrix} \\ y &= \begin{pmatrix}
    1 & 1 \end{pmatrix} \begin{pmatrix} c \\ 0 \end{pmatrix} = c. \end{align*} \end{split}\]
- en: 'So we will predict \(y_n = |x_n| = c\), a sensible result! Similarly, if \(x_n
    = -c\), we would again obtain the valid prediction \(y_n = |x_n| = c\). ReLU is
    able to achieve this result by activating a different channel depending on the
    value of \(x_n\): if \(x_n\) is greater than 0, it activates \(y_n = x_n\), and
    if \(x_n\) is less than 0, it activates \(y_n = -x_n\).'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将预测 \(y_n = |x_n| = c\)，这是一个合理的结论！同样地，如果 \(x_n = -c\)，我们再次会得到有效的预测 \(y_n
    = |x_n| = c\)。ReLU 通过根据 \(x_n\) 的值激活不同的通道来实现这一结果：如果 \(x_n\) 大于 0，它激活 \(y_n = x_n\)；如果
    \(x_n\) 小于 0，它激活 \(y_n = -x_n\)。
- en: 'As we will see in the next section, fitting a neural network consists of taking
    gradients of our activation functions. Fortunately ReLU has a straightforward
    derivative:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到，拟合神经网络包括取激活函数的梯度。幸运的是，ReLU 有一个直接的导数：
- en: \[\begin{split} \frac{\partial}{\partial x} \text{ReLU}(x) = \begin{cases} 1,
    & x > 0 \\ 0, & x \leq 0\. \end{cases} \end{split}\]
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \frac{\partial}{\partial x} \text{ReLU}(x) = \begin{cases} 1,
    & x > 0 \\ 0, & x \leq 0\. \end{cases} \end{split}\]
- en: Note that this derivative is not technically defined at 0\. In practice, it
    is very unlikely that we will be applying an activation function to 0 *exactly*,
    though in that case the convention is to set its derivative equal to 0.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个导数在 0 处技术上是没有定义的。在实践中，我们不太可能将激活函数应用于 0 的确切值，尽管在这种情况下，惯例是将其导数设置为 0。
- en: Sigmoid
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Sigmoid
- en: '![](../Images/f99d9f109382dc57dff15439e009ffe8.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f99d9f109382dc57dff15439e009ffe8.png)'
- en: A second common activation function is the *logistic sigmoid function*, often
    referred to as just *the sigmoid function*. This function was introduced in [chapter
    3](../c3/s1/logistic_regression.html) in the context of the logistic regression.
    The sigmoid function is defined as
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个常见的激活函数是 *对数 sigmoid 函数*，通常简称为 *sigmoid 函数*。这个函数在第 3 章的 [第 3 章](../c3/s1/logistic_regression.html)
    中，在逻辑回归的背景下被引入。sigmoid 函数定义为
- en: \[ \sigma(x) = \frac{1}{1 + \exp(-x)}. \]
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma(x) = \frac{1}{1 + \exp(-x)}. \]
- en: Note that the sigmoid function takes any real value and returns a value between
    0 and 1\. As a result, the sigmoid function is commonly applied to the last hidden
    layer in a network in order to return a probability estimate in the output layer.
    This makes it common in classification problems.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，sigmoid 函数接受任何实数值，并返回一个介于 0 和 1 之间的值。因此，sigmoid 函数通常应用于网络的最后一隐藏层，以便在输出层返回概率估计。这使得它在分类问题中很常见。
- en: As we saw in chapter 3, a convenient fact about the sigmoid function is that
    we can express its derivative in terms of itself.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第 3 章中看到的，关于 sigmoid 函数的一个方便的事实是，我们可以用自身来表示其导数。
- en: \[ \dadb{\sigma(x)}{x} = \frac{\exp(-x)}{\left( 1 + \exp(-x) \right)^2} = \frac{1}{1
    + \exp(-x)} \cdot \frac{\exp(-x)}{1 + \exp(-x)} = \sigma(x)\left(1 - \sigma(x)\right).
    \]
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\sigma(x)}{x} = \frac{\exp(-x)}{\left( 1 + \exp(-x) \right)^2} = \frac{1}{1
    + \exp(-x)} \cdot \frac{\exp(-x)}{1 + \exp(-x)} = \sigma(x)\left(1 - \sigma(x)\right).
    \]
- en: The Linear Activation Function
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线性激活函数
- en: Another possible activation function is the “linear” activation function, which
    is the same as skipping the activation function altogether. The linear activation
    function simply returns its input. It is defined with
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能的激活函数是“线性”激活函数，它与完全不使用激活函数相同。线性激活函数简单地返回其输入。它定义为
- en: \[ f(x) = x, \]
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = x, \]
- en: and has derivative
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 并且具有导数
- en: \[ \dadb{f(x)}{x} = 1\. \]
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{f(x)}{x} = 1\. \]
- en: The linear activation function is often used before the last layer in a neural
    network for regression. Rather than constraining the fitted values to be in some
    range or setting half of them equal to 0, we want to leave them as they are.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 线性激活函数通常用于神经网络中的最后一层之前，用于回归。我们不想将拟合值约束在某个范围内，或者将其中一半设置为 0，我们希望保持它们不变。
- en: ReLU
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReLU
- en: '![](../Images/a2c490c62bae3864640ecdbee382bd4a.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2c490c62bae3864640ecdbee382bd4a.png)'
- en: ReLU is a simple yet extremely common activation function. It is defined as
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 是一个简单但极其常见的激活函数。它定义为
- en: \[ f(x) = \text{max}(x, 0). \]
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = \text{max}(x, 0). \]
- en: How can such a simple function benefit a neural network? ReLU acts like a switch,
    selectively turning channels on and off. Consider fitting a neural network to
    the dataset above generated with \(y_n = |x_n| + \epsilon_n\). Let’s use a very
    simple network represented by the diagram below. This network has one predictor,
    a single hidden layer with two neurons, and one output variable.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一个简单的函数如何使神经网络受益？ReLU 就像是一个开关，选择性地打开和关闭通道。考虑将一个神经网络拟合到上面生成的数据集，其中 \(y_n =
    |x_n| + \epsilon_n\)。让我们使用下面图表表示的非常简单的网络。这个网络有一个预测器，一个包含两个神经元的单个隐藏层，以及一个输出变量。
- en: '![](../Images/7956d5c1c598913c656afdc0767e7ee1.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7956d5c1c598913c656afdc0767e7ee1.png)'
- en: 'Now let’s say we decide to use \(f(\bx) = \text{ReLU}(\bx)\) and we land on
    the following parameters:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们决定使用 \(f(\bx) = \text{ReLU}(\bx)\) 并得到以下参数：
- en: \[\begin{split} \super{\mathbf{W}}{1} = \begin{pmatrix} 1 \\ -1 \end{pmatrix},
    \hspace{1mm} \super{\mathbf{c}}{1} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \hspace{1mm}
    \super{\mathbf{W}}{2} = \begin{pmatrix} 1 & 1 \end{pmatrix}, \hspace{1mm} \mathbf{c}^{(2)}
    = 0. \end{split}\]
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \super{\mathbf{W}}{1} = \begin{pmatrix} 1 \\ -1 \end{pmatrix},
    \hspace{1mm} \super{\mathbf{c}}{1} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \hspace{1mm}
    \super{\mathbf{W}}{2} = \begin{pmatrix} 1 & 1 \end{pmatrix}, \hspace{1mm} \mathbf{c}^{(2)}
    = 0. \end{split}\]
- en: This is equivalent to the following complete model
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于以下完整的模型
- en: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    1 \\ -1 \end{pmatrix} x \right) \\ y &= \begin{pmatrix} 1 & 1 \end{pmatrix} \super{\bz}{1}.
    \end{align*} \end{split}\]
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    1 \\ -1 \end{pmatrix} x \right) \\ y &= \begin{pmatrix} 1 & 1 \end{pmatrix} \super{\bz}{1}.
    \end{align*} \end{split}\]
- en: Will this model be able to fit our dataset? Suppose \(x_n = c\) for some *positive*
    constant \(c\). We will then get
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型能否拟合我们的数据集？假设 \(x_n = c\) 对于某个正的常数 \(c\)。那么我们将得到
- en: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    c \\ -c \end{pmatrix} \right) = \begin{pmatrix} c \\ 0 \end{pmatrix} \\ y &= \begin{pmatrix}
    1 & 1 \end{pmatrix} \begin{pmatrix} c \\ 0 \end{pmatrix} = c. \end{align*} \end{split}\]
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \super{\bz}{1} &= \text{ReLU}\left( \begin{pmatrix}
    c \\ -c \end{pmatrix} \right) = \begin{pmatrix} c \\ 0 \end{pmatrix} \\ y &= \begin{pmatrix}
    1 & 1 \end{pmatrix} \begin{pmatrix} c \\ 0 \end{pmatrix} = c. \end{align*} \end{split}\]
- en: 'So we will predict \(y_n = |x_n| = c\), a sensible result! Similarly, if \(x_n
    = -c\), we would again obtain the valid prediction \(y_n = |x_n| = c\). ReLU is
    able to achieve this result by activating a different channel depending on the
    value of \(x_n\): if \(x_n\) is greater than 0, it activates \(y_n = x_n\), and
    if \(x_n\) is less than 0, it activates \(y_n = -x_n\).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将预测 \(y_n = |x_n| = c\)，这是一个合理的结果！同样地，如果 \(x_n = -c\)，我们又会得到有效的预测 \(y_n
    = |x_n| = c\)。ReLU通过根据 \(x_n\) 的值激活不同的通道来实现这一结果：如果 \(x_n\) 大于 0，它激活 \(y_n = x_n\)；如果
    \(x_n\) 小于 0，它激活 \(y_n = -x_n\)。
- en: 'As we will see in the next section, fitting a neural network consists of taking
    gradients of our activation functions. Fortunately ReLU has a straightforward
    derivative:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到，拟合神经网络包括计算激活函数的梯度。幸运的是，ReLU的导数非常直接：
- en: \[\begin{split} \frac{\partial}{\partial x} \text{ReLU}(x) = \begin{cases} 1,
    & x > 0 \\ 0, & x \leq 0\. \end{cases} \end{split}\]
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \frac{\partial}{\partial x} \text{ReLU}(x) = \begin{cases} 1,
    & x > 0 \\ 0, & x \leq 0\. \end{cases} \end{split}\]
- en: Note that this derivative is not technically defined at 0\. In practice, it
    is very unlikely that we will be applying an activation function to 0 *exactly*,
    though in that case the convention is to set its derivative equal to 0.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个导数在 0 处在技术上是没有定义的。在实践中，我们不太可能将激活函数应用于 0 的确切值，尽管在这种情况下，惯例是将其导数设为 0。
- en: Sigmoid
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Sigmoid
- en: '![](../Images/f99d9f109382dc57dff15439e009ffe8.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f99d9f109382dc57dff15439e009ffe8.png)'
- en: A second common activation function is the *logistic sigmoid function*, often
    referred to as just *the sigmoid function*. This function was introduced in [chapter
    3](../c3/s1/logistic_regression.html) in the context of the logistic regression.
    The sigmoid function is defined as
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个常见的激活函数是 *逻辑sigmoid函数*，通常简称为 *sigmoid函数*。这个函数在 [第3章](../c3/s1/logistic_regression.html)
    中，在逻辑回归的上下文中被引入。sigmoid函数定义为
- en: \[ \sigma(x) = \frac{1}{1 + \exp(-x)}. \]
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma(x) = \frac{1}{1 + \exp(-x)}. \]
- en: Note that the sigmoid function takes any real value and returns a value between
    0 and 1\. As a result, the sigmoid function is commonly applied to the last hidden
    layer in a network in order to return a probability estimate in the output layer.
    This makes it common in classification problems.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，sigmoid函数接受任何实数值并返回介于 0 和 1 之间的值。因此，sigmoid函数通常应用于网络的最后一个隐藏层，以便在输出层返回概率估计。这使得它在分类问题中很常见。
- en: As we saw in chapter 3, a convenient fact about the sigmoid function is that
    we can express its derivative in terms of itself.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第3章中看到的，sigmoid函数的一个方便的事实是，我们可以用其自身来表示其导数。
- en: \[ \dadb{\sigma(x)}{x} = \frac{\exp(-x)}{\left( 1 + \exp(-x) \right)^2} = \frac{1}{1
    + \exp(-x)} \cdot \frac{\exp(-x)}{1 + \exp(-x)} = \sigma(x)\left(1 - \sigma(x)\right).
    \]
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\sigma(x)}{x} = \frac{\exp(-x)}{\left( 1 + \exp(-x) \right)^2} = \frac{1}{1
    + \exp(-x)} \cdot \frac{\exp(-x)}{1 + \exp(-x)} = \sigma(x)\left(1 - \sigma(x)\right).
    \]
- en: The Linear Activation Function
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线性激活函数
- en: Another possible activation function is the “linear” activation function, which
    is the same as skipping the activation function altogether. The linear activation
    function simply returns its input. It is defined with
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能的激活函数是“线性”激活函数，它等同于完全跳过激活函数。线性激活函数简单地返回其输入。它定义为
- en: \[ f(x) = x, \]
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = x, \]
- en: and has derivative
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 并且其导数为
- en: \[ \dadb{f(x)}{x} = 1\. \]
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{f(x)}{x} = 1\. \]
- en: The linear activation function is often used before the last layer in a neural
    network for regression. Rather than constraining the fitted values to be in some
    range or setting half of them equal to 0, we want to leave them as they are.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 线性激活函数通常在神经网络的最后一层之前用于回归。我们不想将拟合值限制在某个范围内或设置其中一半为0，而是希望保持它们不变。
- en: 2\. Optimization
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 优化
- en: We have now seen that a neural network operates through a series of linear mappings
    and activation functions. The linear mapping for layer \(\ell\) is determined
    by the parameters in \(\super{\mathbf{W}}{\ell}\) and \(\super{\mathbf{c}}{\ell}\),
    also called the *weights*. This section discusses the process through which the
    weights in a neural network are fit, called *back propagation*.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，神经网络通过一系列线性映射和激活函数来操作。层 \(\ell\) 的线性映射由 \(\super{\mathbf{W}}{\ell}\)
    和 \(\super{\mathbf{c}}{\ell}\) 中的参数确定，也称为*权重*。本节讨论了神经网络中权重拟合的过程，称为*反向传播*。
- en: The rest of this page requires a good amount of matrix differentiation, which
    is introduced in the [math appendix](../appendix/math.html). Note that we use
    the “numerator layout,” meaning for \(\by \in \R^m\) and \(\bx \in \R^n\), we
    write \(\partial\by/\partial\bx\) as
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 本页的其余部分需要大量的矩阵微分，这在[数学附录](../appendix/math.html)中介绍。注意，我们使用“分子布局”，即对于 \(\by
    \in \R^m\) 和 \(\bx \in \R^n\)，我们写 \(\partial\by/\partial\bx\) 为
- en: \[\begin{split} \dadb{\by}{\bx} = \begin{bmatrix} \dadb{y_1}{x_1} & \dots &
    \dadb{y_1}{x_n} \\ & \dots & \\ \dadb{y_m}{x_1} & \dots & \dadb{y_m}{x_n} \end{bmatrix}
    \in \R^{m \times n}. \end{split}\]
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\by}{\bx} = \begin{bmatrix} \dadb{y_1}{x_1} & \dots &
    \dadb{y_1}{x_n} \\ & \dots & \\ \dadb{y_m}{x_1} & \dots & \dadb{y_m}{x_n} \end{bmatrix}
    \in \R^{m \times n}. \end{split}\]
- en: 2.1 Back Propagation
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 反向传播
- en: Suppose we choose some loss function \(\mathcal{L}\) for our network to minimize.
    Note that because our target variable is multi-dimensional, \(\boldsymbol{\mathcal{L}}\)
    function will be a vector of losses (e.g. the loss for the first target, the loss
    for the second target, etc.). To find the network’s optimal weights, we can conduct
    gradient descent, repeatedly taking the derivative of our loss function with respect
    to each weight and adjusting accordingly. As we will see, this involves finding
    the gradient of the network’s final weights, then using the chain rule to find
    the gradient of the weights that came earlier. In this process, we move backward
    through the network, and hence the name “back propagation.”
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们选择某个损失函数 \(\mathcal{L}\) 来最小化我们的网络。注意，因为我们的目标变量是多维的，\(\boldsymbol{\mathcal{L}}\)
    函数将是一个损失向量（例如，第一个目标的损失，第二个目标的损失等）。为了找到网络的最佳权重，我们可以进行梯度下降，重复地取损失函数关于每个权重的导数并相应地调整。正如我们将看到的，这涉及到找到网络最终权重的梯度，然后使用链式法则找到先前权重的梯度。在这个过程中，我们通过网络向后移动，因此得名“反向传播。”
- en: '![](../Images/1227d5a61b04d2213382374c634abd8a.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1227d5a61b04d2213382374c634abd8a.png)'
- en: Consider conducting gradient descent for the network above. Write the loss function
    as \(\mathcal{L}(\hat{\by})\), where \(\hat{\by}\) is the network’s output. Let’s
    start by writing out the derivative of \(\mathcal{L}\) with respect to \(\super{\mathbf{W}}{L}\),
    the final matrix of weights in our network. We can do this with the chain rule,
    as below.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑对上述网络进行梯度下降。将损失函数表示为 \(\mathcal{L}(\hat{\by})\)，其中 \(\hat{\by}\) 是网络的输出。让我们首先写出
    \(\mathcal{L}\) 关于 \(\super{\mathbf{W}}{L}\) 的导数，即我们网络中的最终权重矩阵。我们可以使用链式法则，如下所示。
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
- en: The gradient of \(\super{\mathbf{c}}{L}\) is equivalent. The math behind these
    calculations is covered in the following section. Next, we want to find the gradient
    of \(\super{\mathbf{W}}{L-1}\), shown below.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: \(\super{\mathbf{c}}{L}\) 的梯度是等效的。这些计算背后的数学将在下一节中介绍。接下来，我们想要找到 \(\super{\mathbf{W}}{L-1}\)
    的梯度，如下所示。
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L-1}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}
    \cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}} \cdot \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{z}}{L-1}}
    \cdot \dadb{\super{\mathbf{z}}{L-1}}{\super{\mathbf{h}}{L-1}} \cdot \dadb{\super{\mathbf{h}}{L-1}}{\super{\mathbf{W}}{L-1}}
    \]
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L-1}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}
    \cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}} \cdot \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{z}}{L-1}}
    \cdot \dadb{\super{\mathbf{z}}{L-1}}{\super{\mathbf{h}}{L-1}} \cdot \dadb{\super{\mathbf{h}}{L-1}}{\super{\mathbf{W}}{L-1}}
    \]
- en: 'This expression is pretty ugly, but there is a shortcut. This gradient and
    the gradient of \(\super{\mathbf{W}}{L}\) share the first two terms, which represent
    the gradient of \(\super{\mathbf{h}}{L}\). To save time (both in writing out the
    gradients and in calculating them in practice), we can record this gradient, \(\nabla
    \super{\mathbf{h}}{L}\), and apply it where necessary. We can do the same with
    \(\nabla \mathbf{h}^{(L-1)}\), which simplifies the gradient of \(\mathbf{W}^{(L-2)}\):'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式看起来相当丑陋，但有一个捷径。这个梯度与 \(\super{\mathbf{W}}{L}\) 的梯度共享前两个项，这代表了 \(\super{\mathbf{h}}{L}\)
    的梯度。为了节省时间（在写出梯度以及在实践中计算它们时），我们可以记录这个梯度，\(\nabla \super{\mathbf{h}}{L}\)，并在需要的地方应用它。我们也可以对
    \(\nabla \mathbf{h}^{(L-1)}\) 做同样的事情，这简化了 \(\mathbf{W}^{(L-2)}\) 的梯度：
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L-2}} = \nabla \super{\mathbf{h}}{L-1}
    \cdot \dadb{\super{\mathbf{h}}{L-1}}{\super{\mathbf{z}}{L-2}} \cdot \dadb{\super{\mathbf{z}}{L-2}}{\super{\mathbf{h}}{L-2}}
    \cdot \dadb{\super{\mathbf{h}}{L-2}}{\super{\mathbf{W}}{L-2}}. \]
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L-2}} = \nabla \super{\mathbf{h}}{L-1}
    \cdot \dadb{\super{\mathbf{h}}{L-1}}{\super{\mathbf{z}}{L-2}} \cdot \dadb{\super{\mathbf{z}}{L-2}}{\super{\mathbf{h}}{L-2}}
    \cdot \dadb{\super{\mathbf{h}}{L-2}}{\super{\mathbf{W}}{L-2}}. \]
- en: We continue this same process until we reach the first set of weights.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续这个过程，直到达到第一组权重。
- en: We’ve now seen intuitively how to find the gradients for our network’s many
    weights. To conduct back propagation, we simply use these gradients to run gradient
    descent. Next, let’s see how to actually calculate these gradients.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在直观地看到了如何找到我们网络中许多权重的梯度。为了进行反向传播，我们只需使用这些梯度来运行梯度下降。接下来，让我们看看如何实际计算这些梯度。
- en: 2.2 Calculating Gradients
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 计算梯度
- en: 'In this section we will derive the gradients used in back propagation. For
    each iteration in this process we need to know the derivative of our loss function
    with respect to each weight in the entire network. For the network shown above,
    this requires calculating the following gradients:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将推导出反向传播中使用的梯度。在这个过程中，对于每次迭代，我们需要知道损失函数相对于整个网络中每个权重的导数。对于上面显示的网络，这需要计算以下梯度：
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{1}}, \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{1}},
    \dots, \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{L}}, \text{ and } \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{L}}.
    \]
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{1}}, \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{1}},
    \dots, \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{L}}, \text{ and } \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{L}}.
    \]
- en: Since we will find these with the chain rule, we will need to calculate other
    gradients along the way. All the necessary gradients are derived below. Note that
    the following sub-sections cover the stages within a single layer of a network
    in reverse order (as back propagation does).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用链式法则来找到这些梯度，因此我们需要在途中计算其他梯度。所有必要的梯度都在下面推导出来。注意，以下子节按反向顺序（正如反向传播所做的那样）覆盖了网络单层中的各个阶段。
- en: Note
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the rest of this section considers only one observation at a time.
    The vector \(\by\), for instance, refers to the output variables for a single
    observation, rather than a vector of 1-dimensional output variables for several
    observations. Similarly, \(\partial \mathcal{L}(\hat{\by})/\partial\hat{\by}\)
    refers to the derivative of the loss with respect to a single observation’s output.
    The final section discusses how to combine the derivatives from multiple observations.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，本节的其余部分每次只考虑一个观察值。例如，向量 \(\by\) 指的是单个观察值的输出变量，而不是多个观察值的1维输出变量的向量。同样，\(\partial
    \mathcal{L}(\hat{\by})/\partial\hat{\by}\) 指的是损失相对于单个观察值输出的导数。最后一节讨论了如何将多个观察值的导数组合起来。
- en: For the following, let there be \(L\) layers in total. Also let layer \(\ell\)
    have size \(D_\ell\), except the input and output layers which have sizes \(D_x\)
    and \(D_y\), respectively.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下内容，假设总共有 \(L\) 层。除了输入层和输出层分别具有 \(D_x\) 和 \(D_y\) 的大小外，层 \(\ell\) 的大小为 \(D_\ell\)。
- en: 2.2.1 Loss Functions and their Gradients
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 损失函数及其梯度
- en: '![](../Images/6e20022b03b555510592958ec46ee03d.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e20022b03b555510592958ec46ee03d.png)'
- en: 'Back propagation begins where the network ends: the loss function \(\mathcal{L}(\hat{\by})\).
    Let’s start by introducing some common loss functions and their derivatives with
    respect to our predictions, \(\hat{\by}\). Later, using the chain rule, we will
    use these derivatives to calculate the derivatives with respect to our network’s
    weights.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播从网络结束的地方开始：损失函数\(\mathcal{L}(\hat{\by})\)。让我们首先介绍一些常见的损失函数及其关于我们预测的\(\hat{\by}\)的导数。稍后，我们将使用链式法则，使用这些导数来计算关于我们网络权重的导数。
- en: A common loss function for quantitative output variables is the residual sum
    of squares. For a single observation, the loss is
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定量输出变量，一个常见的损失函数是残差平方和。对于单个观测值，损失如下：
- en: \[ \mathcal{L}_{RSS}(\hat{\by}) = (\by - \hat{\by})^2. \]
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}_{RSS}(\hat{\by}) = (\by - \hat{\by})^2. \]
- en: Note
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the loss is a function of both our predictions (\(\hat{\by}\)) and
    the true targets (\(\by\)). However, since the true targets are fixed, we can
    only manipulate \(\hat{\by}\), so we write the loss as only a function of \(\hat{\by}\).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，损失是预测(\(\hat{\by}\))和真实目标(\(\by\))的函数。然而，由于真实目标是固定的，我们只能操作\(\hat{\by}\)，因此我们将损失仅作为\(\hat{\by}\)的函数来表示。
- en: Note that we have a vector of losses because there are multiple output variables
    and we consider the loss for each variable independently. Now for the first step
    in back propagation, we calculate the derivative of this loss with respect to
    \(\hat{\by}\), which is simply given by
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们有一个损失向量，因为存在多个输出变量，并且我们独立地考虑每个变量的损失。现在，对于反向传播的第一步，我们计算这个损失关于\(\hat{\by}\)的导数，它简单地给出如下
- en: \[ \dadb{\mathcal{L}_{RSS}(\hat{\by})}{\hat{\by}} = -2(\by - \hat{\by})^\top
    \in \R^{1 \times D_y}. \]
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}_{RSS}(\hat{\by})}{\hat{\by}} = -2(\by - \hat{\by})^\top
    \in \R^{1 \times D_y}. \]
- en: Since we are using the numerator layout convention, this derivative is a length-\(D_y\)
    row vector, or equivalently a \(1\) by \(D_y\) matrix.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是分子布局约定，这个导数是一个长度为\(D_y\)的行向量，或者等价地是一个\(1\)乘以\(D_y\)的矩阵。
- en: For binary classification problems, a common loss function is the log loss or
    cross entropy, given by
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类问题，一个常见的损失函数是对数损失或交叉熵，给出如下
- en: \[ \mathcal{L}_{Log}(\hat{\by}) = -\Big(\by\log \hat{\by}+(1-\by)\log(1-\hat{\by})\Big),
    \]
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}_{Log}(\hat{\by}) = -\Big(\by\log \hat{\by}+(1-\by)\log(1-\hat{\by})\Big),
    \]
- en: where the \(i^\text{th}\) entry in \(\hat{\by}\) gives the estimated probability
    that the \(i^\text{th}\) output variable equals 1\. The derivative of this loss
    function with respect to \(\hat{\by}\) is given by
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，\(\hat{\by}\)中的第\(i\)个元素给出了第\(i\)个输出变量等于1的估计概率。该损失函数关于\(\hat{\by}\)的导数如下所示：
- en: \[ \begin{align*} \dadb{\mathcal{L}_{Log}(\hat{\by})}{\hat{\by}} &= \left(-\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top\in \R^{1 \times D_y}. \end{align*} \]
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\mathcal{L}_{Log}(\hat{\by})}{\hat{\by}} &= \left(-\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top\in \R^{1 \times D_y}. \end{align*} \]
- en: Once we calculate \(\partial \mathcal{L}(\hat{\by})/\partial\hat{\by}\), we
    can move further back into the network. Since \(\hat{\by}\) is the result of an
    activation function, the next step in back propagation is to calculate the derivative
    of our activation functions.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算出\(\partial \mathcal{L}(\hat{\by})/\partial\hat{\by}\)，我们就可以进一步回溯到网络中。由于\(\hat{\by}\)是激活函数的结果，反向传播的下一步是计算激活函数的导数。
- en: 2.2.2 Gradients of the Activation Functions
  id: totrans-373
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 激活函数的梯度
- en: '![](../Images/73cd03e425f8718e9a2e8b211cbb8b9c.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73cd03e425f8718e9a2e8b211cbb8b9c.png)'
- en: Recall that \(\superb{z}{\ell}\), the output layer of \(\ell\), is the result
    of an activation function applied to a linear mapping \(\superb{h}{\ell}\). This
    includes the output of the final layer, \(\mathbf{\hat{y}}\), which we can also
    write as \(\superb{z}{L}\).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，\(\superb{z}{\ell}\)，\(\ell\)层的输出层，是应用激活函数到线性映射\(\superb{h}{\ell}\)的结果。这包括最终层的输出，\(\mathbf{\hat{y}}\)，我们也可以将其写作\(\superb{z}{L}\)。
- en: ReLU
  id: totrans-376
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ReLU
- en: Suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) where
    \(\super{f}{\ell}\) is the ReLU function. We are interested in \(\partial \superb{z}{\ell}/\partial
    \superb{h}{\ell}\). For \(i \neq j\), we have
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有\(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)，其中\(\super{f}{\ell}\)是ReLU函数。我们感兴趣的是\(\partial
    \superb{z}{\ell}/\partial \superb{h}{\ell}\)。对于\(i \neq j\)，我们有
- en: \[ \frac{\partial \super{z}{\ell}_i}{\partial \super{h}{\ell}_j} = 0, \]
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \super{z}{\ell}_i}{\partial \super{h}{\ell}_j} = 0, \]
- en: since \(\super{z}{\ell}_i\) is not a function of \(\super{h}{\ell}_j\). Then
    using the ReLU derivative, we have
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 由于\(\super{z}{\ell}_i\)不是\(\super{h}{\ell}_j\)的函数。然后使用ReLU导数，我们有
- en: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \begin{cases}
    1, & \super{h}{\ell}_i > 0 \\ 0, & \super{h}{\ell}_i \leq 0\. \end{cases} \end{split}\]
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \begin{cases}
    1, & \super{h}{\ell}_i > 0 \\ 0, & \super{h}{\ell}_i \leq 0\. \end{cases} \end{split}\]
- en: We can then compactly write the entire derivative as
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以紧凑地写出整个导数
- en: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = \text{diag}(\superb{h}{\ell}
    > 0) \in \R^{D_\ell \times D_\ell}. \]
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\super{z}{\ell}}{\super{h}{\ell}} = \text{diag}(\superb{h}{\ell} >
    0) \in \R^{D_\ell \times D_\ell}. \]
- en: Sigmoid
  id: totrans-383
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Now suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)
    where \(\super{f}{\ell}\) is the sigmoid function. Again, the partial derivative
    \(\partial \super{z}{\ell}_i/\partial \super{h}{\ell}_j\) is 0 for \(i \neq j\).
    By the sigmoid derivative, we have
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们有 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) 其中 \(\super{f}{\ell}\)
    是sigmoid函数。再次，偏导数 \(\partial \super{z}{\ell}_i/\partial \super{h}{\ell}_j\) 对于
    \(i \neq j\) 是0。通过sigmoid导数，我们有
- en: \[ \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \sigma(\super{h}{\ell}_i)(1-\sigma(\super{h}{\ell}_i)).
    \]
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \sigma(\super{h}{\ell}_i)(1-\sigma(\super{h}{\ell}_i)).
    \]
- en: We can again write the entire result compactly as
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次将整个结果紧凑地写为
- en: \[ \begin{align*} \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} &= \text{diag}\left(\sigma(\superb{h}{\ell})(1-\sigma(\superb{h}{\ell})\right)
    \in \R^{D_\ell \times D_\ell}. \end{align*} \]
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\super{z}{\ell}}{\super{h}{\ell}} &= \text{diag}\left(\sigma(\superb{h}{\ell})(1-\sigma(\superb{h}{\ell})\right)
    \in \R^{D_\ell \times D_\ell}. \end{align*} \]
- en: Linear
  id: totrans-388
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 线性
- en: Finally, suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)
    where \(\super{f}{\ell}\) is the linear function. We then have
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，假设我们有 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) 其中 \(\super{f}{\ell}\)
    是线性函数。然后我们有
- en: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_j} = \begin{cases}
    1 , & i = j\\ 0, & i \neq j. \end{cases} \end{split}\]
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_j} = \begin{cases}
    1 , & i = j\\ 0, & i \neq j. \end{cases} \end{split}\]
- en: The entire gradient is then simply
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 整个梯度很简单
- en: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = I_{D_{\ell}} \in \R^{D_\ell \times
    D_\ell}. \]
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\super{z}{\ell}}{\super{h}{\ell}} = I_{D_{\ell}} \in \R^{D_\ell \times
    D_\ell}. \]
- en: 2.2.3 Gradients of the Weights
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 权重的梯度
- en: '![](../Images/cff599cc6963d9f0e1254e3990346bc0.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cff599cc6963d9f0e1254e3990346bc0.png)'
- en: We are now finally able to calculate the gradients of our weights. Specifically,
    we will calculate \(\partial \superb{h}{\ell}/ \partial \superb{c}{\ell}\) and
    \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\) which, when combined
    with our previous results through the chain rule, will allow us to obtain the
    derivative of the loss function with respect the layer \(\ell\) weights.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于能够计算我们权重的梯度。具体来说，我们将计算 \(\partial \super{h}{\ell}/ \partial \super{c}{\ell}\)
    和 \(\partial \super{h}{\ell}/ \partial \super{W}{\ell}\)，当与我们的先前结果通过链式法则结合时，将使我们能够获得损失函数相对于层
    \(\ell\) 权重的导数。
- en: Recall that we obtain \(\superb{h}{\ell}\) through
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们通过以下方式获得 \(\superb{h}{\ell}\)
- en: \[ \superb{h}{\ell} = \superb{W}{\ell}\superb{z}{\ell-1} + \superb{c}{\ell},
    \]
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \superb{h}{\ell} = \superb{W}{\ell}\superb{z}{\ell-1} + \superb{c}{\ell},
    \]
- en: giving us the simple derivative
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 给我们简单的导数
- en: \[ \dadb{\super{\mathbf{h}}{\ell}}{\superb{c}{\ell}} = I_{D_\ell} \in \R^{D_\ell
    \times D_\ell}. \]
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\super{\mathbf{h}}{\ell}}{\superb{c}{\ell}} = I_{D_\ell} \in \R^{D_\ell
    \times D_\ell}. \]
- en: The derivative \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\) is more
    complicated. Since we are taking the derivative of a vector with respect to a
    matrix, our result will be a tensor. The shape of this tensor will be \(D_\ell
    \times (D_\ell \times D_{\ell - 1})\) since \(\superb{h}{\ell} \in R^{D_\ell}\)
    and \(\superb{W}{\ell} \in \R^{D_\ell \times D_{\ell-1}}\). The first element
    of this tensor is given by \(\partial \super{h}{\ell}_1/ \partial \superb{W}{\ell}\).
    Using the expression for \(\superb{h}{\ell}\) above, we see that this is a matrix
    with \((\superb{z}{\ell - 1})^\top\) in the first row and 0s everywhere else.
    More generally, the \(i^\text{th}\) entry in this derivative will have all 0s
    except \((\superb{z}{\ell - 1})^\top\) in its \(i^\text{th}\) row. This is represented
    below.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的导数 \(\partial \super{h}{\ell}/ \partial \super{W}{\ell}\) 更复杂。由于我们是对一个向量相对于一个矩阵求导，我们的结果将是一个张量。这个张量的形状将是
    \(D_\ell \times (D_\ell \times D_{\ell - 1})\)，因为 \(\super{h}{\ell} \in R^{D_\ell}\)
    且 \(\super{W}{\ell} \in \R^{D_\ell \times D_{\ell-1}}\)。这个张量的第一个元素由 \(\partial
    \super{h}{\ell}_1/ \partial \superb{W}{\ell}\) 给出。使用上面给出的 \(\super{h}{\ell}\)
    的表达式，我们看到这是一个矩阵，其第一行有 \((\super{z}{\ell - 1})^\top\)，其余地方都是0。更一般地，这个导数的第 \(i^\text{th}\)
    个元素将除了其第 \(i^\text{th}\) 行有 \((\super{z}{\ell - 1})^\top\) 外，其余都是0。这如下所示。
- en: \[\begin{split} \begin{align*} \dadb{\superb{h}{\ell}}{\superb{W}{\ell}} &=
    \begin{bmatrix} \dadb{\super{h}{\ell}_1}{\superb{W}{\ell}} \\ \\ \dots \\ \\ \dadb{\super{h}{\ell}_{n_\ell}}{\superb{W}{\ell}}
    \end{bmatrix} = \begin{bmatrix} \begin{bmatrix} \superb{z}{\ell - 1})^\top \\
    ... \\ \mathbf{0}^\top \end{bmatrix}\\ \dots \\ \begin{bmatrix} \mathbf{0}^\top
    \\ \dots \\ (\superb{z}{\ell - 1})^\top\end{bmatrix}\end{bmatrix} \in \R^{D_\ell
    \times (D_\ell \times D_{\ell - 1})}. \end{align*} \end{split}\]
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \dadb{\superb{h}{\ell}}{\superb{W}{\ell}} &=
    \begin{bmatrix} \dadb{\super{h}{\ell}_1}{\superb{W}{\ell}} \\ \\ \dots \\ \\ \dadb{\super{h}{\ell}_{n_\ell}}{\superb{W}{\ell}}
    \end{bmatrix} = \begin{bmatrix} \begin{bmatrix} \superb{z}{\ell - 1})^\top \\
    ... \\ \mathbf{0}^\top \end{bmatrix}\\ \dots \\ \begin{bmatrix} \mathbf{0}^\top
    \\ \dots \\ (\superb{z}{\ell - 1})^\top\end{bmatrix}\end{bmatrix} \in \R^{D_\ell
    \times (D_\ell \times D_{\ell - 1})}. \end{align*} \end{split}\]
- en: 2.2.4 One Last Gradient
  id: totrans-402
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 最后一个梯度
- en: We now have all the results necessary to calculate the derivative of the loss
    function with respect to the weights in the *final* layer. For instance, we can
    evaluate
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了计算相对于最终层权重的损失函数导数所需的所有结果。例如，我们可以评估
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
- en: 'using the results from sections 2.1, 2.2, and 2.3\. However, to obtain the
    derivative of the loss function with respect to weights in the *previous* layers,
    we need one more derivative: the derivative of \(\superb{h}{\ell}\), the linear
    mapping in layer \(\ell\), with respect to \(\superb{z}{\ell - 1}\), the output
    of the previous layer. Fortunately, this derivative is simple:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第2.1节、2.2节和2.3节的结果。然而，为了获得相对于前一层权重的损失函数的导数，我们需要一个额外的导数：\(\superb{h}{\ell}\)的导数，即层\(\ell\)中的线性映射，相对于\(\superb{z}{\ell
    - 1}\)，即前一层输出。幸运的是，这个导数很简单：
- en: \[ \dadb{\superb{h}{\ell}}{\superb{z}{\ell - 1}} = {\superb{W}{\ell}}. \]
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\superb{h}{\ell}}{\superb{z}{\ell - 1}} = {\superb{W}{\ell}}. \]
- en: Now that we have \(\partial \superb{h}{\ell}/\partial \superb{z}{\ell - 1}\),
    we reuse the results from sections 2.2 and 2.3 to calculate \(\partial \superb{z}{\ell
    - 1}/\partial \superb{h}{\ell - 1}\) and \(\partial \superb{h}{\ell - 1}/ \partial
    \superb{W}{\ell - 1}\) (respectively); this gives us all the necessary results
    to compute the gradient of the weights in the previous layer. We then rinse, lather,
    and repeat with layer \(\ell - 2\) through the first layer.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了\(\partial \superb{h}{\ell}/\partial \superb{z}{\ell - 1}\)，我们重新使用第2.2节和第2.3节的结果来计算\(\partial
    \superb{z}{\ell - 1}/\partial \superb{h}{\ell - 1}\)和\(\partial \superb{h}{\ell
    - 1}/ \partial \superb{W}{\ell - 1}\)（分别）；这为我们计算前一层权重梯度提供了所有必要的结果。然后我们用层\(\ell
    - 2\)到第一层重复这个过程。
- en: 2.3 Combining Results with the Chain Rule
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 使用链式法则结合结果
- en: '![](../Images/ddd4201d5226ba07c2ffe7c422934b04.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddd4201d5226ba07c2ffe7c422934b04.png)'
- en: We’ve seen lots of individual derivatives. Ultimately, we really care about
    the derivatives of the loss function with respect to the network’s weights. Let’s
    review by calculating the derivatives of the loss function with respect to the
    weights in the final layer for the familiar network above. Suppose \(\super{f}{2}\)
    is the Sigmoid function and we use the log loss. For \(\superb{W}{2}\) we get
    the following.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了很多个单独的导数。最终，我们真正关心的是相对于网络权重的损失函数的导数。让我们通过计算相对于上面熟悉网络的最终层权重的损失函数的导数来回顾一下。假设\(\super{f}{2}\)是Sigmoid函数，我们使用对数损失。对于\(\superb{W}{2}\)，我们得到以下结果。
- en: \[\begin{split} \begin{align*} \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{2}}
    &= \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{2}}\cdot
    \dadb{\super{\mathbf{h}}{2}}{\superb{W}{2}} \\ &=-\left(\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top \cdot \text{diag}\left(\sigma(\superb{h}{2})(1-
    \sigma(\superb{h}{2}))\right)\cdot \mathbf{T} \\ &= -\begin{bmatrix} (\frac{y_1}{\hat{y}_1}
    + \frac{1-y_1}{1-\hat{y}_1})\cdot \sigma(\super{h}{2}_1)(1-\sigma(\super{h}{2}_1))\cdot
    \superb{z}{1} \\ \dots \\ (\frac{y_{n_2}}{\hat{y}_{n_2}} + \frac{1-y_{n_2}}{1-\hat{y}_{n_2}})\cdot
    \sigma(\super{h}{2}_{n_2})(1-\sigma(\super{h}{2}_{n_2}))\cdot \superb{z}{1} \end{bmatrix}
    \in \R^{n_2 \times n_1}, \end{align*} \end{split}\]
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{2}}
    &= \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{2}}\cdot
    \dadb{\super{\mathbf{h}}{2}}{\superb{W}{2}} \\ &=-\left(\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top \cdot \text{diag}\left(\sigma(\superb{h}{2})(1-
    \sigma(\superb{h}{2}))\right)\cdot \mathbf{T} \\ &= -\begin{bmatrix} (\frac{y_1}{\hat{y}_1}
    + \frac{1-y_1}{1-\hat{y}_1})\cdot \sigma(\super{h}{2}_1)(1-\sigma(\super{h}{2}_1))\cdot
    \superb{z}{1} \\ \dots \\ (\frac{y_{n_2}}{\hat{y}_{n_2}} + \frac{1-y_{n_2}}{1-\hat{y}_{n_2}})\cdot
    \sigma(\super{h}{2}_{n_2})(1-\sigma(\super{h}{2}_{n_2}))\cdot \superb{z}{1} \end{bmatrix}
    \in \R^{n_2 \times n_1}, \end{align*} \end{split}\]
- en: where \(\mathbf{T}\) is the tensor derivative discussed in section 2.2.3.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{T}\) 是第 2.2.3 节中讨论的张量导数。
- en: For \(\superb{c}{2}\), we get
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(\superb{c}{2}\)，我们得到
- en: \[\begin{split} \begin{align*} \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{2}}
    &= \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{2}}\cdot
    \dadb{\super{\mathbf{h}}{2}}{\superb{c}{2}} \\ &=-\left(\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top \cdot \text{diag}\left(\sigma(\superb{h}{2})(1-
    \sigma(\superb{h}{2}))\right)\cdot I_{n_2} \\ &= -\begin{bmatrix} (\frac{y_1}{\hat{y}_1}
    + \frac{1-y_1}{1-\hat{y}_1})\cdot \sigma(\super{h}{2}_1)(1-\sigma(\super{h}{2}_1))
    \\ \dots \\ (\frac{y_{n_2}}{\hat{y}_{n_2}} + \frac{1-y_{n_2}}{1-\hat{y}_{n_2}})\cdot
    \sigma(\super{h}{2}_{n_2})(1-\sigma(\super{h}{2}_{n_2})) \end{bmatrix} \in \R^{n_2}.
    \end{align*} \end{split}\]
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{2}}
    &= \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{2}}\cdot
    \dadb{\super{\mathbf{h}}{2}}{\superb{c}{2}} \\ &=-\left(\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top \cdot \text{diag}\left(\sigma(\superb{h}{2})(1-
    \sigma(\superb{h}{2}))\right)\cdot I_{n_2} \\ &= -\begin{bmatrix} (\frac{y_1}{\hat{y}_1}
    + \frac{1-y_1}{1-\hat{y}_1})\cdot \sigma(\super{h}{2}_1)(1-\sigma(\super{h}{2}_1))
    \\ \dots \\ (\frac{y_{n_2}}{\hat{y}_{n_2}} + \frac{1-y_{n_2}}{1-\hat{y}_{n_2}})\cdot
    \sigma(\super{h}{2}_{n_2})(1-\sigma(\super{h}{2}_{n_2})) \end{bmatrix} \in \R^{n_2}.
    \end{align*} \end{split}\]
- en: 3\. Combining Observations
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3. 结合观察
- en: 'So far, we’ve only considered the derivative of the loss function for a *single*
    observation. When training a network, we will of course want to consider the entire
    dataset. One way to do so is to simply add the derivatives of the loss function
    with respect to the weights across observations. Since the loss over the dataset
    is the sum of the individual observation losses and the derivative of a sum is
    the sum of the derivatives, we can simply add the results above. For instance,
    to find the derivative of the loss with respect to the final matrix of weights
    \(\superb{W}{L}\), we could loop through observations and sum the individual derivatives:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了单个观察的损失函数的导数。在训练网络时，我们当然希望考虑整个数据集。这样做的一种方法是将观察之间的损失函数相对于权重的导数相加。由于数据集上的损失是单个观察损失的加和，而导数的和是各个导数的和，我们可以简单地相加上述结果。例如，为了找到相对于最终权重矩阵
    \(\superb{W}{L}\) 的损失导数，我们可以遍历观察并累加单个导数：
- en: \[ \dadb{\mathcal{L}(\{\hat{\by}_n\}_{n = 1}^N))}{\superb{W}{L}} = \dadb{\sumN
    \mathcal{L}(\hat{\by}_n)}{\superb{W} {L}} = \sumN \dadb{\mathcal{L}(\hat{\by}_n)}{\superb{W}{L}}.
    \]
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\{\hat{\by}_n\}_{n = 1}^N))}{\superb{W}{L}} = \dadb{\sumN
    \mathcal{L}(\hat{\by}_n)}{\superb{W} {L}} = \sumN \dadb{\mathcal{L}(\hat{\by}_n)}{\superb{W}{L}}.
    \]
- en: While straightforward, this approach is computationally inefficient. The rest
    of this section outlines a more complicated but *much* faster method.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法简单直接，但在计算上效率不高。本节的其余部分概述了一种更复杂但*快得多*的方法。
- en: 3.1 A New Representation
  id: totrans-419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1 新的表示方法
- en: So far, we’ve treated our predictors and outputs as vectors. The network starts
    with \(\bx\) and outputs \(\superb{z}{1}\). Then it predicts with \(\superb{z}{1}\)
    and outputs \(\superb{z}{2}\). It repeats this process until \(\superb{z}{L-1}\)
    outputs \(\mathbf{\hat{y}}\). To incorporate multiple observations, we can turn
    these vectors into matrices. Again suppose our dataset consists of \(N\) observations
    with \(\bx_n \in \R^{D_x}\) and \(\by_n \in \R^{D_y}\). We start with \(\bX \in
    \R^{N \times D_x}\), whose \(n^\text{th}\) row is \(\bx_n\). Note that in \(\bX\),
    \(\bx_n\) is a row vector; to keep consistent with our previous sections, we want
    it to be a column vector. So, we’ll work with \(\bX^\top\) rather than \(\bX\).
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直将预测值和输出视为向量。网络从 \(\bx\) 开始，输出 \(\superb{z}{1}\)。然后它用 \(\superb{z}{1}\)
    进行预测，并输出 \(\superb{z}{2}\)。它重复这个过程，直到 \(\superb{z}{L-1}\) 输出 \(\mathbf{\hat{y}}\)。为了包含多个观测值，我们可以将这些向量转换为矩阵。再次假设我们的数据集由
    \(N\) 个观测值组成，其中 \(\bx_n \in \R^{D_x}\) 和 \(\by_n \in \R^{D_y}\)。我们从 \(\bX \in
    \R^{N \times D_x}\) 开始，其第 \(n^\text{th}\) 行是 \(\bx_n\)。注意在 \(\bX\) 中，\(\bx_n\)
    是一个行向量；为了与前面的章节保持一致，我们希望它是一个列向量。因此，我们将使用 \(\bX^\top\) 而不是 \(\bX\)。
- en: '![](../Images/741660f5553d62898ffec20314efc58c.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/741660f5553d62898ffec20314efc58c.png)'
- en: Rather than feeding each observation through the network at once, we will feed
    all observations together and give each observation its own column. Each column
    in \(\bX^\top\) represents an observation’s predictors. We then multiply this
    matrix by \(\superb{W}{1}\) and add \(\superb{c}{1}\) *element-wise* to get \(\superb{H}{1}\).
    Each column in \(\superb{H}{1}\) represents a vector of linear combinations of
    the corresponding column in \(\bX^\top\). We then pass \(\superb{H}{1}\) through
    an activation function to obtain \(\superb{Z}{1}\). Similarly, each column in
    \(\superb{Z}{1}\) represents the output vector for the corresponding observation
    in \(\bX^\top\). We then repeat, with \(\superb{Z}{1}\) acting as the matrix of
    predictors for the next layer. Ultimately, we will obtain a matrix \(\hat{\mathbf{Y}}^\top
    \in \R^{D_y \times N}\) whose \(n^\text{th}\) column represents the vector of
    fitted values for the \(n^\text{th}\) observation.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是一次性将每个观测值通过网络，我们将所有观测值一起输入，并为每个观测值分配其自己的列。在 \(\bX^\top\) 中的每一列代表一个观测值的预测因子。然后我们用
    \(\superb{W}{1}\) 乘以这个矩阵，并 *逐元素* 加上 \(\superb{c}{1}\) 以得到 \(\superb{H}{1}\)。在
    \(\superb{H}{1}\) 中的每一列代表 \(\bX^\top\) 中对应列的线性组合向量。然后我们通过激活函数将 \(\superb{H}{1}\)
    传递，以获得 \(\superb{Z}{1}\)。同样，在 \(\superb{Z}{1}\) 中的每一列代表 \(\bX^\top\) 中对应观测值的输出向量。然后我们重复这个过程，其中
    \(\superb{Z}{1}\) 作为下一层的预测因子矩阵。最终，我们将获得一个矩阵 \(\hat{\mathbf{Y}}^\top \in \R^{D_y
    \times N}\)，其第 \(n^\text{th}\) 列代表第 \(n^\text{th}\) 个观测值的拟合值向量。
- en: 3.2 Gradients
  id: totrans-423
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2 梯度
- en: While this new representation is more efficient, it also makes the gradients
    more complicated since we are taking derivatives with respect to matrices rather
    than vectors. Ordinarily, the derivative of one matrix with respect to another
    would be a four-dimensional tensor. Luckily, there’s a shortcut.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种新的表示方法更高效，但它也使得梯度更复杂，因为我们是在矩阵而不是向量上求导。通常，一个矩阵相对于另一个矩阵的导数将是一个四维张量。幸运的是，有一个捷径。
- en: For each parameter \(\theta\) in our network, we will find its gradient by asking
    “which parameters does \(\theta\) affect in the next layer”. Supposing the answer
    is some set \(\{\psi_1, \psi_2, \dots, \psi_n\},\) we will calculate the derivative
    of the loss function with respect to \(\theta\) as
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们网络中的每个参数 \(\theta\)，我们将通过询问“\(\theta\) 影响下一层的哪些参数”来找到其梯度。假设答案是某个集合 \(\{\psi_1,
    \psi_2, \dots, \psi_n\}\)，我们将计算损失函数相对于 \(\theta\) 的导数。
- en: \[ \dadb{\mathcal{L}}{\theta} = \sum_{i = 1}^n \dadb{L}{\psi_i}\cdot \dadb{\psi_i}{\theta}.
    \]
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\theta} = \sum_{i = 1}^n \dadb{L}{\psi_i}\cdot \dadb{\psi_i}{\theta}.
    \]
- en: '![](../Images/93567a4ed40bb43e9a9bdb2977ce6007.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93567a4ed40bb43e9a9bdb2977ce6007.png)'
- en: Recall that our loss function is a vector \(\bf{\mathcal{L}}\) of size \(D_y\)
    since we have \(D_y\) output variables. This loss vector is a *row-wise* function
    of the prediction matrix, \(\hat{\mathbf{Y}}^\top\), meaning the \(d^\text{th}\)
    entry in \(\mathbf{\mathcal{L}}\) is a function of only the \(d^\text{th}\) row
    of \(\hat{\mathbf{Y}}^\top\) (which represents the fitted values for the \(d^\text{th}\)
    output variable across observations). For the \((i, d)^\text{th}\) entry in \(\hat{\mathbf{Y}}^\top\),
    then, we only need to consider the derivative of the \(d^\text{th}\) entry in
    \(\mathbf{\mathcal{L}}\)—the derivative of any other entry in \(\mathcal{L}\)
    with respect \(\hat{\mathbf{Y}}^\top_{i, d}\) is 0\. We can then use the following
    gradient in place of a four-dimensional tensor.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的损失函数 \(\bf{\mathcal{L}}\) 是一个大小为 \(D_y\) 的向量，因为我们有 \(D_y\) 个输出变量。这个损失向量是预测矩阵
    \(\hat{\mathbf{Y}}^\top\) 的 *行向量* 函数，这意味着 \(\mathbf{\mathcal{L}}\) 中的第 \(d^\text{th}\)
    个元素仅是 \(\hat{\mathbf{Y}}^\top\) 的第 \(d^\text{th}\) 行的函数（它代表了第 \(d^\text{th}\)
    个输出变量在观察中的拟合值）。对于 \(\hat{\mathbf{Y}}^\top\) 中的 \((i, d)^\text{th}\) 个元素，那么，我们只需要考虑
    \(\mathbf{\mathcal{L}}\) 中第 \(d^\text{th}\) 个元素的导数——\(\mathcal{L}\) 中其他任何元素的导数相对于
    \(\hat{\mathbf{Y}}^\top_{i, d}\) 都是 0。然后，我们可以使用以下梯度来代替四维张量。
- en: \[\begin{split} \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top} = \begin{bmatrix}
    \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,1}} & ... & \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,N}}
    \\ & ... & \\ \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,1}} & ... &
    \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,N}}\end{bmatrix} \end{split}\]
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top} = \begin{bmatrix}
    \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,1}} & ... & \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,N}}
    \\ & ... & \\ \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,1}} & ... &
    \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,N}}\end{bmatrix} \end{split}\]
- en: Next, we consider the derivative of \(\mathbf{\hat{Y}}^\top\) with respect to
    \(\superb{H}{L}\). Note that \(\mathbf{\hat{Y}}^\top\) is an *element-wise* function
    of \(\superb{H}{L}\). This means we only need to consider the gradient of each
    element in the former with respect to its corresponding element in the latter.
    This gives us
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们考虑 \(\mathbf{\hat{Y}}^\top\) 对 \(\superb{H}{L}\) 的导数。注意，\(\mathbf{\hat{Y}}^\top\)
    是 \(\superb{H}{L}\) 的 *逐元素* 函数。这意味着我们只需要考虑前者中每个元素相对于后者中相应元素的梯度。这给我们
- en: \[\begin{split} \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}} = \begin{bmatrix}
    \dadb{\mathbf{\hat{Y}}^\top_{1,1}}{\superb{H}{L}_{1,1}} & ... & \dadb{\mathbf{\hat{Y}}^\top_{1,N}}{\superb{H}{L}_{1,N}}
    \\ & ... & \\ \dadb{\mathbf{\hat{Y}}^\top_{D_y,1}}{\superb{H}{1}_{D_y,1}} & ...
    & \dadb{\mathbf{\hat{Y}}^\top_{D_y,N}}{\superb{L}{L}_{D_y,N}} \end{bmatrix}. \end{split}\]
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}} = \begin{bmatrix}
    \dadb{\mathbf{\hat{Y}}^\top_{1,1}}{\superb{H}{L}_{1,1}} & ... & \dadb{\mathbf{\hat{Y}}^\top_{1,N}}{\superb{H}{L}_{1,N}}
    \\ & ... & \\ \dadb{\mathbf{\hat{Y}}^\top_{D_y,1}}{\superb{H}{1}_{D_y,1}} & ...
    & \dadb{\mathbf{\hat{Y}}^\top_{D_y,N}}{\superb{L}{L}_{D_y,N}} \end{bmatrix}. \end{split}\]
- en: Now let’s use the shortcut described above. Since each element in \(\superb{H}{L}\)
    only affects the corresponding element in \(\mathbf{\hat{Y}}^\top\), we calculate
    \(\partial \mathcal{L}/\partial \superb{H}{L}\) by multiplying the two gradients
    above *element-wise*. I.e.,
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用上面描述的快捷方式。由于 \(\superb{H}{L}\) 中的每个元素只影响 \(\mathbf{\hat{Y}}^\top\) 中的相应元素，我们通过逐元素乘以上述两个梯度来计算
    \(\partial \mathcal{L}/\partial \superb{H}{L}\)。即，
- en: \[ \dadb{\mathcal{L}}{\superb{H}{L}} = \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top}
    \circ \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}}, \]
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\superb{H}{L}} = \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top}
    \circ \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}}, \]
- en: where \(\circ\) is the element-wise multiplication operator, also known as the
    *Hadamard product*.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\circ\) 是逐元素乘法运算符，也称为 *Hadamard 积*。
- en: Next up is \(\superb{c}{L}\). Whereas each element in \(\superb{H}{L}\) affected
    only one element in \(\mathbf{\hat{Y}}^\top\), each element in \(\superb{c}{L}\)
    affects \(N\) elements in \(\superb{H}{L}\)—every element in its corresponding
    row. Consider the first entry in \(\superb{c}{L}\). Since this entry affects each
    entry in the first row of \(\superb{H}{L}\), the chain rule gives us
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 \(\superb{c}{L}\)。与 \(\superb{H}{L}\) 中的每个元素只影响 \(\mathbf{\hat{Y}}^\top\)
    中的一个元素不同，\(\superb{c}{L}\) 中的每个元素会影响 \(\superb{H}{L}\) 中的 \(N\) 个元素——其对应行的每个元素。考虑
    \(\superb{c}{L}\) 中的第一个元素。由于这个元素影响 \(\superb{H}{L}\) 第一行的每个元素，链式法则给出
- en: \[ \dadb{\mathcal{L}}{\super{c}{L}_1} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{1,n}}\cdot\dadb{\superb{H}{L}_{1,n}}{\super{c}{L}_1}.
    \]
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\super{c}{L}_1} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{1,n}}\cdot\dadb{\superb{H}{L}_{1,n}}{\super{c}{L}_1}.
    \]
- en: Fortunately \(\partial \superb{H}{L}_{1,n}/\partial \super{c}{L}_1\) is just
    1 since \(\super{c}{L}_1\) is an intercept term. This implies that the derivative
    of the loss function with respect to \(\superb{c}{1}\) is just the row sum of
    \(\partial\mathcal{L}/\partial \superb{H}{L}\), or
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是 \(\partial \superb{H}{L}_{1,n}/\partial \super{c}{L}_1\) 只是 1，因为 \(\super{c}{L}_1\)
    是一个截距项。这意味着损失函数相对于 \(\superb{c}{1}\) 的导数只是 \(\partial\mathcal{L}/\partial \superb{H}{L}\)
    的行和，或者
- en: \[ \dadb{\mathcal{L}}{\super{c}{L}_i} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,n}}.
    \]
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\super{c}{L}_i} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,n}}.
    \]
- en: Next, we have \(\superb{W}{L}\). Using our shortcut, we ask “which values does
    the \((i, j)^\text{th}\) entry in \(\superb{W}{L}\) affect?” Since \(\superb{H}{L}
    = \superb{W}{L}\superb{Z}{L-1}\), we have that
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有 \(\superb{W}{L}\)。使用我们的快捷方式，我们问“\(\superb{W}{L}\) 的 \((i, j)^\text{th}\)
    个元素影响哪些值？”由于 \(\superb{H}{L} = \superb{W}{L}\superb{Z}{L-1}\)，我们有
- en: \[ \superb{H}{L}_{i,n} = \superb{W}{L}_{i, j} \superb{Z}{L-1}_{j,n} \hspace{1mm}
    \forall \hspace{1mm} n \in \{1, \dots, N\}. \]
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \superb{H}{L}_{i,n} = \superb{W}{L}_{i, j} \superb{Z}{L-1}_{j,n} \hspace{1mm}
    \forall \hspace{1mm} n \in \{1, \dots, N\}. \]
- en: This tells us that \(\superb{W}{L}_{i,j}\) affects each entry in the \(i^\text{th}\)
    row of \(\superb{H}{L}\) and gives us the derivative \(\partial{\superb{H}{L}_{i,
    n}}/\partial \superb{W}{L}_{i, j} = \superb{Z}{L-1}_{j, n}.\) Therefore,
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们 \(\superb{W}{L}_{i,j}\) 影响到 \(\superb{H}{L}\) 的第 \(i\) 行的每一个元素，并给出了导数
    \(\partial{\superb{H}{L}_{i, n}}/\partial \superb{W}{L}_{i, j} = \superb{Z}{L-1}_{j,
    n}.\) 因此，
- en: \[ \dadb{\mathcal{L}}{\superb{W}{L}_{i, j}} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,
    n}}\cdot\dadb{\superb{H}{L}_{i, n}}{\superb{W}{L}_{i, j}} = \sumN (\nabla \superb{H}{L})_{i,
    n}\cdot{\superb{Z}{L-1}_{j,n}}, \]
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\superb{W}{L}_{i, j}} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,
    n}}\cdot\dadb{\superb{H}{L}_{i, n}}{\superb{W}{L}_{i, j}} = \sumN (\nabla \superb{H}{L})_{i,
    n}\cdot{\superb{Z}{L-1}_{j,n}}, \]
- en: where \(\nabla \superb{H}{L}\) is the matrix representing \(\partial \mathcal{L}/\partial\superb{H}{L}\).
    This can be computed for each element in \(\superb{W}{L}\) using a tensor dot
    product, which will be covered in the construction section.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\nabla \superb{H}{L}\) 是表示 \(\partial \mathcal{L}/\partial\superb{H}{L}\)
    的矩阵。这可以通过张量点积计算 \(\superb{W}{L}\) 中的每个元素，这将在构造部分进行介绍。
- en: Finally, we have \(\superb{Z}{L-1}\). This case is symmetric to \(\superb{W}{L}\),
    and the same approach gives us the result
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有 \(\superb{Z}{L-1}\)。这种情况与 \(\superb{W}{L}\) 是对称的，同样的方法给出了结果
- en: \[ \dadb{\mathcal{L}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R \dadb{\mathcal{L}}{\superb{H}{L}_{r,n}}\cdot\dadb{\superb{H}{L}_{r,
    n}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R {(\nabla \superb{H}{L})}_{r, n}\cdot{\superb{W}{L}_{r,i}}.
    \]
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R \dadb{\mathcal{L}}{\superb{H}{L}_{r,n}}\cdot\dadb{\superb{H}{L}_{r,
    n}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R {(\nabla \superb{H}{L})}_{r, n}\cdot{\superb{W}{L}_{r,i}}.
    \]
- en: Again, the derivative for all of \(\superb{Z}{L-1}\) can be calculated at once
    using a tensor dot product.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，所有 \(\superb{Z}{L-1}\) 的导数可以使用张量点积一次计算出来。
- en: 2.1 Back Propagation
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 反向传播
- en: Suppose we choose some loss function \(\mathcal{L}\) for our network to minimize.
    Note that because our target variable is multi-dimensional, \(\boldsymbol{\mathcal{L}}\)
    function will be a vector of losses (e.g. the loss for the first target, the loss
    for the second target, etc.). To find the network’s optimal weights, we can conduct
    gradient descent, repeatedly taking the derivative of our loss function with respect
    to each weight and adjusting accordingly. As we will see, this involves finding
    the gradient of the network’s final weights, then using the chain rule to find
    the gradient of the weights that came earlier. In this process, we move backward
    through the network, and hence the name “back propagation.”
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们为我们的网络选择一个损失函数 \(\mathcal{L}\) 以最小化。请注意，因为我们的目标变量是多维的，\(\boldsymbol{\mathcal{L}}\)
    函数将是一个损失向量（例如，第一个目标的损失，第二个目标的损失等）。为了找到网络的优化权重，我们可以进行梯度下降，重复地对我们损失函数相对于每个权重的导数进行计算并相应地调整。正如我们将看到的，这涉及到找到网络最终权重的梯度，然后使用链式法则找到先前权重的梯度。在这个过程中，我们沿着网络向后移动，因此得名“反向传播。”
- en: '![](../Images/1227d5a61b04d2213382374c634abd8a.png)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1227d5a61b04d2213382374c634abd8a.png)'
- en: Consider conducting gradient descent for the network above. Write the loss function
    as \(\mathcal{L}(\hat{\by})\), where \(\hat{\by}\) is the network’s output. Let’s
    start by writing out the derivative of \(\mathcal{L}\) with respect to \(\super{\mathbf{W}}{L}\),
    the final matrix of weights in our network. We can do this with the chain rule,
    as below.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑对上述网络进行梯度下降。将损失函数表示为 \(\mathcal{L}(\hat{\by})\)，其中 \(\hat{\by}\) 是网络的输出。让我们首先写出
    \(\mathcal{L}\) 对 \(\super{\mathbf{W}}{L}\)，即我们网络中最终权重矩阵的导数。我们可以使用链式法则，如下所示。
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
- en: The gradient of \(\super{\mathbf{c}}{L}\) is equivalent. The math behind these
    calculations is covered in the following section. Next, we want to find the gradient
    of \(\super{\mathbf{W}}{L-1}\), shown below.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: \(\super{\mathbf{c}}{L}\) 的梯度是等效的。这些计算背后的数学将在下一节中介绍。接下来，我们想要找到 \(\super{\mathbf{W}}{L-1}\)
    的梯度，如下所示。
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L-1}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}
    \cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}} \cdot \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{z}}{L-1}}
    \cdot \dadb{\super{\mathbf{z}}{L-1}}{\super{\mathbf{h}}{L-1}} \cdot \dadb{\super{\mathbf{h}}{L-1}}{\super{\mathbf{W}}{L-1}}
    \]
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L-1}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}
    \cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}} \cdot \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{z}}{L-1}}
    \cdot \dadb{\super{\mathbf{z}}{L-1}}{\super{\mathbf{h}}{L-1}} \cdot \dadb{\super{\mathbf{h}}{L-1}}{\super{\mathbf{W}}{L-1}}
    \]
- en: 'This expression is pretty ugly, but there is a shortcut. This gradient and
    the gradient of \(\super{\mathbf{W}}{L}\) share the first two terms, which represent
    the gradient of \(\super{\mathbf{h}}{L}\). To save time (both in writing out the
    gradients and in calculating them in practice), we can record this gradient, \(\nabla
    \super{\mathbf{h}}{L}\), and apply it where necessary. We can do the same with
    \(\nabla \mathbf{h}^{(L-1)}\), which simplifies the gradient of \(\mathbf{W}^{(L-2)}\):'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式看起来相当丑陋，但有一个捷径。这个梯度与 \(\super{\mathbf{W}}{L}\) 的梯度共享前两个项，这代表了 \(\super{\mathbf{h}}{L}\)
    的梯度。为了节省时间（在写出梯度以及在实践中计算它们时），我们可以记录这个梯度，\(\nabla \super{\mathbf{h}}{L}\)，并在需要的地方应用它。我们也可以对
    \(\nabla \mathbf{h}^{(L-1)}\) 做同样的事情，这简化了 \(\mathbf{W}^{(L-2)}\) 的梯度：
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L-2}} = \nabla \super{\mathbf{h}}{L-1}
    \cdot \dadb{\super{\mathbf{h}}{L-1}}{\super{\mathbf{z}}{L-2}} \cdot \dadb{\super{\mathbf{z}}{L-2}}{\super{\mathbf{h}}{L-2}}
    \cdot \dadb{\super{\mathbf{h}}{L-2}}{\super{\mathbf{W}}{L-2}}. \]
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L-2}} = \nabla \super{\mathbf{h}}{L-1}
    \cdot \dadb{\super{\mathbf{h}}{L-1}}{\super{\mathbf{z}}{L-2}} \cdot \dadb{\super{\mathbf{z}}{L-2}}{\super{\mathbf{h}}{L-2}}
    \cdot \dadb{\super{\mathbf{h}}{L-2}}{\super{\mathbf{W}}{L-2}}. \]
- en: We continue this same process until we reach the first set of weights.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续这个过程，直到达到第一组权重。
- en: We’ve now seen intuitively how to find the gradients for our network’s many
    weights. To conduct back propagation, we simply use these gradients to run gradient
    descent. Next, let’s see how to actually calculate these gradients.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在直观地看到了如何找到我们网络中许多权重的梯度。为了进行反向传播，我们只需使用这些梯度来运行梯度下降。接下来，让我们看看如何实际计算这些梯度。
- en: 2.2 Calculating Gradients
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 计算梯度
- en: 'In this section we will derive the gradients used in back propagation. For
    each iteration in this process we need to know the derivative of our loss function
    with respect to each weight in the entire network. For the network shown above,
    this requires calculating the following gradients:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将推导反向传播中使用的梯度。对于这个过程中的每一迭代，我们需要知道我们的损失函数相对于整个网络中每个权重的导数。对于上述网络，这需要计算以下梯度：
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{1}}, \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{1}},
    \dots, \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{L}}, \text{ and } \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{L}}.
    \]
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{1}}, \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{1}},
    \dots, \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{L}}, \text{ and } \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{L}}.
    \]
- en: Since we will find these with the chain rule, we will need to calculate other
    gradients along the way. All the necessary gradients are derived below. Note that
    the following sub-sections cover the stages within a single layer of a network
    in reverse order (as back propagation does).
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用链式法则来找到这些梯度，因此我们需要在途中计算其他梯度。所有必要的梯度都在下面推导。注意，以下子节以反向顺序（正如反向传播所做的那样）覆盖了网络单层中的各个阶段。
- en: Note
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the rest of this section considers only one observation at a time.
    The vector \(\by\), for instance, refers to the output variables for a single
    observation, rather than a vector of 1-dimensional output variables for several
    observations. Similarly, \(\partial \mathcal{L}(\hat{\by})/\partial\hat{\by}\)
    refers to the derivative of the loss with respect to a single observation’s output.
    The final section discusses how to combine the derivatives from multiple observations.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，本节的其余部分只考虑一次观察。例如，向量 \(\by\) 指的是单个观察的输出变量，而不是多个观察的 1 维输出变量的向量。同样，\(\partial
    \mathcal{L}(\hat{\by})/\partial\hat{\by}\) 指的是相对于单个观察输出变量的损失导数。最后一节讨论如何将多个观察的导数组合起来。
- en: For the following, let there be \(L\) layers in total. Also let layer \(\ell\)
    have size \(D_\ell\), except the input and output layers which have sizes \(D_x\)
    and \(D_y\), respectively.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下内容，假设总共有 \(L\) 层。也假设层 \(\ell\) 的大小为 \(D_\ell\)，除了输入层和输出层，它们的大小分别为 \(D_x\)
    和 \(D_y\)。
- en: 2.2.1 Loss Functions and their Gradients
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 损失函数及其梯度
- en: '![](../Images/6e20022b03b555510592958ec46ee03d.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e20022b03b555510592958ec46ee03d.png)'
- en: 'Back propagation begins where the network ends: the loss function \(\mathcal{L}(\hat{\by})\).
    Let’s start by introducing some common loss functions and their derivatives with
    respect to our predictions, \(\hat{\by}\). Later, using the chain rule, we will
    use these derivatives to calculate the derivatives with respect to our network’s
    weights.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播从网络结束的地方开始：损失函数 \(\mathcal{L}(\hat{\by})\)。让我们首先介绍一些常见的损失函数及其相对于预测 \(\hat{\by}\)
    的导数。稍后，我们将使用链式法则，使用这些导数来计算相对于我们网络权重的导数。
- en: A common loss function for quantitative output variables is the residual sum
    of squares. For a single observation, the loss is
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定量输出变量，一个常见的损失函数是残差平方和。对于单个观察，损失是
- en: \[ \mathcal{L}_{RSS}(\hat{\by}) = (\by - \hat{\by})^2. \]
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}_{RSS}(\hat{\by}) = (\by - \hat{\by})^2. \]
- en: Note
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the loss is a function of both our predictions (\(\hat{\by}\)) and
    the true targets (\(\by\)). However, since the true targets are fixed, we can
    only manipulate \(\hat{\by}\), so we write the loss as only a function of \(\hat{\by}\).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，损失是我们的预测 (\(\hat{\by}\)) 和真实目标 (\(\by\)) 的函数。然而，由于真实目标是固定的，我们只能操作 \(\hat{\by}\)，因此我们将损失写成仅是
    \(\hat{\by}\) 的函数。
- en: Note that we have a vector of losses because there are multiple output variables
    and we consider the loss for each variable independently. Now for the first step
    in back propagation, we calculate the derivative of this loss with respect to
    \(\hat{\by}\), which is simply given by
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们有一个损失向量，因为存在多个输出变量，并且我们独立地考虑每个变量的损失。现在，在反向传播的第一步中，我们计算这个损失相对于 \(\hat{\by}\)
    的导数，它简单地给出如下：
- en: \[ \dadb{\mathcal{L}_{RSS}(\hat{\by})}{\hat{\by}} = -2(\by - \hat{\by})^\top
    \in \R^{1 \times D_y}. \]
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}_{RSS}(\hat{\by})}{\hat{\by}} = -2(\by - \hat{\by})^\top
    \in \R^{1 \times D_y}. \]
- en: Since we are using the numerator layout convention, this derivative is a length-\(D_y\)
    row vector, or equivalently a \(1\) by \(D_y\) matrix.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是分子布局约定，这个导数是一个长度为 \(D_y\) 的行向量，或者等价地是一个 \(1\) 乘以 \(D_y\) 的矩阵。
- en: For binary classification problems, a common loss function is the log loss or
    cross entropy, given by
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类问题，一个常见的损失函数是对数损失或交叉熵，给出如下：
- en: \[ \mathcal{L}_{Log}(\hat{\by}) = -\Big(\by\log \hat{\by}+(1-\by)\log(1-\hat{\by})\Big),
    \]
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}_{Log}(\hat{\by}) = -\Big(\by\log \hat{\by}+(1-\by)\log(1-\hat{\by})\Big),
    \]
- en: where the \(i^\text{th}\) entry in \(\hat{\by}\) gives the estimated probability
    that the \(i^\text{th}\) output variable equals 1\. The derivative of this loss
    function with respect to \(\hat{\by}\) is given by
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，\(\hat{\by}\) 中的第 \(i^\text{th}\) 个条目给出了第 \(i^\text{th}\) 个输出变量等于 1 的估计概率。这个损失函数相对于
    \(\hat{\by}\) 的导数给出如下：
- en: \[ \begin{align*} \dadb{\mathcal{L}_{Log}(\hat{\by})}{\hat{\by}} &= \left(-\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top\in \R^{1 \times D_y}. \end{align*} \]
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\mathcal{L}_{Log}(\hat{\by})}{\hat{\by}} &= \left(-\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top\in \R^{1 \times D_y}. \end{align*} \]
- en: Once we calculate \(\partial \mathcal{L}(\hat{\by})/\partial\hat{\by}\), we
    can move further back into the network. Since \(\hat{\by}\) is the result of an
    activation function, the next step in back propagation is to calculate the derivative
    of our activation functions.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算出 \(\partial \mathcal{L}(\hat{\by})/\partial\hat{\by}\)，我们就可以进一步向网络后退。由于
    \(\hat{\by}\) 是激活函数的结果，反向传播的下一步是计算我们的激活函数的导数。
- en: 2.2.2 Gradients of the Activation Functions
  id: totrans-480
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 激活函数的梯度
- en: '![](../Images/73cd03e425f8718e9a2e8b211cbb8b9c.png)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73cd03e425f8718e9a2e8b211cbb8b9c.png)'
- en: Recall that \(\superb{z}{\ell}\), the output layer of \(\ell\), is the result
    of an activation function applied to a linear mapping \(\superb{h}{\ell}\). This
    includes the output of the final layer, \(\mathbf{\hat{y}}\), which we can also
    write as \(\superb{z}{L}\).
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下 \(\superb{z}{\ell}\)，层 \(\ell\) 的输出层，是应用激活函数到线性映射 \(\superb{h}{\ell}\)
    的结果。这包括最终层的输出，\(\mathbf{\hat{y}}\)，我们也可以将其写作 \(\superb{z}{L}\)。
- en: ReLU
  id: totrans-483
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ReLU
- en: Suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) where
    \(\super{f}{\ell}\) is the ReLU function. We are interested in \(\partial \superb{z}{\ell}/\partial
    \superb{h}{\ell}\). For \(i \neq j\), we have
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) 其中 \(\super{f}{\ell}\)
    是ReLU函数。我们感兴趣的是 \(\partial \superb{z}{\ell}/\partial \superb{h}{\ell}\)。对于 \(i
    \neq j\)，我们有
- en: \[ \frac{\partial \super{z}{\ell}_i}{\partial \super{h}{\ell}_j} = 0, \]
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \super{z}{\ell}_i}{\partial \super{h}{\ell}_j} = 0, \]
- en: since \(\super{z}{\ell}_i\) is not a function of \(\super{h}{\ell}_j\). Then
    using the ReLU derivative, we have
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\super{z}{\ell}_i\) 不是 \(\super{h}{\ell}_j\) 的函数。然后使用ReLU导数，我们有
- en: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \begin{cases}
    1, & \super{h}{\ell}_i > 0 \\ 0, & \super{h}{\ell}_i \leq 0\. \end{cases} \end{split}\]
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \begin{cases}
    1, & \super{h}{\ell}_i > 0 \\ 0, & \super{h}{\ell}_i \leq 0\. \end{cases} \end{split}\]
- en: We can then compactly write the entire derivative as
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以紧凑地写出整个导数
- en: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = \text{diag}(\superb{h}{\ell}
    > 0) \in \R^{D_\ell \times D_\ell}. \]
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = \text{diag}(\superb{h}{\ell}
    > 0) \in \R^{D_\ell \times D_\ell}. \]
- en: Sigmoid
  id: totrans-490
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Now suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)
    where \(\super{f}{\ell}\) is the sigmoid function. Again, the partial derivative
    \(\partial \super{z}{\ell}_i/\partial \super{h}{\ell}_j\) is 0 for \(i \neq j\).
    By the sigmoid derivative, we have
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们有 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) 其中 \(\super{f}{\ell}\)
    是sigmoid函数。再次，偏导数 \(\partial \super{z}{\ell}_i/\partial \super{h}{\ell}_j\) 对于
    \(i \neq j\) 是0。通过sigmoid导数，我们有
- en: \[ \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \sigma(\super{h}{\ell}_i)(1-\sigma(\super{h}{\ell}_i)).
    \]
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \sigma(\super{h}{\ell}_i)(1-\sigma(\super{h}{\ell}_i)).
    \]
- en: We can again write the entire result compactly as
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次紧凑地写出整个结果
- en: \[ \begin{align*} \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} &= \text{diag}\left(\sigma(\superb{h}{\ell})(1-\sigma(\superb{h}{\ell})\right)
    \in \R^{D_\ell \times D_\ell}. \end{align*} \]
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} &= \text{diag}\left(\sigma(\superb{h}{\ell})(1-\sigma(\superb{h}{\ell})\right)
    \in \R^{D_\ell \times D_\ell}. \end{align*} \]
- en: Linear
  id: totrans-495
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Linear
- en: Finally, suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)
    where \(\super{f}{\ell}\) is the linear function. We then have
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，假设我们有 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) 其中 \(\super{f}{\ell}\)
    是线性函数。我们然后有
- en: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_j} = \begin{cases}
    1 , & i = j\\ 0, & i \neq j. \end{cases} \end{split}\]
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_j} = \begin{cases}
    1 , & i = j\\ 0, & i \neq j. \end{cases} \end{split}\]
- en: The entire gradient is then simply
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 整个梯度就是
- en: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = I_{D_{\ell}} \in \R^{D_\ell \times
    D_\ell}. \]
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = I_{D_{\ell}} \in \R^{D_\ell \times
    D_\ell}. \]
- en: 2.2.3 Gradients of the Weights
  id: totrans-500
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 权重的梯度
- en: '![](../Images/cff599cc6963d9f0e1254e3990346bc0.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cff599cc6963d9f0e1254e3990346bc0.png)'
- en: We are now finally able to calculate the gradients of our weights. Specifically,
    we will calculate \(\partial \superb{h}{\ell}/ \partial \superb{c}{\ell}\) and
    \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\) which, when combined
    with our previous results through the chain rule, will allow us to obtain the
    derivative of the loss function with respect the layer \(\ell\) weights.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在终于能够计算我们权重的梯度。具体来说，我们将计算 \(\partial \superb{h}{\ell}/ \partial \superb{c}{\ell}\)
    和 \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\)，这将通过链式法则与我们的先前结果结合，使我们能够获得损失函数相对于层
    \(\ell\) 权重的导数。
- en: Recall that we obtain \(\superb{h}{\ell}\) through
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们通过
- en: \[ \superb{h}{\ell} = \superb{W}{\ell}\superb{z}{\ell-1} + \superb{c}{\ell},
    \]
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \superb{h}{\ell} = \superb{W}{\ell}\superb{z}{\ell-1} + \superb{c}{\ell},
    \]
- en: giving us the simple derivative
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 给我们简单的导数
- en: \[ \dadb{\super{\mathbf{h}}{\ell}}{\superb{c}{\ell}} = I_{D_\ell} \in \R^{D_\ell
    \times D_\ell}. \]
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\super{\mathbf{h}}{\ell}}{\superb{c}{\ell}} = I_{D_\ell} \in \R^{D_\ell
    \times D_\ell}. \]
- en: The derivative \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\) is more
    complicated. Since we are taking the derivative of a vector with respect to a
    matrix, our result will be a tensor. The shape of this tensor will be \(D_\ell
    \times (D_\ell \times D_{\ell - 1})\) since \(\superb{h}{\ell} \in R^{D_\ell}\)
    and \(\superb{W}{\ell} \in \R^{D_\ell \times D_{\ell-1}}\). The first element
    of this tensor is given by \(\partial \super{h}{\ell}_1/ \partial \superb{W}{\ell}\).
    Using the expression for \(\superb{h}{\ell}\) above, we see that this is a matrix
    with \((\superb{z}{\ell - 1})^\top\) in the first row and 0s everywhere else.
    More generally, the \(i^\text{th}\) entry in this derivative will have all 0s
    except \((\superb{z}{\ell - 1})^\top\) in its \(i^\text{th}\) row. This is represented
    below.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 导数 \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\) 更为复杂。由于我们是相对于一个矩阵对向量求导，所以我们的结果将是一个张量。这个张量的形状将是
    \(D_\ell \times (D_\ell \times D_{\ell - 1})\)，因为 \(\superb{h}{\ell} \in R^{D_\ell}\)
    且 \(\superb{W}{\ell} \in \R^{D_\ell \times D_{\ell-1}}\)。这个张量的第一个元素由 \(\partial
    \super{h}{\ell}_1/ \partial \superb{W}{\ell}\) 给出。使用上面给出的 \(\superb{h}{\ell}\)
    的表达式，我们可以看到这是一个矩阵，第一行有 \((\superb{z}{\ell - 1})^\top\)，其他地方都是 0。更一般地，这个导数的第 \(i^\text{th}\)
    个元素除了在其第 \(i^\text{th}\) 行有 \((\superb{z}{\ell - 1})^\top\) 外，其他都是 0。这将在下面表示。
- en: \[\begin{split} \begin{align*} \dadb{\superb{h}{\ell}}{\superb{W}{\ell}} &=
    \begin{bmatrix} \dadb{\super{h}{\ell}_1}{\superb{W}{\ell}} \\ \\ \dots \\ \\ \dadb{\super{h}{\ell}_{n_\ell}}{\superb{W}{\ell}}
    \end{bmatrix} = \begin{bmatrix} \begin{bmatrix} \superb{z}{\ell - 1})^\top \\
    ... \\ \mathbf{0}^\top \end{bmatrix}\\ \dots \\ \begin{bmatrix} \mathbf{0}^\top
    \\ \dots \\ (\superb{z}{\ell - 1})^\top\end{bmatrix}\end{bmatrix} \in \R^{D_\ell
    \times (D_\ell \times D_{\ell - 1})}. \end{align*} \end{split}\]
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \dadb{\superb{h}{\ell}}{\superb{W}{\ell}} &=
    \begin{bmatrix} \dadb{\super{h}{\ell}_1}{\superb{W}{\ell}} \\ \\ \dots \\ \\ \dadb{\super{h}{\ell}_{n_\ell}}{\superb{W}{\ell}}
    \end{bmatrix} = \begin{bmatrix} \begin{bmatrix} \superb{z}{\ell - 1})^\top \\
    ... \\ \mathbf{0}^\top \end{bmatrix}\\ \dots \\ \begin{bmatrix} \mathbf{0}^\top
    \\ \dots \\ (\superb{z}{\ell - 1})^\top\end{bmatrix}\end{bmatrix} \in \R^{D_\ell
    \times (D_\ell \times D_{\ell - 1})}. \end{align*} \end{split}\]
- en: 2.2.4 One Last Gradient
  id: totrans-509
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 最后一个梯度
- en: We now have all the results necessary to calculate the derivative of the loss
    function with respect to the weights in the *final* layer. For instance, we can
    evaluate
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经拥有了计算损失函数相对于最终层权重导数的所有必要结果。例如，我们可以评估
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
- en: 'using the results from sections 2.1, 2.2, and 2.3\. However, to obtain the
    derivative of the loss function with respect to weights in the *previous* layers,
    we need one more derivative: the derivative of \(\superb{h}{\ell}\), the linear
    mapping in layer \(\ell\), with respect to \(\superb{z}{\ell - 1}\), the output
    of the previous layer. Fortunately, this derivative is simple:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第 2.1、2.2 和 2.3 节的结果。然而，为了获得相对于前层权重的损失函数导数，我们还需要一个额外的导数：\(\superb{h}{\ell}\)，即第
    \(\ell\) 层的线性映射，相对于 \(\superb{z}{\ell - 1}\)，即前一层的输出。幸运的是，这个导数很简单：
- en: \[ \dadb{\superb{h}{\ell}}{\superb{z}{\ell - 1}} = {\superb{W}{\ell}}. \]
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\superb{h}{\ell}}{\superb{z}{\ell - 1}} = {\superb{W}{\ell}}. \]
- en: Now that we have \(\partial \superb{h}{\ell}/\partial \superb{z}{\ell - 1}\),
    we reuse the results from sections 2.2 and 2.3 to calculate \(\partial \superb{z}{\ell
    - 1}/\partial \superb{h}{\ell - 1}\) and \(\partial \superb{h}{\ell - 1}/ \partial
    \superb{W}{\ell - 1}\) (respectively); this gives us all the necessary results
    to compute the gradient of the weights in the previous layer. We then rinse, lather,
    and repeat with layer \(\ell - 2\) through the first layer.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 \(\partial \superb{h}{\ell}/\partial \superb{z}{\ell - 1}\)，我们重新使用第 2.2
    和 2.3 节的结果来计算 \(\partial \superb{z}{\ell - 1}/\partial \superb{h}{\ell - 1}\)
    和 \(\partial \superb{h}{\ell - 1}/ \partial \superb{W}{\ell - 1}\)（分别）；这为我们计算前一层权重梯度提供了所有必要的结果。然后我们用
    \(\ell - 2\) 层到第一层重复这个过程。
- en: 2.2.1 Loss Functions and their Gradients
  id: totrans-515
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 损失函数及其梯度
- en: '![](../Images/6e20022b03b555510592958ec46ee03d.png)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e20022b03b555510592958ec46ee03d.png)'
- en: 'Back propagation begins where the network ends: the loss function \(\mathcal{L}(\hat{\by})\).
    Let’s start by introducing some common loss functions and their derivatives with
    respect to our predictions, \(\hat{\by}\). Later, using the chain rule, we will
    use these derivatives to calculate the derivatives with respect to our network’s
    weights.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播从网络结束的地方开始：损失函数\(\mathcal{L}(\hat{\by})\)。让我们首先介绍一些常见的损失函数及其相对于我们的预测\(\hat{\by}\)的导数。稍后，我们将使用链式法则，使用这些导数来计算相对于我们网络权重的导数。
- en: A common loss function for quantitative output variables is the residual sum
    of squares. For a single observation, the loss is
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定量输出变量的一个常见损失函数是残差平方和。对于单个观测值，损失是
- en: \[ \mathcal{L}_{RSS}(\hat{\by}) = (\by - \hat{\by})^2. \]
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}_{RSS}(\hat{\by}) = (\by - \hat{\by})^2. \]
- en: Note
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the loss is a function of both our predictions (\(\hat{\by}\)) and
    the true targets (\(\by\)). However, since the true targets are fixed, we can
    only manipulate \(\hat{\by}\), so we write the loss as only a function of \(\hat{\by}\).
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，损失是我们的预测(\(\hat{\by}\))和真实目标(\(\by\))的函数。然而，由于真实目标是固定的，我们只能操作\(\hat{\by}\)，因此我们将损失仅作为\(\hat{\by}\)的函数来表示。
- en: Note that we have a vector of losses because there are multiple output variables
    and we consider the loss for each variable independently. Now for the first step
    in back propagation, we calculate the derivative of this loss with respect to
    \(\hat{\by}\), which is simply given by
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们有一个损失向量，因为存在多个输出变量，我们独立地考虑每个变量的损失。现在，在反向传播的第一步中，我们计算这个损失相对于\(\hat{\by}\)的导数，它简单地给出
- en: \[ \dadb{\mathcal{L}_{RSS}(\hat{\by})}{\hat{\by}} = -2(\by - \hat{\by})^\top
    \in \R^{1 \times D_y}. \]
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}_{RSS}(\hat{\by})}{\hat{\by}} = -2(\by - \hat{\by})^\top
    \in \R^{1 \times D_y}. \]
- en: Since we are using the numerator layout convention, this derivative is a length-\(D_y\)
    row vector, or equivalently a \(1\) by \(D_y\) matrix.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是分子布局约定，这个导数是一个长度为\(D_y\)的行向量，或者等价地，一个\(1\)乘以\(D_y\)的矩阵。
- en: For binary classification problems, a common loss function is the log loss or
    cross entropy, given by
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类问题，一个常见的损失函数是对数损失或交叉熵，其表达式为
- en: \[ \mathcal{L}_{Log}(\hat{\by}) = -\Big(\by\log \hat{\by}+(1-\by)\log(1-\hat{\by})\Big),
    \]
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}_{Log}(\hat{\by}) = -\Big(\by\log \hat{\by}+(1-\by)\log(1-\hat{\by})\Big),
    \]
- en: where the \(i^\text{th}\) entry in \(\hat{\by}\) gives the estimated probability
    that the \(i^\text{th}\) output variable equals 1\. The derivative of this loss
    function with respect to \(\hat{\by}\) is given by
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 其中\(\hat{\by}\)中的第\(i^\text{th}\)个条目给出了第\(i^\text{th}\)个输出变量等于1的估计概率。这个损失函数相对于\(\hat{\by}\)的导数由以下给出
- en: \[ \begin{align*} \dadb{\mathcal{L}_{Log}(\hat{\by})}{\hat{\by}} &= \left(-\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top\in \R^{1 \times D_y}. \end{align*} \]
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\mathcal{L}_{Log}(\hat{\by})}{\hat{\by}} &= \left(-\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top\in \R^{1 \times D_y}. \end{align*} \]
- en: Once we calculate \(\partial \mathcal{L}(\hat{\by})/\partial\hat{\by}\), we
    can move further back into the network. Since \(\hat{\by}\) is the result of an
    activation function, the next step in back propagation is to calculate the derivative
    of our activation functions.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算出\(\partial \mathcal{L}(\hat{\by})/\partial\hat{\by}\)，我们就可以进一步向网络内部移动。由于\(\hat{\by}\)是激活函数的结果，反向传播的下一步是计算激活函数的导数。
- en: 2.2.2 Gradients of the Activation Functions
  id: totrans-530
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 激活函数的梯度
- en: '![](../Images/73cd03e425f8718e9a2e8b211cbb8b9c.png)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/73cd03e425f8718e9a2e8b211cbb8b9c.png)'
- en: Recall that \(\superb{z}{\ell}\), the output layer of \(\ell\), is the result
    of an activation function applied to a linear mapping \(\superb{h}{\ell}\). This
    includes the output of the final layer, \(\mathbf{\hat{y}}\), which we can also
    write as \(\superb{z}{L}\).
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，\(\superb{z}{\ell}\)，\(\ell\)层的输出层，是应用到一个线性映射\(\superb{h}{\ell}\)的激活函数的结果。这包括最终层的输出，\(\mathbf{\hat{y}}\)，我们也可以将其写作\(\superb{z}{L}\)。
- en: ReLU
  id: totrans-533
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ReLU
- en: Suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) where
    \(\super{f}{\ell}\) is the ReLU function. We are interested in \(\partial \superb{z}{\ell}/\partial
    \superb{h}{\ell}\). For \(i \neq j\), we have
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有\(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)，其中\(\super{f}{\ell}\)是ReLU函数。我们感兴趣的是\(\partial
    \superb{z}{\ell}/\partial \superb{h}{\ell}\)。对于\(i \neq j\)，我们有
- en: \[ \frac{\partial \super{z}{\ell}_i}{\partial \super{h}{\ell}_j} = 0, \]
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \super{z}{\ell}_i}{\partial \super{h}{\ell}_j} = 0, \]
- en: since \(\super{z}{\ell}_i\) is not a function of \(\super{h}{\ell}_j\). Then
    using the ReLU derivative, we have
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 由于\(\super{z}{\ell}_i\)不是\(\super{h}{\ell}_j\)的函数。然后使用ReLU导数，我们有
- en: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \begin{cases}
    1, & \super{h}{\ell}_i > 0 \\ 0, & \super{h}{\ell}_i \leq 0\. \end{cases} \end{split}\]
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \begin{cases}
    1, & \super{h}{\ell}_i > 0 \\ 0, & \super{h}{\ell}_i \leq 0\. \end{cases} \end{split}\]
- en: We can then compactly write the entire derivative as
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步紧凑地写出整个导数表达式
- en: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = \text{diag}(\superb{h}{\ell}
    > 0) \in \R^{D_\ell \times D_\ell}. \]
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = \text{diag}(\superb{h}{\ell}
    > 0) \in \R^{D_\ell \times D_\ell}. \]
- en: Sigmoid
  id: totrans-540
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Now suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)
    where \(\super{f}{\ell}\) is the sigmoid function. Again, the partial derivative
    \(\partial \super{z}{\ell}_i/\partial \super{h}{\ell}_j\) is 0 for \(i \neq j\).
    By the sigmoid derivative, we have
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们有一个 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)，其中 \(\super{f}{\ell}\)
    是 Sigmoid 函数。再次，偏导数 \(\partial \super{z}{\ell}_i/\partial \super{h}{\ell}_j\)
    对于 \(i \neq j\) 是 0。通过 Sigmoid 导数，我们有
- en: \[ \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \sigma(\super{h}{\ell}_i)(1-\sigma(\super{h}{\ell}_i)).
    \]
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \sigma(\super{h}{\ell}_i)(1-\sigma(\super{h}{\ell}_i)).
    \]
- en: We can again write the entire result compactly as
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次将整个结果紧凑地写为
- en: \[ \begin{align*} \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} &= \text{diag}\left(\sigma(\superb{h}{\ell})(1-\sigma(\superb{h}{\ell})\right)
    \in \R^{D_\ell \times D_\ell}. \end{align*} \]
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} &= \text{diag}\left(\sigma(\superb{h}{\ell})(1-\sigma(\superb{h}{\ell})\right)
    \in \R^{D_\ell \times D_\ell}. \end{align*} \]
- en: Linear
  id: totrans-545
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 线性
- en: Finally, suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)
    where \(\super{f}{\ell}\) is the linear function. We then have
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，假设我们有一个 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)，其中 \(\super{f}{\ell}\)
    是线性函数。那么我们有
- en: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_j} = \begin{cases}
    1 , & i = j\\ 0, & i \neq j. \end{cases} \end{split}\]
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_j} = \begin{cases}
    1 , & i = j\\ 0, & i \neq j. \end{cases} \end{split}\]
- en: The entire gradient is then simply
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 整个梯度因此可以简单地表示为
- en: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = I_{D_{\ell}} \in \R^{D_\ell \times
    D_\ell}. \]
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = I_{D_{\ell}} \in \R^{D_\ell \times
    D_\ell}. \]
- en: ReLU
  id: totrans-550
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ReLU
- en: Suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\) where
    \(\super{f}{\ell}\) is the ReLU function. We are interested in \(\partial \superb{z}{\ell}/\partial
    \superb{h}{\ell}\). For \(i \neq j\), we have
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)，其中 \(\super{f}{\ell}\)
    是 ReLU 函数。我们感兴趣的是 \(\partial \super{z}{\ell}/\partial \super{h}{\ell}\)。对于 \(i
    \neq j\)，我们有
- en: \[ \frac{\partial \super{z}{\ell}_i}{\partial \super{h}{\ell}_j} = 0, \]
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \super{z}{\ell}_i}{\partial \super{h}{\ell}_j} = 0, \]
- en: since \(\super{z}{\ell}_i\) is not a function of \(\super{h}{\ell}_j\). Then
    using the ReLU derivative, we have
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\super{z}{\ell}_i\) 不是 \(\super{h}{\ell}_j\) 的函数。然后使用 ReLU 导数，我们得到
- en: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \begin{cases}
    1, & \super{h}{\ell}_i > 0 \\ 0, & \super{h}{\ell}_i \leq 0\. \end{cases} \end{split}\]
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \begin{cases}
    1, & \super{h}{\ell}_i > 0 \\ 0, & \super{h}{\ell}_i \leq 0\. \end{cases} \end{split}\]
- en: We can then compactly write the entire derivative as
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步紧凑地写出整个导数表达式
- en: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = \text{diag}(\superb{h}{\ell}
    > 0) \in \R^{D_\ell \times D_\ell}. \]
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = \text{diag}(\superb{h}{\ell}
    > 0) \in \R^{D_\ell \times D_\ell}. \]
- en: Sigmoid
  id: totrans-557
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Now suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)
    where \(\super{f}{\ell}\) is the sigmoid function. Again, the partial derivative
    \(\partial \super{z}{\ell}_i/\partial \super{h}{\ell}_j\) is 0 for \(i \neq j\).
    By the sigmoid derivative, we have
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们有一个 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)，其中 \(\super{f}{\ell}\)
    是 Sigmoid 函数。再次，偏导数 \(\partial \super{z}{\ell}_i/\partial \super{h}{\ell}_j\)
    对于 \(i \neq j\) 是 0。通过 Sigmoid 导数，我们有
- en: \[ \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \sigma(\super{h}{\ell}_i)(1-\sigma(\super{h}{\ell}_i)).
    \]
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_i} = \sigma(\super{h}{\ell}_i)(1-\sigma(\super{h}{\ell}_i)).
    \]
- en: We can again write the entire result compactly as
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次将整个结果紧凑地写为
- en: \[ \begin{align*} \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} &= \text{diag}\left(\sigma(\superb{h}{\ell})(1-\sigma(\superb{h}{\ell})\right)
    \in \R^{D_\ell \times D_\ell}. \end{align*} \]
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} &= \text{diag}\left(\sigma(\superb{h}{\ell})(1-\sigma(\superb{h}{\ell})\right)
    \in \R^{D_\ell \times D_\ell}. \end{align*} \]
- en: Linear
  id: totrans-562
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 线性
- en: Finally, suppose we have \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)
    where \(\super{f}{\ell}\) is the linear function. We then have
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，假设我们有一个 \(\superb{z}{\ell} = \super{f}{\ell}(\superb{h}{\ell})\)，其中 \(\super{f}{\ell}\)
    是线性函数。那么我们有
- en: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_j} = \begin{cases}
    1 , & i = j\\ 0, & i \neq j. \end{cases} \end{split}\]
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\super{z}{\ell}_i}{\super{h}{\ell}_j} = \begin{cases}
    1 , & i = j\\ 0, & i \neq j. \end{cases} \end{split}\]
- en: The entire gradient is then simply
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 整个梯度随后就很简单了
- en: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = I_{D_{\ell}} \in \R^{D_\ell \times
    D_\ell}. \]
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\superb{z}{\ell}}{\superb{h}{\ell}} = I_{D_{\ell}} \in \R^{D_\ell \times
    D_\ell}. \]
- en: 2.2.3 Gradients of the Weights
  id: totrans-567
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 权重的梯度
- en: '![](../Images/cff599cc6963d9f0e1254e3990346bc0.png)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cff599cc6963d9f0e1254e3990346bc0.png)'
- en: We are now finally able to calculate the gradients of our weights. Specifically,
    we will calculate \(\partial \superb{h}{\ell}/ \partial \superb{c}{\ell}\) and
    \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\) which, when combined
    with our previous results through the chain rule, will allow us to obtain the
    derivative of the loss function with respect the layer \(\ell\) weights.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在终于能够计算我们权重的梯度了。具体来说，我们将计算 \(\partial \superb{h}{\ell}/ \partial \superb{c}{\ell}\)
    和 \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\)，当与我们的先前结果通过链式法则结合时，将使我们能够获得相对于层
    \(\ell\) 权重的损失函数的导数。
- en: Recall that we obtain \(\superb{h}{\ell}\) through
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们通过
- en: \[ \superb{h}{\ell} = \superb{W}{\ell}\superb{z}{\ell-1} + \superb{c}{\ell},
    \]
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \superb{h}{\ell} = \superb{W}{\ell}\superb{z}{\ell-1} + \superb{c}{\ell},
    \]
- en: giving us the simple derivative
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个简单的导数
- en: \[ \dadb{\super{\mathbf{h}}{\ell}}{\superb{c}{\ell}} = I_{D_\ell} \in \R^{D_\ell
    \times D_\ell}. \]
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\super{\mathbf{h}}{\ell}}{\superb{c}{\ell}} = I_{D_\ell} \in \R^{D_\ell
    \times D_\ell}. \]
- en: The derivative \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\) is more
    complicated. Since we are taking the derivative of a vector with respect to a
    matrix, our result will be a tensor. The shape of this tensor will be \(D_\ell
    \times (D_\ell \times D_{\ell - 1})\) since \(\superb{h}{\ell} \in R^{D_\ell}\)
    and \(\superb{W}{\ell} \in \R^{D_\ell \times D_{\ell-1}}\). The first element
    of this tensor is given by \(\partial \super{h}{\ell}_1/ \partial \superb{W}{\ell}\).
    Using the expression for \(\superb{h}{\ell}\) above, we see that this is a matrix
    with \((\superb{z}{\ell - 1})^\top\) in the first row and 0s everywhere else.
    More generally, the \(i^\text{th}\) entry in this derivative will have all 0s
    except \((\superb{z}{\ell - 1})^\top\) in its \(i^\text{th}\) row. This is represented
    below.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 导数 \(\partial \superb{h}{\ell}/ \partial \superb{W}{\ell}\) 更为复杂。由于我们是在矩阵相对于向量的导数，我们的结果将是一个张量。这个张量的形状将是
    \(D_\ell \times (D_\ell \times D_{\ell - 1})\)，因为 \(\superb{h}{\ell} \in R^{D_\ell}\)
    且 \(\superb{W}{\ell} \in \R^{D_\ell \times D_{\ell-1}}\)。这个张量的第一个元素由 \(\partial
    \super{h}{\ell}_1/ \partial \superb{W}{\ell}\) 给出。使用上面给出的 \(\superb{h}{\ell}\)
    的表达式，我们看到这是一个矩阵，第一行有 \((\superb{z}{\ell - 1})^\top\)，其他地方都是 0。更一般地，这个导数的第 \(i^\text{th}\)
    个元素除了在其第 \(i^\text{th}\) 行有 \((\superb{z}{\ell - 1})^\top\) 外，其他都是 0。这如下所示。
- en: \[\begin{split} \begin{align*} \dadb{\superb{h}{\ell}}{\superb{W}{\ell}} &=
    \begin{bmatrix} \dadb{\super{h}{\ell}_1}{\superb{W}{\ell}} \\ \\ \dots \\ \\ \dadb{\super{h}{\ell}_{n_\ell}}{\superb{W}{\ell}}
    \end{bmatrix} = \begin{bmatrix} \begin{bmatrix} \superb{z}{\ell - 1})^\top \\
    ... \\ \mathbf{0}^\top \end{bmatrix}\\ \dots \\ \begin{bmatrix} \mathbf{0}^\top
    \\ \dots \\ (\superb{z}{\ell - 1})^\top\end{bmatrix}\end{bmatrix} \in \R^{D_\ell
    \times (D_\ell \times D_{\ell - 1})}. \end{align*} \end{split}\]
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \dadb{\superb{h}{\ell}}{\superb{W}{\ell}} &=
    \begin{bmatrix} \dadb{\super{h}{\ell}_1}{\superb{W}{\ell}} \\ \\ \dots \\ \\ \dadb{\super{h}{\ell}_{n_\ell}}{\superb{W}{\ell}}
    \end{bmatrix} = \begin{bmatrix} \begin{bmatrix} \superb{z}{\ell - 1})^\top \\
    ... \\ \mathbf{0}^\top \end{bmatrix}\\ \dots \\ \begin{bmatrix} \mathbf{0}^\top
    \\ \dots \\ (\superb{z}{\ell - 1})^\top\end{bmatrix}\end{bmatrix} \in \R^{D_\ell
    \times (D_\ell \times D_{\ell - 1})}. \end{align*} \end{split}\]
- en: 2.2.4 One Last Gradient
  id: totrans-576
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 最后一个梯度
- en: We now have all the results necessary to calculate the derivative of the loss
    function with respect to the weights in the *final* layer. For instance, we can
    evaluate
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了计算相对于最终层权重损失函数导数的所有必要结果。例如，我们可以评估
- en: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\hat{\by})}{\super{\mathbf{W}}{L}} = \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{L}}\cdot
    \dadb{\super{\mathbf{h}}{L}}{\super{\mathbf{W}}{L}} \]
- en: 'using the results from sections 2.1, 2.2, and 2.3\. However, to obtain the
    derivative of the loss function with respect to weights in the *previous* layers,
    we need one more derivative: the derivative of \(\superb{h}{\ell}\), the linear
    mapping in layer \(\ell\), with respect to \(\superb{z}{\ell - 1}\), the output
    of the previous layer. Fortunately, this derivative is simple:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第2.1节、2.2节和2.3节的结果。然而，为了获得相对于 *前一层* 权重的损失函数导数，我们还需要一个额外的导数：层 \(\ell\) 中的线性映射
    \(\superb{h}{\ell}\) 相对于前一层输出 \(\superb{z}{\ell - 1}\) 的导数。幸运的是，这个导数很简单：
- en: \[ \dadb{\superb{h}{\ell}}{\superb{z}{\ell - 1}} = {\superb{W}{\ell}}. \]
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\superb{h}{\ell}}{\superb{z}{\ell - 1}} = {\superb{W}{\ell}}. \]
- en: Now that we have \(\partial \superb{h}{\ell}/\partial \superb{z}{\ell - 1}\),
    we reuse the results from sections 2.2 and 2.3 to calculate \(\partial \superb{z}{\ell
    - 1}/\partial \superb{h}{\ell - 1}\) and \(\partial \superb{h}{\ell - 1}/ \partial
    \superb{W}{\ell - 1}\) (respectively); this gives us all the necessary results
    to compute the gradient of the weights in the previous layer. We then rinse, lather,
    and repeat with layer \(\ell - 2\) through the first layer.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了 \(\partial \superb{h}{\ell}/\partial \superb{z}{\ell - 1}\)，我们可以重用第2.2节和第2.3节的结果来计算
    \(\partial \superb{z}{\ell - 1}/\partial \superb{h}{\ell - 1}\) 和 \(\partial \superb{h}{\ell
    - 1}/ \partial \superb{W}{\ell - 1}\)（分别）；这为我们计算前一层权重梯度提供了所有必要的结果。然后，我们对 \(\ell
    - 2\) 层到第一层重复这个过程。
- en: 2.3 Combining Results with the Chain Rule
  id: totrans-582
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 使用链式法则组合结果
- en: '![](../Images/ddd4201d5226ba07c2ffe7c422934b04.png)'
  id: totrans-583
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddd4201d5226ba07c2ffe7c422934b04.png)'
- en: We’ve seen lots of individual derivatives. Ultimately, we really care about
    the derivatives of the loss function with respect to the network’s weights. Let’s
    review by calculating the derivatives of the loss function with respect to the
    weights in the final layer for the familiar network above. Suppose \(\super{f}{2}\)
    is the Sigmoid function and we use the log loss. For \(\superb{W}{2}\) we get
    the following.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了很多个单独的导数。最终，我们真正关心的是损失函数相对于网络权重的导数。让我们通过计算上述熟悉网络中最终层权重的损失函数导数来回顾一下。假设
    \(\super{f}{2}\) 是Sigmoid函数，我们使用对数损失。对于 \(\superb{W}{2}\)，我们得到以下结果。
- en: \[\begin{split} \begin{align*} \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{2}}
    &= \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{2}}\cdot
    \dadb{\super{\mathbf{h}}{2}}{\superb{W}{2}} \\ &=-\left(\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top \cdot \text{diag}\left(\sigma(\superb{h}{2})(1-
    \sigma(\superb{h}{2}))\right)\cdot \mathbf{T} \\ &= -\begin{bmatrix} (\frac{y_1}{\hat{y}_1}
    + \frac{1-y_1}{1-\hat{y}_1})\cdot \sigma(\super{h}{2}_1)(1-\sigma(\super{h}{2}_1))\cdot
    \superb{z}{1} \\ \dots \\ (\frac{y_{n_2}}{\hat{y}_{n_2}} + \frac{1-y_{n_2}}{1-\hat{y}_{n_2}})\cdot
    \sigma(\super{h}{2}_{n_2})(1-\sigma(\super{h}{2}_{n_2}))\cdot \superb{z}{1} \end{bmatrix}
    \in \R^{n_2 \times n_1}, \end{align*} \end{split}\]
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \dadb{\mathcal{L}(\hat{\by})}{\superb{W}{2}}
    &= \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{2}}\cdot
    \dadb{\super{\mathbf{h}}{2}}{\superb{W}{2}} \\ &=-\left(\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top \cdot \text{diag}\left(\sigma(\superb{h}{2})(1-
    \sigma(\superb{h}{2}))\right)\cdot \mathbf{T} \\ &= -\begin{bmatrix} (\frac{y_1}{\hat{y}_1}
    + \frac{1-y_1}{1-\hat{y}_1})\cdot \sigma(\super{h}{2}_1)(1-\sigma(\super{h}{2}_1))\cdot
    \superb{z}{1} \\ \dots \\ (\frac{y_{n_2}}{\hat{y}_{n_2}} + \frac{1-y_{n_2}}{1-\hat{y}_{n_2}})\cdot
    \sigma(\super{h}{2}_{n_2})(1-\sigma(\super{h}{2}_{n_2}))\cdot \superb{z}{1} \end{bmatrix}
    \in \R^{n_2 \times n_1}, \end{align*} \end{split}\]
- en: where \(\mathbf{T}\) is the tensor derivative discussed in section 2.2.3.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{T}\) 是第2.2.3节中讨论的张量导数。
- en: For \(\superb{c}{2}\), we get
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(\superb{c}{2}\)，我们得到
- en: \[\begin{split} \begin{align*} \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{2}}
    &= \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{2}}\cdot
    \dadb{\super{\mathbf{h}}{2}}{\superb{c}{2}} \\ &=-\left(\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top \cdot \text{diag}\left(\sigma(\superb{h}{2})(1-
    \sigma(\superb{h}{2}))\right)\cdot I_{n_2} \\ &= -\begin{bmatrix} (\frac{y_1}{\hat{y}_1}
    + \frac{1-y_1}{1-\hat{y}_1})\cdot \sigma(\super{h}{2}_1)(1-\sigma(\super{h}{2}_1))
    \\ \dots \\ (\frac{y_{n_2}}{\hat{y}_{n_2}} + \frac{1-y_{n_2}}{1-\hat{y}_{n_2}})\cdot
    \sigma(\super{h}{2}_{n_2})(1-\sigma(\super{h}{2}_{n_2})) \end{bmatrix} \in \R^{n_2}.
    \end{align*} \end{split}\]
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \dadb{\mathcal{L}(\hat{\by})}{\superb{c}{2}}
    &= \dadb{\mathcal{L}(\hat{\by})}{\hat{\by}}\cdot\dadb{\hat{\by}}{\super{\mathbf{h}}{2}}\cdot
    \dadb{\super{\mathbf{h}}{2}}{\superb{c}{2}} \\ &=-\left(\frac{\by}{\hat{\by}}
    + \frac{1-\by}{1-\hat{\by}} \right)^\top \cdot \text{diag}\left(\sigma(\superb{h}{2})(1-
    \sigma(\superb{h}{2}))\right)\cdot I_{n_2} \\ &= -\begin{bmatrix} (\frac{y_1}{\hat{y}_1}
    + \frac{1-y_1}{1-\hat{y}_1})\cdot \sigma(\super{h}{2}_1)(1-\sigma(\super{h}{2}_1))
    \\ \dots \\ (\frac{y_{n_2}}{\hat{y}_{n_2}} + \frac{1-y_{n_2}}{1-\hat{y}_{n_2}})\cdot
    \sigma(\super{h}{2}_{n_2})(1-\sigma(\super{h}{2}_{n_2})) \end{bmatrix} \in \R^{n_2}.
    \end{align*} \end{split}\]
- en: 3\. Combining Observations
  id: totrans-589
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3. 结合观测值
- en: 'So far, we’ve only considered the derivative of the loss function for a *single*
    observation. When training a network, we will of course want to consider the entire
    dataset. One way to do so is to simply add the derivatives of the loss function
    with respect to the weights across observations. Since the loss over the dataset
    is the sum of the individual observation losses and the derivative of a sum is
    the sum of the derivatives, we can simply add the results above. For instance,
    to find the derivative of the loss with respect to the final matrix of weights
    \(\superb{W}{L}\), we could loop through observations and sum the individual derivatives:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了单个观测值的损失函数的导数。在训练网络时，我们当然会想要考虑整个数据集。这样做的一种方法是将损失函数相对于权重的导数在观测值之间相加。由于数据集上的损失是单个观测值损失的加权和，而和的导数是导数的和，我们可以简单地相加上述结果。例如，为了找到损失相对于最终权重矩阵
    \(\superb{W}{L}\) 的导数，我们可以遍历观测值并求和单个导数：
- en: \[ \dadb{\mathcal{L}(\{\hat{\by}_n\}_{n = 1}^N))}{\superb{W}{L}} = \dadb{\sumN
    \mathcal{L}(\hat{\by}_n)}{\superb{W} {L}} = \sumN \dadb{\mathcal{L}(\hat{\by}_n)}{\superb{W}{L}}.
    \]
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}(\{\hat{\by}_n\}_{n = 1}^N))}{\superb{W}{L}} = \dadb{\sumN
    \mathcal{L}(\hat{\by}_n)}{\superb{W} {L}} = \sumN \dadb{\mathcal{L}(\hat{\by}_n)}{\superb{W}{L}}.
    \]
- en: While straightforward, this approach is computationally inefficient. The rest
    of this section outlines a more complicated but *much* faster method.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法很简单，但计算效率不高。本节的其余部分概述了一种更复杂但 *速度更快* 的方法。
- en: 3.1 A New Representation
  id: totrans-593
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1 新的表示方法
- en: So far, we’ve treated our predictors and outputs as vectors. The network starts
    with \(\bx\) and outputs \(\superb{z}{1}\). Then it predicts with \(\superb{z}{1}\)
    and outputs \(\superb{z}{2}\). It repeats this process until \(\superb{z}{L-1}\)
    outputs \(\mathbf{\hat{y}}\). To incorporate multiple observations, we can turn
    these vectors into matrices. Again suppose our dataset consists of \(N\) observations
    with \(\bx_n \in \R^{D_x}\) and \(\by_n \in \R^{D_y}\). We start with \(\bX \in
    \R^{N \times D_x}\), whose \(n^\text{th}\) row is \(\bx_n\). Note that in \(\bX\),
    \(\bx_n\) is a row vector; to keep consistent with our previous sections, we want
    it to be a column vector. So, we’ll work with \(\bX^\top\) rather than \(\bX\).
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直将预测器和输出视为向量。网络从 \(\bx\) 开始，输出 \(\superb{z}{1}\)。然后它使用 \(\superb{z}{1}\)
    进行预测并输出 \(\superb{z}{2}\)。它重复这个过程，直到 \(\superb{z}{L-1}\) 输出 \(\mathbf{\hat{y}}\)。为了包含多个观测值，我们可以将这些向量转换为矩阵。再次假设我们的数据集由
    \(N\) 个观测值组成，其中 \(\bx_n \in \R^{D_x}\) 和 \(\by_n \in \R^{D_y}\)。我们开始于 \(\bX \in
    \R^{N \times D_x}\)，其第 \(n\) 行是 \(\bx_n\)。注意在 \(\bX\) 中，\(\bx_n\) 是一个行向量；为了与前面的章节保持一致，我们希望它是一个列向量。因此，我们将使用
    \(\bX^\top\) 而不是 \(\bX\)。
- en: '![](../Images/741660f5553d62898ffec20314efc58c.png)'
  id: totrans-595
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/741660f5553d62898ffec20314efc58c.png)'
- en: Rather than feeding each observation through the network at once, we will feed
    all observations together and give each observation its own column. Each column
    in \(\bX^\top\) represents an observation’s predictors. We then multiply this
    matrix by \(\superb{W}{1}\) and add \(\superb{c}{1}\) *element-wise* to get \(\superb{H}{1}\).
    Each column in \(\superb{H}{1}\) represents a vector of linear combinations of
    the corresponding column in \(\bX^\top\). We then pass \(\superb{H}{1}\) through
    an activation function to obtain \(\superb{Z}{1}\). Similarly, each column in
    \(\superb{Z}{1}\) represents the output vector for the corresponding observation
    in \(\bX^\top\). We then repeat, with \(\superb{Z}{1}\) acting as the matrix of
    predictors for the next layer. Ultimately, we will obtain a matrix \(\hat{\mathbf{Y}}^\top
    \in \R^{D_y \times N}\) whose \(n^\text{th}\) column represents the vector of
    fitted values for the \(n^\text{th}\) observation.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会一次将每个观测值通过网络，而是将所有观测值一起输入，并为每个观测值分配其自己的列。在 \(\bX^\top\) 中的每一列代表一个观测值的预测器。然后我们乘以矩阵
    \(\superb{W}{1}\) 并将 \(\superb{c}{1}\) *逐元素* 添加到得到 \(\superb{H}{1}\)。在 \(\superb{H}{1}\)
    中的每一列代表 \(\bX^\top\) 中对应列的线性组合向量。然后我们通过激活函数传递 \(\superb{H}{1}\) 以获得 \(\superb{Z}{1}\)。同样，在
    \(\superb{Z}{1}\) 中的每一列代表 \(\bX^\top\) 中对应观测值的输出向量。然后我们重复这个过程，其中 \(\superb{Z}{1}\)
    作为下一层的预测器矩阵。最终，我们将获得一个矩阵 \(\hat{\mathbf{Y}}^\top \in \R^{D_y \times N}\)，其第 \(n\)
    列代表第 \(n\) 个观测值的拟合值向量。
- en: 3.2 Gradients
  id: totrans-597
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2 梯度
- en: While this new representation is more efficient, it also makes the gradients
    more complicated since we are taking derivatives with respect to matrices rather
    than vectors. Ordinarily, the derivative of one matrix with respect to another
    would be a four-dimensional tensor. Luckily, there’s a shortcut.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种新的表示方式更高效，但它也使得梯度更加复杂，因为我们是在矩阵而不是向量上求导。通常，一个矩阵相对于另一个矩阵的导数将是一个四维张量。幸运的是，有一个快捷方式。
- en: For each parameter \(\theta\) in our network, we will find its gradient by asking
    “which parameters does \(\theta\) affect in the next layer”. Supposing the answer
    is some set \(\{\psi_1, \psi_2, \dots, \psi_n\},\) we will calculate the derivative
    of the loss function with respect to \(\theta\) as
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们网络中的每个参数 \(\theta\)，我们将通过询问“\(\theta\) 影响下一层的哪些参数”来找到其梯度。假设答案是某个集合 \(\{\psi_1,
    \psi_2, \dots, \psi_n\}\)，我们将计算损失函数相对于 \(\theta\) 的导数，如下所示
- en: \[ \dadb{\mathcal{L}}{\theta} = \sum_{i = 1}^n \dadb{L}{\psi_i}\cdot \dadb{\psi_i}{\theta}.
    \]
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\theta} = \sum_{i = 1}^n \dadb{L}{\psi_i}\cdot \dadb{\psi_i}{\theta}.
    \]
- en: '![](../Images/93567a4ed40bb43e9a9bdb2977ce6007.png)'
  id: totrans-601
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93567a4ed40bb43e9a9bdb2977ce6007.png)'
- en: Recall that our loss function is a vector \(\bf{\mathcal{L}}\) of size \(D_y\)
    since we have \(D_y\) output variables. This loss vector is a *row-wise* function
    of the prediction matrix, \(\hat{\mathbf{Y}}^\top\), meaning the \(d^\text{th}\)
    entry in \(\mathbf{\mathcal{L}}\) is a function of only the \(d^\text{th}\) row
    of \(\hat{\mathbf{Y}}^\top\) (which represents the fitted values for the \(d^\text{th}\)
    output variable across observations). For the \((i, d)^\text{th}\) entry in \(\hat{\mathbf{Y}}^\top\),
    then, we only need to consider the derivative of the \(d^\text{th}\) entry in
    \(\mathbf{\mathcal{L}}\)—the derivative of any other entry in \(\mathcal{L}\)
    with respect \(\hat{\mathbf{Y}}^\top_{i, d}\) is 0\. We can then use the following
    gradient in place of a four-dimensional tensor.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的损失函数 \(\bf{\mathcal{L}}\) 是一个大小为 \(D_y\) 的向量，因为我们有 \(D_y\) 个输出变量。这个损失向量是预测矩阵
    \(\hat{\mathbf{Y}}^\top\) 的 *逐行* 函数，这意味着 \(\mathbf{\mathcal{L}}\) 中的第 \(d^\text{th}\)
    个条目仅是 \(\hat{\mathbf{Y}}^\top\) 的第 \(d^\text{th}\) 行的函数（它代表了第 \(d^\text{th}\)
    个输出变量在观测中的拟合值）。对于 \(\hat{\mathbf{Y}}^\top\) 中的 \((i, d)^\text{th}\) 个条目，我们只需要考虑
    \(\mathbf{\mathcal{L}}\) 中第 \(d^\text{th}\) 个条目的导数——相对于 \(\hat{\mathbf{Y}}^\top_{i,
    d}\) 的 \(\mathcal{L}\) 中任何其他条目的导数都是 0。然后我们可以使用以下梯度来代替四维张量。
- en: \[\begin{split} \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top} = \begin{bmatrix}
    \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,1}} & ... & \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,N}}
    \\ & ... & \\ \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,1}} & ... &
    \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,N}}\end{bmatrix} \end{split}\]
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top} = \begin{bmatrix}
    \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,1}} & ... & \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,N}}
    \\ & ... & \\ \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,1}} & ... &
    \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,N}}\end{bmatrix} \end{split}\]
- en: Next, we consider the derivative of \(\mathbf{\hat{Y}}^\top\) with respect to
    \(\superb{H}{L}\). Note that \(\mathbf{\hat{Y}}^\top\) is an *element-wise* function
    of \(\superb{H}{L}\). This means we only need to consider the gradient of each
    element in the former with respect to its corresponding element in the latter.
    This gives us
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们考虑 \(\mathbf{\hat{Y}}^\top\) 相对于 \(\superb{H}{L}\) 的导数。注意，\(\mathbf{\hat{Y}}^\top\)
    是 \(\superb{H}{L}\) 的 *逐元素* 函数。这意味着我们只需要考虑前一个中每个元素相对于后一个中相应元素的梯度。这给我们
- en: \[\begin{split} \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}} = \begin{bmatrix}
    \dadb{\mathbf{\hat{Y}}^\top_{1,1}}{\superb{H}{L}_{1,1}} & ... & \dadb{\mathbf{\hat{Y}}^\top_{1,N}}{\superb{H}{L}_{1,N}}
    \\ & ... & \\ \dadb{\mathbf{\hat{Y}}^\top_{D_y,1}}{\superb{H}{1}_{D_y,1}} & ...
    & \dadb{\mathbf{\hat{Y}}^\top_{D_y,N}}{\superb{L}{L}_{D_y,N}} \end{bmatrix}. \end{split}\]
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}} = \begin{bmatrix}
    \dadb{\mathbf{\hat{Y}}^\top_{1,1}}{\superb{H}{L}_{1,1}} & ... & \dadb{\mathbf{\hat{Y}}^\top_{1,N}}{\superb{H}{L}_{1,N}}
    \\ & ... & \\ \dadb{\mathbf{\hat{Y}}^\top_{D_y,1}}{\superb{H}{1}_{D_y,1}} & ...
    & \dadb{\mathbf{\hat{Y}}^\top_{D_y,N}}{\superb{L}{L}_{D_y,N}} \end{bmatrix}. \end{split}\]
- en: Now let’s use the shortcut described above. Since each element in \(\superb{H}{L}\)
    only affects the corresponding element in \(\mathbf{\hat{Y}}^\top\), we calculate
    \(\partial \mathcal{L}/\partial \superb{H}{L}\) by multiplying the two gradients
    above *element-wise*. I.e.,
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用上面描述的快捷方式。由于 \(\superb{H}{L}\) 中的每个元素只影响 \(\mathbf{\hat{Y}}^\top\) 中相应的元素，我们通过
    *逐元素* 相乘上述两个梯度来计算 \(\partial \mathcal{L}/\partial \superb{H}{L}\)。即，
- en: \[ \dadb{\mathcal{L}}{\superb{H}{L}} = \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top}
    \circ \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}}, \]
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\superb{H}{L}} = \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top}
    \circ \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}}, \]
- en: where \(\circ\) is the element-wise multiplication operator, also known as the
    *Hadamard product*.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\circ\) 是逐元素乘法运算符，也称为 *Hadamard 积*。
- en: Next up is \(\superb{c}{L}\). Whereas each element in \(\superb{H}{L}\) affected
    only one element in \(\mathbf{\hat{Y}}^\top\), each element in \(\superb{c}{L}\)
    affects \(N\) elements in \(\superb{H}{L}\)—every element in its corresponding
    row. Consider the first entry in \(\superb{c}{L}\). Since this entry affects each
    entry in the first row of \(\superb{H}{L}\), the chain rule gives us
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 \(\superb{c}{L}\)。由于 \(\superb{H}{L}\) 中的每个元素只影响 \(\mathbf{\hat{Y}}^\top\)
    中的一个元素，而 \(\superb{c}{L}\) 中的每个元素影响 \(\superb{H}{L}\) 中的 \(N\) 个元素——其对应行的每个元素。考虑
    \(\superb{c}{L}\) 的第一个元素。由于这个元素影响 \(\superb{H}{L}\) 第一行的每个元素，链式法则给出了
- en: \[ \dadb{\mathcal{L}}{\super{c}{L}_1} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{1,n}}\cdot\dadb{\superb{H}{L}_{1,n}}{\super{c}{L}_1}.
    \]
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\super{c}{L}_1} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{1,n}}\cdot\dadb{\superb{H}{L}_{1,n}}{\super{c}{L}_1}.
    \]
- en: Fortunately \(\partial \superb{H}{L}_{1,n}/\partial \super{c}{L}_1\) is just
    1 since \(\super{c}{L}_1\) is an intercept term. This implies that the derivative
    of the loss function with respect to \(\superb{c}{1}\) is just the row sum of
    \(\partial\mathcal{L}/\partial \superb{H}{L}\), or
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是 \(\partial \superb{H}{L}_{1,n}/\partial \super{c}{L}_1\) 只是 1，因为 \(\super{c}{L}_1\)
    是一个截距项。这意味着损失函数相对于 \(\superb{c}{1}\) 的导数只是 \(\partial\mathcal{L}/\partial \superb{H}{L}\)
    的行和，即
- en: \[ \dadb{\mathcal{L}}{\super{c}{L}_i} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,n}}.
    \]
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\super{c}{L}_i} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,n}}.
    \]
- en: Next, we have \(\superb{W}{L}\). Using our shortcut, we ask “which values does
    the \((i, j)^\text{th}\) entry in \(\superb{W}{L}\) affect?” Since \(\superb{H}{L}
    = \superb{W}{L}\superb{Z}{L-1}\), we have that
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有 \(\superb{W}{L}\)。使用我们的快捷方式，我们询问“\(\superb{W}{L}\) 的 \((i, j)^\text{th}\)
    个元素影响哪些值？”由于 \(\superb{H}{L} = \superb{W}{L}\superb{Z}{L-1}\)，我们有
- en: \[ \superb{H}{L}_{i,n} = \superb{W}{L}_{i, j} \superb{Z}{L-1}_{j,n} \hspace{1mm}
    \forall \hspace{1mm} n \in \{1, \dots, N\}. \]
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \superb{H}{L}_{i,n} = \superb{W}{L}_{i, j} \superb{Z}{L-1}_{j,n} \hspace{1mm}
    \forall \hspace{1mm} n \in \{1, \dots, N\}. \]
- en: This tells us that \(\superb{W}{L}_{i,j}\) affects each entry in the \(i^\text{th}\)
    row of \(\superb{H}{L}\) and gives us the derivative \(\partial{\superb{H}{L}_{i,
    n}}/\partial \superb{W}{L}_{i, j} = \superb{Z}{L-1}_{j, n}.\) Therefore,
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们 \(\superb{W}{L}_{i,j}\) 影响到 \(\superb{H}{L}\) 的第 \(i\) 行的每一个元素，并给出了导数
    \(\partial{\superb{H}{L}_{i, n}}/\partial \superb{W}{L}_{i, j} = \superb{Z}{L-1}_{j,
    n}.\) 因此，
- en: \[ \dadb{\mathcal{L}}{\superb{W}{L}_{i, j}} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,
    n}}\cdot\dadb{\superb{H}{L}_{i, n}}{\superb{W}{L}_{i, j}} = \sumN (\nabla \superb{H}{L})_{i,
    n}\cdot{\superb{Z}{L-1}_{j,n}}, \]
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\superb{W}{L}_{i, j}} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,
    n}}\cdot\dadb{\superb{H}{L}_{i, n}}{\superb{W}{L}_{i, j}} = \sumN (\nabla \superb{H}{L})_{i,
    n}\cdot{\superb{Z}{L-1}_{j,n}}, \]
- en: where \(\nabla \superb{H}{L}\) is the matrix representing \(\partial \mathcal{L}/\partial\superb{H}{L}\).
    This can be computed for each element in \(\superb{W}{L}\) using a tensor dot
    product, which will be covered in the construction section.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\nabla \superb{H}{L}\) 是表示 \(\partial \mathcal{L}/\partial\superb{H}{L}\)
    的矩阵。这可以通过张量点积计算 \(\superb{W}{L}\) 中的每个元素，这将在构造部分进行介绍。
- en: Finally, we have \(\superb{Z}{L-1}\). This case is symmetric to \(\superb{W}{L}\),
    and the same approach gives us the result
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有 \(\superb{Z}{L-1}\)。这种情况与 \(\superb{W}{L}\) 是对称的，同样的方法给出了结果
- en: \[ \dadb{\mathcal{L}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R \dadb{\mathcal{L}}{\superb{H}{L}_{r,n}}\cdot\dadb{\superb{H}{L}_{r,
    n}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R {(\nabla \superb{H}{L})}_{r, n}\cdot{\superb{W}{L}_{r,i}}.
    \]
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R \dadb{\mathcal{L}}{\superb{H}{L}_{r,n}}\cdot\dadb{\superb{H}{L}_{r,
    n}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R {(\nabla \superb{H}{L})}_{r, n}\cdot{\superb{W}{L}_{r,i}}.
    \]
- en: Again, the derivative for all of \(\superb{Z}{L-1}\) can be calculated at once
    using a tensor dot product.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，所有 \(\superb{Z}{L-1}\) 的导数可以使用张量点积一次计算出来。
- en: 3.1 A New Representation
  id: totrans-621
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1 新的表示
- en: So far, we’ve treated our predictors and outputs as vectors. The network starts
    with \(\bx\) and outputs \(\superb{z}{1}\). Then it predicts with \(\superb{z}{1}\)
    and outputs \(\superb{z}{2}\). It repeats this process until \(\superb{z}{L-1}\)
    outputs \(\mathbf{\hat{y}}\). To incorporate multiple observations, we can turn
    these vectors into matrices. Again suppose our dataset consists of \(N\) observations
    with \(\bx_n \in \R^{D_x}\) and \(\by_n \in \R^{D_y}\). We start with \(\bX \in
    \R^{N \times D_x}\), whose \(n^\text{th}\) row is \(\bx_n\). Note that in \(\bX\),
    \(\bx_n\) is a row vector; to keep consistent with our previous sections, we want
    it to be a column vector. So, we’ll work with \(\bX^\top\) rather than \(\bX\).
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们将预测值和输出值视为向量。网络从 \(\bx\) 开始，输出 \(\superb{z}{1}\)。然后它使用 \(\superb{z}{1}\)
    进行预测，并输出 \(\superb{z}{2}\)。它重复这个过程，直到 \(\superb{z}{L-1}\) 输出 \(\mathbf{\hat{y}}\)。为了包含多个观察值，我们可以将这些向量转换为矩阵。再次假设我们的数据集由
    \(N\) 个观察值组成，其中 \(\bx_n \in \R^{D_x}\) 和 \(\by_n \in \R^{D_y}\)。我们从 \(\bX \in
    \R^{N \times D_x}\) 开始，其第 \(n^\text{th}\) 行是 \(\bx_n\)。注意在 \(\bX\) 中，\(\bx_n\)
    是一个行向量；为了与前面的章节保持一致，我们希望它是一个列向量。因此，我们将使用 \(\bX^\top\) 而不是 \(\bX\)。
- en: '![](../Images/741660f5553d62898ffec20314efc58c.png)'
  id: totrans-623
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/741660f5553d62898ffec20314efc58c.png)'
- en: Rather than feeding each observation through the network at once, we will feed
    all observations together and give each observation its own column. Each column
    in \(\bX^\top\) represents an observation’s predictors. We then multiply this
    matrix by \(\superb{W}{1}\) and add \(\superb{c}{1}\) *element-wise* to get \(\superb{H}{1}\).
    Each column in \(\superb{H}{1}\) represents a vector of linear combinations of
    the corresponding column in \(\bX^\top\). We then pass \(\superb{H}{1}\) through
    an activation function to obtain \(\superb{Z}{1}\). Similarly, each column in
    \(\superb{Z}{1}\) represents the output vector for the corresponding observation
    in \(\bX^\top\). We then repeat, with \(\superb{Z}{1}\) acting as the matrix of
    predictors for the next layer. Ultimately, we will obtain a matrix \(\hat{\mathbf{Y}}^\top
    \in \R^{D_y \times N}\) whose \(n^\text{th}\) column represents the vector of
    fitted values for the \(n^\text{th}\) observation.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会一次性将每个观察值通过网络，而是将所有观察值一起输入，并为每个观察值分配其自己的列。在 \(\bX^\top\) 中的每一列代表一个观察值的预测因子。然后我们将这个矩阵乘以
    \(\superb{W}{1}\)，并 *逐元素* 加上 \(\superb{c}{1}\) 以得到 \(\superb{H}{1}\)。在 \(\superb{H}{1}\)
    中的每一列代表 \(\bX^\top\) 中对应列的线性组合向量。然后我们通过激活函数将 \(\superb{H}{1}\) 传递，以获得 \(\superb{Z}{1}\)。同样，在
    \(\superb{Z}{1}\) 中的每一列代表 \(\bX^\top\) 中对应观察值的输出向量。然后我们重复这个过程，其中 \(\superb{Z}{1}\)
    作为下一层的预测因子矩阵。最终，我们将获得一个矩阵 \(\hat{\mathbf{Y}}^\top \in \R^{D_y \times N}\)，其第 \(n^\text{th}\)
    列代表第 \(n^\text{th}\) 个观察值的拟合值向量。
- en: 3.2 Gradients
  id: totrans-625
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2 梯度
- en: While this new representation is more efficient, it also makes the gradients
    more complicated since we are taking derivatives with respect to matrices rather
    than vectors. Ordinarily, the derivative of one matrix with respect to another
    would be a four-dimensional tensor. Luckily, there’s a shortcut.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种新的表示方式更高效，但它也使得梯度更加复杂，因为我们是在矩阵而不是向量上求导。通常，一个矩阵相对于另一个矩阵的导数将是一个四维张量。幸运的是，有一个捷径。
- en: For each parameter \(\theta\) in our network, we will find its gradient by asking
    “which parameters does \(\theta\) affect in the next layer”. Supposing the answer
    is some set \(\{\psi_1, \psi_2, \dots, \psi_n\},\) we will calculate the derivative
    of the loss function with respect to \(\theta\) as
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络中的每个参数 \(\theta\)，我们将通过询问“\(\theta\) 影响下一层的哪些参数”来找到其梯度。假设答案是某个集合 \(\{\psi_1,
    \psi_2, \dots, \psi_n\}\)，我们将计算损失函数相对于 \(\theta\) 的导数，如下所示：
- en: \[ \dadb{\mathcal{L}}{\theta} = \sum_{i = 1}^n \dadb{L}{\psi_i}\cdot \dadb{\psi_i}{\theta}.
    \]
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\theta} = \sum_{i = 1}^n \dadb{L}{\psi_i}\cdot \dadb{\psi_i}{\theta}.
    \]
- en: '![](../Images/93567a4ed40bb43e9a9bdb2977ce6007.png)'
  id: totrans-629
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93567a4ed40bb43e9a9bdb2977ce6007.png)'
- en: Recall that our loss function is a vector \(\bf{\mathcal{L}}\) of size \(D_y\)
    since we have \(D_y\) output variables. This loss vector is a *row-wise* function
    of the prediction matrix, \(\hat{\mathbf{Y}}^\top\), meaning the \(d^\text{th}\)
    entry in \(\mathbf{\mathcal{L}}\) is a function of only the \(d^\text{th}\) row
    of \(\hat{\mathbf{Y}}^\top\) (which represents the fitted values for the \(d^\text{th}\)
    output variable across observations). For the \((i, d)^\text{th}\) entry in \(\hat{\mathbf{Y}}^\top\),
    then, we only need to consider the derivative of the \(d^\text{th}\) entry in
    \(\mathbf{\mathcal{L}}\)—the derivative of any other entry in \(\mathcal{L}\)
    with respect \(\hat{\mathbf{Y}}^\top_{i, d}\) is 0\. We can then use the following
    gradient in place of a four-dimensional tensor.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的损失函数 \(\bf{\mathcal{L}}\) 是一个大小为 \(D_y\) 的向量，因为我们有 \(D_y\) 个输出变量。这个损失向量是预测矩阵
    \(\hat{\mathbf{Y}}^\top\) 的 *行向量* 函数，这意味着 \(\mathbf{\mathcal{L}}\) 中的第 \(d^\text{th}\)
    个元素仅是 \(\hat{\mathbf{Y}}^\top\) 的第 \(d^\text{th}\) 行的函数（它代表了第 \(d^\text{th}\)
    个输出变量在观察中的拟合值）。对于 \(\hat{\mathbf{Y}}^\top\) 中的 \((i, d)^\text{th}\) 个元素，我们只需要考虑
    \(\mathbf{\mathcal{L}}\) 中第 \(d^\text{th}\) 个元素的导数——\(\mathcal{L}\) 中其他任何元素的导数相对于
    \(\hat{\mathbf{Y}}^\top_{i, d}\) 都是 0。然后我们可以使用以下梯度来代替四维张量。
- en: \[\begin{split} \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top} = \begin{bmatrix}
    \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,1}} & ... & \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,N}}
    \\ & ... & \\ \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,1}} & ... &
    \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,N}}\end{bmatrix} \end{split}\]
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top} = \begin{bmatrix}
    \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,1}} & ... & \dadb{\mathcal{L}_{1}}{\mathbf{\hat{Y}}^\top_{1,N}}
    \\ & ... & \\ \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,1}} & ... &
    \dadb{\mathcal{L}_{D_y}}{\mathbf{\hat{Y}}^\top_{D_y,N}}\end{bmatrix} \end{split}\]
- en: Next, we consider the derivative of \(\mathbf{\hat{Y}}^\top\) with respect to
    \(\superb{H}{L}\). Note that \(\mathbf{\hat{Y}}^\top\) is an *element-wise* function
    of \(\superb{H}{L}\). This means we only need to consider the gradient of each
    element in the former with respect to its corresponding element in the latter.
    This gives us
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们考虑 \(\mathbf{\hat{Y}}^\top\) 对 \(\superb{H}{L}\) 的导数。注意，\(\mathbf{\hat{Y}}^\top\)
    是 \(\superb{H}{L}\) 的 *逐元素* 函数。这意味着我们只需要考虑前一个中每个元素相对于后一个中相应元素的梯度。这给我们
- en: \[\begin{split} \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}} = \begin{bmatrix}
    \dadb{\mathbf{\hat{Y}}^\top_{1,1}}{\superb{H}{L}_{1,1}} & ... & \dadb{\mathbf{\hat{Y}}^\top_{1,N}}{\superb{H}{L}_{1,N}}
    \\ & ... & \\ \dadb{\mathbf{\hat{Y}}^\top_{D_y,1}}{\superb{H}{1}_{D_y,1}} & ...
    & \dadb{\mathbf{\hat{Y}}^\top_{D_y,N}}{\superb{L}{L}_{D_y,N}} \end{bmatrix}. \end{split}\]
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}} = \begin{bmatrix}
    \dadb{\mathbf{\hat{Y}}^\top_{1,1}}{\superb{H}{L}_{1,1}} & ... & \dadb{\mathbf{\hat{Y}}^\top_{1,N}}{\superb{H}{L}_{1,N}}
    \\ & ... & \\ \dadb{\mathbf{\hat{Y}}^\top_{D_y,1}}{\superb{H}{1}_{D_y,1}} & ...
    & \dadb{\mathbf{\hat{Y}}^\top_{D_y,N}}{\superb{L}{L}_{D_y,N}} \end{bmatrix}. \end{split}\]
- en: Now let’s use the shortcut described above. Since each element in \(\superb{H}{L}\)
    only affects the corresponding element in \(\mathbf{\hat{Y}}^\top\), we calculate
    \(\partial \mathcal{L}/\partial \superb{H}{L}\) by multiplying the two gradients
    above *element-wise*. I.e.,
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用上面描述的快捷方式。由于 \(\superb{H}{L}\) 中的每个元素只影响 \(\mathbf{\hat{Y}}^\top\) 中相应的元素，我们通过逐元素相乘上述两个梯度来计算
    \(\partial \mathcal{L}/\partial \superb{H}{L}\)。即，
- en: \[ \dadb{\mathcal{L}}{\superb{H}{L}} = \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top}
    \circ \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}}, \]
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\superb{H}{L}} = \dadb{\mathbf{\mathcal{L}}}{\mathbf{\hat{Y}}^\top}
    \circ \dadb{\mathbf{\hat{Y}}^\top}{\superb{H}{L}}, \]
- en: where \(\circ\) is the element-wise multiplication operator, also known as the
    *Hadamard product*.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\circ\) 是逐元素乘法运算符，也称为 *Hadamard 积*。
- en: Next up is \(\superb{c}{L}\). Whereas each element in \(\superb{H}{L}\) affected
    only one element in \(\mathbf{\hat{Y}}^\top\), each element in \(\superb{c}{L}\)
    affects \(N\) elements in \(\superb{H}{L}\)—every element in its corresponding
    row. Consider the first entry in \(\superb{c}{L}\). Since this entry affects each
    entry in the first row of \(\superb{H}{L}\), the chain rule gives us
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 \(\superb{c}{L}\)。由于 \(\superb{H}{L}\) 中的每个元素只影响 \(\mathbf{\hat{Y}}^\top\)
    中的一个元素，而 \(\superb{c}{L}\) 中的每个元素影响 \(\superb{H}{L}\) 中的 \(N\) 个元素——其对应行的每个元素。考虑
    \(\superb{c}{L}\) 中的第一个元素。由于这个元素影响 \(\superb{H}{L}\) 第一行的每个元素，链式法则给出
- en: \[ \dadb{\mathcal{L}}{\super{c}{L}_1} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{1,n}}\cdot\dadb{\superb{H}{L}_{1,n}}{\super{c}{L}_1}.
    \]
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\super{c}{L}_1} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{1,n}}\cdot\dadb{\superb{H}{L}_{1,n}}{\super{c}{L}_1}.
    \]
- en: Fortunately \(\partial \superb{H}{L}_{1,n}/\partial \super{c}{L}_1\) is just
    1 since \(\super{c}{L}_1\) is an intercept term. This implies that the derivative
    of the loss function with respect to \(\superb{c}{1}\) is just the row sum of
    \(\partial\mathcal{L}/\partial \superb{H}{L}\), or
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，\(\partial \superb{H}{L}_{1,n}/\partial \super{c}{L}_1\) 等于 1，因为 \(\super{c}{L}_1\)
    是一个截距项。这意味着损失函数相对于 \(\superb{c}{1}\) 的导数仅仅是 \(\partial\mathcal{L}/\partial \superb{H}{L}\)
    的行和，或者说
- en: \[ \dadb{\mathcal{L}}{\super{c}{L}_i} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,n}}.
    \]
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\super{c}{L}_i} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,n}}.
    \]
- en: Next, we have \(\superb{W}{L}\). Using our shortcut, we ask “which values does
    the \((i, j)^\text{th}\) entry in \(\superb{W}{L}\) affect?” Since \(\superb{H}{L}
    = \superb{W}{L}\superb{Z}{L-1}\), we have that
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有 \(\superb{W}{L}\)。使用我们的快捷方式，我们询问“\(\superb{W}{L}\) 的 \((i, j)^\text{th}\)
    个元素影响哪些值？”由于 \(\superb{H}{L} = \superb{W}{L}\superb{Z}{L-1}\)，我们有
- en: \[ \superb{H}{L}_{i,n} = \superb{W}{L}_{i, j} \superb{Z}{L-1}_{j,n} \hspace{1mm}
    \forall \hspace{1mm} n \in \{1, \dots, N\}. \]
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \superb{H}{L}_{i,n} = \superb{W}{L}_{i, j} \superb{Z}{L-1}_{j,n} \hspace{1mm}
    \forall \hspace{1mm} n \in \{1, \dots, N\}. \]
- en: This tells us that \(\superb{W}{L}_{i,j}\) affects each entry in the \(i^\text{th}\)
    row of \(\superb{H}{L}\) and gives us the derivative \(\partial{\superb{H}{L}_{i,
    n}}/\partial \superb{W}{L}_{i, j} = \superb{Z}{L-1}_{j, n}.\) Therefore,
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们 \(\superb{W}{L}_{i,j}\) 影响到 \(\superb{H}{L}\) 的第 \(i\) 行的每一个元素，并给出导数 \(\partial{\superb{H}{L}_{i,
    n}}/\partial \superb{W}{L}_{i, j} = \superb{Z}{L-1}_{j, n}.\) 因此，
- en: \[ \dadb{\mathcal{L}}{\superb{W}{L}_{i, j}} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,
    n}}\cdot\dadb{\superb{H}{L}_{i, n}}{\superb{W}{L}_{i, j}} = \sumN (\nabla \superb{H}{L})_{i,
    n}\cdot{\superb{Z}{L-1}_{j,n}}, \]
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\superb{W}{L}_{i, j}} = \sumN \dadb{\mathcal{L}}{\superb{H}{L}_{i,
    n}}\cdot\dadb{\superb{H}{L}_{i, n}}{\superb{W}{L}_{i, j}} = \sumN (\nabla \superb{H}{L})_{i,
    n}\cdot{\superb{Z}{L-1}_{j,n}}, \]
- en: where \(\nabla \superb{H}{L}\) is the matrix representing \(\partial \mathcal{L}/\partial\superb{H}{L}\).
    This can be computed for each element in \(\superb{W}{L}\) using a tensor dot
    product, which will be covered in the construction section.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\nabla \superb{H}{L}\) 是表示 \(\partial \mathcal{L}/\partial\superb{H}{L}\)
    的矩阵。这可以通过张量点积为 \(\superb{W}{L}\) 的每个元素计算出来，这将在构造部分进行介绍。
- en: Finally, we have \(\superb{Z}{L-1}\). This case is symmetric to \(\superb{W}{L}\),
    and the same approach gives us the result
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到 \(\superb{Z}{L-1}\)。这种情况与 \(\superb{W}{L}\) 对称，采用相同的方法可以得到结果
- en: \[ \dadb{\mathcal{L}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R \dadb{\mathcal{L}}{\superb{H}{L}_{r,n}}\cdot\dadb{\superb{H}{L}_{r,
    n}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R {(\nabla \superb{H}{L})}_{r, n}\cdot{\superb{W}{L}_{r,i}}.
    \]
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\mathcal{L}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R \dadb{\mathcal{L}}{\superb{H}{L}_{r,n}}\cdot\dadb{\superb{H}{L}_{r,
    n}}{\superb{Z}{L-1}_{i, n}} = \sum_{r = 1}^R {(\nabla \superb{H}{L})}_{r, n}\cdot{\superb{W}{L}_{r,i}}.
    \]
- en: Again, the derivative for all of \(\superb{Z}{L-1}\) can be calculated at once
    using a tensor dot product.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，所有 \(\superb{Z}{L-1}\) 的导数可以通过张量点积一次计算出来。
