# 第二章 硬件架构

> 原文：[`ppml.dev/hardware.html`](https://ppml.dev/hardware.html)

构建一个运行机器学习软件的计算系统需要周密的规划。其性能如何取决于选择正确的硬件；一套能够高效有效地执行我们想要执行的任务的机器学习算法；以及如何表示我们将使用的数据、模型及其输出。为了本书的目的，我们将“计算系统”定义为一种计算机系统，不一定是服务器级别的，它将执行学习或使用机器学习模型所需的一个或多个任务。我们通常会称之为“机器学习系统”，以突出其目的。其他类型的系统，如专注于存储（数据库服务器、数据湖、对象存储）或交付（可读性仪表板、计算机可读 API 端点）的系统，将仅作简要提及。

在本章中，我们专注于硬件，第三章将转向数据，第四章将转向算法。在介绍计算系统的关键方面（第 2.1 节）之后，我们讨论如何最大限度地发挥其潜力（第 2.2 节），集成远程系统所涉及的权衡（第 2.3 节）以及如何根据我们的需求进行设计（第 2.4 节）。现代机器学习库试图为我们做出这些决定，但它们在许多实际应用中都有局限性：在这种情况下，能够对硬件进行推理是一项无价的技能。

![现代计算系统中可能出现的关键组件的示意图（不一定同时出现）。尺寸和距离仅供参考。](img/547fdc3726b6a903570f2227ea79c1de.png)

图 2.1：现代计算系统中可能出现的关键组件的示意图（不一定同时出现）。尺寸和距离仅供参考。

## 2.1 硬件类型

计算系统有多种配置，其组件在图 2.1 中总结。总的来说，我们可以认为它们沿着三个轴进行变化：

1.  *计算*，执行学习并运行我们的机器学习模型所需的处理器。

1.  *内存*，用于存储数据、模型及其输出作为变量和数据结构；以及

1.  *连接*，我们使用它来移动数据和模型。

这些维度虽然可以说是有些任意，但非常适合讨论本书中将要涉及的主题。在这种情况下，选择合适的硬件意味着在计算、内存和连接方面做出权衡，从而使机器学习系统能够实现其设计目标，同时适应可用的预算。引用 RFC 1925（Callon 1996）中的一个普遍真理：

> “（7a）好、快、便宜：你可以选择任意两个（你不能三者兼得）。”

### 2.1.1 计算

我们在机器学习系统中通常可以找到的三种处理器类型是：

1.  *中央处理器*（CPUs），通常是来自 AMD 或 Intel 的 x86-64 处理器，或者 ARM 处理器。

1.  来自 NVidia 或 AMD 的*图形处理器*（GPUs）。

1.  *张量处理器*（TPUs），通常来自 Google。当然，还有其他专门用于加速机器学习的硬件（Reuther et al. 2020），但就实际用途而言，与 TPUs 发挥着相同的作用。

CPU、GPU 和 TPU 在速度、功能和通用性方面代表了不同的权衡。为了另一个而牺牲一个是无法避免的：摩尔定律（芯片上的晶体管数量每年或每两年翻倍）和丹纳德定律（随着晶体管变得更小，功率密度保持不变，也就是说，我们得到更多的晶体管）赋予的“简单”性能提升的终结意味着我们不能再期望通用处理器以我们习惯的速度变快。晶体管不能无限制地变得更小，否则会违反物理定律。电流和电压不能进一步降低，同时保持晶体管的可靠性，所以我们不能再轻易地将芯片上的晶体管数量翻倍。摆脱这一困境的唯一方法是使用特定领域的架构，这些架构利用它们的晶体管和电力预算来最大化特定类型的操作（Jouppi et al. 2018）。因此，GPU 和，更近期的，TPU 在机器学习应用中的兴起。

CPU 是最通用的计算类型：它们可以通过它们实现的指令执行广泛的操作；它们可以在一定程度上并行执行多个操作（无论是在不同的核心上还是在同一核心的不同线程上）；并且它们可以高效地处理任何类型的数据。同时，CPU 包含*逻辑单元*，这些单元实现了专门的单一指令多数据（SIMD）指令集，如流式 SIMD 扩展（x86 上的 SSE1 到 SSE4，ARM 上的 Neon）和高级向量扩展（x86 上的 AVX、AVX2、AVX512，ARM 上的 SVM），以高效且同时地在多个变量上执行数值计算。²通过使用这些指令可以获得显著的加速效果，从 10-15%到 10 倍不等（例如，参见 Williams-Young 和 Li 2019；Fortin 等人 2021）。SIMD 指令的主要限制是它们只能操作 CPU 内部最小型、最快的内存*寄存器*的内容。例如，在 x86-64 CPU 上，每个寄存器可以存储 2-16 个浮点数，每个核心有 16 个（SSE、AVX、AVX2）或 32 个（AVX512）寄存器。

另一种并行执行多个操作的指令类型是*融合操作*，例如 FMA（“融合加法和乘法”），它对多个变量执行预定义的操作集，并且所需时间大约与执行单个操作的时间相同。它们还具有额外的优势，即在最后一个操作之后只需进行一次浮点数四舍五入到精度，从而消除了许多舍入问题（更多内容请参阅第 3.1.2 节）。

GPU 擅长处理大量数据的并行计算。它们与 CPU 在本质上不同：它们的行为类似于异步设备，其中我们加载数据，等待数据被处理，然后收集结果。从实用角度来看，现代 GPU 可以被视为多线程、多核向量处理器（Volkov 和 Demmel 2008），旨在处理一维数据数组。数据在内部被细分为将被数百个独立、相同的*单元*处理的块。³每个单元极其简单：在功能上接近 CPU 的 SIMD/FMA 逻辑单元。它有自己的寄存器集和局部内存缓存，但一次只能应用一种类型的操作，并且完全由 GPU 调度器驱动。GPU 调度器通过以最大化*占用率*（GPU 的整体负载）的方式分配任务，并确保它们从 GPU 的全局内存中获取数据，来确保单元忙碌。

这种级别的并行性使它们在理论上比 CPU 快得多：CPU 最多只有几十个核心，而 GPU 有数百个单元，这些单元都能同样有效地使用 SIMD 指令。这使得 GPU 调度器能够处理大小不等且具有不同延迟的任务，从而限制它们对并行计算效率的影响。此外，每个 GPU 单元比 CPU 核心拥有更多的寄存器⁴并且可以在极低的延迟下处理大量数据。

另一方面，使所有这些成为可能的硬件设计限制了 GPU 的功能。单元被优化以处理 32 位浮点数和整数；现代硬件也很好地支持 16 位和 64 位浮点数，但处理其他类型的变量则比较困难。将数据传输到单元需要先将它们复制到 GPU 全局内存中，在那里它们将被存储在一个或多个内存银行中。不同的单元不能在同一时间从同一个内存银行中读取不同的数据，因此我们应该仔细优化内存中数据的布局。此外，数据被假定为组织成一维数组：进一步的结构被忽略。对分支（即 if-then-else 编程结构）的支持仅限于 GPU 调度器：单元根本不具备条件执行的概念。最后，单元被组织成 32 到 64 个一组，GPU 调度器只能将任务分配给组。任何大小不是组大小倍数的任务都会导致组利用率不足并降低占用率。

TPUs 更加专业化：它们是专门为训练和执行深度神经网络模型的推理而构建的，以实现最佳的平均和尾部性能。TPU 核心⁵的架构基于五个设计决策（Jouppi 等人 2018）：

+   每个处理器包含一个非常简单的核心；

+   将大部分计算能力集中在一个大型的二维矩阵乘法单元中。

+   将内存组织成网状网络，允许核心之间进行异步、无锁通信；

+   实现对整数和有限精度的浮点数的硬件支持，这可以节省内存；

+   丢弃所有不是严格必需用于处理深度神经网络的功能；

这种对深度学习的单一关注使 TPU 成为使用此类模型的最佳硬件，性能提升在每瓦性能方面达到 20-30 倍（Jouppi 等人 2018）。特别是，（Jouppi 等人 2020）报告称，TPU 在推理方面的每瓦性能比 GPU 快 50 倍，在训练方面快 5 到 10 倍。这些改进主要是由 TPU 核心比 GPU 或 CPU 核心小得多（面积小 38 倍）的事实驱动的，因此它们消耗的能量（少 13 倍）更少，并将更多的晶体管份额留给了矩阵乘法单元。另一个重要因素是内存布局，它允许 TPU 核心之间以每秒 500Gb 的速度进行无死锁通信，并消除了定期同步它们的需要。直观地，我们可以预期这些性能改进将转移到其他类型的机器学习模型上，这些模型需要类似的数学运算模式，尤其是那些可以完全用矩阵操作来表述的模型。

我们为此级别的性能付出的代价是 TPUs（张量处理单元）完全缺乏灵活性和多功能性，它们实际上只擅长矩阵乘法。TPU 核心无法执行任何指令调度，不支持多线程，并且总体上没有我们在 CPU 和 GPU 中可以找到的任何复杂功能。它们完全由它们所连接的计算系统的 CPU 驱动。为了弥补这一点，可以使用 Google 的 XLA 编译器（Tensorflow 2021）来编译代码，以实现无需动态调度并最大化数据级和指令级并行性，将操作组合起来使用 SIMD/FMA 指令，并确保矩阵乘法单元始终处于忙碌状态。XLA 对 Tensorflow 和 PyTorch 模型的结构具有完全的可见性，并且比传统的编译器或 CPU 和 GPU 调度器更好地跨操作进行优化。它有效到可以实现持续 70%的峰值性能（Jouppi 等人 2020）。TPUs 还针对 Google 的“大脑”浮点格式（“bfloat”）进行了大量优化，对于工业标准 IEEE 754 浮点格式的变量来说较慢（Overton 2001）。（更多内容请参阅第 3.1.2 节。）此外，它们专门设计用于 16 位（“bfloat16”）浮点运算，而不是 32 位运算。这两种格式在处理深度神经网络方面经验上足够好，并且比 IEEE 格式操作效率高八倍（Jouppi 等人 2020）。

### 2.1.2 内存

CPU、GPU 和 TPU 的实际性能受限于我们需要提供它们操作的数据。每个处理器只能访问其直接连接的内存：CPU 的寄存器和内部缓存，GPU 和 TPU 的板载全局和局部内存（见图 2.1）。这意味着将输入数据从系统 RAM 传输到它们的专用内存，并将它们产生的输出复制回系统 RAM 以进行进一步处理。

在不同内存之间移动数据需要时间，在每种类型的内存中移动数据也是如此，尽管较少。理想情况下，我们希望尽可能多的数据尽可能靠近将要处理它的处理器。此外，我们希望该处理器尽可能长时间地处理这些数据，以分摊在大量操作中移动数据成本。这种愿望受到三个因素的影响：

1.  每种处理器可用的和直接可访问的内存**数量**；

1.  访问不同类型内存的**延迟**；

1.  以及我们连接到不同类型内存的**带宽**，这决定了数据在访问后可以多快地传输。

换句话说，延迟是等待内存复制开始所花费的时间，而带宽是我们每秒可以复制的最大内存量。

![不同类型内存的示意图，它们的大小和它们的延迟（CPU 访问它们所需的时间）。时间以纳秒（1ns = $10^{-9}$s）或微秒（1μs = $10^{-6}$s）表示。](img/57ea91307361ffaf0282f31cde38e80e.png)

图 2.2：不同类型内存的示意图，它们的大小和它们的延迟（CPU 访问它们所需的时间）。时间以纳秒（1ns = \(10^{-9}\)s）或微秒（1μs = \(10^{-6}\)s）表示。

显然，每种处理器在访问其专用内存时都会最快，因为它们在物理上位于其旁边或内部。距离在确定内存访问延迟方面起着关键作用：电信号的传播速度限制了我们可以多快地访问该内存。例如，在途中处理这些信号的需求，例如，将不同格式的内存位置地址进行转换，可能会进一步延迟访问。CPU 寄存器、GPU 单元的本地内存和 TPU 的本地内存直接连接到相应的处理器，以最小化距离并使它们可以直接寻址。这将延迟从微秒（\(10^{-6}\)秒）或数十微秒（\(10^{-5}\)秒）减少到数百或数十微秒（\(10^{-9}\)到\(10^{-7}\)秒）。再次引用 RFC 1925：

> “(2) 无论你多么努力，无论优先级如何，你都无法提高光速。”

访问特定类型内存的延迟通常与它们的尺寸成反比，并且受限于访问它的处理器的频率。我们在图 2.2 中说明了这一点，重点关注 CPU，但我们的考虑也适用于 GPU 和 TPU。CPU 寄存器和各种 CPU 缓存是最小的，因为它们的大小受 CPU 的物理尺寸限制。例如，Sandy Bridge Intel CPU 上的三级缓存（L1、L2、L3）的大小分别为 32kB（L1）、256kB（L2）和 20MB（L3），分别可以在 4、12 和 29 个周期内访问（Williams-Young 和 Li 2019）。相比之下，寄存器只能存储几百字节，但访问它们只需要一个周期。为了实际目的，我们可以将 CPU 周期视为 CPU 时钟频率的倒数。假设我们有一个 2GHz 的 CPU：\[\begin{equation*} \text{$1$ cycle} \operatorname{/} (2 \times 10⁹ \mbox{Hz}) = 5 \times 10^{-10}\mathrm{s} = 0.5\mathrm{ns}. \end{equation*}\]使用这个公式，我们可以推导出图 2.2 中显示的延迟：访问寄存器需要 0.5ns，访问 CPU 缓存需要 2ns 到 14.5ns。很容易看出，如果 CPU 被迫在每 0.5ns 的计算过程中等待几个纳秒来从 CPU 缓存中获取数据，性能会迅速下降。对于需要超过 1 个周期才能完成的指令（如除法、三角函数和超越函数），这种下降可能不太明显，因为计算所花费的时间与等待时间相比更大。

在内存层次结构中接下来是不同的 RAM 集合：从 CPU 可访问的系统 RAM，GPU 板上的视频 RAM 以及 TPU 板上的 RAM。如图 2.2 所示，访问 RAM 的延迟可以达到数百纳秒，这使得它比 CPU 缓存慢至少 10 倍。图 2.1 中称为“全局内存”的 GPU 和 TPU RAM 可能甚至更慢，因为 GPU 和 TPU 通过 PCI Express 总线（PCIe）连接到 CPU，⁶这增加了延迟。然而，RAM 比 CPU 缓存大得多，从几个 GB（GPU 和 TPU RAM）到几个 TB（系统 RAM）不等。

RAM 的延迟如此之高，以至于我们希望尽可能少地从中读取数据，并且每次读取尽可能多的数据。例如，考虑在 RAM 中访问 10MB 的数据以应用一组 10 个 FMA 指令。

+   如果我们将数据作为一个单独的批次传输到 CPU，我们必须等待 60–100ns 才能访问它，然后花费 5ns 进行计算。

+   如果我们以 200 个 50kB 的批次传输数据，我们必须等待 12000–10000ns（12-20μs）来花费同样的 5ns 进行计算。

传输本身花费相同的时间，因为它只取决于 CPU 和 RAM 之间 PCIe 连接的带宽：在 64GB/s 的速度下，10MB 需要 216μs。然而，在第一种情况下，内存传输引入的延迟可以忽略不计，而在第二种情况下，它将总时间增加了大约 20%。这就是为什么 GPU 和 TPU 都是通过一次性从系统 RAM 复制所有数据来初始化的，这使得内存传输（通常称为“内核启动”）成为固定开销成本，该成本将在整个计算过程中分摊。

在内存层次结构的底部，我们有热存储和冷存储。热存储旨在包含我们经常需要立即访问的数据，它将包括连接到计算系统的本地硬盘（主要是固态硬盘）。冷存储用于我们访问频率较低且不需要快速访问时间的数据。它包括磁带、较慢的硬盘和网络附加存储。热存储通常有数十个 terabytes 的大小，冗余度较低；冷存储可能扩展到 petabytes，并且通常具有更高的冗余度，因为它包含应该长期保存的数据。热存储是本地的，因此受 PCIe 延迟和带宽的限制；冷存储可能是远程的，因此受网络延迟和带宽的限制。存储介质很少成为限制因素：它几乎总是比我们用来访问它的连接具有更多的带宽，这成为瓶颈。

### 2.1.3 连接

计算系统最后、最重要的部分是允许数据在不同处理器和内存类型之间移动的连接。内存的性能必然受到其连接到各种处理器的方式的限制。直接连接到特定处理器的内存（CPU 缓存和寄存器、集成在 GPU 和 TPU 板上的内存）总是对该处理器访问速度最快的，因为它以全速运行。这意味着延迟最小化，带宽最大化。例如，TPU 内存的数据吞吐量为 500Gb/s（Jouppi 等人 2020），而 GPU 内存的数据吞吐量为 500-1000Gb/s（Nvidia Quadro 显卡（Mujtaba 2018））。两者的延迟都可以忽略不计。

然而，GPU 不能直接访问系统 RAM；CPU 也不能访问 GPU 板上的内存。这意味着任何要传输到 GPU 进行处理的数据都必须从系统 RAM 复制到板载内存。速度因此受连接 GPU 和系统 PCIe 总线的带宽限制，延迟增加到图 2.2 中所示的水平。对于 TPU 也是如此。

这就是为什么**数据局部性**，保持数据“靠近”将要处理它的处理器，很重要的原因：直接连接具有最佳可能的延迟和带宽，而间接连接则受 PCIe 总线限制。此外，在不同类型的内存之间传输数据通常涉及将其复制到系统 RAM 作为中间步骤，这进一步降低了性能。

热存储和冷存储在几个方面与其他类型的内存不同。首先，它们没有任何计算能力，因此我们无法避免将包含的数据传输到系统 RAM 以进行处理。其次，任何一种存储类型都不一定会饱和其与系统 RAM 的连接：连接本身不会引入任何瓶颈。热存储通常通过 PCIe 连接到计算系统，但其持续读写速度（SATA 3 为 8GB/s，NVMe 为 4GB/s）舒适地小于 PCIe。冷存储甚至更慢，或者只能通过网络连接（如 100Gb/s 以太网）获得。

## 2.2 让硬件满足期望

所有这些硬件类型都有强大的能力，每种都有其独特之处，但为了有效地使用它们，我们需要能够利用它们的编译器（如果我们能从源代码编译软件）或者已经构建来这样做库（如果我们不能）。这意味着使用理解系统内存布局和可用专用硬件指令的编译器，或者基于优化库（如 CUDA（Nvidia 2021））（用于 NVIDIA GPU）或 Intel 的数学内核库（MKL）（Intel 2021）（用于 CPU）构建的软件。一些流行的机器学习框架和库，如 PyTorch（Paszke 等人 2019），甚至更进一步，抽象出所有硬件特定的优化，自动适应它们运行的硬件特性。

从现代计算系统中获得最佳性能的关键是认识到它们有许多处理器（专用或其他），并且我们希望尽可能多地保持所有这些处理器忙碌。换句话说，我们需要**并行性**：

+   在**指令级别**上，我们希望软件使用可以同时执行的硬件指令。

+   在**数据级别**上，我们希望任务具有模块化的输入和输出，这样我们就可以独立地对它们的每个元素进行操作，而无需等待其他操作完成。

+   在**线程级别**上，我们希望我们的机器学习软件的不同部分尽可能少地相互依赖，这样我们就可以在不同的核心、处理器甚至系统上分别运行它们。

线程级并行化的程度取决于我们使用什么算法以及它们的实现方式（参见第 4.6 节）。数据级并行化也是如此：数据点和随机变量是否可以被认为是独立的，参数是否可以独立估计，以及预测是否可以独立计算，这取决于我们使用的机器学习模型以及我们如何学习它。另一方面，指令级并行化则关键取决于使用适当硬件指令的软件（CPU 和 GPU 中的 SIMD 和 FMA，TPU 中的矩阵乘法单元）。这对于数据级并行化也是正确的，因为如果软件没有告诉各个处理器同时操作多个数据点，那么能够同时操作多个数据点是无用的。

利用并行化需要我们向所有参与计算的处理器提供数据，以便它们有东西可以操作。我们如何做到这一点决定了软件的*操作强度*：在执行过程中每访问一个 RAM 字节数的操作数。数据局部性然后是提高这一点的关键：加载数据比在处理器本地内存中操作数据具有更高的延迟，因此处理器最终会空闲等待数据到达。正如我们在第 2.1.3 节中讨论的那样，每次我们从图 2.2 中的不同级别加载数据时，这种情况都会发生。在某种程度上，当我们接近以全容量运行处理器时，这种情况也会发生。例如，CPU 经常会空闲等待从 GPU 和 TPU 接收结果。而且，我们越接近全占用，我们在处理器之间和内部优化加载的空间就越少。根据边际效用递减定律，我们最终会降低它们的整体性能，因为增加占用的收益被管理不同线程和进程争夺资源的开销所抵消。

换句话说，增加操作强度意味着减少内存访问次数。在原地执行数据转换是实现这一目标的方法：它减少了从 RAM 到和从 RAM 到的高延迟数据传输的数量和体积，同时最大化了更快本地内存的使用。这样做，我们防止了处理器在等待数据时停滞（它们处于“饥饿”状态），并允许它们持续运行（我们“用数据喂饱”它们）。代价是内存使用可能会增加，因为我们可能需要在内存中重新排列数据，并且可能需要保留多个副本。根据算法的不同，有时在不牺牲空间复杂度的情况下，可以获得大部分强度，就像（Fortin 等人 2021）中那样。

当我们最终被迫从 RAM 中读取时，大容量 RAM 读取比多次小容量 RAM 读取要好：我们应该在 RAM 中连续布局数据以允许这样做。如果我们不这样做，大多数算法将变得内存受限。（关于这一点，请参阅第三章。）首先限制内存使用也将有助于这一点。因此，对具有较小精度的数值格式（如 16 位浮点数和整数）以及通过压缩或使模型更稀疏来减少机器学习模型参数数量的兴趣增加（Jouppi 等人 2018，2020）；以及通过压缩或使模型更稀疏来减少机器学习模型参数数量的兴趣增加（Hazelwood 等人 2018）。

## 2.3 本地和远程硬件

在第 2.1 和 2.2 节中讨论计算系统的关键方面时，隐含地假设所有硬件都是单个系统的一部分。这种情况不太可能发生：机器学习系统通常由具有特定目的的不同系统组成，因为不同的任务在不同的硬件上运行效果最佳，同时在单个系统中最大化内存、存储和计算成本是昂贵的。拥有不同的系统使得它们可以专业化，并提高性能同时降低成本。我们可以将它们视为 *远程存储* 和 *远程计算*，正如它们在图 2.1 中的标签所示，通过本地或地理网络连接。

同一本地网络中的远程系统通常通过高速以太网连接连接。即使在 Facebook 运营的规模上，50Gb 以太网也足够好（Hazelwood 等人 2018），因此吞吐量不是较小机器学习系统的限制因素。延迟则是更大的问题：在网络中路由流量的网络设备可能会在建立新连接时引入几个微秒的延迟。

对于位于不同地理位置的远程系统，延迟和带宽都是限制因素。一个典型的例子是*云实例*，这些虚拟服务器我们可以快速创建（“提供”）或销毁（“退役”），并且运行在我们拥有的硬件上（私有云）或我们从公共云提供商如亚马逊网络服务（AWS）、微软 Azure 或谷歌云计算服务（GCP）租用的硬件上。延迟源于信号穿越多层网络设备所需的时间，以到达可能位于不同国家的系统。例如，如果我们位于美国西海岸，连接到东海岸的延迟是 40ms，到英国的延迟是 81ms，到澳大利亚的延迟是 183ms（Gregg 2021）。如果远程系统是按需激活的，我们还需要等待它启动，我们才能开始处理任何数据：这需要 1 秒到 40 秒不等，具体取决于实例底层虚拟化的类型（第 7.1.3 节[deploying-code.html#vm-packaging]和第 7.1.4 节[deploying-code.html#container-packaging]）。因此，与图 2.2[hardware.html#fig:memory-hierarchy]中的延迟相比，访问远程系统上的数据比本地系统的存储慢 3-6 个数量级。例如，对于 AWS spot 实例：虽然它们运行成本较低，但每次使用时都必须启动，并且可能会在没有警告的情况下关闭。

一方面，我们希望尽可能保持局部性：将数据和所有将处理它的计算系统放置在同一位置，以避免在不同地点之间进行大量重复的数据传输。设计连接不同系统的本地网络拓扑可以最小化网络内部数据传输的影响，并使将训练复杂模型的负载分散到不同系统成为可能（Hazelwood 等人 2018）。这种方法被称为*分布式*或*联邦学习*（Li 等人 2021）：例如，参见 DeepMind 进行的分布式深度强化学习研究（Espeholt 等人 2018）和谷歌为从移动设备进行联邦学习设计的系统架构（Bonawitz 等人 2019）。后者是*边缘计算*（Khan 等人 2019）的一个实例，通过将数据处理推向靠近数据源的低功耗设备来解决数据隐私、安全和延迟限制（第 5.3.1 节[design-code.html#scoping-pipeline]）。

另一方面，出于灾难恢复的目的，保持地理分布是可取的。在不同位置保留数据和模型的多份副本，使得硬件故障不太可能导致关键资源的丢失：实现这一目标的一个传统策略是“3-2-1 备份规则”（3 份数据副本，2 份生产数据备份，2 种不同存储类型的备份，以及 1 份异地备份）。这也有助于本地化，但需要小心同步不同位置的数据和模型，以确保正确版本在所有地方都得到使用。

## 2.4 为任务选择合适的硬件

没有任何单一的计算机系统配置是整体上最佳的：实际性能是算法类型、模型和硬件类型之间复杂交互的结果。因此，要实现最佳性能，首先应该定义机器学习系统的目标（见第 5.3.1 节），然后选择实现这些目标的软件和硬件。关于这个主题的全面讨论可以在(Gregg 2021)中找到，它探讨了硬件、操作系统、协议、基准测试和配置文件的所有不同方面。在接下来的内容中，我们将关注机器学习模型、支撑它们的软件栈以及硬件之间的交互。数值库，如 BLAS (Blackford et al. 2002)、LAPACK (Anderson et al. 1999)和 GSL (Galassi et al. 2021)，框架如 TensorFlow (TensorFlow 2021a)和 PyTorch，以及底层库如 XLA 和 CUDA，本质上充当模型的编译器，并将它们转换为最适合处理器(们)的最佳可用硬件指令集。

一些任务更适合特定的硬件类型。例如，考虑深度神经网络。训练和推理之间存在重要的计算差异：后者更难以并行化，对内存的需求更高，并且需要更广泛的数据来保持足够的精度，避免在最终模型中发生灾难性错误（Jouppi 等人 2020）。因此，训练最好在配备有大量 GPU 和 TPUs 的计算系统上执行，而推理即使在 CPU 上也能良好运行（Hazelwood 等人 2018）。毕竟，GPU 和 TPUs 并非魔法“加速”设备！它们仅对某些类型的问题有益，这些问题具有明显的并行性，并且主要由向量（GPU）或矩阵（TPU）运算组成。我们应该购买多少个呢？这取决于我们执行每个任务的相对规模和频率。通常，模型训练每隔几天进行一次，而推理是实时进行的，每天可能进行数百万次。例如，亚马逊的整体计算成本中 90%是推理（亚马逊网络服务 2022c)：在那个规模下，使用 GPU 再次成为必要。但由于 GPU 不适合推理，需要一个专门的软件调度器（Jain 等人 2018)来高效且一致、可预测地使用它们。在较小的规模上，具有许多 CPU 内核的计算系统将足够且易于设置。

特定的机器学习模型可能只能在某些类型的硬件上使用。那些大量使用矩阵运算的模型在 GPU 和 TPUs 上表现更好，但它们的大小受到 GPU 和 TPUs 可以访问的板载内存量的限制。整个模型必须适应，同时，还必须有足够的内存来存储模型将要操作的数据。此外，我们希望以尽可能大的批次操作数据以增加占用率：如果被迫以小批量处理数据，因为模型占用了大部分板载内存，性能可能会令人失望。将多个 GPU 或 TPUs 放在同一计算系统中并不能减轻这个问题，因为模型之间不会共享。如果内存需求超出了 GPU 和 TPUs 的能力，我们只能将模型运行在 CPU 和系统 RAM 上，尽管其容量更大但速度较慢。然而，对于不太容易并行化的模型或任务，CPU 可能会表现得更好，因为不同的核心可以执行完全不同的操作。

最后，来自我们未来自己的一个备注：我们应该为比我们现在严格需要的更多硬件做出规划，以适应可能增长的计算、内存和存储需求（*容量规划*）。训练机器学习模型的计算需求在 2012 年到 2018 年之间增长了 10 倍（Jouppi 等人 2020）。此外，自动模型选择技术（对于某些类型的模型也称为“超参数调整”）如 AutoML（He，Zhao 和 Chu 2021）变得越来越普遍，并且平均使用比学习它们选择的模型类型所需的计算能力多 50 倍。数据量（Katal，Wazid 和 Goudar 2013）和模型大小（Zellers 等人 2019；Cohen，Pavlick 和 Tellex 2019）随着时间的推移也在不断增长，需要更多的热存储和更多类型的内存来存储和使用它们（Beam，Manrai 和 Ghassemi 2020）。

云计算可能会减少对容量规划的需求：如果实例相对便宜，并且可以快速创建和销毁，我们可能一开始购买更少的硬件，并根据需要扩展。事实上，动态扩展算法可以在大多数云服务中自动完成这一点。此外，所有主要的云服务提供商都提供带有 GPU 的实例，在谷歌的情况下，还提供 TPUs 以供需要这些技术的应用程序使用。然而，云计算并不是容量规划的万能解决方案。首先，我们从公共云提供商租用的云实例的计费是基于它们的使用时间和它们允许我们如何以及多快地根据我们需求的突然变化进行扩展。因此，如果我们预见经常或长时间使用它们，并且我们有可预测的工作负载和网络流量模式，那么直接购买硬件可能更便宜。其次，云计算只能提供*水平可扩展性*（增加我们可以使用的系统数量），而不适合实现*垂直可扩展性*（增加单个系统的计算能力）。对于至少在一定程度上不是模块化或可并行化的机器学习模型，水平可扩展性可能不会提高其性能，如果我们需要处理必须保存在内存中的大量数据，它可能根本无法提供帮助。第三，云实例更难以进行性能分析和追踪，这使得理解它们的行为（系统的*可观察性*有限）和诊断它们可能存在的问题变得更加困难。（关于这一点，请参阅第 5.3.6 节。）

### 参考文献

亚马逊网络服务。2022c。《AWS Trainium》[`aws.amazon.com/machine-learning/trainium/`](https://aws.amazon.com/machine-learning/trainium/)。

安德森，E.，白，Z.，比肖夫，C.，布莱福特，S.，德梅尔，J.，东加拉，J.，杜克罗兹，J.，等人。1999。*LAPACK 用户指南*。第 3 版。SIAM。

Beam, A. L., A. K. Manrai, 和 M. Ghassemi. 2020\. “在医疗保健中机器学习模型可重复性的挑战.” *美国医学会杂志* 323 (4): 305–6.

Blackford, L. S., J. Demmel, J. Dongarra, I. Duff, S. Hammarling, G. Henry, Heroux, 等人. 2002\. “更新后的基本线性代数子程序（BLAS）集.” *ACM 数学软件事务* 28 (2): 135–51.

Bonawitz, K., H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, 等人. 2019\. “迈向大规模联邦学习：系统设计.” *机器学习与系统会议论文集*，第 374–88 页.

Callon, Ross. 1996\. *《十二个网络真理》*. [`rfc-editor.org/rfc/rfc1925.txt`](https://rfc-editor.org/rfc/rfc1925.txt).

Cohen, A. Gokaslan V., E. Pavlick, 和 S. Tellex. 2019\. *《OpenGPT-2：因为你可以，所以我们复现了 GPT-2》*. [`blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc`](https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc).

Espeholt, L., H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, 等人. 2018\. “IMPALA：基于重要性加权演员-学习架构的可扩展分布式深度强化学习.” *第 35 届国际机器学习会议（ICML）论文集*，第 1407–16 页.

Fortin, P, A. Fleury, F. Lemaire, 和 M. Monagan. 2021\. “用于多项式评估的高性能 SIMD 模数运算.” *并发计算：实践与经验* 33 (16): e6270.

Galassi, M., J. Davies, J. Theiler, B. Gough, G. Jungman, P. Alken, M. Booth, F. Rossi, 和 R. Ulerich. 2021\. *《GNU 科学库》*. [`www.gnu.org/software/gsl/doc/latex/gsl-ref.pdf`](https://www.gnu.org/software/gsl/doc/latex/gsl-ref.pdf).

Gregg, B. 2021\. *《系统性能：企业和云》*. 2nd ed. Addison-Wesley.

Hazelwood, K., S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov, M. Fawzy, 等人. 2018\. “从数据中心基础设施的角度看 Facebook 的应用机器学习.” *IEEE 国际高性能计算机架构会议（HPCA）论文集*，第 620–29 页.

He, X., K. Zhao, 和 X. Chu. 2021\. “AutoML：现状调查.” *基于知识的系统* 212: 106622.

Intel. 2021\. *《Intel oneAPI Math Kernel Library》*. [`software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html`](https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html).

Jain, P., X. Mo, A. Jain, H. Subbaraj, R. Durrani, A. Tumanov, J. Gonzalez, 和 I. Stoica. 2018\. “GPU 推理的动态时空调度.” *NeurIPS 2018 系统与开源软件研讨会论文集*，第 1–9 页.

Jouppi, N. P., D. H. Yoon, G. Kurian, S. Li, N. Patil, J. Laudon, C. Young, 和 D. Patterson. 2020\. “用于训练深度神经网络的专用超级计算机.” *ACM 通讯* 63 (7): 67–78.

Jouppi, N. P., C. Young, N. Patil, and D. Patterson. 2018. “深度神经网络专用架构。” *ACM 通讯* 61 (9): 50–59.

Katal, A., M. Wazid, and R. H. Goudar. 2013. “大数据：问题、挑战、工具和良好实践。” 在 *当代计算国际会议论文集* 中，404–9 页.

Khan, W. Z., E. Ahmed, S. Hakak, I. Yaqoob, and A. Ahmed. 2019. “边缘计算：综述。” *未来一代计算机系统* 97: 219–35.

Li, Q., Z. Wen, Z. Wu, S. Hu, N. Wang, Y. Li, X. Liu, and B. He. 2021. “联邦学习系统综述：数据隐私和保护的前景、炒作和现实。” *IEEE 知识数据工程 Transactions* 提前出版.

Mujtaba, Hassan. 2018. *三星为 NVIDIA Quadro RTX 显卡配备 16Gb GDDR6 内存*. [`wccftech.com/nvidia-quadro-rtx-turing-gpu-samsung-gddr6-memory/`](https://wccftech.com/nvidia-quadro-rtx-turing-gpu-samsung-gddr6-memory/).

Nvidia. 2018. *Nvidia Turing GPU 架构：图形革新*. [`images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf`](https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf).

Nvidia. 2021. *CUDA Toolkit Documentation*. [`docs.nvidia.com/cuda/`](https://docs.nvidia.com/cuda/).

Overton, M. L. 2001. *使用 IEEE 浮点算术进行数值计算*。 SIAM.

Paszke, A., S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, et al. 2019. “PyTorch：一种命令式风格、高性能深度学习库。” 在 *神经信息处理系统（Nips）进展* 中，32:8026–37.

Reuther, A., P. Michaleas, M. Jones, V. Gadepally, S. Samsi, and J. Kepner. 2020. “机器学习加速器综述。” 在 *2020 年 IEEE 高性能极端计算会议（Hpec）论文集* 中，1–12 页.

Tensorflow. 2021. *XLA：机器学习优化编译器*. [`www.tensorflow.org/xla`](https://www.tensorflow.org/xla).

TensorFlow. 2021a. *TensorFlow*. [`www.tensorflow.org/overview/`](https://www.tensorflow.org/overview/).

Volkov, V., and J. W. Demmel. 2008. “基准测试 GPU 以调整密集线性代数。” 在 *2008 年 ACM/IEEE 超级计算会议论文集* 中，1–11 页.

Williams-Young, D. B., and X. Li. 2019. *关于四元数矩阵乘法的有效性和高性能实现*。[`arxiv.org/abs/1903.05575`](https://arxiv.org/abs/1903.05575).

Zellers, R., A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and Y. Choi. 2019. “防御神经网络假新闻。” 在 *神经信息处理系统（NeurIPS）进展* 中，9054–65 页.

* * *

1.  目前，我们将“数据”和“变量”互换使用。不同类型变量中数据的实际表示将是第三章的主题。↩︎

1.  命名规范因供应商而异。在 Nvidia GPU 中，它们被称为“流单元”，组织在“流多处理器”中；在 AMD GPU 中，它们被称为“计算单元”和“工作组处理器”；在 Intel GPU 中，“执行单元”和“执行核心”。↩︎

1.  例如，Nvidia RTX 2060 中的每个单元都有 256kB 的寄存器（Nvidia 2018），而 CPU 每个核心只有 32 \(\times\) 16 = 512 字节的 AVX512 寄存器（因此得名指令集）。↩︎

1.  在描述 TPUs 时，我们遵循 Google 的命名规范，因为在撰写本文时，那是在机器学习中广泛使用的唯一 TPU。↩︎

1.  PCIe 在 x86-64 和 ARM 系统中都在使用，并且有多种版本和速度。在撰写本文时，当前版本是 PCIe 4.0，它使用多达 16 个并行通道来传输高达 64GB/s 的数据。↩︎
