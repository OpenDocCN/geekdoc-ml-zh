## 第十五章：**文本数据增强**

![Image](img/common.jpg)

数据增强如何有用？文本数据最常见的增强技术有哪些？

数据增强有助于通过人工增加数据集的大小来提升模型性能，例如减少过拟合的程度，正如第五章所讨论的那样。这些技术通常也应用于计算机视觉模型，比如旋转、缩放和翻转。

类似地，还有几种增强文本数据的技术。最常见的包括同义词替换、词语删除、词语位置交换、句子洗牌、噪声注入、回译和由大语言模型（LLMs）生成的文本。本章将讨论这些技术，每个技术都有可选的代码示例，存放在*补充/q15-text-augment*子文件夹中，地址是 *[`github.com/rasbt/MachineLearning-QandAI-book`](https://github.com/rasbt/MachineLearning-QandAI-book)*。

### **同义词替换**

在*同义词替换*中，我们随机选择句子中的词语——通常是名词、动词、形容词和副词——并用同义词替换它们。例如，我们可以从句子“猫迅速跳过懒狗”开始，然后增强句子为：“猫快速跳过懒狗。”

同义词替换可以帮助模型学习不同的单词可能具有相似的意思，从而提高其理解和生成文本的能力。在实际应用中，同义词替换通常依赖于如 WordNet 这样的词库。然而，使用这种技术时需要小心，因为并不是所有的同义词在所有语境下都是可以互换的。大多数自动文本替换工具都提供调整替换频率和相似度阈值的设置。然而，自动同义词替换并不完美，你可能需要应用后处理检查，以过滤掉可能不合适的替换。

### **词语删除**

*词语删除*是另一种数据增强技术，帮助模型学习。与通过同义词替换单词改变文本的同义词替换不同，词语删除是通过从文本中删除某些词语来创建新的变体，同时尽量保持句子的整体意思。例如，我们可以从句子“猫迅速跳过懒狗”开始，然后删除词语*迅速*，变成“猫跳过懒狗。”

通过随机删除训练数据中的词语，我们教会模型即使在信息缺失的情况下也能做出准确预测。这可以让模型在遇到现实场景中的不完整或噪声数据时更加鲁棒。而且，通过删除非关键性词语，我们可能教会模型专注于与当前任务最相关的文本关键部分。

然而，我们必须小心，不要删除那些可能显著改变句子意义的关键字。例如，在前面的句子中，如果去掉了*cat*这个词，“The quickly jumped over the lazy dog”就会变得不太合适。我们还必须谨慎选择删除率，以确保删除某些词后，文本依然能够保持合理性。典型的删除率可能在 10%到 20%之间，但这只是一个大致的指导，具体情况可能会根据特定的使用场景有所不同。

### **词语位置交换**

在*词语位置交换*，也被称为*词语洗牌*或*排列组合*中，句子中的词语位置会被交换或重新排列，以创建新的句子版本。例如，如果我们从“猫快速跳过懒狗”开始，我们可能会交换一些词的位置，得到以下句子：“快速猫跳懒狗过。”

尽管这些句子在英语中可能听起来语法不正确或奇怪，但它们为数据增强提供了宝贵的训练信息，因为模型仍然能够识别重要的词汇以及它们之间的关联。然而，这种方法也有其局限性。例如，过度洗牌词语，或者以某些方式洗牌，可能会极大地改变句子的意义，甚至使其变得完全无意义。此外，词语洗牌可能会干扰模型的学习过程，因为某些词语之间的位置关系在这些语境中可能至关重要。

### **句子洗牌**

在*句子洗牌*中，段落或文档中的整个句子被重新排列，以创建输入文本的新版本。通过在文档中洗牌句子，我们让模型接触到同一内容的不同排列方式，帮助它学习识别主题元素和关键概念，而不是仅仅依赖于特定的句子顺序。这促进了对文档整体主题或类别的更全面理解。因此，这种技术特别适用于处理文档级分析或段落级理解的任务，如文档分类、主题建模或文本摘要。

与前述的基于词语的方法（如词语位置交换、词语删除和同义词替换）不同，句子洗牌保持了单个句子的内部结构。这避免了通过改变词语选择或顺序而导致句子语法错误或完全改变意义的问题。

句子重排对于那些句子顺序对整体意思不重要的文本来说很有用。不过，如果句子之间有逻辑或时间上的联系，它可能会不太适用。例如，考虑以下段落：“我去了超市。然后我买了做披萨的材料。之后，我做了些美味的披萨。”将这些句子重新排列为：“之后，我做了些美味的披萨。然后我买了做披萨的材料。我去了超市。”这种重排打乱了叙事的逻辑和时间顺序。

### **噪声注入**

*噪声注入*是一个总括性术语，用来描述通过各种方式改变文本并创造文本变异的技术。它既可以指前面提到的方法，也可以指诸如插入随机字母、字符或拼写错误等基于字符层面的技术，以下示例便展示了这一点：

**随机字符插入**     “猫快速跳过懒狗。”（在单词*quickly*中插入了一个*z*。）

**随机字符删除**     “猫 quickl 跳过懒狗。”（从*quickly*中删除了*y*。）

**拼写错误引入**     “猫 qickuly 跳过懒狗。”（在*quickly*中引入了拼写错误，将其改为*qickuly*。）

这些修改对涉及拼写检查和文本修正的任务非常有帮助，但它们也有助于使模型在面对不完美输入时更加稳健。

### **反向翻译**

*反向翻译*是创造文本变异最常用的技术之一。在这个方法中，句子首先从原始语言翻译成一种或多种不同的语言，然后再翻译回原始语言。往返翻译通常会产生与原句语义相似但结构、词汇或语法上有轻微差异的句子。这种方式可以生成额外的多样化示例用于训练，而不改变整体意义。

举个例子，假设我们将“猫快速跳过懒狗”翻译成德语。我们可能会得到“Die Katze sprang schnell über den faulen Hund。”然后，我们可以将这个德语句子翻译回英文，得到“猫跳过懒狗快速。”

通过反向翻译，一个句子变化的程度取决于所使用的语言以及机器翻译模型的具体情况。在这个例子中，句子的变化非常小。然而，在其他情况下或使用其他语言时，你可能会看到词语或句子结构发生更显著的变化，但整体意义保持不变。

这种方法需要访问可靠的机器翻译模型或服务，并且必须小心确保反向翻译后的句子保留原句的核心含义。

### **合成数据**

*合成数据生成*是一个总括性术语，描述了用于创建模仿或复制真实世界数据结构的人工数据的方法和技术。本章讨论的所有方法都可以视为合成数据生成技术，因为它们通过对现有数据进行小的修改来生成新数据，从而在创建新事物的同时保持整体意义。

现代生成合成数据的技术现在还包括使用解码器风格的 LLM，例如 GPT（解码器风格的 LLM 将在第十七章中详细讨论）。我们可以通过使用“完成句子”或“生成示例句子”提示等方式，利用这些模型从头生成新数据。我们还可以将 LLM 用作反向翻译的替代方法，提示它们重写句子，如图 15-1 所示。

![Image](img/15fig01.jpg)

*图 15-1：使用 LLM 重写句子*

请注意，LLM（如图 15-1 所示）默认以非确定性模式运行，这意味着我们可以多次提示它，以获得多种重写的句子。

### **建议**

本章讨论的数据增强技术通常用于文本分类、情感分析和其他 NLP 任务，这些任务中可用的标注数据量可能有限。

LLM 通常在如此庞大和多样化的数据集上进行预训练，以至于它们可能不像在其他更具体的 NLP 任务中那样广泛依赖这些增强技术。这是因为 LLM 的目标是捕捉语言的统计特性，而它们所训练的数据量通常提供了足够的上下文和表达方式的多样性。然而，在 LLM 的微调阶段，其中一个预训练模型被调整到一个特定任务，并使用较小的任务特定数据集时，数据增强技术可能会变得更加相关，特别是当任务特定的标注数据集较小时。

### **练习**

**15-1.** 使用文本数据增强能否帮助解决隐私问题？

**15-2.** 在哪些情况下数据增强可能对特定任务没有帮助？

### **参考文献**

+   WordNet 词库：George A. Miller，“WordNet: A Lexical Database for English”（1995），* [`dl.acm.org/doi/10.1145/219717.219748`](https://dl.acm.org/doi/10.1145/219717.219748) *。
