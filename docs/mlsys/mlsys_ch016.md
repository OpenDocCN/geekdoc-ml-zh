# 模型优化

*DALL·E 3 提示：展示一个作为繁忙建筑工地表示的神经网络模型，有各种族、男女各种族的建设工人，被标记为‘剪枝’、‘量化’和‘稀疏性’。他们共同努力使神经网络更高效、更小，同时保持高精度。‘剪枝’工人，一位西班牙裔女性，正在从网络的中间剪除不必要的连接。‘量化’工人，一位白人男性，正在调整或微调所有地方的权重。‘稀疏性’工人，一位非洲裔女性，正在移除不必要的节点以缩小模型。背景中有建筑卡车和起重机，协助工人完成任务。神经网络在视觉上从复杂的大型结构转变为更简洁、更小的结构。*

![图片](img/file146.png)

## 目的

*研究优化模型与生产部署约束之间的不匹配如何在机器学习系统中创造关键工程挑战？*

机器学习研究优先考虑精度，产生了在需要的地方无法部署的卓越性能模型：资源受限的移动设备、成本敏感的云环境或延迟关键的前端应用。模型优化连接了理论能力和实际部署，将计算密集型研究模型转化为高效系统，在满足对内存、能源、延迟和成本严格的约束的同时保持性能。没有系统化的优化技术，高级人工智能能力仍然被困在研究实验室中。理解优化原理使工程师能够通过使复杂模型在多样化的部署环境中可访问，从而民主化人工智能能力，从在移动设备上运行的数十亿参数语言模型到嵌入式传感器。

**学习目标**

+   比较包括剪枝、量化、知识蒸馏和神经架构搜索在内的模型优化技术，从其机制和应用方面进行比较

+   评估数值精度级别与其对模型精度、能耗和硬件兼容性的影响之间的权衡

+   将三分优化框架（模型表示、数值精度、架构效率）应用于设计针对特定硬件约束的部署策略

+   分析硬件感知设计原则如何影响模型架构决策和不同部署平台上的计算效率

+   实施稀疏性利用和动态计算技术，在管理精度保持的同时提高推理性能

+   设计集成优化管道，结合多种技术，在资源约束内实现特定的部署目标

+   评估自动化优化方法及其在发现超出手动调整之外的新优化策略中的作用

## 模型优化基础

成功部署机器学习系统需要解决模型复杂性与计算可行性之间的紧张关系。当代机器学习研究产生了越来越强大的模型，其资源需求往往超过了现实世界部署环境的实际限制。这代表了将理论进步转化为可行系统的经典工程挑战，影响了机器学习应用的可用性和可扩展性。

这种资源差距的幅度是巨大的，并且是多方面的。最先进的语言模型可能需要数百 GB 的内存来存储全精度参数（T. B. Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Saxena, 等人 2020；Chowdhery 等人 2022），而目标部署平台如移动设备通常只提供几 GB 的可用内存。这种差异不仅限于内存限制，还包括计算吞吐量、能耗和延迟要求。部署环境的异构性质进一步加剧了这一挑战，每个环境都施加独特的约束和性能要求。

生产型机器学习系统运行在一个复杂的优化景观中，该景观由多个、通常是冲突的性能目标组成。实时应用施加严格的延迟限制，移动部署需要节能以保护电池寿命，嵌入式系统必须在热约束范围内运行，而云服务需要在大规模上实现成本效益的资源利用。这些限制共同定义了一个多目标优化问题，需要系统的方法来实现所有相关性能维度的满意解决方案。

**模型优化**是对机器学习模型进行系统性的转换，以最大化**计算效率**同时保持**任务性能**，使部署能够跨越**多样化的硬件约束**。

模型优化这一工程学科已经通过将算法创新与硬件感知设计原则相结合的系统方法来应对这些挑战。有效的优化策略需要深入理解模型架构、数值精度、计算模式和目标硬件特性之间的相互作用。这种跨学科的方法将优化从一种临时的技术集合转变为一个由理论基础和经验验证指导的原则性工程学科。

本章建立了一个围绕三个相互关联维度构建的全面理论和实践框架，这三个维度是：模型表示中的结构效率、通过精度优化实现的数值效率以及通过硬件感知实现计算效率。通过这个框架，我们研究了量化等现有技术如何实现内存减少和推理加速，剪枝方法如何消除参数冗余同时保持模型精度，以及知识蒸馏如何使复杂模型到高效架构的能力迁移。总体目标超越了简单的性能指标，使复杂的机器学习能力能够在整个计算环境和应用领域得到部署。

## 优化框架

优化过程通过三个相互关联的维度进行，这些维度将软件算法与硬件执行连接起来，如图 10.1 所示。图 10.1 展示了这些维度及其关系，理解这些维度及其关系为本章探讨的所有技术提供了概念基础。

![优化框架图](img/file147.svg)

图 10.1：**优化栈**：模型优化通过三个层次（高效模型表示、高效数值表示和高效硬件实现）进行，每个层次都针对系统性能和资源利用的不同方面。这些层次允许在模型精度、计算成本和内存占用之间进行结构化权衡，以满足不同部署环境的需求。

理解这些层之间的相互作用揭示了优化工程的系统性。模型表示技术（剪枝、蒸馏、结构化近似）降低计算复杂性，同时为数值精度优化创造机会。量化和低精度算术利用硬件能力实现更快执行，而架构效率技术使计算模式与处理器设计相匹配。软件优化通过创建结构化、可预测的工作负载，为硬件加速奠定基础，这些工作负载可以由专用处理器高效执行。

本章从工程角度审视每个优化层，提供了量化（训练后量化和量化感知训练）、剪枝策略（基于幅度、结构化和动态）以及蒸馏程序（温度缩放、特征迁移）的具体算法。我们探讨了这些技术如何协同作用，以及它们的有效性如何依赖于目标硬件特性。该框架指导系统性的优化决策，确保模型转换与部署约束相一致，同时保留基本功能。

本章通过系统地应用优化原则，将早期基础中的效率概念转化为可操作的工程实践。掌握量化、剪枝和蒸馏技术为从业者提供了部署复杂机器学习模型到各种计算环境的基本工具。所提出的优化框架弥合了理论模型能力与实际部署需求之间的差距，使机器学习系统能在现实应用中提供性能和效率。

## 部署环境

机器学习模型作为更大系统的一部分运行，具有复杂的约束、依赖和权衡。模型优化不能被视为一个纯粹算法问题；它必须被视为一个系统级挑战，需要考虑计算效率、可扩展性、部署可行性和整体系统性能。第十三章中的操作原则为理解模型优化的系统视角提供了基础，强调了优化为何重要，驱动优化努力的约束关键，以及定义有效优化策略的原则。

### 实际部署

现代机器学习模型在基准数据集上通常能实现令人印象深刻的准确性，但使它们适用于实际应用远非易事。机器学习系统在计算、内存、延迟和能源约束下运行，这些约束对训练和推理都有重大影响 (Choudhary et al. 2020)。在研究环境中表现良好的模型，在集成到更广泛的系统中时可能证明不切实际，无论部署环境包括云环境、智能手机集成还是微控制器实现。

除了这些部署复杂性之外，实际可行性包括训练、存储和执行效率，而不仅仅是准确性。1

效率要求在不同部署环境中表现不同。在大规模云机器学习设置中，优化模型有助于最小化训练时间、计算成本和功耗，使大规模人工智能工作负载更加高效 (Jeff Dean, Patterson, and Young 2018)。相比之下，边缘机器学习 2 要求模型在有限的计算资源下运行，需要优化以减少内存占用和计算复杂性。移动机器学习引入了额外的约束，如电池寿命和实时响应性，而微型机器学习 3 将效率推向极致，要求模型适应超低功耗设备的内存和处理限制 (C. R. Banbury et al. 2020)。

优化有助于可持续和可访问的人工智能部署，遵循在第十八章 Chapter 18 中确立的可持续性原则。随着人工智能工作负载的扩展，减少模型的能耗足迹非常重要，有助于减轻大规模机器学习训练和推理的环境影响(D. Patterson et al. 2021a)。同时，优化的模型可以扩大机器学习的应用范围，支持在资源匮乏的环境中的应用，从农村医疗保健到在野外运行的自主系统。

### 平衡权衡

准确性与效率之间的紧张关系推动了所有维度的优化决策。增加模型容量通常可以增强预测性能，同时增加计算成本，导致推理速度变慢，资源消耗更多。这些改进引入了与内存占用 4、推理延迟、功耗和训练效率相关的挑战。随着机器学习系统部署在广泛的硬件平台上，平衡准确性和效率成为模型优化中的关键挑战。

这种紧张关系在不同的部署环境中表现不同。训练需要与模型大小成比例的计算资源，而推理则要求实时应用中具有严格的延迟和功耗限制。

## 框架应用与导航

本节提供了将优化技术应用于实际问题的实用指导，探讨系统约束如何映射到优化维度，并提供了技术选择导航策略。

### 约束映射

了解系统约束如何映射到优化维度，在检查具体技术之前提供了一个导航框架。面对部署挑战时，这种映射指导实践者走向最相关的方法。内存带宽限制表明了模型表示和数值精度优化的关注领域，而延迟瓶颈则建议检查模型表示和架构效率技术。

表 10.1 总结了不同的系统约束如何映射到模型优化的三个核心维度。

表 10.1：**优化维度**：系统约束推动优化沿着三个核心维度——模型表示、数值精度和架构效率——进行，每个维度解决不同的资源限制和性能目标。该表将计算成本映射到精度和效率，内存/存储映射到表示和精度，延迟/吞吐量映射到表示和效率，指导选择适当的优化技术。

| **系统约束** | **模型表示** | **数值精度** | **架构效率** |
| --- | --- | --- | --- |
| **计算成本** | ✗ | ✓ | ✓ |
| **内存和存储** | ✓ | ✓ | ✗ |
| **延迟和吞吐量** | ✓ | ✗ | ✓ |
| **能源效率** | ✗ | ✓ | ✓ |
| **可扩展性** | ✓ | ✗ | ✓ |

这种系统映射建立在第九章（ch015.xhtml#sec-efficient-ai）中确立的效率原则之上。在这里，我们特别关注通过具体技术实现这些效率原则的模型级优化。尽管每个系统约束主要与一个或多个优化维度相对应，但关系并非严格一对一。许多优化技术同时影响多个约束。沿着这三个维度构建模型优化并将技术映射到特定的系统约束，使从业者能够更有效地分析权衡并选择与部署要求最佳匹配的优化。

### 导航策略

本章介绍了一套全面的优化技术工具包，涵盖了模型表示、数值精度和架构效率。然而，并非所有技术都适用于每个问题，而且种类繁多可能会让人感到不知所措。本导航指南帮助您根据具体的约束和目标确定从哪里开始。

表 10.1 识别了哪些优化维度针对特定的瓶颈。内存或模型大小限制表明应关注模型表示和数值精度技术，这些技术可以减少参数数量和位宽。推理延迟需求表明应检查模型表示和架构效率方法，这些方法可以减少计算工作量并提高硬件利用率。训练或推理成本约束优先考虑数值精度和架构效率方法，这些方法可以最小化每操作的计算成本。不接受的精度下降表明应将训练感知优化技术集成到训练过程中，而不是事后应用。

生产系统通常遵循既定模式，而不是随机技术探索。快速部署方法应用后训练修改，需要最少的代码更改，在几小时内实现 4-8 倍的压缩，精度损失在 1-2% (Gholami 等人 2021；Nagel 等人 2021a)。生产级优化按顺序结合多种技术（减少参数，通过训练细化恢复精度，然后应用量化），在几周内实现 8-15 倍的压缩，精度损失小于 1%。针对小于 1MB 模型的极端约束场景需要从开始就进行架构更改，包括自动架构发现和超低精度，这需要数月的专门工程。

模型优化代表的是一个系统工程挑战，而不是一个通用的解决方案。优化的好处高度依赖于目标硬件，相同的量化技术在专用加速器上可以实现 4 倍的速度提升，而在通用处理器上只能实现 1.5 倍（Jacob 等人 2018b；Krishnamoorthi 2018）。准确性的保持因模型架构和任务而异，因为视觉模型通常比语言模型更能有效地容忍激进优化。优化需要迭代测量而不是单一应用。当数据预处理或网络 I/O 主导延迟时，系统级瓶颈可能会限制好处，使得模型优化效果最小化。在优化投资之前进行系统级分析仍然至关重要（在策略和实施部分详细说明）。

这章全面支持非线性阅读方法。部署现有模型的 ML 工程师可以从数值精度部分的训练后技术中受益，这些技术通过最小的代码更改提供快速改进。研究人员和高级实践者需要进行彻底的审查，特别关注数学公式和集成原理。对优化新接触的学生来说，遵循渐进的复杂性标记是有益的，从基础技术到高级方法，从基本概念到专用算法。每个主要部分都是系统地从易到难的方法构建的。

## 优化维度

每个优化维度都值得详细审查。如图 10.1 图所示，模型表示优化减少了需要执行的计算，数值精度优化改变了计算执行的方式，而架构效率优化确保操作在目标硬件上高效运行。

### 模型表示

第一个维度，模型表示优化，专注于消除机器学习模型结构中的冗余。大型模型通常包含过多的参数 5，这些参数对整体性能贡献不大，但会显著增加内存占用和计算成本。优化模型表示涉及的技术包括去除不必要的组件同时保持预测准确性。这些技术包括剪枝、知识蒸馏和自动架构搜索方法，这些方法可以细化模型结构以平衡效率和准确性。这些优化主要影响模型在算法层面的设计，确保它们在计算上可管理的同时保持有效性。

### 数值精度

虽然表示技术修改了执行的计算，但精度优化通过降低权重、激活和算术操作的数值精度来改变这些计算的执行方式。第二个维度，数值精度优化，关注机器学习模型中数值的表示和处理方式。本节中详细介绍的精度优化技术解决了这些效率挑战。量化技术将高精度权重和激活映射到低比特表示，使得在 GPU、TPU 和专用 AI 芯片等硬件加速器上高效执行成为可能（第十一章）。混合精度训练 6 在训练过程中动态调整精度水平，在效率和精度之间取得平衡。

仔细的数值精度优化可以在保持可接受的精度水平的同时，显著降低计算成本，在资源受限的环境中提供复杂的模型访问。

### 架构效率

第三维，架构效率，关注训练和推理期间的计算性能效率。当执行仍然不理想时，良好的模型结构证明是不够的。许多机器学习模型在其计算图中包含冗余，导致操作调度和执行的不效率。稀疏性 7 代表了一种关键的架构效率技术，其中模型利用零值参数来减少计算。

架构效率涉及利用模型权重和激活中的稀疏性，将大型计算组件分解成更有效的形式 8，并根据输入复杂性动态调整计算的技术。

这些架构优化方法提高了不同硬件平台上的执行效率，降低了延迟和功耗。这些效率原则自然地扩展到训练场景，其中梯度检查点和低秩适应 9 等技术有助于减少内存开销和计算需求。

### 三维优化框架

当检查技术交互时，这个三维框架的相互关联性显现出来。剪枝主要解决模型表示问题，但通过减少推理操作也影响了架构效率。量化主要关注数值精度，但会影响内存占用和执行效率。理解这些相互依赖关系可以使得优化组合达到最佳。

这种相互关联的特性意味着优化选择是由系统约束驱动的，这些约束定义了模型必须在其内运行的实用限制。在数据中心部署的机器学习模型与在移动设备或嵌入式系统上运行的模型有不同的约束。计算成本、内存使用、推理延迟和能效都会影响针对特定场景最合适的优化方案。对于资源受限的设备来说，如果模型过大，可能需要进行激进的剪枝和量化，而对于对延迟敏感的应用程序，可能从算子融合 10 和硬件感知调度中受益。

在表 10.1 中建立的约束-维度映射展示了优化策略与现实世界约束之间的相互依赖性。这些关系超越了一对一的对应关系，因为许多优化技术同时影响多个约束。

对每个维度的系统审查从模型表示优化开始，包括修改神经网络结构和参数以消除冗余同时保持准确性的技术。

## 结构化模型优化方法

模型表示优化通过修改神经网络结构和参数来提高效率同时保持准确性。现代模型通常优先考虑准确性而不是效率，包含过多的参数，这增加了成本并减慢了推理速度。这种优化通过两个目标来解决低效：消除冗余（利用过度参数化，即模型在更少的参数下也能达到相似的性能）以及通过梯度检查点 11 和并行处理模式 12 等技术对计算进行结构化以实现高效的硬件执行。

优化挑战在于平衡相互竞争的约束 13。激进的压缩可能会降低准确性，使得模型在生产使用中不可靠，而优化不足则可能导致模型过大或运行速度过慢，不适合目标部署环境。选择适当的技术需要理解模型大小、计算复杂性和泛化性能之间的权衡。

三种关键技术应对这一挑战：剪枝消除低影响参数，知识蒸馏将能力转移到更小的模型，而 NAS 针对特定约束自动化架构设计。每种技术提供独特的优化路径，同时保持模型性能。

这三种技术代表了我们优化框架中独特但互补的方法。剪枝和知识蒸馏减少了现有模型中的冗余，而 NAS 从底层构建优化架构。在许多情况下，它们可以结合使用，以实现更大的优化。

### 剪枝

存储墙限制了系统性能：随着模型变大，内存带宽成为瓶颈，而不是计算能力。修剪通过参数消除来降低内存需求，直接解决了这一限制。最先进的机器学习模型通常包含数百万或数十亿个参数，其中许多对最终预测的贡献很小。虽然大型模型增强了表示能力和泛化能力，但它们也引入了内存占用、计算成本和可扩展性的低效，这影响了在云、边缘和移动环境中的训练和部署。

为了保持准确度，参数的必要性差异很大。许多权重对决策过程的影响很小，通过去除这些权重可以实现显著的效率提升，而不会对性能造成实质性下降。这种冗余存在的原因是现代神经网络过度参数化严重，这意味着它们拥有的权重远多于解决任务所必需的。这种过度参数化在训练期间提供了多个优化路径，并有助于避免局部最小值，但在部署期间为压缩创造了机会。模型压缩通过来自第三章的信息论原理来保持性能，其中神经网络的过度参数化创造了压缩机会。这一观察结果促使人们进行修剪，这是一种优化技术，它系统地去除冗余参数，同时保持模型准确度。

***修剪***是一种模型优化技术，它从神经网络中去除*冗余参数*，同时保持*性能*，以减少*模型大小*和*计算成本*，从而实现高效的部署。

修剪技术使得模型可以变得更小、更快、更高效，而无需重新设计架构。通过消除冗余，修剪直接解决了机器学习系统在内存、计算和可扩展性方面的限制，使其在跨不同硬件平台部署模型时变得至关重要。

现代框架提供了内置的 API，使得这些优化技术易于访问。PyTorch 提供了`torch.nn.utils.prune`用于修剪操作，而 TensorFlow 提供了模型优化工具包 14，其中包括`tfmot.sparsity.keras.prune_low_magnitude()`等函数。这些工具将复杂的研究算法转化为实用的函数调用，使得所有级别的从业者都能实现优化。

#### 修剪示例

修剪可以通过系统性的示例来展示。修剪识别对模型预测贡献最小的权重，并在保持准确度的同时去除它们。最直观的方法是检查权重的大小，因为绝对值较小的权重通常对输出影响很小，因此它们是移除的候选者。

代码列表 10.1 演示了在 3×3 权重矩阵上基于幅度的剪枝，展示了如何通过简单的阈值规则创建稀疏性。

代码列表 10.1：**基于幅度的剪枝**：移除低于阈值的权重以创建稀疏矩阵，将非零参数的数量从 9 减少到 4。

```py
import torch
import torch.nn.utils.prune as prune

# Original dense weight matrix
weights = torch.tensor(
    [[0.8, 0.1, -0.7], [0.05, -0.9, 0.03], [-0.6, 0.02, 0.4]]
)

# Simple magnitude-based pruning: remove weights with magnitude < 0.5
threshold = 0.5
mask = torch.abs(weights) >= threshold
pruned_weights = weights * mask

print("Original:", weights)
print("Pruned:", pruned_weights)
# Result: 4 out of 9 weights remain (56% sparsity)
```

这个例子说明了核心剪枝目标：在保持模型性能的同时最小化参数数量。我们将非零参数从 9 减少到 4（仅保留 4 个权重，因此预算为<semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">k=4</annotation></semantics>）。幅度最小的权重（0.4, 0.1, 0.05, 0.03, 0.02）被移除，而幅度最大的四个权重（0.8, -0.7, -0.9, -0.6）被保留。

将这种直觉扩展到完整的神经网络需要考虑两个问题：要移除多少个参数（稀疏度水平）以及要移除哪些参数（选择标准）。接下来的可视化展示了这一过程在更大的权重矩阵上的应用。

如图 10.2 所示，剪枝通过消除小幅度值来减少非零权重数量，将密集的权重矩阵转换为稀疏表示。这种对稀疏性的显式强制与我们的优化公式中的<semantics><msub><mo>ℓ</mo><mn>0</mn></msub><annotation encoding="application/x-tex">\ell_0</annotation></semantics>-范数约束相一致。

![](img/file148.svg)

图 10.2：**稀疏矩阵转换**：剪枝移除小幅度权重（在右侧矩阵中显示为白色/零），同时保留大幅度权重（以颜色显示），创建一个既减少内存使用和计算，又保持模型精度的稀疏表示。

#### 数学公式

剪枝的目标可以简单地陈述：我们希望找到我们的模型版本，它具有最少的非零权重（最小尺寸），同时引起预测误差（损失）的最小增加。这个直观的目标转化为一个数学优化问题，指导实际的剪枝算法。

剪枝过程可以形式化为一个优化问题。给定一个具有参数<semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics>的训练模型，我们寻求一个稀疏版本<semantics><mover><mi>W</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{W}</annotation></semantics>，它仅保留最重要的参数。目标被表达为：

<semantics><mrow><munder><mo>min</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover></munder><mi>ℒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mtext mathvariant="normal">subject to</mtext><mo stretchy="false" form="postfix">∥</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover><msub><mo stretchy="false" form="postfix">∥</mo><mn>0</mn></msub><mo>≤</mo><mi>k</mi></mrow> <annotation encoding="application/x-tex">\min_{\hat{W}} \mathcal{L}(\hat{W}) \quad \text{subject to} \quad \|\hat{W}\|_0 \leq k</annotation></semantics>

其中，<semantics><mrow><mi>ℒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}(\hat{W})</annotation></semantics> 表示剪枝后的模型损失函数，<semantics><mover><mi>W</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{W}</annotation></semantics> 表示剪枝模型的参数，<semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover><msub><mo stretchy="false" form="postfix">∥</mo><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\|\hat{W}\|_0</annotation></semantics> 是 L0 范数（非零参数的数量），而<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics> 是约束最大模型大小的参数预算。

L0 范数直接通过计数非零参数来衡量模型大小，这决定了内存使用和计算成本。然而，L0 范数最小化是 NP 难的，这使得这种优化具有挑战性。实际的剪枝算法使用基于幅度的选择、基于梯度的重要性或二阶敏感性等启发式方法来有效地近似解。

在列表 10.1 中，这个约束变得具体：我们将<semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mover><mi>W</mi><mo accent="true">̂</mo></mover><msub><mo stretchy="false" form="postfix">∥</mo><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\|\hat{W}\|_0</annotation></semantics>从 9 减少到 4（满足<semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">k=4</annotation></semantics>），使用幅度阈值作为我们的选择启发式方法。使用 L1 或 L2 范数的替代公式鼓励小的权重，但不能保证精确为零，没有明确的阈值就无法减少实际的内存或计算。

为了使剪枝在计算上可行，实际方法用软正则化项替换了硬约束：<semantics><mrow><munder><mo>min</mo><mi>W</mi></munder><mi>ℒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>λ</mi><mo stretchy="false" form="postfix">∥</mo><mi>W</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>1</mn></msub></mrow> <annotation encoding="application/x-tex">\min_W \mathcal{L}(W) + \lambda \| W \|_1</annotation></semantics> 其中 <semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics> 控制稀疏度。<semantics><msub><mo>ℓ</mo><mn>1</mn></msub><annotation encoding="application/x-tex">\ell_1</annotation></semantics>-范数鼓励较小的权重值并促进稀疏性，但并不严格强制零值。其他方法使用迭代启发式算法，在连续步骤中剪除幅度最小的参数，随后进行微调以恢复丢失的精度 (Gale, Elsen, and Hooker 2019a; Labarge, n.d.)。

#### 目标结构

剪枝方法根据从神经网络中移除的结构而有所不同。主要目标包括神经元、通道和层，每个都对模型的架构和性能有独特的含义。

+   **神经元剪枝**移除整个神经元及其相关的权重和偏差，从而减少层的宽度。这种技术通常应用于全连接层。

+   **通道剪枝**（或称为滤波器剪枝），在卷积神经网络中常用，它消除了整个通道或滤波器。这减少了特征图的深度，影响了网络提取某些特征的能力。通道剪枝在图像处理任务中尤其有价值，在这些任务中，计算效率是首要考虑的因素。

+   **层剪枝**从网络中移除整个层，显著减少深度。虽然这种方法可以带来显著的效率提升，但它需要仔细平衡，以确保模型保留足够的容量来捕捉复杂模式。

图 10.3 展示了通道剪枝和层剪枝之间的差异。当剪枝一个通道时，模型的架构必须调整以适应结构变化。具体来说，后续层的输入通道数量必须修改，需要改变应用于移除通道的层的滤波器深度。相比之下，层剪枝移除层中的所有通道，需要更大的架构修改。在这种情况下，必须重新配置剩余层之间的连接以绕过移除的层。无论采用哪种剪枝方法，微调都是重要的，以适应剩余网络并恢复性能。

![](img/file149.svg)

图 10.3：**剪枝策略**：通道剪枝调整层内的滤波器大小，而层剪枝则移除整个层，并需要重新连接剩余的网络组件。这些方法可以减少模型大小和计算成本，但需要微调以减轻由于模型容量减少而导致的性能损失。

#### 无结构剪枝

无结构剪枝在保留整体网络架构的同时移除单个权重。在训练过程中，一些连接变得冗余，对最终计算贡献甚微。剪除这些弱连接可以减少内存需求，同时保留模型的大部分精度。

无结构剪枝的数学基础有助于理解稀疏性是如何系统地引入的。从数学上讲，无结构剪枝将稀疏性引入神经网络的权重矩阵中。用 <semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W \in \mathbb{R}^{m \times n}</annotation></semantics> 表示网络给定层的权重矩阵。剪枝通过应用二进制掩码 <semantics><mrow><mi>M</mi><mo>∈</mo><mo stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><msup><mo stretchy="false" form="postfix">}</mo><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">M \in \{0,1\}^{m \times n}</annotation></semantics> 来移除权重的一个子集，从而得到剪枝后的权重矩阵：<semantics><mrow><mover><mi>W</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>M</mi><mo>⊙</mo><mi>W</mi></mrow> <annotation encoding="application/x-tex">\hat{W} = M \odot W</annotation></semantics> 其中 <semantics><mi>⊙</mi><annotation encoding="application/x-tex">\odot</annotation></semantics> 表示逐元素 Hadamard 积。掩码 <semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics> 是基于剪枝标准构建的，通常是权重幅度。一种常见的方法是基于幅度的剪枝，它移除幅度最低的权重的一部分。这是通过定义一个阈值 <semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics> 来实现的，使得：<semantics><mrow><msub><mi>M</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mn>1</mn><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if</mtext></mrow> <mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>W</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo>></mo><mi>τ</mi></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mn>0</mn><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">otherwise</mtext></mtd></mtr></mtable></mrow></mrow> <annotation encoding="application/x-tex">M_{i,j} = \begin{cases} 1, & \text{if } |W_{i,j}| > \tau \\ 0, & \text{otherwise} \end{cases}</annotation></semantics> 其中 <semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics> 被选择以确保只有最大的 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(1 - s)</annotation></semantics> 权重分数保留。这种方法假设较大的幅度权重对网络的函数贡献更多，因此它们更适合保留。

非结构化剪枝的主要优势是内存效率。通过减少非零参数的数量，剪枝模型需要更少的存储空间，这在将模型部署到内存有限的嵌入式或移动设备时尤其有益。

然而，非结构化剪枝并不一定能在现代机器学习硬件上提高计算效率。标准 GPU 和 TPU 针对密集矩阵乘法进行了优化，而稀疏权重矩阵通常无法充分利用硬件加速，除非有专门的稀疏计算内核。因此，非结构化剪枝主要有利于模型存储，而不是推理加速。虽然非结构化剪枝在参数级别上提高了模型效率，但它并没有改变网络的架构组织。

#### 结构化剪枝

与非结构化剪枝从神经网络中移除单个权重不同，结构化剪枝 15 移除整个计算单元，例如神经元、滤波器、通道或层。这种方法特别有利于硬件效率，因为它产生了更小的密集模型，可以直接映射到现代机器学习加速器。与需要专用执行内核来利用计算优势的非结构化剪枝不同，结构化剪枝通过减少网络架构的整体大小，在通用硬件上实现更有效的推理。

结构化剪枝的动机源于观察到的现象：并非所有神经元、滤波器或层对模型预测的贡献程度相同。一些单元主要携带冗余或低影响的信息，移除它们不会显著降低模型性能。挑战在于识别哪些结构可以被剪枝同时保持准确性。

图 10.4 展示了非结构化剪枝和结构化剪枝之间的关键差异。在左侧，非结构化剪枝移除单个权重（表示为虚线连接），创建了一个稀疏权重矩阵。这可能会破坏原始网络结构，如图所示，在完全连接的网络中，某些连接已被随机剪枝。虽然这减少了活跃参数的数量，但结果稀疏性需要专用执行内核才能充分利用计算优势。

与之相反，结构化剪枝（如图中中间和右侧部分所示）在保留网络整体结构的同时移除整个神经元或滤波器。在中间部分，剪枝后的全连接网络保持了其全连接的特性，但神经元数量减少。在右侧，结构化剪枝应用于卷积神经网络（CNN），通过移除卷积核或整个通道（虚线方块）来实现。这种方法在保持 CNN 核心卷积操作的同时减少了计算负担，使其更兼容硬件加速器。

![图片](img/file150.svg)

图 10.4：**剪枝策略**：非结构化剪枝通过移除单个权重来实现稀疏性，需要专用硬件进行高效计算，而结构化剪枝移除整个神经元或滤波器，保留网络结构，并允许在标准硬件上加速。此图对比了两种方法产生的权重矩阵和网络架构，突出了稀疏程度和计算效率之间的权衡。来源：(C. Qi 等 2021)。

结构化剪枝的常见方法是基于幅度的剪枝，其中根据相关权重的幅度移除整个神经元或滤波器。这种方法背后的直觉是，幅度较小的参数对模型输出的贡献较小，因此它们是消除的理想候选者。神经元或滤波器的重要性通常使用范数函数来衡量，例如对与该单元相关的权重应用<semantics><msub><mo>ℓ</mo><mn>1</mn></msub><annotation encoding="application/x-tex">\ell_1</annotation></semantics>-范数或<semantics><msub><mo>ℓ</mo><mn>2</mn></msub><annotation encoding="application/x-tex">\ell_2</annotation></semantics>-范数。如果范数低于预定义的阈值，则相应的神经元或滤波器将被剪枝。这种方法易于实现，并且除了在层间计算范数之外，不需要额外的计算开销。

另一种策略是激活基于的剪枝，它评估神经元或滤波器在数据集上的平均激活值。持续产生低激活的神经元对网络决策过程的贡献较小，可以安全地移除。这种方法捕捉了网络的动态行为，而不是仅仅依赖于静态的权重值。激活基于的剪枝需要在代表性数据集上对模型进行配置文件分析，以在剪枝决策之前估计平均激活幅度。

基于梯度的剪枝使用模型训练过程中的信息来识别不太重要的神经元或滤波器。关键思想是，梯度幅度较小的单元对减少损失函数的贡献较小，因此对学习的重要性较低。通过根据梯度值对神经元进行排名，结构化剪枝可以移除对模型优化影响最小的那些。与基于幅度或基于激活的剪枝不同，后者依赖于训练模型的静态属性，基于梯度的剪枝需要访问梯度计算，并且通常在训练期间而不是作为后处理步骤应用。

这些方法在计算复杂度和有效性方面都存在权衡。基于幅度的剪枝计算成本低，易于实现，但未考虑神经元在不同数据分布中的行为。基于激活的剪枝提供了一种更数据驱动的剪枝方法，但需要额外的计算来估计神经元的重要性。基于梯度的剪枝利用训练动态，但如果应用于大规模模型可能会引入额外的复杂性。方法的选择取决于目标部署环境的特定约束和剪枝模型的性能要求。

#### 动态剪枝

传统的剪枝方法，无论是无结构的还是结构的，通常涉及静态剪枝，即在训练后或训练期间固定间隔内永久移除参数。然而，这种方法假设参数的重要性是固定的，这并不总是如此。相比之下，动态剪枝根据输入数据或训练动态调整剪枝决策，允许模型实时调整其结构。

动态剪枝可以通过运行时稀疏技术实现，其中模型根据输入特征主动确定要利用哪些参数。激活条件剪枝通过选择性地停用对特定输入表现出低激活值的神经元或通道来例证这种方法（J. Hu 等人 2023）。这种方法引入了输入相关的稀疏模式，在推理期间有效地减少了计算工作量，而不会永久修改模型架构。

例如，考虑一个处理具有不同复杂度图像的卷积神经网络。在推理主要由均匀区域组成的简单图像时，许多卷积滤波器可能产生可忽略的激活。动态剪枝识别这些低影响滤波器，并暂时将它们排除在计算之外，从而提高效率，同时保持对当前输入的准确性。这种自适应行为在延迟敏感的应用中特别有利，在这些应用中，必须根据输入复杂度合理分配计算资源，这与性能测量策略（第十二章）相联系。

另一类动态剪枝发生在训练过程中，其中稀疏性逐渐引入并在整个优化过程中进行调整。例如，渐进幅度剪枝从密集网络开始，随着训练的进行逐渐增加剪枝参数的比例。这些方法不是永久移除参数，而是允许网络通过在训练后期证明重要的连接重新生长来从剪枝引起的容量损失中恢复。

动态剪枝相较于静态剪枝具有多个优势。它允许模型适应不同的工作负载，在保持精度的同时可能提高效率。与静态剪枝不同，静态剪枝可能会过度剪枝并降低性能，动态剪枝提供了一个机制，在必要时可以选择性重新激活参数。然而，实现动态剪枝需要额外的计算开销，因为剪枝决策必须在实时进行，无论是在训练期间还是在推理期间。这使得它比静态剪枝更难集成到标准的机器学习流程中，需要更复杂的生产部署策略和监控框架，这些内容在第十三章中有详细说明。

尽管存在挑战，动态剪枝在边缘计算和自适应人工智能系统中特别有用（第十四章），在这些系统中，不同输入的资源约束和实时效率要求各不相同。下一节将探讨在选择适合特定机器学习系统的剪枝方法时涉及的实际考虑因素和权衡。

#### 剪枝权衡

剪枝技术在内存效率、计算效率、精度保持、硬件兼容性和实现复杂性方面提供了不同的权衡。剪枝策略的选择取决于机器学习系统的具体约束和部署环境，需要与操作考虑因素相结合（第十三章）。

无结构化剪枝在减少模型大小和内存占用方面特别有效，因为它在保持整体模型架构完整的同时移除了单个权重。然而，由于机器学习加速器针对密集矩阵操作进行了优化，无结构化剪枝并不总是能转化为显著的计算速度提升，除非使用了专门的稀疏执行内核。

相比之下，结构化剪枝会消除整个神经元、通道或层，从而得到一个更符合硬件的模型。这种技术提供了直接的计算节省，因为它减少了推理过程中所需的浮点运算次数（FLOPs）16。

不足之处在于，修改网络结构可能会导致精度下降更大，需要仔细微调以恢复丢失的性能。

动态剪枝通过在运行时根据输入数据或训练动态调整要剪枝的参数，将适应性引入剪枝过程。这允许在精度和效率之间取得更好的平衡，因为模型保留了在需要时重新引入先前剪枝参数的灵活性。然而，动态剪枝增加了实现复杂性，因为它需要额外的计算来确定哪些参数需要即时剪枝。

表 10.2 总结了这些剪枝方法之间的关键结构差异，概述了每种方法如何修改模型及其执行的影响。

表 10.2：**剪枝策略**：无结构、结构化和动态剪枝以不同的方式修改模型权重，影响模型大小和计算效率；无结构剪枝提供最大的压缩，但需要专用硬件，而动态剪枝则根据输入数据调整以在精度和资源使用之间取得平衡。

| **方面** | **无结构剪枝** | **结构化剪枝** | **动态剪枝** |
| --- | --- | --- | --- |
| **移除的内容** | 模型中的单个权重 | 整个神经元、通道、滤波器或层 | 根据运行时条件调整剪枝 |
| **模型结构** | 稀疏权重矩阵；原始架构保持不变 | 模型架构被修改；剪枝层被完全移除 | 结构动态适应 |
| **对内存的影响** | 通过消除非零权重来减少模型存储 | 通过移除整个组件来减少模型存储 | 根据实时剪枝而变化 |
| **对计算的影响** | 有限；除非使用专门的稀疏计算，否则仍需要密集矩阵运算 | 直接减少 FLOPs 并加快推理速度 | 动态平衡精度和效率 |
| **硬件兼容性** | 稀疏权重矩阵需要专门的执行支持以提高效率 | 与标准深度学习硬件高效工作 | 需要自适应推理引擎 |
| **是否需要微调** | 剪枝后通常需要微调以恢复精度 | 由于较大的结构修改，更有可能需要微调 | 动态调整，减少重新训练的需求 |
| **用例** | 内存高效的模型压缩，尤其是针对云部署 | 实时推理优化、移动/边缘 AI 和高效训练 | 自适应 AI 应用、实时系统 |

#### 剪枝策略

在无结构、结构化和动态剪枝的广泛类别之外，不同的剪枝工作流程可以影响模型效率和精度保持。两种广泛使用的剪枝策略是迭代剪枝和一次性剪枝，每种策略都有其自身的优点和权衡。

##### 迭代剪枝

迭代剪枝通过多次剪枝循环和后续的微调实现结构移除的渐进式方法。在每一轮中，算法基于预定义的重要性指标移除一小部分结构。然后，模型进行微调以适应这些结构修改，然后再进行下一轮剪枝迭代。这种方法有助于防止精度突然下降，同时允许网络逐步调整到降低的复杂性。

为了说明这个过程，考虑从卷积神经网络中剪除六个通道，如图图 10.5 所示。迭代剪枝不是一次性移除所有通道，而是在三个周期内每次迭代移除两个通道。在每个剪枝步骤之后，模型都会进行微调以恢复性能。第一次迭代移除两个通道导致准确率从 0.995 降至 0.971，但随后的微调将准确率恢复到 0.992。完成两个额外的剪枝-微调周期后，最终模型达到 0.991 的准确率，这仅比原始模型减少了 0.4%，同时运行时通道数减少了 27%。通过将结构修改分布在多个迭代中，网络在实现改进的计算效率的同时保持了其性能能力。

![](img/file151.svg)

图 10.5：**迭代剪枝性能**：逐步移除通道并穿插微调可以在减少模型尺寸的同时保持高准确率；此图显示通道减少了 27%，准确率下降了 0.4%，展示了将结构修改分布在多个迭代中的好处。这种方法与单步剪枝形成对比，后者通常会导致性能显著下降。

##### 单步剪枝

单步剪枝在单步中移除多个架构组件，随后进行广泛的微调阶段以恢复模型准确率。这种激进的方法可以快速压缩模型，但风险是更大的准确率下降，因为网络必须同时适应重大的结构变化。

考虑将单步剪枝应用于迭代剪枝示例中的相同网络。与每次迭代移除两个通道不同，单步剪枝同时消除所有六个通道，如图图 10.6 所示。同时移除网络 27%的通道会导致准确率显著下降，从 0.995 降至 0.914。即使经过微调，网络也只能恢复到 0.943 的准确率，这比原始未剪枝网络下降了 5%。虽然迭代和单步剪枝最终产生相同的网络结构，但迭代剪枝的渐进方法更好地保留了模型性能。

![](img/file152.svg)

图 10.6：**单步剪枝影响**：像图中显示的 27%的通道这样的架构组件的激进移除会导致显著的初始准确率损失，因为网络难以同时适应重大的结构变化。微调部分恢复了性能，但确立了迭代剪枝比单步方法更有效地保留准确率。

选择修剪策略需要仔细考虑几个关键因素，这些因素会影响模型效率和性能。期望的参数减少水平，或稀疏度目标，直接影响策略选择。较高的减少目标通常需要迭代方法来维持准确性，而适度的稀疏度目标可能通过更简单的单次方法实现。

可用的计算资源对策略选择有重大影响。迭代修剪需要为多个微调周期提供大量资源，而单次方法需要的资源较少，但可能牺牲准确性。这种资源考虑与性能要求相联系，具有严格准确性要求的应程序通常从逐步、迭代的修剪中受益，以仔细保留模型能力。具有更灵活性能约束的用例可能更适合更激进的单一方法。

开发时间表也会影响修剪决策。单次方法在时间有限时能实现更快的部署，尽管迭代方法在足够的优化周期内通常能实现更好的结果。最后，目标平台的能力显著影响策略选择，因为某些硬件架构可能更好地支持特定的稀疏模式，使得特定的修剪方法在部署上更有优势。

在修剪策略之间进行选择需要仔细评估项目需求和约束。单次修剪可以通过同时移除多个参数来实现快速模型压缩，这使得它在部署速度比准确性更受重视的场合适用。然而，这种激进的方法通常会导致与更渐进的方法相比性能下降更大。另一方面，迭代修剪虽然计算密集且耗时，但通常通过在多个周期中结构化参数减少，通常能实现更高的准确性保持。这种方法使网络能够逐步适应结构变化，保留维持模型性能的重要连接。这种权衡是增加了优化时间和计算开销。通过系统地评估这些因素，从业者可以选择一种修剪方法，以最佳方式平衡效率提升与特定用例的模型性能。

#### 彩票假设

修剪技术被广泛用于减少神经网络的大小和计算成本，但在确定要移除哪些参数的过程中并不总是直接的。虽然传统的修剪方法基于权重的大小、结构或动态条件来消除权重，但最近的研究表明，修剪不仅仅是减少冗余；它还可能揭示存在于原始模型中的本质上高效的子网络。

这种观点导致了彩票假设 17（LTH），它通过提出在大型神经网络中存在小型、良好初始化的子网络，称为“中奖彩票”，当独立训练时可以达到与完整模型相当准确度，从而挑战了传统的剪枝工作流程。LTH 不是将剪枝仅仅视为训练后的压缩步骤，而是建议它可以作为一种发现机制，在训练早期就识别这些高效的子网络。

LTH 通过迭代剪枝过程得到验证，如图 10.7 图 10.7 所示。首先训练一个大型网络到收敛。然后剪除最低幅度的权重，并将剩余的权重重置到它们的原始初始化状态，而不是重新随机化。这个过程迭代进行，逐渐减小网络的大小同时保持性能。经过多次迭代后，剩余的子网络，被称为“中奖彩票”，证明能够训练到与原始完整模型相同或更高的准确度。

![图片](img/file153.svg)

图 10.7：**中奖彩票发现**：迭代剪枝和权重重置识别出在大模型内部的子网络，当独立训练时，可以达到相当或更高的准确度，挑战了将剪枝视为仅仅是压缩技术的传统观点。这个过程证明了良好初始化的子网络存在，并且可以高效地训练，暗示大型网络的大部分容量可能是冗余的。

彩票假设的启示超出了传统的剪枝技术。LTH 建议，而不是训练大型模型然后在后面剪枝，可以直接从开始训练紧凑、高性能的子网络，从而消除对过度参数化的需求。这一见解挑战了传统的假设，即模型大小对于有效学习是必要的。它还强调了初始化的重要性，因为只有在将中奖彩票重置到其原始权重值时，它们才能保持其性能。这一发现引发了关于初始化在塑造网络学习轨迹中作用的更深层次问题。

该假设进一步强化了迭代剪枝相对于一次性剪枝的有效性。逐渐细化模型结构允许网络在每个阶段进行适应，比一次性移除模型的大部分部分更有效地保留准确度。这个过程与实际部署中使用的剪枝策略非常吻合，在部署中，在减少计算的同时保持准确度非常重要。

尽管前景广阔，但在实践中应用 LTH 仍然计算成本高昂，因为识别获胜的子网络需要多次剪枝和重新训练的周期。正在进行的研究探索是否可以在不进行完整训练的情况下早期检测到获胜的子网络，这可能导致更有效的稀疏训练技术。如果此类方法变得实用，LTH 将从根本上改变机器学习模型的训练方式，将重点从训练后剪枝大型网络转移到从一开始就发现和训练重要组件。

虽然 LTH 在剪枝方面提供了一个有说服力的理论视角，但实际实现依赖于已建立的框架级工具来集成结构和无结构剪枝技术。

#### 剪枝实践

几个机器学习框架提供了内置工具来应用结构和无结构剪枝，微调剪枝模型，并优化云、边缘和移动环境中的部署。

如 PyTorch、TensorFlow 和 ONNX 等机器学习框架提供专门的剪枝实用工具，允许从业者高效地实现这些技术，同时确保与部署硬件的兼容性。

在 PyTorch 中，剪枝可以通过 `torch.nn.utils.prune` 模块实现，该模块提供了将基于幅度的剪枝应用于单个层或整个模型的功能。用户可以通过将最小幅度的权重设置为零来执行无结构剪枝，或者应用结构化剪枝以移除整个神经元或滤波器。PyTorch 还允许自定义剪枝策略，用户可以定义超出权重幅度的剪枝标准，例如基于激活或梯度的剪枝。一旦模型被剪枝，就可以进行微调以恢复丢失的精度，然后再导出用于推理。

TensorFlow 通过 TensorFlow 模型优化工具包（TF-MOT）提供剪枝支持。此工具包通过应用稀疏性诱导正则化直接将剪枝集成到训练过程中。TensorFlow 的剪枝 API 支持全局和层级剪枝，根据权重幅度动态选择要移除的参数。与 PyTorch 不同，TensorFlow 的剪枝通常在训练期间应用，允许模型从一开始就学习稀疏表示，而不是在训练后进行剪枝。TF-MOT 还提供导出工具，将剪枝模型转换为 TFLite 格式，使其与移动和边缘设备兼容。

ONNX18，一种用于模型表示的开源标准，不直接实现剪枝，但为从 PyTorch 和 TensorFlow 剪枝的模型提供导出和兼容性支持。由于 ONNX 旨在实现硬件无关性，它允许在不同框架中经过剪枝的模型针对 TensorRT19、OpenVINO 和 EdgeTPU 等推理引擎进行优化。这些推理引擎可以进一步利用结构化和动态剪枝来提高执行效率，尤其是在专用硬件加速器上。

尽管在框架层面上的剪枝支持已经取得了显著进展，但在实际应用中实施剪枝需要仔细考虑硬件兼容性和软件优化。标准 CPU 和 GPU 通常不原生加速稀疏矩阵运算，这意味着无结构剪枝可能减少内存使用，但不会提供显著的计算速度提升。相比之下，结构化剪枝在推理引擎中支持更广泛，因为它直接减少了执行过程中所需的计算量。动态剪枝，当与推理引擎正确集成时，可以根据工作负载变化和硬件约束优化执行，对自适应人工智能应用尤其有益。

在实际层面，选择合适的剪枝策略取决于几个关键权衡，包括内存效率、计算性能、准确度保持和实现复杂性。这些权衡影响剪枝方法在实际机器学习工作流程中的应用，影响基于资源约束和系统要求的部署选择。

为了帮助指导这些决策，表 10.3 提供了这些权衡的高级比较，总结了实践者在选择剪枝方法时必须考虑的关键效率和可用性因素。

这些权衡突出了将剪枝方法与实际部署需求对齐的重要性。例如，PyTorch、TensorFlow 和 ONNX 等框架使开发者能够实施这些策略，但剪枝方法的有效性取决于底层硬件和应用需求。

表 10.3：**剪枝权衡**：不同的剪枝策略在内存效率、计算速度、准确度保持和硬件兼容性之间取得平衡，影响实际模型部署选择和系统性能。无结构剪枝提供高内存节省，但需要专用硬件，而结构化剪枝以降低准确度为代价，优先考虑计算效率。

| **标准** | **无结构剪枝** | **结构化剪枝** | **动态剪枝** |
| --- | --- | --- | --- |
| **内存效率** | ↑↑ 高 | ↑ 中等 | ↑ 中等 |
| **计算效率** | → 中性 | ↑↑ 高 | ↑ 高 |
| **准确度保持** | ↑ 中等 | ↓↓ 低 | ↑↑ 高 |
| **硬件兼容性** | ↓ 低 | ↑↑ 高 | → 中立 |
| **实现复杂性** | → 中立 | ↑ 中等 | ↓↓ 高 |

例如，结构化剪枝由于其与标准推理引擎的兼容性，常用于移动和边缘应用，而动态剪枝则更适合需要动态调整稀疏度级别的自适应人工智能工作负载。虽然非结构化剪枝对于减少内存占用很有用，但需要专门的稀疏执行内核才能完全实现计算节省。

在实际环境中部署剪枝模型时，理解这些权衡很重要。一些知名模型已经成功地将剪枝集成到优化性能中。MobileNet，一种为移动和嵌入式应用设计的轻量级卷积神经网络，已被剪枝以减少推理延迟，同时保持准确性（A. G. Howard 等人 2017）。BERT20，一种广泛使用的自然语言处理变压器模型，已经经历了注意力头和中间层的结构化剪枝，创建了如 DistilBERT21 和 TinyBERT 等高效的版本，这些版本在减少计算开销的同时保留了大部分原始性能（Sanh 等人 2019）。在计算机视觉领域，EfficientNet22 已被剪枝以去除不必要的滤波器，优化其在资源受限环境中的部署（Tan 和 Le 2019a）。

### 知识蒸馏

想象一位世界级的教授（教师模型），他阅读了数千本书，对某一学科有着深刻而细腻的理解。现在，想象一位聪明的学生（学生模型），他需要快速学习这门学科。教师不是仅仅给学生提供教科书上的答案（硬标签），而是提供丰富的解释，指出为什么一个答案比另一个答案更好，以及不同概念之间的关系（软标签）。学生从这种丰富的指导中比从教科书本身学到的更多。这就是知识蒸馏的精髓。

知识蒸馏通过从更大的预训练教师模型中获取指导来训练一个较小的学生模型，学习教师丰富的输出分布，而不是简单的正确/错误标签。这种区别很重要，因为教师模型提供的比真实标签更丰富的学习信号。以图像分类为例：一个真实标签可能说“这是一只狗”（独热编码：[0, 1, 0, 0, …]）。但一个训练好的教师模型可能会输出[0.02, 0.85, 0.08, 0.05, …]，表明“狗”是最可能的，但图像与“狼”（0.08）和“狐狸”（0.05）有一些共同的特征。这种类间相似性信息有助于学生学习特征关系，这是硬标签无法传达的。

知识蒸馏与剪枝不同。虽然剪枝从现有模型中移除参数，但蒸馏使用来自较大预训练教师模型的指导来训练一个单独的、较小的架构 (Gou 等人 2021)。学生模型优化以匹配教师的软预测（类别的概率分布）而不是简单地从标记数据中学习 (Jiong Lin 等人 2020)。

图 10.8 展示了蒸馏过程。教师模型使用带有温度<semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>的软化的 softmax 函数生成概率分布，学生模型使用这两个软目标和真实标签进行训练。

![图片](img/file154.svg)

图 10.8：**知识蒸馏**：学生模型从预训练教师模型生成的软概率分布中学习，将知识传递到硬标签之外。这个过程使学生模型能够通过使用教师模型的泛化能力和类间关系，以更少的参数达到与教师模型相当的性能。

学生模型的训练过程包含两个损失项：

+   **蒸馏损失**：一个损失函数（通常基于 Kullback-Leibler (KL) 散度 23），它最小化学生模型和教师模型的软标签分布之间的差异。

+   **学生损失**：一个标准的交叉熵损失，确保学生模型正确分类硬标签。

这两个损失函数的组合使学生模型能够吸收来自教师模型的有序知识以及来自数据集的标签监督。这种方法允许较小的模型达到接近其较大教师模型的准确度水平，使知识蒸馏成为模型压缩和高效部署的关键技术。

知识蒸馏允许较小的模型达到通过标准训练单独实现难以达到的准确度水平。这使得它在以推理效率为优先的机器学习系统中特别有用，例如实时应用、云到边缘模型压缩和低功耗 AI 系统 (S. Sun 等人 2019)。

#### 蒸馏理论

知识蒸馏基于这样一个观点：一个训练良好的教师模型编码了关于数据分布的信息，而不仅仅是正确的类标签。在传统的监督学习中，模型被训练以最小化其预测与真实标签之间的交叉熵损失 24。然而，这种方法只为每个类提供了一个硬决策边界，丢弃了关于模型如何将不同类相互关联的潜在有用信息 (G. Hinton, Vinyals, and Dean 2015)。

知识蒸馏通过通过教师模型产生的软概率分布来传递额外的信息，解决了这一限制。它不是训练学生模型仅匹配正确的标签，而是训练学生模型匹配教师对所有可能类别的完整概率分布。这是通过引入一个温度缩放的 softmax 函数来实现的，该函数平滑了概率分布，使得学生模型更容易从教师输出中学习(Gou et al. 2021))。

#### 知识蒸馏数学

为了将这种基于温度的方法形式化，设<semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics>为模型对类别<semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics>的 logits（预 softmax 输出）。标准的 softmax 函数计算类别概率如下：<semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow> <annotation encoding="application/x-tex">p_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}</annotation></semantics>其中，更高的 logits 对应于对类别预测更高的置信度。

在知识蒸馏中，我们引入一个温度参数 25 <semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>，该参数在应用 softmax 之前对 logits 进行缩放：<semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mi>/</mi><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>j</mi></msub><mi>/</mi><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow> <annotation encoding="application/x-tex">p_i(T) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}</annotation></semantics>其中，更高的温度会产生更软的概率分布，揭示了模型如何在不同类别间分配不确定性的更多信息。

然后使用一个损失函数来训练学生模型，该损失函数最小化其输出分布与教师软输出分布之间的差异。最常见的形式结合了两个损失项：<semantics><mrow><msub><mi>ℒ</mi><mtext mathvariant="normal">distill</mtext></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>ℒ</mi><mtext mathvariant="normal">CE</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>s</mi></msub><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>α</mi><msup><mi>T</mi><mn>2</mn></msup><munder><mo>∑</mo><mi>i</mi></munder><msubsup><mi>p</mi><mi>i</mi><mi>T</mi></msubsup><mo>log</mo><msubsup><mi>p</mi><mrow><mi>i</mi><mo>,</mo><mi>s</mi></mrow><mi>T</mi></msubsup></mrow> <annotation encoding="application/x-tex">\mathcal{L}_{\text{distill}} = (1 - \alpha) \mathcal{L}_{\text{CE}}(y_s, y) + \alpha T² \sum_i p_i^T \log p_{i, s}^T</annotation></semantics> 其中：

+   <semantics><mrow><msub><mi>ℒ</mi><mtext mathvariant="normal">CE</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>s</mi></msub><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{CE}}(y_s, y)</annotation></semantics> 是学生预测 <semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding="application/x-tex">y_s</annotation></semantics> 和真实标签 <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics> 之间的标准交叉熵损失。

+   第二项最小化了教师软预测 <semantics><msubsup><mi>p</mi><mi>i</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">p_i^T</annotation></semantics> 和学生预测 <semantics><msubsup><mi>p</mi><mrow><mi>i</mi><mo>,</mo><mi>s</mi></mrow><mi>T</mi></msubsup><annotation encoding="application/x-tex">p_{i, s}^T</annotation></semantics> 之间的 Kullback-Leibler (KL) 散度。

+   因子 <semantics><msup><mi>T</mi><mn>2</mn></msup><annotation encoding="application/x-tex">T²</annotation></semantics> 确保在采用高温值时梯度保持适当的缩放。

+   超参数 <semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics> 平衡了标准训练损失与蒸馏损失的重要性。

通过从硬标签和软教师输出中学习，学生模型从教师的泛化能力中受益，即使参数较少，也能提高其区分相似类别的能力。

#### 蒸馏直觉

通过从硬标签和软教师输出中学习，学生模型从教师的泛化能力中受益，即使参数较少，也能提高其区分相似类别的能力。与传统的训练不同，传统的训练中模型只从二元正确性信号中学习，知识蒸馏允许学生从教师的预测中吸收对数据分布的更丰富理解。

软目标的一个关键优势是它们提供相对置信水平，而不仅仅是单个正确答案。考虑一个图像分类任务，其目标是区分不同的动物种类。使用硬标签训练的标准模型只会收到关于其预测是否正确的反馈。如果一个图像包含猫，正确的标签是“猫”，而所有其他类别，如“狗”和“狐狸”，都被视为同样不正确。然而，一个训练良好的教师模型自然理解猫在视觉上比狐狸更接近狗，其软输出概率可能看起来像图 10.9，其中相对置信水平表明虽然“猫”是最可能的类别，但“狗”仍然是一个合理的替代品，而“狐狸”则不太可能。

![图片](img/file155.svg)

图 10.9：**软目标分布**：相对置信水平表明对于给定的输入，哪些类别更有可能，显示模型可以表达不确定性并提供比简单的正确或错误标签更细微的输出。

与简单地将学生模型强制分类为猫不同，教师模型提供了一个更细微的学习信号，表明虽然“狗”是不正确的，但它比“狐狸”是一个更合理的错误。这种微妙的信息帮助学生模型在相似类别之间建立更好的决策边界，使其对现实世界数据中的模糊性更具鲁棒性。

这种效果在训练数据有限或噪声较大的情况下尤其有用。在大量数据上训练的大型教师模型已经学会了很好地泛化，捕捉到可能难以从较小数据集发现的模式。学生通过继承这种结构化知识，仿佛它能够访问比明确可用的更大的训练信号。

知识蒸馏的另一个关键好处是其正则化效果。由于软目标将概率质量分布在多个类别上，它们防止学生模型过度拟合到特定的硬标签。这种正则化提高了模型泛化能力并减少了对抗性输入的敏感性。学生模型不是自信地将正确类别的概率分配为 1.0，而将所有其他类别的概率分配为 0.0，而是学会做出更准确的预测，这提高了其泛化性能。这对于学生模型参数较少的情况尤为重要，因为较小的网络更容易过度拟合。

最后，蒸馏技术有助于将大型模型压缩成更小、更高效的版本，而不会造成主要性能损失。这种压缩能力直接促进了可持续的 AI 实践第十八章，通过减少模型部署的环境影响同时保持性能标准。从头开始训练小型模型通常会导致较低的准确率，因为模型缺乏学习复杂表示的能力，而这些能力是大型网络能够捕捉到的。然而，通过使用训练有素的教师的知识，学生可以达到比自身更高的准确率，这使得它成为现实世界 ML 部署的更实际选择，尤其是在边缘计算、移动应用和其他资源受限的环境中，这些内容在第十四章第十四章中有所探讨。

#### 效率提升

知识蒸馏的效率优势涵盖了三个关键领域：内存效率、计算效率和部署灵活性。与修改训练模型的剪枝技术不同，蒸馏从一开始就使用教师指导来训练紧凑的模型，这使得仅通过标准训练难以达到的准确率水平成为可能(Sanh 等人，2019)，支持第十二章第十二章中提到的结构化评估方法。

##### 内存和模型压缩

知识蒸馏的一个关键优势是它使小型模型能够保留大型模型的大部分预测能力，从而显著减少内存占用。这在资源受限的环境中特别有用，如移动和嵌入式 AI 系统，因为模型大小直接影响存储需求和加载时间。

例如，NLP 中的 DistilBERT(Sanh 等人，2019)和计算机视觉中的 MobileNet 蒸馏变体(A. G. Howard 等人，2017)等模型已被证明在仅使用一半参数的情况下，保留了其较大教师模型高达 97%的准确率。这种压缩水平通常优于剪枝，因为激进的参数减少可能导致表示能力的下降。

知识蒸馏的另一个关键好处是它能够将鲁棒性和泛化能力从教师模型转移到学生模型。大型模型通常使用大量数据集进行训练，并发展出强大的泛化能力，这意味着它们对噪声和数据变化不太敏感。一个训练良好的学生模型继承了这些属性，使其更不容易过拟合，并在不同的部署条件下更加稳定。这在低数据环境中尤其有用，因为在低数据环境中，从头开始训练一个小模型可能会由于训练样本不足而导致泛化能力差。

##### 计算和推理速度

通过训练学生模型以更紧凑的表示近似教师的知识，蒸馏导致模型在推理时需要的 FLOPs 更少，从而实现更快的执行时间。与可能需要专门硬件支持进行稀疏计算的未结构化剪枝不同，蒸馏模型保持密集结构，使其与现有的机器学习加速器（如 GPU、TPU 和边缘 AI 芯片）更加兼容 (Jiao et al. 2020)。

在实际部署中，这转化为：

+   减少了推理延迟，这对于语音识别、推荐系统和自动驾驶感知模型等实时 AI 应用非常重要。

+   降低能耗，使蒸馏对于移动设备和物联网系统上的低功耗 AI 特别相关。

+   云推理中的更高吞吐量，其中提供蒸馏模型允许大规模 AI 应用在保持模型质量的同时降低计算成本。

例如，当部署用于 NLP 的 transformer 模型时，组织通常使用教师-学生蒸馏来创建在 2-4 倍更低延迟下达到相似精度的模型，这使得每天处理数十亿请求成为可能，同时显著降低了计算开销。

##### 部署和系统考虑

知识蒸馏在多任务学习场景中也非常有效，其中单个教师模型可以指导多个针对不同任务的学生模型。例如，在多语言 NLP 模型中，一个在多种语言上训练的大型教师模型可以将特定于语言的知识转移到较小、针对特定任务的模型中，从而实现跨不同语言的快速部署，无需从头开始重新训练。同样，在计算机视觉中，一个在多种对象类别上训练的教师模型可以将知识蒸馏到针对人脸识别、医学成像或自动驾驶等任务的专用学生模型中。

一旦 student 模型被蒸馏，可以使用剪枝、量化和图优化等技术进一步优化以适应特定硬件的加速。这确保了压缩模型在多个硬件环境中保持推理效率，尤其是在边缘 AI 和移动部署中(Gordon, Duh, and Andrews 2020)。

尽管知识蒸馏有其优势，但也存在一些局限性。蒸馏的有效性取决于教师模型的质量，一个训练不当的教师模型可能会将错误的偏差传递给学生。蒸馏引入了一个额外的训练阶段，其中教师和学生模型必须一起使用，这增加了训练期间的计算成本。在某些情况下，设计一个能够充分利用教师知识的学生模型架构仍然是一个挑战，因为过小的学生模型可能没有足够的容量来吸收所有相关信息。

#### 权衡

与剪枝相比，知识蒸馏在保持准确率方面表现更好，但需要通过训练新模型而不是修改现有模型来提高训练复杂性。然而，剪枝提供了更直接的计算效率提升，尤其是在使用结构化剪枝时。在实践中，结合剪枝和蒸馏通常能获得最佳权衡，正如 DistilBERT 和 MobileBERT 等模型所示，其中剪枝首先减少不必要的参数，然后蒸馏优化最终的 student 模型。表 10.4 总结了知识蒸馏和剪枝之间的关键权衡。

表 10.4：**模型压缩权衡**：知识蒸馏和剪枝是减少模型大小和提高效率的不同方法，每种方法在准确率、计算成本和实现复杂性方面都有独特的优势和劣势。蒸馏优先考虑通过知识传递来保持准确率，而剪枝通过消除冗余参数直接减少计算需求，因此它们的结合使用是获得最佳性能的常见策略。

| **标准** | **知识蒸馏** | **剪枝** |
| --- | --- | --- |
| **准确率保持** | 高 – 学生从教师那里学习，更好的泛化能力 | 变化 – 如果过度剪枝，可能会降低准确率 |
| **训练成本** | 较高 – 需要训练教师和学生模型 | 较低 – 只需微调 |
| **推理速度** | 高 – 生成密集、优化的模型 | 取决于 – 结构化剪枝效率高，非结构化需要特殊支持 |
| **硬件兼容性** | 高 – 在标准加速器上工作 | 有限 – 稀疏模型可能需要专门的执行 |
| **实现难度** | 复杂 – 需要设计教师-学生流水线 | 简单 – 训练后应用 |

知识蒸馏仍然是机器学习系统优化的一个重要技术，通常与剪枝和量化一起用于部署就绪的模型。理解蒸馏如何与这些互补技术相互作用对于构建有效的多阶段优化管道至关重要。

### 结构化近似

基于近似的压缩技术重构模型表示，以降低复杂性同时保持表达能力，补充了之前讨论的剪枝和蒸馏方法。

相比于消除单个参数，近似方法将大的权重矩阵和张量分解为低维组件，使得模型可以更高效地存储和执行。这些技术利用了这样一个观察：许多高维表示可以通过低秩结构很好地近似，从而在不显著损失性能的情况下减少参数数量。与选择性移除连接的剪枝或转移学习知识的蒸馏不同，基于分解的方法通过结构化近似优化了模型的内部表示。

最广泛使用的近似技术包括：

+   **低秩矩阵分解（LRMF）**：一种将权重矩阵分解为低秩矩阵乘积的方法，从而降低存储和计算复杂性。

+   **张量分解**：LRMF 到高维张量的推广，使得神经网络中多向交互的更高效表示成为可能。

这两种方法都提高了机器学习中的模型效率，尤其是在资源受限的环境，如边缘机器学习和 Tiny ML 中。低秩分解和张量分解通过减少所需操作的数量来加速模型训练和推理。以下几节将详细探讨低秩矩阵分解和张量分解，包括它们的数学基础、应用和相关权衡。

#### 低秩分解

许多机器学习模型的权重矩阵中包含大量的冗余，导致计算、存储和部署效率低下。在前几节中，介绍了剪枝和知识蒸馏作为减小模型尺寸的方法，剪枝是通过选择性移除参数，而蒸馏是通过从较大的模型向较小的模型转移知识。然而，这些技术并没有改变模型参数的结构。相反，它们专注于减少冗余权重或优化训练过程。

低秩矩阵分解（LRMF）通过用低秩表示来近似模型的权重矩阵，而不是显式地删除或转移信息，提供了一种替代方法。这项技术将大型参数矩阵重构为紧凑的、低维度的组件，在显著降低存储和计算成本的同时，保留了大部分原始信息。与创建稀疏表示的剪枝技术不同，或者需要额外训练过程的蒸馏技术不同，LRMF 是一种纯粹的数学变换，它将权重矩阵分解为两个或更多较小的矩阵。

这种结构化压缩在机器学习系统中特别有用，在这些系统中效率是一个主要关注点，例如边缘计算、云推理和硬件加速的机器学习执行。通过使用低秩近似，模型可以在保持预测准确性的同时实现参数存储的显著减少，使 LRMF 成为优化机器学习架构的有价值工具。

##### 训练数学

LRMF 是一种在线性代数和机器学习系统中使用的数学技术，通过将其分解为低维矩阵的乘积来近似高维矩阵。这种分解使得模型参数的表示更加紧凑，同时减少了内存占用和计算复杂度，并保留了重要的结构信息。在机器学习系统的背景下，LRMF 在优化模型效率方面发挥着重要作用，尤其是在资源受限的环境，如边缘人工智能和嵌入式部署中。

正式来说，给定一个矩阵 <semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in \mathbb{R}^{m \times n}</annotation></semantics>，LRMF 寻找两个矩阵 <semantics><mrow><mi>U</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">U \in \mathbb{R}^{m \times k}</annotation></semantics> 和 <semantics><mrow><mi>V</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>k</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">V \in \mathbb{R}^{k \times n}</annotation></semantics>，使得：<semantics><mrow><mi>A</mi><mo>≈</mo><mi>U</mi><mi>V</mi></mrow> <annotation encoding="application/x-tex">A \approx UV</annotation></semantics> 其中 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics> 是近似的秩，通常远小于 <semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics> 和 <semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>。这种近似通常通过奇异值分解（SVD）获得，其中 <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics> 被分解为：<semantics><mrow><mi>A</mi><mo>=</mo><mi>U</mi><mi>Σ</mi><msup><mi>V</mi><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">A = U \Sigma V^T</annotation></semantics> 其中 <semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics> 是包含奇异值的对角矩阵，而 <semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics> 和 <semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics> 是正交矩阵。通过仅保留前 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics> 个奇异值，获得 <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics> 的低秩近似。

图 10.10 展示了低秩矩阵分解带来的参数化减少。观察矩阵<semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics>如何通过矩阵<semantics><msub><mi>L</mi><mi>k</mi></msub><annotation encoding="application/x-tex">L_k</annotation></semantics>和<semantics><msubsup><mi>R</mi><mi>k</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">R_k^T</annotation></semantics>的乘积进行近似。为了直观理解，网络中的大多数全连接层都存储为投影矩阵<semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics>，这需要<semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics>参数在计算过程中加载。然而，通过将其分解并近似为两个低秩矩阵的乘积，我们只需要在存储方面存储<semantics><mrow><mi>m</mi><mo>×</mo><mi>k</mi><mo>+</mo><mi>k</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times k + k \times n</annotation></semantics>参数，同时承担额外的矩阵乘法计算成本。只要<semantics><mrow><mi>k</mi><mo><</mo><mi>n</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">k < n/2</annotation></semantics>，这种分解在存储总参数数量方面更少，同时增加了运行时的计算复杂度<semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mi>k</mi><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(mkn)</annotation></semantics> (I. Gu 2023).

![图片](img/file156.svg)

图 10.10：**低秩分解**：将矩阵分解为低秩近似可以减少存储和计算所需的参数数量，从而实现高效的模型表示。通过将矩阵<semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics>表示为两个较小矩阵<semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics>和<semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics>的乘积，我们由存储<semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics>参数转变为存储<semantics><mrow><mi>m</mi><mo>×</mo><mi>k</mi><mo>+</mo><mi>k</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times k + k \times n</annotation></semantics>参数，其中<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>代表降低的秩。来源：The Clever Machine.

LRMF 通过减少参数冗余，广泛用于提高机器学习模型的效率，尤其是在完全连接和卷积层中。在更广泛的机器学习系统背景下，因式分解技术有助于优化模型推理速度、存储效率和适应专用硬件加速器的能力。

完全连接层通常包含大型权重矩阵，这使得它们成为因式分解的理想候选者。LRMF（低秩矩阵分解）不是存储一个密集的 <semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics> 权重矩阵，而是允许使用两个更小的矩阵来表示，这两个矩阵的维度分别是 <semantics><mrow><mi>m</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">m \times k</annotation></semantics> 和 <semantics><mrow><mi>k</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k \times n</annotation></semantics>，从而显著降低存储和计算成本。这种降低在云到边缘的机器学习管道中尤其有价值，因为最小化模型大小可以促进在嵌入式设备上的实时执行。

卷积层也可以通过将卷积滤波器分解为可分离结构来从 LRMF 中受益。深度可分离卷积等技术利用因式分解原理，在不显著损失精度的同时实现计算效率。这些方法与现代 AI 加速框架中使用的硬件感知优化技术相吻合。

LRMF 在协同过滤推荐系统中得到了广泛的应用。通过因式分解用户-项目交互矩阵，可以提取出与用户偏好和项目属性相对应的潜在因子，从而实现高效且准确的推荐。在大型机器学习系统中，此类优化直接影响到生产环境中的可扩展性和性能。

##### 因式分解效率和挑战

通过将权重矩阵分解为低秩分量，存储所需的参数数量从<semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(mn)</annotation></semantics>减少到<semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mi>k</mi><mo>+</mo><mi>k</mi><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(mk + kn)</annotation></semantics>，其中<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>远小于<semantics><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m, n</annotation></semantics>。然而，这种减少是以在推理期间增加额外的矩阵乘法操作为代价的，这可能会增加计算延迟。在机器学习系统中，这种权衡被仔细管理，以平衡存储效率和实时推理速度。

在 LRMF（低秩矩阵分解）中选择一个合适的秩<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>是一个关键挑战。较小的<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>会导致更高的压缩率，但可能会造成显著的信息损失，而较大的<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>则能保留更多信息，但提供的效率提升有限。交叉验证和启发式方法等通常被用来确定最优的秩，尤其是在大规模机器学习部署中，计算和存储约束各不相同。

在现实世界的机器学习应用中，数据集可能包含噪声或缺失值，这可能会影响分解的质量。正则化技术，如添加一个<semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding="application/x-tex">L_2</annotation></semantics>惩罚，可以帮助减轻过拟合并提高 LRMF 的鲁棒性，确保在不同机器学习系统架构中保持稳定的性能。

低秩矩阵分解提供了一种有效的方法，在保持机器学习模型表达能力的同时降低其复杂性。通过用低秩表示来近似权重矩阵，LRMF 促进了高效的推理和模型部署，尤其是在资源受限的环境，如边缘计算中。在机器学习系统中，分解技术有助于可扩展、硬件感知的优化，从而提升现实世界模型的性能。尽管存在诸如秩选择和计算开销等挑战，LRMF 仍然是在机器学习系统设计和部署中提高效率的有价值工具。

#### 张量分解

虽然低秩矩阵分解为压缩机器学习模型中的大型权重矩阵提供了一种有效的方法，但许多现代架构依赖于多维张量而不是二维矩阵。卷积层、注意力机制和嵌入表示通常涉及多向交互，这些交互无法使用标准的矩阵分解技术有效地捕捉。在这种情况下，张量分解提供了一种更通用的方法来降低模型复杂性，同时保持数据中的结构关系。

![图片](img/file157.svg)

图 10.11：**张量分解**：通过将多维张量分解为低秩分量，可以紧凑地表示高维数据，与直接操作原始张量相比，减少了计算成本和内存需求。这项技术将矩阵分解扩展到处理现代机器学习模型（如卷积神经网络）中常见的多向交互。来源：(Richter 和 Zhao 2021)。

张量分解（TD）将低秩分解的原理扩展到高阶张量，允许大型的多维数组通过低秩分量来表示（参见图 10.11）。鉴于张量经常出现在机器学习系统中作为权重参数、激活和输入特征的表示，它们的直接存储和计算通常变得不切实际。通过将这些张量分解成一组较小的因子，张量分解显著降低了内存需求和计算开销，同时保持了原始结构的完整性。

张量分解提高了各种机器学习架构的效率。在卷积神经网络中，它使得可以通过低维因子来近似卷积核，在保留表示能力的同时减少参数。在自然语言处理中，高维嵌入可以被分解成更紧凑的表示，从而实现更快的推理和降低内存消耗。在硬件加速中，张量分解有助于优化张量操作以在专用处理器上执行，确保计算资源的有效利用。

##### 训练数学

张量是矩阵的多维扩展，表示数据跨越多个轴而不是局限于二维结构。在机器学习中，张量自然出现在各种情境中，包括权重参数、激活和输入特征的表示。鉴于这些张量的高维性，直接存储和计算通常变得不切实际，需要高效的分解技术。

张量分解通过用一组低秩组件近似高阶张量，推广了低秩矩阵分解的原则。形式上，对于一个给定的张量 <semantics><mrow><mi>𝒜</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{A} \in \mathbb{R}^{m \times n \times p}</annotation></semantics>，分解的目标是用需要较少参数存储和处理的分解组件来表示 <semantics><mi>𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics>。这种分解减少了内存占用和计算需求，同时保留了原始张量中存在的结构关系。

为张量分解开发了多种分解方法，每种方法都适用于机器学习中的不同应用。一种常见的方法是 CANDECOMP/PARAFAC (CP) 分解，它将张量表示为秩一组件的和。在 CP 分解中，张量 <semantics><mrow><mi>𝒜</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{A} \in \mathbb{R}^{m \times n \times p}</annotation></semantics> 被近似为 <semantics><mrow><mi>𝒜</mi><mo>≈</mo><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>u</mi><mi>r</mi></msub><mo>⊗</mo><msub><mi>v</mi><mi>r</mi></msub><mo>⊗</mo><msub><mi>w</mi><mi>r</mi></msub></mrow> <annotation encoding="application/x-tex">\mathcal{A} \approx \sum_{r=1}^{k} u_r \otimes v_r \otimes w_r</annotation></semantics> 其中 <semantics><mrow><msub><mi>u</mi><mi>r</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">u_r \in \mathbb{R}^{m}</annotation></semantics>，<semantics><mrow><msub><mi>v</mi><mi>r</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">v_r \in \mathbb{R}^{n}</annotation></semantics>，和 <semantics><mrow><msub><mi>w</mi><mi>r</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>p</mi></msup></mrow><annotation encoding="application/x-tex">w_r \in \mathbb{R}^{p}</annotation></semantics> 是分解向量，而 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics> 是近似的秩。

另一种广泛使用的方法是 Tucker 分解，它通过引入核心张量 <semantics><mrow><mi>𝒢</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>k</mi><mn>1</mn></msub><mo>×</mo><msub><mi>k</mi><mn>2</mn></msub><mo>×</mo><msub><mi>k</mi><mn>3</mn></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{G} \in \mathbb{R}^{k_1 \times k_2 \times k_3}</annotation></semantics> 和因子矩阵 <semantics><mrow><mi>U</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><msub><mi>k</mi><mn>1</mn></msub></mrow></msup></mrow><annotation encoding="application/x-tex">U \in \mathbb{R}^{m \times k_1}</annotation></semantics>，<semantics><mrow><mi>V</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo>×</mo><msub><mi>k</mi><mn>2</mn></msub></mrow></msup></mrow><annotation encoding="application/x-tex">V \in \mathbb{R}^{n \times k_2}</annotation></semantics>，以及 <semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>p</mi><mo>×</mo><msub><mi>k</mi><mn>3</mn></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W \in \mathbb{R}^{p \times k_3}</annotation></semantics>，使得 <semantics><mrow><mi>𝒜</mi><mo>≈</mo><mi>𝒢</mi><msub><mo>×</mo><mn>1</mn></msub><mi>U</mi><msub><mo>×</mo><mn>2</mn></msub><mi>V</mi><msub><mo>×</mo><mn>3</mn></msub><mi>W</mi></mrow> <annotation encoding="application/x-tex">\mathcal{A} \approx \mathcal{G} \times_1 U \times_2 V \times_3 W</annotation></semantics> 其中 <semantics><msub><mo>×</mo><mi>i</mi></msub><annotation encoding="application/x-tex">\times_i</annotation></semantics> 表示第 <semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics> 个模的张量矩阵乘法。

另一种方法，张量-训练（TT）分解，将高阶张量分解为一系列低秩矩阵，从而降低存储和计算复杂度。给定一个张量 <semantics><mrow><mi>𝒜</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>×</mo><msub><mi>m</mi><mn>2</mn></msub><mo>×</mo><mi>…</mi><mo>×</mo><msub><mi>m</mi><mi>d</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{A} \in \mathbb{R}^{m_1 \times m_2 \times \dots \times m_d}</annotation></semantics>，TT 分解将其表示为低维张量核的乘积 <semantics><msup><mi>𝒢</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathcal{G}^{(i)}</annotation></semantics>，其中每个核 <semantics><msup><mi>𝒢</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathcal{G}^{(i)}</annotation></semantics> 的维度为 <semantics><msup><mi>ℝ</mi><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>×</mo><msub><mi>m</mi><mi>i</mi></msub><mo>×</mo><msub><mi>r</mi><mi>i</mi></msub></mrow></msup><annotation encoding="application/x-tex">\mathbb{R}^{r_{i-1} \times m_i \times r_i}</annotation></semantics>，而完整的张量则被重建为 <semantics><mrow><mi>𝒜</mi><mo>≈</mo><msup><mi>𝒢</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>×</mo><msup><mi>𝒢</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>×</mo><mi>…</mi><mo>×</mo><msup><mi>𝒢</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow> <annotation encoding="application/x-tex">\mathcal{A} \approx \mathcal{G}^{(1)} \times \mathcal{G}^{(2)} \times \dots \times \mathcal{G}^{(d)}</annotation></semantics>，其中 <semantics><msub><mi>r</mi><mi>i</mi></msub><annotation encoding="application/x-tex">r_i</annotation></semantics> 是 TT 秩。

这些张量分解方法在优化机器学习模型中发挥着重要作用，通过减少参数冗余同时保持表达能力。下一节将探讨这些技术如何应用于机器学习架构，并讨论它们的计算权衡。

##### 张量分解应用

张量分解方法在机器学习系统中被广泛应用以提高效率和可扩展性。通过将高维张量分解为低秩表示，这些方法在保留模型表达能力的同时，减少了内存使用和计算需求。本节探讨了机器学习中张量分解的几个关键应用，重点关注其对卷积神经网络、自然语言处理和硬件加速的影响。

在卷积神经网络（CNNs）中，张量分解用于压缩卷积滤波器，并在推理过程中减少所需的操作数量。标准的卷积层包含一组权重张量，这些张量定义了输入特征如何被转换。这些权重张量通常存在冗余，这意味着它们可以被分解为较小的组件，而不会显著降低性能。CP 分解和 Tucker 分解等技术使得卷积滤波器可以使用低秩张量进行近似，从而减少了参数数量和卷积操作的计算复杂度。这种结构化压缩形式在边缘和移动机器学习应用中尤其有价值，在这些应用中，内存和计算资源是受限的。

在自然语言处理（NLP）中，张量分解通常应用于嵌入层和注意力机制。许多 NLP 模型，包括转换器，依赖于高维嵌入来表示单词、句子或整个文档。这些嵌入可以通过张量分解进行分解，以减少存储需求，同时不损害其捕捉语义关系的能力。同样，在基于转换器的架构中，自注意力机制需要大量的张量乘法，这些可以通过分解技术进行优化，以降低计算负担并加速推理。

机器学习的硬件加速也得益于张量分解，因为它使得在专门的处理器（如 GPU、张量处理单元（TPUs）和现场可编程门阵列（FPGAs））上执行更加高效。许多机器学习框架包括利用张量分解来提高模型执行速度和降低能耗的优化。将张量分解为结构化的低秩组件与现代硬件加速器的内存层次结构相吻合，从而促进了更高效的数据移动和并行计算。

尽管有这些优势，张量分解引入了一些必须谨慎管理的权衡。分解方法和秩的选择显著影响模型精度和计算效率。选择过于激进的秩减少可能导致信息损失过多，而保留过多的成分会减少效率提升。分解过程本身可能引入计算开销，当将张量分解应用于大规模机器学习系统时需要仔细考虑。

##### TD 权衡与挑战

虽然张量分解在机器学习系统中提供了显著的效率提升，但它引入了必须谨慎管理的权衡，以保持模型精度和计算可行性。这些权衡主要涉及分解秩的选择、分解的计算复杂性和分解表示的稳定性。

张量分解的主要挑战之一是确定分解表示的适当秩。在低秩矩阵分解中，秩定义了分解矩阵的维度，直接影响压缩和信息保留之间的平衡。在张量分解中，秩的选择变得更加复杂，因为不同的分解方法以不同的方式定义秩。例如，在 CANDECOMP/PARAFAC (CP) 分解中，秩对应于用于逼近原始张量的秩一张量的数量。在 Tucker 分解中，秩由核心张量的维度决定，而在 Tensor-Train (TT) 分解中，分解成分的秩决定了压缩级别。选择一个不充分的秩可能导致信息损失过多，降低模型的预测性能，而过度的保守秩减少则导致压缩效益有限。

另一个关键挑战是与执行张量分解相关的计算开销。分解过程本身需要解决一个优化问题，通常涉及交替最小二乘（ALS）或随机梯度下降等迭代过程。这些方法可能计算成本高昂，尤其是在机器学习模型中使用的大规模张量中。在推理过程中，需要从分解成分重建张量，这引入了额外的矩阵和标量乘法，可能会增加计算延迟。张量分解在实际中的效率取决于在减少参数存储和最小化分解表示引起的额外计算成本之间取得平衡。

在将张量分解应用于机器学习模型时，数值稳定性也是一个需要考虑的问题。分解表示可能存在数值不稳定性，尤其是在原始张量包含高度相关结构或分解方法引入病态因子时。正则化技术，如对因子矩阵添加约束或逐步应用低秩近似，可以帮助缓解这些问题。用于分解的优化过程必须仔细调整，以避免收敛到次优解，这些解未能保留原始张量的重要属性。

尽管存在这些挑战，张量分解仍然是优化机器学习模型的有价值工具，尤其是在减少内存占用和计算复杂性的应用中。自适应分解方法、自动秩选择策略和硬件感知分解技术的进步继续提高张量分解在机器学习中的实际效用。下一节将总结从低秩矩阵分解和张量分解中获得的关键见解，突出它们在设计高效机器学习系统中的作用。

##### LRMF vs. TD

低秩矩阵分解和张量分解都是通过用低秩表示来近似大型参数结构，从而降低机器学习模型复杂性的核心技术。虽然它们共享提高存储效率和计算性能的共同目标，但它们的应用、计算权衡和结构假设存在显著差异。本节对这些两种技术进行了比较分析，突出了它们的优点、局限性和在机器学习系统中的实际应用案例。

LRMF（低秩矩阵分解）与张量分解之间的一个关键区别在于它们操作数据的维度。LRMF 适用于二维矩阵，这使得它在压缩全连接层或嵌入中的权重矩阵方面特别有用。另一方面，张量分解将分解扩展到多维张量，这些张量在卷积层、注意力机制和多模态学习中自然出现。这种推广使得张量分解能够利用 LRMF 无法捕捉的高维数据的额外结构属性。

从计算角度来看，这两种方法在存储节省和推理速度之间引入了权衡。LRMF 通过将权重矩阵分解为两个较小的矩阵来减少模型中的参数数量，从而减少内存占用，但在推理过程中会引入额外的矩阵乘法。相比之下，tensor decomposition 通过将 tensor 分解为多个低秩分量来进一步减少存储，但代价是更复杂的 tensor 收缩，这可能会引入更高的计算开销。这两种方法之间的选择取决于主要约束是内存存储还是推理延迟。

表 10.5 总结了 LRMF 和 tensor decomposition 之间的关键区别：

表 10.5：**维度与分解**：低秩矩阵分解（LRMF）和 tensor 分解通过使用更少的参数来表示数据，从而减少模型存储需求，但在推理过程中引入了计算权衡；LRMF 适用于二维矩阵，而 tensor decomposition 将这种方法扩展到多维 tensor 以实现更大的压缩潜力。

| **特征** | **低秩矩阵分解（LRMF）** | **tensor 分解** |
| --- | --- | --- |
| **适用数据结构** | 二维矩阵 | 多维 tensor |
| **压缩机制** | 将矩阵分解为两个或更多低秩矩阵 | 将 tensor 分解为多个低秩分量 |
| **常用方法** | 奇异值分解（SVD）、交替最小二乘法（ALS） | CP 分解、Tucker 分解、tensor-Train（TT） |
| **计算复杂度** | 通常较低，对于秩为 k 的近似通常为$ O(mnk) $ | 较高，由于迭代优化和 tensor 收缩 |
| **存储减少** | 将存储从$ O(mn) $减少到$ O(mk + kn) $ | 实现更高的压缩，但需要更复杂的存储表示 |
| **推理开销** | 需要额外的矩阵乘法 | 引入额外的 tensor 操作，可能增加推理延迟 |
| **主要用例** | 全连接层、嵌入、推荐系统 | 卷积核、注意力机制、多模态学习 |
| **实现复杂性** | 实现起来更容易，通常涉及直接分解方法 | 更复杂，需要迭代优化和秩选择 |

尽管存在这些区别，LRMF 和 tensor decomposition 并不是相互排斥的。在许多机器学习模型中，这两种方法可以同时应用以优化架构的不同组件。例如，全连接层可以使用 LRMF 进行压缩，而卷积核和注意力 tensor 则进行 tensor decomposition。技术的选择最终取决于模型的具体特征以及存储效率与计算复杂度之间的权衡。

### 神经架构搜索

在前几节中探讨的剪枝、知识蒸馏和其他技术依赖于人类专业知识来确定最佳模型配置。虽然这些手动方法已经导致了重大进步，但选择最佳架构需要大量的实验，即使是经验丰富的从业者也可能忽略更有效的设计(Elsken, Metzen, and Hutter 2019a)。神经架构搜索(NAS)通过系统地探索大量可能的架构来识别那些最佳平衡准确度、计算成本、内存效率和推理延迟的架构，从而自动化这个过程。

图 10.12 展示了 NAS 过程。NAS26 通过三个相互关联的阶段运作：定义搜索空间（架构组件和约束）、应用搜索策略（强化学习(Zoph and Le 2017a)、进化算法或基于梯度的方法）来探索候选架构，并评估性能以确保发现的设计满足准确性和效率目标。这种自动化使得发现的新架构通常与人类设计的模型相匹配或超过它们，同时需要显著减少专家的努力。

![](img/file158.svg)

图 10.12：**神经架构搜索流程**：自动化的 NAS 技术迭代优化模型架构及其权重，共同优化性能和效率，与依赖人类专业知识和大量试错的手动设计方法不同。这个过程能够发现针对特定计算约束的新颖、高性能架构。

NAS 搜索策略采用多种优化技术。强化学习 27 将架构选择视为一个序列决策问题，使用准确度作为奖励信号。进化算法 28 通过变异和交叉进化架构群体。基于梯度的方法使可微架构搜索成为可能，从而降低计算成本。

#### 模型效率编码

NAS 在三个关键阶段运作：定义搜索空间、探索候选架构和评估其性能。搜索空间定义了 NAS 可以修改的架构组件和约束。搜索策略确定 NAS 如何探索可能的架构，根据过去的观察选择有希望的候选者。评估过程确保发现架构满足多个目标，包括准确度、效率和硬件适应性。

1.  搜索空间定义：这个阶段建立了 NAS 可以修改的架构组件和约束，例如层数、卷积类型、激活函数和针对特定硬件的优化。一个定义良好的搜索空间在创新与计算可行性之间取得平衡。

1.  搜索策略：NAS 使用强化学习、进化算法或基于梯度的技术等方法来探索搜索空间。这些方法引导搜索向最大化性能的同时满足资源约束的架构。

1.  评估标准：候选架构基于多个指标进行评估，包括准确性、FLOPs、内存消耗、推理延迟和能效。NAS 确保所选架构与部署需求相一致。

NAS 将结构设计和优化统一到一个单一的自动化框架中。结果是发现不仅高度准确，而且计算效率高且非常适合目标硬件平台的架构。

#### 搜索空间定义

NAS 的第一步是确定它允许探索的架构集合，即搜索空间。这个空间的大小和结构直接影响 NAS 发现最优模型的有效性。一个定义良好的搜索空间必须足够广泛以允许创新，同时又要足够狭窄以避免对不切实际的设计进行不必要的计算。

典型的 NAS 搜索空间由定义模型结构的模块化构建块组成。这些包括可供选择的层类型，如标准卷积、深度可分离卷积、注意力机制和残差块。搜索空间还定义了网络深度和宽度的约束，指定模型可以有多少层以及每层应包含多少通道。NAS 还考虑了激活函数，如 ReLU、Swish 或 GELU，这些函数会影响模型的表达能力和计算效率。

搜索空间内的其他架构决策包括内核大小、感受野和跳跃连接，这些都会影响特征提取和模型复杂度。一些 NAS 实现还结合了硬件感知优化，确保发现的架构与特定的硬件（如 GPU、TPU 或移动 CPU）相匹配。

搜索空间的选择决定了 NAS 能够优化模型的程度。如果空间过于受限，搜索算法可能无法发现新颖且高效的架构。如果空间过大，搜索将变得计算成本高昂，需要大量资源来探索大量的可能性。找到合适的平衡点可以确保 NAS 能够高效地识别出优于人工设计的架构。

#### 搜索空间探索

一旦定义了搜索空间，NAS 必须确定如何有效地探索不同的架构。搜索策略通过根据过去的观察选择要评估的架构来指导这一过程。一个有效的搜索策略必须在探索（测试新的架构）与利用（完善有希望的设计）之间取得平衡。

已经开发出几种方法来有效地探索搜索空间。基于强化学习的 NAS 将搜索过程表述为一个决策问题，其中代理依次选择架构组件，并根据生成的模型性能接收奖励信号。随着时间的推移，代理通过最大化这一奖励来学习生成更好的架构。虽然有效，但基于强化学习的 NAS 可能计算成本较高，因为它需要在收敛到最佳设计之前训练许多候选模型。

另一种方法是使用进化算法，它维护一个候选架构种群，并通过变异和选择迭代地改进它们。具有更高精度和效率的强大架构被保留，而诸如更改层类型或滤波器大小等修改则引入了新的变体。这种方法已被证明比基于强化学习的 NAS 更有效地平衡了探索和计算可行性。

最近的方法，如基于梯度的 NAS，引入了表示架构选择的可微分参数。与将架构视为离散实体不同，基于梯度的方 法同时使用标准梯度下降优化模型权重和架构参数。这显著降低了搜索的计算成本，使 NAS 在现实世界应用中更加实用。

搜索策略的选择对 NAS（神经架构搜索）的可行性有直接影响。早期依赖强化学习的 NAS 方法需要数周的 GPU 计算才能发现一个单一架构。最近的方法，尤其是基于梯度搜索的方法，显著降低了这一成本，使 NAS 更加高效和易于访问。

#### 候选架构评估

NAS 探索的每一个架构都必须根据一组预定义的标准进行评估。虽然准确性是一个核心指标，但 NAS 还针对效率约束进行优化，以确保模型在实际部署中是实用的。评估过程决定了架构是否应该保留以进行进一步优化，或者为了更有希望的设计而被舍弃。

主要评估指标包括计算复杂度、内存消耗、推理延迟和能效 29。计算复杂度，通常以 FLOPs 衡量，决定了模型的总体资源需求。NAS 倾向于选择在提高准确率的同时减少不必要的计算的架构。内存消耗，包括参数数量和激活存储，确保模型符合硬件约束。对于实时应用，推理延迟是一个关键因素，NAS 会选择在特定硬件平台上最小化执行时间的架构。最后，一些 NAS 实现明确优化功耗，确保模型适用于移动和边缘设备。

例如，FBNet30，这是一个为移动推理优化的 NAS 生成架构，将延迟约束纳入了搜索过程。

通过将这些约束整合到搜索过程中，NAS 系统地发现能够平衡准确率、效率和硬件适应性的架构。而不是手动微调这些权衡，NAS 自动化了最优架构的选择，确保模型适合现实世界的部署场景。

#### NAS 优化问题

神经架构搜索可以表述为一个双层优化问题，它同时搜索最优架构并评估其性能。外层循环搜索架构空间，而内层循环训练候选架构以衡量其质量。

形式上，NAS 旨在从搜索空间<semantics><mi>𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics>中找到最优架构<semantics><msup><mi>α</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\alpha^*</annotation></semantics>，该搜索空间最小化验证损失<semantics><msub><mi>ℒ</mi><mtext mathvariant="normal">val</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{val}}</annotation></semantics>，同时遵守部署约束<semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>（延迟、内存、能耗）：

<semantics><mrow><msup><mi>α</mi><mo>*</mo></msup><mo>=</mo><mo>arg</mo><munder><mo>min</mo><mrow><mi>α</mi><mo>∈</mo><mi>𝒜</mi></mrow></munder><msub><mi>ℒ</mi><mtext mathvariant="normal">val</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>w</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mtext mathvariant="normal">subject to</mtext><mi>C</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><msub><mi>C</mi><mtext mathvariant="normal">max</mtext></msub></mrow> <annotation encoding="application/x-tex">\alpha^* = \arg\min_{\alpha \in \mathcal{A}} \mathcal{L}_{\text{val}}(w^*(\alpha), \alpha) \quad \text{subject to} \quad C(\alpha) \leq C_{\text{max}}</annotation></semantics>

其中 <semantics><mrow><msup><mi>w</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">w^*(\alpha)</annotation></semantics> 代表了架构 <semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics> 的最优权重，通过最小化训练损失获得：

<semantics><mrow><msup><mi>w</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>arg</mo><munder><mo>min</mo><mi>w</mi></munder><msub><mi>ℒ</mi><mtext mathvariant="normal">train</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>w</mi><mo>,</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">w^*(\alpha) = \arg\min_{w} \mathcal{L}_{\text{train}}(w, \alpha)</annotation></semantics>

这种公式揭示了 NAS 的核心挑战：评估每个候选架构都需要昂贵的训练以达到收敛，这使得穷举搜索变得不可行。一个每层有 10 个设计选择的搜索空间，在 20 层的情况下会产生 <semantics><msup><mn>10</mn><mn>20</mn></msup><annotation encoding="application/x-tex">10^{20}</annotation></semantics> 种可能的架构。对每种架构进行 100 个 epoch 的训练将需要数百万 GPU 年。高效的 NAS 方法通过三个关键的设计决策来应对这一挑战：定义可处理的搜索空间、采用高效的搜索策略和加速架构评估。

#### 搜索空间设计

搜索空间定义了 NAS 可以发现的架构。设计良好的搜索空间结合领域知识，将搜索集中在有希望的区域内，同时保持足够的灵活性以发现新颖的模式。

**基于 Cell 的搜索空间**

与搜索整个网络架构不同，基于单元的 NAS 搜索可重用计算块（单元），这些块可以堆叠形成完整的网络。例如，一个卷积单元可能从 3×3 卷积、5×5 卷积、深度可分离卷积、最大池化或恒等连接等操作中选择。具有 4 个节点和每条边 2 个操作的简化单元大约有 10,000 种可能的设计，这比搜索完整架构要容易得多。EfficientNet 使用这种方法来发现可扩展的单元设计，这些设计可以推广到不同的模型大小。

**硬件感知搜索空间**

硬件感知 NAS 将搜索空间扩展到包括部署约束作为首要目标。而不是仅仅为了优化准确性和 FLOPs，搜索明确地最小化在目标硬件（移动 CPU、GPU、边缘加速器）上的实际延迟。MobileNetV3 的搜索空间包括一个延迟预测模型，该模型估计每个候选架构在 Pixel 手机上的推理时间，而无需实际部署它们。这种硬件在环方法确保发现的架构在真实设备上运行效率高，而不仅仅是实现低理论 FLOP 计数。

#### 搜索策略

搜索策略决定了如何有效地在架构空间中导航，而不进行穷举枚举。不同的策略在搜索成本、架构多样性和最优性保证之间做出不同的权衡，如表 10.6 中总结。

表 10.6：**NAS 搜索策略比较**：不同 NAS 方法在搜索效率、用例和限制之间的权衡。强化学习以高成本提供无约束的探索，进化方法利用并行性，基于梯度的方法通过潜在的优化权衡实现显著加速。

| **策略** | **搜索效率** | **何时使用** | **主要挑战** |
| --- | --- | --- | --- |
| 强化学习 | 400-1000 GPU 天 | 新领域，无约束搜索 | 高计算成本 |
| 进化算法 | 200-500 GPU 天 | 可用并行基础设施 | 需要大量种群 |
| 基于梯度的（DARTS） | 1-4 GPU 天 | 计算预算有限 | 可能收敛到次优局部最小值 |

基于强化学习的 NAS 将架构搜索视为一个序列决策问题，其中控制器生成架构并接收准确性作为奖励。控制器（通常是 LSTM）通过策略梯度优化学习在时间上提出更好的架构。虽然这种方法发现了像 NASNet 这样的突破性架构，但其序列性质限制了并行性，需要数百个 GPU 天。

进化算法维护一个候选架构的种群，并通过迭代应用突变（改变操作，添加连接）和交叉（结合父架构）来生成后代。基于适应度的选择保留高性能架构用于下一代。AmoebaNet 利用进化实现了最先进的成果，通过大规模并行化将成本分摊到数千个工作者身上。

基于梯度的方法，如 DARTS（可微分架构搜索），将搜索空间表示为一种连续松弛，其中所有可能的操作都是加权组合。DARTS 不同于离散采样，它通过梯度下降联合优化架构权重和模型权重。通过使搜索可微分，DARTS 将搜索成本从数百个 GPU 天减少到仅 1-4 个 GPU 天，尽管连续松弛可能会错过离散搜索方法发现的离散架构模式。

#### 实践中的 NAS

硬件感知的 NAS 超越了 FLOPs 作为效率代理的角色，直接优化实际部署指标。MnasNet 的搜索结合了一个在真实手机上测量的数千个架构-延迟对上训练的延迟预测模型。搜索目标通过加权乘积结合准确性和延迟：

<semantics><mrow><mtext mathvariant="normal">奖励</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">准确度</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msub><mi>L</mi><mtext mathvariant="normal">目标</mtext></msub></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>β</mi></msup></mrow> <annotation encoding="application/x-tex">\text{Reward}(\alpha) = \text{Accuracy}(\alpha) \times \left(\frac{L(\alpha)}{L_{\text{target}}}\right)^\beta</annotation></semantics>

其中，<semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\alpha)</annotation></semantics>表示测量的延迟，<semantics><msub><mi>L</mi><mtext mathvariant="normal">target</mtext></msub><annotation encoding="application/x-tex">L_{\text{target}}</annotation></semantics>是延迟约束，而<semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>控制精度-延迟权衡。这种公式惩罚超过延迟目标的架构，同时奖励在预算内实现高精度的架构。MnasNet 发现，具有可变扩张比的倒残差比均匀扩张实现了更好的精度-延迟权衡，这是一个手动探索可能错过的设计洞察。

#### 何时使用 NAS

神经架构搜索是一个强大的工具，但其显著的计算成本要求仔细考虑何时进行投资是合理的。

当处理具有独特约束（如新的加速器架构、极端边缘设备）的新型硬件平台时，NAS 变得有意义，在这些平台上，现有的架构优化不佳。在需要大规模部署（数十亿推理）的情况下，即使 1-2%的效率提升也能证明前期搜索成本是合理的，或者当需要多个部署配置（云、边缘、移动）且这些配置可以分摊到许多变体上时，这也同样合理。

相反，当与标准部署约束（例如，在 NVIDIA GPU 上实现 ResNet-50 精度）工作时，应避免使用 NAS，因为已经存在经过良好优化的架构。同样，如果计算预算有限（可用的 GPU 天数少于 100 天），即使是高效的 NAS 方法如 DARTS 也变得不可行。快速变化的需求也使得 NAS 不切实际，因为架构选择可能在搜索完成之前就过时了。

对于大多数从业者来说，从现有的 NAS 发现的架构（EfficientNet、MobileNetV3、MnasNet）开始，比从头开始运行 NAS 提供更好的投资回报率。这些架构经过高度调优，在多个任务上具有良好的泛化能力。将定制的 NAS 保留在具有真正新颖约束或部署规模可以证明投资合理性的场景中。

#### 架构示例

NAS 已被成功用于设计几个最先进的架构，这些架构在效率和精度方面优于手动设计的模型。这些架构展示了 NAS 如何将缩放优化、计算减少、内存效率和硬件感知设计整合到自动化流程中。

最著名的 NAS 生成的模型之一是 EfficientNet，它是通过一个 NAS 框架发现的，该框架搜索深度、宽度和分辨率缩放的最有效组合。与独立调整这些因素的常规缩放策略不同，NAS 使用复合缩放优化模型，该缩放将一组固定的缩放系数应用于网络，以确保网络以平衡的方式增长。EfficientNet 与之前的架构相比，具有更高的准确性和更少的参数以及更低的 FLOPs，使其非常适合云和移动部署。

另一个关键例子是 MobileNetV3，它使用 NAS 优化了其网络结构以适应移动硬件。搜索过程导致了具有挤压和激励层的倒残差块被发现，这些块在降低计算成本的同时提高了准确性。NAS 还选择了优化的激活函数和高效的深度可分离卷积，与早期的 MobileNet 版本相比，FLOPs 减少了 <semantics><mrow><mn>5</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">5\times</annotation></semantics>。

FBNet 是另一个由 NAS 生成的模型，专门针对移动 CPU 上的实时推理进行了优化。与为通用加速设计的架构不同，FBNet 的搜索过程在训练期间明确考虑了延迟约束，确保最终模型在低功耗硬件上高效运行。类似的方法也用于 TPU 优化的 NAS 模型，其中搜索过程由硬件感知的成本函数引导，以最大化并行执行效率。

NAS 也已被应用于卷积网络之外。NAS-BERT 探索基于 Transformer 的架构，寻找在减少计算和内存开销的同时保持强大自然语言理解能力的有效模型结构。NAS 在设计高效的视觉 Transformer（ViTs）方面特别有用，通过自动发现针对边缘 AI 应用量身定制的轻量级注意力机制。

这些由 NAS 生成的每个模型都展示了自动化架构搜索如何揭示人类设计师可能不会立即直观的新的效率权衡。将效率约束显式编码到搜索过程中，使 NAS 能够系统地产生比手动设计的更计算高效、内存友好和硬件适应的架构 (Radosavovic 等人 2020)。

模型表示优化已经带来了实质性的改进。通过结构剪枝和知识蒸馏，我们将 440MB 的 BERT-Base 模型（Devlin 等人 2018b）转换成了 110MB 的变体，内存占用减少了 75%，只损失了 0.8%的准确度。剪枝模型消除了 40%的注意力头和中间维度，显著减少了参数数量。这一成功引发了一个自然的问题：在模型结构优化后，为什么移动部署仍然无法达到我们的 50ms 延迟目标，持续运行在 120ms？

分析揭示了答案。虽然我们消除了 75%的参数，但每个剩余的矩阵乘法仍然使用 32 位浮点运算（FP32）。剩余的 2750 万个参数消耗了过多的内存带宽：从 DRAM 加载权重到计算单元主导了执行时间。模型结构已优化，但数值表示未优化。每个参数占用 4 字节，有限的移动内存带宽（25-35 GB/s 与服务器 GPU 上的 900 GB/s 相比）创建了一个瓶颈，仅结构优化无法解决。

这说明了为什么模型表示优化只代表全面效率策略的第一个维度。表示技术修改了执行的计算（哪些操作、哪些参数、哪些层执行）。数值精度优化，第二个维度，通过降低权重、激活和算术运算的数值保真度来改变这些计算的执行方式。从 32 位到 8 位表示的转换将内存流量减少了 4 倍，并使移动处理器上执行速度比浮点等效值快 4-8 倍的专用整数算术单元成为可能。

这些精度优化与表示优化协同工作。剪枝的 110MB BERT 模型，当进一步量化到 INT8 精度时，缩小到 28MB，推理延迟降低到 45ms，最终达到了部署目标。量化提供了缺失的部分：结构效率（更少的参数）与数值效率（每个参数的更低精度）相结合，带来了单一技术无法实现的复合效益。

## 量化与精度优化

***量化***是一种模型压缩技术，通过将权重和激活的数值精度从浮点数降低到低比特表示，以最小的精度损失减少了*模型大小*和*计算成本*。

当模型表示优化确定执行的计算时，这些计算的效率关键取决于数值精度——我们优化框架的第二个维度。

数值精度决定了权重和激活在计算过程中的表示方式，直接影响内存使用、计算效率和功耗。许多最先进的模型使用高精度浮点格式，如 FP32（32 位浮点），这提供了数值稳定性和高精度（S. Gupta 等人 2015），但增加了存储需求、内存带宽使用和功耗。现代 AI 加速器包括用于低精度计算的专用硬件，允许 FP16 和 INT8 操作以比 FP32 显著更高的吞吐量运行（Y. E. Wang, Wei, 和 Brooks 2019）。降低精度会引入量化误差，这可能会降低精度，其容忍度取决于模型架构、数据集属性和硬件支持。

精度降低与系统性能之间的关系比硬件规格所暗示的要复杂。虽然激进的精度降低（例如，INT8）可以带来令人印象深刻的芯片级性能提升（通常比 FP32 高 4 倍 TOPS），但这些微基准测试可能不会转化为端到端系统的益处。超低精度训练通常需要更长的收敛时间、复杂的混合精度编排和复杂的精度恢复技术，这些技术可能会抵消硬件加速。在数值格式之间进行精度转换会引入计算开销和内存带宽压力，而芯片级基准测试通常忽略这些因素。平衡的方法，如 FP16 混合精度训练，通常在硬件效率和训练收敛之间提供最佳折衷，避免了更激进的量化策略带来的系统级复杂性。

本节探讨了跨越三个复杂级别的精度优化技术：用于快速部署的培训后量化、用于生产系统的量化感知训练，以及针对资源受限环境的极端量化（二值化和三值化）。我们探讨了精度格式之间的权衡、硬件-软件协同设计考虑因素，以及最小化精度下降同时最大化效率提升的方法。

### 精度和能源

高效的数值表示可以显著减少存储需求、计算延迟和功耗，对移动 AI、嵌入式系统和云推理特别有益。精度级别可以根据特定硬件能力进行调整，以在 AI 加速器（如 GPU、TPU、NPU 和边缘 AI 芯片）上最大化吞吐量。

#### 能源成本

除了计算和内存的好处之外，不同数值精度相关的能耗进一步突显了降低精度的益处。如图图 10.13 所示，执行 32 位浮点数加法（FAdd）大约消耗 0.9 pJ，而 16 位浮点数加法只需要 0.4 pJ。同样，32 位整数加法需要 0.1 pJ，而 8 位整数加法的能耗显著降低，仅为 0.03 pJ。当考虑到在数十亿操作中运行的大型模型时，这些节省会累积起来，支持第十八章中概述的可持续性目标。通过量化获得的能效也增强了第十五章中讨论的安全态势，通过减少潜在攻击者可用的计算资源。

![图片](img/file159.svg)

图 10.13：**能耗**：降低精度减少了计算能耗，说明了模型精度之间的权衡。机器学习系统可以通过将浮点运算从 32 位降低到 16 位甚至更低位来优化效率，从而实现显著的节省。来源：IEEE spectrum.

除了直接的计算节省之外，降低数值精度对内存能耗的影响也很大，这通常占用了整个系统的功耗。低精度表示减少了数据存储需求和内存带宽使用，导致更少且更有效的内存访问。这很重要，因为访问内存，尤其是片外 DRAM，比执行算术运算消耗的能量要大得多。例如，DRAM 访问需要比缓存访问高几个数量级的能量（例如，8 KB L1 缓存访问需要 10 pJ），而缓存访问只需要 10 pJ。指令能耗的分解突显了在内存层次结构中移动数据所付出的代价，其中指令的总能耗可能会受到内存访问模式的影响 31。

通过降低数值精度，模型不仅可以更高效地执行计算，还可以减少数据移动，从而降低整体能耗。这对于硬件加速器和边缘设备尤为重要，在这些设备中，内存带宽和能效是关键约束。

#### 性能提升

图 10.14 使用双轴条形图展示了量化对推理时间和模型大小的影响。每个类别中的左侧条形显示了从 FP32 转换为 INT8 时的推理时间改进，而右侧条形则描述了相应的模型大小减少。结果表明，量化模型可以实现高达<semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">4\times</annotation></semantics>的更快推理速度，同时将存储需求减少一个<semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">4\times</annotation></semantics>的因子，这使得它们在资源受限的环境中高度适用。

![图片](img/file160.svg)

图 10.14：**量化影响**：从 FP32 转换为 INT8 可以将推理时间减少高达 4 倍，同时将模型大小减少一个 4 倍的因子，这使得模型在资源受限的环境中更加高效。

然而，降低数值精度会带来权衡。较低的精度格式可能导致数值不稳定和量化噪声，可能影响模型精度。一些架构，如大型基于 Transformer 的自然语言处理模型，对量化有很好的容忍度，而其他架构可能会经历显著的退化。因此，选择适当的数值精度需要平衡精度约束、硬件支持和效率提升。

![图片](img/file161.png)

图 10.15：按 p(x)加权的量化误差。

图 10.15 展示了按值概率分布加权的量化误差，比较了不同的数值格式（FP8 变体和 INT8）。误差分布突出了不同格式如何在值范围内引入不同水平的量化噪声，这反过来又影响模型的精度和稳定性。

### 数值编码和存储

在机器学习系统中，数值数据的表示不仅限于精度水平，还包括编码格式和存储机制，这两者都显著影响计算效率。数值值的编码决定了浮点数和整数表示如何在内存中存储并由硬件处理，这直接影响到机器学习工作负载的性能。随着机器学习模型的大小和复杂性增加，优化数值编码对于确保效率变得越来越重要，尤其是在专用硬件加速器上（Mellempudi 等人 2019）。

在机器学习中广泛使用的浮点数表示遵循[IEEE 754 标准](https://standards.ieee.org/standard/754-2019.html)，该标准定义了如何使用符号、指数和尾数（分数）比特的组合来表示数字。标准格式如 FP32（单精度）和 FP64（双精度）提供了高精度，但需要大量的内存和计算资源。为了提高效率，引入了低精度格式如 FP16、[bfloat16](https://cloud.google.com/tpu/docs/bfloat16)和[FP8](https://arxiv.org/abs/2209.05433)，它们提供了较低的存储需求，同时保持了机器学习计算所需的足够数值范围。与 FP16 不同，bfloat16 保留了与 FP32 相同的指数大小，允许它表示更宽的动态范围，同时在分数上降低精度。这一特性使得 bfloat16 特别适用于机器学习训练，在保持动态范围对于稳定的梯度更新很重要的情况下。

基于整数的表示，包括 INT8 和 INT4，通过消除指数和尾数编码的需要，进一步减少了存储和计算开销。这些格式在量化推理中常用，其中模型权重和激活被转换为离散的整数值，以加速计算并降低功耗。整数算术的确定性简化了硬件上的执行，使其特别适合边缘 AI 和移动设备。在极端情况下，二进制和三进制表示将值限制在仅一个或两个比特，从而显著减少了内存占用和功耗。然而，除非辅以专门的训练技术或架构调整，否则这种激进的量化可能会降低模型精度。

新兴的数值格式试图在效率和精度之间取得平衡。[TF32](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)，由 NVIDIA 为 Ampere GPU 引入，通过减少尾数大小而保持指数宽度，允许以最小的精度损失进行更快的计算。类似地，FP8，在 AI 加速器中得到采用，提供了一个更低精度的浮点数替代方案，同时保留了一个与机器学习工作负载很好地对齐的结构(Micikevicius 等人 2022)。其他如[Posit](https://ieeexplore.ieee.org/document/9399648)、[Flexpoint](https://arxiv.org/abs/1711.02213)和[BF16ALT](https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/BFMLALB--BFMLALT--vector---BFloat16-floating-point-widening-multiply-add-long--vector--)等替代格式也在探索其潜在的数值稳定性和硬件适应性优势。

数字编码的效率还受到数据在内存中存储和访问方式的影响。AI 加速器通过优化内存层次结构来最大化减少精度格式的优势，使用专门的硬件，如张量核心、矩阵乘法单元（MMUs）和向量处理引擎来加速低精度计算。在这些平台上，数据对齐、内存分块和压缩技术在确保减少精度计算带来实际性能提升方面发挥着重要作用。

随着机器学习系统的演变，数值编码和存储策略将继续适应以满足大规模模型和多样化硬件环境的需求。针对 AI 工作负载定制的精度格式的持续开发突出了与底层硬件能力协同设计数值表示的重要性，确保机器学习模型在最小化计算成本的同时实现最佳性能。

### 数值格式比较

表 10.7 比较了机器学习中常用的数值精度格式，突出了它们在存储效率、计算速度和能耗方面的权衡。FP8 和 TF32 等新兴格式已被引入以进一步优化性能，尤其是在 AI 加速器上。

表 10.7：数值精度格式比较。

| **精度格式** | **位宽** | **存储减少（与 FP32 相比）** | **计算速度（与 FP32 相比）** | **功耗** | **用例** |
| --- | --- | --- | --- | --- | --- |
| --- | --- | --- | --- | --- | --- |
| **FP32 (单精度浮点)** | 32 位 | 基准（1 倍） | 基准（1 倍） | 较高 | 训练与推理（通用） |
| **FP16 (半精度浮点)** | 16 位 | 小 2 倍 | 在 FP16 优化硬件上快 2 倍 | 较低 | 加速训练，推理（NVIDIA 张量核心，TPUs） |
| **bfloat16 (脑浮点)** | 16 位 | 小 2 倍 | 与 FP16 速度相似，动态范围更好 | 较低 | 在 TPUs 上训练，基于转换器的模型 |
| **TF32 (TensorFloat-32)** | 19 位 | 类似于 FP16 | 在 NVIDIA Ampere GPU 上最高可快 8 倍 | 较低 | 在 NVIDIA GPU 上训练 |
| **FP8 (浮点 8 位)** | 8 位 | 小 4 倍 | 在某些情况下比 INT8 快 | 显著较低 | 高效训练/推理（H100，AI 加速器） |
| **INT8 (8 位整数)** | 8 位 | 小 4 倍 | 比 FP32 快 4-8 倍 | 显著较低 | 量化推理（边缘 AI，移动 AI，NPUs） |
| **INT4 (4 位整数)** | 4 位 | 小 8 倍 | 依赖于硬件 | 极低 | 超低功耗 AI，实验性量化 |
| **二进制/三进制（1 位/2 位）** | 1-2 位 | 16-32 倍更小 | 极度依赖于硬件 | 最低 | 极端效率（二进制/三进制神经网络） |

FP16 和 bfloat16 格式在保持模型精度的同时提供了适度的效率提升。许多 AI 加速器，如 NVIDIA Tensor Cores 和 TPUs，都包括对 FP16 计算的专用支持，这使得与 FP32 相比，矩阵运算速度提高了<semantics><mrow><mn>2</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">2\times</annotation></semantics>。特别是 BFloat16，它保留了与 FP32 相同的 8 位指数，但具有减少的 7 位尾数，使其能够保持类似的动态范围(~<semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>38</mn></mrow></msup><annotation encoding="application/x-tex">10^{-38}</annotation></semantics>到<semantics><msup><mn>10</mn><mn>38</mn></msup><annotation encoding="application/x-tex">10^{38}</annotation></semantics>)，同时牺牲了精度。相比之下，FP16 具有 5 位指数和 10 位尾数，其动态范围显著减小(~<semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>5</mn></mrow></msup><annotation encoding="application/x-tex">10^{-5}</annotation></semantics>到<semantics><msup><mn>10</mn><mn>5</mn></msup><annotation encoding="application/x-tex">10⁵</annotation></semantics>)，使其更适合推理而不是训练。由于 BFloat16 保留了 FP32 的指数大小，它更好地处理训练过程中遇到的极端值，而 FP16 可能难以处理下溢或上溢。这使得 BFloat16 成为需要广泛动态范围的深度学习工作负载的更稳健的替代方案。

图 10.16 突出了这些差异，展示了位宽分配如何影响精度和数值范围之间的权衡。

![](img/file162.svg)

图 10.16：**浮点精度**：FP16 和 bfloat16 等低精度格式以计算效率和内存节省为代价，牺牲了数值范围。BFloat16 保持了 FP32 的指数大小，保留了其动态范围和训练适用性，而 FP16 较小的指数限制了其在推理或仔细缩放的训练场景中的应用。

INT8 精度提供了更激进的效率提升，尤其是在推理工作负载中。许多量化模型使用 INT8 进行推理，通过<semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">4\times</annotation></semantics>减少存储，并在优化硬件上加速计算 4–8<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>。INT8 在移动和嵌入式 AI 中得到了广泛应用，在这些领域中能源限制是显著的。

二进制和三进制网络代表了量化的极端，其中权重和激活被限制为 1 位（二进制）或 2 位（三进制）值。这导致了巨大的存储和能源节省，但除非使用专门的架构，否则模型精度通常会显著下降。

### 精度降低的权衡

在机器学习系统中降低数值精度可以显著提高效率，包括降低内存需求、减少功耗和增加计算吞吐量。然而，这些好处伴随着权衡，因为低精度表示引入了数值误差和量化噪声，这可能会影响模型精度。这种影响程度取决于多个因素，包括模型架构、数据集和使用的特定精度格式。

模型对量化的容忍度各不相同。大型架构，如卷积神经网络和基于转换器的模型，即使在使用 bfloat16 或 INT8 等低精度格式的情况下，通常也能保持高精度。相比之下，小型模型或训练需要高数值精度的任务可能会经历性能的更大下降。神经网络中的所有层对精度降低的反应并不相同。某些层，如批归一化和注意力机制，可能比标准前馈层对数值精度更敏感。因此，混合精度训练等技术，其中不同层以不同精度级别运行，可以帮助在优化计算效率的同时保持精度。

硬件支持是决定精度降低有效性的另一个重要因素。人工智能加速器，包括 GPU、TPU 和 NPU，都配备了专门的低精度算术单元，这些单元能够使用 FP16、bfloat16、INT8，以及最近出现的 FP8 进行高效计算。这些架构利用降低的精度来执行高吞吐量矩阵运算，提高速度和能源效率。相比之下，通用 CPU 通常缺乏用于低精度计算的专用硬件，限制了数值量化潜在效益的实现。引入新的浮点格式，如 NVIDIA GPU 的 TF32 和人工智能加速器的 FP8，旨在优化精度和效率之间的权衡，为未明确设计用于极端量化的硬件提供替代方案。

除了硬件限制之外，降低数值精度还会影响功耗。低精度算术减少了所需的内存访问次数并简化了计算操作，从而降低了整体能耗。这对于移动设备和边缘人工智能系统等能源受限的环境尤其有利。在极端情况下，包括 INT4 和二进制/三进制表示在内的超低精度格式，在功耗和内存使用方面提供了显著的降低。然而，这些格式通常需要专门的架构来补偿与这种激进量化相关的精度损失。

为了减轻与降低精度相关的精度损失，可以采用各种量化策略。最终，为给定的机器学习模型选择适当的数值精度需要在效率提升和精度约束之间进行平衡。这个选择取决于模型的架构、目标应用的计算需求以及底层硬件对低精度操作的支持。通过利用硬件和软件优化技术的进步，从业者可以有效地将低精度数值集成到机器学习管道中，在保持性能的同时最大化效率。

### 精度降低策略

降低数值精度是提高机器学习模型效率的重要优化技术。通过降低权重和激活的位宽，模型可以减少内存占用，提高计算吞吐量，并降低功耗。然而，简单的量化可能会引入量化误差，导致精度下降。为了解决这个问题，已经开发出不同的精度降低策略，允许模型在保持预测性能的同时平衡效率提升。

量化技术可以在模型生命周期的不同阶段应用。训练后量化在训练后降低精度，使其成为优化推理的简单且低成本方法。量化感知训练将量化效果纳入训练过程，使模型能够适应较低的精度并保持较高的精度。混合精度训练利用硬件支持动态地为不同的计算分配精度级别，优化执行效率而不牺牲精度。

为了帮助导航日益增加的复杂性，图 10.17 将量化技术根据实现复杂性、资源需求和目标用例分为三个渐进层次。

![图 10.17](img/file163.svg)

图 10.17：**量化复杂性路线图**：量化技术的三个渐进层次，从适合快速部署的基础方法到针对极端资源限制的研究前沿方法，反映了不断增加的实现努力、资源需求和潜在的精度权衡。

#### 训练后量化

量化是在解决内存墙问题时实现显著内存带宽降低的特定算法技术。这些量化方法在不同平台上提供标准化的 API，明确展示了如何实现之前建立的效率原则。

训练后量化（PTQ）在训练后降低数值精度，将权重和激活从高精度格式（FP32）转换为低精度表示（INT8 或 FP16）而无需重新训练 (Jacob et al. 2018b)。这实现了更小的模型尺寸、更快的计算和更低的能耗，使其适用于资源受限的环境，如移动设备、边缘 AI 系统和云推理平台 (H. Wu et al. 2020)。

PTQ 的主要优势是计算成本低——无需重新训练或访问训练数据。然而，降低精度会引入量化误差，这可能会降低准确性，尤其是对于需要精细数值精度的任务。机器学习框架（TensorFlow Lite、ONNX Runtime、PyTorch）提供了内置的 PTQ 支持。

##### PTQ 功能

PTQ 将训练模型的权重和激活从高精度浮点表示（例如，FP32）转换为低精度格式（例如，INT8 或 FP16）。这个过程减少了模型的内存占用，加速了推理，并降低了功耗。然而，由于低精度格式具有较小的数值范围，量化会引入舍入误差，这可能会影响模型准确性。

PTQ 的核心机制是将高精度值缩放并映射到较小的数值范围。一种广泛使用的方法是均匀量化，它使用一致的缩放因子将浮点值映射到离散整数级别。在均匀量化中，每个量化值之间的间隔是恒定的，这简化了实现并确保了在硬件上的高效执行。量化值 <semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics> 的计算如下：<semantics><mrow><mi>q</mi><mo>=</mo><mtext mathvariant="normal">round</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">q = \text{round} \left(\frac{x}{s} \right)</annotation></semantics> 其中：

+   <semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics> 是量化整数表示，

+   <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics> 是原始浮点值，

+   <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics> 是一个缩放因子，它将浮点范围映射到可用的整数范围。

列表 10.2 展示了从 FP32 到 INT8 的均匀量化。

列表 10.2：**均匀量化**：将 FP32 权重转换为 INT8 格式，实现 4 倍内存减少的同时测量量化误差。

```py
import torch

# Original FP32 weights
weights_fp32 = torch.tensor(
    [0.127, -0.084, 0.392, -0.203], dtype=torch.float32
)
print(f"Original FP32: {weights_fp32}")
print(f"Memory per weight: 32 bits")

# Simple uniform quantization to INT8 (-128 to 127)
# Step 1: Find scale factor
max_val = weights_fp32.abs().max()
scale = max_val / 127  # 127 is max positive INT8 value

# Step 2: Quantize using our formula q = round(x/s)
weights_int8 = torch.round(weights_fp32 / scale).to(torch.int8)
print(f"Quantized INT8: {weights_int8}")
print(f"Memory per weight: 8 bits (reduced from 32)")

# Step 3: Dequantize to verify
weights_dequantized = weights_int8.float() * scale
print(f"Dequantized: {weights_dequantized}")
print(
    f"Quantization error: "
    f"{(weights_fp32 - weights_dequantized).abs().mean():.6f}"
)
```

此示例演示了从 32 位到每个权重 8 位的压缩，量化误差最小。

例如，在 INT8 量化中，模型的浮点值（通常范围在<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mi>r</mi><mo>,</mo><mi>r</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[-r, r]</annotation></semantics>之间）被映射到一个整数范围<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mn>128</mn><mo>,</mo><mn>127</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[-128, 127]</annotation></semantics>。缩放因子确保在减少精度损失的同时保留了最重要的信息。一旦模型被量化，推理就使用整数算术进行，这在许多硬件平台上比浮点运算效率高得多（Gholami 等人 2021）。然而，由于舍入误差和数值逼近，量化模型与全精度模型相比可能会出现轻微的精度下降。

一旦模型被量化，推理就使用整数算术进行，这在许多硬件平台上比浮点运算效率高得多。然而，由于舍入误差和数值逼近，量化模型与全精度模型相比可能会出现轻微的精度下降。

除了均匀量化之外，还可以采用非均匀量化来在特定场景中保留精度。与使用一致缩放因子的均匀量化不同，非均匀量化将更细粒度的精度分配给数值范围，这些范围更密集。这种方法对于权重分布集中在某些值的模型来说是有益的，因为它允许在最重要的地方保留更多细节。然而，非均匀量化通常需要更复杂的校准，并可能涉及额外的计算开销。尽管在生产环境中非均匀量化不如均匀量化常用，但对于特别敏感于精度变化的模型，非均匀技术可以有效地保留精度。

PTQ 对于计算机视觉模型特别有效，其中 CNN 通常可以很好地容忍量化。然而，依赖于微小数值差异的模型，如 NLP 转换器或语音识别模型，可能需要额外的调整或替代量化技术，包括非均匀策略，以保留性能。

##### 校准

PTQ 的一个重要方面是校准步骤，它涉及选择对量化模型权重和激活最有效的裁剪范围 [<semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>, <semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>]。在 PTQ 过程中，模型的权重和激活被转换为低精度格式（例如，INT8），但这种减少的有效性在很大程度上取决于所选的量化范围。如果没有适当的校准，量化过程可能会引起显著的精度下降，即使整体精度有所降低。校准确保所选范围最小化信息损失，并有助于在精度降低后保持模型性能。

图 10.18 展示了训练后量化的整体工作流程。该过程从预训练模型开始，作为优化的起点。为了确定有效的量化范围，一个校准数据集，即训练或验证数据的一个代表性子集，被传递到模型中。这一步骤允许校准过程估计激活和权重的数值分布，然后用于定义量化的裁剪范围。校准之后，量化步骤将模型参数转换为低精度格式，生成最终的量化模型，在内存和计算效率方面更为高效。

![](img/file164.svg)

图 10.18：**训练后量化**：使用代表性数据集进行校准，确定模型权重和激活的最佳量化范围，最小化量化过程中的信息损失，以创建高效、低精度的模型。此过程将预训练模型转换为适合在资源受限设备上部署的量化版本。

例如，考虑将原本在-6 到 6 之间的浮点范围激活量化为 8 位整数。简单地使用-128 到 127 的完整整数范围进行量化可能不是最有效的方法。相反，校准涉及通过模型传递一个代表性数据集，并观察激活的实际范围。然后可以使用观察到的范围来设置一个更有效的量化范围，从而减少信息损失。

###### 校准方法

常用的校准方法有几种：

+   **最大值**：此方法使用校准期间观察到的最大绝对值作为裁剪范围。虽然简单，但容易受到异常数据的影响。例如，在图 10.19 所示的激活分布中，我们看到 2.1 附近的异常值簇，而其余的值则聚集在较小的值周围。如果异常值显著影响量化，最大值方法可能导致不高效的范围。

+   **熵**：此方法通过使用 KL 散度，最小化原始浮点值与量化格式所能表示的值之间的信息损失。这是 TensorRT 使用的默认校准方法，当试图保留原始值的分布时效果良好。

+   **百分位数**：此方法将裁剪范围设置为校准期间观察到的绝对值分布的百分位数。例如，99%的校准将裁剪最大值中最大的 1%。此方法有助于避免异常值的影响，因为异常值不代表一般数据分布。

![](img/file165.png)

图 10.19：**激活分布**：Resnet50 层的激活表现出长尾分布，其中一小部分值显著大于大多数值；这种分布影响量化范围的选择，因为如果不小心处理，异常值可能导致精度使用效率低下。来源：(H. Wu 等人 2020)。

校准的质量直接影响量化模型的性能。不良的校准可能导致模型精度损失严重，而良好的校准模型在量化后仍能保留大部分原始性能。需要考虑两种类型的校准范围：

+   **对称校准**：裁剪范围围绕零对称，意味着正负范围具有相同的缩放。

+   **非对称校准**：裁剪范围不是对称的，这意味着正负范围可能具有不同的缩放因子。当数据不是围绕零中心时，这可能很有用。

选择正确的校准方法和范围对于在提高效率的同时保持模型精度非常重要。

###### 校准范围

在训练后量化中的一个关键挑战是选择合适的校准范围<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>α</mi><mo>,</mo><mi>β</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[\alpha, \beta]</annotation></semantics>，以将浮点值映射到低精度表示。这个范围的选择直接影响量化误差，从而影响量化模型的精度。如图图 10.20 所示，有两种主要的校准策略：对称校准和非对称校准。

![图片](img/file166.svg)

图 10.20：**校准范围选择**：对称校准使用围绕零的固定范围，而非对称校准则根据数据分布调整范围，可能最小化量化误差并保持模型精度。选择合适的校准策略需要在精度与异常值饱和风险之间取得平衡。

在图 10.20 的左侧，展示了对称校准，其中裁剪范围以零为中心。范围从<semantics><mrow><mi>α</mi><mo>=</mo><mi>−</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha = -1</annotation></semantics>扩展到<semantics><mrow><mi>β</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\beta = 1</annotation></semantics>，将这些值映射到整数范围<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mn>127</mn><mo>,</mo><mn>127</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[-127, 127]</annotation></semantics>。这种方法确保了正负值被同等对待，保留了以零为中心的分布。对称校准的一个关键优势是其简化了实现，因为相同的比例因子应用于正负值。然而，这种方法可能不适合激活分布偏斜的数据集，导致数据的重要部分表示不佳。

在右侧，展示了非对称校准，其中 <semantics><mrow><mi>α</mi><mo>=</mo><mi>−</mi><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\alpha = -0.5</annotation></semantics> 和 <semantics><mrow><mi>β</mi><mo>=</mo><mn>1.5</mn></mrow><annotation encoding="application/x-tex">\beta = 1.5</annotation></semantics>。在这里，零被映射到一个偏移的量化值 <semantics><mrow><mi>−</mi><mi>Z</mi></mrow><annotation encoding="application/x-tex">-Z</annotation></semantics>，范围非对称地扩展。在这种情况下，量化尺度被调整以考虑非零均值分布。非对称校准在激活或权重表现出偏斜时特别有用，确保了整个量化范围的充分利用。然而，它引入了确定最佳偏移和缩放因子的额外计算复杂性。

这些校准策略之间的选择取决于模型和数据集的特征：

+   当权重分布围绕零中心时，对称校准通常被使用，这对于良好初始化的机器学习模型来说是常见情况。它简化了计算和硬件实现，但可能不是所有场景的最佳选择。

+   当数据分布偏斜时，非对称校准是有用的，确保了整个量化范围的充分利用。它可以提高准确性保留，但可能在确定最佳量化参数时引入额外的计算复杂性。

许多机器学习框架，包括 TensorRT 和 PyTorch，都支持这两种校准模式，使从业者能够经验性地评估最佳方法。选择合适的校准范围对于 PTQ 很重要，因为它直接影响数值精度和效率之间的权衡，最终影响量化模型的性能。

##### 粒度

在确定截断范围之后，优化量化的下一步涉及调整截断范围的粒度，以确保模型尽可能保留准确性。例如，在卷积神经网络（CNNs）中，某一层的输入激活会与多个卷积滤波器进行卷积，每个滤波器可能具有独特的值范围。因此，量化过程必须考虑到这些滤波器之间范围的不同，以保持模型性能。

如图 10.21 所示，滤波器 1 的范围显著小于滤波器 3 的范围，这展示了不同滤波器之间值的大小的变化。确定权重剪裁范围[<semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>, <semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>]的精度成为有效量化的重要因素。这种范围的可变性是采用基于粒度的不同量化策略的关键原因。

![图片](img/file167.svg)

图 10.21：**量化范围变化**：不同的卷积滤波器表现出独特的激活范围，需要针对每个滤波器进行量化以最小化量化过程中的精度损失。调整剪裁范围的粒度——如每个滤波器不同的刻度所示——优化了模型大小和性能之间的权衡。来源：(Gholami 等人 2021)。

确定量化粒度的几种常用方法各有其权衡，包括精度、效率和计算成本。

###### 层量化

在这种方法中，剪裁范围是通过考虑层中卷积滤波器的所有权重来确定的。相同的剪裁范围应用于层内的所有滤波器。虽然这种方法易于实现，但由于不同滤波器之间值范围的广泛性，它通常会导致次优精度。例如，如果同一层中的一个卷积核的值范围比另一个窄，则较窄范围的量化分辨率可能会受损，从而导致信息丢失。

###### 分组量化

分组量化将卷积滤波器分为组，并为每个组计算一个共享的剪裁范围。当层内值的分布高度可变时，这种方法可能有益。例如，Q-BERT 模型(Shen 等人 2019)在量化 Transformer 模型(Vaswani 等人 2017)时应用了这项技术，特别是对于全连接注意力层。虽然分组量化比层量化提供了更高的精度，但它由于需要考虑多个缩放因子而增加了额外的计算成本。

###### 通道量化

通道量化为每个卷积滤波器分配一个专门的裁剪范围和缩放因子。这种方法确保了量化具有更高的分辨率，因为每个通道都是独立量化的。通道量化在实践中被广泛使用，因为它通常比以前的方法提供更好的精度。通过允许每个滤波器都有自己的裁剪范围，这种方法确保了量化过程针对每个滤波器的特定特征进行了定制。

###### 子通道量化

子通道量化将每个卷积滤波器细分为更小的组，每组都有自己的裁剪范围。尽管它提供了对量化的非常精细的控制，但它引入了显著的计算开销，因为必须在滤波器内的每个组中管理多个缩放因子。因此，尽管计算成本增加，子通道量化通常仅在需要最大精度的场景中使用。

在这些方法中，通道量化是量化卷积滤波器的当前标准。它在更细粒度带来的精度提升和实际部署所需的计算效率之间取得了平衡。调整每个单独内核的裁剪范围可以显著提高模型精度，同时开销最小，这使得它成为机器学习应用中最广泛采用的方法。

##### 权重与激活

权重量化涉及将模型的连续、高精度权重转换为低精度值，例如将 32 位浮点（Float32）权重转换为 8 位整数（INT8）权重。如图图 10.22 所示，权重量化发生在输入乘法的第二步（红色方块）中。这个过程显著减小了模型大小，减少了存储模型所需的内存和推理所需的计算资源。例如，一个具有 Float32 权重的神经网络层的权重矩阵，如<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0.215</mn><mo>,</mo><mi>−</mi><mn>1.432</mn><mo>,</mo><mn>0.902</mn><mo>,</mo><mi>…</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0.215, -1.432, 0.902,\ldots]</annotation></semantics>，可能映射到 INT8 值，如<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>27</mn><mo>,</mo><mi>−</mi><mn>183</mn><mo>,</mo><mn>115</mn><mo>,</mo><mi>…</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[27, -183, 115, \ldots]</annotation></semantics>，从而显著减少内存使用。

![图片](img/file168.svg)

图 10.22：**量化和权重精度**：将权重和激活精度从 float32 降低到 INT8，通过使用更少的位来表示值，可以显著降低推理过程中的模型大小和计算成本，尽管这可能会与模型精度产生权衡。这个过程改变了模型参数和中间结果的数值表示，影响内存使用和处理速度。来源：HarvardX。

激活量化是指在模型推理过程中量化激活值或层输出的过程。这种量化可以减少推理过程中所需的计算资源，尤其是在针对优化整数运算的硬件时。它引入了与保持模型精度相关的问题，因为中间计算的精度降低了。例如，在 CNN 中，由卷积层产生的激活图（或特征图），最初以 Float32 表示，在推理过程中可能被量化为 INT8。这可以在能够高效处理低精度整数的硬件上显著加速计算。

最近的研究探索了用于大型语言模型（LLMs）压缩和加速的激活感知权重量化（AWQ）。这种方法通过观察激活而不是权重本身，仅保护一小部分最显著的权重，大约 1%。这种方法已被证明可以提高模型效率，同时保持精度，如 Ji Lin, Tang, 等人 2023 中所讨论。

##### 静态量化与动态量化

在确定截断范围的类型和粒度之后，从业者必须决定在量化算法中何时计算截断范围。量化激活值主要有两种方法：静态量化和动态量化。

静态量化是更常用的方法。在静态量化中，截断范围是预先计算的，并在推理过程中保持固定。这种方法在运行时不引入任何额外的计算开销，因此在计算资源方面效率很高。然而，固定的范围可能导致与动态量化相比的精度降低。静态量化的典型实现包括运行一系列校准输入来计算激活值的典型范围（Jacob 等人 2018b；Yao 等人 2021）。

与此相反，动态量化在运行时动态计算每个激活图的范围。这种方法允许量化过程根据输入实时调整，可能由于范围是针对每个输入激活专门计算的，因此可能产生更高的准确度。然而，动态量化需要更高的计算开销，因为必须在每个步骤中重新计算范围。尽管这通常会导致更高的准确度，但实时计算可能非常昂贵，尤其是在大规模部署时。

下表 表 10.8 总结了训练后量化、量化感知训练和动态量化的特征，概述了它们各自的优势、局限性和权衡。这些方法在各个规模的机器学习系统中广泛部署，理解它们的优缺点对于选择适合特定应用的方法非常重要。

表 10.8：**量化权衡**：训练后量化、量化感知训练和动态量化代表了模型压缩的不同方法，每种方法都在机器学习系统中平衡准确度、计算成本和实现复杂性。理解这些权衡对于根据应用需求和资源限制选择最佳量化策略非常重要。

| **方面** | **训练后量化** | **量化感知训练** | **动态量化** |
| --- | --- | --- | --- |
| **Pros** |  |  |  |
| **简单性** | ✓ | ✗ | ✗ |
| **准确度保持** | ✗ | ✓ | ✓ |
| **适应性** | ✗ | ✗ | ✓ |
| **优化性能** | ✗ | ✓ | 可能 |
| **Cons** |  |  |  |
| **准确度下降** | ✓ | ✗ | 可能 |
| **计算开销** | ✗ | ✓ | ✓ |
| **实现复杂性** | ✗ | ✓ | ✓ |
| **权衡** |  |  |  |
| **速度 vs. 准确度** | ✓ | ✗ | ✗ |
| **准确度 vs. 成本** | ✗ | ✓ | ✗ |
| **适应性 vs. 开销** | ✗ | ✗ | ✓ |

##### PTQ 优点

PTQ 的一个关键优点是其低计算成本，因为它不需要重新训练模型。这使得它成为快速部署训练模型的吸引人选择，尤其是在重新训练计算成本高昂或不可行的情况下。由于 PTQ 只修改权重和激活的数值表示，因此底层模型架构保持不变，允许它应用于广泛的预训练模型而无需修改。

PTQ 通过减少模型参数的位宽提供了显著的内存和存储节省。例如，将模型从 FP32 转换为 INT8 会导致存储大小减少<semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">4\times</annotation></semantics>，这使得在资源受限的设备上部署更大的模型成为可能，例如手机、边缘 AI 硬件和嵌入式系统。这些内存占用减少也导致在通过网络系统传输模型时带宽需求降低。

在计算效率方面，PTQ 允许使用整数算术进行推理，这在许多硬件平台上比浮点运算更快。AI 加速器，如 TPUs 和神经网络单元（NPUs），针对低精度计算进行了优化，在执行量化模型时能够实现更高的吞吐量和降低功耗。这使得 PTQ 特别适用于需要实时推理的应用，例如自主系统中的目标检测或移动设备上的语音识别。

##### PTQ 的挑战和局限性

尽管 PTQ 具有优势，但在将浮点值映射到离散的低精度表示时，由于舍入效应，它引入了量化误差。虽然一些模型对这些变化具有鲁棒性，但其他模型可能会经历显著的准确度下降，尤其是在依赖于微小数值差异的任务中。

准确度损失的程度取决于模型架构和任务领域。用于图像分类的 CNN 通常对 PTQ 具有容忍性，即使在激进的 INT8 量化下也能保持接近原始的准确度。在 NLP 和语音识别中使用的基于 Transformer 的模型则更敏感，因为这些架构依赖于注意力机制中数值关系的精度。

为了减轻准确度损失，通常会应用如基于 KL 散度的缩放或逐通道量化等校准技术来微调缩放因子并最小化信息损失。一些框架，包括 TensorFlow Lite 和 PyTorch，提供了具有内置校准方法的自动化量化工具，以提高准确度保留。

PTQ 的另一个局限性是并非所有硬件都支持高效的整数算术。虽然 GPU、TPUs 和专门的边缘 AI 芯片通常包括针对 INT8 推理的专用支持，但通用 CPU 可能缺乏用于低精度执行的优化指令，导致性能提升不佳。

PTQ 并不总是适合用于训练目的。由于 PTQ 在训练后应用量化，需要进一步微调或适应的模型可能从替代方法中受益更多，例如量化感知训练（下文将讨论），以确保在学习过程中充分考虑到精度约束。

培训后量化仍然是提高推理效率最实用和最广泛使用的技术之一。它提供了显著的内存和计算节省，同时开销最小，使其成为在资源受限环境中部署机器学习模型的理想选择。然而，PTQ 的成功取决于模型架构、任务敏感性和硬件兼容性。在精度下降不可接受的情况下，可能需要采用替代量化策略，例如量化感知训练。

培训后量化是更高级量化方法的基础。核心概念——量化工作流程、数值格式权衡和校准方法——在所有精度优化技术中始终至关重要。对于生产截止日期在两周以内且可接受的精度损失为 1-2% 的快速部署场景，PTQ 与 min-max 校准通常提供完整的解决方案。需要低于 1% 精度损失的量产系统应考虑量化感知训练，它通过量化模拟的微调来恢复精度，但代价是额外 20-50% 的训练时间。极端限制，如小于 1MB 的模型或小于 10mW 的功耗预算，可能需要 INT4 或二进制量化，接受 5-20% 的精度下降，这需要架构变更。

#### 量化感知训练

QAT 将量化约束直接集成到训练过程中，在正向传播过程中模拟低精度算术，以允许模型适应量化效应（Jacob 等人 2018b）。这种方法对于需要精细数值精度的模型尤为重要，例如在自然语言处理和语音识别系统中使用的变压器（Nagel 等人 2021b）。图 10.23 展示了 QAT 的过程：量化应用于预训练模型，随后进行微调以适应低精度约束。

![图片](img/file169.svg)

图 10.23：**量化感知训练**：使用模拟的低精度算术重新训练预训练模型，以适应降低数值精度的部署，从而减轻精度损失，在资源受限的设备上实现高效的推理。这个过程使模型对量化效应具有鲁棒性，即使在精度较低的情况下也能保持性能。

在许多情况下，QAT 也可以建立在 PTQ 之上（如前节所述），如图 10.24 所示。不是从全精度模型开始，PTQ 首先应用于使用校准数据生成一个初始量化模型。然后，这个量化模型作为 QAT 的起点，使用训练数据进行额外的微调，有助于模型更好地适应低精度约束。这种混合方法结合了 PTQ 的效率与 QAT 的精度保持，减少了仅使用训练后方法通常相关的退化。

![图片](img/file170.svg)

图 10.24：**混合量化方法**：训练后量化（PTQ）生成一个初始量化模型，该模型作为量化感知训练（QAT）的预热启动，与随机初始化网络的量化相比，加速收敛并减轻精度损失。这个两阶段过程利用了 PTQ 的效率，同时使用训练数据来优化低精度约束下的性能。

##### 训练数学

在正向传播过程中，权重和激活被量化和解量化以模拟降低精度。这个过程通常表示为：<semantics><mrow><mi>q</mi><mo>=</mo><mtext mathvariant="normal">round</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><mi>s</mi></mrow> <annotation encoding="application/x-tex">q = \text{round} \left(\frac{x}{s} \right) \times s</annotation></semantics> 其中 <semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics> 表示模拟的量化值，<semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics> 表示全精度权重或激活，而 <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics> 是缩放因子，将浮点值映射到低精度整数。

虽然正向传播使用量化值，但在反向传播中的梯度计算保持全精度。这是通过使用直通估计器（STE）32 实现的，该估计器通过将舍入操作视为具有导数为一的导数来近似量化函数的梯度。这种方法防止了由于量化操作的非可微性质而阻碍梯度，从而允许有效模型训练（Y. Bengio, Léonard, and Courville 2013a）。

在训练过程中整合量化效果使模型能够学习一个最优的权重和激活分布，以最小化数值精度损失的影响。当使用真正的低精度算术（例如，INT8 推理）部署时，所得到的模型与事后量化的模型相比，保持了显著更高的精度(Krishnamoorthi 2018)。

##### QAT 优势

QAT33 的一个主要优势是，即使在低精度推理条件下，也能保持模型精度。在训练过程中引入量化有助于模型补偿精度损失，减少舍入误差和数值不稳定性。这对于在 NLP、语音识别和高分辨率计算机视觉中常用的量化敏感模型来说非常重要(Gholami 等人 2021)。

另一个主要好处是，QAT 允许在硬件加速器上进行低精度推理，而不会显著降低精度。AI 处理器，如 TPUs、NPUs 和专用边缘设备，包括用于整数操作的专用硬件，这使得 INT8 模型比 FP32 模型运行得更快，能耗更低。考虑到量化效果进行训练确保最终模型可以充分利用这些硬件优化(H. Wu 等人 2020)。

##### QAT 挑战与权衡

尽管 QAT 有其好处，但在训练过程中会引入额外的计算开销。每次前向传递时的模拟量化会减慢训练速度，相对于全精度方法。这个过程增加了训练计划的复杂性，使得 QAT 对于可能因额外训练时间而变得不可行的超大规模模型来说不太实用。

QAT 引入了额外的超参数和设计考虑，例如选择合适的量化方案和缩放因子。与 PTQ（在训练后应用量化）不同，QAT 需要仔细调整训练动态，以确保模型能够适当地适应低精度约束(Gong 等人 2019)。

表 10.9 总结了 QAT 与 PTQ 的关键权衡：

表 10.9：**量化权衡**：量化感知训练（QAT）通过将量化纳入训练过程，最小化由降低数值精度引起的精度损失，而训练后量化（PTQ）提供了更快的部署，但可能需要校准以减轻精度退化。与将 PTQ 应用于预训练模型相比，QAT 的重新训练需求增加了训练复杂性。

| **方面** | **QAT (量化感知训练)** | **PTQ (训练后量化)** |
| --- | --- | --- |
| **精度保留** | 最小化量化带来的精度损失 | 可能会遭受精度退化 |
| **推理效率** | 优化用于低精度硬件（例如，TPU 上的 INT8） | 优化但可能需要校准 |
| **训练复杂度** | 需要使用量化约束重新训练 | 不需要重新训练 |
| **训练时间** | 由于正向传播中模拟量化而变慢 | 由于量化是在事后应用而变快 |
| **部署准备** | 适用于对量化误差敏感的模型 | 优化模型以用于推理的最快方式 |

将量化集成到训练过程中比训练后量化更有效地保留了模型准确性，尽管这需要额外的训练资源和时间。

##### PTQ 与 QAT

PTQ 和 QAT 之间的选择取决于精度、计算成本和部署约束之间的权衡。PTQ 提供了一种计算成本低的优化方法，只需要训练后的转换，使其非常适合快速部署。然而，其有效性因架构而异——CNN 对 PTQ 的容忍度较好，而 NLP 和语音模型可能由于依赖精确数值表示而经历退化。

当高精度保留至关重要时，QAT 是必要的。在训练过程中集成量化效果允许模型适应低精度算术，减少量化误差(Jacob et al. 2018c)。虽然实现了更高的低精度准确性，但 QAT 需要额外的训练时间和计算资源。在实践中，一种混合方法从 PTQ 开始，并针对准确性至关重要的模型选择性地应用 QAT，在效率和性能之间提供了最佳平衡。

### 极端量化

除此之外，极端量化技术使用 1 位（二值化）或 2 位（三值化）表示来显著降低内存使用和计算需求(Courbariaux, Bengio, and David 2016)。二值化将权重和激活值限制为两个值（通常是-1 和+1，或 0 和 1），大幅减小模型大小并加速在二进制神经网络等专用硬件上的推理(Rastegari et al. 2016)。然而，这种限制严重限制了模型的表达能力，通常会导致在需要高精度的任务（如图像识别或自然语言处理）上的准确性下降(Hubara et al. 2018)。

三值化通过允许三个值（-1、0、+1）扩展了二值化，提供了额外的灵活性，在纯二值化之上略微提高了精度（Zhu 等人 2017）。零值使得稀疏性更高，同时保持了更强的表示能力。这两种技术都需要梯度近似方法，如直通估计器（STE），以处理训练期间的非可微量化操作（Y. Bengio、Léonard 和 Courville 2013b），QAT 集成有助于减轻精度损失（J. Choi 等人 2018）。

##### 挑战和局限性

尽管为嵌入式系统和移动设备实现了超低功耗的机器学习，但二值化和三值化面临着重大挑战。在这种极端量化下，性能维护变得困难，需要能够高效处理二进制或三值操作的专用硬件（Umuroglu 等人 2017）。传统处理器缺乏对这些计算的优化，需要定制硬件加速器。

精度损失仍然是一个关键问题。这些方法适用于对高精度不是关键的任务，或者 QAT 可以补偿精度约束的任务。尽管存在挑战，但能够在保持可接受精度的同时大幅减少模型大小，这使得它们对边缘 AI 和资源受限环境具有吸引力（Jacob 等人 2018c）。未来在专用硬件和训练技术方面的进步可能会增强它们在高效、可扩展 AI 中的作用。

### 多技术优化策略

在探讨了量化技术（PTQ、QAT、二值化和三值化）、剪枝方法和知识蒸馏之后，我们现在研究如何系统地结合这些互补方法以实现卓越的优化结果。而不是单独应用技术，集成策略利用不同优化维度之间的协同效应，在保持模型精度的同时最大限度地提高效率。

每种优化技术都针对模型效率的不同方面：量化减少了数值精度，剪枝消除了冗余参数，知识蒸馏将能力转移到紧凑架构，而 NAS 优化结构设计。这些技术表现出互补特性，使得强大的组合成为可能。

剪枝和量化产生了协同效应，因为剪枝减少了参数数量，而量化减少了精度，从而产生了乘法压缩效果。首先应用剪枝可以减少参数集，使后续的量化更加有效，并减少最佳量化策略的搜索空间。这种顺序方法可以实现超过任何一种技术单独使用的压缩比率。

知识蒸馏通过减轻激进量化带来的精度损失，有效地与量化结合。这种方法训练学生模型以匹配教师的行为，而不仅仅是最小化任务损失，这在直接量化会导致不可接受的精度退化的极端量化场景中特别有效。

神经架构搜索使协同设计方法成为可能，该方法针对量化约束优化模型结构，识别出在低精度操作下保持精度的架构。这种协同设计方法产生的模型天生适合后续优化，提高了量化和剪枝技术的有效性。

如 图 10.25 所示，不同的压缩策略，如剪枝、量化和奇异值分解（SVD），在模型大小和精度损失之间表现出不同的权衡。虽然剪枝与量化（红色圆圈）结合实现了高压缩比和最小精度损失，但单独量化（黄色方块）也提供了合理的平衡。相比之下，SVD（绿色菱形）需要更大的模型大小来保持精度，说明了不同技术如何影响压缩的有效性。

![图片](img/file171.svg)

图 10.25：**压缩权衡**：结合剪枝和量化在保持最小精度损失的情况下实现了优于单独量化或奇异值分解（SVD）的压缩比率，展示了不同数值精度优化技术对模型大小和性能的影响。如图所示，架构和数值优化可以相互补充，以有效地通过此图部署机器学习模型。来源：(Han, Mao, and Dally 2015a)。

量化与剪枝、知识蒸馏和 NAS 不同，它专门关注减少权重和激活的数值精度。虽然量化本身可以提供显著的计算优势，但其有效性可以通过与剪枝、蒸馏和 NAS 等互补技术相结合而得到增强。这些方法针对模型效率的不同方面，共同工作以创建更紧凑、更快、更节能的模型，在受限环境中实现更好的性能。

我们的最优化之旅仍在继续。我们通过结构化剪枝和知识蒸馏将 BERT-Base 从 440MB 剪枝到 110MB，然后将其量化为 INT8，将模型大小减少到 28MB，在移动硬件上的推理延迟从 120ms 降低到 45ms。这些优化将一个不可用的模型转变为接近部署可行性的模型。然而，分析揭示了一个令人困惑的低效问题：理论上的 FLOP 计数表明推理应在 25ms 内完成，但实际执行却需要 45ms。剩下的 20ms 去哪里了？

详细分析揭示了答案。虽然量化降低了精度，但模型仍然不必要地计算零值。结构化剪枝移除了整个注意力头，但剩余的稀疏权重矩阵以密集格式存储，浪费了内存带宽和零值元素的计算。层归一化操作尽管具有内在的并行性，却按顺序运行。模型对所有标记进行相同的处理，即使简单输入可以从浅层退出。GPU 有 40%的执行时间处于空闲状态，等待内存传输而不是执行操作。

这些观察揭示了为什么模型表示和数值精度优化，虽然必要，但不足以解决问题。表示技术确定要执行的计算。精度技术确定单个操作的执行方式。但它们都没有解决如何组织计算和调度以最大化硬件利用率的问题。这是建筑效率优化的领域，也是我们框架的第三个维度。

建筑效率技术本身改变了执行模式。通过专用内核利用稀疏性消除了对剪枝权重的计算。算子融合将顺序操作（层归一化、注意力、前馈）合并为单个 GPU 内核，通过减少内存流量 40%来提高效率。动态计算使得简单输入在 6 层后即可退出，而不是处理所有 12 层。硬件感知调度并行化操作以保持高 GPU 利用率。将这些技术应用于我们的优化 BERT 模型，将推理时间从 45ms 减少到 22ms，最终实现了 25ms 的理论目标，使部署真正可行。

这个进展说明了为什么所有三个优化维度必须协同工作。模型表示提供结构效率（更少的参数）。数值精度提供计算效率（更低的精度算术）。建筑效率提供执行效率（优化的调度和硬件利用率）。只有当所有维度都系统地解决时，才能出现复合效应，即 440MB/120ms → 28MB/22ms（内存减少 16 倍，延迟提高 5.5 倍）。

## 建筑效率技术

建筑效率优化确保通过将模型操作与处理器能力和内存层次结构对齐，在目标硬件上高效执行计算。与表示优化（确定要执行的计算）和精度优化（确定数值精度）不同，建筑效率解决的是操作如何调度、内存如何访问以及工作负载如何适应输入特性和硬件约束的问题。

这种优化维度在资源受限的场景（第十四章）中尤其重要，在这些场景中，从剪枝和量化中得到的理论 FLOP 减少可能不会在没有架构修改的情况下转化为实际加速。以密集格式存储的稀疏权重矩阵浪费了内存带宽。本可以并行执行的顺序操作未能充分利用 GPU 核心。固定的计算图以相同的方式处理简单和复杂输入，浪费了在非必要工作上的资源。

本节探讨了四种互补的架构效率方法：在模型开发期间积极整合部署约束的硬件感知设计原则、加速剪枝模型计算的稀疏性利用技术、适应输入复杂性的动态计算策略以及通过组合操作减少内存流量的算子融合方法。这些技术将算法优化转化为实际性能提升。

### 硬件感知设计

硬件感知设计将目标平台约束（内存带宽、处理能力、并行性能力和能量预算）直接纳入模型架构决策中。这种方法不是在训练后优化模型，而是确保计算模式、内存访问和操作类型从一开始就与硬件能力相匹配，从而在多种部署平台上最大化效率。

#### 高效设计原则

为硬件效率设计机器学习模型需要构建架构以考虑计算成本、内存使用、推理延迟和功耗，同时保持强大的预测性能。与训练后优化不同，硬件感知模型设计从一开始就积极整合硬件考虑因素。这确保了模型在计算效率上高效，并且能够在各种硬件环境中以最小的适应性进行部署。

在这种积极的方法中，硬件感知设计的一个关键方面是利用特定硬件平台（例如，GPU、TPU、移动或边缘设备）的优势，通过硬件优化的操作来最大化并行性、优化内存层次结构并最小化延迟。如表 10.10 中总结的，硬件感知模型设计可以划分为几个原则，每个原则都针对计算和系统约束的核心方面。

表 10.10：**硬件感知设计原则**：通过按其对计算成本、内存使用和推理延迟的影响对模型设计选择进行分类，可以实现对不同硬件平台和部署场景的结构化优化。该表概述了关键原则——例如最小化数据移动和利用并行性——以及体现这些概念的代表性网络架构。

| **原则** | **目标** | **示例网络** |
| --- | --- | --- |
| **缩放优化** | 调整模型深度、宽度和分辨率以平衡效率和硬件约束。 | EfficientNet、RegNet |
| **计算减少** | 通过利用硬件特定的优化（例如，在移动芯片上使用深度可分离卷积）来最小化冗余操作，以降低计算成本。 | MobileNet、ResNeXt |
| **内存优化** | 通过减少激活和参数存储需求，使用硬件特定的内存层次结构（例如，GPU 中的局部和全局内存）来确保高效的内存使用。 | DenseNet、SqueezeNet |
| **硬件感知设计** | 优化架构以适应特定的硬件约束（例如，低功耗、并行性、高吞吐量）。 | TPU 优化模型、MobileNet |

表 10.10 中的原则协同工作：缩放优化根据可用资源适当地调整模型大小，计算减少通过深度可分离卷积等技术消除冗余操作，内存优化将访问模式与硬件层次结构对齐，而硬件感知设计确保架构决策与平台能力相匹配。这些原则共同作用，使模型在保持部署环境一致性行为的同时，在准确性和效率之间取得平衡。

#### **缩放优化**

缩放模型架构涉及平衡准确性与计算成本，并优化以与目标硬件的能力相匹配。模型的每个组件，无论是深度、宽度还是输入分辨率，都会影响资源消耗。在硬件感知设计中，这些维度不仅应该优化以获得准确性，还应该优化内存使用、处理能力和能耗的效率，尤其是在模型部署在特定的硬件（如 GPU、TPU 或边缘设备）上时。

从硬件感知的角度来看，考虑不同的硬件平台（如 GPU、TPU 或边缘设备）如何与缩放维度交互是很重要的。例如，更深的模型可以捕捉更复杂的表示，但过深的深度可能导致推理延迟增加、训练时间延长和内存消耗增加，这些问题在资源受限的平台上是尤其成问题的。同样，增加模型的宽度以处理更多的并行信息可能对具有高并行的 GPU 和 TPU 有益，但它需要仔细管理内存使用。相比之下，增加输入分辨率可以为图像分类等任务提供更精细的细节，但它会指数级增加计算成本，可能超载硬件内存或在边缘设备上造成能源效率低下。

从数学上讲，卷积模型的总体浮点运算次数（FLOPs）可以近似为：<semantics><mrow><mtext mathvariant="normal">FLOPs</mtext><mo>∝</mo><mi>d</mi><mo>⋅</mo><msup><mi>w</mi><mn>2</mn></msup><mo>⋅</mo><msup><mi>r</mi><mn>2</mn></msup><mo>,</mo></mrow> <annotation encoding="application/x-tex">\text{FLOPs} \propto d \cdot w² \cdot r²,</annotation></semantics> 其中 <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics> 是深度，<semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics> 是宽度，而 <semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics> 是输入分辨率。不考虑硬件限制而增加所有三个维度可能会导致性能不佳，尤其是在计算能力或内存带宽有限的设备上。

为了高效地进行模型扩展，以平衡的方式管理这些参数变得至关重要，确保模型在硬件限制范围内运行，同时最大化性能。这正是复合扩展发挥作用的地方。复合扩展不是独立调整深度、宽度和分辨率，而是通过应用相对于基础模型的固定比率 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo>,</mo><mi>β</mi><mo>,</mo><mi>γ</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\alpha, \beta, \gamma)</annotation></semantics> 来平衡这三个维度：<semantics><mrow><mi>d</mi><mo>=</mo><msup><mi>α</mi><mi>ϕ</mi></msup><msub><mi>d</mi><mn>0</mn></msub><mo>,</mo><mi>w</mi><mo>=</mo><msup><mi>β</mi><mi>ϕ</mi></msup><msub><mi>w</mi><mn>0</mn></msub><mo>,</mo><mi>r</mi><mo>=</mo><msup><mi>γ</mi><mi>ϕ</mi></msup><msub><mi>r</mi><mn>0</mn></msub></mrow> <annotation encoding="application/x-tex">d = \alpha^\phi d_0, \quad w = \beta^\phi w_0, \quad r = \gamma^\phi r_0</annotation></semantics> 在这里，<semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics> 是一个缩放系数，而 <semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>、<semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics> 和 <semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics> 是基于硬件限制和经验数据确定的缩放因子。这种方法确保模型以优化硬件资源使用的方式增长，在提高准确性的同时保持效率。

例如，采用复合缩放的 EfficientNet 展示了如何精心平衡深度、宽度和分辨率，从而实现计算高效且性能高的模型。复合缩放降低了计算成本同时保持精度，使其成为硬件感知模型设计的关键考虑因素。这种方法在将模型部署到 GPU 或 TPU 时尤其有益，在这些设备上可以充分利用并行性，但需要仔细管理内存和功耗，这与第十二章中提到的性能评估方法相联系。

这一原则不仅适用于卷积模型，还适用于其他架构，如 Transformer。调整层数、注意力头或嵌入维度以类似的方式影响计算效率。硬件感知缩放已成为优化跨各种计算约束的模型性能的关键，尤其是在处理大型模型或资源受限的设备时。

#### 计算减少

现代架构利用分解计算将复杂操作分解成更简单的组件，在保持表示能力的同时减少计算开销。标准卷积在所有空间位置和通道上均匀应用滤波器，在资源受限的硬件上造成计算瓶颈。分解技术通过重新结构操作以最小化冗余计算来解决这个问题。

深度可分离卷积，在 MobileNet 中引入，通过将标准卷积分解为两个阶段来体现这种方法：深度卷积（独立地对每个输入通道应用单独的滤波器）和点卷积（1×1 卷积混合通道间的输出）。标准卷积的计算复杂度，对于输入大小为<semantics><mrow><mi>h</mi><mo>×</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">h \times w</annotation></semantics>，<semantics><msub><mi>C</mi><mtext mathvariant="normal">in</mtext></msub><annotation encoding="application/x-tex">C_{\text{in}}</annotation></semantics>输入通道，和<semantics><msub><mi>C</mi><mtext mathvariant="normal">out</mtext></msub><annotation encoding="application/x-tex">C_{\text{out}}</annotation></semantics>输出通道是：<semantics><mrow><mi>𝒪</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>w</mi><msub><mi>C</mi><mtext mathvariant="normal">in</mtext></msub><msub><mi>C</mi><mtext mathvariant="normal">out</mtext></msub><msup><mi>k</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{O}(h w C_{\text{in}} C_{\text{out}} k²)</annotation></semantics> 其中 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics> 是内核大小。深度可分离卷积将此降低到：<semantics><mrow><mi>𝒪</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>w</mi><msub><mi>C</mi><mtext mathvariant="normal">in</mtext></msub><msup><mi>k</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>𝒪</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>w</mi><msub><mi>C</mi><mtext mathvariant="normal">in</mtext></msub><msub><mi>C</mi><mtext mathvariant="normal">out</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{O}(h w C_{\text{in}} k²) + \mathcal{O}(h w C_{\text{in}} C_{\text{out}})</annotation></semantics> 消除了通道混合操作中的 <semantics><msup><mi>k</mi><mn>2</mn></msup><annotation encoding="application/x-tex">k²</annotation></semantics> 因子，实现了 5×-10×的 FLOP 减少。这直接转化为降低移动和边缘设备上的内存带宽需求和改进推理延迟。

补充的分解技术扩展了这些优势。分组卷积（ResNeXt）将特征图划分为独立的组，在合并之前分别处理，保持精度同时减少冗余操作。瓶颈层（ResNet）在昂贵操作之前应用 1×1 卷积以降低特征维度，将计算集中在提供最大价值的地方。结合稀疏性和硬件感知调度，这些技术在 GPU、TPU 和专用边缘处理器上最大化加速器的利用率。

#### 内存优化

内存优化 35 解决了在资源受限设备上，当激活、特征图和参数的内存需求超过硬件容量时出现的性能瓶颈。现代架构采用内存高效策略来减少存储需求，同时保持性能，确保在 GPU、TPU 和边缘 AI 平台上计算的可处理性和能源效率。

一种有效的内存优化技术是特征重用，这是 DenseNet 采用的一种策略。在传统的卷积网络中，每一层通常计算一组新的特征图，增加了模型的内存占用。然而，DenseNet 通过重复使用先前层的特征图并选择性地应用变换来减少冗余激活的需求。这种方法减少了需要存储的总特征图数量，从而在不牺牲准确性的情况下降低了内存需求。在一个具有<semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>层的标准卷积网络中，如果每一层生成<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>个新的特征图，特征图的总数将线性增长：<semantics><mrow><mi>𝒪</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{O}(L k)</annotation></semantics>

与之相反，DenseNet 重复使用早期层的特征图，减少了存储的特征图数量。这提高了参数效率并减少了内存占用，这对于内存资源有限的硬件来说非常重要。

另一种有用的技术是激活检查点 36，这在训练期间特别有益。在典型的神经网络中，反向传播需要存储所有前向激活以进行反向传递。这可能导致显著的内存开销，特别是对于大型模型。激活检查点通过仅存储激活子集并在需要时重新计算剩余的激活来减少内存消耗。

如果一个架构需要存储<semantics><msub><mi>A</mi><mtext mathvariant="normal">total</mtext></msub><annotation encoding="application/x-tex">A_{\text{total}}</annotation></msub></semantics>激活，标准的反向传播方法需要完整存储：<semantics><mrow><mi>𝒪</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mtext mathvariant="normal">total</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{O}(A_{\text{total}})</annotation></semantics>

然而，使用激活检查点时，只有一部分激活被存储，其余的激活在运行时重新计算，从而将存储需求降低到：<semantics><mrow><mi>𝒪</mi><mo minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">(</mo><msqrt><msub><mi>A</mi><mtext mathvariant="normal">total</mtext></msub></msqrt><mo minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">)</mo></mrow> <annotation encoding="application/x-tex">\mathcal{O}\Big(\sqrt{A_{\text{total}}}\Big)</annotation></semantics>

特征重用可以显著减少峰值内存消耗，这对于在内存有限的硬件上训练大型模型特别有用。

参数减少是另一种重要的技术，尤其是对于使用大滤波器的模型。例如，SqueezeNet 使用了一种新颖的架构，它在应用标准卷积之前，先使用<semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics>卷积来减少输入通道的数量。通过首先使用<semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics>卷积来减少通道数，SqueezeNet 显著减小了模型大小，同时没有牺牲模型的表达能力。标准卷积层中的参数数量为：<semantics><mrow><mi>𝒪</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>C</mi><mtext mathvariant="normal">in</mtext></msub><msub><mi>C</mi><mtext mathvariant="normal">out</mtext></msub><msup><mi>k</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{O}(C_{\text{in}} C_{\text{out}} k²)</annotation></semantics>

通过使用<semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics>卷积来减少<semantics><msub><mi>C</mi><mtext mathvariant="normal">in</mtext></msub><annotation encoding="application/x-tex">C_{\text{in}}</annotation></semantics>，SqueezeNet37 减少了参数数量，与 AlexNet 相比，模型大小减少了 50 倍，同时保持了相似的性能。这种方法对于具有严格内存和存储约束的边缘设备尤其有价值。

特征重用、激活检查点和参数减少是硬件感知模型设计的关键组成部分，允许模型适应现代加速器的内存限制，并通过减少内存访问来降低功耗。专门的加速器，如 TPUs 和 GPU，利用内存层次结构、缓存和高带宽内存来有效地处理稀疏或减少内存表示，从而实现快速推理并最小化开销。

### 自适应计算方法

动态计算使模型能够根据输入复杂度调整计算负载，比传统的固定架构方法更有效地分配资源。而传统模型对所有输入都应用统一的处理，无论其复杂度如何——在简单情况下浪费资源，并增加功耗——动态计算允许模型在简单输入时跳过层或操作，而在复杂情况下处理更深的网络。

这种自适应方法优化了计算效率，降低了能耗，最小化了延迟，并保持了预测性能。基于输入复杂度的动态调整对于资源受限的硬件至关重要，尤其是在移动设备、嵌入式系统和自动驾驶汽车中，这些设备对计算效率和实时处理能力要求极高。

#### 动态方案

动态方案使模型能够在输入简单时选择性减少计算，同时保持预测性能，从而节省资源。以下讨论的方法，从早期退出架构开始，说明了如何有效地实施这种自适应策略。

##### 早期退出架构

早期退出架构允许模型在网络中间点进行预测，而不是对每个输入完成完整的正向传播。这种方法对于实时应用和节能推理特别有效，因为它可以根据单个输入的复杂度进行选择性计算（Teerapittayanon, McDanel, 和 Kung 2017）。

早期退出架构的核心机制涉及在网络中嵌入多个退出点。对于可以早期以高置信度分类的简单输入，它们在中间层退出，减少了不必要的计算。相反，更复杂的输入继续通过更深层的处理以确保准确性。

一个著名的例子是 BranchyNet38，它在整个网络中引入了多个退出点。对于每个输入，模型使用置信度阈值评估中间预测。如果在退出点预测的置信度超过预定义的阈值，模型将终止进一步的计算并输出结果。否则，它将继续处理直到最终层（Teerapittayanon, McDanel, 和 Kung 2017）。这种方法在不影响对挑战性输入性能的情况下最小化了推理时间。

另一个例子是多退出视觉 Transformer，它将早期退出扩展到基于 Transformer 的架构。这些模型在各个 Transformer 层使用轻量级分类器，使得在可能的情况下可以早期生成预测（Scardapane, Wang, 和 Panella 2020）。这项技术显著减少了推理时间，同时保持了复杂样本的稳健性能。

早期退出模型对于资源受限的设备，如移动处理器和边缘加速器特别有利。通过动态调整计算工作量，这些架构可以降低功耗和处理延迟，使它们非常适合实时决策 (B. Hu, Zhang, and Fu 2021)。

当部署在硬件加速器如 GPU 和 TPUs 上时，早期退出架构可以通过利用并行性进一步优化。例如，可以同时评估不同的退出路径，从而提高吞吐量同时保持自适应计算的好处 (Yu, Li, and Wang 2023)。这种方法在 图 10.26 中得到说明，其中每个 Transformer 层后面跟着一个分类器和一个基于置信度估计或延迟-精度权衡（LTE）的可选早期退出机制。在每一个阶段，系统可以选择在达到足够置信度时提前退出，或者继续通过更深的层进行处理，从而实现计算资源的动态分配。

![图片](img/file172.svg)

图 10.26：**早期退出架构**：Transformer 层通过分类每一层的输出并在达到足够置信度时启用早期终止来动态调整计算，从而降低资源受限设备的延迟和功耗。这种方法允许并行评估不同的退出路径，提高硬件加速器如 gpus 和 tpus 的吞吐量。来源：(Xin 等人 2021)。

##### 条件计算

条件计算指的是神经网络根据输入决定激活模型哪些部分的能力，从而减少不必要的计算。这种方法在资源受限的环境中非常有用，例如移动设备或实时系统，在这里减少操作数量可以直接转化为降低计算成本、功耗和推理延迟 (E. Bengio 等人 2015)。

与早期退出架构不同，其中通常在达到阈值置信度水平时做出提前退出的决定，条件计算通过根据输入的特征动态选择网络中哪些层、单元或路径应该进行计算来工作。这可以通过门控函数或动态路由等机制实现，这些机制“关闭”对于特定输入不需要的网络部分，使模型能够将计算资源集中在最需要的地方。

条件计算的一个例子是 SkipNet，它使用门控机制在输入被认为足够简单时跳过 CNN 中的层。门控机制使用轻量级分类器来预测是否应该跳过该层。这个预测基于输入，模型根据推理过程中使用的层数进行调整（X. Wang 等人 2018)。如果门控函数确定输入简单，某些层将被绕过，从而实现更快的推理。然而，对于更复杂的输入，模型使用网络的全部深度以达到必要的准确性。

另一个例子是动态路由网络，例如在胶囊网络（CapsNets）中，路由机制会动态选择激活信号在网络中传递的路径。在这些网络中，决策过程涉及根据输入的复杂性选择特定的信息流路径，这可以显著减少所需操作和计算的数量（Sabour, Frosst, 和 Hinton 2017）。这种机制通过使用不同的路由策略引入了适应性，同时提供计算效率并保持预测质量。

这些条件计算策略在计算资源有限的现实世界应用中具有显著优势。例如，在自动驾驶中，系统必须处理各种不同复杂性的输入（例如行人、交通标志、道路车道）。在输入简单的情况下，可以采取更简单、计算需求更低的路径，而在更复杂的情况（如检测障碍物或执行详细场景理解）下，则需要充分利用模型的能力。条件计算确保系统根据输入的实时复杂性调整其计算，从而提高速度和效率（W. Huang, Chen 和 Zhang 2023)。

##### 基于门控的计算

基于门控的条件计算引入了学习到的门控机制，这些机制根据输入复杂性动态控制神经网络哪些部分被激活。与处理所有输入都使用相同计算努力的静态架构不同，这种方法通过在训练期间学习决策边界，使子网络或层的动态激活成为可能（Shazeer, Mirhoseini, Maziarz 和其他人 2017)。

门控机制通常使用二进制或连续的门控函数来实现，其中轻量级控制模块（通常称为路由器或门控网络）预测特定层或路径是否应该执行。这种决策在推理时动态发生，允许模型自适应地分配计算资源。

这种范例的一个著名例子是动态滤波网络（DFN），它在运行时通过选择不同的卷积核来应用输入相关的滤波。DFN 通过避免在输入上应用均匀滤波器来减少不必要的计算，根据输入复杂性定制其计算 (Xu Jia 等人 2016)。

另一种广泛采用的策略是专家混合（MoE）框架。在这个架构中，门控网络选择一组专门的专家子网络来处理每个输入 (Shazeer, Mirhoseini, Maziarz, 以及其他人 2017)。这仅允许模型的一小部分在给定输入时处于活动状态，显著提高了计算效率，同时不牺牲模型容量。这一想法的一个显著实例是谷歌的 Switch Transformer，它通过基于专家的条件计算扩展了 Transformer 架构 (Fedus, Zoph, 和 Shazeer 2021a)。

![图片](img/file173.svg)

图 10.27：**条件计算**：开关变压器通过动态路由标记到专门的专家子网络，从而提高效率，实现并行处理并减少每个输入的计算负载。这种架构实现了一种专家混合形式，其中门控网络选择处理每个标记的专家，从而在不增加计算量的情况下提高模型容量。*来源 (Fedus, Zoph, 和 Shazeer 2021a)*。

如图 10.27 所示，Switch Transformer 用开关 FFN 层替换了传统的前馈层。对于每个标记，一个轻量级路由器从一组前馈网络中选择一个专家。路由器输出一个关于可用专家的概率分布，每个标记激活最高概率的专家。这种设计使得大型模型能够在不按比例增加推理成本的情况下扩展参数数量。

基于门控的条件计算对于多任务和迁移学习设置特别有效，在这些设置中，输入可能从专门的加工路径中受益。通过允许对模型执行进行细粒度控制，这些机制允许在保持效率的同时，在任务之间进行自适应专业化。

然而，这些好处是以增加架构复杂性为代价的。路由和门控操作本身引入了额外的开销，包括延迟和内存访问。在硬件加速器（如 GPU、TPU 或边缘设备）上的高效部署，需要仔细关注专家激活的调度和批处理 (Lepikhin 等人 2020)。

##### 自适应推理

自适应推理指的是模型在推理过程中根据输入复杂性动态调整其计算工作量的能力。与依赖于预定义退出点或离散层跳过的早期方法不同，自适应推理根据实时置信度和任务复杂性连续调节计算深度和资源分配（Yang 等人 2020）。

这种灵活性允许模型即时决定所需的计算量，平衡效率和精度，而不需要固定的阈值。与承诺固定的计算路径不同，自适应推理使模型能够根据对输入的中间评估动态分配层、操作或专用计算（Yang 等人 2020）。

自适应推理的一个例子是快速神经网络（FNNs），它根据实时复杂性估计调整活动层的数量。如果一个输入被认为是简单的，则只激活部分层，从而减少推理时间。然而，如果早期层产生低置信度的输出，则会激活额外的层来细化预测（Jian Wu, Cheng, 和 Zhang 2019）。

一种相关的方法是动态层缩放，模型根据不确定性估计逐步增加计算深度。这项技术在需要细粒度分类任务中特别有用，其中一些输入只需要粗粒度处理，而其他输入则需要更深入的特征提取（Contro 等人 2021）。

自适应推理在资源约束动态变化的延迟敏感应用中特别有效。例如，在自主系统中，如车道检测这样的任务可能只需要最小的计算量，而在密集环境中的多目标跟踪则可能需要额外的处理能力。通过实时调整计算工作量，自适应推理确保模型在严格的时序约束下运行，同时避免不必要的资源消耗。

在如 GPU 和 TPU 这样的硬件加速器上，自适应推理通过动态分配工作负载来利用并行处理能力。这种适应性最大化吞吐量同时最小化能耗，使其非常适合实时、对功耗敏感的应用。

#### 实施挑战

动态计算通过允许模型根据输入复杂性调整其计算工作量，引入了灵活性和效率。然而，这种适应性伴随着几个挑战，必须解决这些挑战才能使动态计算实用且可扩展。这些挑战出现在训练、推理效率、硬件执行、泛化和评估中，每个都带来了独特的困难，影响了模型的设计和部署。

##### 训练和优化困难

与遵循每个输入固定计算路径的标准神经网络不同，动态计算需要额外的控制机制，如门控网络、置信度估计器或专家选择策略。这些机制确定模型应该激活或跳过的部分，增加了训练过程的复杂性。一个主要困难是许多这些决策是离散的，这意味着它们不能使用标准反向传播进行优化。相反，模型通常依赖于强化学习或连续近似等技术，但这些方法引入了额外的计算成本，并可能减慢收敛速度。

训练动态模型也带来了不稳定性，因为不同的输入遵循不同的路径，导致训练示例中梯度更新的不一致。这种可变性可能会降低优化效率，需要仔细的正则化策略来维持平滑的学习动态。动态模型引入了新的超参数，例如门控阈值或早期退出的置信度分数。为这些参数选择适当的值对于确保模型有效地平衡准确性和效率至关重要，但它显著增加了训练过程的复杂性。

##### 负载和延迟变化

尽管动态计算减少了不必要的操作，但确定要执行的计算的过程引入了额外的开销。在执行推理之前，模型必须首先决定要激活哪些层、路径或子网络。这个决策过程，通常通过轻量级门控网络实现，增加了计算成本，并可能部分抵消了跳过计算所获得的节省。虽然这些开销通常很小，但在每个操作都很重要的资源受限环境中，它们变得很重要。

一个更大的挑战是推理时间的可变性。在静态模型中，推理遵循一系列固定的操作，导致可预测的执行时间。相比之下，动态模型根据输入复杂度表现出可变的处理时间。对于具有严格实时约束的应用，如自动驾驶或机器人技术，这种不可预测性可能成为问题。一个模型可能以毫秒处理某些输入，但在显著更长的时间框架内处理其他输入，可能无法满足严格的延迟要求，限制其实际部署。

##### 硬件执行效率低下

现代硬件加速器，如 GPU 和 TPU，针对[统一、并行计算模式](https://pytorch.org/xla/master/perf/recompilation.html)进行了优化。这些加速器通过在大批量数据上同时执行相同的操作来实现最大效率。然而，动态计算引入了条件分支，这可能会破坏这种并行执行模型。当不同的输入遵循不同的计算路径时，一些处理单元可能会闲置，而其他处理单元处于活动状态，导致硬件利用率不佳。

这种发散的执行模式对硬件效率提出了重大挑战。例如，在一个 GPU 中，多个线程并行处理数据时，条件分支会导致线程发散，其中一些线程必须等待，而其他线程完成其操作。同样，TPU 是为大型矩阵运算设计的，当所有处理单元都得到充分利用时，可以达到峰值性能。动态计算可能会阻止这些加速器保持高吞吐量，从而降低大规模部署的成本效益。

在需要实时处理或高吞吐量推理的场景中，这种影响尤为明显。当硬件资源没有得到充分利用时，动态计算的理论计算优势可能无法转化为实际性能提升。在大规模部署中，这种低效变得更加显著，因为最大化硬件利用率对于管理运营成本和维持服务水平协议至关重要。

在动态模型中，内存访问模式也变得不太可预测。标准的机器学习模型以结构化的方式处理数据，优化高效的内存访问。相比之下，动态模型需要频繁的分支，导致不规则的内存访问和增加的延迟。为了优化这些模型在硬件上的执行，需要专门的调度策略和编译器优化来减轻这些低效，但这些解决方案增加了部署的复杂性。

##### **泛化**和**鲁棒性**

由于动态计算允许不同的输入通过模型的不同路径，存在某些数据分布接收的计算量少于必要的风险。如果门控函数没有精心设计，模型可能会学会持续为特定类型的输入分配较少的资源，导致预测偏差。这个问题在安全至关重要的应用中尤其令人担忧，未能为罕见但重要的输入分配足够的计算可能导致灾难性故障。

另一个担忧是训练时间计算路径的过拟合。如果一个模型在特定的计算选择分布下进行训练，它可能难以泛化到需要采取不同路径的新输入。确保动态模型能够适应未见数据需要额外的鲁棒性机制，例如基于熵的正则化或不确定性驱动的门控，但这些机制引入了额外的训练复杂性。

动态计算也带来了新的对抗性攻击漏洞。在标准模型中，攻击者可能会尝试以改变最终预测的方式修改输入。在动态模型中，攻击者可以操纵门控机制本身，迫使模型选择错误或次优的计算路径。防御此类攻击需要额外的安全措施，这进一步复杂了模型的设计和部署。

##### 评估和基准测试

大多数机器学习基准假设固定的计算预算，这使得评估动态模型的性能变得困难。传统的指标，如 FLOPs 或延迟，并不能完全捕捉这些模型的适应性，因为计算量会根据输入的复杂性而变化。因此，标准基准未能反映动态架构中准确性和效率之间的真实权衡。

另一个问题是对可重复性的担忧。由于动态模型会根据输入做出决策，在不同的硬件或略微不同的条件下运行相同的模型可能会导致执行路径的变化。这种可变性使得模型之间的公平比较变得复杂，并需要新的评估方法来准确评估动态计算的好处。没有考虑到自适应缩放的标准化基准，测量和比较动态模型与其静态对应物仍然具有挑战性。

尽管存在这些挑战，动态计算仍然是优化机器学习效率的有希望的方向。解决这些限制需要更鲁棒的训练技术、硬件感知的执行策略和改进的评估框架，这些框架能够正确考虑动态缩放。随着机器学习继续扩展，计算约束变得更加紧迫，解决这些挑战将是解锁动态计算全部潜力的关键。

### 稀疏性利用

机器学习中的稀疏性指的是张量（如权重矩阵或激活张量）中很大一部分元素为零或接近零的状态。更正式地说，对于一个张量 <semantics><mrow><mi>T</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">T \in \mathbb{R}^{m \times n}</annotation></semantics>（或更高维度），其稀疏性 <semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics> 可以表示为：<semantics><mrow><mi>S</mi><mo>=</mo><mfrac><mrow><mo stretchy="false" form="postfix">‖</mo><msub><mn>𝟏</mn><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>T</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>0</mn><mo stretchy="false" form="postfix">}</mo></mrow></msub><msub><mo stretchy="false" form="postfix">‖</mo><mn>0</mn></msub></mrow><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></mfrac></mrow> <annotation encoding="application/x-tex">S = \frac{\Vert \mathbf{1}_{\{T_{ij} = 0\}} \Vert_0}{m \times n}</annotation></semantics> 其中 <semantics><msub><mn>𝟏</mn><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>T</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>0</mn><mo stretchy="false" form="postfix">}</mo></mrow></msub><annotation encoding="application/x-tex">\mathbf{1}_{\{T_{ij} = 0\}}</annotation></semantics> 是一个指示函数，当 <semantics><mrow><msub><mi>T</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">T_{ij} = 0</annotation></semantics> 时返回 1，否则返回 0，而 <semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mo>⋅</mo><msub><mo stretchy="false" form="postfix">‖</mo><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\Vert \cdot \Vert_0</annotation></semantics> 表示 L0 范数，它计算非零元素的数量。

由于浮点表示的性质，我们通常将此定义扩展到包括接近零的元素。这导致：<semantics><mrow><msub><mi>S</mi><mi>ϵ</mi></msub><mo>=</mo><mfrac><mrow><mo stretchy="false" form="postfix">‖</mo><msub><mn>𝟏</mn><mrow><mo stretchy="false" form="prefix">{</mo><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>T</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo><</mo><mi>ϵ</mi><mo stretchy="false" form="postfix">}</mo></mrow></msub><msub><mo stretchy="false" form="postfix">‖</mo><mn>0</mn></msub></mrow><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></mfrac></mrow> <annotation encoding="application/x-tex">S_{\epsilon} = \frac{\Vert \mathbf{1}_{\{|T_{ij}| < \epsilon\}} \Vert_0}{m \times n}</annotation></semantics> 其中 <semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics> 是一个小的阈值值。

稀疏性可以在训练过程中自然出现，通常是由于正则化技术的结果，或者通过剪枝等方法故意引入，其中低于特定阈值的元素被强制设为零。有效地利用稀疏性可以带来显著的计算效率、内存节省和降低功耗，这在将模型部署在资源有限的设备上，如手机、嵌入式系统和边缘设备时尤其有价值。

#### 稀疏性类型

神经网络中的稀疏性可以大致分为两种类型：非结构化稀疏性和结构化稀疏性。

非结构化稀疏性发生在单个权重被设置为零而没有任何特定模式的情况下。这种类型的稀疏性可以通过剪枝等技术实现，其中被认为不那么重要的权重（通常基于幅度或其他标准）被移除。虽然非结构化稀疏性非常灵活，可以应用于网络的任何部分，但在硬件上可能效率较低，因为它缺乏可预测的结构。在实践中，利用非结构化稀疏性需要专门的硬件或软件优化，以最大限度地发挥其作用。

相比之下，结构化稀疏性涉及以更结构化的方式移除网络中的整个组件，如过滤器、神经元或通道。通过消除网络的整个部分，结构化稀疏性在硬件加速器（如 GPU 或 TPU）上更为高效，这些硬件可以利用这种结构进行更快地计算。结构化稀疏性通常在需要计算资源中的可预测性和效率时使用，因为它使硬件能够充分利用网络中的常规模式。

#### 稀疏性利用方法

有效地利用稀疏性需要专门的技术和硬件支持，以将理论参数减少转化为实际性能提升（Hoefler, Alistarh, Ben-Nun, Dryden, 和 Peste 2021）。剪枝通过移除不那么重要的权重（非结构化）或整个组件，如过滤器、通道或层（结构化）来引入稀疏性（Han 等人 2015）。结构化剪枝在硬件效率上更为优越，使得 GPU 和 TPU 等加速器能够充分利用常规模式。

利用稀疏性的第三个重要技术是低秩近似。这种方法中，大型密集权重矩阵被更小、秩更低的矩阵近似，这些矩阵捕捉最重要的信息同时丢弃冗余组件。这降低了存储需求和计算成本。例如，一个大小为<semantics><mrow><mn>1000</mn><mo>×</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">1000 \times 1000</annotation></semantics>的权重矩阵，包含一百万个参数，可以被分解成两个较小的矩阵，比如<semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics>（大小<semantics><mrow><mn>1000</mn><mo>×</mo><mn>50</mn></mrow><annotation encoding="application/x-tex">1000 \times 50</annotation></semantics>）和<semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics>（大小<semantics><mrow><mn>50</mn><mo>×</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">50 \times 1000</annotation></semantics>），这仅包含 10 万个参数，远少于原始的一百万个。这种较小的表示保留了原始矩阵的关键特征，同时显著降低了计算负担（Denton, Chintala, 和 Fergus 2014）。

低秩近似，如奇异值分解，常用于压缩神经网络中的权重矩阵。这些近似在推荐系统和自然语言处理模型中得到广泛应用，以降低计算复杂性和内存使用，同时不会显著损失性能（Joulin 等人 2017）。

除了这些核心方法之外，其他技术如稀疏感知训练也可以帮助模型在训练过程中学习稀疏表示。例如，使用稀疏梯度下降，其中训练算法只更新非零元素，可以帮助模型以更少的活跃参数运行。虽然剪枝和低秩近似直接减少参数或分解权重矩阵，但稀疏感知训练有助于在整个训练过程中保持模型的效率（C. Liu 等人 2018）。

#### 稀疏性硬件支持

虽然稀疏性在理论上可以降低计算成本、内存使用和功耗，但要实现实际的速度提升，需要克服硬件和软件不匹配的问题。通用处理器如 CPU 缺乏对稀疏矩阵操作的优化（Han, Mao, 和 Dally 2016），而现代加速器（GPU、TPU、FPGA）在高效处理不规则稀疏数据模式方面面临架构挑战。硬件支持对于模型优化至关重要——专门的加速器必须高效地处理稀疏数据，以便在训练和推理过程中将理论压缩转化为实际性能提升。

稀疏操作也可以通过软件很好地映射到硬件上。例如，MegaBlocks (Gale et al. 2022) 将稀疏混合专家训练重新表述为块稀疏操作，并开发了针对 GPU 的特定内核，以高效地处理这些计算在硬件上的稀疏性，并保持高加速器利用率。

#### 结构化模式

已经开发了各种稀疏格式，每种格式都有独特的结构特性和影响。其中最突出的是块稀疏矩阵和 N:M 稀疏模式。块稀疏矩阵通常具有孤立的零和非零密集子矩阵块，使得对大型稀疏矩阵的操作可以很容易地重新表述为对子矩阵的较小（从算术角度看）数量的密集操作。这种稀疏性允许更有效地存储密集子矩阵，同时保持与矩阵或向量乘法等操作形状兼容。例如，图 10.28 展示了 NVIDIA 的[cuSPARSE](https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/)库如何支持稀疏块矩阵操作和存储。其他一些工作，如 Monarch 矩阵(Dao et al. 2022)，在此基础上扩展了块稀疏性，以在矩阵表达性和计算/内存效率之间取得更好的平衡。

![图片](img/file174.svg)

图 10.28：**块稀疏表示**：NVIDIA 的 cusparse 库通过利用密集子矩阵结构有效地存储块稀疏矩阵，从而实现加速矩阵操作，同时通过块索引与密集矩阵计算保持兼容。这种方法减少了稀疏线性代数的内存占用和算术复杂性，这对于扩展机器学习模型非常重要。*来源：NVIDIA.*。

同样，<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>:<semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics>的稀疏模式是一种结构化稀疏格式，其中在每组<semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics>个连续元素（例如，权重或激活）中，恰好有<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>个非零，其余两个为零（周等人 2021）。这种确定性模式促进了高效的硬件加速，因为它允许预测内存访问模式和优化的计算。通过强制执行这种结构，模型可以在稀疏引起的效率提升和保持足够的学习复杂表示能力之间取得平衡。图 10.29 下面展示了加速密集矩阵乘法和 2:4 稀疏矩阵乘法之间的比较，这是模型训练中常用的稀疏模式。后续工作如 STEP(Lu 等人 2023)考察了学习更通用的<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>:<semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics>稀疏掩码，以在相同的原则下加速深度学习推理。

![图片](img/file175.svg)

图 10.29：**稀疏矩阵乘法**：块稀疏通过仅存储非零元素和使用结构化索引来优化矩阵运算，从而为神经网络计算提供高效的 GPU 加速。这种技术保持了与密集矩阵运算的兼容性，同时减少了内存访问和计算成本，对于大规模模型特别有益。来源：PyTorch 博客。

##### GPU 和稀疏运算

图形处理单元（GPUs）因其执行高度并行计算的能力而广为人知，这使得它们非常适合处理机器学习中常见的大规模矩阵运算。现代 GPU，如 NVIDIA 的 Ampere 架构，包括专门用于加速稀疏矩阵乘法的稀疏张量核心。这些张量核心被设计用来识别并跳过稀疏矩阵中的零元素，从而减少所需的操作次数（Abdelkhalik 等人 2022）。这对于结构化剪枝技术特别有利，其中整个滤波器、通道或层被剪枝，从而显著减少了计算量。通过跳过零值，GPU 可以将矩阵乘法的速度提高两倍或更多，从而降低稀疏网络的处理时间和功耗。

GPU 利用其并行架构来同时处理多个操作。这种并行性对于稀疏操作特别有益，因为它允许硬件更有效地利用数据中的固有稀疏性。然而，GPU 上稀疏操作的全部好处需要稀疏性以与底层硬件架构相匹配的方式组织，这使得结构化剪枝在优化方面更有优势 (Hoefler, Alistarh, Ben-Nun, Dryden, 和 Peste 2021)。

##### TPUs 和稀疏优化

TPUs 是由谷歌开发的定制硬件加速器，专门设计用于以比传统处理器更高的效率处理张量计算。TPUs，例如 TPU v4，内置了对稀疏权重矩阵的支持，这对于像 BERT 和 GPT 这样的模型特别有益，因为这些模型依赖于大规模矩阵乘法 (Norman P. Jouppi 等人 2021a)。TPUs 通过减少与零元素相关的计算负载来优化稀疏权重矩阵，从而实现更快的处理和更高的能源效率。

TPUs 的效率得益于它们执行操作时的高吞吐量和低延迟，这得益于它们定制的矩阵乘法单元。这些单元能够通过直接处理非零元素来加速稀疏矩阵操作，这使得它们非常适合包含大量稀疏性的模型，无论是通过剪枝还是低秩近似。随着对更大模型的需求数量增加，TPUs 继续在保持性能的同时，最小化与密集计算相关的能源和计算成本发挥重要作用。

##### FPGAs 和稀疏计算

场可编程门阵列 (FPGA) 是稀疏网络的另一类重要硬件加速器。与 GPU 和 TPUs 不同，FPGA 具有高度的定制性，提供了在设计上的灵活性，以优化特定的计算任务。这使得它们特别适合需要精细控制硬件执行的稀疏操作。FPGA 可以编程以执行稀疏矩阵-向量乘法和其他稀疏矩阵操作，最小化开销，为使用非结构化剪枝或需要自定义稀疏模式的模型提供高性能。

FPGA 在稀疏网络中的一个主要优势是它们能够针对特定应用进行定制，这允许进行通用硬件无法实现的优化。例如，可以通过定制数据路径和内存管理来设计 FPGA 跳过矩阵中的零元素，从而在计算和内存使用上提供显著的节省。FPGA 还允许低延迟执行，这使得它们非常适合需要高效处理稀疏数据流的实时应用。

##### 内存和能源优化

稀疏网络中的一个关键挑战是管理内存带宽，因为矩阵操作通常需要大量的内存访问。稀疏网络通过减少需要访问的元素数量来提供解决方案，从而最小化内存流量。第十一章第十一章中详细介绍的硬件加速器针对这些稀疏矩阵进行了优化，利用专门的内存访问模式跳过零值，从而减少使用的总内存带宽(Baraglia 和 Konno 2019)。

例如，GPU 和 TPU 通过利用其高内存带宽来最小化内存访问延迟。通过仅访问非零元素，这些加速器确保内存使用更加高效。这些设备中的内存层次结构也针对稀疏计算进行了优化，允许更快的数据检索和降低功耗。

计算和内存访问次数的减少直接转化为能源节约 39。稀疏操作需要更少的算术运算和更少的内存读取，导致训练和推理所需的能耗降低。这种能源效率对于在边缘设备上运行的应用尤其重要，因为这些设备对电力限制很敏感，如第十四章第十四章所述。

##### 未来：硬件和稀疏网络

随着硬件的不断进化，我们可以期待更多专门针对稀疏网络的创新。未来的硬件加速器可能会提供与稀疏感知训练和优化算法的更深入集成，从而实现计算和内存成本的进一步降低。受大脑结构启发的神经形态计算等新兴领域可能为以节能方式处理稀疏网络提供新的途径(Mike Davies 等人 2021)。这些进步有望进一步提高机器学习模型的效率和可扩展性，尤其是在需要实时处理并在电力受限设备上运行的应用中，这与第十八章第十八章中提到的可持续 AI 原则相连接。

#### 挑战和限制

虽然利用稀疏性在降低计算成本和内存使用方面提供了显著优势，但在有效实施稀疏网络时，必须考虑几个挑战和限制。表 10.11 总结了与稀疏优化相关的一些挑战和限制。

表 10.11：**稀疏优化挑战**：非结构化稀疏性，虽然减少了模型大小，但由于不规则的内存访问模式，阻碍了硬件加速，限制了潜在的节省计算量，并需要专门的硬件或软件来实现效率提升。本表总结了有效部署稀疏神经网络的要点挑战。

| **挑战** | **描述** | **影响** |
| --- | --- | --- |
| **非结构化稀疏优化** | 不规则的稀疏模式使得在硬件上利用稀疏性变得困难。 | 有限的硬件加速和减少的计算节省。 |
| **算法复杂性** | 复杂的修剪和稀疏矩阵操作需要复杂的算法。 | 大型模型的高计算开销和算法复杂性。 |
| **硬件支持** | 硬件加速器针对结构化稀疏性进行了优化，这使得非结构化稀疏性更难优化。 | 非结构化稀疏性的次优硬件利用和较低性能。 |
| **准确性与权衡** | 如果没有仔细平衡，激进的稀疏性可能会降低模型准确性。 | 可能的性能损失，需要仔细的调整和验证。 |
| **能源效率** | 稀疏矩阵存储和管理带来的开销可能会抵消减少计算带来的能源节省。 | 如果开销超过了稀疏计算带来的节省，功耗可能不会改善。 |
| **适用性有限** | 稀疏性可能不会对所有模型或任务有益，尤其是在需要密集表示的领域。 | 并非所有模型或硬件都能从稀疏性中同等受益。 |

稀疏性的主要挑战之一是非结构化稀疏性的优化。在非结构化修剪中，根据其重要性移除单个权重，导致不规则的稀疏模式。这种不规则性使得在硬件上完全利用稀疏性变得困难，因为大多数硬件加速器（如 GPU 和 TPU）设计时是为了更有效地处理结构化数据。没有规则的结构，这些加速器可能无法有效地跳过零元素，这可能会限制计算节省。

另一个挑战是修剪和稀疏矩阵操作中涉及的算法复杂性。决定哪些权重需要修剪的过程，尤其是在非结构化的方式下，需要复杂的算法，这些算法必须在模型准确性和计算效率之间取得平衡。这些修剪算法本身可能计算成本高昂，并且在大型模型上应用它们可能导致显著的开销。稀疏矩阵的优化也需要专门的技巧，这些技巧可能并不总是容易实现或在不同架构之间推广。

硬件支持是另一个重要的限制。尽管现代 GPU、TPU 和 FPGA 具有专门为加速稀疏操作而设计的功能，但要在硬件上完全优化稀疏网络，需要仔细地对硬件架构和稀疏格式进行对齐。虽然结构化稀疏在这些加速器上更容易利用，但非结构化稀疏仍然是一个挑战，因为硬件加速器可能难以高效地处理不规则稀疏模式。即使硬件针对稀疏操作进行了优化，与稀疏矩阵存储格式相关的开销以及需要专门的内存管理，仍然可能导致性能不佳。

稀疏性和精度之间总是存在权衡。激进地剪枝或低秩近似技术，这些技术会大幅度减少参数数量，可能会导致精度下降。在减少参数和保持高模型性能之间找到正确的平衡是一个微妙的过程，需要大量的实验。在某些情况下，引入过多的稀疏性可能会导致模型过小或欠拟合，无法实现高性能。

虽然稀疏性可能导致节能，但能源效率并不总是有保证。尽管稀疏操作需要的浮点运算较少，但管理稀疏数据并确保硬件最优地跳过零值所带来的开销，可能会引入额外的功耗。在边缘设备或功率预算紧张的移动环境中，如果与稀疏数据结构和硬件利用率相关的开销超过了节能效果，稀疏性的好处可能就不那么明显了。

稀疏性在特定类型的模型或任务上的适用性有限。并非所有模型都能从稀疏性中同等受益，尤其是那些密集表示对性能至关重要的模型。例如，图像分割或某些类型的强化学习中的模型，在引入稀疏性时可能不会显示出显著的增益。稀疏性可能对所有硬件平台都无效，尤其是对于缺乏计算能力或专门功能以利用稀疏矩阵操作的较老或低端设备。

#### 结合优化

虽然神经网络中的稀疏性是一种提高计算效率和减少内存使用的强大技术，但它的全部潜力通常是在与其他优化策略结合使用时实现的。这些优化包括剪枝、量化和高效模型设计等技术。了解稀疏性与这些方法如何相互作用对于有效地结合它们以实现最佳性能至关重要 (Hoefler, Alistarh, Ben-Nun, Dryden, and Ziogas 2021)。

##### 稀疏性和剪枝

剪枝和稀疏性是密切相关的技术。当应用剪枝时，生成的模型可能变得稀疏，但稀疏模式，例如它是有结构的还是非结构的，会影响模型如何有效地针对硬件进行优化。例如，结构化剪枝（例如，剪枝整个过滤器或层）通常会产生更有效的稀疏性，因为像 GPU 和 TPU 这样的硬件加速器更适合处理稀疏矩阵中的常规模式（Elsen et al. 2020）。另一方面，非结构化剪枝可能会引入不规则稀疏模式，这些模式可能无法被硬件有效地处理，尤其是在与其他技术如量化结合使用时。

由剪枝生成的稀疏模式必须与底层硬件架构相匹配，以实现计算节省（Gale, Elsen, and Hooker 2019b）。结构化剪枝在硬件优化方面特别有效。

##### 稀疏性和量化

将稀疏性和量化结合起来，可以显著减少内存使用和计算，但也带来了独特的挑战（Nagel et al. 2021a）。非结构化稀疏性加剧了低精度权重处理挑战，尤其是在缺乏对不规则稀疏矩阵有效支持的硬件上。当与低精度算术结合使用时，GPU 和 TPU 可以放大稀疏矩阵加速，而 CPU 则难以处理组合开销（Yi Zhang et al. 2021）。

##### 稀疏性和模型设计

高效的模型设计通过深度可分离卷积、低秩近似和动态计算等技术，天生地创造出高效的架构。稀疏性通过进一步减少内存和计算需求来放大这些好处（Dettmers and Zettlemoyer 2019）。然而，高效的稀疏模型需要硬件对稀疏操作的支持，以避免次优性能。硬件对齐确保了计算成本和内存使用的最小化（Elsen et al. 2020）。

##### 稀疏性和优化挑战

通过协调稀疏性、剪枝、量化和高效设计，涉及管理精度权衡（Labarge, n.d.）。硬件加速器如 GPU 和 TPU 优化结构化稀疏性，但在处理非结构化模式或稀疏性-量化组合时遇到困难。最佳性能需要选择与硬件能力相匹配的技术组合（Gale, Elsen, and Hooker 2019b），仔细平衡模型精度、计算成本、内存使用和硬件效率。

## 实施策略和评估

我们现在来探讨系统化应用策略。我们研究过的个别技术很少能单独成功；生产系统通常采用协调的优化策略，同时平衡多个约束。有效的部署需要结构化的方法来分析系统、衡量优化影响以及结合技术以实现部署目标。

本节提供了从理论理解到实际实施的方法论指导，针对三个关键问题：优化努力应该集中在哪里？我们如何衡量优化是否达到了预期的目标？我们如何结合多种技术而不引入冲突或减少回报？

### 性能分析及机会分析

优化的基础在于彻底的性能分析，以确定计算资源消耗在哪里以及哪些组件具有最大的优化潜力。然而，一个关键的第一步是确定模型优化是否真的能提高系统性能，因为在生产环境中，模型计算通常只代表总系统开销的一小部分。

现代机器学习模型表现出异构的资源消耗模式，其中特定的层、操作或数据路径不成比例地贡献于内存使用、计算成本或延迟。理解这些模式对于优先考虑优化努力和以最小的精度损失实现最大影响至关重要。

有效的性能分析始于在所有相关性能维度上建立基线测量。内存分析揭示了静态内存消耗（模型参数和缓冲区）以及训练和推理过程中的动态内存分配模式。计算分析确定了瓶颈操作，通常以 FLOPS 和实际墙钟执行时间来衡量。对于电池供电和边缘部署场景，能耗分析变得尤为重要，因为功耗直接影响到操作的可行性。延迟分析测量端到端响应时间，并确定哪些操作对推理延迟贡献最大。

考虑对用于边缘部署的视觉 Transformer（ViT）进行性能分析。使用 PyTorch Profiler 可以发现，注意力层消耗了 65%的总 FLOPS（非常适合结构化剪枝），层归一化消耗了 8%的延迟，尽管只消耗了 2%的 FLOPS（内存密集型操作），而最终的分类头消耗了 1%的计算但 15%的参数内存。此分析表明，将基于幅度的剪枝应用于注意力层作为首要任务（具有高 FLOP 减少潜力），将分类头量化为 INT8 作为第二优先级（大量节省内存，最小化精度影响），以及将层归一化操作融合作为第三优先级（减少内存带宽瓶颈）。

超出这些基线测量范围，现代优化需要理解模型对不同类型修改的敏感性。并非所有参数对模型准确性的贡献都相同，结构化敏感性分析有助于识别哪些组件可以大胆优化，而哪些组件则需要谨慎保留。层级敏感性分析揭示了哪些网络组件对保持准确性最为重要，指导决策在何处应用大胆的剪枝或量化，以及在何处使用保守的方法。

### 测量优化有效性

优化需要严格的测量框架，它不仅超越了简单的准确率指标，还捕捉了优化决策的全面影响。有效的测量同时考虑多个目标，包括准确率保持、计算效率提升、内存减少、延迟改进和节能。挑战在于在保持结构化决策过程的同时，平衡这些经常相互竞争的目标。

测量框架在应用任何优化之前应建立清晰的基线，全面捕捉所有相关指标的性能概况。准确性基线不仅包括分类准确率等主要指标，还包括校准、不同人口群体间的公平性以及对抗输入变化的鲁棒性等更细致的度量。效率基线包括计算成本（FLOPS、内存带宽）、不同硬件平台上的实际执行时间、训练和推理期间的峰值内存消耗以及能耗概况。

当将 ResNet-50 从 FP32 量化到 INT8 时，基线指标显示 Top-1 准确率为 76.1%，V100 上的推理延迟为 4.2ms，模型大小为 98MB，每次推理能耗为 0.31J。量化后的指标揭示 Top-1 准确率为 75.8%（下降 0.3%），推理延迟为 1.3ms（加速 3.2 倍），模型大小为 25MB（减少 3.9 倍），每次推理能耗为 0.08J（提高 3.9 倍）。额外的分析显示每类准确率下降从 0.1%到 1.2%，对细粒度类别影响最大，校准误差从 2.1%增加到 3.4%，INT8 量化在 GPU 上提供 3.2 倍的速度提升，但在 CPU 上仅提供 1.8 倍，这表明硬件依赖的收益。

在这些全面的基线建立之后，测量框架必须系统地跟踪优化影响。而不是单独评估技术，应用我们的三维框架需要理解不同方法结合时如何相互作用。顺序应用可能导致累积效益或意外的相互作用，从而降低整体有效性。

### 多技术集成策略

最显著的优化收益来自在我们三维框架内结合多种技术。模型表示技术（剪枝）减少参数数量，数值精度技术（量化）减少每操作的计算成本，而架构效率技术（算子融合、动态计算）减少执行开销。这些技术在不同的优化维度上操作，当适当排序时提供乘法效益。

序列化对结果有重大影响。考虑通过三个阶段在移动设备上部署 BERT-Base。第一阶段应用结构化剪枝，移除 30%的注意力头和 40%的中间 FFN 维度，参数减少 75%，准确率从 76.2%下降到 75.1%。第二阶段使用知识蒸馏将准确率恢复到 75.9%。第三阶段应用量化感知训练，使用 INT8 量化，实现额外的 4 倍内存减少，最终准确率为 75.6%。综合影响显示内存减少 16 倍（从 440MB 到 28MB），在移动 CPU 上推理速度提高 12 倍，与在剪枝前应用量化相比，最终准确率损失为 0.6%，而如果量化在剪枝前应用，损失将达 2.1%。

这个例子说明了为什么排序很重要：剪枝首先将重要权重集中到更小的范围内，使后续的量化更有效。在剪枝前应用量化会减少基于重要性的剪枝决策可用的数值精度，降低最终准确度。有效的组合需要理解这些依赖关系，并开发出最大化累积效益的应用序列。在接下来的 AutoML 部分中，现代自动化方法利用我们的维度框架系统地发现有效的技术组合。

## 自动机器学习与自动化优化策略

随着机器学习模型复杂性的增加，为了在现实世界中部署它们，需要平衡多个因素，包括准确度、效率和硬件限制。我们已经探索了各种优化技术，包括剪枝、量化和神经架构搜索，每种技术都针对模型效率的特定方面。然而，有效地应用这些优化通常需要大量的手动工作、领域专业知识和迭代实验。

自动机器学习（AutoML）旨在通过自动化搜索最优模型配置的过程来简化这一过程，基于第八章中的训练方法。AutoML 框架利用机器学习算法来优化架构、超参数、模型压缩技术和其他重要参数，减少了对人类干预的需求（F. Hutter, Kotthoff, and Vanschoren 2019）。通过系统地探索可能的模型设计空间，AutoML 可以在保持竞争性准确度的同时提高效率，通常可以发现通过手动调整可能被忽视的新颖解决方案（Zoph 和 Le 2017b）。

自动机器学习（AutoML）并不取代人类专业知识的需求，而是通过提供一种结构化和可扩展的模型优化方法来增强它。如图图 10.30 所示，传统工作流程与 AutoML 之间的关键区别在于，后者自动化了预处理、训练和评估。而不是手动调整剪枝阈值、量化策略或架构设计，从业者可以定义高级目标，包括延迟约束、内存限制和准确度目标，并允许 AutoML 系统探索最能满足这些约束的配置（Feurer 等人 2019），从而实现第十六章中详细描述的稳健部署策略。

![图片](img/file176.svg)

图 10.30：**AutoML 工作流程**：自动机器学习（automl）通过结构化自动化数据预处理、模型选择和超参数调整来简化模型开发，与传统工作流程形成对比，后者在每个阶段都需要大量的手动工作。这种自动化使从业者能够定义高级目标和约束，允许 automl 系统高效地探索广阔的设计空间并识别最优模型配置。

本节探讨了 AutoML 的核心方面，从优化的关键维度开始，然后是 AutoML 系统中使用的方法，最后是挑战和限制。这种审查揭示了 AutoML 如何作为一个综合框架，统一了之前讨论的许多优化策略。

### AutoML 优化

AutoML 旨在优化机器学习模型的多个方面，确保效率、准确性和可部署性。与专注于单个技术（如量化以降低数值精度或剪枝以压缩模型）的传统方法不同，AutoML 通过综合考虑这些因素采取了一种整体方法。这使搜索最优模型配置更加彻底，在性能与实际约束之间取得平衡（Yihui He 等人 2018）。

AutoML 的主要优化目标之一是神经网络架构搜索。设计一个高效的模型架构是一个复杂的过程，需要平衡层配置、连接模式和计算成本。NAS 通过结构性地探索不同的网络结构、评估其效率并选择最优设计来实现自动化 (Elsken, Metzen, and Hutter 2019b)。这个过程导致了诸如 MobileNetV3 和 EfficientNet 等架构的发现，这些架构在关键效率指标上优于手动设计的模型 (Tan and Le 2019b)。

除了架构设计之外，AutoML 还专注于超参数优化 40，这在确定模型性能方面起着重要作用。学习率、批量大小 41、权重衰减和激活函数等参数必须仔细调整以确保稳定性和效率。

与依赖试错不同，AutoML 框架采用结构化搜索策略，包括贝叶斯优化 42、进化算法和自适应启发式方法，以有效地识别给定模型和数据集的最佳超参数设置 (Bardenet 等人 2015)。

AutoML 的另一个重要方面是模型压缩。例如，剪枝和量化等技术有助于减少模型的内存占用和计算需求，使其更适合在资源受限的硬件上部署。AutoML 框架自动化选择剪枝阈值、稀疏模式和量化级别，优化模型以实现速度和能效 (Jiaxiang Wu 等人 2016)。这对于边缘人工智能应用尤为重要，在这些应用中，模型需要以最小的延迟和功耗运行 (Chowdhery 等人 2021)。

最后，AutoML 考虑了部署感知优化，确保最终模型适合实际执行。不同的硬件平台对模型执行施加不同的约束，例如内存带宽限制、计算吞吐量和能效要求。AutoML 框架结合了硬件感知优化技术，通过调整计算工作量、内存访问模式和执行策略来针对特定设备定制模型 (H. Cai, Gan, and Han 2020)。

在这些维度上的优化使得 AutoML 能够为增强机器学习模型提供一个统一的框架，简化流程以实现效率，同时不牺牲准确性。这种整体方法确保模型不仅在理论上最优，而且在实际部署到各种应用和硬件平台上也是实用的。

### 优化策略

AutoML 系统系统地探索不同的配置，以识别架构、超参数和压缩策略的最佳组合。与需要广泛领域专业知识的手动调整不同，AutoML 利用算法搜索方法在平衡精度、效率和部署约束的同时，在广阔的设计空间中导航。

NAS 通过强化学习、进化算法和基于梯度的优化自动化架构设计，构成了 AutoML 的基础(Zoph 和 Le 2017b)。通过系统地评估候选架构，NAS 识别出优于人工设计的模型的结构(Real 等人 2019)。超参数优化(HPO)通过使用贝叶斯优化和自适应启发式方法来微调训练参数——学习率、批量大小、权重衰减——以实现比网格搜索更快的收敛速度(Feurer 等人 2019)来补充这一点。

模型压缩优化根据部署需求自动选择剪枝和量化策略，评估模型大小、延迟和精度之间的权衡。这使得在资源受限的设备上高效部署成为可能(第十四章)，无需手动调整。数据处理策略通过自动特征选择、自适应增强策略和数据集平衡来进一步提高性能，这些方法在不增加计算开销的情况下提高了鲁棒性(第十六章)。

元学习方法是最近的一项进展，其中先前优化任务的知识加速了新模型搜索的过程(Vanschoren 2018)。通过从先前实验中学习，AutoML 系统智能地探索优化空间，减少训练和评估成本，同时使系统能够更快地适应新任务和数据集。

最后，许多现代 AutoML 框架提供端到端自动化，将架构搜索、超参数调整和模型压缩集成到一个单一的流程中。例如，Google AutoML、Amazon SageMaker Autopilot 和 Microsoft Azure AutoML 等平台提供完全自动化的工作流程，简化了整个模型优化过程(L. Li 等人 2017)。

这些策略的集成使得 AutoML 系统能够提供一种可扩展且高效的模型优化方法，减少了对手动实验的依赖。这种自动化不仅加速了模型开发，还使得发现可能被忽视的新型架构和配置成为可能，支持第十二章中提到的结构化评估方法(第十二章)。

### AutoML 优化挑战

尽管 AutoML 为优化机器学习模型提供了一个强大的框架，但它也引入了几个挑战和权衡，必须仔细考虑。尽管 AutoML 能够自动化模型设计和超参数调整，但它并不是万能的解决方案。AutoML 的有效性取决于计算资源、数据集特征以及特定应用的特定约束。

AutoML 中最显著的挑战之一是计算成本。搜索最优架构、超参数和压缩策略的过程需要评估大量候选模型，每个模型都必须进行训练和验证。像 NAS 这样的方法可能特别昂贵，通常需要成千上万的 GPU 小时来探索大型的搜索空间。虽然像早期停止、权重共享和代理模型这样的技术有助于降低搜索成本，但计算开销仍然是一个主要的限制，特别是对于有限访问高性能计算资源的组织。

另一个挑战是搜索策略中的偏差，这可能会影响最终的模型选择。AutoML 中的优化过程由启发式方法和预定义的目标指导，这可能导致根据搜索空间定义的方式产生偏差的结果。如果搜索算法优先考虑某些架构或超参数而不是其他，它可能无法发现对特定任务可能更有效的替代配置。训练数据中的偏差可能会通过 AutoML 过程传播，加强最终模型中的不希望出现的模式。

泛化和迁移性提出了额外的担忧。AutoML 生成的模型针对特定数据集和部署条件进行了优化，但将它们应用于新任务或环境时，其性能可能会下降。与手动设计的模型不同，手动设计的模型中人类的直觉可以指导选择泛化能力良好的架构，而 AutoML 依赖于在约束搜索空间内的经验评估。这种限制引发了关于 AutoML 优化模型在面对现实世界变化时的鲁棒性的疑问。

可解释性是另一个关键考虑因素。许多 AutoML 生成的架构和配置在效率上进行了优化，但在设计选择上缺乏透明度。理解为什么特定的 AutoML 发现模型表现良好可能具有挑战性，这使得从业者难以调试问题或针对特定需求调整模型。某些 AutoML 技术的黑盒性质限制了人类对底层优化过程的洞察。

除了技术挑战之外，自动化与控制之间也存在权衡。虽然 AutoML 减少了手动干预的需求，但它也抽象了许多专家可能对特定应用进行微调的决策过程。在某些情况下，领域知识对于指导模型优化很重要，而完全自动化的系统可能并不总是考虑到由问题域施加的微妙但重要的约束。

尽管存在这些挑战，AutoML 仍在不断发展，持续的研究集中在降低计算成本、提高泛化能力和增强可解释性。随着这些改进的出现，AutoML 预计将在优化机器学习模型的发展中扮演越来越突出的角色，使 AI 系统对广泛的适用性更加易于访问和高效。

探索的优化技术——包括模型表示、数值精度、架构效率和自动化选择——为高效机器学习系统提供了一个全面的工具包。然而，实际实施需要稳健的软件基础设施，通过易于使用的 API、高效的实现和无缝的工作流程集成，弥合优化研究与部署之间的差距。

## 实现工具和软件框架

对于模型优化技术（如剪枝、量化和高效数值计算）的理论理解很重要，但它们的实际实施高度依赖于稳健的软件支持。没有广泛的框架开发和工具，这些优化方法将主要对实践者不可及。实现量化需要手动修改模型定义并在整个网络中仔细插入量化操作。剪枝将涉及直接操作权重张量，随着模型规模的扩大，这些任务变得极其复杂。

现代机器学习框架提供了高级 API 和自动化工作流程，抽象了实现复杂性，使复杂的优化技术对实践者可及。框架解决了关键挑战：提供常见优化技术的预构建模块，协助超参数调整（剪枝计划、量化位宽），通过自动化评估管理精度-压缩权衡，并通过特定于设备的代码生成确保硬件兼容性。

该软件基础设施将理论优化技术转化为可直接在生产环境中应用的实用工具（第十三章）。生产优化工作流程涉及额外的考虑因素，包括模型版本控制策略、监控优化对数据管道的影响、管理开发与部署环境中的优化工件，以及在优化失败时建立回滚程序。这种可访问性弥合了学术研究与工业应用之间的差距，使得高效的机器学习模型得以广泛部署。

### 模型优化 API 和工具

领先的框架，如 TensorFlow、PyTorch 和 MXNet，提供了全面的 API，使从业者能够应用优化技术，而无需从头实现复杂的算法（第七章）。这些内置优化增强了模型效率，同时确保遵循既定的最佳实践。

TensorFlow 的模型优化工具包支持量化、剪枝和聚类。QAT 将浮点模型转换为低精度格式（INT8），同时保持精度，系统性地管理跨不同架构的权重和激活量化。剪枝算法通过在不同粒度级别上移除冗余连接来引入稀疏性——从单个权重到整个层——允许从业者根据特定需求定制策略。权重聚类将相似的权重分组以进行压缩，同时保持功能，提供多条途径来提高模型效率。

类似地，PyTorch 通过内置的量化和剪枝模块提供全面的优化支持。`torch.quantization`包提供了将模型转换为低精度表示的工具，支持训练后量化和量化感知训练，如列表 10.3 所示。

列表 10.3：**量化感知训练**：准备模型以在低精度格式下进行训练，确保在训练过程中考虑到量化误差。

```py
import torch
from torch.quantization import QuantStub, DeQuantStub,
     prepare_qat

# Define a model with quantization support
class QuantizedModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.quant = QuantStub()
        self.conv = torch.nn.Conv2d(3, 64, 3)
        self.dequant = DeQuantStub()

    def forward(self, x):
        x = self.quant(x)
        x = self.conv(x)
        return self.dequant(x)

# Prepare model for quantization-aware training
model = QuantizedModel()
model.qconfig = torch.quantization.get_default_qat_qconfig()
model_prepared = prepare_qat(model)
```

对于剪枝，PyTorch 提供了`torch.nn.utils.prune`模块，它支持无结构和有结构的剪枝。这两种剪枝策略的示例在列表 10.4 中给出。

列表 10.4：**PyTorch 剪枝 API**：应用无结构和有结构的剪枝技术以降低模型复杂度，同时保持性能。*来源：PyTorch 文档*

```py
import torch.nn.utils.prune as prune

# Apply unstructured pruning
module = torch.nn.Linear(10, 10)
prune.l1_unstructured(module, name="weight", amount=0.3)
# Prune 30% of weights

# Apply structured pruning
prune.ln_structured(module, name="weight", amount=0.5, n=2, dim=0)
```

这些工具无缝集成到 PyTorch 的训练管道中，使得对不同的优化策略进行高效实验成为可能。

内置的优化 API 提供了显著的好处，使得模型优化更加易于访问和可靠。通过提供预测试、生产就绪的工具，这些 API 大大降低了从业者优化模型时所面临的实现复杂性。开发者无需从头开始实现复杂的优化算法，而是可以利用经过彻底审查的标准接口。

这些内置 API 提供的一致性在跨不同模型架构工作时尤其有价值。标准化的接口确保了优化技术被统一应用，减少了由于定制解决方案可能出现的实现错误或不一致的风险。这种标准化有助于在不同项目和团队之间保持可靠和可重复的结果。

这些框架也充当了前沿研究与实际应用之间的桥梁。随着新的优化技术从研究社区中涌现，框架维护者将这些进步整合到他们的 API 中，使得最先进的方法对从业者来说易于获取。这种持续集成研究进展确保了开发者能够访问最新的优化策略，而无需独立实现。

内置 API 的全面性使得对不同的优化方法进行快速实验成为可能。开发者可以轻松测试各种策略，比较其有效性，并快速迭代以找到特定用例的最佳配置。这种高效实验的能力对于在模型性能和资源限制之间找到合适的平衡至关重要。

随着模型优化不断演进，主要框架维护并扩展了其内置支持，进一步降低了高效模型部署的障碍。这些 API 的标准化在民主化访问模型效率技术的同时，确保了高质量实现的一致性和可靠性。

### 硬件特定优化库

在第七章中涵盖的现代机器学习框架中的硬件优化库，使得在不同硬件平台上高效部署优化模型成为可能。这些库直接与训练和部署管道集成，为模型表示、数值精度和架构效率等维度上的各种优化技术提供硬件特定的加速。

对于模型表示优化，如剪枝，TensorRT、XLA43 和 OpenVINO 等库通过优化的内核提供稀疏感知加速，这些内核能够高效地处理稀疏计算。TensorRT 特别支持结构化稀疏模式，允许使用如两出四结构剪枝等技术训练的模型在 NVIDIA GPU 上高效运行。类似地，TPU 利用 XLA 的稀疏矩阵优化，而 FPGA 通过如 Vitis AI 等框架实现定制的稀疏执行。

知识蒸馏受益于硬件感知优化，有助于紧凑的学生模型实现高推理效率。TensorRT、OpenVINO 和 SNPE 等库优化蒸馏模型以实现低功耗执行，通常将蒸馏与量化或架构重构相结合，以满足硬件约束。对于通过神经架构搜索（NAS）发现的模型，TVM44 和 TIMM 等框架提供编译器支持，以调整适用于各种硬件后端的架构。

在数值精度优化的方面，这些库为 PTQ 和 QAT 提供了广泛的支持。TensorRT 和 TensorFlow Lite 在模型转换期间实现 INT8 和 INT4 量化，降低计算复杂度，同时在移动 SoC 和边缘 AI 芯片上使用专用硬件加速。

在 SNPE 和 OpenVINO 等框架中支持更细粒度的量化方法，如通道和组量化。PyTorch 和 ONNX Runtime 中的动态量化功能使模型能够进行运行时激活量化，使模型能够适应不同的硬件条件。对于极端量化，通过 CMSIS-NN 等库优化了二值化和三值化技术，使得在 ARM Cortex-M 微控制器上高效执行二权重模型成为可能。

架构效率技术紧密集成于特定硬件的执行框架中。TensorFlow XLA 和 TVM 通过积极的融合和内核重排提供操作级别的调整，从而提高 GPU、TPU 和边缘设备的效率。

对稀疏感知执行的广泛支持跨越多个硬件平台。NVIDIA GPU 使用专门的稀疏张量核心来加速结构化稀疏模型，而 TPU 在硬件级别实现稀疏矩阵优化。在 FPGA 上，如 Vitis AI 等供应商特定的编译器使定制的稀疏计算得到高度优化。

这种将硬件优化库与机器学习框架的彻底集成，使得开发者能够有效地实现剪枝、量化、NAS、动态计算和稀疏感知执行，同时确保针对目标硬件的最佳适应，支持第十三章中详细说明的部署策略。能够在多个维度上优化，包括模型表示、数值精度和架构效率，这对于在多样化的平台上高效部署机器学习模型非常重要。

### 优化过程可视化

模型优化技术会改变模型结构和数值表示，但没有可视化工具，其影响可能难以解释。专门的框架有助于从业者通过稀疏模式、量化误差分布和激活变化的图形表示来理解剪枝、量化和其他优化如何影响模型行为。

#### 可视化量化效应

量化降低了数值精度，引入了舍入误差，这些误差可能会影响模型精度。可视化工具可以直接揭示这些误差的分布情况，有助于诊断和减轻与精度相关的性能下降。

一种常用的技术是量化误差直方图，它描绘了权重和激活之间的误差分布。这些直方图揭示了量化误差是否遵循高斯分布或包含异常值，这可能会指示有问题的层。TensorFlow 的量化调试器和 PyTorch 的 FX 图模式量化工具允许用户分析此类直方图，并比较不同量化方法之间的错误模式。

激活可视化也有助于检测由降低数值精度引起的溢出问题。例如，ONNX 运行时的量化可视化工具和 NVIDIA 的 TensorRT 检查器允许从业者对量化前后的激活进行颜色映射，使饱和和截断问题可见。这使校准调整能够防止信息损失过多，保持数值稳定性。例如，图 10.31 是 AlexNet 卷积核的颜色映射。

![图片](img/file177.jpg)

图 10.31：**卷积核权重**：颜色映射揭示了学习到的卷积滤波器中的模式，指示输入图像中的边缘、纹理或特定形状的特征检测器。分析这些权重分布有助于从业者了解神经网络优先考虑哪些特征，并诊断潜在的死或饱和滤波器等问题——这对于模型校准和性能优化非常重要。来源：(Krizhevsky, Sutskever, and Hinton 2017c)。

除了静态可视化之外，在训练过程中跟踪量化误差也很重要。在量化感知训练（QAT）期间监控均方量化误差（MSQE）有助于识别数值精度对学习产生重大影响的发散点。TensorBoard 和 PyTorch 的量化调试 API 提供实时跟踪，突出显示训练过程中的不稳定性。

通过将这些可视化工具集成到优化工作流程中，从业者可以早期识别和纠正问题，确保优化模型既保持准确性又保持效率。这些经验见解提供了对稀疏性、量化和架构优化如何影响模型的更深入理解，指导有效的模型压缩和部署策略。

#### 可视化稀疏模式

稀疏性可视化工具通过映射出哪些权重已被移除以及稀疏性如何分布在不同层之间，为修剪模型提供详细的洞察。例如，TensorBoard（用于 TensorFlow）和 Netron（用于 ONNX）等框架允许用户在层和权重级别检查修剪的网络。

一种常见的可视化技术是稀疏性热图，其中颜色渐变表示从每个层移除的权重比例。稀疏性较高的层看起来较暗，揭示了受修剪影响最大的模型区域，如图 10.32 所示。此类可视化将修剪从黑盒操作转变为可解释的过程，使从业者能够更好地理解和控制稀疏感知优化。

![图片](img/file178.png)

图 10.32：**稀疏分布**：修剪后的神经网络在层之间表现出不同程度的权重移除；较深的阴影表示更高的稀疏性，揭示了模型中哪些部分最受修剪过程的影响。分析这种分布有助于从业者理解和细化针对模型压缩和效率的稀疏感知优化策略。来源：[numenta](https://www.numenta.com/blog/)

除了静态快照之外，趋势图跟踪多个修剪迭代中的稀疏性进展。这些可视化展示了全局模型稀疏性的演变，通常显示初始快速增加后是更渐进的细化。TensorFlow 的模型优化工具包和 SparseML 的监控实用程序提供此类跟踪功能，显示随时间变化的每层修剪级别。这些见解允许从业者通过调整单个层的稀疏性约束来微调修剪策略。

如 DeepSparse 的可视化套件和 PyTorch 的修剪实用程序之类的库能够生成这些可视化工具，帮助分析修剪决策如何影响不同的模型组件。通过使稀疏数据可视化，这些工具帮助从业者更有效地优化他们的模型。

## 技术比较

深入研究了三种主要优化方法后，比较分析揭示了不同技术如何解决效率-精度权衡的不同方面。这种比较有助于根据部署约束和可用资源选择技术。

表 10.12：**优化技术权衡**：比较三种主要优化方法在关键性能维度上的差异，突出每种技术如何解决不同的约束和部署场景。剪枝在计算减少方面表现优异，但需要稀疏硬件支持；量化提供了平衡的大小和速度改进，具有广泛的硬件兼容性；而蒸馏在更高的训练成本下产生高质量的压缩模型。

| **技术** | **主要目标** | **精度影响** | **训练成本** | **硬件依赖** | **最佳适用** |
| --- | --- | --- | --- | --- | --- |
| **剪枝** | 减少 FLOPs/大小 | 中等 | 低（微调） | 高（对于稀疏操作） | 延迟关键应用 |
| **量化** | 减少大小/延迟 | 低 | 低（PTQ）/高（QAT） | 高（INT8 支持） | 边缘/移动部署 |
| **蒸馏** | 减少大小 | 低-中 | 高（重新训练） | 低 | 创建更小、高质量的模型 |

理解这些权衡有助于系统地选择技术（表 10.12）。剪枝在稀疏计算硬件可用且减少浮点运算至关重要时效果最佳。量化提供了最灵活的方法，具有广泛的硬件支持，使其成为各种部署场景的理想选择。知识蒸馏需要大量的计算投入，但能产生一致的高质量压缩模型，因此在精度保护至关重要的场合非常有价值。

这些技术协同作用，量化通常在剪枝或蒸馏之后应用，以实现复合压缩效益。生产系统通常采用顺序应用：初始剪枝减少参数数量，量化优化数值表示，通过蒸馏原则进行微调以恢复任何精度损失。顺序应用可以实现 10-50 倍的压缩比率，同时在各种部署场景中保持有竞争力的精度。

## 谬误和陷阱

模型优化是机器学习系统中技术最复杂的领域之一，其中必须协调多种技术以实现效率提升而不牺牲精度。剪枝、量化和蒸馏技术的复杂性和它们之间复杂的相互依赖性创造了大量误用和次优结果的机会，这些结果可能会损害部署的成功。

**谬误**：*优化技术可以独立应用，无需考虑它们之间的相互作用。*

这种误解导致团队在没有理解它们如何相互作用的情况下同时应用多种优化技术。将剪枝与激进的量化相结合可能会使精度损失超过可接受水平，而从大量剪枝的模型中提取的知识蒸馏可能会将次优行为转移到学生网络上。不同的优化方法可能会相互干扰，产生复杂的权衡，需要仔细协调。成功的优化需要理解技术之间的相互作用，并协调一致地应用它们，而不是作为独立的修改。

**陷阱：** *优化理论指标而不是实际部署性能。*

许多从业者专注于减少参数数量、FLOPs 或模型大小，而没有衡量实际部署性能的提升。一个参数更少的模型可能仍然具有较差的缓存局部性、不规则的内存访问模式或低效的硬件利用率，从而抵消了理论上的效率提升。量化减少模型大小可能会由于格式转换开销而在某些硬件平台上增加推理延迟。有效的优化需要衡量和优化实际的部署指标，而不是依赖于理论上的复杂性降低。

**谬误：** *激进的量化可以在最小精度损失的情况下保持模型性能。*

这种信念驱使团队在不理解数值精度与模型表达能力之间关系的情况下应用极端的量化级别。虽然许多模型可以很好地容忍适度的量化，但极端的量化可能会导致灾难性的精度下降、数值不稳定或训练发散。不同的模型架构和任务对量化的敏感性不同，需要仔细分析，而不是假设其普遍适用性。一些操作，如注意力机制或归一化层，可能需要更高的精度以保持其功能。

**陷阱：** *使用后训练优化而不考虑训练感知替代方案。*

团队通常在训练完成后应用优化技术，以避免修改现有的训练流程。后训练优化虽然方便，但通常比优化感知训练方法的结果差。量化感知训练、训练过程中的逐步剪枝和蒸馏集成训练可以在应用这些技术后获得更好的精度-效率权衡。后训练优化的便利性是以次优结果为代价的，这些结果可能不符合部署要求。

**陷阱：** *专注于单个模型优化，而不考虑系统级性能瓶颈。*

许多优化工作仅专注于降低模型复杂性，而没有分析模型运行的整体系统环境，需要详细说明的第十二章中的结构化分析方法。如果数据预处理管道、I/O 操作或网络通信主导了整体系统延迟，那么高度优化的模型可能只提供微小的益处。内存带宽限制、缓存未命中或低效的批量处理可能会抵消激进模型优化的优势。同样，针对单个模型推理的优化可能会错过通过批量处理、模型并行或请求流水线提高吞吐量的机会。有效的优化需要分析整个系统以确定实际瓶颈，并确保模型级别的改进转化为可衡量的系统级别性能提升。这种系统视角在多模型集成、实时服务系统或边缘部署中尤为重要，在这些场景中，资源限制超出了单个模型效率的范围。整体优化方法通过确保优化有助于整体系统可靠性和可维护性，直接与操作卓越原则第十三章相联系。

## 摘要

模型优化代表了理论机器学习进步与实际部署现实之间的重要桥梁，在计算约束、内存限制和能源效率要求下，需要复杂的工程解决方案。本章展示了模型精度与资源效率之间的核心紧张关系如何驱动一个丰富的优化技术生态系统，这些技术可以在多个维度上同时操作。现代优化方法不是简单地减少模型大小或复杂性，而是战略性地重新组织模型表示、数值精度和计算模式，以保留重要能力的同时，显著提高效率特性。

我们的优化框架展示了如何系统地细化模型设计的各个方面，以满足部署限制。从 440MB 的 BERT-Base 模型(Devlin 等人 2018b)到 28MB 的部署版本的过程，展示了结合互补技术的力量：结构化剪枝将模型缩小到 110MB，使用 DistilBERT(Sanh 等人 2019)的知识蒸馏在保持性能的同时进一步减小了大小，INT8 量化实现了最终的 28MB 目标。硬件感知设计原则的集成确保了优化策略与底层计算架构相一致，最大化了不同部署环境中的实际效益。

**关键要点**

+   模型优化需要在表示、精度和架构效率方面采取协调一致的方法——正如 BERT 通过联合剪枝、蒸馏和量化实现 16 倍压缩所证明的那样

+   硬件感知优化将模型特性与计算架构对齐，以最大化实际性能效益

+   通过 AutoML 进行自动化优化可以发现优于手动优化策略的技术组合

+   优化技术必须在保持准确性的同时平衡部署限制——DistilBERT 在参数减少 40%的情况下保留了 BERT 的 97%的性能

+   成功需要理解没有单一技术能提供通用的解决方案；最佳策略取决于特定的部署限制、硬件特性和应用需求

优化自动化框架的出现代表了向自动化发现适应特定部署环境和性能要求的优化策略的范式转变。这些自动化方法建立在训练方法之上，同时指向自我优化系统的新兴前沿。这样的系统使从业者能够比手动方法更系统地探索广泛的优化空间，通常揭示出实现更优效率-准确性权衡的新技术组合。随着模型变得越来越大，部署环境变得更加多样化，掌握这些优化技术对于弥合研究准确性与生产效率之间的差距变得越来越关键。

* * *
