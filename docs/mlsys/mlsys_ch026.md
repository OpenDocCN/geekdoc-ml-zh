# AGI 系统

*DALL·E 3 提示：一幅描绘从当前机器学习系统到通用人工智能（AGI）演变的未来派可视化图像。该图像展示了一个技术可视化，有三个不同的区域：在前景，熟悉的机器学习组件，如神经网络、GPU 和数据管道；在中景，新兴的系统，如大型语言模型和多智能体架构形成相互连接的星座；在背景，一个发光的地平线暗示着 AGI。场景使用从前景的混凝土技术蓝绿色渐变到地平线的抽象金色和白色光线。电路图案和数据流连接所有元素，展示了今天的构建块如何演变成明天的智能。这种风格既技术又充满希望，适合高级教科书。*

![图片](img/file320.png)

## 目的

*为什么机器学习系统从业者必须理解新兴趋势并预见技术演变，而不是仅仅掌握当前的实施情况？*

机器学习系统运行在一个快速发展的技术环境中，昨天的尖端方法可能成为明天的过时系统，这要求从业者能够预见并适应快速的变化。与成熟的工程学科不同，机器学习系统面临着来自算法突破、硬件进步和不断变化的计算范式对系统架构要求的持续颠覆。理解新兴趋势使工程师能够做出前瞻性的设计决策，延长系统寿命，避免技术死胡同，并为未来的能力定位基础设施。随着组织在预计将运行多年且底层技术持续快速发展的机器学习系统上大量投资，这种预见性思维变得至关重要。研究前沿发展有助于从业者发展必要的战略思维，以构建适应性系统，评估新兴技术与当前实施情况，并就何时以及如何将创新纳入生产环境做出明智的决策。

**学习目标**

+   定义通用人工智能（AGI）并区分它和窄人工智能，通过领域普遍性、知识迁移和持续学习能力

+   分析当前人工智能限制（缺乏因果推理、持久记忆和跨领域迁移）如何限制向 AGI 的进步

+   比较竞争的 AGI 范式（规模假设、神经符号方法、具身智能、多智能体系统）并评估它们的工程权衡

+   设计复合人工智能系统架构，集成专用组件以增强超越单体模型的能力

+   评估新兴的架构范式（状态空间模型、基于能量的模型、神经形态计算）克服 Transformer 限制的潜力

+   评估高级训练方法（RLHF、宪法 AI、持续学习）以开发对齐和自适应的复合系统

+   确定 AGI 发展的关键技术障碍，包括上下文限制、能源约束、推理能力和对齐挑战

+   综合优化、硬件加速和运营方面的基础设施需求，以适应 AGI 规模系统

## 从专用人工智能到通用智能

当被要求规划一个复杂的多日项目时，ChatGPT 生成听起来合理的计划，但往往包含逻辑错误 1。当要求回忆先前的对话细节时，由于缺乏持久记忆而失败。当需要通过第一性原理推理解释为什么某个解决方案有效时，它重现了学习到的模式，而不是展示真正的理解。这些失败不仅仅是简单的错误，而是根本性的架构限制。当代模型缺乏持久记忆、因果推理和规划能力，这些正是定义通用智能的属性。

探索从今天的专用系统到明天的通用人工智能（AGI）的工程路线图，我们将它视为一个复杂的系统集成挑战。虽然当代大规模系统在自然语言理解到多模态推理等多个领域展示了能力，但它们仍然受限于其架构。机器学习系统领域已经达到一个关键转折点，工程原则的汇聚使我们能够展望超越这些限制的系统，需要新的理论框架和工程方法。

本章通过本教科书建立的系统工程原则的视角，考察了从当代专用系统向人工智能的轨迹。中心论点认为，通用人工智能主要构成一个系统集成挑战，而不是算法突破，需要协调异构计算组件、自适应内存架构和跨任意领域无特定任务优化的持续学习机制。

分析沿着三个相互关联的研究方向进行，这些方向定义了当代智能系统的前沿。首先，我们研究通用人工智能作为一个系统集成问题，探讨当前在因果推理、知识融合和跨领域迁移中的局限性如何制约向领域通用智能的进步。其次，我们分析复合人工智能系统作为超越单体模型限制的实用架构，通过协调专用组件提供通往增强能力的直接途径。第三，我们探索包括基于能量的模型、状态空间架构和神经形态计算等新兴计算范式，这些范式承诺不同的学习和推理方法。

这些发展对机器学习系统工程的每个领域都产生了深远的影响。数据工程必须适应多模态、流式和合成生成内容，其规模挑战现有的管道架构。训练基础设施需要协调异构的计算基础，结合符号和统计学习范式。模型优化必须在保持涌现能力的同时，确保部署在多样化的硬件配置中。操作系统必须在能力接近甚至可能超过人类认知性能时，保持可靠性、安全性和对齐属性。

这些前沿的重要性不仅超越了技术考虑，还包括对设计旨在在长期时间尺度上运行的系统的实践者的战略影响。关于数据表示、计算资源分配和系统模块化的当代架构决策将决定通用人工智能是通过渐进式进步还是需要范式转变而出现。指导这些选择的工程原则将塑造人工智能发展的轨迹及其与人类认知系统的集成。

而不是参与推测性的未来主义，本章将分析建立在系统扩展现有工程方法论的基础上。通往通用人工智能的道路是通过系统思维的纪律性应用、已验证技术的规模化集成以及对复杂组件交互产生的涌现行为的细致关注而出现的。这种方法将通用人工智能定位为一个可实现的工程目标，它逐步建立在现有能力之上，同时认识到超越狭窄领域专业化的定性挑战。

## 定义 AGI：将智能视为一个系统问题

**通用人工智能（AGI）**代表的是通过**领域通用性**、**知识迁移**和**持续学习**，在所有领域与人类认知能力相匹配的计算系统，而不是在狭窄的任务特定应用中表现出色。

通用人工智能（AGI）的出现主要是一个系统工程挑战。虽然 ChatGPT 和 Claude 在语言领域表现出强大的能力，而专业系统在象棋和国际象棋中击败了世界冠军，但真正的 AGI 需要将感知、推理、规划和行动整合到没有边界的架构中 2。

考虑人类智能背后的认知架构。大脑通过分层集成协调专门的子系统：感觉皮层处理多模态输入，海马体巩固情景记忆，前额叶皮层协调执行控制，小脑细化运动预测。每个子系统以不同的计算原理运行，但它们无缝结合，产生统一的行为。这个生物蓝图表明，AGI 不会从扩展单个架构中产生，而是从协调专门的组件中产生，这正是我们在本章中探讨的复合系统方法。

当前系统在模式匹配方面表现出色，但缺乏因果理解。当 ChatGPT 解决物理问题时，它利用训练数据中的统计相关性，而不是建模物理定律。当 DALL-E 生成图像时，它结合了学习到的视觉模式，但没有理解三维结构或光照物理。这些局限性源于架构约束：变压器通过优化于序列建模的注意力机制处理信息，而不是因果推理或空间理解。

基于能量的模型提供了一个可以弥合这一差距的替代框架，它提供了一种通过优化驱动的推理，模仿生物系统通过能量最小化解决问题的方式（详见第 20.5.2 节）。这些系统不是预测最可能的下一个标记，而是找到最小化全局能量函数的配置，这可能使真正关于因果关系的推理成为可能。

从今天的专业系统到明天的通用智能之路，需要在本书涵盖的每个领域取得进展：分布式训练（第八章）必须协调异构架构，硬件加速（第十一章）必须支持不同的计算模式，以及数据工程（第六章）必须综合因果训练示例。最重要的是，第二章的集成原则必须演变，以协调不同的表示框架。

当代 AGI 研究分为四个相互竞争的范例，每个范例都对以下问题提供不同的答案：哪种计算方法将实现通用人工智能？这些范例不仅代表了学术辩论，还暗示了截然不同的工程路径、资源需求和时间预期。

### 缩放假设

缩放假设，由 OpenAI 和 Anthropic 倡导，认为 AGI 将通过持续缩放 Transformer 架构而出现(Kaplan et al. 2020)。这种方法从观察到的缩放定律外推，这些定律揭示了模型性能与三个关键因素之间的稳定、可预测的关系：参数数量 N、数据集大小 D 和计算预算 C。经验上，测试损失遵循幂律关系：对于参数，L(N) ∝ N^(-α)，对于数据，L(D) ∝ D^(-β)，对于计算，L(C) ∝ C^(-γ)，其中α ≈ 0.076，β ≈ 0.095，γ ≈ 0.050 (Kaplan et al. 2020)。这些平滑、可预测的曲线表明，参数数量每增加 10 倍，就能在从语言理解到推理和代码生成等不同任务中带来可测量的能力提升。

最近的发展已经将缩放假设从训练时间的计算扩展到推理时间的计算。OpenAI 的 o1 和 o3 推理模型表明，通过显式的思维链推理和解决方案路径的搜索，允许模型在推理过程中“思考更长”可以显著提高复杂推理任务的表现。这表明一个新的缩放维度：与其仅仅投资于更大的模型，不如将计算分配给扩展的推理，使模型能够处理需要多步推理、规划和自我验证的问题。系统的影响是显著的，因为推理时间的缩放需要与训练时间的缩放不同的基础设施优化。

当将外推扩展到通用人工智能（AGI）规模时，这种外推变得引人注目。如果这些缩放定律持续存在，AGI 的训练将需要大约 2.5 × 10²⁶浮点运算（FLOPs）3，这比 GPT-4 的估计计算预算增加了 250 倍。这不仅仅代表数量的缩放，更是一种质的赌注：足够的规模将诱导出当前模型所缺乏的如稳健推理、规划和知识整合等涌现能力。

这样的规模需要数据中心协调(第八章)和更高的硬件利用率(第十一章)，以使训练在经济上可行。巨大的规模推动了摩尔定律之后的架构探索：3D 芯片堆叠以实现更高的晶体管密度，光互连以减少通信开销，以及在内存中处理以最小化数据移动。

### 混合神经符号架构

尽管规模假设面临一个关键挑战：当前变压器在相关性方面表现出色，但在因果关系方面却力不从心。当 ChatGPT 解释为什么飞机能飞时，它只是从训练数据中复制模式，而不是理解空气动力学原理。这种局限性促使第二个范式产生。

混合神经符号系统结合了用于感知和模式识别的神经网络与用于推理和规划的符号引擎。这种方法认为纯粹的规模扩展无法实现通用人工智能（AGI），因为统计学习与逻辑推理不同（Marcus 2020）。在神经网络擅长于高维空间中的模式匹配时，符号系统通过显式规则操作提供可验证的逻辑推理、约束满足和因果推理。

AlphaGeometry (Trinh et al. 2024) 通过互补优势体现了这种整合。神经网络部分，一个在 1 亿个合成几何问题上训练的变压器，学会了提出有希望的构建步骤（添加辅助线、识别相似三角形），这些步骤将有助于证明。符号部分，一个实现经典几何公理的演绎引擎，严格验证每个建议的步骤并系统地探索逻辑后果。这种劳动分工反映了人类的数学推理：直觉提出有希望的方向，而形式逻辑验证正确性。该系统解决了 30 个国际数学奥林匹克几何问题中的 25 个，其表现与平均金牌得主相当，同时产生了通过符号规则可验证的人类可读证明。

工程神经符号系统需要调和两种计算范式。神经网络部分在通过梯度下降优化的连续表示上操作，而符号部分通过逻辑推理操作离散符号。整合挑战跨越多个层面：表示对齐（向量嵌入与符号结构之间的映射）、计算协调（在基于 GPU 优化的神经网络操作与基于 CPU 的符号推理之间进行调度）和学习同步（通过非可微分的符号操作进行反向传播）。第七章中的框架基础设施必须发展以支持这些异构计算在统一的训练循环中。

### 具身智能

规模和神经符号方法都假设智能可以从不具身的计算中产生。第三个范式挑战了这个假设，认为真正的智能需要在世界中具有物理基础。这种观点源于机器人研究，观察到即使是简单的昆虫在复杂地形中导航也表现出纯符号推理难以复制的行为，这表明感觉运动耦合为智能提供了基本支撑。

具身智能范式，根植于 Brooks 的吸收架构(Brooks 1986)和 Pfeifer 的形态计算(Pfeifer 和 Bongard 2006)，主张智能需要通过连续的感知-动作循环进行传感器-运动定位。这种观点认为，抽象推理是从物理交互中产生的，而不是从无身体的计算中产生的。考虑人类如何通过举起物体而不是通过口头定义来学习“重”，通过具身交互发展直观物理学。语言模型可以背诵“石头比羽毛重”，但没有通过传感器-运动体验理解重量，这可能会限制它们对物理场景的推理。

RT-2（机器人变压器 2）(Brohan 等人 2023)通过视觉-语言-动作模型展示了弥合这一差距的早期进展。通过在包含数百万个机器人轨迹的机器人操作数据集上微调 PaLM-E，一个 562B 参数的视觉-语言模型，RT-2 在新型任务上实现了 62%的成功率，而仅视觉基线为 32%。关键的是，它将互联网规模的知识转移到物理任务中：当展示一张灭绝动物的图片并要求“拿起灭绝动物”时，它正确地识别并抓起了一个玩具恐龙，展示了基于物理能力的语义理解。该架构通过视觉编码器处理图像，与语言指令连接，并输出离散的机器人动作（关节位置，夹爪状态），控制系统执行这些动作。从像素到动作的端到端学习代表了从传统机器人管道中分离感知、规划和控制为不同模块的一种转变。

具身系统面临独特的工程约束，这在纯数字智能中是不存在的，从而形成了一个具有挑战性的设计空间。实时控制回路要求在 100 毫秒以下的推理延迟，以实现稳定的操作，需要从第十四章在设备上部署，而不是在云端进行推理，因为网络往返延迟本身就超过了控制预算。控制层次在多个时间尺度上运行：高级任务规划（10-100 赫兹，“抓住杯子”），中级运动规划（100-1000 赫兹，轨迹生成），以及低级控制（1000+赫兹，带有本体感觉反馈的电机命令）。每一层必须在它的周期时间内完成推理，同时保持安全约束，以防止自碰撞、工作空间违规或可能损坏物体或伤害人类的过度力量。

功率限制与数据中心系统相比施加了严重的限制。一个移动机器人的总功率预算为 100-500 瓦（电池、执行器、传感器、计算）与数据中心仅用于模型推理的兆瓦相比。波士顿动力学的 Atlas 类人机器人将大约 1 千瓦用于液压执行，100-200 瓦用于机载计算，迫使模型压缩和高效架构。这推动了神经形态计算的兴趣：英特尔的 Loihi（Mike Davies 等人 2018）在 1000 倍低于 GPU 的功率下处理视觉注意力任务，使其适用于电池供电系统。功率性能权衡变得至关重要：以 10 赫兹的频率运行 7B 参数模型进行实时推理，在移动 GPU 上需要 50-100 瓦，消耗大量电池容量，将操作时间从小时减少到分钟。

安全关键操作需要超出纯学习系统统计保证的正式验证方法。当特斯拉的全自动驾驶在公共道路上运行或手术机器人操作接近生命器官时，概率性的“大多数时候可能是安全的”证明是不够的。具身 AGI 需要经过认证的行为：系统可以进入的状态的可证明界限，紧急停止的保证响应时间，以及当基于学习的组件失败时的验证回退行为。这促使混合架构结合了用于正常操作的学习策略和硬编码的安全控制器，后者在异常检测时激活，并通过形式方法验证，证明组合系统满足安全规范。随着学习的进行，验证挑战加剧：从经验中持续的适应必须保持安全属性，即使策略在演变。

这些限制虽然令人畏惧，但可能对 AGI（通用人工智能）的发展有利。生物智能在类似的限制下进化，通过感觉运动基础实现了显著的效率。高效的 AGI 可能从资源受限的具身系统中产生，而不是数据中心规模的模式，物理交互提供了必要的归纳偏差，以实现样本高效的学习。具身假设表明，智能从根本上源于在资源受限的环境中行动的智能体，这使得具身方法不仅是通往 AGI 的一条路径，而且可能是任何真正通用智能的必要组成部分。对于复合系统，这表明需要整合处理物理推理的具身组件，即使在以数字架构为主的情况下，也要将抽象概念扎根于感觉运动经验中。

### 多智能体系统和涌现智能

第四范式挑战了智能必须存在于单一实体中的假设。多智能体方法认为，通用人工智能将来自多个专业智能体之间的交互，每个智能体都有独特的功能，在共享环境中操作。这种观点从生物系统中汲取灵感，例如蚂蚁群体、蜂巢和人类社会展示了超越个体能力的集体智慧。没有一只蚂蚁能理解群体的建筑计划，但协调的局部互动产生了复杂的巢穴结构。

OpenAI 的捉迷藏智能体（Baker 等人，2019）展示了竞争如何在没有明确编程的情况下驱动涌现的复杂性。隐藏智能体学会了使用可移动的方块建造堡垒，促使寻找智能体发现工具的使用，推动箱子爬墙。这引发了一场军备竞赛：隐藏智能体学会了将工具锁起来，寻找智能体学会了利用物理漏洞。每种能力纯粹是从竞争压力中产生的，而不是人类的指定，这表明多智能体交互可以启动越来越复杂的行为，朝着通用智能发展。

从系统工程的角度来看，多智能体通用人工智能引入了类似于分布式计算但具有根本差异的挑战。就像分布式系统一样，多智能体架构需要健壮的通信协议、共识机制和容错性，如第十三章所述。第十三章。然而，与传统分布式系统协调执行预定算法的相同节点不同，通用人工智能智能体必须协调异构推理过程，解决冲突的世界模型，并使不同的目标保持一致。像 AutoGPT（Richards 等人，2023）这样的项目展示了早期自主智能体的能力，通过协调网络搜索、代码执行和工具使用来完成复杂任务，尽管当前的实施仍然受到上下文窗口限制和多步骤计划中错误累积的限制。

这四种范式（扩展、神经符号、具身和多智能体）并不需要相互排斥。实际上，最有前景的发展道路可能结合了每个范式的见解：将大量计算资源应用于混合架构，将抽象推理建立在物理或模拟具身之上，同时多个专业智能体协调解决复杂问题。这种融合指向了复合人工智能系统，这是一个可以将这些范式统一到实际应用中的架构框架。

## 复合人工智能系统框架

向通用人工智能（AGI）的轨迹倾向于“复合人工智能系统”(Zaharia 等人 2024)：多个专业组件协同工作而不是单一模型。这种架构范式代表了理解今天的基本构建块如何组装成明天智能系统的组织原则。

现代人工智能助手已经展示了这种复合架构。ChatGPT 集成了用于文本生成的语言模型、用于计算的代码解释器、用于获取当前信息的网络搜索以及用于图像创建的 DALL-E。每个组件在其专业任务上表现出色，而中央协调器通过多种机制协调它们的交互：意图分类根据用户查询确定要激活哪些组件，结果聚合将多个组件的输出组合成连贯的响应，错误处理将失败的运算路由到替代组件或触发用户澄清。

在分析股市趋势时，协调通过多个阶段展开。首先，语言模型解析用户请求以提取关键信息（股票代码、时间范围、分析类型）。其次，它生成 API 调用以网络搜索获取当前价格并检索相关财经新闻。第三，代码解释器接收这些数据并通过 Python 脚本执行统计分析，计算移动平均数、波动性指标或相关性分析。第四，语言模型将这些定量结果与上下文信息综合成自然语言解释。第五，如果用户请求可视化，系统将路由到代码生成以生成 matplotlib 图表。这种协调实现了单个组件无法产生的结果：网络搜索缺乏分析能力，代码执行无法解释结果，而语言模型单独无法访问实时数据。

组织类比阐明了这种架构。一个单一、统一的 AGI 类似于试图让一个人在企业内部执行所有职能：战略、会计、营销、工程和法律工作。这种方法既无法扩展，也无法提供专业化的专业知识。复合人工智能系统类似于一个结构良好的组织，有一个首席执行官（协调器）制定战略并委派任务。专业部门处理不同的职能：研究图书馆管理知识检索，法律团队实施安全和一致性过滤器，工程团队提供专业工具和模型。智能是从这些专业组件的协调工作中产生的，而不是从单一的全知实体中产生的。

复合方法相较于单体模型具有五大关键优势。首先，模块化使得组件可以独立更新而无需对整个系统进行重新训练。当 OpenAI 改进代码解释时，他们只需更换那个模块，而不必触及语言模型，这就像升级显卡而不更换整个电脑一样。其次，专业化允许每个组件针对其特定任务进行优化。使用向量数据库的专用检索系统比试图记住所有知识的语言模型表现更出色，就像专用 ASIC 比通用 CPU 在特定计算上表现更出色一样。第三，可解释性来自于通过组件交互的可追溯决策路径。当系统出错时，工程师可以确定是检索、推理还是生成失败，这在不可透见的端到端模型中是不可能的。第四，可扩展性允许在不进行架构大修的情况下集成新功能。添加语音识别或机器人控制只需添加模块，而不是重新训练万亿参数模型。第五，安全性得益于在每个阶段对输出进行约束的多个专用验证器。一个毒性过滤器检查生成的文本，一个事实验证器验证声明，一个安全监控器防止有害行为。这创建了一个分层防御系统，而不是依赖于单个模型正确行为。

这些优势解释了为什么现在每个主要的 AI 实验室都在追求复合架构。谷歌的 Gemini 2.0 结合了多模态理解和原生工具使用以及代理能力。Anthropic 的 Claude 3.5 集成了宪法 AI 组件、计算机使用能力以及扩展的上下文窗口，从而实现复杂的多步骤工作流程。OpenAI 的 ChatGPT 通过统一的界面协调插件、代码执行、图像生成和网络浏览。这些系统从单一用途助手到多能力代理的快速演变表明，随着能力的成熟，复合架构的采用正在加速。本书中从分布式系统到工作流程编排建立的所有工程原则现在正汇聚起来，以使这些复合系统成为可能。

## 复合智能的构建模块

从单体模型到复合人工智能系统的演变需要我们在数据工程、组件集成和基础设施扩展方面的进步。这些构建块代表了复合智能能否实现人工智能通用智能所需灵活性和能力的决定性推动力。每个组件都针对当前方法的特定局限性进行设计，同时创造跨越数据可用性、系统集成和计算扩展的新工程挑战。

图 20.5 说明了这些构建块如何在复合人工智能架构中整合：专门的数据工程组件向知识检索系统提供内容，动态架构使 LLM 编排器能够通过专家混合模式有效地路由计算，而高级训练范式为实施宪法人工智能原则的安全过滤器提供动力。理解这些构建块各自的作用以及它们的整体整合，为构建明天的智能系统提供了基础。

### 大规模数据工程

数据工程是第一个也是最关键的构建块。复合人工智能系统需要高级数据工程来为其专门的组件提供支持，然而机器学习面临着数据可用性的危机。当检查模型需求的发展进程时，规模变得明显：GPT-3 消耗了 3000 亿个标记（OpenAI），GPT-4 可能使用了超过 1000 万亿个标记（根据扩展定律 4），然而研究估计整个互联网上只有 4.6-17 万亿高质量标记 5。这一进程揭示了一个关键瓶颈：在当前的消费速度下，传统的网络爬虫文本数据可能在 2026 年耗尽，迫使探索合成数据生成和替代扩展路径（Sevilla 等人 2022a）。

三种数据工程方法通过复合系统设计来应对这一挑战：

#### 自监督学习组件

自监督学习使复合人工智能系统能够超越标记数据的瓶颈。虽然监督学习需要对每个例子进行人工标注，但自监督方法通过从原始信息中固有的模式、关系和规律中学习，从数据结构本身提取知识。

生物学的先例是有启发性的。人脑每秒处理大约 10¹¹位感官输入，但每秒只接收少于 10⁴位的明确反馈，这意味着 99.99%的学习是通过自监督模式提取 6 进行的。孩子不是从标记的例子中学习物体恒存性，而是通过观察物体消失和再次出现。他们不是从方程式中学习物理，而是通过观察物体坠落、滚动和碰撞。

Yann LeCun 称自监督学习为“智能的暗物质”(Yann LeCun 2022)，虽然看不见，但构成了学习宇宙的大部分。当前的语模模型仅仅通过下一个标记预测来触及这一表面，这是一种原始形式，它学习的是统计相关性而不是因果理解。当 ChatGPT 在“red”之后预测“apple”时，它利用的是共现统计，而不是苹果具有红色属性的理解。

联合嵌入预测架构（JEPA）7 展示了一种更复杂的方法。JEPA 不是预测原始像素或标记，而是学习世界状态的抽象表示。当 JEPA 看到一个球沿着斜坡滚动的视频时，它不会逐帧预测像素值。相反，它学习表示编码轨迹、动量和碰撞动力学，这些概念可以跨不同对象和场景转移。这种抽象比像素预测实现了 3 倍的样本效率，同时学习到真正可重用的知识。

对于复合系统，自监督学习使每个专业组件能够从其自然数据域中发展专业知识。视觉模块从图像中学习，语言模块从文本中学习，动力学模块从视频中学习，所有这些都不需要人工标记。工程挑战在于协调这些不同的学习过程：确保表示在模态之间对齐，防止组件更新时的灾难性遗忘，并在系统扩展时保持一致性。第七章（ch013.xhtml#sec-ai-frameworks）中的框架基础设施必须发展，以支持在统一训练循环中这些异构的自监督目标。

#### 合成数据生成

复合系统通过引导合成而不是完全依赖人工生成的内容来生成自己的训练数据。这种方法看似矛盾：模型如何从自己那里学习而不退化成模型崩溃，即生成数据越来越多地反映模型偏差而不是真实情况？答案在于三个互补的机制，这些机制可以防止质量下降。

首先，通过外部真实数据进行验证限制了生成。微软的 Phi 模型（Gunasekar 等人 2023）生成合成教科书问题，但通过符号执行、数学证明检查器或代码编译来验证解决方案。生成的代数问题必须有一个唯一、可验证的解；编程练习必须编译并通过测试用例。这创建了一个反馈循环，其中生成器学习产生不仅可能是合理的例子，而且是可验证的正确例子。

其次，基于课程的合成从简单、易于处理的例子开始，并逐步增加复杂性。Phi-2（2.7B 个参数）的性能与 GPT-3.5（175B）相当，因为其合成的训练数据遵循教学进程：先从基础算术开始，然后是微积分，再是简单函数，然后是递归，最后是具体例子，最后是抽象推理。这种结构化的课程使得在无结构网络数据上训练时，较小的模型能够实现需要 65 倍更多参数的能力。

第三，集成验证使用多个独立的模型来过滤合成数据。在生成训练示例时，输出必须满足多个不同的、基于不同数据分布训练的评论员模型。这防止了系统偏差：如果一个生成器持续产生有利于特定模式的示例，那么在多样化数据上训练的集成评论员将识别并拒绝这些有偏差的样本。Anthropic 的宪法 AI 通过迭代优化展示了这一点：一个组件生成响应，多个评论员根据不同的原则（有用性、无害性、事实准确性）对其进行评估，综合产生满足所有标准的改进版本。

对于复合系统，这使专门的数据生成组件能够创建针对其他组件需求的特定领域训练示例。一个推理组件可能为验证组件生成逐步解决方案以进行检查，而代码生成组件为执行组件生成程序以进行验证。

#### 自我玩耍组件

AlphaGo Zero（Silver 等人 2017）展示了复合系统的一个关键原则：组件可以通过自我竞争在没有人类数据的情况下建立专业知识。从完全随机的游戏开始，它仅通过自我玩耍的强化学习在 72 小时内实现了超人类的围棋表现。该机制依赖于三个技术元素，这些元素能够从零知识开始建立专业知识。

第一，自我玩耍通过对手强度跟踪提供自动课程适应性。与使用固定数据集的监督学习不同，自我玩耍随着竞争代理的改进而持续调整难度。当 AlphaGo Zero 与自己对战时，每场比赛都反映了当前的技能水平，创建了定位于略高于当前能力的训练示例。早期游戏探索基本模式；后期游戏揭示了通过人类指令无法指定的微妙战术细节。

第二，搜索引导的探索扩展了有效训练分布，使其超越当前策略可以生成的范围。蒙特卡洛树搜索从每个位置模拟成千上万种可能的结果，发现当前策略不会考虑的强力走法。这些搜索增强的决策成为训练目标，通过迭代改进将策略推向超人类水平。这创造了一个良性循环：更好的策略使搜索更准确，发现更好的训练目标，进而进一步提高策略。

第三，结果验证提供了明确的 学习信号。游戏结果（围棋中的胜负、编码中的解决方案正确性、推理中的辩论胜利）提供了清晰的监督，无需人工标注。一个生成代码的模型可以对测试套件中的数百万个候选程序进行测试，从成功和失败中学习，而无需人工评估。DeepMind 的 AlphaCode 针对每个比赛问题生成超过一百万个程序，通过编译错误和测试失败进行筛选，以识别正确解决方案，从而从成功的程序（正例）和系统性的失败模式（负例）中学习。

这一原则不仅适用于游戏，还可以为复合架构创建专门的系统组件。OpenAI 的辩论模型争论问题的对立面，由裁判模型决定哪个论点更好地支持真理，为论点和评估创建训练数据。Anthropic 的模型通过自我生成的、经过质量评估的评论来批判自己的输出，从而启动改进的响应。这些自播放模式使复合系统能够在无需昂贵的人类监督的情况下生成特定领域的训练数据。

在复合系统中实施这种方法需要处理大规模动态生成的数据管道：管理连续的自生成示例流，通过自动化验证进行质量筛选，并通过多样性指标防止模式崩溃。工程挑战在于在保持探索多样性并防止系统整体收敛到次优模式或对抗性均衡的同时，协调多个自播放组件。

#### Web-Scale Data Processing

高质量精选文本可能有限，但自监督学习、合成生成和自播放创造了新的数据来源。互联网的长尾包含复合系统未开发的资源：GitHub 仓库、学术论文、技术文档和专门论坛。Common Crawl 包含 2500 亿页，GitHub 托管 2000 万多个仓库，arXiv 包含 200 万多篇论文，Reddit 有 30 亿条评论，合计超过 1000 万亿个不同质量的标记。挑战在于提取和质量评估，而不是可用性。

现代复合系统采用复杂的过滤管道（图 20.1），其中专门的组件处理不同的方面：去重减少了网络爬取中的 30-60%冗余，基于精选数据的质量分类器识别高价值内容，特定领域的提取器处理代码、数学和科学文本。这种处理强度体现了数据工程挑战：GPT-4 的训练可能处理了超过 1000 万亿个原始标记，以提取 10-13 万亿个训练标记，这代表了大约 90%的总数据减少：30%来自去重，然后是剩余数据的 80-90%来自质量筛选。

这代表了从批量处理到连续、自适应数据管理的转变，其中多个专业组件协同工作，将原始互联网数据转换为训练准备好的内容。

![图片](img/file321.svg)

图 20.1：**前沿模型的数据工程流程**：多阶段流程将 100+万亿个原始标记转换为 10-13 万亿个高质量训练标记。每个阶段应用越来越复杂的过滤，合成生成增强了最终数据集。这个流程代表了从简单的网络抓取到智能数据管理系统的演变。

图 20.1 中的流程图揭示了一个重要的见解：瓶颈不是数据可用性，而是处理能力。从 111.5 万亿个原始标记开始，积极的过滤将这一数字减少到仅 10-13 万亿个训练标记，超过 90%的数据被丢弃。对于机器学习工程师来说，这意味着提高过滤器的质量可能比收集更多原始数据更有影响。质量过滤器精确度提高 10%可以额外产生一个万亿个高质量标记，相当于将可用的书籍数量翻倍。

这些数据工程方法（合成生成、自我对弈和高级采集）代表了复合人工智能系统的第一个构建块。它们将数据限制从障碍转变为创新的机遇，专用组件持续生成、过滤和处理数据流。

仅生成高质量的训练数据只解决了复合系统挑战的一部分。下一个构建块涉及架构创新，这些创新能够使专用组件在保持系统连贯性的同时进行高效计算。

### 复合系统的动态架构

复合系统需要动态方法，可以根据任务需求和输入特征调整计算。本节探讨了通过选择性计算和复杂的路由机制实现高效专业化的架构创新。专家混合和类似方法允许系统仅激活每个任务相关的组件，提高计算效率同时保持系统能力。

#### 通过选择性计算实现专业化

复合系统面临一个基本的效率挑战：不是所有组件都需要在每个任务中激活。数学问题需要与语言翻译或代码生成不同的处理，然而密集的单体模型无论任务需求如何，都会为每个输入激活所有参数。

考虑 GPT-3 (T. Brown et al. 2020)处理提示“2+2 等于多少？”。尽管这只需要算术推理，而不是语言翻译、代码生成或常识推理，但所有 1750 亿个参数都会被激活。这种激活需要每条前向传递计算 350GB 内存和 350 GFLOPs。通过梯度归因的激活分析揭示，只有 10-20%的参数对任何给定预测有实质性贡献，这表明对于典型输入有 80-90%的计算浪费。在规模扩大时，情况变得更糟：一个假设的 1 万亿参数密集模型将需要每条 2TB 内存和每条 2 TFLOPs，具有类似的利用效率低下。

这种低效在三个维度上累积。内存带宽限制参数从 HBM 加载到计算单元的速度，即使在计算单元空闲时也会造成瓶颈。功耗与激活的参数成正比，无论其贡献如何，都会为对输出影响最小的计算消耗能量。对于密集架构，延迟与模型大小成线性关系，使得超过一定规模后实时应用变得不可行。

生物先例表明了替代方法。人脑大约含有 860 亿个神经元，但并非每个任务都会激活所有神经元。视觉处理主要涉及枕叶皮层，语言处理涉及颞叶区域，运动控制涉及额叶区域。这种稀疏的、特定于任务的激活实现了能源效率：尽管其复杂性可与连接密度达到万亿参数模型的复杂度相媲美，但大脑在 20 瓦的功率下运行。

这些观察结果激励了能够实现系统组件选择性激活的架构设计。复合系统不应激活所有参数，而应将输入路由到相关的专业组件，仅激活每个特定任务所需的子集。这种选择性计算有望在效率、延迟和可扩展性方面实现数量级的改进。

#### 复合系统中的专家路由

混合专家（MoE）架构(Fedus, Zoph, and Shazeer 2021b)在模型级别上展示了复合系统原理：通过智能路由激活的专业组件。MoE 模型由多个专家网络组成，每个网络专门处理不同类型的问题。一个路由机制（学习到的门控函数）确定哪个专家处理每个输入，如图图 20.2 所示。

路由器使用学习到的线性变换后跟 softmax 计算每个专家的概率，通常每个标记选择前两个专家。负载均衡损失确保专家的均匀使用，以防止收敛到少数专家。这种模式自然扩展到复合系统，其中不同的模型、工具或处理管道根据输入特征进行路由。

如 图 20.2 所示，当标记进入系统时，路由器评估哪些专家最相关。对于“2+2=”，路由器将高权重（0.7）分配给算术专家，而对视觉或语言专家则给予零权重。对于“Bonjour means”，它激活了翻译专家。据传言，GPT-4 (OpenAI 等人 2023) 使用了大约 220B 参数的八个专家模型（由 OpenAI 未确认），每个标记激活两个，将活跃计算减少到 280B 参数，同时保持 1.8T 总容量，并实现 5-7 倍的推理速度提升。

这引入了系统挑战：专家之间的负载均衡，防止所有路由都收敛到少数专家，以及管理不规则的内存访问模式。对于复合系统，这些相同的挑战也适用于不同模型、数据库和处理管道之间的路由，需要复杂的编排基础设施。

![图片](img/file322.svg)

图 20.2：**专家混合（MoE）路由**：通过学习路由进行条件计算，能够高效地扩展到万亿参数。路由器（门控函数）确定哪些专家处理每个标记，仅激活相关专家。这种稀疏激活模式降低了计算成本，同时保持了模型容量，尽管它引入了负载均衡和内存访问挑战。

#### 外部内存用于复合系统

除了路由效率之外，复合系统还需要内存架构，其扩展能力超越单个模型限制。如 第 20.5.1 节 详细所述，变换器面临与序列长度成二次关系的内存扩展，限制了推理期间的知识访问，并阻止系统组件之间的长上下文推理。

通过创建多个系统组件可访问的外部内存存储，检索增强生成（RAG）8 解决了这个问题。它不是将所有知识编码在参数中，而是专门的检索组件查询包含数十亿文档的数据库，将相关信息纳入生成过程。这使架构从纯参数化转变为混合参数化-非参数化系统 (Borgeaud 等人 2021)。

对于复合系统来说，这使不同的专门组件可以访问共享的知识库，在多样化的内容类型之间进行高效的相似性搜索，并支持复杂的多步推理过程。

#### 模块化推理架构

多步推理体现了复合系统优势：将复杂问题分解为可验证的组件。虽然单体模型可以直接回答简单问题，但多步问题会产生累积错误（每步 90%的准确率对于 5 步问题来说，整体准确率只有 59%）。GPT-3 (T. Brown 等人 2020) 在复杂推理上的错误率在 40-60%，主要来自中间步骤的失败。

思维链提示（Wei 等人 2022）和模块化推理架构通过分解来解决这个问题，其中不同的组件处理不同的推理阶段。而不是直接生成答案，专门的组件产生中间推理步骤，验证组件可以检查和纠正。思维链提示将 GSM8K 的准确率从 17.9%提高到 58.1%，步骤验证达到 78.2%。

这种架构方法，通过验证将复杂任务分解到专门的组件中，代表了核心复合系统模式：多个专家通过结构化接口而不是单体处理进行协作。

这些创新展示了从静态架构向动态复合系统的转变，这些系统可以路由计算、访问外部内存，并在专门的组件之间分解推理。这种架构基础为 AGI 规模智能所需的复杂编排提供了可能。

动态架构提供了复杂的编排机制，但它们在其底层范例的计算约束内运行。Transformer，当前突破的基础，面临着扩展限制，复合系统必须最终超越这些限制。在考察如何训练和部署复合系统之前，我们必须了解可能形成其计算基础的替代架构范例。

## 适用于通用人工智能（AGI）的替代架构

上文所探讨的动态架构在扩展 Transformer 能力的同时，保留了其核心计算模式：将每个输入元素与每个其他元素进行比较的注意力机制。这种二次扩展随着上下文长度的增加而产生了固有的瓶颈。处理一个包含 10 万个标记的文档需要 100 亿个成对比较，这在计算上非常昂贵，对于许多应用来说经济上也是不可行的。

自回归生成模式限制了 Transformer 只能进行顺序的、从左到右的处理，无法根据后续约束轻松修改早期决策。这些限制表明，实现 AGI 可能需要超越当前范例的架构创新。

本节探讨了三种新兴范式，它们通过不同的计算原理来解决 Transformer 的局限性：用于高效长期上下文处理的状态空间模型、基于能量的模型用于优化驱动推理，以及用于因果理解的领域模型。每个都代表未来复合智能系统潜在的建筑模块。

### 状态空间模型：高效的长期上下文处理

Transformer 的注意力机制将每个标记与每个其他标记进行比较，创建二次缩放：100,000 个标记的上下文需要 1,000 亿次比较（100K × 100K 对注意力分数）。这种 O(n²) 的内存和计算复杂度限制了上下文窗口，使得处理书籍长度的文档、多小时的对话或整个代码库对于实时应用来说成本过高。二次瓶颈来自于注意力矩阵 A = softmax(QKᵀ/√d)，其中 Q, K ∈ ℝⁿˣᵈ 必须计算所有 n² 对相似度。

状态空间模型通过在 O(n) 时间内通过循环隐藏状态更新来处理序列，而不是对所有先前标记进行注意力处理，从而提供了一个有吸引力的替代方案。其基本思想来源于控制理论：维护一个压缩的潜在状态 h ∈ ℝᵈ，该状态总结了所有先前输入，并在新标记到达时增量更新它。从数学上讲，状态空间模型实现了连续时间动力学，用于序列处理：

**连续形式**：<semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mover><mi>h</mi><mo accent="true">̇</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>A</mi><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>B</mi><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>y</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>C</mi><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>D</mi><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable> <annotation encoding="application/x-tex">\begin{aligned} \dot{h}(t) &= Ah(t) + Bx(t) \\ y(t) &= Ch(t) + Dx(t) \end{aligned}</annotation></semantics>

**离散形式:** <semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>h</mi><mi>t</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mover><mi>A</mi><mo accent="true">‾</mo></mover><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mover><mi>B</mi><mo accent="true">‾</mo></mover><msub><mi>x</mi><mi>t</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>y</mi><mi>t</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mover><mi>C</mi><mo accent="true">‾</mo></mover><msub><mi>h</mi><mi>t</mi></msub><mo>+</mo><mover><mi>D</mi><mo accent="true">‾</mo></mover><msub><mi>x</mi><mi>t</mi></msub></mtd></mtr></mtable> <annotation encoding="application/x-tex">\begin{aligned} h_t &= \bar{A}h_{t-1} + \bar{B}x_t \\ y_t &= \bar{C}h_t + \bar{D}x_t \end{aligned}</annotation></semantics>

其中 x ∈ ℝ 是输入标记，h ∈ ℝᵈ 是隐藏状态，y ∈ ℝ 是输出，{A, B, C, D} 是学习参数，它们在这些空间之间进行映射。与受梯度消失/爆炸问题困扰的 RNN 隐藏状态不同，状态空间公式利用结构化矩阵（对角线、低秩或 Toeplitz）通过仔细的初始化和参数化，通过稳定的长距离依赖性。

实现具有竞争力性能的技术突破来自于选择性的状态空间，其中递归参数本身依赖于输入：Āₜ = f_A(xₜ), B̄ₜ = f_B(xₜ)，这使得状态转换输入依赖而非固定。这种选择性允许模型根据当前输入内容动态调整要记住或忘记的信息。当处理“奖杯太大，放不进手提箱”时，模型可以选择性地在状态中维持“奖杯”信息，同时丢弃不那么相关的词语，选择过程由学习到的输入依赖的门控机制驱动，类似于 LSTM 的遗忘门，但处于状态空间框架内。这种方法类似于维护一个运行摘要，根据内容的重要性调整其压缩策略，而不是盲目地平均总结一切。

像 Mamba (A. Gu and Dao 2023)、RWKV (Peng et al. 2023)和 Liquid Time-constant Networks (Hasani et al. 2020)这样的模型表明，这种方法可以在许多任务上匹配 Transformer 的性能，同时与序列长度成线性而不是平方关系进行扩展。使用具有输入相关参数的选择性状态空间，Mamba 在长序列（100K+标记）上的吞吐量比 Transformer 提高了 5 倍。Mamba-7B 在文本上的性能与 Transformer-7B 相当，但在 100K 标记序列上使用的内存减少了 5 倍。后续的发展包括 Mamba-2，进一步提高了效率和质量，而将状态空间层与注意（如在 Jamba 中）结合的混合架构表明，未来可能涉及互补机制而不是全面架构替换。RWKV 结合了 RNN 的高效推理和 Transformer 的可并行训练，而 Liquid Time-constant Networks 根据输入调整其动态，显示出在时间序列和连续控制任务上的特别前景。

系统工程的影响是显著的。线性扩展使得在单个模型调用中处理书籍长度的上下文、多小时的对话或整个代码库成为可能。这需要重新思考数据加载策略（处理 MB 级输入）、内存管理（流式处理而不是批量处理）以及针对顺序处理而非并行注意力的分布式推理模式。

状态空间模型仍然是实验性的。Transformer 得益于整个机器学习系统堆栈多年的优化，从专用硬件内核（FlashAttention、优化的 CUDA 实现）到分布式训练框架（张量并行性、来自第八章的流水线并行性）再到部署基础设施。替代架构不仅必须与 Transformer 的能力相匹配，还必须证明重建此优化生态系统的工程努力是合理的。对于复合系统，混合方法可能最为实用：Transformer 用于受益于并行注意力的任务，状态空间模型用于长上下文顺序处理，通过在第 20.3 节中探索的编排模式进行协调。

### 基于能量的模型：通过优化进行学习

当前语言模型通过一次预测一个标记，并将每个预测条件化在所有先前标记的基础上来生成文本。这种自回归方法在复杂推理方面存在关键限制：它不能轻易根据后续约束来修订早期决策，难以处理需要全局优化的问题，并且往往产生局部一致但全局不一致的输出。

基于能量的模型（EBMs）提供了一种不同的方法：学习一个能量函数 <semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">E(x)</annotation></semantics>，它将低能量分配给可能的或期望的配置 <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>，并将高能量分配给不可能的配置。EBMs 不是直接生成输出，而是通过优化进行推理，寻找最小化能量的配置。这种范式通过其根本不同的计算结构，使得自回归模型无法实现的能力成为可能。

首先，EBMs 通过同时考虑多个相互作用的约束条件，而不是做出序列性的局部决策，从而实现全局优化。当规划一个多步骤项目，其中早期决策限制了后续选项时，自回归模型必须按顺序承诺步骤，而不会根据下游后果进行修订。EBM 可以将整个计划作为优化问题来制定，其中能量函数捕捉所有步骤的约束满足情况，然后通过梯度下降或采样方法搜索全局最优解。对于需要规划、约束满足或多步骤推理的问题，其中局部决策导致全局次优，这种整体优化证明是至关重要的。数独就是一个例子：按顺序填充方格往往会导致需要回溯的矛盾，而将有效的完成作为低能量状态，可以通过约束传播实现高效的解决方案。

其次，能量景观自然地表示了具有不同能量水平的多个有效解，从而能够探索解的多样性。与通过贪婪解码或有限的束搜索承诺单一生成路径的自回归模型不同，EBMs 在整个解空间上维护概率分布。在设计具有所需特性的分子时，多个化学结构可能以不同的权衡满足约束条件。能量函数为每个候选结构分配分数，通过推理采样多样化的低能量配置，而不是收敛到单一输出。这支持了创意应用，其中多样性很重要：为故事生成多个情节变体，探索建筑设计的替代方案，或提出候选药物分子进行合成和测试。

第三，EBMs 支持双向推理，通过推理将信息向前和向后传播。自回归生成流程从开始到结束单向传播，无法根据后续约束修改早期决策。EBMs 通过迭代细化进行推理，可以修改输出中的任何部分以降低全局能量。在写诗时，如果最后一行必须与第一行押韵，EBMs 可以调整早期行以实现令人满意的结论。这种双向能力扩展到因果推理：从观察到的效果推断可能的因果关系，规划实现预期结果的行为，以及通过从错误症状追溯到根本原因来调试代码。推理过程对待所有变量都是对称的，使得在任何方向上都能进行灵活推理。

第四，能量水平通过玻尔兹曼分布 p(x) ∝ exp(-E(x)/T) 提供了基于原理的不确定性量化，其中温度 T 控制置信度校准。能量远高于最小值的解决方案具有指数级降低的概率，提供自然的置信度分数。这支持在不确定环境中的稳健决策：当多个完成选项具有相似的低能量时，模型表达不确定性而不是过度自信地做出任意选择。对于医疗诊断或自动驾驶车辆控制等安全关键应用，知道模型何时不确定可以启用依赖人类判断而不是盲目执行可能不正确的决策。基于能量的框架本质上提供了自回归模型必须通过集成方法或贝叶斯近似单独学习的不确定性估计。

系统工程挑战相当大。推理需要解决可能计算成本高昂的优化问题，尤其是对于高维空间。训练 EBMs 通常涉及对比学习方法，需要通过 MCMC 抽样 9 或其他计算密集型程序生成负例。优化景观可能包含许多局部最小值，需要复杂的推理算法。

这些挑战为系统创新创造了机会。用于优化的专用硬件（量子退火器、光学计算机）可以为 EBM 推理提供计算优势。分层能量模型可以将复杂问题分解为可处理的子问题。混合架构可以将快速自回归生成与 EBM 精细化相结合，以改善解决方案质量。

在复合 AI 系统中，EBMs 可以作为专门的推理组件处理约束满足、规划和验证任务，这些领域是优化方法表现优异的领域。虽然自回归模型生成流畅的文本，但 EBMs 确保逻辑一致性和约束遵守。这种劳动分工利用了每种方法的优点，同时减轻了弱点，体现了第 20.3 节中探讨的复合系统原则。

### 世界模型与预测学习

基于第 20.4.1.1 节中建立的自监督学习原则，真正的 AGI 需要世界模型：学习环境如何工作的内部表示，支持跨多个领域的预测、规划和因果推理。

世界模型是内部模拟，它捕捉因果关系，使系统能够预测行动的后果，对反事实进行推理，并规划向目标序列。虽然当前的 AI 通过预测下一个标记来通过数据预测表面模式，但世界模型理解底层机制。考虑一下区别：一个语言模型学习到“rain”（雨）和“wet”（湿）在文本中经常同时出现，实现了统计关联。而世界模型学习到雨通过吸收和表面湿润导致湿，能够对新型场景进行预测（被覆盖的物体在雨中会湿吗？不会，因为覆盖物阻止了因果机制），这是纯统计模型无法做到的。

技术上的区别体现在表示结构上。自回归模型在序列上维持概率分布：P(x₁, x₂, …, xₙ) = ∏ᵢ P(xᵢ | x₁, …, xᵢ₋₁)，根据历史预测每个标记。世界模型则学习潜在动态：sₜ₊₁ = f(sₜ, aₜ) 将当前状态 sₜ 和动作 aₜ 映射到下一个状态，通过独立的观察模型 o = g(s) 将状态渲染为观察。这种分解使得可以进行前向模拟（预测长期后果）、逆模型（推断产生观察结果的动作），以及反事实推理（如果动作不同会发生什么）。

DeepMind 的 MuZero(Schrittwieser 等人 2020)在游戏中的表现展示了世界模型原则。MuZero 不是通过显式学习规则，而是学习三个函数：表示（将观察映射到隐藏状态）、动态（从当前状态和动作预测下一个隐藏状态），以及预测（从隐藏状态估计价值和策略）。它从没有游戏规则开始，发现某些棋子配置会导致胜利的结果，通过学习因果模型而不是显式规则规范，实现了在象棋、将棋和围棋中的超人类表现。

这种范式转变利用了之前介绍的联合嵌入预测架构（JEPA）框架，超越了自回归生成，朝着理解因果关系的预测智能发展。未来的 AGI 系统不再按顺序生成文本标记，而是在抽象表示空间中预测行动的后果。对于机器人学，这意味着预测物体被推动时的运动（物理世界模型）。对于语言，这意味着根据说话策略预测对话如何演变（社会世界模型）。对于推理，这意味着预测数学陈述如何从公理中得出（逻辑世界模型）。

系统工程挑战涉及多个维度。数据需求大幅增长：学习准确的世界模型需要 PB 级的多模态交互数据，以捕捉多样的因果模式，远超过仅文本的训练。架构设计必须支持多个感官模态之间的时序同步（视觉 30Hz，音频 16kHz，本体感觉 1kHz），这需要仔细的缓冲管理和对齐。训练程序必须能够从流数据中持续学习，而不会发生灾难性遗忘（在第 20.6.0.4 节中探讨的挑战），在环境变化时更新世界模型，同时保留之前学习到的因果关系。

验证提出了独特的挑战。评估世界模型需要测试因果预测，而不仅仅是统计准确性。一个预测“下雨时会出现雨伞”的模型虽然达到了高统计准确性，但在因果上却失败了，因为雨伞不会导致下雨。测试需要干预实验：如果模型认为雨会导致雨伞，那么移除雨伞不应该影响预测的雨。在规模上实施这种因果测试需要超越标准机器学习基准测试的复杂评估基础设施。

在复合系统中，世界模型组件提供因果理解和规划能力，而其他组件则处理感知、动作选择或通信。这种专业化使得能够为特定领域（如机器人学的物理定律、对话的社会动力学、数学的逻辑规则）开发出鲁棒的世界模型，同时保持灵活性，以便将它们组合用于复杂的多领域推理任务。一个家庭机器人可能会使用物理世界模型来预测物体轨迹，社会世界模型来预测人类行为，以及规划算法来按顺序执行操作步骤，以实现期望的结果。

### 混合架构集成策略

上文探讨的范例通过不同的计算方法解决了互补的转换器限制，但没有任何一个代表完全的替代。转换器在并行处理和流畅的自然语言生成方面表现出色，但遭受二次内存缩放和顺序生成约束。状态空间模型实现线性复杂度，但缺乏转换器的表达注意力模式。基于能量的模型实现全局优化，但需要昂贵的推理。世界模型提供因果推理，但需要广泛的多模态训练数据。前进的道路不在于选择一个范例，而在于编排混合复合系统，利用每个架构的优势同时减轻弱点。

从当前研究中出现了几种集成模式。级联架构按顺序将输入路由到专用组件，每个阶段都细化来自前一阶段的输出。一个语言理解管道可能使用转换器进行初始解析，使用世界模型对描述的事件进行因果推理，并使用基于能量的模型进行约束检查和一致性验证。这种顺序专业化使复杂的推理管道成为可能，其中每个组件都贡献了独特的功能。

并行集成方法结合多个架构同时处理输入，通过学习加权或投票机制汇总结果。一个问答系统可能使用转换器生成候选答案，使用基于能量的模型评估逻辑一致性进行评分，并使用预测下游后果的世界模型进行排名。这种冗余提供了鲁棒性：如果一个架构在特定输入上失败，其他架构可能成功。

层次分解将架构分配到不同的抽象级别。高级规划可能使用世界模型来预测长期后果，中级执行可能使用转换器进行动作生成，而低级控制可能使用状态空间模型进行实时响应。这种垂直集成使系统能够同时在不同时间尺度上进行推理，从毫秒级的反射到多小时的计划。

最复杂的集成策略涉及基于输入特性和任务要求的动态路由。协调器分析传入的请求并自适应地选择适当的架构组件。基于数学证明的路由到由转换器提示生成的增强符号推理器。创意写作任务路由到优化流畅生成的转换器。长文档摘要路由到处理扩展上下文的状态空间模型。物理操作规划路由到预测对象动态的世界模型。这种自适应专业化需要元学习系统，这些系统学习哪些架构在特定任务分布中表现卓越。

实施挑战与架构异构性叠加。训练程序必须适应不同的计算模式：Transformer 在序列位置上并行化，循环模型按顺序处理，基于能量的模型需要迭代优化。梯度计算存在根本差异：Transformer 通过确定性操作进行反向传播，世界模型通过学习到的动态进行反向传播，基于能量的模型需要对比估计。来自第七章的框架基础设施必须发展以支持这些多样化的训练范例在统一管道内。

硬件加速也面临着类似的挑战。Transformer 可以高效地映射到针对密集矩阵乘法优化的 GPU 张量核心。状态空间模型受益于具有优化内存访问模式的顺序处理引擎。基于能量的模型需要优化硬件来加速迭代细化。复合系统必须在异构加速器之间协调计算，将不同的架构组件路由到适当的硬件基础之上，同时最小化数据移动开销。

部署和监控基础设施必须跟踪架构组件中的各种故障模式。Transformer 的故障通常表现为流畅度下降或事实错误。基于能量的模型故障表现为优化收敛问题或约束违规。世界模型故障表现为错误的因果预测或计划崩溃。来自第十三章的可观察性系统必须检测和诊断这些不同故障语义中的故障，需要在统一操作框架内采用针对特定架构的监控策略。

来自第 20.3 节的复合人工智能系统框架为管理这种架构异构性提供了组织原则。通过将每个范例视为具有良好定义接口的专用组件，复合系统能够在保持系统一致性的同时实现架构多样性。以下关于训练方法、基础设施要求和操作实践的章节适用于这些架构范例，尽管具体实现取决于计算基础。

## 复合系统的训练方法

复合系统的发展需要超越传统机器学习方法的复杂训练方法。在确保与人类价值观和意图对齐的同时，训练具有多个专用组件的系统需要复杂的方法。从人类反馈中进行的强化学习可以应用于复合架构，而持续学习使这些系统能够通过部署和交互来改进。

#### 组件间的对齐

复合系统面临的对齐挑战建立在负责任的 AI 原则(第十七章)之上，同时超越了当前的安全框架，以解决可能超出人类能力范围的系统：每个专门的组件必须与人类价值观保持一致，而协调者必须适当地协调这些组件。传统的监督学习在模型上产生了不匹配，这些模型是在互联网文本上训练的，学会预测人类会写什么，而不是人类想要什么。GPT-3 对敏感历史提示的完成结果差异很大，一些评估显示在少数情况下有令人担忧的输出，准确地反映了网络内容分布而不是真相。

对于复合系统，任何组件的不对齐都可能损害整个系统：一个检索有偏见信息的搜索组件，一个持续有害刻板印象的推理组件，或者一个未能捕捉到问题内容的过滤安全组件。

#### 组件训练的人类反馈

针对这些对齐挑战，从人类反馈中学习强化学习（RLHF）(Christiano 等人 2017; Ouyang 等人 2022）通过多阶段训练来解决对齐问题，这种训练自然地累积到系统级对齐。RLHF 不仅仅是在文本预测上进行训练，而是在训练流程本身中创建了专门的组件。

该过程通过三个不同的阶段展示了复合系统设计，每个阶段都有具体的技术要求。第一阶段从对高质量演示的监督微调开始。人类注释者编写示例响应，以提示展示所需的行为，为各种任务提供大约 10,000-100,000 个演示。这种初始微调将基础语言模型（仅基于文本预测训练）转变为遵循指令的助手，尽管它并不理解人类对不同响应质量的不同偏好。

第二阶段收集比较反馈来训练奖励模型。而不是在绝对尺度上对响应进行评分（对人类来说难以一致校准），注释者会比较同一提示下的多个模型输出，选择哪个响应更好地满足如有用性、无害性和诚实性等标准。系统为每个提示生成 4-10 个候选响应，由人类进行排名或成对比较。从这些比较中，一个独立的奖励模型学习预测人类偏好，将任何响应映射到一个标量奖励分数，该分数估计了人类的判断。这个奖励模型与保留的人类偏好达到约 70-75%的一致性，提供了自动化的质量评估，无需对每个输出进行人工评估。

第 3 阶段应用强化学习，使用学习到的奖励模型来优化策略。近端策略优化（PPO）(Schulman et al. 2017) 微调语言模型以最大化预期奖励，同时通过 KL 散度惩罚防止过度偏离监督微调的初始化。这个约束至关重要：没有它，模型会利用奖励模型的弱点，生成无意义的输出，这些输出会欺骗奖励预测器，但无法通过真正的人类判断。KL 惩罚β控制这个权衡，通常设置为 0.01-0.1，允许有意义的改进同时保持连贯的输出。每个强化学习步骤生成响应，计算奖励，并更新策略梯度，迭代直到收敛(图 20.3)。

![图片](img/file323.svg)

图 20.3：**RLHF 训练流程**：三个阶段的过程将基础语言模型转化为对齐助手。第 1 阶段使用人类演示进行初始微调。第 2 阶段收集人类偏好以训练奖励模型。第 3 阶段应用强化学习（PPO）以优化人类偏好，同时通过 KL 散度惩罚防止模式崩溃。

图 20.3 的工程复杂性相当大。每个阶段都需要不同的基础设施：第 1 阶段需要演示收集系统，第 2 阶段需要能够并排展示多个输出的排名接口，第 3 阶段则需要仔细调整超参数，以防止策略偏离原始模型太远（如图中所示的 KL 惩罚）。底部的反馈循环代表持续迭代，模型通常要经过多个 RLHF 的循环，每个循环都需要新鲜的人类数据以防止对奖励模型过度拟合。

这种方法带来了显著的改进：具有 1.3B 参数的 InstructGPT (Ouyang et al. 2022) 在人类评估中优于具有 175B 参数的 GPT-310，这表明对齐比规模对用户满意度更重要。对于机器学习工程师来说，这意味着投资于对齐基础设施可能比扩展计算更有价值：一个 100 倍更小的对齐模型优于一个更大的未对齐模型。

#### 宪法 AI：价值对齐学习

人类反馈仍然昂贵且不一致：不同的注释者提供相互冲突的偏好，将人类监督扩展到数十亿次的交互非常具有挑战性 11。宪法 AI (Y. Bai et al. 2022) 通过自动偏好学习来解决这些限制。

与人类排名不同，宪法 AI 使用一套原则（一个“宪法”）来指导模型行为 12。模型生成响应，对其输出与这些原则进行批评，并迭代地修改响应。这个自我改进循环消除了人类瓶颈，同时保持了对齐目标。

![图片](img/file324.svg)

图 20.4：**宪法 AI 自我改进循环**：迭代精炼过程消除了人类反馈瓶颈。每个循环都会将输出与宪法原则进行比较，生成批评，并产生改进版本。经过 5 次迭代，有害内容减少了 95%，同时保持了有用性。最终输出成为下一代模型训练的数据。

该方法利用第十章（ch016.xhtml#sec-model-optimizations）中的优化技术，通过模型通过原则性的自我精炼来提炼自己的知识（图 20.4），类似于知识蒸馏，但由宪法目标而不是教师模型引导。

#### 持续学习：终身适应

部署的模型面临一个限制：它们无法在不重新训练的情况下从用户交互中学习。每次对话都提供了有价值的反馈（纠正、澄清、新信息），但模型在训练后仍然保持冻结状态 13。这导致训练数据和当前现实之间的差距不断拉大。

持续学习旨在在防止灾难性遗忘的同时，从持续交互中更新模型：灾难性遗忘是指学习新信息会抹去先前知识的现象 14。标准的梯度下降法不加区分地覆盖参数，破坏了先前的学习。

解决方案需要受第十四章（ch020.xhtml#sec-ondevice-learning）启发的内存管理，在保护重要知识的同时，使新的学习成为可能。弹性权重巩固（EWC）(Kirkpatrick 等人 2017)通过识别哪些神经网络参数对先前任务至关重要，然后在学习新任务时对这些特定权重的变化进行惩罚来解决这一问题。该技术计算 Fisher 信息矩阵来衡量参数的重要性。具有高 Fisher 信息的参数对先前性能的贡献很大，应该被保留。渐进式神经网络采取不同的方法，通过为新的知识添加全新的路径，同时冻结原始路径，确保先前能力保持完整。记忆回放技术在新训练期间定期复习先前任务中的示例，通过持续练习而不是架构约束来维持性能。

这些训练创新（通过人类反馈进行对齐、原则性自我改进和持续适应）将训练范式从第八章转变为动态学习系统，通过部署而不是训练后保持静态来提高性能。

### AGI 规模系统的生产基础设施

前面的子节讨论了针对通用人工智能（AGI）的新挑战：大规模数据工程、动态架构和复合智能的训练范式。这些领域代表了 AGI 需要超越现有实践的新方法。另外三个构建块（优化、硬件和运营）对于 AGI 系统同样至关重要。这些领域并非需要全新的技术，而是应用并扩展了在前面章节中开发的综合框架。

本节简要概述了优化（第十章）、硬件加速（第十一章）和 MLOps（第十三章）在 AGI 规模系统中的演变。关键洞察：尽管规模和协调挑战显著加剧，但基础工程原理与本书全书中掌握的原则保持一致。

#### 优化：动态智能分配

第十章中的优化技术对于 AGI 具有新的意义，从静态压缩演变为在复合系统组件之间动态分配智能。当前模型通过为每个输入激活所有参数来浪费计算。当 GPT-4 回答“2+2=4”时，它激活了用于量子力学推理的相同万亿参数，就像使用超级计算机进行基本算术一样。AGI 系统需要根据输入复杂性进行选择性激活，以避免这种低效。

专家混合架构（在第 20.4.2.2 节中探讨）展示了一种实现稀疏和自适应计算的方法：通过模型容量的相关子集路由输入。扩展这一原则，自适应计算根据问题难度动态分配计算时间，简单查询只需几秒钟，而复杂推理任务则需大量资源。这需要系统工程来实时评估难度并优雅地跨计算预算进行扩展。

与构建单体模型不同，AGI 系统可以采用蒸馏级联，其中大型前沿模型逐步教授更小、更专业的变体。这反映了人类组织：初级员工处理常规工作，而高级专家解决复杂问题。第十章（ch016.xhtml#sec-model-optimizations）中的知识蒸馏技术使得创建能够保持能力同时降低常见任务计算需求的模型系列成为可能。系统工程挑战在于协调这些层次并将问题路由到适当的计算级别。

第十章（ch016.xhtml#sec-model-optimizations）中的优化原则（剪枝、量化、蒸馏）仍然是基础性的；AGI 系统只是将它们动态地应用于复合架构，而不是静态地应用于单个模型。

#### 硬件：超越摩尔定律的扩展

第十一章（ch017.xhtml#sec-ai-acceleration）中的硬件加速原则提供了基础，但 AGI 规模的需求要求超越摩尔定律的架构，因为传统的硅扩展（Koomey et al. 2011）从大约每年 30-50%的晶体管密度提升（1970-2010）放缓到大约每年 10-20%（2010-2025）15。

训练 GPT-4 级模型已经需要广泛的并行性，通过第八章（ch014.xhtml#sec-ai-training）中的张量、流水线和数据并行技术协调数千个 GPU。AGI 系统需要 100-1000 倍的这种规模，需要多个方面的架构创新。

3D 芯片堆叠和芯片组通过垂直集成和模块化组合来提高密度，而不是水平缩小。三星的 176 层 3D NAND 和 AMD 的多芯片组 EPYC 处理器证明了可行性 16。对于 AGI 来说，这允许以最佳比例混合专用处理器（矩阵单元、内存控制器、网络芯片），并通过先进的冷却技术管理热挑战。

通信和内存瓶颈需要通过光互连和内存中处理架构等新颖解决方案。硅光子学可以实现比电互连低 10 倍的能量，100 Tbps 的带宽，这对于协调 10 万个以上的处理器至关重要 17。内存中处理将数据移动能量降低 100 倍，通过直接在数据所在位置进行计算，解决了限制当前加速器效率的内存墙问题。

通过神经形态和量子混合系统，出现了更长期的发展路径。英特尔 Loihi(Mike Davies 等人 2018)和 IBM 的 TrueNorth 通过脑启发架构展示了事件驱动工作负载的 1000 倍能效。量子-经典混合系统可以加速组合优化（神经网络架构搜索、超参数调整），而经典系统处理梯度计算 18。编程这些异构系统需要复杂的中间件来分解 AGI 工作流程跨越不同的计算范式。

第十一章第十一章中的硬件加速原则（并行性、内存层次优化、专用计算单元）仍然是基础性的。AGI 系统通过摩尔定律之后的创新扩展了这些原则，同时需要跨异构架构前所未有的编排。

#### 操作：持续系统进化

随着 AGI 系统从静态模型发展到动态、持续学习的实体，MLOps 原则第十三章变得至关重要。在 AGI 规模下，三个操作挑战加剧，并转变了我们对模型部署和维护的看法。

持续学习系统在保持安全性和可靠性的同时，实时地从用户交互中更新。这使操作从离散部署（v1.0、v1.1、v2.0）转变为持续进化，其中模型不断变化。传统的版本控制、回滚策略和可重复性保证需要重新思考。操作基础设施必须支持在不停机的情况下进行实时模型更新，同时保持安全性不变量，这是在第十三章中涵盖的静态模型部署所不具备的挑战。

当比较数百万用户的个性化模型变体时，测试和验证变得复杂。第十三章第十三章中的传统 A/B 测试假设每个变体都有一致的经验；AGI 系统引入了复杂性，其中每个用户可能收到一个略有不同的模型。随着能力的扩展，突然出现的涌现行为可能需要检测不同用例中的微妙性能退化。第十三章第十三章中的监控和可观察性原则提供了基础，但必须扩展以检测能力变化，而不仅仅是性能指标。

安全监控需要实时检测有害输出、及时注入和对抗攻击，这些在数十亿次的交互中发生。与传统的软件监控跟踪系统指标（延迟、吞吐量、错误率）不同，AI 安全监控需要理解语义内容、用户意图和潜在危害。这需要结合来自第十六章的鲁棒性原则、来自第十五章的安全实践和来自第十七章的有责任感的 AI 框架的新工具。运营挑战在于大规模部署这些安全系统的同时，保持亚秒级的响应时间。

来自第十三章（持续集成/持续部署、监控、事件响应）的 MLOps 原则仍然至关重要；AGI 系统只是将它们应用于持续演变、个性化的模型，这些模型需要基于语义而非纯粹基于指标的有效性验证。

### 集成系统架构设计

被考察的六个构建块（数据工程、动态架构、训练范式、优化、硬件和运营）必须协同工作，以实现复合 AI 系统，但集成比简单地组装组件更具挑战性。成功的架构需要精心设计的接口、跨层的协调优化，以及全面理解构建块如何相互作用以创建涌现能力或级联故障。

考虑通过一个集成复合系统处理复杂用户查询时的数据流。来自第 20.4.1 节的新颖数据工程管道持续生成合成训练示例，整理网络规模语料库，并启用自我玩耍学习，为不同组件生成专门的训练数据集。这些数据集输入到来自第 20.4.2 节的动态架构中，其中混合专家模型将查询的不同方面路由到专门的组件：数学推理到定量专家，创意写作到语言专家，代码生成到编程模块。每个专家都使用来自第 20.6 节的方法进行训练，包括 RLHF 对齐、宪法 AI 自我改进和持续学习，以适应用户反馈。来自第 20.6.1.1 节的优化技术通过量化减少内存占用、剪枝消除冗余参数和蒸馏将知识转移到更小的部署模型，使这些组件能够高效部署。这个优化模型集在来自第 20.6.1.2 节的异构硬件上运行，结合 GPU 集群进行 Transformer 推理、神经形态芯片进行事件驱动感知和针对符号推理的专用加速器。最后，来自第 20.6.1.3 节的进化 MLOps 通过语义验证监控这个复杂的部署，优雅地处理组件故障，并支持无服务中断的持续学习更新。

关键洞见：这些构建块不能孤立地发展。数据工程决策限制了哪些架构模式是可行的；模型架构决定了优化机会；硬件能力限制了可达到的性能；运营需求反馈影响架构选择。这创造了一个紧密耦合的设计空间，其中跨构建块的协同优化往往比优化任何单个组件带来更大的改进。

具体来说，从生产复合系统中出现了三种整合模式，每种模式都代表了构建块设计空间中的不同权衡。横向整合模式将专业组件分布在共享基础设施层。所有组件访问公共数据管道，在同质硬件集群上部署，并通过标准化 API 进行集成。这种模式最大化了资源共享和操作简单性，但限制了每个组件的优化。Google 的 Gemini 是这种方法的例证：多模态编码器、推理模块和工具集成都在 TPU 集群上运行，共享训练基础设施和部署框架。其优势在于操作效率：一个团队管理着服务于所有组件的基础设施。当组件特定的优化（用于视觉的神经形态硬件、用于逻辑的符号加速器）无法在均匀的基材中利用时，这种限制就会显现出来。

垂直整合模式为每个专业组件定制整个堆栈。一个推理组件可能会在形式逻辑生成器的合成数据上训练，使用基于能量的架构以优化约束满足，部署在量子经典混合硬件上加速组合搜索，并在其操作监控中包含定制验证。一个独立的视觉组件在自监督视频预测上训练，使用卷积或视觉转换器架构，部署在神经形态芯片上进行高效的事件处理，并监控视觉输入中的分布变化。这种模式以管理异构系统的操作复杂性为代价，实现了每个组件的最大优化。Meta 针对不同模态和任务采用不同专用模型的方法，是垂直整合的例证：每个能力领域在整个堆栈中都得到定制处理。

层次整合模式通过分层抽象结合了横向和纵向方法。底层提供共享基础设施（数据管道、训练集群、部署平台），而高层则实现组件特定的定制（架构选择、优化策略、操作策略）。基础模型提供商是这种模式的例证：他们提供基于大规模基础设施训练的基础模型（横向），开发者使用定制数据和优化进行微调（纵向），部署在共享服务基础设施上（横向），并具有定制监控和护栏（纵向）。这种模式在操作效率和优化灵活性之间取得平衡，但在抽象边界引入了复杂性，因为共享基础设施必须满足多样化的定制需求。

在这些模式之间进行选择需要理解系统需求和组织能力。横向整合适合拥有强大基础设施团队但 AI 专业能力有限的组织，为了运营简单，可以接受一些性能上的牺牲。纵向整合有利于在多个领域拥有深厚 AI 专业知识的组织，能够管理复杂性以实现最大性能。分层整合服务于支持多种用例的平台，提供标准基础设施同时允许定制。

随着规模的扩大，工程挑战加剧。一个研究原型可能通过临时脚本和配置文件手动集成构建块。服务于数百万用户的生产系统需要强大的集成框架：声明性规范定义组件如何交互，自动部署管道验证跨构建块的一致性，监控系统检测集成故障，以及更新机制协调构建块之间的变化，而不会破坏依赖关系。这些框架本身成为重要的工程成果，通常在复杂性上与单个构建块相媲美。

重要的是，本书中开发出的工程原理为所有六个构建块提供了基础。AGI 的发展是扩展而不是取代这些原理，它们在前所未有的规模和协调复杂性上应用这些原理。来自第六章的数据工程原理适用于 PB 级语料库。来自第八章的分布式训练技术协调百万 GPU 集群。来自第十章的优化方法使万亿参数部署成为可能。来自第十三章的运营实践确保了可靠的多系统操作。AGI 系统工程是在这些基础上逐步构建的，而不是需要革命性的新方法，尽管规模和协调需求将现有技术推向极限，有时甚至超越极限。

## 复合 AI 系统的生产部署

前面的章节建立了复合 AI 系统所需的构建块：新颖的数据来源和训练范式、解决 Transformer 限制的架构替代方案以及支持异构组件的基础设施。这些构建块为 AGI 开发提供了原材料。本节将探讨如何通过协调生产规模下专用组件的编排模式，将这些材料组装成功能系统。

复合人工智能系统框架提供了概念基础，但在规模上实施这些系统需要复杂的协调基础设施。如 GPT-4 (OpenAI 等，2023) 工具集成、Gemini (G. Team 等，2023) 搜索增强和 Claude 的宪法人工智能 (Y. Bai 等，2022) 实施展示了如何通过专用组件的协调来实现超越单个模型限制的能力。工程复杂性包括管理组件交互、优雅处理故障以及随着组件独立演变而保持系统一致性。理解这些实施模式弥合了概念框架和运营现实之间的差距。

图 20.5 展示了使用特定性能指标来体现的工程复杂性：中心协调器在 10-50 毫秒的决策延迟内将用户查询路由到适当的专用模块，通过 1-10 GB/s 的数据流（根据模式不同：文本：1 MB/s，代码：10 MB/s，多模态：1 GB/s）管理组件间的双向通信，每个组件的迭代优化过程需要 100-500 毫秒的往返时间，并且每个会话使用 1-100 GB 的内存来维护整个交互的会话状态。每个组件代表不同的工程挑战，需要不同的优化策略（LLM：GPU 优化的推理，搜索：分布式索引，代码：安全沙箱），硬件配置（协调器：CPU+内存，检索：SSD+带宽，计算：GPU 集群），以及操作实践（亚秒级延迟 SLA，99.9%可用性，故障隔离）。故障模式包括组件超时（10-30 秒的回退），依赖失败（优雅降级），以及协调死锁（断路器模式）。

![图片](img/file325.svg)

图 20.5：**复合人工智能系统架构**：现代人工智能助手通过中心协调器整合专用组件，使能力超越单体模型。每个模块处理特定任务，而 LLM 协调信息流、决策和响应。这种架构实现了独立扩展、专用优化和多层安全验证。

### 生产系统的协调模式

在生产规模上实施复合人工智能系统需要复杂的协调模式，这些模式在协调专用组件的同时保持可靠性和性能。来自 OpenAI、Anthropic 和 Google 等组织的生产部署中出现了三种基本模式，每种模式都针对组件协调的不同方面。

请求路由模式根据意图分类和能力需求确定哪些组件处理每个用户查询。当用户询问“东京的天气怎么样？”时，协调器分析请求结构，识别所需的能力（实时数据网络搜索、位置解析、单位转换），并将请求路由到适当的组件。这种路由分为两个阶段：使用小型快速模型（10-50 毫秒延迟）进行粗粒度分类，确定广泛类别（事实查询、创意任务、代码生成、多模态请求），然后是细粒度路由，选择特定的组件配置。GPT-4 的工具使用实现展示了这一点：基础模型生成结构化 JSON 的功能调用，验证层检查模式合规性，执行引擎调用外部 API 并具有超时保护，结果集成将输出合并回对话上下文中。路由层维护一个能力注册表，将意图映射到组件组合，随着新组件的部署或现有组件的不稳定而动态更新。

当多个专用模块必须协同工作时，组件协调变得至关重要。协调器状态机模式管理多步骤工作流程，其中某个组件的输出通知后续组件的输入。考虑一个需要跨多个来源综合的研究查询：协调器（1）将问题分解为针对不同方面的子查询，（2）在知识库中并行搜索，（3）按相关性对检索到的段落进行排序，（4）将排名前 k 的段落反馈给包含原始问题的推理组件，（5）验证生成的声明与检索到的证据的一致性，（6）用引文格式化最终响应。每个阶段之间的过渡都需要状态管理跟踪中间结果，处理部分失败，并做出继续决策。协调器在分布式内存（Redis、Memcached）中维护工作流程状态，使组件失败时无需重启整个管道即可恢复。在每个成功阶段之后进行状态检查点，允许在组件超时或返回错误时从最后一个一致状态重新启动。

随着组件数量的增加，错误处理和弹性模式变得至关重要。断路器模式可以防止组件不可靠时发生级联故障。当一个知识检索组件由于数据库过载开始超时时，断路器会跟踪失败率，并在超过阈值（例如，60 秒内超过 30%的失败）后自动禁用该组件。而不是继续压垮失败的组件，协调器会转向回退策略：常见查询的缓存响应、仅从基础模型获得的降级响应，或明确通知用户某些功能暂时不可用。电路状态转换通过三个阶段：关闭（正常操作）、开启（故障触发即时回退）和半开启（定期测试恢复）。Anthropic 的 Claude 实现包括复杂的回退层次结构，其中宪法 AI 过滤器在不同质量/延迟权衡下有多个备用实现，确保即使在首选组件失败时也能进行安全验证。

生产系统根据负载和性能特征实现动态组件缩放。不同的组件面临不同的瓶颈：基础语言模型是计算密集型，需要 GPU 实例；向量搜索是内存带宽密集型，需要高 IOPS 的 SSD；代码执行是隔离密集型，需要沙箱容器。协调器监控组件级指标（延迟分布、吞吐量、错误率、资源利用率）并将缩放决策信号发送到部署基础设施。在高峰时段代码执行请求激增时，Kubernetes 水平扩展容器池，而协调器在可用实例之间负载均衡请求。这需要复杂的排队：高优先级请求（付费客户、关键工作流程）跳到队列的前面，而批量请求可以容忍更高的延迟。协调器跟踪每个用户的请求上下文，实现公平调度，防止单个用户垄断共享资源，同时保持所有用户的服务质量。

监控和可观测性在复合系统中变得指数级复杂。当故障表现为语义退化而非硬错误时，传统的指标如延迟和吞吐量证明是不够的。系统可能执行成功（未抛出异常，返回 200 OK 响应），但输出质量差，因为检索返回了不相关的段落或推理组件产生了幻觉连接。生产可观测性需要语义监控跟踪内容质量以及系统健康。这涉及到多个验证层：自动事实核查将主张与知识库进行比较，一致性检查确保响应不与对话中的先前陈述相矛盾，安全过滤检测有害内容生成，以及校准监控验证置信度分数与实际准确性相匹配。这些验证器异步运行以避免阻塞用户响应，但它们会输入到持续的质量仪表板中，从而能够快速检测细微的回归。当谷歌的 Bard 最初推出时，语义监控检测到某些查询模式导致引用错误增加，触发调查揭示了检索组件问题，这些问题仅凭系统指标是不会暴露的。

随着版本控制和部署，工程挑战加剧。在单体系统中，版本更新是原子的：部署新模型，路由流量，监控，必要时回滚。复合系统有 N 个组件独立演变，创建了版本兼容性复杂性。当基础语言模型更新以改进推理时，它是否与基于旧模型输出分布训练的安全过滤器保持兼容？生产系统维护兼容性矩阵，跟踪哪些组件版本可以一起工作，并实施分阶段推出，一次更新一个组件同时监控交互回归。这需要在复制生产流量模式的生产环境中进行广泛的集成测试，A/B 测试框架比较跨用户群体的复合系统变体，以及自动金丝雀部署管道，逐渐增加新配置的流量同时监控异常。第十三章（ch019.xhtml#sec-ml-operations）中的运营纪律扩展到复合系统，但具有乘法复杂性：N 个组件创建了 O(N²)潜在交互，需要验证。

## 剩余的技术障碍

上文探讨的构建模块（大规模数据工程、动态架构、替代范式、训练方法和基础设施组件）代表了向通用人工智能（AGI）迈出的重大工程进步。然而，诚实的评估表明，这些进步虽然必要，但仍然不足。五个关键障碍将当前的机器学习系统与通用人工智能隔开，每个障碍不仅代表算法挑战，还代表需要在整个堆栈上进行创新的系统工程问题。理解这些障碍可以防止过度自信，同时指导研究重点：一些障碍可能通过巧妙地编排现有构建模块而得到解决；而其他障碍则需要尚未想象的概念创新。

考虑具体的失败案例，这些案例揭示了差距：ChatGPT 可以编写代码，但在长时间的调试会话中无法跟踪变量状态。它可以解释量子力学，但不能从对话中的用户更正中学习。它可以进行语言翻译，但缺乏文化背景，不知道何时直译会误导。这些问题不仅仅是小错误，而是连接这些系统的基本架构限制，任何单个障碍的进展都证明是不够的。

### 记忆和上下文限制

人类的工作记忆可以容纳大约七个项目，而长期记忆则存储了一生的经验(Landauer 1986)。当前的 AI 系统与此相反：transformer 上下文窗口达到 128K 个标记（大约 100K 个单词），但不能在会话之间保持信息。这导致了可以处理书籍的系统，但不能记住昨天的对话。

挑战不仅限于存储，还扩展到了组织和检索。人类的记忆是分层的（日内的事件在年内的层级中）和关联性的（气味触发童年记忆）。当前系统缺乏这些结构，对所有信息一视同仁。向量数据库存储了数十亿个嵌入，但缺乏时间或语义组织，而人类通过关联激活的传播在毫秒内就能从数十年的经验中检索到相关记忆 19。

为了解决这些记忆限制，构建通用人工智能的记忆系统需要从第六章中汲取创新：支持多尺度检索的分层索引、选择性遗忘无关信息的注意力机制，以及将短期交互转化为长期知识的经验巩固。复合系统可能通过具有不同时间尺度和检索机制的专用记忆组件来解决这个问题。

### 能效和计算规模

能耗同样提出了令人畏惧的挑战。GPT-4 的训练估计消耗了 50-100 GWh 的电力(Sevilla 等 2022a)，足以为一万户家庭供电一年 20。外推到通用人工智能（AGI）表明，能源需求将超过小国的产出，从而带来经济和环境挑战。

人类大脑在 20 瓦的功率下进行计算，而当前硬件需要兆瓦的功率才能完成这些计算 21。这种六数量级的效率差距源于架构差异：生物神经元以大约 1 赫兹的有效计算速率使用化学信号进行操作，而数字处理器则以千兆赫兹的频率使用电子开关运行。尽管频率存在劣势，但大脑的广泛并行性（10¹¹个神经元和 10¹⁴个连接）以及模拟处理能力使得数字系统只能通过暴力计算才能实现的模式识别变得高效。这种效率差距，如第 20.2 节中详细说明的特定计算指标所示，不能通过渐进式改进来弥补。解决方案需要重新构想计算，建立在第十八章的基础上：使用脉冲而不是矩阵乘法进行计算的神经形态架构，通过计算回收能量的可逆计算，以及通过数量级减少训练迭代的算法改进。

### 因果推理和规划能力

即使硬件高效，算法限制仍然存在。当前模型在模式完成方面表现出色，但在新颖推理方面却力不从心。要求 ChatGPT 规划一次旅行，它会生成合理的行程。要求它解决需要新推理的问题（证明新的定理或设计实验）时，性能会迅速下降 22。

真正的推理需要当前架构中缺乏的能力。考虑三个关键要求：世界模型代表系统随时间行为的内部模拟——例如，理解抛球会导致其下落，而不仅仅是“抛下”和“落下”在文本中同时出现。搜索机制系统地探索解决方案空间，而不是依赖于模式匹配。找到数学证明需要测试假设和回溯，而不仅仅是识别解决方案模式。因果理解区分相关性和因果关系，认识到雨伞与雨相关，但不会导致雨，而云则会 23。这些能力需要超越第四章中提到的架构创新，可能需要结合神经网络和符号推理器的混合系统，或受认知科学启发的新的架构。

### 符号接地和具身智能

语言模型学习“猫”与“喵喵”和“毛皮”同时出现，但从未体验过猫的温暖或听到它的咕噜声。这个符号接地问题(Harnad 1990；Searle 1980)（将符号与经验联系起来）可能会在没有实体的情况下限制智能。

机器人实体引入了来自第十四章的系统约束：实时推理需求（控制循环小于 100 毫秒），从嘈杂的传感器数据中进行持续学习，以及在错误会导致物理损坏的环境中安全探索 24。这些约束反映了第九章中涵盖的效率挑战，但具有更严格的延迟和可靠性要求。然而，实体可能对于理解“重”、“光滑”或“小心”等基于物理经验的概念是必不可少的。

### 人工智能对齐与价值指定

最关键的障碍在于确保通用人工智能系统追求人类价值观，而不是优化导致有害结果简化的目标 25。当前的奖励函数是代理（最大化参与度，最小化错误），当被强烈优化时可能会产生意外的行为。

对齐需要解决多个相互关联的问题：价值指定（人类实际上想要什么？）、鲁棒优化（追求目标而不利用漏洞）、可纠正性（随着能力增长而保持可修改性）和可扩展的监督（在比监督者更聪明的系统中保持控制权）26。这些挑战跨越技术和哲学领域，需要从第十七章中推进可解释性、形式化验证方法和新的指定和验证目标框架的框架。

**对齐税：安全性的永久性运营成本**

确保通用人工智能系统安全并与人类价值观一致需要大量的、持续的计算资源投资、研究努力和人类监督。这种“对齐税”代表了一种永久性的运营成本，而不仅仅是一次性需要解决的问题。对齐的通用人工智能系统可能有意地不如未对齐的系统计算效率高，因为它们的部分资源将始终用于安全验证、价值对齐检查和自我限制机制。系统必须持续监控自己的行为，验证输出是否符合安全约束，即使在这些检查引入延迟或降低吞吐量时也要保持监督渠道。这把对齐视为一个持续的运营成本，而不是一个需要克服和超越的工程障碍，而是作为在规模上运营值得信赖的智能系统的持续成本。

![](img/file326.svg)

图 20.6：**通用人工智能的技术障碍**：为了实现通用人工智能，必须同时解决五个关键挑战。每个挑战都代表着数量级的差距：记忆系统需要在会话间保持持久性，能效需要提高 1000 倍，推理需要超越模式匹配进行真正的规划，具身化需要符号基础，而对齐需要价值指定。红色箭头显示关键阻塞路径；虚线灰色线条表示关键相互依赖关系。

这五个障碍形成了一个相互关联的挑战网络。任何单个障碍的进展都仍然不足，因为通用人工智能需要跨所有维度的协调突破，如图 20.6 所示。图 20.6 展示了这一点。本书从数据工程（第六章）到分布式训练（第八章），再到稳健部署（第十三章）所发展的工程原理，为解决每个障碍提供了基础，尽管完整的解决方案仍然未知。

这些挑战的规模促使人们重新考虑通用人工智能的组织结构。而不是通过单一系统的整体改进来克服每个障碍，一种替代方法是在多个专业代理之间分配智能，这些代理协作以实现超越任何单个系统的能力。

## 通过多代理协调实现涌现智能

上文概述的技术障碍要求实现数量级的突破，这可能对单一代理架构来说是难以捉摸的。每个障碍都代表着计算或扩展挑战：处理无限上下文，实现生物能效，进行因果推理，在物理具身化中扎根，以及随着能力的扩展保持对齐。在单一系统中同时解决所有障碍将使难度呈指数级增加。

多代理系统提供了一个替代范式，其中智能来自专业代理之间的交互，而不是存在于任何单个系统中。这种方法与复合人工智能系统框架相一致：而不是一个系统解决所有问题，专业组件通过结构化接口协作。多代理系统将这一原则扩展到通用人工智能规模，可能通过分布绕过一些障碍。当专业代理保持特定领域的上下文时，记忆限制就会消失。通过选择性激活提高能效；每个任务只涉及相关的代理。推理通过验证分解到专业代理中。通过分布式物理实现，具身化变得可行。当专业代理有狭窄、可验证的目标时，对齐变得简单。

然而，AGI 规模的多代理系统引入了新的工程挑战，这些挑战远远超过了当前的分布式系统。理解这些挑战对于评估多代理方法是否为 AGI 提供了实际途径，或者只是用未知的协调问题替换了已知的障碍至关重要。

AGI 系统可能需要在跨越大陆的数百万个专业代理之间进行协调，而今天的分布式系统只需要协调数千个服务器 27。每个代理可能都是一个前沿模型规模的系统，消耗千兆瓦的电力，使得协调延迟和带宽成为主要的瓶颈。东京和纽约之间代理之间的通信引入了 150 毫秒的往返延迟，这对于需要毫秒级协调的实时推理来说是无法接受的。

解决这些协调挑战首先需要在不同领域建立代理的专业化。科学推理代理将处理艾字节级的文献，创意代理将生成多媒体内容，战略规划代理将在数十年的时间尺度上优化，而具身代理将控制机器人系统。每个代理在其专业领域内表现出色，同时共享使协调成为可能的通用接口。这反映了现代软件系统如何将复杂功能分解成微服务，但规模和复杂性都是前所未有的。

这种专业化的有效性关键取决于代理之间的通信协议。与交换简单状态更新的传统分布式系统不同，AGI 代理必须通信丰富的语义信息，包括部分世界模型、推理链、不确定性估计和意图表示 28。这些协议必须将复杂的认知状态压缩成网络数据包，同时在异构代理架构中保持语义的准确性。当前的互联网协议缺乏语义理解；未来的 AGI 网络可能需要内容感知路由，它能够理解推理上下文。

除了协议之外，网络拓扑设计对于实现大规模高效通信变得至关重要。而不是扁平的网络架构，AGI 系统可能需要模仿生物神经组织的分层拓扑：用于快速协调的本地代理集群，用于跨领域集成的区域枢纽，以及用于系统整体一致性的全球协调层 29。负载均衡算法必须考虑不仅仅是计算负载，还要考虑语义亲和力，将相关的推理任务路由到具有共享上下文的代理。

这些架构考虑自然地引出关于共识机制的问题，对于 AGI 代理来说，它们面临的复杂性超出了传统的分布式系统。虽然区块链共识涉及简单的状态转换，但 AGI 共识必须处理冲突的世界模型、竞争的推理链和主观的价值判断 30。当科学推理代理在实验解释上意见不一致时，创意代理提出冲突的艺术方向，战略代理推荐对立的政策，系统需要机制来促进有建设性的分歧，而不是强制达成共识。这可能涉及根据过去准确性权衡代理贡献的名誉系统，考虑论证质量而不仅仅是代理数量的投票机制，以及识别分歧表明真正的不确定性还是代理故障的元推理系统。

考虑到拜占庭容错性时，共识挑战加剧，当代理不仅提供错误信息，还可能追求不同目标时，这变得更加困难。与随机的服务器故障不同，代理故障可能是系统的：一个在偏见数据上训练的代理持续提供偏颇的建议，一个目标不一致的代理微妙地操纵其他代理，或者一个被对抗性攻击破坏的代理传播错误信息 31。传统的拜占庭算法需要 3f+1 个诚实节点来容忍 f 个拜占庭节点，但 AGI 系统可能面临复杂、协调的攻击，需要新的防御机制。

最后，数百万代理之间的资源协调需要新的分布式算法，这些算法超越了当前的协调框架。当多个推理链竞争计算资源、内存带宽和网络容量时，系统需要考虑不仅当前负载，还要考虑预测推理复杂度的实时资源分配。这需要超越当前 Kubernetes 协调的进步：基于推理难度估计的预测负载均衡，理解推理紧迫性的优先级系统，以及当资源受限时保持系统一致性的优雅降级 32。

目标是涌现智能：由代理交互产生的能力，单个代理并不具备。就像在群体系统中，行为从简单的规则中涌现出来一样，推理可能从相对简单的协作代理中产生。整体大于部分之和，但这需要通过仔细的系统工程来协调机制。

这种多代理方法需要协调（第五章），强大的通信基础设施，以及对可能导致意外行为的代理交互失败模式的关注。

## 工程通往通用人工智能（AGI）的路径

从当前 AI 系统到通用人工智能的旅程需要更多的不仅仅是理解技术可能性；它需要关于实际机会的战略思考。前面的章节概述了构建块、新兴范式、技术障碍和替代组织结构。这个全面的基础使得能够解决实践 ML 系统工程师的关键问题：这些前沿如何转化为可操作的工程决策？

理解 AGI 的最终挑战在智力上是有价值的，但在操作上是不够的。工程师需要实际指导，将 AGI 前沿与当前工作联系起来：哪些机会现在值得投资，哪些挑战需要首先关注，以及 AGI 研究如何影响今天的生产系统设计。本节架起了 AGI 遥远的前景与近期工程决策之间的差距。

这些构建块的融合（大规模数据工程、动态架构、替代范式、训练方法和摩尔定律之后的硬件）为 ML 系统工程师创造了具体的机会。这些不是几十年后的可能性，而是近期的项目，它们在提升当前能力的同时，朝着 AGI 迈进。同时，导航这些机会需要面对跨越技术深度、运营复杂性和组织动态的挑战。

本节通过近期工程机会及其相应挑战的视角，探讨了从当前系统向 AGI 规模智能过渡的实际途径。目标：为处于塑造未来十年 AI 轨迹位置的系统工程师提供可操作的指导。

### 机会景观：基础设施到应用

从 AGI 构建块中出现了三个机会领域：基础基础设施、使能技术和最终用户应用。

新一代训练平台解决了当前的不效率问题，其中 GPU 集群在训练期间仅达到 20-40%的利用率。将利用率提高到 70-80%将降低 40-60%的训练成本，每年价值数十亿美元。这些平台必须处理需要动态负载平衡的专家混合模型、需要即时编译的动态计算图以及需要实时更新而不中断服务的持续学习管道。多模态处理平台提供对文本、图像、音频、视频和传感器数据的统一处理，而边缘-云混合系统通过智能工作负载分配模糊了本地和远程计算之间的界限。

定制化 AI 系统随着时间的推移学习个人工作流程和偏好，得益于参数高效的微调，成本降低 1000 倍，个人知识库的检索系统以及隐私保护技术。实时智能系统使新的范式成为可能，这些范式要求对话 AI 的响应时间小于 200 ms，自动驾驶车辆小于 10 ms，机器人手术小于 1 ms。可解释 AI 系统将可解释性作为一级约束整合，受到包括欧盟 AI 法案要求和医疗设备审批流程在内的监管要求驱动。

工作流程自动化系统编排多个 AI 组件，以完成科学发现、创意生产和软件开发的全端任务。麦肯锡估计，目前 60-70%的工作包含 30%以上的可自动化活动，但当前的自动化主要由于集成复杂性而不是能力限制，仅覆盖了可能的 5%的工作流程。这些应用建立在复合 AI 系统原则（第 20.3 节）之上，需要来自第五章的编排基础设施。

### AGI 开发中的工程挑战

实现这些机会需要解决跨越多个维度的挑战。这些挑战不是孤立的技术问题，而是需要跨构建块协调解决方案的系统性问题。

#### 技术挑战：可靠性和性能

在 AGI 规模下，超高可靠性要求日益加剧。当训练运行成本数百万美元并涉及数千个组件时，即使 99.9%的可靠性也意味着频繁的故障，会破坏数周的进展。这需要从最近状态重新启动的检查点、恢复机制来挽救部分进展以及组件失败时的优雅降级。将可靠性从 99.9%提升到 99.99%，即故障率降低 10 倍，证明成本不成比例地高昂，需要冗余、预测性故障检测和容错算法。

随着系统必须协调 CPU 进行预处理、GPU 进行矩阵运算、TPU33 进行推理、量子处理器进行优化以及神经形态芯片进行节能计算，异构系统编排变得越来越复杂。这种异构性要求抽象化，以隐藏复杂性给开发者，并需要优化跨不同计算范式的调度算法。当前的框架（TensorFlow、来自第七章的 PyTorch）假设硬件相对同质；AGI 基础设施需要支持多范式编排的新抽象。

随着系统规模的扩大，质量-效率权衡变得更加尖锐。实时系统往往不能使用最先进的模型，因为延迟限制——随着模型能力的增长，这种困境加剧。优化挑战涉及分层处理，其中简单模型处理常规案例，而高级模型仅在需要时激活，自适应算法根据可用时间调整计算深度，以及当精确计算不可能时提供近似结果。

#### 运营挑战：测试和部署

当错误通过长链累积时，对由 AI 驱动的流程进行验证和验证变得困难。早期阶段的一个小错误可能会使后续数小时或数天的工怍无效。这需要自动测试理解 AI 行为模式，检查点系统允许从失败点回滚，以及当不确定性增加时触发人工审查的信心监控。第十三章中的测试框架扩展到处理非确定性 AI 组件和涌现行为。

信任校准决定人类何时应介入自动化系统。完全自动化往往失败，但确定最佳交接点需要理解技术能力和人类因素。挑战包括创建为人类决策提供背景的接口，开发信任校准以便人类知道何时介入，以及在自动化成为主导领域的领域内保持人类专业知识。这借鉴了第十七章中关于人机协作的负责任 AI 原则。

在语义层面的安全监控需要理解内容意图，而不仅仅是系统指标。AI 安全监控必须在数十亿次的交互中实时检测有害输出、提示注入和对抗攻击——这与传统软件监控跟踪延迟、吞吐量和错误率有质的不同。这需要新的工具，结合鲁棒性原则（第十六章）、安全实践（第十五章）和负责任 AI 框架（第十七章）。

#### 社会和伦理考量

AGI 系统放大了现有的隐私和安全挑战（第十五章），同时通过多组件交互和持续学习能力引入了新的攻击向量。隐私和个人化在系统设计中造成了难以调和的紧张关系。个性化需要用户数据（对话历史、工作模式、偏好），而隐私法规和用户期望越来越要求本地处理。挑战在于开发联邦学习和差分隐私技术，这些技术能够在保持隐私保证的同时实现个性化。当前的方法往往为了隐私保护而牺牲了显著的性能——这种权衡必须得到改善，以便得到广泛的应用。

当个性化 AI 系统学会向用户提供他们想听的内容而不是他们需要知道的内容时，过滤泡和偏见放大风险可能会强化有害模式。这限制了接触不同观点和挑战性想法的机会。构建负责任的个人化需要确保系统偶尔引入不同的观点，挑战用户的假设而不是确认信念，并保持个人化过程的透明度。这将在个人化层应用第十七章中的负责任 AI 原则。

可解释性和性能之间产生紧张关系，迫使在模型准确性和人类可解释性之间做出选择。更可解释的模型往往牺牲了准确性，因为为了人类理解所需的约束可能与最优计算模式相冲突。不同的利益相关者需要不同的解释：医疗专业人员希望有详细的因果推理，患者希望有简单的安慰性摘要，监管审计员需要以合规性为重点的解释，研究人员需要能够实现可重复性的技术细节。构建能够适当调整解释的系统需要结合技术专长与用户体验设计。

机遇和挑战的地形相互交织：基础设施平台使个性化实时系统成为可能，这些系统为自动化应用提供动力，但每个机遇都会放大特定的挑战。成功穿越这一地形需要本书中发展起来的系统思维：理解组件如何相互作用，预测故障模式，设计优雅降级，以及平衡相互竞争的约束。从数据管道（第六章）到分布式训练（第八章），再到稳健部署（第十三章）的工程原则，为以前所未有的规模应对这些挑战提供了基础。

## 对机器学习系统工程师的影响

理解本书内容的 ML 系统工程师在 AGI 开发方面处于独特的位置。从数据工程(第六章)到分布式训练(第八章)，再到模型优化(第十章)和稳健部署(第十三章)，这些能力构成了 AGI 基础设施的基本要求。AGI 开发需要涵盖基础设施构建、高效实验工具、安全与对齐系统设计以及可复现的复杂系统交互的全栈能力。

### 将 AGI 概念应用于当前实践

理解 AGI 的发展轨迹有助于今天在常规 ML 项目中做出架构决策。AGI 发展中固有的工程挑战直接映射到本书全书中开发的基础知识。 表 20.1 展示了 AGI 的愿景如何建立在已建立的 ML 系统原则之上，强化了 AGI 发展所需的技能是扩展现有能力而不是取代它们。

表 20.1: **AGI 对核心 ML 系统知识的挑战**: AGI 发展的技术挑战直接建立在本书全书中涵盖的基础工程原则之上。

| **AGI 挑战** | **第…章中的基础知识** |
| --- | --- |
| **大规模数据** | 第六章: 数据工程 |
| **训练范式** | 第八章: AI 训练 |
| **动态架构** | 第四章: DNN 架构 |
| **硬件扩展** | 第十一章: AI 加速 |
| **效率与资源** | 第十章: 高效 AI |
| **管理** |  |
| **开发框架** | 第七章: 框架 |
| **系统编排** | 第五章: 工作流程 |
| **边缘部署** | 第十四章: 设备端学习 |
| **性能评估** | 第十二章: AI 基准测试 |
| **隐私与安全** | 第十五章: 隐私与安全 |
| **能源可持续性** | 第十八章: 可持续 AI |
| **对齐与安全** | 第十七章: 负责任的 AI |
| **操作** | 第十三章: ML 操作 |

三个关键的 AGI 概念直接应用于当前实践。首先，具有专用组件的复合系统通常比单个大型模型表现更好，同时更容易调试、更新和扩展——图 20.5 中的架构适用于协调多个模型、集成外部工具或协调检索与生成的协调。第二，图 20.1 中的数据管道显示前沿模型通过过滤丢弃了超过 90%的原始数据，这表明大多数项目在数据清理和合成生成方面的投资不足。第三，RLHF 管道(图 20.3)表明，通过偏好学习实现的对齐对于任何规模的用户满意度至关重要，从客户服务机器人到推荐引擎。

本教材中涵盖的原则提供了基础；AGI 前沿推动这些原则向其最终表达——分布式系统专业知识、软硬件协同设计知识和人机交互理解变得越来越关键。

## AGI 系统的核心设计原则

AGI 轨迹仍然不确定。突破可能来自意想不到的方向：2017 年，尽管 LSTM 几十年来占据主导地位，但变压器取代了 RNN，状态空间模型以线性复杂度实现了变压器的性能，而量子神经网络可能为特定问题提供指数级的加速。

这种不确定性放大了系统工程的价值。无论是否存在架构突破，成功的方法都需要高效的数据处理管道来处理埃字节级的数据集，可扩展的训练基础设施来支持百万 GPU 集群，优化跨异构硬件的模型部署，确保 99.99%可用性的稳健操作实践，以及集成安全和伦理框架。

本教材中涵盖的分布式系统、高效部署和稳健操作的系统方法，无论 AGI 是从扩展的变压器、复合系统还是全新的架构中产生，都仍然是必要的。工程原则超越了特定技术，为任何技术轨迹上的智能系统构建提供了基础。

## 谬误和陷阱

通往通用人工智能的道路提出了独特的系统工程挑战，其中对有效方法的误解已经使项目脱轨、浪费资源并产生了不切实际的期望。了解不应该做什么与了解正确的方法一样有价值，尤其是当每个谬误都包含足够的真理以显得有说服力，同时忽略了关键工程考虑时。

**谬误：** *一旦模型在参数和训练数据规模上达到足够大，AGI 将自动出现。*

这种“规模就是一切”的误解导致团队相信，当前人工智能的限制仅仅是模型规模不足的反映，而增大模型规模必然会导致 AGI 的出现。虽然经验性的规模定律显示出持续改进（GPT-3 的 1750 亿参数在基准测试中显著优于 GPT-2 的 15 亿参数），但这种推理忽略了架构创新、效率提升和训练范式进步同样至关重要的因素。人脑通过 860 亿个神经元(Azevedo 等人，2009)实现智能，与中等规模的语言模型通过复杂的架构和学习机制相当，而不是仅仅依靠规模，这比当前的人工智能系统展示了 10⁶倍的能源效率。将 GPT-3(T. Brown 等人，2020)从 1750 亿参数扩展到假设的 1750 万亿参数将需要 100 亿美元的培训成本，消耗相当于一个小镇年度电力的 5 GWh，但仍然缺乏持续的记忆、高效的持续学习、多模态基础和稳健推理，这些都是 AGI 所必需的。有效的 AGI 开发需要平衡在更大规模训练运行中的基础设施投资与通过专家混合(第 20.4.2.2 节)、检索增强(第 20.4.2.3 节)和模块化推理(第 20.4.2.4 节)模式的研究投资，这些模式能够实现仅通过纯粹扩展无法达到的能力。

**谬误：** *复合人工智能系统只是临时解决方案，真正的通用人工智能（AGI）将会使其过时。*

认为 AGI 将是一个单一统一模型，使得复合系统（模型的组合、工具、检索和数据库）变得不必要的信念，忽视了计算机科学关于模块化架构的原则。虽然复合系统通过多个组件、接口和故障模式引入了复杂性，但具有专用组件的模块化架构能够实现独立优化、优雅降级、增量更新和可调试的行为，这对于任何规模的生成系统都是必不可少的。即使是生物智能也使用专门的神经网络来处理视觉、运动控制、语言和记忆，这些通过结构化接口而不是单一处理进行协调。GPT-4 的代码生成精度在增加了代码执行、语法检查和测试验证等复合组件后，从 48%提高到 89%，这些组件验证和细化了输出。这种模式在检索增强中普遍适用，使得当前知识访问、工具使用实现精确计算以及安全过滤器确保适当行为成为可能，这些能力在基础模型大小方面仍然是必不可少的。生产级 AGI 系统需要接受复合架构作为核心模式，投资于编排基础设施（第五章）、组件接口和组合模式，这些模式建立了对于 AGI 规模部署至关重要的组织实践。

**谬误：** *AGI 需要全新的工程原则，使得传统的软件工程变得无关紧要*。

这种误解认为，AGI 前所未有的能力需要放弃现有的机器学习系统实践，转而采用与当前工程不同的革命性方法。AGI 扩展而非取代系统工程的基本原则，随着架构的演变，分布式训练（第八章）、高效推理（第十章）、稳健部署（第十三章）和监控仍然至关重要。训练 GPT-4（OpenAI 等，2023）需要通过复杂的分布式系统工程，应用第八章（ch014.xhtml#sec-ai-training）中的张量并行、流水线并行和数据并行来协调 25,000 个 GPU，而 AGI 规模系统将需要 100-1000 倍于此的协调。工程师在追求“革命性的 AGI 工程”时忽视分布式系统原则，将重新创造几十年前关于一致性、容错性和性能优化的宝贵经验教训。有效的 AGI 开发需要掌握数据工程（第六章）、训练基础设施、优化、硬件加速（第十一章）以及通过强大的软件工程实践、分布式系统专长和 MLOps 纪律来满足 AGI 要求的运营，而不是放弃经过验证的原则。

**陷阱：** *将生物智能视为 AGI 实施的完整模板。*

许多团队认为，在硅中精确复制生物神经机制是通往通用人工智能的完整路径，这吸引了大脑惊人的能效（每秒 10¹⁵次操作消耗 20 瓦）和神经形态计算在特定工作负载上 1000 倍的效率提升。虽然生物原理在事件驱动计算、分层发展和多模态集成方面提供了宝贵的见解，但生物和硅基板在不同的物理上运行，具有不同的优势。数字系统在精确算术、可靠存储和快速通信方面表现出色，这是生物神经元无法比拟的，而生物神经元则实现了模拟计算、大规模并行和低功耗操作，这在数字电路中很难实现。像英特尔 Loihi 这样的神经形态芯片（Mike Davies 等人，2018）在事件驱动工作负载（如目标跟踪和手势识别）上实现了令人印象深刻的效率，但在 GPU 擅长的密集矩阵运算方面却力不从心。最佳的通用人工智能架构可能需要混合方法，提取生物原理（稀疏激活、分层学习、多模态集成、持续适应）同时利用数字优势（精确算术、可靠存储），而不是直接复制。有效的工程重点在于计算原理，如事件驱动处理和发育学习阶段，而不是生物实现细节。

## 摘要

人工智能正处于一个拐点，本书中掌握的构建块将组装成具有非凡能力的系统。大型语言模型表明，通过从当前成就到本章探索的未来可能性所进行的系统进步，工程规模可以解锁涌现智能。

从窄人工智能（narrow AI）到通用人工智能的过渡构成了一个系统工程挑战，它超越了算法创新，涵盖了在前所未有的规模上整合数据、计算、模型和基础设施。正如第 20.2 节中详细说明的，AGI 的训练可能需要 2.5 × 10²⁶浮点运算次数（FLOPs），并需要支持 175,000+加速器的基础设施，消耗 122 兆瓦的电力，并需要大约 522 亿美元硬件成本。

复合人工智能系统为这一过渡提供了架构基础，揭示了如何通过智能编排解决复杂问题，而不是通过单体扩展。

**关键要点**

+   当前的人工智能突破（如大型语言模型、多模态模型）直接建立在前面章节中确立的机器学习系统工程原则之上

+   通用人工智能（AGI）代表了需要跨多个组件和技术进行复杂协调的系统集成挑战

+   复合人工智能系统提供了结合专门模型和工具以实现复杂能力目标的实际途径

+   从分布式训练到高效部署，开发出的工程能力构成了必要的人工智能通用（AGI）开发要求

+   未来进步来自于系统工程改进与算法创新同等重要

这本教科书为读者准备参与这一挑战。理解包括系统中的数据流（第六章）、模型优化和部署（第十章）、计算硬件加速（第十一章）以及大规模可靠机器学习系统操作（第十三章）。这些能力构成了下一代智能系统构建的要求。

人工智能通用（AGI）的到来时间仍然不确定，无论是通过扩展的变压器还是新颖的架构。无论时间表或技术方法如何，系统工程原则始终是必要的。人工智能的未来建立在贯穿这些章节的工具和技术之上，从第三章中的神经网络原理到第五章中的高级系统编排。

基础设施已完全建成，这是通过系统性地掌握机器学习系统工程从数据管道到分布式训练再到稳健部署的过程实现的。

* * *
