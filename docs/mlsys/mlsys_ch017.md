# AI 加速

*DALL·E 3 提示：创建一个复杂且色彩丰富的矩形格式芯片级联（SoC）设计图。展示各种专用机器学习加速器和芯片片，所有这些都被集成到处理器中。提供芯片内部的详细视图，突出电子的快速运动。每个加速器和芯片片都应设计为与神经网络神经元、层和激活相互作用，强调其处理速度。将神经网络描绘为相互连接的节点网络，展示加速器部件之间充满活力的数据流，展示增强的计算速度。*

![](img/file179.png)

## 目的

*是什么使得专用硬件加速不仅有益，而且对于实际机器学习部署来说是必不可少的，这为什么代表了我们在计算系统设计方法上的根本转变？*

实际的机器学习系统完全依赖于硬件加速。没有专用处理器，计算需求在经济和物理上都是不可行的。通用 CPU 在神经网络操作中只能达到 100 GFLOPS1 (Sze 等人 2017a)，而现代训练工作负载需要每秒数万亿次的操作，这造成了传统扩展无法弥合的性能差距。硬件加速将计算上不可能的任务转化为实际部署，使全新的应用类别成为可能。与现代 AI 系统合作的工程师必须了解加速原理，以利用 100-1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics> 的性能提升，使实时推理、大规模训练和边缘部署在经济上可行。

**学习目标**

+   跟踪硬件加速从浮点协处理器到现代 AI 加速器的演变过程，并解释推动这一进程的架构原则

+   对 AI 计算原语（向量操作、矩阵乘法、阵列波前传播）进行分类，并分析它们在当代加速器中的实现

+   评估 AI 加速器的内存层次结构设计，并使用带宽和能耗指标预测其对性能瓶颈的影响

+   设计将神经网络层映射到专用硬件架构的映射策略，考虑数据流模式和资源利用权衡

+   应用编译器优化技术（图优化、内核融合、内存规划）将高级 ML 模型转换为高效的硬件执行计划

+   比较多芯片扩展方法（芯片片、多 GPU、分布式系统）并评估它们对不同 AI 工作负载特性的适用性

+   批判关于硬件加速的常见误解，并识别加速器选择和部署策略中的潜在陷阱

## AI 硬件加速基础

现代机器学习系统对通用处理器的底层架构假设提出了挑战。虽然前一章中探讨的软件优化技术通过精度降低、结构剪枝和执行优化等手段提供了系统化的算法效率方法，但它们在现有计算基础结构的约束下运行。传统的 CPU 在执行典型的机器学习工作负载时，利用率仅为 5-10% (Gholami 等人 2024)，这是由于顺序处理模型与神经网络计算高度并行、数据密集的本质之间存在架构不匹配。

这种性能差距推动了计算机架构向特定领域硬件加速的转变。硬件加速补充了软件优化，通过架构重设计而不是算法修改来解决效率限制。机器学习算法和专用计算架构的协同进化，使得从高性能计算系统上进行的计算成本高昂的研究过渡到在多样化的计算环境中无处不在的部署成为可能，从超大规模数据中心到资源受限的边缘设备。

机器学习系统的硬件加速位于计算机系统工程、计算机架构和应用机器学习的交汇点。对于开发生产系统的从业者来说，关于包括图形处理单元、张量处理单元和神经形态处理器在内的加速器技术的架构选择决策，直接决定了系统级性能特征、能效配置文件和实现复杂性。在自然语言处理、计算机视觉和自主系统等领域的部署系统，与通用实现相比，性能提高了两个到三个数量级。

本章探讨了机器学习系统的硬件加速原理和方法。分析从特定领域计算架构的历史演变开始，展示了从浮点协处理器到图形处理单元的设计模式如何指导当代人工智能加速策略。然后我们讨论了表征机器学习工作负载的计算原语，包括矩阵乘法、向量操作和非线性激活函数，并分析了通过诸如脉动阵列架构和张量处理核心等创新来优化这些操作的专用硬件的架构机制。

在数据移动能耗通常超过计算能耗两个数量级的情况下，内存层次结构设计在加速有效性中起着至关重要的作用。本分析涵盖了内存架构设计原则，从片上 SRAM 缓冲区优化到高带宽内存接口，并探讨了最小化能耗密集型数据移动模式的方法。我们还讨论了编译器优化和运行时系统支持，这些因素决定了理论硬件能力转化为可测量的系统性能的程度。

本章以需要超出单芯片实现计算能力的系统扩展方法结束。从芯片级集成到分布式仓库规模系统，多芯片架构在计算并行性和芯片间通信开销之间引入了权衡。通过对包括 NVIDIA GPU 架构、谷歌 Tensor Processing Units 和新兴神经形态计算平台在内的当代系统的详细分析，我们确立了在多样化的系统环境中有效部署 AI 加速的理论基础和实践考虑。

## 硬件专用化的演变

计算架构遵循一个反复出现的模式：随着计算工作负载的复杂性增加，通用处理器变得越来越低效，这促使专用硬件加速器的开发。对更高计算效率、降低能耗和优化特定领域工作负载执行的需求推动了这一转变。机器学习加速代表了这一持续演变的最新阶段，这一阶段遵循了先前领域（如浮点运算、图形处理和数字信号处理）观察到的轨迹。

这种进化过程为理解现代 ML 加速器（包括具有张量核心（加速矩阵运算的专用单元）的 GPU、谷歌的 TPU2 和苹果的 Neural Engine）如何从既定的架构原则中产生提供了背景。这些技术使得广泛部署的应用成为可能，如实时语言翻译、图像识别和个性化推荐。使这些能力成为可能的架构策略源于数十年的硬件专用化研究和开发。

硬件专用化构成了这一转变的基础，通过优化频繁执行的计算模式并通过专用电路实现来提高性能和效率。虽然这种方法带来了显著的收益，但它也在灵活性、硅面积利用率和编程复杂性方面引入了权衡。随着计算需求不断演变，专用加速器必须平衡这些因素，以实现效率和性能的持续改进。

硬件专业化的发展为理解现代机器学习加速器提供了视角。塑造早期浮点图形加速器发展的许多原则现在正影响着 AI 专用硬件的设计。审视这些历史趋势为分析当代 AI 加速方法提供了一个框架，并预测专用计算的未来发展。

### 专用计算

向专用计算架构的转变源于通用处理器的局限性。早期的计算系统依赖于中央处理单元(CPU)按顺序执行所有计算任务，采用一种一刀切的方法。随着计算工作负载的多样化和复杂性增长，某些操作，尤其是浮点运算，成为性能瓶颈，这些瓶颈无法仅通过 CPU 有效处理。这些低效率促使开发出专门设计的硬件架构，以加速特定的计算模式(Flynn 1966)。

硬件专业化的早期例子之一是 1980 年推出的英特尔 8087 数学协处理器 3。这个浮点单元(FPU)被设计用来从主 CPU 卸载密集型算术计算，显著提高了科学和工程应用的性能。8087 展示了前所未有的效率，与通用处理器上基于软件的实现相比，浮点操作的性能提升了高达 100 倍(Fisher 1981)。这一里程碑在计算机架构中确立了一个原则：精心设计的硬件专业化可以为定义明确、计算密集型任务提供数量级的改进。

浮点协处理器的成功 4 最终导致了它们被集成到主流处理器中。1989 年发布的英特尔 486DX 集成了片上浮点单元，消除了对外部协处理器的需求。这种集成提高了处理效率，并在计算机架构中确立了一个反复出现的模式：成功的专用功能成为后续一代通用处理器的标准特性(David A. Patterson and Hennessy 2021c)。

早期浮点加速确立的原则继续影响着现代硬件专业化：

1.  通过工作负载分析识别计算瓶颈

1.  频繁操作专用电路的开发

1.  高效的软硬件接口的创建

1.  确认的专用功能的渐进式集成

从特定领域专业化到通用集成的这一进展塑造了现代计算架构。随着计算工作负载超越了算术运算，这些核心原则被应用于新的领域，如图形处理、数字信号处理，最终是机器学习加速。每个领域都引入了针对其独特计算需求的专用架构，确立了硬件专业化作为提高计算性能和效率在日益复杂的工作负载中的方法。

专用计算硬件的演变遵循一致的轨迹，其中引入了架构创新来解决新兴的计算瓶颈，随后被纳入主流计算平台。如图 11.1 所示，每个计算时代都产生了针对该时期主导工作负载特性的加速器。这些发展提高了架构效率，并塑造了当代机器学习系统运行的基础。如实时语言翻译、个性化推荐和设备端推理等任务所需的计算能力，依赖于在早期领域（包括浮点计算、图形处理和数字信号处理）中建立的基石性原则和架构创新。

![图片](img/file180.svg)

图 11.1：**硬件专业化轨迹**：计算架构逐步融入专用加速器，以应对新兴的性能瓶颈和工作负载需求，这与从浮点单元到图形处理器，最终到机器学习加速器的历史模式相呼应。这种演变反映了一种通过针对特定任务特性定制硬件并推进日益复杂的应用来提高计算效率的策略。

### 并行计算和图形处理

通过浮点加速建立的原则为解决新兴的计算挑战提供了蓝图。随着计算应用的多样化，出现了新的计算模式，这些模式超出了通用处理器的功能。这种专用计算的发展体现在多个领域，每个领域都为硬件加速策略贡献了独特的见解。

图形处理在 20 世纪 90 年代成为硬件专业化的主要驱动因素。早期的图形加速器专注于特定的操作，如位图传输和多边形填充。1999 年，随着 NVIDIA 的 GeForce 256 引入可编程图形管道，代表了专用计算的重大进步。图形处理单元（GPU）展示了并行处理架构如何高效地处理数据并行工作负载，在 3D 渲染任务如纹理映射和顶点变换中实现了 50-100 倍的速度提升。到 2004 年，高端 GPU 每秒可以处理超过 1 亿个多边形(Owens et al. 2008)。

同时，数字信号处理（DSP）处理器建立了具有专用乘加单元和优化过滤和变换操作的循环缓冲区的并行数据路径架构。德州仪器的 TMS32010（1983 年）展示了特定领域的指令集如何显著提高信号处理应用的性能(Lyons 2011)。

网络处理引入了额外的专业化模式。网络处理器开发了独特的架构来以线路速率处理数据包，包括多个处理核心、专门的包操作单元和复杂的内存管理系统。英特尔 IXP2800 网络处理器展示了如何将多个级别的硬件专业化结合起来以满足复杂的处理需求。

这些多样化的专业化领域表现出几个共同的特征：

1.  识别特定领域的计算模式

1.  开发专门的处理器和内存层次结构

1.  创建特定领域的编程模型

1.  向更灵活架构的渐进式进化

这一扩展专业化的时期表明，硬件加速策略可以满足多个领域多样化的计算需求。GPU 在并行化 3D 图形管道方面的成功使其随后被用于训练深度神经网络，例如 2012 年的 AlexNet5，它在消费级 NVIDIA GPU 上运行。低功耗信号处理中的 DSP 创新促进了边缘设备上的实时推理，包括语音助手和可穿戴设备。这些领域为 ML 硬件设计提供了信息，并确立了加速器可以部署在云和嵌入式环境中的原则，这些原则继续影响着当代 AI 生态系统的发展。

### 特定领域架构的出现

特定领域架构（DSA）6 的出现标志着计算机系统设计的转变，由两个因素驱动：传统缩放定律的崩溃和专用工作负载计算需求的增加。摩尔定律 7 的放缓，该定律以前确保了每 18 到 24 个月晶体管密度的可预测增强，以及 Dennard 缩放 8 的结束，它允许频率增加而不伴随功率增加，在通用计算中创造了性能和效率瓶颈。正如约翰·亨尼斯和戴夫·帕特森在 2017 年的图灵讲座(亨尼斯和帕特森 2019)中指出的，这些限制预示着计算机架构新时代的到来，这一时代以特定领域解决方案为中心，这些解决方案优化硬件以适应专用工作负载。

历史上，处理器性能的提升依赖于半导体工艺的缩小和时钟速度的提高。然而，随着功率密度限制进一步限制了频率提升，并且随着晶体管小型化遇到越来越多的物理和经济约束，架构师探索了替代方法以维持计算增长。这导致了向特定领域架构的转变，这些架构将硅资源专门用于优化特定应用领域的计算，以效率换取灵活性。

特定领域架构通过以下几个关键原则实现卓越的性能和能效：

1.  **定制数据路径**：设计专门针对目标应用模式优化的处理路径，使常见操作能够直接在硬件上执行。例如，人工智能加速器中的矩阵乘法单元实现了脉动阵列——由处理单元组成的网格状网络，这些单元以节奏计算并通过相邻单元传递数据，专门用于神经网络计算。

1.  **专用内存层次结构**：针对特定领域的访问模式和数据重用特性优化内存系统。这包括定制的缓存配置、预取逻辑和针对预期工作负载优化的内存控制器。

1.  **减少指令开销**：实现特定领域的指令集，通过将常见操作序列编码为单个指令来最小化解码和调度复杂性。这提高了性能和能效。

1.  **直接硬件实现**：创建专用电路块，这些电路块能够原生执行常用操作，无需软件干预。这消除了指令处理开销，并最大化了吞吐量。

这些原则在现代智能手机中得到了令人信服的展示。现代智能手机可以在仅消耗几瓦功率的情况下，以每秒 60 帧的速度解码 4K 视频，尽管视频处理每秒需要数十亿次操作。这种效率是通过实现行业标准的专用硬件视频编解码器实现的，如 H.264/AVC（2003 年推出）和 H.265/HEVC（2013 年完成）(Sullivan et al. 2012)。这些专用电路与基于通用处理器的软件解码相比，在性能和功耗效率方面提供了 100-1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的改进。

专业化趋势持续加速，新的架构不断涌现，以适应不断扩大的领域范围。基因组处理得益于定制加速器，这些加速器优化了序列对齐和变异调用，从而减少了 DNA 分析所需的时间（Shang, Wang, and Liu 2018）。同样，区块链计算产生了针对加密散列优化的专用集成电路（ASICs）9，显著提高了挖矿操作的效率（Bedford Taylor 2017）。这些例子表明，特定领域的架构代表了计算系统中的根本性变革，提供了针对现代计算工作负载日益复杂性和多样性的定制解决方案。

### 机器学习硬件专用化

机器学习构成了一个具有独特特征的计算领域，这些特征推动了专用硬件架构的发展。与表现出不规则内存访问模式和多样化指令流的传统计算工作负载不同，神经网络的特点是具有可预测的模式：密集的矩阵乘法、规律的数据流以及对降低精度的容忍。这些特性使得专用硬件优化成为可能，这对于通用计算是无效的，但为机器学习工作负载提供了显著的加速。

**机器学习加速器**是针对神经网络**计算模式**优化的专用计算硬件，通过**并行处理**、**专用内存层次结构**和**降低精度算术**实现了卓越的**性能每瓦**。

机器学习的计算需求揭示了传统处理器的局限性。CPU 在神经网络工作负载上仅实现 5-10%的利用率，提供大约 100 GFLOPS10，同时消耗数百瓦的功率。这种低效源于架构不匹配：CPU 优化单线程性能和不规则内存访问，而神经网络需要大规模并行和可预测的数据流。内存带宽 11 限制变得尤为严重：单个神经网络层可能需要访问数 GB 的参数，这超出了为千字节规模的工作集设计的 CPU 缓存层次 12。

数据移动的能量经济学影响了加速器的设计。从 DRAM 访问数据需要大约 640 皮焦耳，而执行乘累加操作仅消耗 3.7 皮焦耳，大约是 173 倍的惩罚（具体数值因技术节点和设计而异），这确立了最小化数据移动作为主要优化目标。这种差异解释了从改用的图形处理器到专门设计的神经网络加速器的演变。GPU 通过大规模并行处理实现 15,000+ GFLOPS，但遭遇了来自其图形传统的效率挑战。TPU 和其他定制加速器通过实现收缩阵列和其他最大化数据重用同时最小化移动的架构，实现了超过 85%的利用率。

训练和推理呈现不同的计算特征，这影响了加速器的设计。训练需要高精度算术（FP32 或 FP16）进行梯度计算和权重更新，双向数据流进行反向传播 13，以及大内存容量来存储激活。推理可以利用降低的精度（INT8 或 INT4），只需要正向计算，并优先考虑延迟 14 而不是吞吐量。这些差异推动了专用架构：训练加速器最大化 FLOPS 和内存带宽，而推理加速器优化能效和确定性的延迟。

部署环境塑造了架构选择。数据中心加速器接受 700 瓦的功率预算，以最大化训练大型模型的吞吐量。边缘设备必须在毫瓦级约束内提供实时推理，这推动了消除所有不必要的数据移动的架构。移动处理器在性能和电池寿命之间取得平衡，而汽车系统优先考虑安全关键应用的确定性响应时间。这种多样性产生了一个丰富的专用加速器生态系统，每个都针对特定的部署场景和计算需求进行了优化。

在数据中心，如 NVIDIA H100 和 Google TPUv4 这样的训练加速器通过大规模并行和高带宽内存系统将模型开发时间从数周缩短至数天。这些系统优先考虑原始的计算吞吐量，接受 700 瓦的功耗以实现每秒万亿次性能。这种经济性支持这种权衡——将训练时间从数月缩短至数天可以减少数百万的运营成本并加快 AI 应用的上市时间。

在相反的极端情况下，边缘部署需要不同的优化策略。内存中处理架构通过直接将计算与内存集成来消除数据移动。动态电压调整在低强度操作期间可将功耗降低 50-90%。神经形态设计仅处理变化的输入，对于时序工作负载实现 1000 倍的功耗降低。这些技术使得复杂的 AI 模型能够在电池供电的情况下持续运行，支持从智能手机摄影到无需外部电源即可运行数年的自主传感器等应用。

专用加速器的成功表明，没有单一的架构能够高效地解决所有 ML 工作负载。预计到 2030 年将有 1560 亿个边缘设备，这将需要针对能源效率和实时保证进行优化的架构，而云规模训练将继续推进计算吞吐量的边界。这种多样性推动了专用架构的持续创新，每个架构都针对其特定的部署环境和计算需求进行了优化。

专用硬件架构的演变说明了计算机系统中的一个原则：随着计算模式的出现和成熟，硬件专业化随之而来，以实现最佳性能和能源效率。这种进步在机器学习加速中表现得尤为明显，特定领域的架构已经发展起来，以满足机器学习模型日益增长的计算需求。与优先考虑灵活性的通用处理器不同，专用加速器针对明确的工作负载进行优化，平衡性能、能源效率和与软件框架的集成。

表 11.1 总结了硬件专业化演变的关键里程碑，展示了每个时代如何产生针对当时计算需求的定制架构。虽然这些加速器最初出现是为了优化特定领域的负载，包括浮点运算、图形渲染和媒体处理，但它们也引入了在当代系统中持续存在的架构策略。早期代际中概述的专业化原则现在支撑着现代 AI 加速器的设计。理解这一历史轨迹为分析硬件专业化如何继续在多样化的部署环境中实现可扩展、高效的机器学习工作负载提供了背景。

表 11.1：**硬件专业化趋势**：连续的计算时代逐渐整合专用硬件以加速流行的负载，从通用 CPU 发展到特定领域的架构，最终到可定制的 AI 加速器。这一演变反映了一个基本原理：根据计算模式定制硬件可以提高性能和能源效率，推动机器学习系统中的创新。

| **时代** | **计算模式** | **架构示例** | **特点** |
| --- | --- | --- | --- |
| **1980 年代** | 浮点运算与信号处理 | FPU, DSP |

+   单用途引擎

+   焦点指令集

+   协处理器接口

|

| **1990 年代** | 3D 图形与多媒体 | GPU, SIMD 单元 |
| --- | --- | --- |

+   许多相同的计算单元

+   常规数据模式

+   宽内存接口

|

| **2000 年代** | 实时媒体编码 | 媒体编解码器、网络处理器 |
| --- | --- | --- |

+   固定功能流水线

+   高吞吐量处理

+   功耗性能优化

|

| **2010 年代** | 深度学习张量运算 | TPU、GPU 张量核心 |
| --- | --- | --- |

+   矩阵乘法单元

+   巨大并行性

+   内存带宽优化

|

| **2020 年代** | 应用特定加速 | 机器学习引擎、智能网络接口卡、领域加速器 |
| --- | --- | --- |

+   针对特定工作负载的数据路径

+   定制内存层次结构

+   应用优化设计

|

这一历史进步揭示了一个反复出现的模式：每一波硬件专业化都响应了计算瓶颈，无论是图形渲染、媒体编码还是神经网络推理。2020 年代的特点不仅在于专业化，还在于其普遍性：AI 加速器现在支撑着从 YouTube 上的产品推荐到自动驾驶汽车中的目标检测等一切事物。与早期的加速器不同，今天的 AI 硬件必须紧密集成动态软件框架，并扩展到云到边缘的部署。表格不仅展示了过去，还展示了向越来越定制、高影响力的计算平台发展的轨迹。

对于人工智能加速来说，这一转变带来了挑战，这些挑战远远超出了硬件设计的范畴。机器学习加速器必须通过在计算堆栈的多个级别上进行优化来无缝集成到机器学习工作流程中。它们必须与广泛采用的框架如 TensorFlow、PyTorch 和 JAX 有效协同工作，确保在各种硬件平台上部署的平滑和一致性。编译器和运行时支持变得必要；高级优化技术，如图级别转换、内核融合和内存调度，对于充分利用这些专用加速器的全部潜力至关重要。

可扩展性推动了额外的复杂性，因为人工智能加速器部署在从高吞吐量数据中心到资源受限的边缘和移动设备等多样化的环境中，需要定制化的性能调整和能源效率策略。集成到异构计算环境中 15 需要互操作性，这使专用单元能够与分布式系统中的传统 CPU 和 GPU 有效协调工作。

人工智能加速器代表了一种系统级的转变，它需要紧密的软硬件耦合。这种转变体现在三个具体的计算模式上，即计算原语，这些模式驱动了加速器的设计决策。理解这些原语决定了能够通过协调的硬件专业化和软件优化策略实现 100-1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>性能提升的体系结构特征，这些策略将在后续章节中探讨。

从浮点协处理器到人工智能加速器的演变揭示了一个一致的规律：计算瓶颈推动专用硬件的发展。英特尔 8087 处理器解决了占科学计算时间 80%的浮点运算问题，而现代人工智能工作负载则是一个更为极端的案例。矩阵乘法和卷积构成了神经网络计算超过 95%的部分。这种计算需求的集中创造了前所未有的专业化机会，这也解释了为什么人工智能加速器在通用处理器上实现了 100-1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的性能提升。

通过数十年的硬件进化所确立的专业化原则，识别主导操作、创建专用数据路径和优化内存访问模式，现在指导着 AI 加速器的设计。然而，神经网络引入了独特的特性，要求新的架构方法：矩阵操作中的大规模并行性、可预测的数据访问模式以实现预取，以及对降低精度的容忍度，这允许进行激进的优化。理解这些计算模式，我们称之为 AI 计算原语，有助于理解现代加速器如何将第十章中的理论效率提升转化为实际性能改进。这些软硬件优化在从第二章边缘设备到云规模推理系统等部署场景中变得至关重要。

在详细检查这些计算原语之前，我们需要了解能够实现其高效执行的架构组织。现代 AI 加速器通过精心编排的、协同工作的专用组件的层次结构实现了显著的性能提升。该架构包括三个子系统，每个子系统都针对计算挑战的不同方面。

处理基板由一系列处理单元组成，每个单元都包含针对特定操作优化的专用计算单元：张量核心执行矩阵乘法，向量单元执行逐元素操作，而特殊功能单元计算激活函数。这些处理单元以网格拓扑组织，实现了大规模并行处理，数十到数百个单元可以同时处理计算的不同部分，利用神经网络工作负载中固有的数据级并行性。

内存层次结构是一个同样关键的架构组件。高带宽内存提供了维持这些众多处理单元所需的总体吞吐量，而从共享 L2 缓存到每个元素的 L1 缓存和临时存储的多级缓存层次结构最小化了数据移动的能量成本。这种层次化组织体现了一个设计原则：在 AI 加速器中，数据移动通常比计算本身消耗更多的能量，需要优先考虑数据重用，通过在计算单元附近保持频繁访问的值（包括权重和部分结果）来实现。

主机接口在专用加速器和更广泛的计算系统之间建立连接，使管理程序控制流的通用 CPU 和执行计算密集型神经网络操作的加速器之间的协调成为可能。这种架构分区反映了系统级别的专业化：CPU 处理控制流、条件逻辑和系统协调，而加速器专注于主导神经网络执行的规律、大规模并行算术操作。

图 11.2 展示了这种架构组织，展示了专用计算单元、分层内存子系统和主机连接如何集成，形成一个针对人工智能工作负载优化的系统。

![图片](img/file181.svg)

图 11.2：**现代人工智能加速器的解剖结构**：人工智能加速器集成了包含张量核心、向量单元和特殊功能单元的专用处理元素，由从高带宽内存到本地缓存的分层内存系统支持。这种架构最大化了数据重用和并行执行，同时最小化了能耗密集型数据移动，为相对于通用处理器的 100-1000 倍性能提升奠定了基础。

## 人工智能计算原语

了解硬件如何向针对人工智能特定设计演变，需要检查推动这种专业化的计算模式。从实现 100 GFLOPS 的通用 CPU 到提供 100,000+ GFLOPS 的专用加速器过渡，反映了针对主导机器学习工作负载的特定计算模式的架构优化。这些模式，我们称之为计算原语，无论应用领域或模型大小如何，都在所有神经网络架构中反复出现。

现代神经网络建立在少数几个核心计算模式之上。无论层类型是全连接、卷积还是基于注意力的层，底层操作通常涉及将输入值乘以学习到的权重并累积结果。这种重复的乘加过程主导了神经网络执行，并定义了人工智能工作负载的算术基础。这些操作的规律性和频率导致了人工智能计算原语的发展：硬件级别的抽象，旨在以高效率执行这些核心计算。

神经网络表现出高度结构化和数据并行的计算，这使架构专业化成为可能。在第 11.2.2 节中建立的并行化原则的基础上，这些模式强调了可预测的数据重用和固定的操作序列。人工智能计算原语将这些模式提炼成可重用的架构单元，支持高吞吐量和节能的执行。

这种分解在列表 11.1 中展示，它定义了框架级别的密集层。

列表 11.1: **密集层定义**: 使用高级 API 定义密集层，展示了神经网络如何实现输入张量上的并行转换。

```py
dense = Dense(512)(input_tensor)
```

这个高级调用扩展为数学操作，如列表 11.2 所示。

列表 11.2: **层计算**: 神经网络通过加权输入求和后应用激活函数转换来计算每一层的输出。

```py
output = matmul(input_weights) + bias
output = activation(output)
```

在处理器级别，计算简化为嵌套循环，这些循环乘以输入和权重，求和结果，并应用非线性函数，如列表 11.3 所示。

列表 11.3: **嵌套循环**: 通过顺序矩阵乘法和偏置添加计算输出值，随后应用激活函数以产生最终输出。

```py
for n in range(batch_size):
    for m in range(output_size):
        sum = bias[m]
        for k in range(input_size):
            sum += input[n, k] * weights[k, m]
        output[n, m] = activation(sum)
```

这种转换揭示了四个计算特性：数据级并行性允许同时执行，结构化矩阵操作定义计算工作负载，可预测的数据移动模式驱动内存优化，以及频繁的非线性变换推动专用功能单元。

人工智能计算原语的设计遵循三个架构标准。首先，原语必须频繁使用，以证明专用硬件资源的合理性。其次，其专用实现必须相对于通用替代方案提供实质性的性能或能效提升。第三，原语必须跨代神经网络架构保持稳定，以确保长期适用性。这些考虑因素决定了向量操作、矩阵操作和特殊功能单元等原语在现代机器学习加速器中的包含。它们共同构成了高效且可扩展神经网络执行的架构基础。

### 向量操作

向量操作通过同时处理多个数据元素提供第一层硬件加速。这种并行性存在于多个尺度上，从单个神经元到整个层，使得向量处理对于高效神经网络执行至关重要。框架级代码转换为硬件指令，揭示了向量处理在神经网络加速器中的关键作用。

#### 高级框架操作

机器学习框架通过高级抽象隐藏硬件复杂性。这些抽象分解为越来越低级的操作，揭示了硬件加速的机会。其中一个这样的抽象在列表 11.4 中展示，它说明了线性层的执行流程。

列表 11.4: **线性层**: 神经网络通过线性映射将输入数据转换到更高维的空间，以实现复杂特征提取。

```py
layer = nn.Linear(256, 512)  # 256 inputs to
# 512 outputs
output = layer(input_tensor)  # Process a batch of inputs
```

这种抽象表示一个全连接层，通过学习到的权重将输入特征进行转换。为了理解硬件加速机会的出现，列表 11.5 展示了框架如何将这一高级表达式转换为数学运算。

列表 11.5：**全连接层**：每个输出被计算为所有输入的加权总和加上一个偏置，然后通过激活函数转换。线性变换使神经网络中的复杂模型架构成为可能。

```py
Z = matmul(weights, input) + bias  # Each output needs all inputs
output = activation(Z)  # Transform each result
```

这些数学运算在处理器执行过程中进一步分解为明确的计算步骤。列表 11.6 展示了实现这些乘加操作的嵌套循环。

列表 11.6：**线性层计算**：每个输出神经元通过将所有特征的加权输入求和，然后应用激活函数来计算。理解这一过程有助于掌握神经网络的基本构建块。

```py
for batch in range(32):            # Process 32 samples at once
    for out_neuron in range(512):  # Compute each output neuron
        sum = 0.0
        for in_feature in range(256): # Each output needs
                                      # all inputs
            sum += input[batch, in_feature] *
                         weights[out_neuron, in_feature]
        output[batch, out_neuron] = activation(sum +
                                    bias[out_neuron])
```

#### 顺序标量执行

传统标量处理器按顺序执行这些操作，一次处理一个单独的值。对于上面提到的包含 32 个样本的线性层示例，计算输出需要超过 400 万次乘加运算。每次操作都涉及加载一个输入值和一个权重值，将它们相乘，并累加结果。当处理神经网络所需的巨大数量的相同操作时，这种顺序方法变得非常低效。

认识到这种低效性，现代处理器利用向量处理来从根本上改变执行模式。

#### 并行向量执行

向量处理单元通过同时操作多个数据元素来实现这种转换。列表 11.7 使用 RISC-V16 汇编代码展示了现代向量处理能力。

列表 11.7：**向量化的乘加循环**：这个循环展示了 RISC-V 向量指令如何通过同时执行 8 次乘加操作来高效地进行批量处理，从而降低神经网络训练中的计算延迟。*来源：RISC-V 架构手册*

```py
vsetvli t0, a0, e32   # Process 8 elements at once
loop_batch:
    loop_neuron:
        vxor.vv v0, v0, v0    # Clear 8 accumulators
        loop_feature:
            vle32.v v1, (in_ptr)    # Load 8 inputs together
            vle32.v v2, (wt_ptr)    # Load 8 weights together
            vfmacc.vv v0, v1, v2    # 8 multiply-adds at once
            add in_ptr, in_ptr, 32  # Move to next 8 inputs
            add wt_ptr, wt_ptr, 32  # Move to next 8 weights
            bnez feature_cnt, loop_feature
```

这种向量实现并行处理八个数据元素，减少了计算时间和能耗。向量加载指令同时传输八个值，最大化内存带宽利用率。向量乘加指令并行处理八个值对，将总指令数从超过 400 万减少到大约 50 万。

为了阐明向量指令如何映射到常见的深度学习模式，表 11.2 介绍了关键向量操作及其在神经网络计算中的典型应用。这些操作，如归约、收集、分散和掩码操作，在池化、嵌入查找和注意力机制等层中经常遇到。这些术语对于解释低级向量硬件如何加速高级机器学习工作负载是必要的。

表 11.2：**向量操作**：神经网络层经常使用核心向量操作，如归约、收集和分散，以加速计算并有效地并行处理数据；这些操作阐明了低级硬件优化如何映射到高级机器学习算法。这些操作使得在深度学习模型中高效实现常见的层，如池化、嵌入查找和注意力机制成为可能。

| **向量操作** | **描述** | **神经网络应用** |
| --- | --- | --- |
| **归约** | 将向量中的元素组合在一起（例如，求和、最大值） | 池化层、注意力分数计算 |
| **收集** | 加载多个非连续内存元素 | 嵌入查找，稀疏操作 |
| **分散** | 写入多个非连续内存位置 | 嵌入的梯度更新 |
| **掩码操作** | 选择性地对向量元素进行操作 | 注意力掩码，填充处理 |
| **向量-标量广播** | 将标量应用于所有向量元素 | 偏置添加，缩放操作 |

向量处理效率的提升不仅限于指令计数减少。由于向量加载在每次操作中传输多个值，内存带宽利用率得到提高。由于控制逻辑在多个操作中共享，能效也得到提升。这些改进在现代神经网络的深层中累积，每个前向传递都会执行数十亿个操作。

#### 向量处理历史

向量操作背后的原理长期以来一直是高性能计算的核心。在 20 世纪 70 年代和 80 年代，向量处理器作为科学计算、天气预报和物理模拟的架构解决方案出现，在这些领域，大量数据需要高效的并行处理。早期的系统，如 Cray-117，作为首批商业上成功的超级计算机之一，引入了专门的向量单元，可以在单条指令中执行整个数据向量的算术运算。这些向量单元与传统的标量执行相比，显著提高了计算吞吐量(Jordan 1982)。

这些概念在机器学习中重新出现，其中神经网络表现出适合向量执行的架构。曾经加速数值模拟的相同操作，如向量加法、乘法和归约，现在驱动着机器学习工作负载的执行。虽然现代 AI 加速器的规模和专业性与其历史前身不同，但底层架构原则保持不变。向量处理在神经网络加速中的复兴突出了其在实现高计算效率方面的效用。

向量运算通过允许独立数据元素的并行处理，为神经网络加速奠定了基础。虽然向量运算在元素级变换，如激活函数方面表现出色，但神经网络还需要结构化计算，将多个输入特征组合以产生输出特征，这些变换自然地表现为矩阵运算。这种在多个维度上同时进行协调计算的需求导致了下一个架构原语：矩阵运算。

### 矩阵运算

矩阵运算构成了神经网络的计算主力，通过权重、激活和梯度的结构化模式转换高维数据(Goodfellow, Courville, and Bengio 2013)。虽然向量运算独立处理元素，但矩阵运算同时协调多个维度的计算。这些操作揭示了驱动硬件加速策略的模式。

#### 神经网络中的矩阵运算

神经网络计算分解为分层矩阵运算。如图列表 11.8 所示，一个线性层通过将输入特征转换为批次中的输出神经元来展示这种层次结构。

列表 11.8：**矩阵运算**：神经网络通过矩阵乘法和偏置来实现输出预测的转换。训练需要仔细管理输入批次和激活函数以优化模型性能。

```py
layer = nn.Linear(256, 512)  # Layer transforms 256 inputs to
# 512 outputs
output = layer(input_batch)  # Process a batch of 32 samples

# Framework Internal: Core operations
Z = matmul(weights, input)  # Matrix: transforms [256 x 32]
# input to [512 x 32] output
Z = Z + bias  # Vector: adds bias to each
# output independently
output = relu(Z)  # Vector: applies activation to
# each element independently
```

这种计算展示了神经网络中矩阵运算的规模。每个输出神经元（总共 512 个）必须处理批次中每个样本的所有输入特征（总共 256 个）。仅权重矩阵就包含<semantics><mrow><mn>256</mn><mo>×</mo><mn>512</mn><mo>=</mo><mn>131</mn><mo>,</mo><mn>072</mn></mrow><annotation encoding="application/x-tex">256 \times 512 = 131,072</annotation></semantics>个参数，这些参数定义了这些转换，说明了为什么高效的矩阵乘法对于性能至关重要。

神经网络在超越简单线性层的各种架构模式中采用矩阵运算。

#### 神经网络中的矩阵计算类型

矩阵运算在现代神经网络架构中始终如一，如列表 11.9 所示。通过 im2col 技术 18 将卷积操作转换为矩阵乘法，从而在针对矩阵操作优化的硬件上实现高效执行。

列表 11.9：**线性层**：层变换将输入特征组合起来产生隐藏表示。神经网络中的矩阵运算能够实现高效的特征提取和转换，构成了许多机器学习架构的骨架。

```py
hidden = matmul(weights, inputs)
# weights: [out_dim x in_dim], inputs: [in_dim x batch]
# Result combines all inputs for each output

# Attention Mechanisms - Multiple matrix operations
Q = matmul(Wq, inputs)
# Project inputs to query space [query_dim x batch]
K = matmul(Wk, inputs)
# Project inputs to key space[key_dim x batch]
attention = matmul(Q, K.T)
# Compare all queries with all keys [query_dim x key_dim]

# Convolutions - Matrix multiply after reshaping
patches = im2col(input)
# Convert [H x W x C] image to matrix of patches
output = matmul(kernel, patches)
# Apply kernels to all patches simultaneously
```

这种普遍的矩阵乘法模式对硬件设计有直接影响。对高效矩阵操作的需求推动了专门硬件架构的发展，这些架构能够大规模处理这些计算。以下章节将探讨现代 AI 加速器如何实现矩阵运算，重点关注其架构特性和性能优化。

#### 矩阵运算硬件加速

矩阵运算的计算需求推动了专门的硬件优化。现代处理器实现了超越向量处理能力的专用矩阵单元。这种矩阵加速的例子在列表 11.10 中展示。

列表 11.10：**矩阵单元操作**：在硬件加速系统中启用高效的块状矩阵乘法和累加，展示了专用单元如何简化对 AI/ML 操作至关重要的计算任务。

```py
mload mr1, (weight_ptr)     # Load e.g., 16x16 block of
                            # weight matrix
mload mr2, (input_ptr)      # Load corresponding input block
matmul.mm mr3, mr1, mr2     # Multiply and accumulate entire
                            # blocks at once
mstore (output_ptr), mr3    # Store computed output block
```

这个矩阵处理单元可以处理之前描述的线性层计算的<semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">16\times16</annotation></semantics>块，与使用向量处理可能的 8 个操作相比，可以同时处理 256 个乘加操作。这些矩阵操作通过实现结构化的多对多转换，补充了向量化的计算。矩阵和向量操作之间的相互作用决定了神经网络执行效率。

矩阵运算通过在多个维度上协调并行处理，为神经网络提供计算能力（参见表 11.3）。虽然它们能够实现诸如注意力机制和卷积等转换，但它们的性能取决于高效的数据处理。相反，向量运算针对一对一转换进行了优化，如激活函数和层归一化。这些操作的区分突出了数据流模式在神经网络加速器设计中的重要性，下文将对此进行探讨(Hwu 2011)。

表 11.3：**操作特性**：矩阵运算擅长神经网络层中常见的多对多转换，而向量运算则高效地处理激活函数和归一化等一对一转换。理解这些区别有助于为不同的机器学习任务选择适当的计算原语，并影响系统性能。

| **操作类型** | **最佳用途** | **示例** | **关键特性** |
| --- | --- | --- | --- |
|  |  | *   层转换 |  |
| **矩阵运算** | 多对多转换 | *   注意力计算*   卷积*   激活函数 | 每个输出依赖于多个输入 |
| **向量运算** | 一对一转换 | *   层归一化*   元素级梯度 | 每个输出只依赖于相应的输入 |

#### 矩阵计算的历史基础

矩阵运算长期以来一直是计算数学的基石，其应用范围从数值模拟扩展到图形处理（Golub 和 Loan 1996）。矩阵乘法和变换的结构性质使它们成为早期计算架构加速的自然目标。在 20 世纪 80 年代和 90 年代，针对矩阵计算优化的专用数字信号处理器（DSPs）和图形处理单元（GPUs）在加速图像处理、科学计算和 3D 渲染等工作负载中发挥了关键作用（Owens 等人 2008）。

机器学习的广泛应用强化了高效矩阵计算的重要性。神经网络，其根本建立在矩阵乘法和张量运算之上，推动了专用硬件架构的发展，这些架构超越了传统的向量处理。现代的张量处理单元（TPUs）和 AI 加速器实现了大规模的矩阵乘法，反映了曾经支撑早期科学计算和图形工作负载的相同架构原则。矩阵中心架构的复兴突显了经典数值计算与当代 AI 加速之间的深层联系。

虽然矩阵运算为神经网络提供了计算骨干，但它们只代表了加速挑战的一部分。神经网络还严重依赖于无法仅通过线性代数有效表达的非线性变换。

### 特殊功能单元

虽然向量矩阵运算能够高效地处理神经网络中的线性变换，但非线性函数提出了独特的计算挑战，需要专门的硬件解决方案。特殊功能单元（SFUs）为这些基本计算提供硬件加速，完成了高效神经网络执行所需的基本处理原语集。

#### 非线性函数

非线性函数在机器学习中扮演着基础角色，通过使神经网络能够模拟复杂关系(Goodfellow, Courville, and Bengio 2013)。列表 11.11 展示了典型的神经网络层序列。

列表 11.11：**非线性变换**：神经网络通过一系列线性变换后跟非线性激活来处理输入数据，以捕捉复杂模式。这个层序列增强了模型的表达能力和学习能力。

```py
layer = nn.Sequential(
    nn.Linear(256, 512), nn.ReLU(), nn.BatchNorm1d(512)
)
output = layer(input_tensor)
```

这个序列引入了多个非线性变换，这些变换超越了简单的矩阵运算。列表 11.12 展示了框架如何将这些操作分解为其数学组成部分。

列表 11.12：**非线性变换**：神经网络应用线性和非线性操作将输入数据转换为学习有意义特征。机器学习模型利用这些变换来有效地捕捉数据中的复杂模式。

```py
Z = matmul(weights, input) + bias  # Linear transformation
H = max(0, Z)  # ReLU activation
mean = reduce_mean(H, axis=0)  # BatchNorm statistics
var = reduce_mean((H - mean) ** 2)  # Variance computation
output = gamma * (H - mean) / sqrt(var + eps) + beta
# Normalization
```

#### 非线性函数的硬件实现

当在传统处理器上检查这些操作的实现时，它们的计算复杂性变得明显。这些看似简单的数学运算转化为复杂的指令序列。考虑批归一化的计算：计算平方根需要多次数值逼近迭代，而在 softmax 等操作中的指数函数需要级数展开或查找表(Ioffe and Szegedy 2015b)。即使是简单的 ReLU 激活也会引入分支逻辑，这可能会破坏指令流水线（参见列表 11.13 中的示例）。

列表 11.13：**ReLU 和 BatchNorm 操作**：神经网络通过可能导致指令流水线中断的条件操作处理输入数据，以及为归一化所需的多次遍历，突出了传统实现中的效率挑战。来源：IEEE Spectrum

```py
for batch in range(32):
    for feature in range(512):
       # ReLU: Requires branch prediction and potential
       # pipeline stalls
       z = matmul_output[batch, feature]
       h = max(0.0, z)    # Conditional operation

       # BatchNorm: Multiple passes over data
       mean_sum[feature] += h    # First pass for mean
       var_sum[feature] += h * h # Additional pass for variance

       temp[batch, feature] = h  # Extra memory storage needed

# Normalization requires complex arithmetic
for feature in range(512):
    mean = mean_sum[feature] / batch_size
    var = (var_sum[feature] / batch_size) - mean * mean

    # Square root computation: Multiple iterations
    scale = gamma[feature] / sqrt(var + eps)  # Iterative
                                              # approximation
    shift = beta[feature] - mean * scale

    # Additional pass over data for final computation
    for batch in range(32):
        output[batch, feature] = temp[batch, feature] *
                                 scale + shift
```

这些操作引入了几个关键的低效因素：

1.  多次遍历数据，增加内存带宽需求

1.  需要许多指令周期的复杂算术

1.  条件操作可能导致流水线停滞

1.  为中间结果提供额外的内存存储

1.  向量处理单元利用率低

更具体地说，每个操作都引入了独特的挑战。批量归一化需要多次遍历数据：一次用于计算均值，另一次用于计算方差，最后一次用于输出转换。每次遍历都通过内存层次结构加载和存储数据。在数学符号中看似简单的操作通常扩展为许多指令。平方根计算通常需要 10-20 次牛顿-拉夫森近似等数值方法的迭代，以达到适当的精度（Goldberg 1991）。像 ReLU 的最大函数这样的条件操作需要可能导致处理器流水线停滞的分支指令。实现需要临时存储中间值，这增加了内存使用和带宽消耗。虽然向量单元擅长常规计算，但像指数和平方根这样的函数通常需要不能充分利用向量处理能力的标量操作。

#### 硬件加速

SFUs 通过专用硬件实现来解决这些低效问题。现代机器学习加速器包括将复杂操作转换为单周期或固定延迟计算的专用电路。加速器可以加载值向量并直接应用非线性函数，消除了多次遍历和复杂指令序列的需要，如列表 11.14 所示。

列表 11.14：**硬件加速**：单周期非线性操作使机器学习加速器中的向量处理效率更高，展示了专用硬件如何减少计算延迟。

```py
vld.v v1, (input_ptr)    # Load vector of values
vrelu.v v2, v1           # Single-cycle ReLU on entire vector
vsigm.v v3, v1           # Fixed-latency sigmoid computation
vtanh.v v4, v1           # Direct hardware tanh implementation
vrsqrt.v v5, v1          # Fast reciprocal square root
```

每个 SFU 通过专用电路实现特定的功能。例如，ReLU 单元在专用逻辑中执行比较和选择，消除了分支开销。平方根操作使用具有固定迭代次数的算法（如牛顿-拉夫森算法）的硬件实现，提供保证的延迟。指数和对数函数通常结合小的查找表和硬件插值电路（Costa 等人 2019）。使用这些自定义指令，SFU 实现消除了对数据的多次遍历，移除了复杂的算术序列，并保持了高计算效率。表 11.4 显示了各种硬件实现及其典型延迟。

表 11.4：**特殊功能单元**：通过消除软件开销并允许并行处理向量数据，专用硬件实现常见的数学函数（如 relu、sigmoid 和倒数平方根）加速了机器学习计算。每个函数的典型延迟为 1-2 周期，这表明通过专用电路而不是通用算术所实现的性能提升。

| **功能单元** | **操作** | **实现策略** | **典型延迟** |
| --- | --- | --- | --- |
| **激活单元** | ReLU, sigmoid, tanh | 分段近似电路 | 1-2 周期 |
| **统计单元** | Mean, variance | 并行归约树 | log(N)周期 |
| **指数单元** | exp, log | 表查找 + 硬件插值 | 2-4 周期 |
| **根/幂单元** | sqrt, rsqrt | 固定迭代牛顿-拉夫森 | 4-8 周期 |

#### SFU 历史

高效的非线性函数评估需求几十年来一直影响着计算机架构。早期的处理器集成了对复杂数学函数的硬件支持，例如对数和三角运算，以加速科学计算和信号处理工作负载（Smith 1997）。在 20 世纪 70 年代和 80 年代，浮点协处理器被引入以独立于主 CPU 处理复杂数学运算（Palmer 1980）。在 20 世纪 90 年代，指令集扩展如 Intel 的 SSE 和 ARM 的 NEON 提供了针对向量化的数学变换的专用硬件，提高了多媒体和信号处理应用的效率。

机器学习工作负载重新引入了对专用功能单元的强烈需求，因为激活函数、归一化层和指数变换是神经网络计算的基础。现代人工智能加速器不再依赖于迭代的软件近似，而是为这些操作实现了快速、固定延迟的 SFU，这与科学计算的历史趋势相呼应。专用特殊功能单元的重新出现强调了硬件演变的持续周期，其中特定领域的需求推动了在新的计算范式中对经典架构概念的重新发明。

向量、矩阵和特殊功能单元的组合为现代人工智能加速器提供了计算基础。然而，这些处理原语的有效利用严重依赖于数据移动和访问模式。这使我们转向研究神经网络执行中实现高效数据流的架构、层次结构和策略。

### 计算单元和执行模型

向量运算、矩阵运算和之前考察的特殊功能单元代表了人工智能加速器中的基本计算原语。现代人工智能处理器将这些原语打包成不同的执行单元，例如 SIMD 单元、张量核心和处理元素，这些单元定义了计算的结构以及如何向用户暴露。理解这种组织结构揭示了开发者可以利用的当代人工智能加速器的理论能力和实际性能特征。

#### 将原语映射到执行单元

从计算原语到执行单元的演变遵循一个结构化的层次结构，反映了人工智能加速器日益复杂化和专业化的趋势：

+   向量运算 → 允许独立数据元素并行处理的 SIMD/SIMT 单元

+   矩阵运算 → 提供结构化矩阵乘法的张量核心和收缩阵列

+   特殊功能 → 集成在处理元素内的专用硬件单元

每个执行单元将这组计算原语与专用内存和控制机制相结合，优化性能和能效。这种结构化封装允许硬件供应商在实现针对特定工作负载需求定制的多样化底层架构的同时，公开标准化的编程接口。执行单元的选择显著影响整体系统效率，影响数据局部性、计算密度和工作负载适应性。后续章节将探讨这些执行单元如何在人工智能加速器中运行，以最大化不同机器学习任务中的性能。

#### 从 SIMD 到 SIMT 架构的演变

单指令多数据（SIMD)19 执行并行地对多个数据元素应用相同的操作，最小化指令开销同时最大化数据吞吐量。这种执行模型广泛应用于具有规则、独立数据并行性的工作负载加速，如神经网络计算。ARM 可伸缩向量扩展（SVE）提供了一个现代架构如何高效实现 SIMD 操作的代表性示例，如列表 11.15 所示。

列表 11.15：**向量运算**：向量乘法和加法运算在机器学习模型中实现高效的并行处理。*来源：ARM 文档*

```py
ptrue p0.s              # Create predicate for vector length
ld1w z0.s, p0/z, [x0]   # Load vector of inputs
fmul z1.s, z0.s, z0.s   # Multiply elements
fadd z2.s, z1.s, z0.s   # Add elements
st1w z2.s, p0, [x1]     # Store results
```

处理器架构继续扩展 SIMD 能力，以适应不断增长的计算需求。Intel 的高级矩阵扩展（AMX）(I. 公司 2021)和 ARM 的 SVE2 架构(Stephens 等人 2017)提供了灵活的 SIMD 执行，使软件能够跨不同的硬件实现进行扩展。

为了解决这些限制，SIMT 通过允许多个独立线程的并行执行扩展了 SIMD 原则，每个线程保持自己的程序计数器和架构状态（E. Lindholm 等人 2008）。这种模型自然地映射到矩阵计算，其中每个线程处理工作负载的不同部分，同时仍然受益于共享指令执行。在 NVIDIA 的 GPU 架构中，每个流多处理器（SM)20 协调成千上万的并行执行的线程，允许高效地扩展神经网络计算，如列表 11.16 所示。线程被组织成 warp21，它们是使 SIMT 效率化的基本执行单元。

列表 11.16：**SIMT 执行**：每个线程并行处理一个独特的输出元素，展示了 SIMT 如何在 GPU 上实现高效的矩阵乘法。

```py
__global__ void matrix_multiply(float* C, float* A, float*
                                B, int N) {
    // Each thread processes one output element
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    float sum = 0.0f;
    for (int k = 0; k < N; k++) {
        // Threads in a warp execute in parallel
        sum += A[row * N + k] * B[k * N + col];
    }
    C[row * N + col] = sum;
}
```

SIMT 执行允许神经网络计算在数千个线程上高效扩展，同时保持对发散执行路径的灵活性。类似的执行模型出现在 AMD 的 RDNA 和 Intel 的 Xe 架构中，强化了 SIMT 作为 AI 加速的基本机制。

#### 张量核心

虽然 SIMD 和 SIMT 单元提供了高效的向量操作执行，但神经网络严重依赖于需要专门执行单元进行结构化多维处理的矩阵计算。矩阵操作的能量经济学推动了这种专业化：传统的标量处理需要每个操作多次 DRAM 访问，每次访问消耗 640 pJ，而张量核心将这种能量成本分摊到整个矩阵块上。张量处理单元通过启用通过专用硬件块执行矩阵乘法和累加的矩阵操作，扩展了 SIMD 和 SIMT 原则，这些硬件块在单个操作中在整个矩阵块上执行。张量核心将能量配置文件从 173×内存受限的低效转变为计算优化的执行，其中 3.7 pJ 的乘累加操作主导了能量预算，而不是数据移动。

张量核心 22，如 NVIDIA 的 Ampere GPU 架构中所实现，是这种方法的例子。它们通过专用指令，如 NVIDIA A100 GPU 上列表 11.17 中所示的张量核心操作，公开了矩阵计算能力。

列表 11.17：**张量核心操作**：矩阵乘法在整个矩阵块上并行执行，优化了神经网络训练的计算效率。

```py

Tensor Core Operation (NVIDIA A100):
mma.sync.aligned.m16n16k16.f16.f16
  {d0,d1,d2,d3},     // Destination registers
  {a0,a1,a2,a3},     // Source matrix A
  {b0,b1,b2,b3},     // Source matrix B
  {c0,c1,c2,c3}      // Accumulator
```

单个张量核心指令处理整个矩阵块，同时在局部寄存器中保持中间结果，与基于标量或向量操作的实施相比，显著提高了计算效率。这种结构化方法使硬件能够实现高吞吐量，同时减少软件层面上的显式循环展开和数据管理的负担。

张量处理单元架构根据设计优先级而有所不同。NVIDIA 的 Ampere 架构集成了针对通用深度学习加速优化的张量核心。Google 的 TPUv4 利用大规模矩阵单元，以阵列形式排列，以最大化持续的训练吞吐量。Apple 的 M1 神经网络引擎 23 集成了针对移动推理工作负载优化的较小矩阵处理器，而 Intel 的 Sapphire Rapids 架构引入了专为高性能数据中心应用设计的 AMX 瓦片。

AI 硬件的日益专业化推动了深度学习工作负载的显著性能提升。图 11.3 展示了 NVIDIA GPU 中 AI 加速器性能的轨迹，突出了从通用浮点执行单元到高度优化的张量处理核心的过渡。

![图片](img/file182.png)

图 11.3：**GPU 性能提升**：NVIDIA GPU 在十年间整数 8 位 TOPS（每秒兆次运算）上实现了<semantics><mrow><mn>10</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">10\times</annotation></semantics>的增长，这一增长是由从浮点运算向张量核心加速的架构创新驱动的。这一趋势反映了硬件在深度学习工作负载中的日益专业化以及对于高效推理能力的不断增长需求。

#### 处理单元

最高级别的执行单元组织将多个张量核心与本地内存集成到处理单元（PEs）中。处理单元是许多 AI 加速器的基本构建块，通过结合不同的计算单元来高效执行神经网络操作。每个 PE 通常包括用于逐元素操作的向量单元、用于矩阵计算的张量核心、用于非线性变换的特殊功能单元以及专门的内存资源，以优化数据局部性和最小化数据移动开销。

处理单元在 AI 硬件中发挥着至关重要的作用，通过平衡计算密度与内存访问效率。它们的设计在不同架构中有所不同，以支持多样化的工作负载和可扩展性要求。Graphcore 的智能处理单元（IPU）将计算分布在 1,472 个瓦片上，每个瓦片包含独立优化的处理单元，以支持细粒度并行性（Graphcore 2020）。Cerebras 在 CS-2 系统中扩展了这种方法，通过晶圆级设备集成了 850,000 个处理单元，以加速稀疏计算。Tesla 的 D1 处理器通过大量本地内存排列处理单元，优化了实时自动驾驶工作负载的吞吐量和延迟（Quinnell 2024）。

处理单元为大规模 AI 加速提供了结构基础。它们的效率不仅取决于计算能力，还取决于互连策略和内存层次结构设计。接下来的几节将探讨这些架构选择如何影响不同 AI 工作负载的性能。

张量处理单元通过使用硬件加速的矩阵计算，在人工智能工作负载中实现了显著的效率提升。随着架构中纳入对高级执行技术的支持，包括结构化稀疏性和特定工作负载优化，它们的作用也在不断演变。然而，这些单元的有效性不仅取决于它们的计算能力，还取决于它们与内存层次结构和数据移动机制如何交互，这些内容将在后续章节中探讨。

#### 系统阵列

当张量核心将矩阵运算打包成结构化计算单元时，系统阵列提供了一种针对连续数据流和操作数重用优化的替代方法。系统架构的基本动机源于之前讨论的能量效率约束——通过架构设计最小化内存访问惩罚的影响。系统阵列以网格模式排列处理单元，其中数据以同步方式在相邻单元之间有节奏地流动，使得每个操作数在通过阵列传播时能够参与多个计算。这种结构化移动通过最大化本地数据重用最小化了外部内存访问——单个权重值可以在通过处理单元时对数十个操作做出贡献，从而从根本上将能量配置文件从内存受限转变为计算高效执行。

系统阵列的概念最初由 Kung 和 Leiserson24 提出，他们在并行计算架构中正式化了其应用，以实现高效的矩阵运算(Kung 1982)。与通用执行单元不同，系统阵列通过在网格中传播时重复使用操作数来利用空间和时间局部性。谷歌的 TPU 就是这种架构方法的例证。在 TPUv4 中，一个<semantics><mrow><mn>128</mn><mo>×</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">128\times128</annotation></semantics>的系统阵列乘累加单元通过以流水线方式通过阵列流数据来处理矩阵运算，如图图 11.4 所示。

系统阵列架构通过在结构化处理单元网格上同步数据移动来实现计算效率。系统阵列围绕以下四个基本组件组织计算：

1.  **控制单元**：协调阵列中的时序和数据分布，在整个计算网格中保持同步操作

1.  数据流：输入矩阵通过协调的路径传播——矩阵 A 的元素水平穿越，而矩阵 B 的元素通过处理网格垂直流动

1.  **处理单元网格**：单个处理单元对流数据执行乘累加运算，生成部分结果，这些结果累积到最终计算中

1.  **输出收集**：结果在指定的输出边界处汇总，其中累积的局部和形成完整的矩阵元素

同步数据流确保矩阵元素 A[i,k]在精确的时间间隔内遇到相应的 B[k,j]元素，执行矩阵乘法所需的乘累加操作 C[i,j] = Σ A[i,k] × B[k,j]。这种在多个处理元素之间系统地重用操作数，通过消除从外部内存子系统中的冗余数据检索，显著降低了内存带宽需求。

考虑在心脏阵列内乘以 2×2 矩阵 A 和 B。在第一个计算周期内，元素 A[0,0]=2 水平传播，而 B[0,0]=1 垂直移动，在处理元素 PE(0,0)处汇聚以执行乘法 2×1=2。在下一个周期，相同的 A[0,0]=2 移动到 PE(0,1)，在那里它遇到 B[0,1]=3，计算 2×3=6。同时，A[0,1]=4 进入 PE(0,0)以与下一个 B 矩阵元素交互。这种协调的数据移动使得在多个计算操作中系统地重用操作数，消除冗余内存访问，并体现了心脏阵列架构的基本效率原则。

![图片](img/file183.svg)

图 11.4：**心脏阵列数据流**：数组内的处理元素通过流水线方式流式传输数据执行矩阵操作，与传统的内存-计算架构相比，最大程度地重用操作数并最小化内存访问。这种空间和时间局部性使得高效并行计算成为可能，如谷歌 tpuv4 中的乘累加单元所示。

数组中的每个处理元素在每个周期内执行乘累加操作：

1.  从上方接收输入激活

1.  从左侧接收权重值

1.  将这些值相乘并加到其运行总和上

1.  将输入激活向下传递，将权重值向右传递到相邻元素

这种结构化计算模型最小化了全局内存和处理元素之间的数据移动，提高了效率和可扩展性。由于心脏阵列以流式操作，因此它们特别适用于高吞吐量工作负载，如深度学习训练和推理。

虽然图 11.4 中的图示说明了常见的心脏阵列实现方式，但心脏架构在不同加速器设计中差异很大。以训练为重点的架构，如谷歌的 TPU，采用大阵列，优化了高计算吞吐量，而边缘设备中发现的推理设计则优先考虑能源效率，采用较小的配置。

基本原则保持一致：数据系统地通过处理元素流动，输入在水平和垂直方向上移动以同步计算部分和。然而，如第 11.4.1 节中详细说明的，实际的有效性最终受到内存带宽瓶颈的限制。

一个能够每周期执行 16,384 次操作的 128×128 收缩阵列需要持续的数据输入来维持利用率——每个周期都需要新鲜输入激活和权重参数，这些参数必须从片外内存通过片上缓冲区传输到阵列边缘。TPU 的 1,200 GB/s 片上带宽使得利用率很高，但当处理内存需求超过片上容量的大型变压器模型时，即使这个巨大的带宽也会变得有限。

回想第十章，量化通过将 FP32 权重转换为 INT8 表示来减少模型内存占用——这种优化直接针对此处确定的内存带宽限制。将 32 位浮点权重转换为 8 位整数可以将内存流量减少 4 倍，将带宽受限的操作转换为计算受限的工作负载，其中收缩阵列可以实现更高的利用率。同样，结构化剪枝移除权重矩阵的整个行或列，减少了必须穿越内存层次结构的数据量以及所需的计算。这些算法优化之所以有价值，正是因为它们针对限制实际加速器性能的内存瓶颈。

#### 人工智能加速中的数值计算

人工智能加速器的效率不仅取决于计算能力，还取决于数值表示的精度。数值格式的选择决定了精度、吞吐量和能耗之间的平衡，影响着不同的执行单元，如 SIMD 和 SIMT 单元、张量核心和收缩阵列的设计和部署。

##### 精度权衡

数值精度是现代人工智能加速器中的一个关键设计参数。虽然更高的精度格式提供了数学上的稳定性和准确性，但它们在功耗、内存带宽和计算吞吐量方面带来了巨大的成本。找到最佳精度点已成为人工智能硬件架构中的一个核心挑战。

早期深度学习模型主要依赖于单精度浮点数（FP32）进行训练和推理。虽然 FP32 提供了足够的动态范围和精度以支持稳定的学习，但它带来了高计算和内存成本，限制了效率，尤其是在模型规模增加时。随着时间的推移，硬件架构发展以支持更低精度的格式，如半精度浮点数（FP16）和 bfloat16（BF16），这些格式可以减少内存使用并提高计算吞吐量，同时保持深度学习任务所需的足够精度。最近，整数格式（INT8、INT4）在推理工作负载中越来越受欢迎，其中小的数值表示可以显著提高能效，同时不会超过可接受的模型精度极限。

从高精度到低精度格式的过渡已经深深集成到硬件执行模型中。正如第 11.3.4.2 节中详细所述，SIMD 和 SIMT 单元为多种精度提供了灵活的支持。张量核心(第 11.3.4.3 节)通过使用降低精度的算术来加速计算，而脉动阵列(第 11.3.4.5 节)通过最小化内存带宽约束来优化性能，通过使用最大化操作数重用的低精度格式。

尽管降低精度具有优势，但深度学习模型并不总是可以完全依赖低比特表示。为了应对这一挑战，现代 AI 加速器实现了混合精度计算，在执行的不同阶段使用不同的数值格式。这些精度选择对模型的公平性和可靠性有重要影响。例如，矩阵乘法可能在 FP16 或 BF16 中执行，而累加则保持在 FP32 以防止精度损失。同样，推理引擎在必要时利用 INT8 算术，同时保留关键激活在更高精度中。

##### 混合精度计算

现代 AI 加速器越来越多地支持混合精度执行，允许在计算的各个阶段使用不同的数值格式。训练工作负载通常利用 FP16 或 BF16 进行矩阵乘法，同时保持 FP32 的累加以保持精度。相比之下，推理工作负载优化为 INT8 甚至 INT4，在保持可接受精度的同时实现高效率。

这种向精度多样性的转变在人工智能硬件的演变中表现得非常明显。早期架构，如 NVIDIA Volta，对低于 FP16 的低精度支持有限，而后续架构，包括 Turing 和 Ampere，扩展了支持的格式范围。Ampere GPU 引入了 TF32 作为 FP32 和 FP16 之间的混合格式，同时更广泛地支持 BF16、INT8 和 INT4。 表 11.5 展示了这一趋势。

表 11.5：**精度支持演变**：GPU 架构逐步扩展了对低精度数据类型的支持，使人工智能工作负载在性能提升和效率改进方面受益。早期架构主要使用 FP32，而后续几代架构则纳入了 FP16、BF16、INT8 和 INT4，以加速训练和推理任务。

| **架构** | **年份** | **支持的 Tensor Core 精度** | **支持的 CUDA Core 精度** |
| --- | --- | --- | --- |
| **Volta** | 2017 | FP16 | FP64, FP32, FP16 |
| **Turing** | 2018 | FP16, INT8 | FP64, FP32, FP16, INT8 |
| **Ampere** | 2020 | FP64, TF32, bfloat16, FP16, INT8, INT4 | FP64, FP32, FP16, bfloat16, INT8 |

表 11.5 突出了如何将更多样化的数值格式纳入到新的架构中，反映了在不同人工智能工作负载中需要更大的灵活性。这一趋势表明，未来的人工智能加速器将继续扩展对自适应精度的支持，优化计算效率和模型精度。

在硬件设计中使用的精度格式具有深远的影响。通过采用低精度格式，执行单元和内存之间的数据传输减少，导致内存带宽要求和存储需求降低。Tensor 核心和收缩阵列可以并行处理更多低精度元素，从而在 FLOPs 方面提高有效吞吐量。由于基于整数的计算（例如，INT8）与浮点运算相比需要更低的功耗，因此能效也得到了提高——这对于推理工作负载来说是一个明显的优势。

随着人工智能模型规模的持续扩大，加速器架构正在演变以支持更高效的数值格式。未来的设计预计将采用自适应精度技术，根据工作负载特性动态调整计算精度。这种演变有望进一步优化深度学习性能，同时在精度和能效之间达到最佳平衡。

#### 建筑集成

计算原语组织到执行单元中的方式决定了 AI 加速器的效率。虽然 SIMD、tensor 核心和波前阵列是基本构建块，但它们在不同 AI 处理器中集成到全芯片架构中的方式差异很大。执行单元的选择、它们支持的数值精度以及它们的连接性影响硬件如何有效地扩展以适应深度学习工作负载。

现代 AI 处理器根据其预期应用表现出各种设计权衡。一些架构，如 NVIDIA 的 A100，集成了大量针对 FP16 训练优化的 tensor 核心，而 Google 的 TPUv4 优先考虑高吞吐量的 BF16 矩阵乘法。以推理为重点的处理器，如 Intel 的 Sapphire Rapids，集成了 INT8 优化的 tensor 核心以最大化效率。专为移动工作负载设计的 Apple M1 采用较小的处理单元，这些处理单元针对低功耗 FP16 执行进行了优化。这些设计选择反映了数值精度和执行单元组织方面的日益灵活性，正如前文所述。

表 11.6 总结了当代 AI 处理器中的执行单元配置。

表 11.6：**AI 处理器配置**：现代 AI 处理器优先考虑不同的执行单元特性，以优化特定工作负载的性能；NVIDIA A100 利用宽 SIMD 和 tensor 核心进行训练，Google TPUv4 强调高吞吐量的 BF16 矩阵乘法，而 Intel Sapphire Rapids 专注于 INT8 优化的推理，而 apple M1 优先考虑低功耗 FP16 在较小的处理单元上的执行。这些在 SIMD 宽度、tensor 核心大小和处理单元数量上的变化反映了 AI 硬件架构及其目标应用的日益多样化。

| **处理器** | **SIMD 宽度** | **Tensor 核心大小** | **处理单元** | **主要工作负载** |
| --- | --- | --- | --- | --- |
| **NVIDIA A100** | 1024 位 | <semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">4\times4\times4</annotation></semantics> FP16 | 108 个 SM | 训练、HPC |
| **Google TPUv4** | 128 位宽 | <semantics><mrow><mn>128</mn><mo>×</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">128\times128</annotation></semantics> BF16 | 每芯片 2 核心 | 训练 |
| **Intel Sapphire** | 512 位 AVX | <semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">32\times32</annotation></semantics> INT8/BF16 | 56 核心 | 推理 |
| **Apple M1** | 128 位 NEON | <semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">16\times16</annotation></semantics> FP16 | 8 个 NPU 核心 | 移动推理 |

表 11.6 突出了执行单元配置在不同架构中的差异，以优化不同的深度学习工作负载。训练加速器优先考虑高吞吐量的浮点张量运算，而推理处理器则专注于低精度整数执行以提高效率。同时，移动加速器在精度和功耗效率之间取得平衡，以满足实时约束。

### 成本效益分析

虽然架构规范定义了计算潜力，但实际的部署决策需要理解不同加速器选项之间的成本效益权衡。然而，仅凭原始的计算指标并不能提供一个完整的画面——现代 AI 加速的基本约束不是计算能力，而是数据移动效率。

之前建立的能量差异——其中内存访问成本主导计算——推动了整个专用硬件革命。这种差异解释了为什么具有高内存带宽的 GPU 实现了 40-60%的利用率，而具有阵列的 TPU 通过最小化数据移动实现了 85%的利用率。

表 11.7 提供了代表性加速器的具体成本效益数据，但经济分析必须考虑利用率效率和能源消耗模式，这些因素决定了实际性能。

表 11.7：**加速器成本效益比较**：硬件成本必须与计算能力相权衡，以确定最佳部署策略。虽然像 H100 这样的新型加速器提供了更好的性价比，但总拥有成本包括功耗、冷却需求和基础设施成本，这些因素会显著影响运营经济性。*TPU 定价基于云服务费率。

| **加速器** | **零售价（美元）** | **峰值 FLOPS（FP16）** | **内存带宽** | **价格/性能** |
| --- | --- | --- | --- | --- |
| **NVIDIA V100** | ~$9,000 (2017-19) | 125 TFLOPS | 900 GB/s | $72/TFLOP |
| **NVIDIA A100** | $15,000 | 312 TFLOPS (FP16) | 1,935 GB/s | $48/TFLOP |
| **NVIDIA H100** | $25,000-30,000 | 756 TFLOPS (TF32) | 3,350 GB/s | $33/TFLOP |
| **Google TPUv4** | ~$8,000* | 275 TFLOPS (BF16) | 1,200 GB/s | $29/TFLOP |
| **Intel H100** | $12,000 | 200 TFLOPS (INT8) | 800 GB/s | $60/TFLOP |

一家训练大型语言模型的初创公司面临着在 8 个 V100（$72K）提供 1,000 TFLOPS 或 4 个 A100（$60K）提供 1,248 TFLOPS 之间的选择。然而，性能分析揭示了真正的性能故事——具有 0.5-2 FLOPS/byte 的算术强度的变压器训练使得两种配置都受内存带宽限制，而不是计算限制。A100 的 1,935 GB/s 带宽比 V100 的 900 GB/s 提供了 2.15 倍的可持续性能，使得有效性能提升达到 115%，而不是峰值 FLOPS 所暗示的 25%。当与 17% 低的硬件成本和 30% 的更好的能效（每有效 TFLOP 400 W 对比 300 W）相结合时，A100 配置在多年部署中提供了令人信服的经济优势。

这些成本动态解释了为何尽管单价更高，但新加速器的采用速度仍然很快。H100 的 $33/TFLOP 比 V100 的 $72/TFLOP 提高了 54%，但更重要的是，其 3,350 GB/s 的带宽使得每美元的内存吞吐量提高了 3.7 倍——这是决定现实世界变压器性能的指标。云部署进一步复杂化了分析，因为提供商通常对高端加速器收取 $2-4/小时 的费用，这使得购买和租赁的盈亏平衡点高度依赖于利用模式和能源成本，这些成本在三年生命周期内可能占运营总成本的 60-70%。

框架选择对经济决策的影响很大——详细的硬件-框架优化策略在第七章中介绍，而性能评估方法在第十二章中讨论。

虽然执行单元定义了加速器的计算潜力，但它们的有效性从根本上受到数据移动和内存层次结构的限制。实现计算资源的充分利用需要高效的内存系统，以最小化数据传输开销并优化局部性。了解这些限制揭示了为什么内存架构在 AI 加速中变得与计算设计一样关键。

## AI 存储系统

在前几节中检查的执行单元——SIMD 单元、张量核心和收缩阵列——提供了令人印象深刻的计算吞吐量：现代加速器在神经网络操作中实现了 100 到 1000 TFLOPS。然而，当内存子系统无法以足够的速率提供数据时，这些理论能力在实践中仍然无法实现。这种基本约束，称为 AI 内存墙，代表了现实世界加速器性能中的主要瓶颈。

与传统工作负载不同，机器学习模型需要频繁访问大量参数、激活和中间结果，导致对内存带宽的大量需求——这是一个与第六章（ch012.xhtml#sec-data-engineering）中介绍的数据管理策略相交的挑战。现代人工智能硬件通过高级内存层次结构、高效的数据移动技术和压缩策略来应对这些挑战，以促进高效的执行和改进的人工智能加速。

本节通过四个相互关联的视角来考察内存系统设计。首先，我们量化了计算吞吐量和内存带宽之间不断扩大的差异，揭示了为什么人工智能内存墙代表了现代加速器中占主导地位的性能约束。其次，我们探讨了内存层次结构如何通过精心构建的层级（从片上 SRAM 到片外 DRAM）来平衡速度、容量和能效之间的竞争需求。第三，我们分析了主机系统和加速器之间的通信模式，揭示了限制端到端性能的传输瓶颈。最后，我们考察了不同的神经网络架构——多层感知器、卷积网络和变换器——如何创建不同的内存压力模式，这些模式为硬件设计决策和优化策略提供了信息。

### 理解人工智能内存墙

人工智能内存墙代表了限制现代加速器性能的基本瓶颈——计算吞吐量和内存带宽之间不断扩大的差异，这阻止了加速器达到其理论能力。虽然计算单元可以通过向量操作和矩阵乘法等专用原语每秒执行数百万次操作，但它们完全依赖于内存系统来提供这些操作所需的连续流权重、激活和中间结果。

#### 量化计算-内存性能差距

在考察扩展趋势时，这种约束的严重性变得显而易见。在过去 20 年中，峰值计算能力每两年增长 3.0 倍，而在此期间 DRAM 带宽仅增长 1.6 倍（Gholami 等人 2024）。这种差异导致了一个指数级扩大的差距，其中加速器拥有巨大的计算能力，但无法快速访问数据以充分利用它。现代硬件体现了这种不平衡：NVIDIA H100 提供 989 TFLOPS 的计算能力，但只有 3.35 TB/s 的内存带宽（Choquette 2023a），需要每字节 295 次操作才能实现完全利用——远超过神经网络中典型的每字节 1-10 次操作。

存储墙通过三个关键约束体现出来。首先，能量差异——访问 DRAM 消耗 640 pJ，而计算仅需 3.7 pJ（Horowitz 2014），这造成了 173 倍的能量惩罚，通常由于功耗预算而非计算能力而限制了性能。其次，带宽限制——即使是 TB/s 的内存系统也无法持续不断地为数千个并行计算单元提供数据，在典型的工作负载中导致 50-70%的空闲时间。第三，延迟层次——片外内存访问需要数百个周期，这会在并行执行单元中产生流水线停滞。

如图 11.5 所示，这种“AI 存储墙”持续扩大，使得内存带宽而不是计算成为 AI 加速的主要约束。

![](img/file184.png)

图 11.5：**AI 存储墙**：计算性能与内存带宽之间不断扩大的差距强调了由于内存限制而维持峰值计算效率的挑战日益增加。在过去 20 年里，尽管计算能力迅速发展，但内存带宽并没有跟上，导致数据密集型应用中可能出现瓶颈。

除了性能限制之外，内存访问还带来了显著的能量成本。从片外 DRAM 中获取数据比执行算术运算消耗的能量要多得多（Horowitz 2014）。这种低效性在机器学习模型中尤为明显，其中大参数尺寸、频繁的内存访问和非均匀的数据移动模式加剧了内存瓶颈。能量差异驱动了架构决策——谷歌的 TPU 通过最小化通过收缩阵列和大型片上内存的数据移动，实现了比当代 GPU 高 30-83<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的能量效率。这些设计选择表明，能量约束而非计算限制通常决定了实际部署的可行性。

#### 机器学习工作负载中的内存访问模式

由于计算中涉及的大量数据，机器学习工作负载对内存系统提出了巨大的需求。与性能通常由算术运算速度决定的传统计算密集型应用不同，ML 工作负载的特点是高数据移动需求。加速器的效率不仅取决于其计算吞吐量，还取决于其连续向处理单元提供数据的能力，而不引入停滞或延迟。

神经网络在其执行过程中处理多种类型的数据，每种数据都有独特的内存访问模式：

+   **模型参数（权重和偏置）**：机器学习模型，尤其是在自然语言处理和计算机视觉等大规模应用中使用的模型，通常包含数百万到数十亿个参数。高效地存储和访问这些权重对于保持吞吐量至关重要。

+   **中间激活值**：在训练和推理过程中，每一层都会产生中间结果，这些结果必须临时存储和检索以供后续操作使用。这些激活值可以显著增加内存开销，尤其是在深度架构中。

+   **梯度（训练期间）**：反向传播需要存储和访问每个参数的梯度，这进一步增加了计算单元和内存之间数据移动的量。

随着模型规模和复杂性的增加，内存容量和带宽的改进变得至关重要。尽管专门的计算单元可以加速矩阵乘法等操作，但它们的整体性能取决于数据持续、高效地传递到处理单元。在自然语言处理和计算机视觉等大规模应用中，模型通常包含数百万到数十亿个参数（T. B. Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, 等人，2020）。因此，实现高性能需要最小化由内存和计算单元之间不高效的数据移动引起的延迟和停滞（D. Narayanan 等人，2021a；Xingyu，2019）。

量化这一挑战的一种方法是将数据传输时间与计算所需时间进行比较。具体来说，我们定义内存传输时间为 <semantics><mrow><msub><mi>T</mi><mtext mathvariant="normal">mem</mtext></msub><mo>=</mo><mfrac><msub><mi>M</mi><mtext mathvariant="normal">total</mtext></msub><msub><mi>B</mi><mtext mathvariant="normal">mem</mtext></msub></mfrac><mo>,</mo></mrow> <annotation encoding="application/x-tex">T_{\text{mem}} = \frac{M_{\text{total}}}{B_{\text{mem}}},</annotation></semantics> 其中 <semantics><msub><mi>M</mi><mtext mathvariant="normal">total</mtext></msub><annotation encoding="application/x-tex">M_{\text{total}}</annotation></semantics> 是总数据量，而 <semantics><msub><mi>B</mi><mtext mathvariant="normal">mem</mtext></msub><annotation encoding="application/x-tex">B_{\text{mem}}</annotation></semantics> 是可用的内存带宽。相比之下，计算时间由 <semantics><mrow><msub><mi>T</mi><mtext mathvariant="normal">compute</mtext></msub><mo>=</mo><mfrac><mtext mathvariant="normal">FLOPs</mtext><msub><mi>P</mi><mtext mathvariant="normal">peak</mtext></msub></mfrac><mo>,</mo></mrow> <annotation encoding="application/x-tex">T_{\text{compute}} = \frac{\text{FLOPs}}{P_{\text{peak}}},</annotation></semantics> 给出，其中浮点运算次数（FLOPs）除以峰值硬件吞吐量，<semantics><msub><mi>P</mi><mtext mathvariant="normal">peak</mtext></msub><annotation encoding="application/x-tex">P_{\text{peak}}</annotation></semantics>。当 <semantics><mrow><msub><mi>T</mi><mtext mathvariant="normal">mem</mtext></msub><mo>></mo><msub><mi>T</mi><mtext mathvariant="normal">compute</mtext></msub></mrow><annotation encoding="application/x-tex">T_{\text{mem}} > T_{\text{compute}}</annotation></semantics> 时，系统变为内存受限，这意味着处理元素花费更多的时间等待数据而不是执行计算。这种不平衡表明了需要内存优化的架构和高效的数据移动策略来维持高性能。

图 11.6 展示了模型增长与硬件内存能力之间的新兴挑战，阐述了“AI 内存墙”。该图以对数尺度跟踪了 AI 模型大小（红色点）和硬件内存带宽（蓝色点）随时间的变化。模型参数呈指数增长，从 2012 年 AlexNet 的约 6230 万个参数到 2023 年 Gemini 1 的万亿级参数，如更陡峭的红色趋势线所示。相比之下，硬件内存带宽，由 NVIDIA GPU 的连续几代（约 100-200 GB/s）和 Google TPUs（约 2-3 TB/s）代表，增长更为缓慢（蓝色趋势线）。这些趋势线之间的扩展阴影区域对应于“AI 内存墙”，这将是一个架构挑战，其中模型扩展速度超过可用的内存带宽。这种不断扩大的差异需要越来越复杂的内存管理和模型优化技术，以保持计算效率。

#### 不规则内存访问

与遵循结构化和可预测模式的传统计算工作负载不同，机器学习模型通常表现出不规则内存访问行为，这使得高效的数据检索成为挑战。这些不规则性是由于机器学习计算的本质，其中内存访问模式受批量大小、层类型和稀疏性等因素的影响。因此，标准的缓存机制和内存层次结构往往难以优化性能，导致内存延迟增加和带宽利用率低下。

![图片](img/file185.png)

图 11.6：**AI 内存墙**：该图强调了模型大小与硬件内存带宽之间的不断扩大的差异，说明了随着模型变得更加复杂，维持性能的挑战。

为了更好地理解机器学习工作负载与传统计算工作负载的不同，比较它们各自的内存访问模式（表 11.8）是有用的。传统的任务，如科学计算、通用 CPU 应用程序和数据库处理，通常表现出定义良好的内存访问特征，这些特征受益于标准的缓存和预取技术。另一方面，机器学习工作负载引入了高度动态的访问模式，这对传统的内存优化策略构成了挑战。

机器学习工作负载中不规则性的一个关键来源是批量大小和执行顺序。输入数据在批量中的处理方式直接影响内存重用，从而产生复杂的优化挑战。小批量大小降低了重用缓存激活和权重的可能性，导致频繁地从较慢的片外内存中获取内存。较大的批量大小可以提高重用并分摊内存访问成本，但同时也对可用的内存带宽提出了更高的要求，可能在不同的内存层次结构级别上造成拥塞。这种微妙的平衡需要仔细考虑模型架构和可用的硬件资源。

表 11.8：**内存访问特性**：传统工作负载表现出可预测的、顺序的内存访问，得益于标准缓存，而机器学习工作负载由于稀疏性和数据依赖性引入了不规则和动态的模式，这挑战了传统的内存优化技术。理解这些差异对于设计能够高效支持现代人工智能应用独特需求的内存系统至关重要。

| **特征** | **传统计算工作负载** | **机器学习工作负载** |
| --- | --- | --- |
| **内存访问模式** | 规则且可预测的（例如，顺序读取，结构化模式） | 不规则且动态的（例如，稀疏性，注意力机制） |
| **缓存局部性** | 高时间局部性和空间局部性 | 通常局部性较低，尤其是在大型模型中 |
| **数据重用** | 具有频繁数据重用的结构化循环 | 依赖于层类型的稀疏和动态重用 |
| **数据依赖性** | 明确的依赖性允许高效的预取 | 基于网络结构的可变依赖性 |
| **工作负载示例** | 科学计算（例如，矩阵分解，物理模拟） | 神经网络（例如，CNN，Transformer，稀疏模型） |
| **内存瓶颈** | DRAM 延迟，缓存未命中 | 片外带宽限制，内存碎片化 |
| **对能耗的影响** | 中等，由密集的浮点运算执行驱动 | 高，主要由数据移动成本主导 |

不同的神经网络层在考虑批量大小之外，以不同的方式与内存交互。卷积层得益于空间局部性，因为图像中的相邻像素一起处理，从而能够高效地缓存小的权重内核。相反，全连接层需要频繁访问大型权重矩阵，这通常会导致与标准缓存策略不匹配的更多随机内存访问模式。Transformer 引入了额外的复杂性，因为注意力机制需要访问存储在各个内存位置的大型键值对。序列长度和注意力范围的动态性质使得传统的预取策略无效，导致不可预测的内存延迟。

另一个导致不规则内存访问的显著因素是神经网络中的稀疏性 25。许多现代机器学习模型采用权重剪枝、激活稀疏性和结构化稀疏性等技术来减少计算开销。然而，这些优化通常导致非均匀的内存访问，因为稀疏表示需要检索分散的元素而不是顺序块，这使得硬件缓存效率降低。包含动态计算路径的模型，如专家混合和自适应计算时间，引入了高度非确定性的内存访问模式，其中活动的神经元或模型组件可以随着每个推理步骤而变化。这种可变性对有效的预取和缓存策略构成了挑战。

这些不规则性具有重大影响。机器学习工作负载通常经历缓存效率降低，因为激活和权重可能不会以可预测的顺序访问。这导致对片外内存流量的依赖性增加，从而减慢执行速度并消耗更多能量。不规则访问模式导致内存碎片化，其中数据分配和检索的方式导致可用内存资源的低效利用。综合影响是，机器学习加速器经常遇到内存瓶颈，限制了它们充分利用可用计算能力的能力。

### 内存层次结构

为了解决机器学习加速中的内存挑战，硬件设计师实施了复杂的内存层次结构，以平衡速度、容量和能源效率。在检查不同机器学习架构如何利用内存资源之前，理解这一层次结构是至关重要的。与通用计算不同，其中内存访问模式通常是不可预测的，机器学习工作负载表现出结构化的重用模式，这些模式可以通过在多个内存级别上仔细组织数据来优化。

在最高级别，大容量但速度较慢的存储设备提供长期模型存储。在最低级别，高速寄存器和缓存确保计算单元可以以最小的延迟访问操作数。在这两个极端之间，中间内存级别，如暂存内存、高带宽内存和片外 DRAM，在性能和容量之间提供权衡。

表 11.9 总结了现代人工智能加速器中不同内存级别的关键特性。层次结构中的每一级都具有独特的延迟、带宽和容量属性，这些属性直接影响神经网络数据，如权重、激活和中间结果，应该如何分配。

表 11.9：**内存层次结构权衡**：AI 加速器利用多级内存层次结构来平衡性能和容量，优化计算密集型机器学习任务的数据访问。每一层提供独特的延迟、带宽和容量特性，决定了神经网络组件（权重、激活和中间结果）应该如何战略性地分配，以最小化瓶颈并最大化吞吐量。

| **Memory Level** | **Approx. Latency** | **Bandwidth** | **Capacity** | **Example Use in Deep Learning** |
| --- | --- | --- | --- | --- |
| **Registers** | ~1 cycle | Highest | Few values | 存储即时计算的操作数 |
| **L1/L2 Cache (SRAM)** | ~1-10 ns | High | KBs-MBs | 缓存频繁访问的激活和小型权重块 |
| **Scratchpad Memory** | ~5-20 ns | High | MBs | 软件管理的中间计算存储 |
| **High-Bandwidth Memory (HBM)** | ~100 ns | Very High | GBs | 存储大型模型参数和激活，以实现高速访问 |
| **Off-Chip DRAM (DDR, GDDR, LPDDR)** | ~50-150 ns | Moderate | GBs-TBs | 存储整个模型权重，这些权重无法存储在芯片上 |
| **Flash Storage (SSD/NVMe)** | ~100 µs - 1 ms | Low | TBs | 存储预训练模型和检查点以供后续加载 |

#### On-Chip Memory

内存层次结构的每一层在 AI 加速中都扮演着独特的角色，在速度、容量和可访问性方面有不同的权衡。位于计算核心内的寄存器提供最快的访问，但一次只能存储几个操作数。这些寄存器最适合用于即时计算，其中操作数可以在几个周期内加载和消耗。然而，由于寄存器存储非常有限，需要频繁的内存访问来获取新的操作数并存储中间结果。

为了减少寄存器和外部内存之间不断的数据移动需求，小型但快速的缓存作为中介缓冲区。这些缓存存储最近访问的激活、权重和中间值，确保频繁使用的数据以最小的延迟保持可用。然而，缓存的容量有限，使得它们不足以存储完整的特征图或大型权重张量在机器学习模型中。因此，在任何给定时间，模型参数或激活的最常用部分只能存储在这里。

对于较大的工作数据集，许多 AI 加速器包括临时存储器，它提供的存储空间比缓存更多，但有一个关键的区别：它允许显式软件控制存储的数据以及何时将其移除。与依赖于基于硬件的淘汰策略的缓存不同，临时存储器使机器学习工作负载能够保留关键值，如激活和多个计算层的滤波器权重。这种能力在卷积神经网络等模型中特别有用，在这些模型中，相同的输入特征图和滤波器权重在多个操作中被重复使用。通过将数据保留在临时存储器中而不是从外部内存重新加载，加速器可以显著减少不必要的内存传输并提高整体效率 (Y.-H. Chen, Emer, and Sze 2017)。

#### 芯片外存储器

除了片上内存之外，高带宽内存提供了对不适合缓存或临时缓冲区的大模型参数和激活的快速访问。HBM 通过堆叠多个内存芯片和采用宽内存接口来实现其高性能，与传统的 DRAM 相比，它以最小的延迟传输大量数据。由于其高带宽和低延迟，HBM 通常用于存储在执行期间必须快速访问的整个机器学习模型层。然而，其成本和功耗限制了其在高性能 AI 加速器中的使用，使其在边缘设备等功耗受限的环境中不太常见。

当机器学习模型超出片上内存和 HBM 的容量时，它必须依赖于芯片外 DRAM，如 DDR、GDDR 或 LPDDR。虽然 DRAM 提供了显著更大的存储容量，但其访问延迟更高，这意味着频繁从 DRAM 中检索数据可能会引入执行瓶颈。为了有效地使用 DRAM，模型必须构建成在任何给定时间只检索必要的权重和激活部分，以最小化长时间内存检索时间的影响。

在层次结构的最高级别，闪存存储和固态硬盘（SSD）存储大型预训练模型、数据集和检查点权重。这些存储设备提供了大容量，但速度太慢，无法进行实时执行，需要在计算开始之前将模型加载到更快的内存层级。例如，在训练场景中，存储在 SSD 中的检查点模型必须在恢复计算之前加载到 DRAM 或 HBM 中，因为直接从 SSD 执行会太慢，无法维持高效的加速器利用率 (D. Narayanan et al. 2021a)。

内存层次结构平衡了速度、容量和能源效率之间的竞争目标。然而，通过多个内存级别移动数据引入了瓶颈，限制了加速器的性能。内存级别之间的数据传输产生延迟成本，尤其是对于片外访问。有限的带宽限制了内存层级之间的数据流动。内存容量限制迫使模型超过本地存储时不断移动数据。这些限制使内存带宽成为现实世界加速器性能的基本决定因素。

### 内存带宽和架构权衡

在第 11.4.1 节中建立的内存墙分析的基础上，本节量化了特定带宽特性如何影响不同部署场景下的系统性能。

现代加速器表现出不同的带宽-容量权衡：NVIDIA H100 GPU 提供 3.35 TB/s HBM3 带宽和 80 GB 容量，优化了跨不同工作负载的灵活性。谷歌的 TPUv4 提供 1.2 TB/s 带宽和 128 MB 片上内存，优先考虑张量操作的能量效率。这个 3:1 的带宽优势使 H100 能够处理内存密集型模型，如大型语言模型，而 TPU 的较低带宽足以处理计算密集型推理，因为数据重用性更高。

不同的神经网络操作实现不同的带宽利用率：由于访问模式不规则，transformer 注意力机制仅达到峰值内存带宽的 20-40%，卷积层通过可预测的空间访问模式达到 60-85%，当批量大小超过 128 时，全连接层接近 90%。

如前所述，片上内存访问消耗 5-10 pJ 每访问，而外部 DRAM 每访问需要 640 pJ——这是一项 65-125<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的能量惩罚。AI 加速器通过三种关键策略最小化 DRAM 访问：权重稳定性（保持模型参数在片上内存中）、输入稳定性（局部缓冲输入激活）和输出稳定性（在片上累积部分和）。

内存带宽扩展在加速器设计中遵循不同的轨迹：

+   **GPU 扩展**：带宽随着内存通道的增加而线性增长，从 900 GB/s（A100）到 3,350 GB/s（H100），从而支持更大的模型

+   **TPU 扩展**：通过阵列设计优化带宽，实现 900 GB/s，比 GPU 替代方案低 35%的功耗

+   **移动加速器扩展**：苹果的 M3 神经网络引擎通过积极的电压扩展实现 400 GB/s 的统一内存带宽，同时消耗<5 W

与 DDR5 每 GB 0.05 美元相比，HBM 内存的成本为每 GB 8-15 美元，造成 160-300<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的成本差异。高性能加速器需要 40-80GB HBM 以实现有竞争力的性能，这增加了 320-1,200 美元的制造成本。边缘加速器牺牲带宽（50-200 GB/s）以实现低于 100 美元的成本目标，同时保持足够的性能以满足推理工作负载。

这些带宽特性直接影响部署决策：云训练优先考虑原始带宽以实现最大模型容量，边缘推理优化带宽效率以适应能源限制，而移动部署在带宽和成本限制之间取得平衡。虽然这些硬件特定的优化是基本的，但第九章中全面介绍了结合硬件加速和软件优化技术的集成系统级效率方法。这些优化在不同系统环境中的部署——从第二章中的移动设备到第十三章中的生产工作流程——决定了它们的实际影响。

### 主机-加速器通信

机器学习加速器，如 GPU 和 TPU，通过并行执行实现高计算吞吐量。然而，它们的效率从根本上受到主机（CPU）和加速器内存之间数据传输的限制。与完全在 CPU 内存子系统中运行的一般工作负载不同，AI 工作负载需要在 CPU 主内存和加速器之间频繁传输数据，这引入了延迟，消耗了带宽，并影响了整体性能。

主机-加速器数据传输遵循一个结构化的顺序，如图 图 11.7 所示。在计算开始之前，数据从 CPU 内存复制到加速器的内存。然后 CPU 发出执行指令，加速器并行处理数据。一旦计算完成，结果存储在加速器内存中，并传输回 CPU。每一步都可能引入潜在的低效，必须管理这些低效以优化性能。

![图片](img/file186.svg)

图 11.7：**主机-加速器数据传输**：AI 工作负载需要在 CPU 内存和加速器之间频繁移动数据；此图详细说明了复制输入数据、执行计算和传输结果的顺序步骤，每一步都可能引入潜在的性能瓶颈。理解这个数据传输序列对于优化 AI 系统性能和最小化延迟至关重要。

主机-加速器数据传输中的关键挑战包括延迟、带宽限制和同步开销。通过高效的内存管理和互连技术优化数据传输对于最大化加速器利用率至关重要。

#### 数据传输模式

ML 加速器的效率不仅取决于其计算能力，还取决于数据的持续供应。即使高性能的 GPU 和 TPU，如果数据传输效率低下，也会被低效利用。主机和加速器内存作为独立的域存在，需要通过 PCIe、NVLink 或专有链路等互连进行显式传输。无效的数据移动可能导致执行停滞，因此传输优化至关重要。

图 11.7 展示了这种结构化序列。在步骤（1）中，数据从 CPU 内存复制到加速器内存，因为 GPU 无法以高速直接访问主机内存。通常，直接内存访问（DMA）26 引擎处理这种传输而不会消耗 CPU 周期。在步骤（2）中，CPU 通过 CUDA、ROCm 或 OpenCL 等 API 发出执行命令。步骤（3）涉及加速器上的并行执行，如果需要数据时数据不可用，可能会发生停滞。最后，在步骤（4）中，计算结果被复制回 CPU 内存以进行进一步处理。

延迟和带宽限制对 AI 工作负载有显著影响。PCIe 的峰值带宽为 32 GB/s（PCIe 4.0），比加速器的高带宽内存慢得多，后者可以超过 1 TB/s。大数据传输加剧了瓶颈，尤其是在深度学习任务中。此外，当计算必须等待数据传输完成时，还会出现同步开销。有效的调度和与执行重叠传输是减轻这些低效的关键。

#### 数据传输机制

主机（CPU）和加速器（GPU、TPU 或其他 AI 硬件）之间的数据移动取决于连接两个处理单元的互连技术。互连的选择决定了传输可用带宽、通信延迟以及主机-加速器执行的整体效率。最常用的传输机制包括 PCIe（外围组件互连扩展）、NVLink、直接内存访问和统一内存架构。这些机制中的每一个都在优化图 11.7 中所示的四个步骤数据移动过程中发挥着关键作用。

##### PCIe 接口

大多数加速器通过 PCIe 与 CPU 通信，PCIe 是数据移动的行业标准互连。PCIe 4.0 提供高达 32 GB/s 的带宽，而 PCIe 5.0 将此翻倍至 64 GB/s。然而，这仍然远低于加速器内部的 HBM 带宽，使得 PCIe 成为大型 AI 工作负载的瓶颈。

PCIe 由于其基于数据包的通信和内存映射 I/O 模型，也引入了延迟开销。频繁的小数据传输效率低下，因此批量数据移动可以减少开销。通过 PCIe 发出的计算命令进一步增加了延迟，需要仔细优化执行调度。

##### NVLink 接口

为了解决 PCIe 的带宽限制，NVIDIA 开发了 NVLink，这是一种专有高速互连，在 GPU 之间以及在某些配置中在 CPU 和 GPU 之间提供了显著更高的带宽。与作为共享总线的 PCIe 不同，NVLink 允许连接设备之间的直接点对点通信，减少争用并提高 AI 工作负载的效率。

对于主机-加速器传输，NVLink 可以在步骤（1）中使用，以远超 PCIe 的速度将输入数据从主内存传输到 GPU 内存，NVLink 4.0 的带宽可达 600 GB/s。这显著减少了数据移动瓶颈，允许加速器以更低的延迟访问输入数据。在多 GPU 配置中，NVLink 还加速了端到端传输，允许加速器交换数据而无需通过主内存路由，从而优化计算过程的步骤（3）。

尽管 NVLink 提供了显著的性能优势，但它并非普遍可用。与适用于所有加速器的行业标准 PCIe 不同，NVLink 仅适用于 NVIDIA 硬件，限制了其在配备 NVLink 启用 GPU 的系统中的应用。

##### 数据传输的 DMA

在传统的内存传输中，CPU 发出加载/存储指令，消耗处理周期。DMA 卸载这项任务，允许在不干预 CPU 的情况下异步移动数据。

在数据传输过程中，CPU 发起 DMA 请求，允许数据在后台复制到加速器内存。同样，结果传输回主内存不会阻塞执行。这使计算与数据移动重叠，减少空闲时间，提高加速器利用率。

DMA 对于启用异步数据移动至关重要，这允许传输与计算重叠。AI 工作负载不需要在传输完成之前开始执行，可以在早期计算仍在进行时将数据流式传输到加速器，减少空闲时间并提高加速器利用率。

##### 统一内存

虽然 PCIe、NVLink 和 DMA 优化了显式内存传输，但某些 AI 工作负载需要更灵活的内存模型，以消除手动数据复制的需求。统一内存提供了一个抽象层，允许主机和加速器访问单个共享内存空间，在需要时自动处理数据移动。

使用统一内存，在执行前不需要在 CPU 和 GPU 内存之间显式复制数据。相反，当计算需要位于主机内存中的内存区域时，系统会自动将其迁移到加速器，透明地处理步骤（1）。同样，当 CPU 访问计算结果时，步骤（4）会自动发生，消除了手动内存管理的需求。

尽管统一内存简化了编程，但它引入了性能权衡。由于内存迁移是在需要时发生的，这可能导致不可预测的延迟，尤其是当需要频繁传输大数据集时。此外，由于统一内存是通过页面迁移技术实现的，小的内存访问可能会触发过多的数据移动，进一步降低效率。

对于需要精细内存控制的人工智能工作负载，使用 PCIe、NVLink 和 DMA 进行显式数据传输通常提供更好的性能。然而，对于开发简便性比绝对速度更重要的应用程序，统一内存提供了一个方便的替代方案。

#### 数据传输开销

主机-加速器数据移动引入了影响人工智能工作负载执行的开销。与在芯片内存访问相比，主机-加速器传输穿越系统互连，增加了延迟、带宽限制和同步延迟。

互连延迟影响传输速度，PCIe，作为标准的主机-加速器链路，由于基于分组的交易和内存映射 I/O 而承担了显著的开销。这使得频繁的小传输变得低效。更快的替代品如 NVLink 减少了延迟并提高了带宽，但仅限于特定的硬件生态系统。

同步延迟进一步加剧了低效。同步传输会阻塞执行，直到数据移动完成，确保数据一致性，但引入了空闲时间。异步传输允许计算和数据移动重叠，减少停滞，但需要仔细协调以避免执行不匹配。

这些因素，包括互连延迟、带宽限制和同步开销，决定了人工智能工作负载的效率。虽然优化技术减轻了这些限制，但了解这些基本的传输机制对于提高性能至关重要。

### 模型内存压力

机器学习模型强加不同的内存访问模式，这显著影响了加速器的性能。数据在主机和加速器之间传输的方式、内存访问的频率以及缓存机制的效率都决定了整体执行效率。虽然多层感知器（MLPs）、卷积神经网络（CNNs）和变换器网络各自需要大量的参数集，但它们不同的内存需求需要为加速器制定定制化的优化策略。了解这些差异有助于理解为什么不同的硬件架构在工作负载中表现出不同的效率水平。

#### 多层感知器

MLPs，也称为全连接网络，是神经网络架构中最简单的之一。每一层都由密集矩阵乘法组成，要求每个神经元与前一层的所有神经元进行交互。这导致了高内存带宽需求，尤其是对于权重，因为每个输入激活都会对大量计算有贡献。

从内存角度来看，MLPs 依赖于大型的密集权重矩阵，这些矩阵经常超过片上内存容量，需要片外内存访问。由于加速器不能直接以高速访问主机内存，数据传输必须通过 PCIe 或 NVLink 等互连显式管理。这些传输引入了延迟并消耗了带宽，影响了执行效率。

尽管 MLPs 具有带宽密集的特性，但它们表现出规律和可预测的内存访问模式，这使得它们适合进行预取和流式内存访问等优化。专用 AI 加速器通过在快速 SRAM 缓存中分阶段存储权重矩阵，并通过直接内存访问引擎重叠数据移动与计算来减轻传输开销，减少执行停滞。这些优化使得加速器即使在处理大型参数集时也能保持高吞吐量 (Y.-H. Chen, Emer, and Sze 2017)。

#### Convolutional Neural Networks

卷积神经网络（CNNs）在图像处理和计算机视觉任务中得到了广泛应用。与需要密集矩阵乘法的 MLPs 不同，CNNs 使用小的滤波核在图像上滑动来处理输入特征图。这种局部计算结构导致了高空间数据重用，即相同的输入像素对多个卷积有贡献。

CNN 加速器受益于片上内存优化，因为卷积滤波器具有广泛的复用性，允许权重存储在快速的片上 SRAM 中，而不是频繁访问片外内存。然而，由于激活图的大小，它们需要仔细管理。由于通过 PCIe 等互连访问主内存会引入延迟和带宽瓶颈，CNN 加速器采用分块技术将特征图划分为更小的区域，这些区域适合片上缓冲区。这最小化了昂贵的片外内存传输，提高了整体效率 (Y.-H. Chen, Emer, and Sze 2017)。

虽然 CNN 工作负载比 MLPs 更节省内存，但管理中间激活仍然是一个挑战。加速器使用分层缓存策略和 DMA 引擎来优化内存移动，确保计算不会被低效的主机-加速器数据传输所阻塞。这些内存优化有助于 CNN 加速器通过减少对片外内存带宽的依赖来保持高吞吐量 (Y.-H. Chen, Emer, and Sze 2017)。

#### Transformer Networks

变压器已成为自然语言处理的主导架构，并在视觉和语音识别等其他领域得到越来越广泛的应用。与依赖于局部计算的 CNN 不同，变压器执行全局注意力机制，其中输入序列中的每个标记都可以与其他所有标记交互。这导致不规则的、带宽密集型的内存访问模式，因为必须频繁地检索和更新大型键值矩阵。

这些模型由于其庞大的参数大小，通常超过片上内存容量，对加速器来说尤其具有挑战性。因此，主机和加速器之间频繁的内存传输引入了大量的延迟开销，尤其是在依赖于 PCIe 等互连的情况下。统一内存架构可以通过动态处理数据移动来缓解这些问题，但它们由于不可预测的按需内存迁移而引入了额外的延迟。由于变压器是内存密集型而非计算密集型，针对它们的优化加速器依赖于高带宽内存、张量分块和内存分区以维持性能（T. B. Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, 等人 2020）。

此外，注意缓存机制和专门的张量布局减少了冗余的内存访问，提高了执行效率。鉴于传统互连的带宽限制，NVLink 启用的架构在大型变压器训练方面提供了显著优势，因为它们比 PCIe 提供了更高的吞吐量和更低的延迟。基于 DMA 的异步内存传输允许在数据移动的同时重叠计算，减少执行停滞（D. Narayanan 等人 2021a）。

### ML 加速器的影响

MLP、CNN 和 Transformer 多样化的内存需求突显了根据特定工作负载定制内存架构的必要性。表 11.10 比较了这些不同模型之间的内存访问模式。

表 11.10：**机器学习模型内存访问**：不同的机器学习模型由于权重大小、激活重用和数据稀疏度的变化表现出不同的内存访问模式和瓶颈；这些特性对硬件加速器设计和性能优化有重大影响。由于其巨大且稀疏访问的权重，变压器需要高带宽和容量，而 CNN 则受益于空间局部性和高激活重用，从而降低了内存压力。

| **模型类型** | **权重大小** | **激活重用** | **内存访问模式** | **主要瓶颈** |
| --- | --- | --- | --- | --- |
| **MLP (密集)** | 大型，密集 | 低 | 规则的，顺序的（流式） | 带宽（片外） |
| **CNN** | 小型，可重用 | 高 | 空间局部性 | 特征图移动 |
| **Transformer** | 大型、稀疏 | 低 | 不规则、高带宽 | 内存容量 + 互连 |

每种模型类型都面临着独特的挑战，这些挑战直接影响加速器设计。MLPs 受益于快速流式访问密集权重矩阵，这使得内存带宽成为性能的关键因素，尤其是在将大型权重从主机内存传输到加速器内存时。CNNs 由于其高激活重用和结构化内存访问模式，可以利用片上缓存和分块策略来最小化片外内存传输。然而，Transformers 对带宽和容量都有显著需求，因为注意力机制需要频繁访问大型键值矩阵，导致高互连流量和增加的内存压力。

为了应对这些挑战，现代人工智能加速器采用多层内存层次结构，以平衡速度、容量和能源效率。片上 SRAM 缓存和暂存内存存储频繁访问的数据，而高带宽的外部内存为大型模型提供可扩展性。高效的互连，如 NVLink，有助于缓解主机-加速器传输瓶颈，尤其是在 Transformer 工作负载中，内存移动限制可能会主导执行时间。

随着机器学习工作负载的复杂性持续增长，内存效率变得与原始计算能力一样关键。分析揭示了内存系统如何主导加速器的性能：DRAM 访问的 173 倍能量惩罚造成了一个基本瓶颈，精心设计的内存层次结构可以提高有效带宽 10-100 倍，不同的神经网络架构产生不同的内存压力模式。这些限制——从带宽限制到通信开销——决定了理论计算能力是否转化为实际性能。在确定了内存系统如何限制加速器有效性之后，我们现在考察映射策略如何系统地解决这些限制。

## 神经网络硬件映射基础

在上一节中考察的内存系统挑战——带宽限制、分层访问成本和模型特定的压力模式——直接决定了神经网络在加速器上的执行效率。如果一个具有 1,200 GB/s 片上带宽和复杂内存层次结构的阵列在映射计算时没有考虑这些内存访问模式，那么它将无法带来性能上的好处。如第 11.4.1 节所确立的，内存访问的极端能量惩罚意味着映射策略必须优先考虑数据重用和局部性，而忽略其他所有考虑因素。这一现实推动了系统化映射方法的需求，这些方法协调计算放置、内存分配和数据移动，以利用硬件能力同时尊重内存限制。

在专用人工智能加速硬件上高效执行机器学习模型需要一种结构化的计算方法，确保充分利用可用资源，同时最小化性能瓶颈。这些映射考虑在分布式训练场景中尤为重要，如第八章[（ch014.xhtml#sec-ai-training）]所述。与依赖于动态任务调度的通用处理器不同，人工智能加速器在结构化执行模型下运行，通过仔细分配计算到处理元素来最大化吞吐量。这个过程，称为映射，决定了计算如何在硬件资源之间分布，影响执行速度、内存访问模式和整体效率。

**人工智能加速中的映射**是通过空间分配、时间调度和内存感知放置将机器学习*计算*分配给*硬件单元*的过程，以最大化执行效率和资源利用率。

由于硬件限制和模型架构的多样性，将机器学习模型映射到人工智能加速器面临几个挑战。考虑到现代加速器的层次化内存系统，映射策略必须仔细管理何时以及在哪里访问数据，以最小化延迟和功耗，同时确保计算单元保持活跃。不良的映射决策可能导致计算资源利用率低下、数据移动过多和执行时间增加，最终降低整体效率。

要理解这一挑战的复杂性，可以考虑一个类比：将神经网络映射到加速器就像规划一个大规模、全厂范围的组装过程。你拥有数千名工人（处理元素）和一套复杂的任务（计算）。你必须决定哪个工人执行哪个任务（计算放置），在哪里存储他们需要的部件（内存分配），以及精确的操作顺序以最小化行走时间（数据流）。计划中的微小变化可能导致工厂产出出现巨大差异。就像一个组织不善的工厂可能会让一些工人闲置，而其他人则不堪重负，或者材料存储得太远，难以到达所需位置一样，一个映射不当的神经网络可能会让处理元素利用率低下，同时造成内存瓶颈，阻碍整个系统的运行。

映射包括三个相互关联的方面，这些方面构成了有效人工智能加速器设计的基石。

+   **计算放置**：系统性地将操作（例如，矩阵乘法、卷积）分配给处理元素，以最大化并行性和减少空闲时间。

+   **内存分配**：仔细确定模型参数、激活和中间结果在内存层次结构中的位置，以优化访问效率。

+   **数据流和执行调度**：结构化计算单元之间的数据流动，以减少带宽瓶颈并确保平稳、连续的执行。

有效的映射策略最小化芯片外内存访问，最大化计算利用率，并有效地管理不同内存层次之间的数据移动。

**编译器的作用**

开发者很少手动执行这种复杂的映射。相反，一个专门的**编译器**（如 NVIDIA 的 NVCC 或 Google 的 XLA）从框架中提取高级模型，并自动探索映射搜索空间，以找到针对目标硬件的最佳执行计划。编译器是至关重要的软件层，它将模型的计算图转换为高效的硬件特定数据流，平衡上述计算放置、内存分配和执行调度三个相互关联的方面。这种编译器支持在第 11.7 节中进行了详细探讨。

以下部分探讨了影响执行效率的关键映射选择，并为优化策略奠定了基础，这些策略可以细化这些决策。

### 计算放置

现代人工智能加速器旨在以大规模并行执行机器学习模型，使用数千到数百万个处理元素同时进行计算。然而，仅仅拥有许多计算单元是不够的。如何将这些计算分配给这些单元决定了整体效率。

没有仔细的放置，一些处理元素可能会闲置，而其他处理元素可能会过载，导致资源浪费、内存流量增加和性能降低。计算放置是将操作战略性地映射到可用硬件资源的过程，以维持高吞吐量、最小化停滞并优化执行效率。

#### 计算放置定义

人工智能加速器包含数千到数百万个处理元素，使得计算放置成为一个大规模问题。现代 GPU，如 NVIDIA H100，拥有超过 16,000 个流处理器和超过 500 个专门的张量核心，每个核心都旨在加速矩阵运算（Choquette 2023a）。TPU 使用由数千个相互连接的乘加（MAC）单元组成的收缩阵列，而像 Cerebras 的 CS-2 这样的晶圆级处理器将并行性进一步推向极致，在一个芯片上集成了超过 850,000 个核心（Systems 2021b）。在这些架构中，计算放置的微小低效可能导致显著的性能损失，因为空闲核心或过度的内存移动在整个系统中累积。

计算放置确保所有处理单元都能有效地参与执行。这意味着工作负载必须以避免不平衡执行的方式分配，即某些处理单元空闲，而其他处理单元过载。同样，放置必须最小化不必要的数据移动，因为过度的内存传输会引入延迟和功耗，从而降低系统性能。

神经网络计算因模型架构的不同而有很大差异，这影响着放置策略的应用。例如，在卷积神经网络（CNN）中，放置策略侧重于将图像区域分配到处理单元中，以最大化并行性。一个通过数千个 GPU 核心处理的<semantics><mrow><mn>256</mn><mo>×</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">256\times256</annotation></semantics>图像可能会被分割成小块，每块映射到不同的处理单元以同时执行卷积操作。相比之下，基于转换器的模型需要适应自注意力机制的放置策略，其中序列中的每个标记都与所有其他标记交互，导致不规则和内存密集的计算模式。同时，图神经网络（GNNs）引入了额外的复杂性，因为计算依赖于稀疏和动态的图结构，这需要自适应的工作负载分配（Zheng 等人 2020）。

由于计算放置直接影响资源利用率、执行速度和功耗效率，它是人工智能加速中最关键的因素之一。一个放置得当的计算可以降低延迟几个数量级，而一个放置不当的计算可能导致数千个处理单元利用率低下。下一节将探讨为什么高效的计算放置至关重要以及次优映射策略的后果。

#### 计算放置的重要性

虽然计算放置是一个硬件驱动的过程，但其重要性从根本上是由神经网络工作负载的结构所决定的。不同类型的机器学习模型表现出不同的计算模式，这直接影响它们如何有效地映射到加速器上。如果没有仔细的放置，工作负载可能会变得不平衡，内存访问模式可能变得低效，系统的整体性能可能会显著下降。

对于具有结构化计算模式的模型，如 CNN，计算放置相对简单。CNN 使用过滤器处理图像，这些过滤器应用于小而局部的区域，这意味着它们的计算可以在处理单元之间均匀分布。由于这些操作高度可并行化，CNN 从空间分区中受益，输入被分割成独立处理的瓦片。这种结构化执行使 CNN 非常适合那些偏好常规数据流的加速器，从而最小化放置决策的复杂性。

然而，对于具有不规则计算模式的模型，例如变换器和 GNNs，计算放置变得显著更具挑战性。依赖于自注意力机制的变换器需要序列中的每个标记与所有其他标记进行交互，从而导致非均匀的计算需求。与每个处理元素执行相似工作量计算的 CNNs 不同，变换器引入了工作负载不平衡，其中某些操作，包括注意力分数的计算，需要比其他操作更多的计算。如果没有仔细放置，这种不平衡可能导致停滞，其中一些处理元素保持空闲，而其他处理元素则难以跟上。

在图神经网络（GNNs）中，挑战更大，因为计算依赖于稀疏和动态变化的图结构。与在密集和规则结构数据上操作的 CNNs 不同，GNNs 必须以高度可变连接度处理节点和边。图的一些区域可能需要比其他区域显著更多的计算，这使得在处理元素之间平衡工作负载变得困难（Zheng 等人 2020）。如果没有战略性地放置计算，一些计算单元将保持空闲，而其他计算单元则可能过载，导致利用率低下和执行效率低下。

不良的计算放置通过创建工作负载不平衡、引起过多的数据移动以及导致执行停滞和瓶颈，对 AI 执行产生不利影响。计算的不均匀分布可能导致处理元素空闲，阻止硬件的充分利用并降低吞吐量。低效的执行分配通过需要在内存层次结构之间频繁传输数据，引入延迟并提高功耗，增加了内存流量。最后，这种错误分配可能导致操作在数据依赖上等待，从而导致流水线效率低下，最终降低整体系统性能。

计算放置确保模型在给定的独特计算结构下高效执行。放置得当的工作负载可以减少执行时间、内存开销和功耗，而放置不当的工作负载可能导致执行流水线停滞和资源利用效率低下。下一节将探讨必须解决的关键考虑因素，以确保计算放置既高效又能够适应不同的模型架构。

#### 有效的计算放置

计算放置是在硬件约束和工作负载特征之间的一种平衡行为。为了实现高效率，放置策略必须考虑并行性、内存访问和工作负载可变性，同时确保处理元素得到充分利用。不良的放置会导致执行不平衡、数据移动增加和性能下降，因此在设计放置策略时考虑关键因素至关重要。

如表 11.11 总结所示，计算放置面临几个影响执行效率的关键挑战。有效的映射策略必须通过平衡工作负载分配、最小化数据移动和优化处理元素之间的通信来解决这些挑战。

表 11.11：**计算放置挑战**：有效的神经网络部署需要战略性地将计算分配给处理元素，平衡工作负载分配、数据移动成本和硬件约束，以最大化执行效率并避免性能瓶颈。理解这些挑战指导了映射策略的设计，这些策略优化资源利用并最小化通信开销。

| **挑战** | **对执行的影响** | **放置的关键考虑因素** |
| --- | --- | --- |
| **工作负载不平衡** | 一些处理元素提前完成，而其他处理元素仍然超载，导致计算资源闲置。 | 均匀分配操作以防止停滞并确保处理元素充分利用。 |
| **不规则的计算模式** | 类似于变换器和 GNNs 的模型引入了非均匀的计算需求，使得静态放置困难。 | 使用基于工作负载特征的自适应放置策略来调整执行。 |
| **过多的数据移动** | 频繁的内存传输引入延迟并增加功耗。 | 将常用数据保持在计算单元附近并最小化片外内存访问。 |
| **有限的互连带宽** | 放置不当的操作可能造成拥塞，减慢处理元素之间的数据移动。 | 优化空间和时间放置以减少通信开销。 |
| **模型特定的执行需求** | CNNs、变换器和 GNNs 需要不同的执行模式，使得单一的放置策略无效。 | 针对每种模型类型的计算结构定制放置策略。 |

每个这些挑战都突显了计算放置的核心权衡：在最大化并行性的同时最小化内存开销。对于卷积神经网络（CNNs），放置策略优先考虑结构化分块以维持高效的数据重用。对于变换器（transformers），放置必须确保注意力层之间的执行平衡。对于图神经网络（GNNs），放置必须动态调整以适应稀疏计算模式。

除了模型特定的需求之外，有效的计算放置还必须是可扩展的。随着模型的大小和复杂性的增长，放置策略必须动态适应，而不是依赖于静态的执行模式。未来的 AI 加速器越来越多地集成运行时感知的调度机制，其中放置是基于实时工作负载行为而不是预定的执行计划进行优化的。

有效的计算放置需要平衡硬件能力和模型特性。下一节将探讨计算放置如何与内存分配和数据移动相互作用，确保人工智能加速器以峰值效率运行。

### 内存分配

高性能人工智能加速需要有效的内存分配。随着人工智能模型复杂性的增加，加速器必须管理大量的数据移动——加载模型参数、存储中间激活和处理梯度计算。这些数据在内存层次结构中的分配方式直接影响执行效率、功耗和整体系统吞吐量。

#### 内存分配定义

虽然计算放置决定了操作在哪里执行，但内存分配定义了数据在哪里存储以及在整个执行过程中如何访问。所有人工智能加速器都依赖于从片上缓存和擦除板到 HBM 和 DRAM 的分层内存系统。不良的内存分配可能导致过多的片外内存访问，增加带宽竞争并减慢执行速度。由于人工智能加速器在太浮点和百万亿浮点运算的规模上运行，低效的内存访问模式可能导致显著的性能瓶颈。

内存分配的主要目标是通过尽可能将频繁访问的数据保留在处理元素附近，以最小化延迟并减少功耗。不同的硬件架构实现了针对人工智能工作负载定制的内存层次结构。GPU 依赖于全局内存、共享内存和寄存器的组合，需要仔细的平铺策略来优化局部性（X. Qi, Kantarci, 和 Liu 2017）。TPU 使用片上 SRAM 擦除板，其中激活和权重必须被有效地预加载以维持阵列执行（Norman P. Jouppi 等人 2017c）。晶圆级处理器，拥有数十万个核心，需要复杂的内存分区策略以避免过多的互连流量（Systems 2021b）。在所有情况下，内存分配的有效性决定了人工智能执行的总体吞吐量、功耗和可扩展性。

内存分配通过数据存储和访问模式直接影响 AI 加速器的效率。与通用计算不同，在通用计算中，内存管理通过缓存和动态分配抽象化，AI 加速器需要显式的数据放置策略来维持高吞吐量并避免不必要的停滞。这在节拍阵列（图 11.4）中尤为明显，其中处理元素之间的节奏性数据流依赖于精确的时间内存访问模式。例如，在 TPU 的节拍阵列中，权重必须预先加载到片上暂存器中，并与输入激活同步流经阵列，以维持流水线计算流程。当内存分配效率不高时，AI 工作负载将遭受延迟开销、过度的功耗和限制计算性能的瓶颈。

#### 不同工作负载的内存挑战

神经网络架构具有不同的内存需求，这影响了适当分配的重要性。CNN 依赖于结构化和局部化的数据访问模式，这意味着不合理的内存分配可能导致冗余数据加载和缓存效率低下（Y.-H. Chen 等人 2016）。相比之下，变换器模型需要频繁访问大型模型参数和中间激活，这使得它们对内存带宽限制非常敏感。GNN 引入了更大的挑战，因为它们的非规则和稀疏数据结构导致不可预测的内存访问模式，可能导致内存资源使用效率低下。不合理的内存分配对 AI 执行有三个主要后果：

1.  **增加内存延迟**：当频繁访问的数据没有存储在正确的位置时，加速器必须从更高延迟的内存中检索它，从而减慢执行速度。

1.  **更高的功耗**：片外内存访问比片上存储消耗的能量显著更多，导致在规模上的低效。

1.  **降低计算吞吐量**：如果数据在需要时不可用，处理元素将保持空闲，从而降低系统的整体性能。

随着 AI 模型在规模和复杂性上的持续增长，可扩展和高效的内存分配的重要性也在增加。内存限制可以决定在给定的加速器上可以部署多大的模型，影响可行性和性能。

表 11.12：**内存分配挑战**：在 AI 加速器中，高效的内存管理需要在数据访问速度与硬件限制之间取得平衡，以减轻由延迟、带宽限制和不规则数据模式引起的性能瓶颈。解决这些挑战对于部署复杂模型至关重要，例如变换器和图模型，它们具有可变和苛刻的内存需求。

| **挑战** | **对执行的影响** | **分配的关键考虑因素** |
| --- | --- | --- |
| **高内存延迟** | 慢速数据访问延迟执行并降低吞吐量。 | 优先将频繁访问的数据放置在更快的内存位置。 |
| **片上存储有限** | 小型本地内存限制了计算单元附近可用的数据量。 | 高效分配存储以最大化数据可用性，同时不超过硬件限制。 |
| **高片外带宽需求** | 频繁访问外部内存会增加延迟和功耗。 | 通过仔细管理何时以及如何移动数据来减少不必要的内存传输。 |
| **不规则内存访问模式** | 一些模型需要不可预测地访问数据，导致内存使用效率低下。 | 组织内存布局以与访问模式对齐并最小化不必要的数据移动。 |
| **特定模型内存需求** | 不同的模型需要不同的分配策略来优化性能。 | 根据工作负载的结构和执行特性定制分配决策。 |

如表 11.12 所述，AI 加速器中的内存分配必须解决影响执行效率的几个关键挑战。有效的分配策略通过精心管理数据放置和移动来减轻高延迟、带宽限制和不规则访问模式。确保频繁访问的数据存储在更快的内存位置，同时最小化不必要的传输，对于保持性能和能源效率至关重要。

每个这些挑战都需要仔细的内存管理来平衡执行效率与硬件限制。虽然结构化模型可能从便于预测访问的明确定义的内存布局中受益，但其他模型，如基于转换器和基于图模型，需要更适应的分配策略来处理可变和复杂的内存需求。除了特定于工作负载的考虑之外，内存分配还必须是可扩展的。随着模型尺寸的持续增长，加速器必须动态管理内存资源，而不是依赖于静态的分配方案。确保在需要时可以访问常用数据，同时不超出内存容量，对于保持高效率至关重要。

### 组合复杂性

在 AI 加速器上高效执行机器学习模型需要仔细考虑放置和分配。放置涉及计算和数据的空间分配，而分配则涵盖资源的时序分布。这些决策相互依赖，每个决策都引入了影响性能、能效和可扩展性的权衡。表 11.13 概述了 AI 加速器中计算放置和资源分配之间的基本权衡。放置决策影响并行性、内存访问模式和通信开销，而分配策略决定资源如何随时间分布以平衡执行效率。这些因素之间的相互作用决定了整体性能，需要仔细平衡以避免如过度同步、内存拥塞或计算资源未充分利用等瓶颈。优化这些权衡对于确保 AI 加速器以峰值效率运行至关重要。

每个这些维度都需要在放置和分配之间平衡权衡。例如，将计算在多个处理单元之间空间分布可以增加吞吐量；然而，如果数据分配没有优化，内存带宽限制可能会引入瓶颈。同样，为细粒度计算分配资源可能增强灵活性，但没有适当的放置策略，可能会导致过度的同步开销。

表 11.13：**放置-分配权衡**：AI 加速器的性能取决于战略性地将计算映射到硬件并在时间上分配资源，平衡并行性、内存访问和执行效率以避免瓶颈。仔细考虑这些相互依赖的因素对于在机器学习系统中最大化吞吐量和最小化能耗至关重要。

| **维度** | **放置考虑因素** | **分配考虑因素** |
| --- | --- | --- |
| **计算粒度** | 细粒度放置能够实现更高的并行性，但会增加同步开销。 | 粗粒度分配减少同步开销，但可能限制灵活性。 |
| **空间映射与时间映射** | 空间放置增强并行执行，但可能导致资源竞争和内存拥塞。 | 时间分配平衡资源共享，但可能降低整体吞吐量。 |
| **内存和数据局部性** | 将数据放置在计算单元附近可以最小化延迟，但可能减少整体内存可用性。 | 在多个内存级别分配数据可以增加容量，但引入更高的访问成本。 |
| **通信和同步** | 将计算单元本地化可以减少通信延迟，但可能引入竞争。 | 分配同步机制可以缓解停滞，但可能引入额外的开销。 |
| **数据流和执行顺序** | 静态放置简化了执行但限制了适应工作负载变化的能力。 | 动态分配提高了适应性但增加了调度复杂性。 |

由于 AI 加速器架构对计算执行的位置和随时间分配的资源都施加了限制，因此选择有效的映射策略需要一种协调的放置和分配方法。理解这些权衡如何影响执行效率对于优化 AI 加速器上的性能至关重要。

#### 探索配置空间

AI 加速器的效率不仅取决于其计算能力，还取决于神经网络计算如何映射到硬件资源。映射定义了计算如何分配给处理元素，数据如何在内存层次结构中放置和移动，以及执行如何调度。在这个过程中做出的选择对性能有重大影响，影响着计算利用率、内存带宽效率和能耗。

将机器学习模型映射到硬件上呈现了一个庞大且复杂的设计空间。与传统的计算工作负载不同，模型执行涉及多个相互作用的因素，包括计算、数据移动、并行性和调度，每个因素都引入了约束和权衡。正如内存系统部分所讨论的，加速器的层次化内存结构通过限制带宽、延迟和数据重用进一步复杂化了这个过程。因此，有效的映射策略必须仔细平衡相互竞争的目标，以最大化效率。

在这个设计空间的中心有三个相互关联的方面：数据放置、计算调度和数据移动时机。数据放置指的是数据在各种内存层次结构中的分配，例如片上缓冲区、缓存和片外 DRAM，其有效管理至关重要，因为它影响延迟和能耗。不合理的放置往往导致频繁且昂贵的内存访问，而战略性的放置确保经常使用的数据保持在快速访问的存储中。计算调度控制操作的执行顺序，影响计算效率和内存访问模式；例如，某些执行顺序可能优化并行性同时引入同步开销，而其他执行顺序可能以吞吐量为代价提高数据局部性。同时，数据移动的时机同样重要，因为在不同内存级别之间传输数据会产生显著的延迟和能耗。因此，有效的映射策略侧重于通过重用数据和将通信与计算重叠来最小化不必要的传输，从而提高整体性能。

这些因素定义了一个庞大的组合设计空间，其中映射决策的微小变化可能导致性能和能效的大幅差异。一个糟糕的映射策略可能导致计算资源利用率低、数据移动过多或不平衡的工作负载，从而形成瓶颈，降低整体效率。相反，一个精心设计的映射最大化了吞吐量和资源利用率，充分利用了可用的硬件。

由于映射决策的相互关联性，没有单一的解决方案——不同的工作负载和硬件架构需要不同的方法。接下来的几节将探讨这个设计空间的结构以及不同的映射选择如何塑造机器学习工作负载的执行。

将机器学习计算映射到专用硬件需要平衡多个约束，包括计算效率、内存带宽和执行调度。挑战来自于将计算分配给处理元素、排序执行和管理数据移动的无数可能方式。每个决策都贡献于一个高维搜索空间，其中映射选择的微小变化都可能对性能产生重大影响。

与具有可预测执行模式的传统工作负载不同，机器学习模型引入了多种计算结构，这些结构需要灵活的映射以适应数据重用、并行化机会和内存限制。搜索空间呈组合式增长，使得穷举搜索变得不可行。为了理解这种复杂性，出现了三个来源的变体：

#### 计算和执行的排序

机器学习工作负载通常以嵌套循环的形式组织，遍历计算的各个维度。例如，一个矩阵乘法内核可能会遍历批量大小（<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>）、输入特征（<semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>）和输出特征（<semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>）。这些循环执行的顺序对数据局部性、重用模式和计算效率有深远的影响。

安排 <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics> 循环的方式遵循阶乘增长模式：<semantics><mrow><mi>𝒪</mi><mo>=</mo><mi>d</mi><mi>!</mi></mrow> <annotation encoding="application/x-tex">\mathcal{O} = d!</annotation></semantics>，这会迅速扩展。一个典型的卷积层可能涉及多达七个循环维度，导致：<semantics><mrow><mn>7</mn><mi>!</mi><mo>=</mo><mn>5</mn><mo>,</mo><mn>040</mn> <mrow><mtext mathvariant="normal">可能的执行顺序。</mtext></mrow></mrow> <annotation encoding="application/x-tex">7! = 5,040 \text{ possible execution orders.}</annotation></semantics>

当考虑多个内存级别时，搜索空间会按如下方式扩展：<semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>!</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>l</mi></msup> <annotation encoding="application/x-tex">(d!)^l</annotation></semantics> 其中 <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics> 是内存层次结构的级别数量。这种快速扩展突出了为什么执行顺序优化至关重要——不良的循环排序可能导致过度的内存流量，而优化的顺序可以提高缓存利用率 (Sze et al. 2017a)。

#### 处理元素间的并行化

现代人工智能加速器利用数千个处理元素以最大化并行性，但确定哪些计算应该并行化并非易事。过度的并行化可能会引入同步开销和增加的带宽需求，而不足的并行化会导致硬件利用率低下。

在并行单元之间分配计算的方法遵循二项式系数：<semantics><mrow><mi>𝒫</mi><mo>=</mo><mfrac><mrow><mi>d</mi><mi>!</mi></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mo>−</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\mathcal{P} = \frac{d!}{(d-k)!}</annotation></semantics> 其中 <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics> 是循环的数量，而 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics> 是用于并行执行的选择数量。对于一个选择三个循环进行并行执行的六循环计算，有效的配置数量为：<semantics><mrow><mfrac><mrow><mn>6</mn><mi>!</mi></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>6</mn><mo>−</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mrow><mo>=</mo><mn>120</mn><mi>.</mi></mrow> <annotation encoding="application/x-tex">\frac{6!}{(6-3)!} = 120.</annotation></semantics>

即使对于单层来说，也可能有数百种有效的并行化策略，每种策略都会影响数据同步、内存竞争和整体计算效率。将这种策略扩展到多层和模型架构中，将进一步放大复杂性。

#### 内存放置和数据移动

人工智能加速器的分层内存结构引入了额外的约束，因为数据必须在寄存器、缓存、共享内存和片外 DRAM 之间高效地放置。数据放置影响延迟、带宽消耗和能源效率——频繁访问慢速内存会形成瓶颈，而优化的放置可以减少昂贵的内存传输。

在内存级别之间分配数据的方法遵循指数增长函数：<semantics><mrow><mi>ℳ</mi><mo>=</mo><msup><mi>n</mi><mrow><mi>d</mi><mo>×</mo><mi>l</mi></mrow></msup></mrow> <annotation encoding="application/x-tex">\mathcal{M} = n^{d \times l}</annotation></semantics> 其中：

+   <semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics> = 每层的放置选择数量，

+   <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics> = 计算维度数量，

+   <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics> = 内存层次级别数量。

对于一个模型，每层有：

+   <semantics><mrow><mi>d</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">d = 5</annotation></semantics> 计算维度，

+   <semantics><mrow><mi>l</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">l = 3</annotation></semantics> 内存级别，

+   <semantics><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">n = 4</annotation></semantics> 种可能的放置选择，

可能的内存分配数量为：<semantics><mrow><msup><mn>4</mn><mrow><mn>5</mn><mo>×</mo><mn>3</mn></mrow></msup><mo>=</mo><msup><mn>4</mn><mn>15</mn></msup><mo>=</mo><mn>1</mn><mo>,</mo><mn>073</mn><mo>,</mo><mn>741</mn><mo>,</mo><mn>824</mn><mi>.</mi></mrow> <annotation encoding="application/x-tex">4^{5 \times 3} = 4^{15} = 1,073,741,824.</annotation></semantics>

这突显了即使是单层也可能有超过十亿种可能的内存配置，使得手动优化变得不切实际。

#### 映射搜索空间

通过结合计算顺序、并行化和内存放置的复杂性，总的映射搜索空间可以近似为：<semantics><mrow><mi>𝒮</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>n</mi><mi>d</mi></msup><mo>×</mo><mi>d</mi><mi>!</mi><mo>×</mo><mfrac><mrow><mi>d</mi><mi>!</mi></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mo>−</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>l</mi></msup></mrow> <annotation encoding="application/x-tex">\mathcal{S} = \left( n^d \times d! \times \frac{d!}{(d-k)!} \right)^l</annotation></semantics> 其中：

+   <semantics><msup><mi>n</mi><mi>d</mi></msup><annotation encoding="application/x-tex">n^d</annotation></semantics> 表示内存放置选择，

+   <semantics><mrow><mi>d</mi><mi>!</mi></mrow><annotation encoding="application/x-tex">d!</annotation></semantics> 考虑了计算顺序选择，

+   <semantics><mfrac><mrow><mi>d</mi><mi>!</mi></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mo>−</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{d!}{(d-k)!}</annotation></semantics> 揭示了并行化的可能性，

+   <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics> 是内存层次结构的层数。

这个方程说明了搜索空间的指数增长，使得除了最简单的情况外，穷举搜索变得不可行。

## 数据流优化策略

映射策略确定了计算在加速器架构中的执行位置和数据存储位置，但它们并没有指定数据在执行过程中如何通过处理单元流动。一个脉动阵列可能使用存储在本地内存中的权重来处理矩阵乘法，但权重、输入和输出在阵列中移动的顺序从根本上决定了内存带宽消耗和能源效率。这些数据流模式——称为优化策略——代表了将抽象映射决策转化为具体执行计划的临界实现维度。

在权重固定、输入固定和输出固定方法之间的选择直接影响到加速器是在计算受限区域还是内存受限区域运行。理解这些权衡是至关重要的，因为编译器（第 11.7 节）和运行时系统（第 11.8 节）必须根据第 11.4.2 节（第 11.4.2 节）中分析的计算特性和内存层次结构能力来选择适当的数据流模式。

将机器学习计算高效映射到硬件上是一个复杂的挑战，因为可能的配置数量庞大。随着模型复杂性的增加，潜在映射的数量呈指数增长。即使是单层，也有数千种计算循环的排序方式，数百种并行化策略，以及指数增长的内存放置选择。这种组合爆炸使得穷举搜索变得不切实际。

为了克服这一挑战，AI 加速器依赖于结构化的映射策略，这些策略系统地平衡计算效率、数据局部性和并行执行。这些方法不是评估每一个可能的配置，而是结合启发式、分析和基于机器学习的技巧，以高效地找到高性能的映射。

有效映射的关键在于理解和应用一系列核心技术，这些技术优化数据移动、内存访问和计算。这些映射策略的构建块为高效执行提供了一个结构化的基础，这在下一节中将会探讨。

### 映射策略的构建块

为了导航映射决策的复杂性，利用一系列基础技术，这些技术优化了数据移动、内存访问和计算效率的执行。这些技术为映射策略提供了必要的结构，以最大化硬件性能同时最小化瓶颈。

关键技术包括数据移动策略，这些策略确定在计算过程中数据在哪里被暂存，以减少冗余传输，例如在权重静止、输出静止和输入静止方法中。通过组织数据为行主序或通道主序等格式，内存感知张量布局也在通过影响内存访问模式和缓存效率方面发挥着重要作用。

其他策略包括内核融合，这是一种将多个操作组合成单个计算步骤的方法，以最小化冗余内存写入。分块技术被用作一种将大型计算分割成更小、内存友好的块的技术，以提高缓存效率并减少内存带宽需求。最后，平衡计算和通信对于在并行执行和内存访问之间管理权衡以实现高吞吐量至关重要。

这些构建块中的每一个都在构建高性能执行结构中发挥着关键作用，为启发式和模型驱动优化技术奠定了基础。下一节将探讨这些策略如何适应不同类型的 AI 模型。

#### 数据移动模式

虽然计算映射确定操作发生的位置和时间，但其成功在很大程度上取决于数据如何在内存层次结构中高效地访问和传输。与通常表现出结构化和可预测内存访问模式的传统计算工作负载不同，机器学习工作负载由于频繁检索权重、激活和中间值而表现出不规则的访问行为。

即使计算单元被高效地映射，不良的数据移动策略也可能严重降低性能，导致频繁的内存停滞和硬件资源利用率低下。如果数据不能以所需的速率供应给处理单元，计算单元将保持空闲状态，增加延迟、内存流量和能耗 (Y.-H. Chen 等人 2016)。

为了说明数据移动效率低下的影响，可以考虑典型的矩阵乘法操作，如列表 11.18 所示，这是许多机器学习模型的基础。

列表 11.18：**矩阵乘法**：数据移动瓶颈可能导致硬件资源利用率低下，说明了在优化机器学习模型性能中高效数据流的重要性。通过此操作

```py
## Matrix multiplication where:
## weights: [512 x 256] - model parameters
## input:   [256 x 32]  - batch of activations
## Z:       [512 x 32]  - output activations

## Computing each output element Z[i,j]:
for i in range(512):
    for j in range(32):
        for k in range(256):
            Z[i, j] += weights[i, k] * input[k, j]
```

这个计算揭示了几个关键的数据流挑战。第一个挑战是需要访问的内存次数。对于每个输出 <semantics><mrow><mi>Z</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Z[i, j]</annotation></semantics>，计算必须从权重矩阵中获取一整行的权重，并从输入矩阵中获取一整列的激活值。由于权重矩阵包含 512 行，输入矩阵包含 32 列，这导致了重复的内存访问，给内存带宽带来了重大负担。

第二个挑战来自权重重用。相同的权重应用于多个输入，这意味着理想的映射策略应该最大化权重的局部性，以避免冗余的内存读取。如果没有适当的重用，加速器将浪费带宽多次加载相同的权重(Tianqi et al. 2018)。

第三个挑战涉及中间结果的累积。由于<semantics><mrow><mi>Z</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Z[i,j]</annotation></semantics>中的每个元素都需要来自 256 个不同的权重-输入对的贡献，必须在计算最终值之前存储和检索部分和。如果这些中间值存储效率低下，系统将需要频繁的内存访问，进一步增加带宽需求。

缓解这些挑战的一种自然方式是利用 SIMD 和 SIMT 执行模型，这些模型允许并行获取多个值。然而，即使有这些优化，数据移动仍然是一个瓶颈。问题不仅在于数据检索的速度，还在于数据必须移动的频率以及它在内存层次结构中的位置(Han et al. 2016)。

由于数据移动比计算贵 100-1000 倍，加速器的最重要的目标就是最小化内存访问。数据流策略是设计用来通过最大化数据重用来实现这一目标的架构模式。问题是：哪种数据最值得保留在本地？这直接针对了第 11.4.1 节中考察的 AI 内存墙挑战，其中内存访问的极端能量惩罚主导了系统性能。

为了解决这些限制，加速器实现数据流策略，以确定哪些数据保持在内存中，哪些数据动态流过。这些策略代表了对于数据局部性基本问题的不同回答：权重静态保持模型参数局部化，输入静态保持激活数据，输出静态保持中间结果。每种方法都权衡不同的内存访问模式，以最大化数据重用并最小化构成 AI 加速主要瓶颈的能量密集型传输。

##### 权重静态

权重静态策略将权重固定在本地内存中，同时输入激活和部分和通过系统流过。在 CNN 和矩阵乘法中，权重静态方法特别有益，因为相同的权重集应用于多个输入。通过确保权重保持静态，这种方法减少了冗余的内存访问，有助于缓解带宽瓶颈并提高能源效率。

权重静态方法的一个关键优势是它最大化了权重重用，减少了对外部存储的内存访问频率。由于权重参数通常在多个计算中共享，将它们保持在本地内存中消除了不必要的数据移动，降低了计算的整体能源成本。这使得它在权重代表主导内存开销的架构中特别有效，例如用于机器学习的阵列和定制加速器。

简化的矩阵乘法权重静态实现示于列表 11.19。

列表 11.19：**权重静态矩阵乘法**：权重静态矩阵乘法将权重固定在本地内存中，同时输入激活流过，展示了如何最大化权重重用以降低能耗。

```py
## Weight Stationary Matrix Multiplication
## - Weights remain fixed in local memory
## - Input activations stream through
## - Partial sums accumulate for final output

for weight_block in weights:  # Load and keep weights stationary
    load_to_local(weight_block)  # Fixed in local storage
    for input_block in inputs:  # Stream inputs dynamically
        for output_block in outputs:  # Compute results
            output_block += compute(weight_block, input_block)
            # Reuse weights across inputs
```

在权重静态执行中，权重一次性加载到本地内存中，并在整个计算过程中保持固定，而输入动态流过，从而减少了冗余的内存访问。同时，部分和在一种有效的方式中累积，以最小化不必要的数据移动，确保系统保持高吞吐量和能源效率。

通过在本地存储中保持权重固定，显著降低了内存带宽需求，因为对于每次新的计算，不需要重新加载权重。相反，系统高效地重复使用存储的权重，跨越多个输入激活，从而实现高吞吐量执行。这使得权重静态数据流对于具有大量权重重用模式的负载，如 CNN 和矩阵乘法，非常有效。

然而，尽管这种策略减少了与权重相关的内存流量，但它也在输入和输出移动方面引入了权衡。由于输入必须在权重保持固定的情况下动态流过，因此这种方法的有效性取决于输入激活能否有效地传递到计算单元而不会造成停滞。此外，代表中间结果的局部和必须谨慎累积以避免过度的内存流量。总性能提升取决于可用的片上内存大小，因为对于具有数百万或数十亿参数的模型，在本地存储较大的权重矩阵可能会成为限制因素。

权重站式策略非常适合于权重具有高重用率和内存带宽是限制因素的工作负载。它通常用于卷积神经网络（CNN）、收缩阵列和矩阵乘法内核，其中结构化权重重用导致性能显著提升。然而，对于输入或输出重用更为关键的模式，可能需要采用其他数据流策略，如输出站式或输入站式，以提供更好的权衡。

##### 输出站式

输出站式策略将部分和在局部内存中保持固定，而权重和输入激活则通过系统流动。这种方法对于全连接层、收缩阵列和其他输出元素累积来自多个权重-输入对贡献的操作特别有效。通过保持部分和固定，这种方法减少了冗余的内存写入，最小化了带宽消耗并提高了能源效率 (Y.-H. Chen 等人 2016)。

输出站式方法的一个关键优势是它优化了累积效率，确保在写入内存之前，每个输出元素都以尽可能高效的方式计算。与优先考虑权重重用的权重站式不同，输出站式执行旨在最小化由频繁写入中间结果引起的内存带宽开销。这使得它非常适合于累积占主导地位的计算模式的工作负载，如全连接层和基于转换器的模型中的矩阵乘法。

列表 11.20 展示了矩阵乘法的简化输出站式实现。

列表 11.20：**输出站式执行**：在矩阵乘法过程中，局部累积部分和以减少内存写入并提高效率，这使得它非常适合基于转换器的模型。

```py
## - Partial sums remain in local memory
## - Weights and input activations stream through dynamically
## - Final outputs are written only once

for output_block in outputs:  # Keep partial sums stationary
    accumulator = 0  # Initialize accumulation buffer
    for weight_block, input_block in zip(weights, inputs):
        accumulator += compute(weight_block, input_block)
        # Accumulate partial sums
    store_output(accumulator)  # Single write to memory
```

此实现遵循输出站式执行的核心原则：

+   在整个计算过程中，部分和保持在局部内存中。

+   权重和输入动态流过，确保中间结果保持局部可访问。

+   最终输出只写入内存一次，减少了不必要的内存流量。

通过在本地累积部分和，这种方法消除了过多的内存写入，提高了整体系统效率。在如收缩阵列等架构中，计算通过一个处理元素网格进行，保持部分和固定自然地与结构化累积工作流程相一致，减少了同步开销。

然而，尽管输出固定减少了内存写入流量，但它引入了权重和输入移动的权衡。由于权重和激活必须动态流过，这种方法的有效性取决于数据如何被有效地输入系统而不会造成停滞。此外，并行实现必须仔细同步部分和的更新，特别是在多个处理元素贡献相同输出的架构中。

输出固定策略对于以累积为主要操作且最小化中间内存写入至关重要的工作负载最为有效。它通常用于全连接层、注意力机制和收缩阵列，其中结构化累积导致显著的性能提升。然而，对于输入重用更为关键的模型，如输入固定等替代数据流策略，可能提供更好的权衡。

##### 输入固定

输入固定策略将输入激活固定在本地内存中，而权重和部分和则通过系统流过。这种方法特别适用于批量处理、转换器模型和基于序列的架构，在这些架构中，输入激活在多个计算中被重用。通过确保激活保持在本地内存中，这种方法减少了冗余的输入检索，提高了数据局部性并最小化了内存流量。

输入固定方法的一个关键优势是它最大化了输入重用，减少了激活的内存访问频率。由于许多模型，尤其是自然语言处理和推荐系统中的模型，在多个计算中处理相同的输入数据，保持输入固定消除了不必要的内存传输，从而降低了能耗。这种策略在处理大型批量数据时特别有用，其中单个输入激活批次对多个权重转换做出贡献。

列表 11.21 展示了一个简化的矩阵乘法输入固定实现。

列表 11.21：**输入固定**：这种方法在动态流过权重的同时保持输入激活固定，以最大化内存重用并减少能耗。

```py
## - Input activations remain in local memory
## - Weights stream through dynamically
## - Partial sums accumulate and are written out

for input_block in inputs:  # Keep input activations stationary
    load_to_local(input_block)  # Fixed in local storage
    for weight_block in weights:  # Stream weights dynamically
        for output_block in outputs:  # Compute results
            output_block += compute(weight_block, input_block)
            # Reuse inputs across weights
```

此实现遵循输入固定执行的核心原则：

+   输入激活被加载到本地内存中，并在计算过程中保持固定。

+   **权重被动态流过**，确保了在多个输入上的高效应用。

+   **部分和被累积并写入**，优化了内存带宽使用。

通过保持输入激活固定，这种策略最小化了输入数据的冗余内存访问，显著降低了外部内存带宽需求。这在变压器架构中尤其有益，其中输入序列中的每个标记在多个注意力头和层中使用。此外，在批量处理场景中，保持输入激活在本地内存中可以提高数据局部性，使其非常适合全连接层和矩阵乘法。

然而，尽管输入固定可以减少激活的内存流量，但它引入了权重和输出移动的权衡。由于权重必须在输入保持固定的情况下动态流式传输，因此这种方法的有效性取决于权重能否有效地传递到计算单元而不会造成停滞。此外，在将部分和写回内存之前，必须有效地累积，这可能需要额外的缓冲机制。

输入固定策略对于输入激活具有高重用率的工作负载最为有效，并且输入的内存带宽是一个关键约束。它通常用于变压器、循环网络和批量处理工作负载，其中结构化输入重用导致显著的性能提升。然而，对于输出累积更为重要的模型，可能需要采用其他数据流策略，如输出固定，以提供更好的权衡。

#### 内存高效的张量布局

机器学习工作负载的有效执行不仅取决于数据如何移动（数据流策略），还取决于数据如何在内存中存储和访问。张量布局，指的是多维数据在内存中的排列，可以显著影响内存访问效率、缓存性能和计算吞吐量。选择不当的布局可能导致过多的内存停滞、不高效的缓存使用和增加的数据移动成本。

在人工智能加速器中，张量布局优化尤为重要，因为数据通常按照底层硬件架构指定的模式频繁访问。选择正确的布局确保内存访问与硬件友好的访问模式对齐，从而最小化昂贵的内存事务开销 (C. NVIDIA 2025)。

虽然开发者有时可以手动指定张量布局，但选择通常由机器学习框架（例如，TensorFlow、PyTorch、JAX）、编译器或 AI 加速器运行时自动确定。低级优化工具，如用于 NVIDIA GPU 的 cuDNN、用于 TPU 的 XLA 和用于自定义加速器的 MLIR，可能会动态地重新排列张量布局以优化性能(X. He 2023a)。在高层框架中，布局转换通常透明地应用，但与自定义内核或低级库（例如，CUDA、Metal 或 OpenCL）一起工作的开发者可能对张量格式选择有直接控制权。

例如，在 PyTorch 中，用户可以使用 tensor.permute()或 tensor.contiguous()手动修改布局，以确保高效的内存访问(Paszke 等人 2019)。在 TensorFlow 中，布局优化通常由 XLA 编译器内部应用，根据目标硬件在 NHWC（行主序）和 NCHW（通道主序）之间进行选择(Brain 2022)。硬件感知的机器学习库，如用于 GPU 的 cuDNN 或用于 CPU 的 OneDNN，强制执行特定的内存布局以最大化缓存局部性和 SIMD 效率。最终，尽管开发者可能对张量布局选择有一些控制权，但大多数布局决策是由编译器和运行时系统驱动的，确保张量以最适合底层硬件的方式存储在内存中。

##### 行主序布局

行主序布局指的是多维张量在内存中的存储方式，其中元素按行排列，确保给定行的所有值在移动到下一行之前连续放置。这种存储格式在通用 CPU 和一些机器学习框架中广泛使用，因为它与顺序内存访问模式自然对齐，使得某些类型的操作更加缓存高效(Intel 2021)。

要理解行主序布局是如何工作的，可以考虑一个表示为形状为（高度，宽度，通道）的张量的单个 RGB 图像。如果图像的大小为<semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics>像素，并且有 3 个通道（RGB），则相应的张量结构为（3，3，3）。值在内存中的存储方式如下：<semantics><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*} I(0,0,0), I(0,0,1), I(0,0,2), I(0,1,0), I(0,1,1), \\ I(0,1,2), I(0,2,0), I(0,2,1), I(0,2,2), \ldots \end{gather*}</annotation></semantics>

每一行都是连续存储的，这意味着第一行中的所有像素值在内存中按顺序放置，然后再移动到第二行。这种排序是有利的，因为 CPU 和缓存层次结构是针对顺序内存访问进行优化的。当以行方式访问数据时，例如在应用逐元素操作（如激活函数或基本算术变换）时，内存读取是高效的，并且缓存利用率最大化 (Sodani 2015)。

行主存储的效率在基于 CPU 的机器学习工作负载中表现得尤为明显，在这些工作负载中，批归一化、矩阵乘法和逐元素算术等操作通常按顺序处理数据行。由于现代 CPU 采用缓存预取机制，行主布局允许在执行之前将下一个所需的数据值预加载到缓存中，从而降低内存延迟并提高整体计算吞吐量。

然而，当执行需要跨通道而不是跨行访问数据的操作时，行主布局可能会引入低效。考虑一个卷积层，它在输入图像的多个通道上应用过滤器。由于通道值在行主存储中是交错存储的，卷积操作必须跳转多个内存位置来获取给定像素的所有必要的通道值。这些跨步内存访问在依赖于矢量化执行和内存归约的硬件架构上可能代价高昂，例如 GPU 和 TPU。

尽管存在这些限制，行主布局仍然是基于 CPU 的机器学习框架中的主导存储格式。例如，TensorFlow 在 CPU 上默认使用 NHWC（行主）格式，确保了缓存局部性优化以适应顺序处理。然而，当针对 GPU 时，框架通常会动态重新排列数据，以利用更有效的内存布局，如通道主存储，这更好地与并行计算相匹配。

##### 通道主布局

与行主布局相反，通道主布局在内存中排列数据，使得给定通道的所有值都存储在一起，然后再移动到下一个通道。这种格式对于 GPU、TPU 和其他 AI 加速器特别有益，在这些设备上，矢量化操作和内存归约对计算效率有显著影响。

要理解通道主布局是如何工作的，可以考虑一个大小为 (高度, 宽度, 通道) = (3, 3, 3) 的相同的 RGB 图像张量。不同于按行存储像素值，数据在内存中按通道优先的结构组织如下：<semantics><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi><mo>,</mo></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*} I(0,0,0), I(1,0,0), I(2,0,0), I(0,1,0), I(1,1,0), I(2,1,0), \ldots, \\ I(0,0,1), I(1,0,1), I(2,0,1), \ldots, I(0,0,2), I(1,0,2), I(2,0,2), \ldots \end{gather*}</annotation></semantics>

在这种格式中，整个图像的所有红色通道值首先存储，然后是所有绿色值，最后是所有蓝色值。这种排序允许硬件加速器有效地并行加载和处理通道中的数据，这对于卷积操作和 SIMD（单指令，多数据）执行模型至关重要(Chetlur 等人 2014)。

当在机器学习模型中执行卷积时，通道主序布局的优势变得明显。卷积层通过在所有通道上应用共享的过滤器集来处理图像。当数据以通道主序格式存储时，卷积核可以有效地加载整个通道，减少分散的内存读取次数。这减少了内存延迟，提高了吞吐量，并增强了矩阵乘法的数据局部性，这对于机器学习工作负载至关重要。

由于 GPU 和 TPU 依赖于内存归约 27，这是一种连续线程获取连续内存地址的技术，通道主序布局自然地与这些处理器执行并行计算的方式相吻合。例如，在 NVIDIA GPU 中，每个 warp（同时执行的线程组）中的每个线程处理同一通道的不同元素，确保内存访问高效，并减少步进内存访问的可能性，这可能会降低性能。

尽管在机器学习加速器中具有优势，但通道主序布局在通用 CPU 上运行时可能会引入低效。由于 CPU 优化的是顺序内存访问，因此在移动到下一个通道之前存储单个通道的所有值会破坏按行操作的缓存局部性。这就是为什么许多机器学习框架（例如 TensorFlow、PyTorch）在 CPU 上默认使用行主序（NHWC），在 GPU 上使用通道主序（NCHW）——优化每种硬件类型的优势。

现代人工智能框架和编译器通常会根据执行环境动态地转换张量布局。例如，TensorFlow 和 PyTorch 会根据模型是在 CPU、GPU 还是 TPU 上运行自动在 NHWC28 和 NCHW 之间切换，确保内存布局与最有效的执行路径相匹配。

##### 比较行主序和通道主序布局

行主序（NHWC）和通道主序（NCHW）布局在机器学习工作负载中具有不同的用途，其效率很大程度上取决于硬件架构、内存访问模式和计算需求。布局的选择直接影响缓存利用率、内存带宽效率和处理吞吐量。表 11.14 总结了行主序（NHWC）和通道主序（NCHW）布局在性能权衡和硬件兼容性方面的差异。

表 11.14：**数据布局策略**：行主序（NHWC）和通道主序（NCHW）布局优化了不同硬件架构的内存访问模式；NHWC 适用于 CPU 和逐元素操作，而 NCHW 加速了基于 GPU 和 TPU 的卷积操作。选择合适的布局通过最大化缓存利用率和内存带宽效率，对性能产生重大影响。

| **特性** | **行主序（NHWC）** | **通道主序（NCHW）** |
| --- | --- | --- |
| **内存存储顺序** | 像素按行存储，通道交错 | 给定通道的所有值首先存储在一起 |
| **最佳适用** | CPU，逐元素操作 | GPU，TPU，卷积操作 |
| **缓存效率** | 高缓存局部性，适用于顺序行访问 | 优化跨通道的内存合并 |
| **卷积性能** | 需要步进内存访问（在 GPU 上效率低下） | 适用于 GPU 卷积内核 |
| **内存获取** | 适用于按顺序处理行的操作 | 优化跨通道的 SIMD 执行 |
| **框架中的默认值** | CPU 上的默认值（例如，TensorFlow NHWC） | GPU 上的默认值（例如，cuDNN 偏好 NCHW） |

使用行主序（NHWC）或通道主序（NCHW）布局的决定并不总是由开发者手动做出。相反，机器学习框架和 AI 编译器通常根据目标硬件和操作类型动态确定最佳布局。由于 CPU 倾向于缓存友好的顺序内存访问，因此它们更偏好 NHWC，而 GPU 在 NCHW 上表现更好，这减少了机器学习计算中的内存获取开销。

在实践中，现代 AI 编译器，如 TensorFlow 的 XLA 和 PyTorch 的 TorchScript，执行自动布局转换，根据需要将张量在 NHWC 和 NCHW 之间转换，以优化不同处理单元的性能。这确保了机器学习模型在不要求开发者手动指定张量布局的情况下，达到最高的吞吐量。

#### 内核融合

AI 加速中最有影响力的优化技术之一是减少操作之间中间数据移动的开销。本节探讨了如何通过内核融合将多个独立的计算转换为统一操作，从而显著提高内存效率和执行性能。我们首先分析由中间写入造成的内存瓶颈，然后探讨融合技术如何消除这些低效性。

##### 中间内存写入

优化内存访问是 AI 加速中的一个基本挑战。虽然 AI 模型依赖于高吞吐量计算，但它们的性能通常受限于内存带宽和中间内存写入，而不是纯算术操作。每当一个操作产生必须写入内存并在稍后读回的中间结果时，由于数据移动开销，执行就会停滞。

基于 第十章 中的软件优化技术以及 第 11.4.1 节 中建立的内存带宽约束，内核融合代表了软件优化和硬件加速之间的关键桥梁。许多 AI 工作负载引入了不必要的中间内存写入，导致内存带宽消耗增加和执行效率降低 (Ye 等人 2025)。

列表 11.22 展示了一个简单的执行模型，其中每个操作都被视为一个独立的内核，这意味着每个中间结果都会写入内存，然后在下一个操作中读取回来。

列表 11.22：**简单执行**：每个步骤在处理下一个操作之前将中间结果写入内存，导致带宽使用增加和效率降低。*来源：NVIDIA GPU 技术大会 2017*[nvidia2017gpu]

```py
import torch

## Input tensor
X = torch.randn(1024, 1024).cuda()

## Step-by-step execution (naïve approach)
X1 = torch.relu(X)  # Intermediate tensor stored
# in memory
X2 = torch.batch_norm(X1)  # Another intermediate tensor stored
Y = 2.0 * X2 + 1.0  # Final result
```

每个操作都会生成一个中间张量，必须将其写入内存并在下一个操作中检索。在大型张量上，这种数据移动的开销可能会超过操作的计算成本 (Shazeer 等人 2018)。表 11.15 展示了简单执行模型中的内存开销。虽然只需要最终的输出 <semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics>，但存储多个中间张量会创建不必要的内存流量和低效的内存使用。这种数据移动瓶颈会显著影响性能，使得内存优化对于 AI 加速器至关重要。

表 11.15：**中间张量存储**：简单的执行模型需要大量内存来存储每个操作生成的中间张量；对于 1024x1024 的张量，此表显示即使只需要最终的输出，存储这些中间结果也会将总内存占用从 4 MB 增加到 16 MB。最小化这种中间数据存储对于提高内存效率和加速 AI 计算至关重要。

| **张量** | **1024 <semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics> 1024 张量的大小 (MB)** |
| --- | --- |
| **X** | 4 MB |
| **X’** | 4 MB |
| **X’’** | 4 MB |
| **Y** | 4 MB |
| **总内存** | 16 MB |

尽管只需要最终的输出 <semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics>，但三个额外的中间张量消耗了额外的内存，而没有对最终的输出存储做出贡献。这种过度的内存使用限制了可扩展性，浪费了内存带宽，尤其是在最小化数据移动至关重要的 AI 加速器中。

##### 内存效率内核融合

内核融合是一种关键的优化技术，旨在最小化中间内存写入，减少机器学习工作负载的内存占用和带宽消耗 (Zhihao Jia, Zaharia, and Aiken 2018)。

内核融合涉及将多个计算步骤合并成一个单一、优化的操作，消除了存储和重新加载中间张量的需求。不是单独执行每一层或逐元素操作，其中每个步骤在下一个步骤开始之前将输出写入内存，融合允许操作之间直接数据传递，保持计算在高速寄存器或局部内存中。

一个常见的机器学习序列可能包括应用非线性激活函数（例如，ReLU），然后是批量归一化，最后是对下一层输入的值进行缩放。在原始实现中，这些步骤中的每一个都会生成一个中间张量，并将其写入内存，然后读取回来，并再次修改：<semantics><mrow><mi>X</mi><mi>′</mi><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>X</mi><mi>″</mi><mo>=</mo><mtext mathvariant="normal">BatchNorm</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>Y</mi><mo>=</mo><mi>α</mi><mo>⋅</mi><mi>X</mi><mi>″</mi><mo>+</mo><mi>β</mi></mrow> <annotation encoding="application/x-tex">X' = \text{ReLU}(X) X'' = \text{BatchNorm}(X') Y = \alpha \cdot X'' + \beta</annotation></semantics>

通过内核融合，这些操作被合并成一个单一的计算步骤，使得整个转换过程无需生成不必要的中间张量：<semantics><mrow><mi>Y</mi><mo>=</mo><mi>α</mi><mo>⋅</mo><mtext mathvariant="normal">BatchNorm</mtext><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>+</mo><mi>β</mi></mrow> <annotation encoding="application/x-tex">Y = \alpha \cdot \text{BatchNorm}\big(\text{ReLU}(X)\big) + \beta</annotation></semantics>

表 11.16 突出了操作融合对内存效率的影响。通过将中间结果保存在寄存器或局部内存中，而不是写入主内存，融合显著减少了内存流量。这种优化在高度并行的架构如 GPU 和 TPU 上特别有益，因为最小化内存访问直接转化为提高了执行吞吐量。与原始执行模型相比，融合执行消除了存储中间张量的需求，大幅降低了总内存占用并提高了整体效率。

表 11.16：**操作融合优势**：融合执行通过消除存储中间张量的需要，直接提高了在内存受限硬件（如 GPU 和 TPU）上的效率。此表量化了内存节省，显示从原始执行的 16 MB 减少到融合操作的 4 MB。

| **执行模型** | **存储的中间张量** | **总内存使用（MB）** |
| --- | --- | --- |
| **原始执行** | X’，X’’ | 16 MB |
| **融合执行** | 无 | 4 MB |

内核融合将总内存消耗从 16 MB 减少到 4 MB，消除了冗余的内存写入，同时提高了执行效率。

##### 性能优势和限制

内核融合带来了几个关键优势，这些优势增强了内存效率和计算吞吐量。通过减少内存访问，融合内核确保中间值保持在寄存器中，而不是反复写入和读取内存。这显著降低了内存流量，这是机器学习工作负载中的主要瓶颈之一。特别是，GPU 和 TPU 从内核融合中受益，因为高带宽内存是一种稀缺资源，减少内存事务可以提高计算单元的利用率 (X. Qi, Kantarci, and Liu 2017)。

然而，并非所有操作都可以融合。元素级操作，如 ReLU、批量归一化和简单的算术变换，是融合的理想候选者，因为它们的计算只依赖于输入张量的单个元素。相比之下，具有复杂数据依赖性的操作，如矩阵乘法和卷积，涉及全局数据移动，使得直接融合不切实际。这些操作需要从多个输入元素中获取值来计算单个输出，这阻止了它们作为一个单一的融合内核执行。

另一个重要的考虑因素是寄存器压力。融合多个操作意味着所有临时值都必须保存在寄存器中，而不是内存中。虽然这消除了冗余的内存写入，但也增加了寄存器需求。如果一个融合内核超过了每个线程可用的寄存器数量，系统必须将多余的值溢出到共享内存中，这引入了额外的延迟，并可能抵消融合的好处。在 GPU 上，由于线程占用（可以并行运行的线程数）受可用寄存器的限制，过度的融合会降低并行性，导致收益递减。

不同的 AI 加速器和编译器以不同的方式处理融合。例如，NVIDIA GPU 倾向于 warp 级别的并行处理，其中元素级融合简单直接。另一方面，TPU 优先考虑脉动阵列执行，这针对矩阵-矩阵操作进行了优化，而不是元素级融合 (X. Qi, Kantarci, and Liu 2017)。AI 编译器如 XLA (TensorFlow)、TorchScript (PyTorch)、TensorRT (NVIDIA) 和 MLIR 自动检测融合机会，并应用启发式方法以平衡内存节省和执行效率 (X. He 2023b)。

尽管融合具有优势，但并不总是有益。一些 AI 框架允许开发者有选择地禁用融合，尤其是在调试性能问题或频繁修改模型时。决定是否融合操作必须考虑内存效率、寄存器使用和硬件执行约束之间的权衡，以确保融合能带来实际性能提升。

#### 内存高效的分块策略

尽管现代人工智能加速器提供了高计算吞吐量，但它们的性能通常受限于内存带宽而不是原始处理能力。如果数据不能足够快地供应给处理单元，就会发生执行停滞，导致周期浪费和硬件利用率低效。

分块是一种通过将计算重新结构化为更小、内存友好的子问题来减轻这一问题的技术。不是一次性处理整个矩阵或张量，这会导致过度的内存流量，分块将计算分割成更小的块（分块），这些块适合于快速局部内存（例如，缓存、共享内存或寄存器）(Lam, Rothberg, and Wolf 1991)。通过这样做，分块增加了数据重用，最小化了内存检索，并提高了整体计算效率。

不高效的内存访问的经典例子是矩阵乘法，这在 AI 模型中得到了广泛应用。没有分块，原始方法会导致对相同数据的重复内存访问，导致不必要的带宽消耗 (Listing 11.23)。

列表 11.23：没有分块的原生矩阵乘法

```py
for i in range(N):
    for j in range(N):
        for k in range(N):
            C[i, j] += A[i, k] * B[k, j]  # Repeatedly fetching
            # A[i, k] and B[k, j]
```

每次迭代都需要从内存中多次加载矩阵 <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics> 和 <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics> 的元素，造成数据移动过多。随着矩阵大小的增加，内存瓶颈加剧，限制了性能。

分块通过确保将矩阵的小部分加载到快速内存中，高效重用，并在必要时才写回主内存来解决此问题。这种技术在人工智能加速器中尤为重要，因为内存访问主导了执行时间。通过将大矩阵分解成小块，如图 11.8 所示，可以在硬件上通过最大化快速内存中的数据重用来更有效地执行计算。在接下来的章节中，将阐述分块的基本原理、其不同的策略以及选择有效分块方法所涉及的关键权衡。

![图片](img/file187.svg)

图 11.8：**矩阵分块**：将大矩阵划分为小块优化了数据重用，并在计算过程中减少了内存访问开销。这项技术通过允许在快速内存中高效加载和处理数据，最小化从较慢的主内存中的传输，从而提高了人工智能加速器的性能。

##### 分块基础

分块（Tiling）基于一个简单但强大的原则：不是一次性对整个数据结构进行操作，而是将计算划分为适合可用快速内存的小块。通过围绕这些小块来结构化执行，最大化数据重用，减少冗余内存访问，从而提高整体效率。

考虑矩阵乘法，这是机器学习工作负载中的关键操作。该操作通过两个输入矩阵 <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics> 和 <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics> 计算输出矩阵 <semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>：<semantics><mrow><mi>C</mi><mo>=</mo><mi>A</mi><mo>×</mo><mi>B</mi></mrow> <annotation encoding="application/x-tex">C = A \times B</annotation></semantics> 其中每个元素 <semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">C[i,j]</annotation></semantics> 的计算如下：<semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>k</mi></munder><mi>A</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>k</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>×</mo><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>k</mi><mo>,</mo><mi>j</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow> <annotation encoding="application/x-tex">C[i,j] = \sum_{k} A[i,k] \times B[k,j]</annotation></semantics>

一种简单的实现直接遵循这个公式（列表 11.24）。

列表 11.24：**朴素矩阵乘法**：此代码直接使用嵌套循环实现矩阵乘法，展示了输出矩阵中的每个元素是如何作为输入矩阵中对应元素乘积之和来计算的。

```py
for i in range(N):
    for j in range(N):
        for k in range(N):
            C[i, j] += A[i, k] * B[k, j]  # Repeatedly fetching
            # A[i, k] and B[k, j]
```

初看之下，这种方法似乎是正确的——它计算了所需的结果，并遵循了数学定义。然而，问题在于内存的访问方式。每当最内层循环运行时，它都会从矩阵 <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics> 和矩阵 <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics> 中从内存中获取一个元素，执行乘法，并在矩阵 <semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics> 中更新一个元素。由于矩阵很大，处理器频繁地从内存中重新加载相同的值，即使这些值在之前的计算中已经被使用过。

这种不必要的数据移动代价高昂。从主内存（DRAM）中获取值比访问片上缓存或寄存器中存储的值慢数百倍。如果必须多次重新加载相同的值而不是将其存储在快速内存中，执行速度会显著减慢。

##### 分块的性能优势

与每次计算一个元素并不断在慢速内存中移动数据不同，分块处理子矩阵（分块）一次，将常用值保持在快速内存中。想法是将矩阵划分为适合处理器缓存或共享内存的小块，确保一旦一个块被加载，它就会被多次重用，然后再移动到下一个块。

列表 11.25 展示了矩阵乘法的分块版本，通过处理数据块来提高内存局部性。

列表 11.25：**分块矩阵乘法**：这种方法将矩阵划分为更小的块，通过在处理器缓存中重用数据来优化内存使用，从而提高计算效率。

```py
TILE_SIZE = 32  # Choose a tile size based on
# hardware constraints

for i in range(0, N, TILE_SIZE):
    for j in range(0, N, TILE_SIZE):
        for k in range(0, N, TILE_SIZE):
            # Compute the submatrix
            # C[i:i+TILE_SIZE, j:j+TILE_SIZE]
            for ii in range(i, i + TILE_SIZE):
                for jj in range(j, j + TILE_SIZE):
                    for kk in range(k, k + TILE_SIZE):
                        C[ii, jj] += A[ii, kk] * B[kk, jj]
```

这种重构显著提高了性能，主要原因有三个：

1.  **更好的内存重用**：而不是反复从 <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics> 和 <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics> 中从慢速内存中获取元素，这种方法将一小块数据加载到快速内存中，使用它进行多次计算，然后才移动到下一个分块。这最小化了冗余的内存访问。

1.  **减少内存带宽使用**：由于每个分块在使用后被多次使用，内存流量减少。与反复访问 DRAM 相比，大多数所需数据都可在 L1/L2 缓存或共享内存中找到，从而加快执行速度。

1.  **提高计算效率**：处理器花费更少的时间等待数据，更多的时间进行有用的计算。在像 GPU 和 TPU 这样的架构中，数千个并行处理单元同时运行，划分确保数据以结构化的方式读取和处理，避免不必要的停滞。

这种技术在 AI 加速器中特别有效，其中机器学习工作负载包括大规模矩阵乘法和张量变换。没有划分，这些工作负载很快就会成为内存限制的，这意味着性能受限于数据检索的速度，而不是处理器的原始计算能力。

##### 划分方法

虽然划分的一般原则保持不变，即通过将大型计算划分为更小的子问题来提高内存重用，但根据计算结构和硬件约束，有不同方式应用划分。两种主要的划分策略是空间划分和时间划分。这些策略优化计算和内存访问的不同方面，在实践中，它们通常结合使用以达到最佳性能。

###### 空间划分

空间划分关注将数据结构划分为更小的块，这些块适合处理器的高速内存。这种方法确保在移动到下一个块之前，每个块都得到完全处理，从而减少了冗余的内存访问。空间划分在矩阵乘法、卷积和变换器模型中的注意力机制等操作中得到了广泛应用。

空间划分在列表 11.26 中得到了说明，其中计算过程是按照输入矩阵的块进行的。

列表 11.26：**空间划分**：通过顺序处理矩阵块来减少冗余内存访问。

```py
TILE_SIZE = 32  # Tile size chosen based on available
# fast memory

for i in range(0, N, TILE_SIZE):
    for j in range(0, N, TILE_SIZE):
        for k in range(0, N, TILE_SIZE):
            # Process a submatrix (tile) at a time
            for ii in range(i, i + TILE_SIZE):
                for jj in range(j, j + TILE_SIZE):
                    for kk in range(k, k + TILE_SIZE):
                        C[ii, jj] += A[ii, kk] * B[kk, jj]
```

在这个实现中，在处理之前，<semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>和<semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>的每个块都加载到缓存或共享内存中，确保相同的数据不需要从较慢的内存中反复获取。在移动到下一个块之前，块被完全使用，从而最小化了冗余的内存访问。由于数据以结构化和局部化的方式访问，缓存效率显著提高。

当处理不适合完全放入快速内存中的大型张量时，空间划分特别有益。通过将它们划分为更小的块，计算保持局部化，避免了在内存级别之间进行过多的数据移动。这项技术在涉及大规模张量操作且需要精心管理内存以实现高性能的机器学习工作负载的 AI 加速器中得到了广泛应用。

###### 时间划分

当空间分割优化数据分区时，时间分割专注于重新组织计算本身，以改善随时间的数据重用。许多机器学习工作负载涉及在多个迭代中重复访问相同数据的操作。没有时间分割，这通常会导致冗余的内存访问，导致效率低下。时间分割，也称为循环阻塞，重构计算以确保频繁使用的数据尽可能长时间地保留在快速内存中，然后再进行下一个计算。

时间分割有益的一个经典例子是卷积操作，其中相同的权重集应用于多个输入区域。没有循环阻塞，这些权重可能需要在每次计算时从内存中多次加载。使用时间分割，计算被重新排序，使得权重在多个输入之间保持快速内存中，减少了不必要的内存访问，并提高了整体效率。

列表 11.27 展示了矩阵乘法中循环阻塞的简化示例。

列表 11.27：**时间分割**：通过在多个矩阵乘法中缓存权重到快速内存中，减少了冗余内存访问。

```py
for i in range(0, N, TILE_SIZE):
    for j in range(0, N, TILE_SIZE):
        for k in range(0, N, TILE_SIZE):
            # Load tile into fast memory before computation
            A_tile = A[i:i+TILE_SIZE, k:k+TILE_SIZE]
            B_tile = B[k:k+TILE_SIZE, j:j+TILE_SIZE]

            for ii in range(TILE_SIZE):
                for jj in range(TILE_SIZE):
                    for kk in range(TILE_SIZE):
                        C[i+ii, j+jj] += A_tile[ii, kk] *
                                         B_tile[kk, jj]
```

时间分割技术通过确保在数据被移除之前多次使用加载到快速内存中的数据，从而提高了性能。在本实现中，在执行计算之前，矩阵的小块<semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>和<semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>被明确加载到临时存储中，从而减少了内存访问开销。这种重构允许计算在移动到下一个块之前处理整个块，从而减少了从较慢内存加载数据的次数。

这种技术在某些值被重复使用的负载中特别有用，例如卷积、循环神经网络（RNN）和变换器中的自注意力机制。通过应用循环阻塞，AI 加速器可以显著减少内存停滞并提高执行吞吐量。

##### 分割的挑战和权衡

虽然分割通过优化内存重用和减少冗余内存访问显著提高了性能，但它引入了几个挑战和权衡。选择合适的块大小是一个关键决策，因为它直接影响计算效率和内存带宽使用。如果块大小太小，分割的好处会减少，因为内存访问仍然主导执行时间。另一方面，如果块大小太大，它可能超过可用的快速内存，导致缓存冲突和性能下降。

负载均衡是另一个关键问题。在如 GPU 和 TPU 这样的架构中，计算在数千个处理单元上并行执行。如果分割不均匀分布，一些单元可能保持空闲，而其他单元可能过载，导致计算资源利用不充分。有效的分割调度确保并行执行保持平衡和高效。

数据移动开销也是一个重要的考虑因素。尽管分割减少了慢速内存访问的次数，但在不同内存级别之间传输分割仍然会产生成本。这在层次化内存系统中尤为重要，从缓存中访问数据比从 DRAM 中访问数据要快得多。需要有效的内存预取和调度策略来最小化延迟并确保数据在需要时可用。

不仅仅是空间和时间分割，混合方法结合了两种策略的元素以实现最佳性能。混合分割通过动态调整分割大小或根据实时执行条件重新排序计算来适应特定工作负载的约束。例如，一些 AI 加速器使用空间分割进行矩阵乘法，同时在卷积层中采用时间分割以重用权重。

除了分割之外，还有其他方法可以优化内存使用和计算效率。例如，寄存器分割、双缓冲和分层分割将基本的分割原则扩展到进一步优化执行。如 TensorFlow XLA、TVM 和 MLIR 等 AI 编译器和运行时系统根据硬件约束自动选择分割策略，从而实现无需人工干预的精细性能优化。

表 11.17 提供了空间分割、时间分割和混合分割方法的比较概述，突出了各自的优缺点。

表 11.17：**分割策略**：空间分割、时间分割和混合分割优化内存访问模式以提升性能；空间分割最大化快速内存中的数据重用，时间分割利用循环结构以减少访问，混合分割结合两种方法以平衡计算效率和内存带宽。这些技术对于 AI 编译器和运行时系统自动优化不同硬件上的模型执行至关重要。

| **方面** | **空间分割（数据分割）** | **时间分割（循环分割）** | **混合分割** |
| --- | --- | --- | --- |
| **主要目标** | 通过将数据保留在快速内存中更长时间来减少内存访问 | 在循环迭代中增加数据重用 | 动态适应工作负载约束 |
| **优化重点** | 将数据结构分割成更小、内存友好的块 | 重新排序计算以最大化重用，在淘汰之前 | 平衡空间和时间重用策略 |
| **内存使用** | 提高缓存局部性并减少 DRAM 访问 | 将频繁使用的数据保留在快速内存中，以便多次迭代 | 最小化数据移动，同时确保高重复使用 |
| **常见用例** | 矩阵乘法、CNN、变换器中的自注意力 | 卷积、循环神经网络（RNN）、迭代计算 | 具有分层内存的 AI 加速器、混合工作负载 |
| **性能提升** | 降低内存带宽需求，更好的缓存利用率 | 降低内存获取延迟，提高数据局部性 | 在多种硬件类型上最大化效率 |
| **挑战** | 需要仔细选择分块大小，对于空间重复使用最少的工作负载效率低下 | 可能会增加寄存器压力，需要循环重构 | 动态调整分块大小和执行顺序的复杂性 |
| **最佳使用时机** | 数据量大且需要分区以进行高效处理 | 相同数据在迭代中多次访问 | 数据分区和基于迭代的重复使用都很重要 |

随着机器学习模型在规模和复杂性上的持续增长，分块仍然是一个提高硬件效率的关键工具，确保 AI 加速器发挥其全部潜力。虽然手动分块策略可以提供大量好处，但现代编译器和硬件感知优化技术通过自动选择针对给定工作负载最有效的分块策略来进一步提高性能。

### 将映射策略应用于神经网络

虽然这些基础映射技术具有广泛的适用性，但它们的有效性取决于不同神经网络架构的计算结构、数据访问模式和并行化机会。每种架构都对数据移动、内存层次结构和计算调度施加了独特的约束，需要定制映射策略以优化性能。

在将计算分配给 AI 加速器时，采用结构化的映射方法对于解决由此产生的组合爆炸式选择至关重要。我们不是将每个模型视为一个独立的优化问题，而是认识到，相同的根本原则适用于不同的架构——只是它们的优先级根据工作负载特性而变化。目标是系统地选择并应用映射策略，以最大化不同类型机器学习模型的效率。

这些原则适用于三个代表性的 AI 工作负载，每个工作负载都有独特的计算需求。CNNs 受益于空间数据重用，使得权重静止执行和应用瓦片技术特别有效。相比之下，Transformers 本质上是内存受限的，依赖于诸如高效的 KV 缓存管理、融合的注意力机制和高度并行执行等策略来减轻内存流量。MLPs 涉及大量的矩阵乘法操作，需要使用结构化瓦片、优化的权重布局和内存感知执行来提高整体性能。

尽管它们之间存在差异，但每个模型都遵循一套共同的映射原则，优化优先级的差异。以下表格提供了不同优化策略及其对 CNNs、Transformers 和 MLPs 适用性的结构化映射。此表作为选择不同机器学习工作负载的适当映射策略的路线图。

| **优化技术** | **CNNs** | **Transformers** | **MLPs** | **理由** |
| --- | --- | --- | --- | --- |
| **数据流策略** | 权重静止 | 激活静止 | 权重静止 | CNNs 在空间位置间重用过滤器；Transformers 重用激活（KV 缓存）；MLPs 在批次间重用权重。 |
| **内存感知张量布局** | NCHW (通道优先) | NHWC (行优先) | NHWC | CNNs 偏好通道优先以提高卷积效率；Transformers 和 MLPs 优先考虑行优先以实现快速内存访问。 |
| **内核融合** | 卷积 + 激活 | 融合注意力 | GEMM 融合 | CNNs 优化卷积+激活融合；Transformers 融合注意力机制；MLPs 从融合的矩阵乘法中受益。 |
| **内存效率的瓦片化** | 空间瓦片 | 时间瓦片 | 分块瓦片 | CNNs 沿空间维度瓦片化；Transformers 使用循环分块以提高序列内存效率；MLPs 使用分块瓦片进行大型矩阵乘法。 |

此表强调，每个机器学习模型都受益于不同组合的优化技术，这强调了根据工作负载的计算和内存特性定制执行策略的重要性。

在以下各节中，我们将探讨这些优化如何应用于每种网络类型，解释 CNNs、Transformers 和 MLPs 如何利用特定的映射策略来提高执行效率和硬件利用率。

#### 卷积神经网络

CNNs 以其结构化的空间计算为特征，其中小型滤波器（或核）在输入特征图上反复应用。这种结构化的权重重用使得权重固定执行成为 CNNs 中最有效的策略。在流激活的同时保持滤波器权重在快速内存中，确保权重不需要从较慢的外部内存中反复获取，从而显著降低内存带宽需求。由于每个权重应用于多个空间位置，权重固定执行最大化了算术强度并最小化了冗余内存传输。

记忆感知的张量布局在 CNN 执行中也发挥着关键作用。卷积操作受益于通道主内存格式，通常表示为 NCHW（批次、通道、高度、宽度）。这种布局与卷积的访问模式相一致，使得在 GPU 和 TPU 等加速器上实现高效的内存归约。通过以优化缓存局部性的格式存储数据，加速器可以有效地获取连续的内存块，减少延迟并提高吞吐量。

核融合是 CNNs 的另一个重要优化。在典型的机器学习管道中，卷积操作通常随后跟随着激活函数，如 ReLU 和批量归一化。将这些操作视为单独的计算步骤而不是融合成一个单一核，可以减少中间内存写入并提高执行效率。这种优化通过将中间值保持在寄存器中而不是写入内存并在后续步骤中重新获取，从而最小化内存带宽压力。

考虑到输入图像和特征图的大小，分块是必要的，以确保计算适合快速内存层次结构。空间分块，即输入特征图在较小的子区域中处理，允许高效地利用片上内存，同时避免过多的片外内存传输。这项技术确保输入激活、权重和中间输出尽可能长时间地保持在高速缓存或共享内存中，减少内存停滞并提高整体性能。

这些优化共同确保了卷积神经网络（CNNs）通过最大化权重重用、优化内存访问模式、减少冗余内存写入以及结构化计算以适应快速内存约束，从而高效地利用可用的计算资源。

#### Transformer 架构

与依赖于结构化空间计算的 CNNs 不同，Transformers 处理可变长度的序列，并且高度依赖于注意力机制。Transformers 的主要计算瓶颈是内存带宽，因为注意力机制需要频繁访问多个查询向量中的存储键值对。鉴于这种访问模式，激活站态执行是最有效的策略。通过将键值激活保留在快速内存中，并动态地流式传输查询向量，最大化了激活的重用，同时最小化了冗余的内存读取。这种方法对于减少带宽开销至关重要，尤其是在自然语言处理等长序列任务中。

内存布局优化对于 Transformers 同样重要。与受益于通道主布局的 CNNs 不同，Transformers 需要有效地访问激活序列，使得行主格式（NHWC）成为首选。这种布局确保激活在内存中连续访问，减少了缓存未命中并提高了矩阵乘法中的内存归约。

内核融合在优化 Transformer 执行中扮演着关键角色。在自注意力中，多个计算步骤，如查询-键点积、softmax 归一化和加权求和，可以融合为一个单一操作。融合的注意力内核通过在单个执行步骤中计算注意力分数和执行加权求和来消除中间内存写入。这种优化显著减少了内存流量，尤其是在大批量大小和长序列中。

由于序列处理的特性，必须对分块进行适配以提高内存效率。与对 CNNs 有效的空间分块不同，Transformers 从时间分块中受益，其中计算被结构化以有效地处理序列块。这种方法确保激活以可管理的块加载到快速内存中，减少了过度的内存传输。时间分块对于长序列模型特别有益，因为键值激活的内存占用显著增加。通过将序列分块为更小的段，提高了内存局部性，使得高效的缓存利用成为可能，并减少了带宽压力。

这些优化通过优先考虑激活重用、结构化内存布局以进行高效的批量计算、融合注意力操作以减少中间内存写入，以及采用适合基于序列处理的分块技术，共同解决了 Transformer 模型的瓶颈。

#### 多层感知器

MLPs 主要由全连接层组成，其中权重和激活的大型矩阵相乘以产生输出表示。鉴于这种结构，权重静态执行是 MLPs 最有效的策略。类似于 CNNs，MLPs 从保持权重在局部内存中同时动态流激活中受益，因为这确保了权重矩阵，通常在批处理中的多个激活之间重复使用，不需要频繁重新加载。

MLPs（多层感知器）的首选内存布局与 Transformers 的布局一致，因为在使用行主序（NHWC）格式时矩阵乘法更高效。由于激活矩阵是分批处理的，这种布局确保了输入激活的访问效率，同时不会引入内存碎片。通过将张量存储与计算友好的内存访问模式对齐，提高了缓存利用率，减少了内存停滞。

MLPs 中的内核融合主要应用于通用矩阵乘法（GEMM）操作 29。由于密集层通常随后跟随着激活函数和偏置添加，将这些操作融合到单个计算步骤中可以减少内存流量。GEMM 融合确保激活、权重和偏置在单个优化的内核中处理，避免了不必要的内存写入和重新加载。

为了进一步提高内存效率，MLPs 依赖于分块填充策略，其中大型矩阵乘法被分成适合加速器共享内存的小型子块。这种方法确保了矩阵频繁访问的部分在整个计算过程中保持在快速内存中，减少了外部内存访问。通过以平衡内存利用与高效并行执行的方式结构化计算，分块填充最小化了带宽限制并最大化了吞吐量。

这些优化确保了 MLPs 通过围绕权重重用结构化执行、优化密集矩阵操作的内存布局、通过内核融合减少冗余内存写入以及采用分块填充策略以最大化片上内存利用率，实现了高计算效率。

### 混合映射策略

虽然通用的映射策略为优化机器学习模型提供了一个结构化的框架，但现实世界的架构往往涉及多样化的计算需求，这些需求无法通过单一、固定的方法有效解决。混合映射策略允许 AI 加速器动态地将不同的优化应用于模型中的特定层或组件，确保每个计算都以最大效率执行。

机器学习模型通常由多种层类型组成，每种类型都表现出独特的内存访问模式、数据重用特性和并行化机会。通过针对这些特定属性定制映射策略，混合方法比统一映射方法实现了更高的计算效率、改进的内存带宽利用率和减少的数据移动开销（参见 Sze 等人 2017b）。

#### 层特定映射

混合映射策略在结合空间局部计算（如卷积）和全连接操作（如密集层或注意力机制）的模型中特别有益。这些操作具有不同的特性，需要不同的映射策略以实现最佳性能。

在卷积神经网络中，混合策略经常被采用以优化性能。具体来说，权重静态执行应用于卷积层，确保滤波器保持在局部内存中，而激活动态流过。对于全连接层，输出静态执行被用来最小化矩阵乘法过程中的冗余内存写入。此外，核融合被集成以将激活函数、批量归一化和逐元素操作合并为单个计算步骤，从而减少中间内存流量。总体而言，这些方法提高了计算效率和内存利用率，从而有助于网络的总体性能。

变换器通过优化内存使用和计算效率来采用多种策略以提高性能。具体来说，它们在自注意力层中使用激活静态映射以最大化存储的关键值对的复用，从而减少内存访问。在前馈层中，应用权重静态映射以确保大型权重矩阵在计算中高效复用。此外，这些模型还集成了融合的注意力内核，将 softmax 和加权求和集成到单个计算步骤中，显著提高了执行速度（参见 Jacobs 等人 2002）。

对于多层感知器，混合映射策略通过结合提高内存效率和计算吞吐量的技术来优化性能。具体来说，使用权重静态执行来最大化激活之间的权重重用，确保这些频繁访问的参数始终可用，并减少冗余内存访问。此外，对于大型矩阵乘法，实施分块填充策略，通过将计算划分为适合快速内存的管理子块来显著提高缓存局部性。补充这些方法，应用通用矩阵乘法融合，通过合并连续的矩阵乘法操作与后续的功能转换来有效减少内存停滞。总体而言，这些优化展示了如何通过定制映射策略在多层感知器架构中系统地平衡内存约束和计算需求。

混合映射策略在视觉 Transformer 中得到了广泛应用，这些模型无缝集成了卷积和自注意力操作。在这些模型中，补丁嵌入层执行类似于卷积的操作，并受益于权重静态映射（Dosovitskiy 等人 2020）。另一方面，自注意力层需要激活静态执行来有效地在多个查询之间重用键值缓存。此外，MLP 组件利用通用矩阵乘法融合和分块填充来有效地执行密集矩阵乘法。这一针对特定层的优化框架有效地平衡了内存局部性和计算效率，使得视觉 Transformer 特别适合人工智能加速器。

### 混合策略的硬件实现

几种现代人工智能加速器采用了混合映射策略，通过针对不同神经网络架构的独特计算需求定制层特定技术来优化执行。例如，谷歌 TPU 在 Transformer 模型中对卷积层使用权重静态映射，对注意力层使用激活静态映射，确保最关键的数据保留在快速内存中。同样，NVIDIA GPU 利用融合内核和混合内存布局，在同一个模型内应用不同的映射策略，以最大化性能。此外，Graphcore IPU 根据每层动态选择执行策略，以优化内存访问，从而提高整体计算效率。

这些现实世界的实现说明了混合映射策略如何弥合不同类型机器学习计算之间的差距，确保每一层都以最大效率执行。然而，硬件支持对于这些技术的实用性至关重要。加速器必须提供可编程内存层次结构、高效的互连和专门的执行流水线等架构特性，以充分利用混合映射。

混合映射为深度学习执行提供了一种灵活且高效的方法，使 AI 加速器能够适应现代架构的多样化计算需求。通过为每一层选择最优的映射技术，混合策略有助于减少内存带宽限制，提高数据局部性，并最大化并行性。

虽然混合映射策略在层特定级别优化计算提供了一个有效的方法，但它们仍然是静态的设计时优化。在现实世界的 AI 工作负载中，由于输入大小、内存竞争或硬件资源可用性的变化，执行条件可以动态变化。机器学习编译器和运行时系统通过引入动态调度、内存优化和自动调整机制来扩展这些映射技术。这些系统确保混合策略不仅是一系列预定义的执行选择，而且是一种适应性机制，允许深度学习工作负载在不同的加速器和部署环境中高效运行。在下一节中，我们将探讨机器学习编译器和运行时堆栈如何通过即时调度、内存感知执行和工作负载平衡策略实现这些适应性优化。

## 编译器支持

机器学习加速的性能不仅取决于硬件能力，还取决于模型如何有效地转换为可执行操作。包括内核融合、分块、内存调度和数据移动策略在内的这些优化技术对于最大化效率至关重要。然而，这些优化必须在执行前系统地应用，以确保它们与硬件约束和计算需求相一致。

这个过程体现了在第 11.1 节中建立的硬件-软件协同设计原则，其中机器学习编译器将高级模型表示与低级硬件执行相连接。编译器通过重构计算、选择高效的执行内核和最大化硬件利用率来优化模型(0001 et al. 2018a)。与为通用计算设计的传统编译器不同，机器学习工作负载需要针对张量计算和并行执行采用专门的方案。

### 机器学习工作负载的编译器设计差异

机器学习工作负载引入了传统编译器未设计来处理的独特挑战。与主要涉及顺序或多线程程序流的传统软件执行不同，机器学习模型以计算图的形式表达，描述了大规模的张量操作。这些图需要专门的优化，而传统编译器无法有效地应用这些优化（Cui, Li, and Xie 2019）。

表 11.18 概述了传统编译器和为机器学习工作负载设计的编译器之间的基本差异。虽然传统编译器通过指令调度和寄存器分配等技术优化线性程序执行，但机器学习编译器则专注于优化计算图以实现高效的张量操作。这种区别至关重要，因为机器学习编译器必须结合领域特定的转换，如内核融合、内存感知调度和硬件加速执行计划，以在如 GPU 和 TPU 等专用加速器上实现高性能。

这种比较突出了为什么机器学习模型需要不同的编译方法。机器学习编译器不是优化指令级执行，而是必须转换整个计算图，应用张量感知的内存优化，并在数千个并行处理元素之间调度操作。这些需求使得传统编译技术不足以应对现代深度学习工作负载。

表 11.18：**编译器优化优先级**：传统编译器和机器学习编译器在优化目标上存在差异；传统编译器优先考虑顺序代码的高效执行，而机器学习编译器则专注于优化计算图中的张量操作以适应专用硬件。此表阐明了机器学习编译器如何结合领域特定的转换——如内核融合和内存感知调度——以在加速器上实现高性能，这与传统软件编译中使用的指令调度和寄存器分配技术不同。

| **方面** | **传统编译器** | **机器学习编译器** |
| --- | --- | --- |
| **输入表示** | 线性程序代码（C、Python） | 计算图（机器学习模型） |
| **执行模型** | 顺序或多线程执行 | 基于张量的大规模并行执行 |
| **优化优先级** | 指令调度、循环展开、寄存器分配 | 图转换、内核融合、内存感知执行 |
| **内存管理** | 栈和堆内存分配 | 张量布局转换、分块、内存感知调度 |
| **目标硬件** | CPU（通用执行） | GPU、TPU 和定制加速器 |
| **编译输出** | CPU 特定的机器代码 | 硬件特定的执行计划（内核、内存调度） |

### 机器学习编译流程

在现代框架中定义的机器学习模型最初以高级计算图的形式表示，该图描述了对张量的操作。然而，这些表示不能直接在 GPU、TPU 和定制 AI 芯片等硬件加速器上执行。为了实现高效执行，模型必须经过一个编译过程，将其转换为针对目标硬件优化的执行计划（Brain 2020）。

机器学习编译工作流程包括几个关键阶段，每个阶段负责应用特定的优化，以确保最小内存开销、最大并行执行和最佳计算利用率。这些阶段包括：

1.  **图优化**：计算图被重构以消除低效部分。

1.  **内核选择**：每个操作映射到一个优化的特定于硬件的实现。

1.  **内存规划**：张量布局和内存访问模式被优化以减少带宽消耗。

1.  **计算调度**：工作负载被分配到并行处理元素，以最大化硬件利用率。

1.  **代码生成**：优化的执行计划被转换为特定于机器的执行指令。

在每个阶段，编译器应用之前讨论过的理论优化，包括内核融合、分块、数据移动策略和计算放置，确保这些优化系统地纳入最终的执行计划。

通过理解这个工作流程，我们可以看到机器学习加速不仅通过硬件改进，还通过编译器驱动的软件优化来实现。

### 图优化

AI 加速器提供专门的硬件以加快计算速度，但原始模型表示并非天生就针对这些加速器的执行进行了优化。机器学习框架使用高级计算图定义模型，其中节点表示操作（如卷积、矩阵乘法和激活），边定义数据依赖关系。然而，如果按照定义执行，这些图通常包含冗余操作、低效的内存访问模式和不理想的执行序列，这可能会阻止硬件以峰值效率运行。

例如，在 Transformer 模型中，自注意力机制涉及在多个注意力头之间重复访问相同的键值对。如果简单编译，模型可能会多次重新加载相同的数据，导致过度的内存流量（Shoeybi 等人 2019a）。同样，在 CNN 中，在每个卷积之后将批量归一化和激活函数作为单独的操作应用会导致不必要的中间内存写入，增加内存带宽使用。这些低效性在图优化阶段得到解决，编译器重构计算图以消除不必要的操作并提高内存局部性（0001 等人 2018a）。

编译的图优化阶段负责在将其映射到硬件之前将这个高级计算图转换为优化的执行计划。编译器不需要手动优化，而是系统地应用改进数据移动、减少冗余计算和重构操作以实现高效并行执行的变化（NVIDIA 2021）。

在这个阶段，编译器仍然在硬件无关的层面上工作，专注于在后续应用更具体的硬件优化之前，提高效率的高级重构。

#### 计算图优化

图优化通过一系列旨在提高执行效率的结构化技术来转换计算图。其中一个关键技术是内核融合，它将连续的操作合并以消除不必要的内存写入并减少内核启动次数。这种方法在卷积神经网络中特别有效，其中融合卷积、批量归一化和激活函数可以显著加速处理。另一个重要技术是计算重排，它调整操作的执行顺序以改善数据局部性和最大化并行执行。例如，在 Transformer 模型中，这种重排使得可以重复使用缓存的键值对，而不是反复从内存中重新加载，从而降低延迟。

此外，冗余计算消除也发挥着重要作用。通过识别和删除重复或不必要的操作，这种方法在具有残差连接的模型中特别有益，在这些模型中，常见的子表达式可能会被冗余计算。内存感知的数据流调整通过优化张量布局和内存移动来提高整体性能。例如，将矩阵乘法分块以满足 TPU 中收缩阵列的结构要求，确保硬件资源得到最优利用。这种结合的方法不仅减少了不必要的处理，而且使数据存储和移动与加速器的优势保持一致，从而在多样化的 AI 工作负载中实现高效的执行。这些技术共同通过最小化开销并确保计算资源和内存资源之间的最佳平衡，为模型的加速做准备。

#### 在 AI 编译器中的实现

现代 AI 编译器通过使用自动模式识别和结构化重写规则来执行图优化，系统地转换计算图以最大化效率，而无需人工干预。例如，TensorFlow 中的 Google XLA（加速线性代数）在 TPU 和 GPU 上应用图级转换，如融合和布局优化，以简化执行。同样，TVM（张量虚拟机）不仅优化张量布局和调整计算结构，还针对不同的硬件后端调整执行策略，这对于在具有严格内存约束的嵌入式 Tiny ML 设备上部署模型特别有益。

NVIDIA 的 TensorRT，另一个专门的深度学习编译器，通过融合操作和优化 GPU 上的执行调度来最小化内核启动开销，从而提高大规模卷积神经网络应用中的利用率并减少推理延迟。此外，MLIR（多级中间表示）通过启用多阶段转换来促进跨各种 AI 加速器的灵活图优化，这些转换可以改善执行顺序和内存访问模式，从而简化模型从基于 CPU 的实现到加速器优化版本的过渡。这些编译器在重写计算图以确保后续的硬件特定优化可以有效地应用的同时，保留了模型的数学完整性。

#### 图优化的重要性

图优化使 AI 加速器能够以峰值效率运行。如果没有这个阶段，即使是最优化的硬件也会被低效利用，因为模型将以引入不必要的内存停滞、冗余计算和低效数据移动的方式执行。通过系统地重构计算图，编译器安排操作以实现高效的执行，在映射到硬件之前缓解瓶颈，最小化内存移动以保持张量在高速内存中，并优化并行执行以减少不必要的序列化并提高硬件利用率。例如，如果没有适当的图优化，运行在边缘设备上的大型 Transformer 模型可能会因为次优的数据访问模式而经历过度的内存停滞；然而，通过有效的图重构，模型可以以显著降低的内存带宽消耗和延迟运行，从而在资源受限的设备上实现实时推理。

计算图现在已完全优化，编译的下一步是内核选择，编译器确定每个操作应使用哪个硬件特定的实现。这确保了结构化执行计划被转换为针对目标加速器的优化低级指令。

### 内核选择

在这个阶段，编译器将计算图中的抽象操作转换为优化后的低级函数，确保在目标加速器的约束下执行尽可能高效。内核是针对特定硬件架构高效运行的计算操作的专用实现。大多数加速器，包括 GPU、TPU 和定制 AI 芯片，为同一操作提供多个内核实现，每个实现针对不同的执行场景进行了优化。为每个操作选择正确的内核对于最大化计算吞吐量、最小化内存停滞并确保加速器的专用处理元素得到充分利用至关重要 (NVIDIA 2021)。

内核选择建立在图优化阶段之上，确保结构化执行计划映射到最有效的实现。虽然图优化消除了模型层面的低效，但内核选择确保每个单独的操作都使用最有效的硬件特定例程执行。此过程的有效性直接影响模型的总体性能，因为较差的内核选择可能会通过引入不必要的计算开销或内存瓶颈来抵消先前优化的好处 (0001 et al. 2018a)。

在 Transformer 模型中，主导自注意力计算的矩阵乘法可以根据可用的硬件采用不同的策略执行。在 CPU 上，通常使用通用矩阵乘法例程，利用向量化执行来提高效率。相比之下，在 GPU 上，编译器可能会选择一种利用张量核心来加速矩阵乘法并使用混合精度算术的实施方案。当模型部署在 TPU 上时，操作可以映射到一个脉动阵列，确保数据以最大化重用和最小化片外内存访问的方式通过加速器。此外，对于推理工作负载，整数算术内核可能更可取，因为它便于在 INT8 而不是浮点精度下进行计算，从而在不显著降低精度的同时减少功耗。

在许多情况下，编译器不是从头开始生成自定义内核，而是从供应商优化的内核库中选择，这些库为不同的架构提供了高度优化的实现。例如，cuDNN 和 cuBLAS 为 NVIDIA GPU 上的深度学习提供了优化内核，而 oneDNN 为 Intel 架构提供了优化执行。同样，ACL（Arm Compute Library）针对基于 Arm 的设备进行了优化，Eigen 和 BLIS 为深度学习操作提供了高效的基于 CPU 的实现。这些库允许编译器选择预优化的、高性能的内核，而不是为每个硬件平台重新发明执行策略。

#### 在 AI 编译器中的实现

AI 编译器使用启发式方法、配置文件分析和成本模型来确定每个操作的最佳内核。这些策略确保每个计算都以最大化吞吐量和最小化内存瓶颈的方式进行执行。

在基于规则的选取中，编译器根据硬件的已知能力应用预定义的启发式方法。例如，TensorFlow 中使用的编译器 XLA，当启用混合精度执行时，会自动选择针对 NVIDIA GPU 的张量核心优化内核。这些预定义的规则允许编译器快速、可靠地做出关于使用哪个内核的决定，而无需进行广泛的分析。

基于配置文件的选择采用了一种更动态的方法，基准测试不同的内核选项，并选择对给定工作负载表现最佳的选项。开源 AI 编译器 TVM 使用 AutoTVM 来经验性地评估内核性能，根据实际的执行时间调整执行策略。通过在部署前测试不同的内核，基于配置文件的选择有助于确保操作在实际执行条件下被分配给最有效的实现。

另一种基于成本模型的选择方法，依赖于性能预测来估计各种内核的执行时间和内存消耗，在选择了最有效的内核之后。MLIR 是一种为机器学习工作负载设计的编译器基础设施，它应用了这种技术来确定最有效的瓦片和内存访问策略（Lattner 等人 2020）。通过模拟不同内核如何与加速器的计算单元和内存层次结构交互，编译器可以选择在最大化性能的同时最小化执行成本的内核。

许多人工智能编译器还集成了精度感知的内核选择，所选内核针对特定的数值格式进行优化，例如 FP32、FP16、BF16 或 INT8。训练工作负载通常优先考虑更高的精度（FP32、BF16），以保持模型精度，而推理工作负载则更倾向于较低的精度（FP16、INT8），以提高速度并减少功耗。例如，运行 TensorRT 进行推理的 NVIDIA GPU 可以根据模型的精度约束动态选择 FP16 或 INT8 内核。精度与性能之间的权衡是内核选择的关键方面，尤其是在资源受限的环境中部署模型时。

一些编译器不仅超越了静态内核选择，还实现了自适应内核调整，即在运行时根据系统的负载和可用资源调整执行策略。TVM 中的 AutoTVM 通过在不同工作负载上测量内核性能，动态地优化执行策略。TensorRT 根据批大小、内存约束和 GPU 负载进行实时优化，动态调整内核选择。谷歌的 TPU 编译器采取了类似的方法，根据云资源可用性和执行环境约束优化内核选择。

#### 内核选择的重要性

人工智能加速的效率不仅取决于计算的构建方式，还取决于它们的执行方式。即使是最精心设计的计算图，如果所选内核没有充分利用硬件的能力，也无法达到峰值性能。

正确的内核选择允许模型使用针对给定硬件的最有效算法执行，确保内存访问方式避免不必要的停滞，并在可能的情况下利用专门的加速功能，例如张量核心或脉动阵列。选择不适当的内核可能导致计算资源利用率低下、内存传输过多以及功耗增加，所有这些都会限制人工智能加速器的性能。

例如，如果一个在 GPU 上运行的 Transformer 模型被分配了一个非张量核心内核进行矩阵乘法，它可能只能达到可能性能的一小部分。相反，如果一个为 FP32 执行设计的模型被迫在一个 INT8 优化的内核上运行，它可能会经历显著的数值不稳定性，降低精度。这些选择说明了为什么内核选择与保持数值正确性一样，关乎优化性能。

内核选择完成后，编译的下一阶段涉及执行调度和内存管理，其中编译器确定如何启动内核以及如何在内存层次结构的不同级别之间传输数据。这些编译管道的最后一步确保计算以最大并行性运行，同时最小化数据移动的开销。内核选择决定了要执行的内容，而执行调度和内存管理则决定了这些内核何时以及如何执行，确保人工智能加速器以最高效率运行。

### 内存规划

内存规划阶段确保数据以最小化内存带宽消耗、降低延迟和最大化缓存效率的方式分配和访问(Y. Zhang, Li, 和 Ouyang 2020)。即使是最优化的执行计划，如果内存管理效率不高，模型仍然可能遭受严重的性能下降。

机器学习工作负载通常是内存密集型的。它们需要在内存层次结构的多个级别之间频繁移动大型张量。编译器必须确定张量的存储方式、访问方式以及中间结果的处理方式，以确保内存不会成为瓶颈。

内存规划阶段专注于优化张量布局、内存访问模式和缓冲区重用，以防止执行过程中的不必要的停滞和内存竞争。在这个阶段，张量以内存高效的格式排列，与硬件访问模式相一致，从而最小化格式转换的需要。此外，内存访问被结构化以减少缓存未命中和停滞，这反过来又降低了整体带宽消耗。缓冲区重用也是一个关键方面，因为它通过智能管理中间结果来减少冗余内存分配。这些策略共同确保数据被有效地放置和访问，从而在人工智能工作负载中提高计算性能和能源效率。

#### 在人工智能编译器中的实现

内存规划是一个复杂的问题，因为人工智能模型必须在跨越内存层次结构的多个级别上平衡内存可用性、重用和访问效率。人工智能编译器使用几种关键策略来有效地管理内存并防止不必要的数据移动。

内存规划的第一步是张量布局优化，编译器确定张量在内存中的排列方式，以最大化局部性并防止不必要的格式转换。不同的硬件加速器有不同的首选存储布局——例如，NVIDIA GPU 通常使用行主序存储（NHWC 格式），而 TPU 则偏好通道主序布局（NCHW 格式）以优化内存归约（Martín Abadi 等人 2016）。编译器根据目标硬件预期的访问模式自动转换张量布局，确保内存访问对齐以实现最大效率。

除了布局优化之外，内存规划还包括缓冲区分配和重用，编译器通过尽可能重用中间存储来最小化内存占用。深度学习工作负载生成许多临时张量，如激活和梯度，如果不加管理，这些张量可能会迅速耗尽片上内存。而不是为每个张量分配新的内存，编译器分析计算图以识别缓冲区重用的机会，确保中间值被高效地存储和覆盖（G. A. Jones 2018）。

内存规划的另一个关键方面是尽量减少不同内存层次之间的数据移动。人工智能加速器通常包含高速片上内存（如缓存或共享 SRAM）和更大但较慢的外部 DRAM。如果张量数据在这些内存层次之间反复移动，模型可能会变得内存受限，降低计算效率。为了防止这种情况，编译器使用分块策略，将大型计算分解成更小、内存友好的块，使执行能够适应快速、局部内存，并减少对昂贵的片外内存访问的需求。

#### 内存规划的重要性

没有适当的内存规划，即使是最优化的计算图和内核选择也无法实现高性能。过度的内存传输、低效的内存布局和冗余的内存分配都可能造成瓶颈，阻碍人工智能加速器达到其峰值吞吐量。

例如，在一个 GPU 上运行的卷积神经网络（CNN）在理论上可能实现高计算效率，但如果其卷积特征图以不兼容的格式存储，例如，如果它使用需要转换为 NCHW 或类似格式的行主序布局，那么频繁的张量格式转换会引入显著的开销。同样，如果内存规划不当，部署在边缘设备上的 Transformer 模型可能难以满足实时推理要求，导致频繁的片外内存访问，增加延迟和功耗。

通过仔细管理张量放置、优化内存访问模式和减少不必要的数据移动，内存规划保证了 AI 加速器的有效运行，从而在实际应用中带来了可观的性能提升。

### 计算调度

在完成图优化、选择内核和最终确定内存规划之后，编译流程中的下一步是计算调度。这一阶段决定每个计算何时何地执行，确保工作负载在可用的处理元素之间高效分布，同时避免不必要的停滞和资源竞争 (Rajbhandari 等人 2020a; Zheng 等人 2020)。

AI 加速器通过大规模并行化实现高性能，但没有有效的调度策略，计算单元可能会闲置，内存带宽可能未被充分利用，执行效率可能会下降。计算调度负责确保所有处理元素保持活跃，执行依赖关系得到正确管理，并且工作负载得到优化分配 (Ziheng Jia 等人 2019)。

在调度阶段，并行执行、同步和资源分配得到系统化管理。任务分区将大量计算分解为更小、更易于管理的任务，这些任务可以高效地分配到多个计算核心。执行顺序优化随后确定启动这些操作的最有效顺序，最大化硬件性能同时减少执行停滞。此外，资源分配和同步得到协调，以确保计算核心、内存带宽和共享缓存得到有效利用，避免竞争。通过这些协调策略，计算调度实现了最优的硬件利用率，最小化了内存访问延迟，并支持了流畅高效的执行过程。

#### AI 编译器中的实现

计算调度高度依赖于底层硬件架构，因为不同的 AI 加速器具有独特的执行模型，在确定工作负载如何调度时必须考虑这些模型。AI 编译器实施了几种关键策略来优化调度以实现高效执行。

调度中最基本的方面之一是任务划分，编译器将大的计算图划分为更小、更易于管理的单元，以便并行执行。在 GPU 上，这通常意味着将矩阵乘法和卷积映射到数千个 CUDA 核心，而在 TPU 上，任务被划分以适应操作结构化数据流的脉冲阵列（Norrie 等人 2021）。在 CPU 上，划分通常侧重于将计算分解为与 SIMD 执行对齐的矢量化块。目标是高效地将工作负载映射到可用的处理单元，确保每个核心在整个执行过程中保持活跃。

调度涉及优化执行顺序以最小化依赖关系并最大化吞吐量，而不仅仅是任务划分。许多 AI 模型包括可以独立计算的运算（例如，批处理管道中的不同批次）以及具有严格依赖关系的运算（例如，RNN 中的循环层）。AI 编译器分析这些依赖关系，并尝试尽可能重新排列执行，以减少闲置时间并提高并行效率。例如，在 Transformer 模型中，调度可能优先将注意力矩阵预加载到内存中，同时早期层仍在执行，确保数据在需要时已准备好（Shoeybi 等人 2019b）。

计算调度的另一个关键方面是资源分配和同步，编译器确定计算核心如何共享内存并协调执行。现代 AI 加速器通常支持重叠计算和数据传输，这意味着当一个任务执行时，下一个任务可以开始获取其所需的数据。编译器通过以这种方式调度任务来利用这一点，隐藏内存延迟，确保执行保持计算密集型而不是内存密集型（0001 等人 2018b）。例如，TensorRT 和 XLA 采用流式执行策略，其中并行启动多个内核，并仔细管理同步以防止执行停滞（Google 2025）。

#### 计算调度的重要性

没有有效的调度，即使是经过优化的模型也可能因为计算资源的利用率不足、内存瓶颈和执行效率低下而受到影响。不良的调度决策可能导致处理元素闲置，迫使昂贵的计算核心在继续执行之前等待数据或同步事件。

例如，在 GPU 上运行的卷积神经网络（CNN）可能具有高度优化的内核和高效的内存布局，但如果其执行调度不当，计算单元在内核启动之间可能会闲置，从而降低吞吐量。同样，部署在 TPU 上的 Transformer 模型可能能够高效地执行矩阵乘法，但如果注意力层没有有效地与内存传输重叠调度，可能会经历性能下降。

有效的计算调度在并行工作负载的编排中占据核心地位，确保处理元素被充分利用，同时防止闲置核心——这是最大化整体吞吐量的关键方面。通过策略性地重叠计算与数据移动，调度机制有效地隐藏了内存延迟，从而防止在数据检索期间出现操作停滞。通过精确解决执行依赖关系，它最小化了等待时间，并增强了计算和数据传输的并发进展。这种调度和数据处理的系统整合不仅提高了性能，而且展示了支撑现代加速器设计的严谨工程原则。

#### 代码生成

与之前需要特定 AI 优化优化的阶段不同，代码生成遵循了许多与传统编译器相同的原理。这个过程包括指令选择、寄存器分配和最终的优化遍历，确保执行充分利用了硬件特定的特性，如向量执行、内存预取和指令重排。

对于 CPU 和 GPU，AI 编译器通常生成机器代码或优化的汇编指令，而对于 TPU、FPGA 和其他加速器，输出可能是优化后的字节码或由硬件的运行时系统解释的执行图。

到这一点，编译管道已经完成：原始的高级模型表示已经被转换成针对目标硬件高效执行的优化、可执行格式。图变换、内核选择、内存感知执行和并行调度的组合确保 AI 加速器以最大效率、最小内存开销和最佳计算吞吐量运行工作负载。

### 编译-运行时支持

编译器在 AI 加速中扮演着基本角色，将高级机器学习模型转换为针对专用硬件约束优化的执行计划。在本节中，我们已经看到图优化如何重构计算，内核选择如何将操作映射到硬件高效的实现，内存规划如何优化数据放置，以及计算调度如何确保高效的并行执行。这些阶段中的每一个对于使 AI 模型充分利用现代加速器至关重要，确保高吞吐量、最小内存开销和高效的执行管道。

然而，仅靠编译本身并不能保证在现实世界的 AI 工作负载中实现高效的执行。虽然编译器根据已知的模型结构和硬件能力静态优化计算，但 AI 执行环境通常是动态和不可预测的。批量大小波动，硬件资源可能被多个工作负载共享，加速器必须适应实时性能约束。在这些情况下，静态执行计划是不够的，运行时管理变得至关重要，以确保模型在现实世界条件下以最佳方式执行。

这种从静态编译到自适应执行的转变正是 AI 运行时发挥作用的地方。运行时提供动态内存分配、实时内核选择、工作负载调度和多芯片协调，使 AI 模型能够适应变化的执行条件，同时保持效率。在下一节中，我们将探讨 AI 运行时如何扩展编译器的功能，使模型能够在各种可扩展的部署场景中有效运行。

## 运行时支持

虽然编译器在执行前优化 AI 模型，但实际部署引入了动态和不可预测的条件，这些条件仅靠静态编译无法完全解决 (NVIDIA 2021)。AI 工作负载在多种执行环境中运行，其中诸如批量大小波动、共享硬件资源、内存竞争和延迟约束等因素需要实时适应。针对固定假设集预先编译的执行计划，当实际运行时条件发生变化时，可能变得次优。

为了弥合这一差距，AI 运行时提供了一层动态的执行管理，通过实时决策扩展了编译时执行的优化。与执行固定指令序列的传统编译程序不同，AI 工作负载需要适应内存分配、内核执行和资源调度的控制。AI 运行时持续监控执行条件，并实时调整以确保机器学习模型充分利用可用硬件，同时保持效率和性能保证。

在高层次上，AI 运行时管理执行过程中的三个关键方面：

1.  **内核执行管理**：AI 运行时根据当前系统状态动态选择和调度计算内核，确保工作负载以最小延迟执行。

1.  **内存适应和分配**：由于 AI 工作负载经常处理具有不同内存占用的大张量，运行时动态调整内存分配以防止瓶颈和过多的数据移动（Y. Huang 等人，2019）。

1.  **执行扩展**：AI 运行时处理跨多个加速器的作业分配，支持在多芯片、多节点或云环境中进行大规模执行（Mirhoseini 等人，2017）。

通过动态处理这些执行方面，AI 运行时补充了基于编译器的优化，确保模型在变化的运行时条件下继续高效运行。下一节将探讨 AI 运行时与传统软件运行时的不同之处，突出为什么机器学习工作负载需要与基于传统 CPU 的程序相比，采用根本不同的执行策略。

### 机器学习系统运行时架构差异

传统软件运行时旨在管理通用程序执行，主要处理 CPU 上的顺序和多线程工作负载。这些运行时在单个函数调用和指令级别分配内存、调度任务和优化执行。相比之下，AI 运行时专门针对机器学习工作负载，这些工作负载需要大规模并行计算、大规模张量操作和动态内存管理。

表 11.19 突出了传统运行时和 AI 运行时的基本差异。其中一个关键区别在于执行流程。传统软件运行时在可预测、结构化的执行模型上运行，其中函数调用和 CPU 线程遵循预定义的控制路径。然而，AI 运行时执行计算图，需要考虑张量操作之间的依赖关系、并行内核执行和高效内存访问的复杂调度决策。

表 11.19：**运行时执行模型**：传统运行时和 AI 运行时在执行方法上存在差异；传统运行时优先考虑顺序或多线程指令处理，而 AI 运行时利用大规模并行张量操作来加速机器学习工作负载的计算。这种区别需要专门设计的 AI 运行时架构，以实现大规模张量数据的有效并行化和内存管理。

| **方面** | **传统运行时** | **AI 运行时** |
| --- | --- | --- |
| **执行模型** | 顺序或多线程执行 | 大规模并行张量执行 |
| **任务调度** | CPU 线程管理 | 核心在加速器之间的调度 |
| **内存管理** | 静态分配（栈/堆） | 动态张量分配，缓冲区重用 |
| **优化优先级** | 低延迟指令执行 | 最小化内存停滞，最大化并行执行 |
| **适应性** | 主要静态执行计划 | 适应批量大小和硬件可用性 |
| **目标硬件** | CPUs（通用执行） | GPUs、TPUs 和定制加速器 |

内存管理是另一个主要区别。传统的软件运行时处理小而频繁的内存分配，优化缓存效率和低延迟访问。相比之下，AI 运行时必须动态分配、重用和优化大型张量，确保内存访问模式与加速器友好的执行相匹配。AI 工作负载中的内存管理不当可能导致性能瓶颈，尤其是由于过度的片外内存传输和低效的缓存使用。

AI 运行时天生具有适应性。虽然传统的运行时通常遵循一个主要静态的执行计划，但 AI 工作负载通常在高度可变的执行环境中运行，例如基于云的加速器或多租户硬件。因此，AI 运行时必须持续调整批量大小、重新分配计算资源，并管理实时调度决策，以保持高吞吐量和最小化执行延迟。

这些区别说明了为什么与传统的软件运行时相比，AI 运行时需要根本不同的执行策略。AI 运行时不仅要管理 CPU 进程，还必须监督大规模张量执行、多设备协调和实时工作负载适应，以确保机器学习模型能够在各种不断变化的部署条件下高效运行。

### 动态内核执行

动态内核执行是将机器学习模型映射到硬件并优化运行时执行的过程。虽然静态编译提供了一个坚实的基础，但高效执行机器学习工作负载需要实时适应波动条件，如可用内存、数据大小和计算负载。运行时充当一个中介，持续调整执行策略以匹配底层硬件的约束和工作负载的特性。

当将机器学习模型映射到硬件时，必须将单个计算操作（包括矩阵乘法、卷积和激活函数）分配给最合适的处理单元。这种映射不是固定的；它必须在运行时根据输入数据、内存可用性和整体系统负载的变化进行修改。动态内核执行允许运行时实时做出关于内核选择、执行顺序和内存管理的决策，确保即使在这些变化条件下，工作负载也能保持高效。

例如，考虑一个执行图像分类的深度神经网络（DNN）的 AI 加速器。如果传入的高分辨率图像批次所需的内存比预期多得多，静态计划的执行可能会导致缓存冲突或过度的片外内存访问。相反，动态运行时可以即时调整分块策略，将张量操作分解成适合高速片上内存的小块。这可以防止内存停滞并确保缓存的最佳利用率。

类似地，当运行基于 transformer 的自然语言处理（NLP）模型时，输入文本的序列长度可能在推理请求之间变化。针对固定序列长度优化的静态执行计划在处理较短的序列时可能会导致计算资源利用率不足，而在处理较长的序列时可能会产生过度的内存压力。动态内核执行可以通过根据实际序列长度选择不同的内核实现来缓解这种情况，动态调整内存分配和执行策略以保持效率。

计算与内存移动重叠是一种缓解性能瓶颈的重要策略。AI 工作负载经常遇到由于内存绑定问题而导致的延迟，其中数据在内存层次结构之间的移动限制了计算速度。为了应对这种情况，AI 运行时实施异步执行和双缓冲等技术，确保计算在没有等待内存传输完成的情况下进行。例如，在一个大规模模型中，可以在对前一个批次进行计算的同时预取图像数据，从而保持数据流的稳定并避免流水线停滞。

另一个实际例子是在 GPU 上执行卷积层。如果需要调度多个卷积内核，静态调度方法可能会因为层大小和计算需求的变化而导致资源利用率低下。通过动态调度内核执行，AI 运行时可以在计算单元部分占用时优先选择较小的内核，从而提高硬件利用率。例如，在 NVIDIA 的 TensorRT 运行时，将小内核融合到较大的执行单元中是动态进行的，以避免启动开销，优化对延迟敏感的推理任务。

动态内核执行在确保机器学习模型高效执行中起着至关重要的作用。通过根据实时系统条件动态调整执行策略，AI 运行时优化了跨各种硬件平台的训练和推理性能。

### 运行时内核选择

虽然编译器可能会根据机器学习模型和硬件目标的静态分析进行内核的初始选择，但 AI 运行时通常需要在执行过程中覆盖这些决策。实时因素，如可用内存、硬件利用率和工作负载优先级，可能与编译期间做出的假设有很大不同。通过在运行时动态选择和切换内核，AI 运行时可以适应这些变化条件，确保模型继续高效运行。

例如，考虑基于 transformer 的语言模型，其中执行时间的大部分都花在了矩阵乘法上。AI 运行时必须根据当前系统状态确定执行这些操作的最有效方式。如果模型运行在具有专用 Tensor Cores 的 GPU 上，运行时可能会从标准 FP32 内核切换到 FP16 内核，以利用硬件加速（Shoeybi 等人 2019a）。相反，如果 FP16 的较低精度导致不可接受的数值不稳定性，运行时可以选择混合精度执行，在需要更高精度的位置选择性地使用 FP32。

内存限制也会影响内核选择。当内存带宽有限时，运行时可能会调整其执行策略，重新排序操作或更改分块策略，以便将计算放入可用的缓存中，而不是依赖于较慢的主内存。例如，一个大的矩阵乘法可能被分解成更小的块，确保计算适合 GPU 的片上内存，从而降低整体延迟。

此外，批处理大小也会影响内核选择。对于处理小批量和大批量混合的工作负载，AI 运行时可能会为小批量选择延迟优化的内核，为大规模批处理选择吞吐量优化的内核。这种调整确保模型在不同执行场景中继续高效运行，无需手动调整。

### 内核调度和利用率

一旦 AI 运行时选择了合适的内核，下一步就是以最大化并行性和资源利用率为目标进行调度。与设计用于管理 CPU 线程的传统任务调度器不同，AI 运行时必须在并行执行单元（如 GPU 核心、张量处理单元或定制 AI 加速器）之间协调大量任务（Norman P. Jouppi 等人 2017a）。有效的调度确保这些计算资源保持完全投入，防止瓶颈并最大化吞吐量。

例如，在采用卷积层的图像识别模型中，操作可以分布在多个处理单元上，使得不同的过滤器可以同时运行。这种并行化确保了可用硬件得到充分利用，从而加快了执行速度。同样，批量归一化和激活函数必须高效地调度，以避免不必要的延迟。如果这些操作没有与其他计算交织进行，它们可能会阻塞流水线并降低整体吞吐量。

高效的内核调度也可能受到实时内存管理的影响。人工智能运行时确保在需要之前，将中间数据，如深度神经网络中的特征图，预先加载到缓存中。这种主动管理有助于防止由于从较慢的内存层级加载数据而导致的延迟，确保连续执行。

这些技术使人工智能运行时能够确保最佳资源利用和高效的并行计算，这对于机器学习模型的高性能执行至关重要，尤其是在需要跨多个硬件加速器进行扩展的环境中。

到目前为止，所检查的编译器和运行时系统优化了单个加速器内的执行——管理计算映射、内存层次结构和内核调度。虽然这些单芯片优化实现了令人印象深刻的性能提升，但现代人工智能工作负载越来越多地超出了任何单个芯片的能力。训练 GPT-3 需要连续运行单个 H100 10 年，消耗 314 十亿浮点运算。为全球应用提供实时推理服务需要超越任何单个加速器的吞吐量。这些计算需求源于第九章中的扩展定律（第九章），需要从单芯片优化到分布式加速策略的根本转变。

## 多芯片人工智能加速

从单芯片到多芯片架构的转变不仅仅是简单的复制——它需要重新思考计算如何在处理器之间分布，数据如何在芯片之间流动，以及系统如何在规模上保持一致性。单芯片优化侧重于在固定资源内最大化利用，而多芯片系统必须在计算分布与通信开销、内存一致性成本和同步复杂性之间取得平衡。这些挑战从根本上改变了优化格局，需要超出为单个加速器开发的抽象和技术之外的新抽象和技术。

现代人工智能工作负载越来越需要超出单片加速器能力的计算资源。本节探讨了人工智能系统如何从单个处理器扩展到多芯片架构，分析了不同扩展方法的动机及其对系统设计的影响。这些扩展考虑因素是第八章中涵盖的分布式训练策略和第十三章中讨论的操作挑战的基础。在第十五章中，探讨了分布式加速的安全影响，特别是在模型保护和数据隐私方面。通过理解这一进展，我们可以更好地欣赏人工智能硬件堆栈的每个组件，从计算单元到内存系统，如何适应以支持大规模机器学习工作负载。

人工智能系统的扩展遵循自然进展，从单个封装内的集成开始，通过芯片模块架构，扩展到服务器内的多 GPU 配置，扩展到分布式加速器集群，最终达到晶圆级集成。每种方法都在计算密度、通信开销和系统复杂性之间提供了独特的权衡。例如，芯片模块架构在封装内保持高速互连，而分布式系统为了巨大的并行性而牺牲了通信延迟。

理解这些扩展策略对于多个原因至关重要。首先，它提供了洞察不同硬件架构如何应对人工智能工作负载不断增长的计算需求。其次，它揭示了在扩展到单芯片执行之外时出现的根本挑战，例如管理芯片间通信和协调分布式计算。最后，它为后续讨论映射策略、编译技术和运行时系统如何演变以支持大规模高效执行奠定了基础。

进展始于芯片模块架构，这代表了多芯片扩展最紧密集成的形式。

### 基于芯片模块的架构

芯片模块 31 架构通过将大型设计划分为更小、模块化的晶圆，并在单个封装内互连来实现扩展，如图 11.9 图所示。

![图片](img/file188.png)

图 11.9：**芯片模块互连**：现代人工智能加速器将大型设计划分为更小的芯片模块，并通过高带宽互连连接它们，从而超越单片晶圆的限制，提高制造良率。HBM 堆栈提供对数据的快速访问，这对于机器学习中常见的内存密集型工作负载至关重要。

现代 AI 加速器，如 AMD 的 Instinct MI300，通过将多个计算芯片和内存芯片集成在一起，并通过高速晶圆到晶圆互连连接，采用这种方法。这种模块化设计允许制造商绕过单片芯片的制造限制，同时仍然实现高密度计算。

然而，即使在单个封装内，扩展也并非没有挑战。随着更多芯片的集成，芯片间通信延迟、内存一致性 32 和热管理成为关键因素。与传统多芯片系统不同，基于芯片的设计必须仔细平衡多个晶圆上的延迟敏感型工作负载，同时避免引入过多的瓶颈。

### 多 GPU 系统

除了基于芯片的设计之外，AI 工作负载通常需要多个离散 GPU 协同工作。在多 GPU 系统中，每个加速器都有自己的专用内存和计算资源，但它们必须有效地共享数据和同步执行。

一个常见的例子是 NVIDIA DGX 系统，它通过 NVLink 或 PCIe 连接多个 GPU。这种架构使得工作负载可以在 GPU 之间分割，通常使用数据并行（每个 GPU 处理不同的数据批次）或模型并行（不同的 GPU 处理神经网络的不同部分）(Ben-Nun 和 Hoefler 2019)。这些并行化策略在第八章中进行了深入探讨。

如图 11.10 所示，NVSwitch 互连使得 GPU 之间能够实现高速通信，从而减少了分布式训练中的瓶颈。然而，增加 GPU 数量会引入基本的分布式协调挑战，这些挑战成为主导性能约束的因素。变换器训练的算术强度（0.5-2 FLOPS/byte）迫使 GPU 之间频繁同步梯度，在 GPT-3 规模模型中，AllReduce 操作必须聚合 1750 亿个参数。NVSwitch 提供 600 GB/s 的双向带宽，但当 8 个 H100 GPU 同时交换梯度时，即使这个大量的互连也变成了带宽限制，产生了 4.8 TB/s 的总需求，超过了可用容量。协调复杂性呈指数级增长——当两个 GPU 只需要一个通信通道时，八个 GPU 需要 28 个互连路径，而容错要求则规定冗余的通信模式。内存一致性协议进一步复杂化了协调，因为不同的 GPU 可能在不同的时间观察到权重更新，需要复杂的同步原语，这可能会在每个训练步骤中增加 10-50μs 的延迟——看似微小的延迟，但在百万次迭代运行中会累积成数小时的训练时间。

![图](img/file189.svg)

图 11.10：**多 GPU 扩展**：NVSwitch 互连使得 GPU 之间能够实现高带宽、低延迟的通信，克服了 PCIe 瓶颈，从而支持大型模型的分布式训练。增加 GPU 数量会引入保持内存一致性和在互连设备间高效调度工作负载的挑战。

#### 通信开销和 Amdahl 定律分析

分布式 AI 训练的基本限制源于 Amdahl 定律，该定律量化了通信开销如何限制并行速度提升，无论可用的计算能力如何。对于分布式神经网络训练，梯度同步期间的通信开销创建了一个顺序瓶颈，即使在无限并行的情况下也限制了可扩展性。

分布式训练所能达到的最大加速受 Amdahl 定律的限制：<semantics><mrow><mtext mathvariant="normal">加速</mtext><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mi>P</mi><mi>N</mi></mfrac></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\text{加速} = \frac{1}{(1-P) + \frac{P}{N}}</annotation></semantics> 其中 <semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics> 是可以并行化的工作比例，<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics> 是处理器的数量。然而，对于 AI 训练，通信开销引入了额外的顺序时间：<semantics><mrow><msub><mtext mathvariant="normal">加速</mtext><mtext mathvariant="normal">AI</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mi>P</mi><mi>N</mi></mfrac><mo>+</mo><mfrac><mi>C</mi><mi>N</mi></mfrac></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\text{加速}_{\text{AI}} = \frac{1}{(1-P) + \frac{P}{N} + \frac{C}{N}}</annotation></semantics> 其中 <semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics> 代表通信开销比例。

以训练一个 175 B 参数模型并使用 1000 个 H100 GPU 作为具体例子：

+   **每次迭代的计算时间**：前向/反向传递 100 ms

+   **通信时间**：在 1000 个 GPU 上对 175 B 参数（FP32 中的 700 GB）进行 AllReduce 操作

+   **可用带宽**：每个 NVSwitch 链路 600 GB/s

+   **通信开销**：<semantics><mrow><mfrac><mrow><mn>700</mn><mtext mathvariant="normal">GB</mtext></mrow><mrow><mn>600</mn><mtext mathvariant="normal">GB/s</mtext></mrow></mfrac><mo>×</mo><msub><mo>log</mo><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mn>1000</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mn>11.6</mn><mtext mathvariant="normal">ms</mtext></mrow><annotation encoding="application/x-tex">\frac{700\text{GB}}{600\text{GB/s}} \times \log_2(1000) \approx 11.6\text{ms}</annotation></semantics>

即使只有 5%的训练需要通信（P = 0.95），最大加速比是：<semantics><mrow><mtext mathvariant="normal">加速比</mtext><mo>=</mo><mfrac><mn>1</mn><mrow><mn>0.05</mn><mo>+</mo><mfrac><mn>0.95</mn><mn>1000</mn></mfrac><mo>+</mo><mfrac><mn>0.116</mn><mn>100</mn></mfrac></mrow></mfrac><mo>≈</mo><mn>8.3</mn><mtext mathvariant="normal">倍</mtext></mrow> <annotation encoding="application/x-tex">\text{加速比} = \frac{1}{0.05 + \frac{0.95}{1000} + \frac{0.116}{100}} \approx 8.3\text{倍}</annotation></semantics>

这说明了为什么在 100 个以上添加更多 GPU 对于大型模型训练来说，收益是递减的。

通信需求随着模型大小呈超线性增长，与参数数量呈线性增长。现代转换器模型在每个训练步骤中需要同步所有参数的梯度：

+   **GPT-3（175 B 参数）**：每步 700 GB 的梯度交换

+   **GPT-4（估计 1.8 T 参数）**：每步约 7 TB 的梯度交换

+   **未来 10 T 参数模型**：每步约 40 TB 的梯度交换

即使拥有像 NVLink 4.0（1.8 TB/s）这样的高级互连，对于 10 T 参数模型的梯度同步也需要每步训练 22+秒，这使得没有像梯度压缩或异步更新这样的算法创新，分布式训练变得不切实际。

多 GPU 系统面临来自内存带宽竞争的额外瓶颈。当 8 个 H100 GPU 在梯度计算期间同时访问 HBM 时，由于内存控制器竞争和 NUMA 效应，每个 GPU 的有效内存带宽从 3.35 TB/s 下降到大约 2.1 TB/s。这种 37%的内存性能下降加剧了通信开销，进一步限制了可扩展性。

理解 Amdahl 定律指导优化策略：

1.  **梯度压缩**：通过稀疏化和量化将通信量减少 10-100<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>

1.  **流水线并行性**：通过重叠通信和计算来隐藏梯度同步延迟

1.  **模型并行性**：将模型分区到设备上以减少梯度同步需求

1.  **异步更新**：放宽一致性要求以消除同步障碍

这些技术修改了 Amdahl 方程中<semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics>和<semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>的有效值，以算法复杂性的代价实现了更好的扩展行为。

### TPU Pods

随着模型和数据集的不断扩展，训练和推理工作负载必须超越单服务器配置。这种扩展需求导致了复杂分布式系统的发展，其中多个加速器通过网络进行通信。谷歌的 TPU Pods 代表了应对这一挑战的先驱方法，将数百个 TPU 互联以作为一个统一系统(Norman P. Jouppi 等人 2020)。

TPU Pods 的架构设计与传统多 GPU 系统有根本性的不同。虽然多 GPU 配置通常依赖于单台机器内的 NVLink 或 PCIe 连接，但 TPU Pods 使用高带宽的光纤链路在数据中心规模上互联加速器。这种设计实现了 2D 环面互连拓扑，使得加速器之间能够高效交换数据，同时最小化通信瓶颈，当工作负载跨节点扩展时。

该架构的有效性体现在其性能扩展能力上。如图 图 11.11 所示，当运行 ResNet-50 时，TPU Pod 的性能在从四分之一 Pod 配置到全 Pod 配置的过程中表现出近乎线性的扩展。当扩展到 1024 个芯片时，与 16-TPU 基线相比，系统实现了显著的 33.0<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>速度提升。这种扩展效率在较大配置中尤其值得注意，即使系统从 128 个芯片扩展到 1024 个芯片，性能仍然能够持续强劲地扩展。

![图片](img/file190.png)

图 11.11：**TPU Pods 的扩展效率**：在 Pod 内增加 TPU 芯片的数量，在 ResNet-50 上保持近乎线性的性能提升，从 16 个芯片到 1024 个芯片实现了 33.0<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的速度提升。这种高效的扩展提供了 2D 环面互连和高带宽光纤链路在最小化通信瓶颈方面的有效性，当工作负载在多个加速器之间扩展时。

然而，在整个数据中心内部分布 AI 工作负载引入了分布式协调挑战，这些挑战与单节点系统根本不同。2D 环面互连虽然提供了高分割带宽，但在训练需要跨所有 1,024 个 TPU 进行 AllReduce 操作的大型变压器模型时，会创建通信瓶颈。每个参数梯度必须通过环面网络进行多次跳跃，最坏情况下，需要 32 次跳跃才能在遥远的 TPU 之间进行通信，这会随着模型大小的增加而累积延迟惩罚。

分布式内存架构加剧了协调复杂性——与具有共享主机内存的多 GPU 系统不同，每个 TPU 节点维护独立的内存空间，迫使显式数据打包和同步协议。网络分区容错变得至关重要，因为光链路故障可以将 Pod 分割成孤立的岛屿，需要复杂的共识算法来保持训练一致性。

协调的能量成本也急剧增加：在 Pod 的光互连中移动数据比在单个 TPU 内部的片上通信消耗 1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>更多的能量，将分布式训练转化为计算并行性和通信效率之间的谨慎平衡，其中 AllReduce 带宽，而不是计算能力，决定了整体训练吞吐量。

### 晶圆级 AI

在人工智能扩展的前沿，晶圆级 33 集成代表了范式转变——放弃传统的多芯片架构，转而采用单一、庞大的 AI 处理器。这种方法不是将计算分配到离散的芯片上，而是将整个硅晶圆视为统一的计算布线，消除了芯片间通信的低效性。

如图 11.12 所示，Cerebras 的晶圆级引擎(WSE)处理器打破了 CPU、GPU 和 TPU 历史上晶体管缩放的趋势。虽然这些架构一直在指数轨迹上稳步增加晶体管数量，但 WSE 引入了一种全新的缩放范式，将数万亿个晶体管集成到单个晶圆上——远远超过了最先进的 GPU 和 TPU。随着 WSE-3 的推出，这一轨迹继续推进，将晶圆级 AI 推向前所未有的水平(Systems 2021a)。

![图片](img/file191.png)

图 11.12：**晶圆级集成**：晶圆级 AI 处理器将数万亿个晶体管集成到单个晶圆上，提供超快的片上通信，以超越传统的多芯片架构并实现前所未有的性能水平。

晶圆级 AI 的基本优势是其超快的芯片内通信。与芯片、GPU 或 TPU Pod 不同，在这些设备中数据必须穿越不同设备之间的物理边界，晶圆级 AI 能够在其庞大的计算阵列中实现近乎瞬间的数据传输。这种架构极大地降低了通信延迟，解锁了传统多芯片系统无法达到的性能水平。

然而，实现这种程度的集成引入了巨大的工程挑战。当制造这种规模的处理器时，热量散失、容错性和制造良率成为主要约束。这些可持续性挑战，包括能源消耗和资源利用，在第十八章中进行了探讨。与通过动态重新路由工作负载来减轻故障的分布式 TPU 系统不同，晶圆级 AI 必须内置冗余机制以容忍硅片上的局部缺陷。成功解决这些挑战对于实现晶圆级计算作为 AI 加速下一个前沿的潜力至关重要。

### AI 系统扩展轨迹

表 11.20 展示了 AI 加速的渐进式扩展，从单芯片处理器到越来越复杂的架构，如基于 Chiplet 的设计、多 GPU 系统、TPU Pod 和晶圆级 AI。这一演变过程中的每一步都引入了与数据移动、内存访问、互连效率和负载分配相关的新挑战。虽然 Chiplet 允许封装内的模块化扩展，但它们引入了延迟和内存一致性问题。多 GPU 系统依赖于高速互连如 NVLink，但面临同步和通信瓶颈。TPU Pod 通过在集群间分配工作负载进一步推动可扩展性，但它们必须应对互连拥塞和工作负载分区。在极端情况下，晶圆级 AI 将整个晶圆集成到一个单一的计算单元中，在热管理和容错性方面提出了独特的挑战。

表 11.20：**AI 加速趋势**：扩展 AI 系统在数据移动和内存访问方面提供了越来越多的挑战，推动了从 Chiplet 到晶圆级集成的架构创新。每种方法都引入了模块化、延迟和复杂性之间的独特权衡，需要仔细考虑互连效率和负载分配。

| **扩展方法** | **关键特性** | **挑战** |
| --- | --- | --- |
| **Chiplet** | 封装内的模块化扩展 | 芯片间延迟，内存一致性 |
| **多 GPU** | 外部 GPU 互连（NVLink） | 同步开销，通信瓶颈 |
| **TPU Pod** | 分布式加速器集群 | 互连拥塞，工作负载分区 |
| **晶圆级 AI** | 整个晶圆作为一个单一处理器 | 热量散失，容错性 |

### 计算和内存扩展变化

随着 AI 系统从单芯片加速器扩展到多芯片架构，计算和内存的基本挑战也在演变。在单个加速器中，执行主要优化局部性——确保计算被高效地映射到可用的处理元素，同时最小化内存访问延迟。然而，当 AI 系统扩展到单个芯片之外时，这些优化的范围显著扩大。计算现在必须分布到多个加速器，内存访问模式受到互连带宽和通信开销的限制。

#### 多芯片执行映射

在单芯片 AI 加速器中，计算放置涉及将工作负载映射到 PE（处理元素）、向量单元和张量核心。映射策略旨在最大化数据局部性，确保计算访问附近的内存以减少昂贵的内存移动。

随着 AI 系统扩展到多芯片执行，计算放置必须考虑几个关键因素。工作负载需要在多个加速器之间分区，这需要显式协调执行顺序和依赖关系。这种划分对于与跨芯片通信相关的固有延迟至关重要，这与受益于共享片上内存的单芯片系统形成鲜明对比。因此，计算调度必须考虑互连以有效管理这些延迟。此外，在加速器之间实现负载平衡至关重要；任务的不均匀分布可能导致某些加速器利用率不足，而其他加速器则处于满负荷运行，最终阻碍整体系统性能。

例如，在多 GPU 训练中，计算映射必须确保每个 GPU 都有平衡的工作负载部分，同时最小化昂贵的跨 GPU 通信。同样，在 TPU Pods 中，映射策略必须与环状互连拓扑相一致，确保计算放置以最小化长距离数据传输。

因此，虽然单芯片系统中的计算放置是一个局部优化问题，但在多芯片架构中，它变成了一个全局优化挑战，其中执行效率取决于最小化芯片间通信和平衡工作负载分配。

#### 分布式访问内存分配

单芯片 AI 加速器中的内存分配策略旨在通过使用片上缓存、SRAM 和 HBM 来最小化片外内存访问。如分块、数据重用和内核融合等技术确保计算有效地使用快速本地内存。

在多芯片 AI 系统中，每个加速器管理自己的本地内存，这需要在设备之间显式分配模型参数、激活和中间数据。与单芯片执行中数据只获取一次并重复使用不同，多芯片设置需要采取明确策略以最小化冗余数据传输，因为数据必须在加速器之间进行通信。此外，当多个加速器处理重叠数据时，共享数据的同步可能会引入显著的开销，这必须被仔细管理以确保高效执行。

例如，在多 GPU 深度学习中，跨 GPU 的梯度同步是一个内存密集型操作，必须优化以避免网络拥塞（Shallue 等人 2019）。在晶圆级 AI 中，内存分配必须考虑容错和冗余机制，确保晶圆上的缺陷区域不会干扰执行。

因此，虽然单芯片加速器中的内存分配侧重于本地缓存效率，但在多芯片架构中，必须在加速器之间显式协调以平衡内存带宽、最小化冗余传输并减少同步开销。

#### 数据移动限制

在单芯片 AI 加速器中，数据移动优化主要集中在最小化芯片内内存访问延迟。诸如权重稳定性、输入稳定性和瓦片化等技术确保常用数据保持靠近执行单元，从而减少芯片外内存流量。

在多芯片架构中，数据移动已超越仅仅是芯片内部问题，并成为系统级的一个重大瓶颈。扩展引入了几个关键挑战，其中最重要的是芯片间带宽限制；如 PCIe、NVLink 和 TPU 互连等通信链路的速度远低于芯片内内存访问速度。此外，当加速器共享模型参数或中间计算时，由此产生的数据同步开销（包括延迟和竞争），可能会显著阻碍执行。最后，对于需要频繁数据交换的工作负载，如深度学习训练中的梯度更新，优化集体通信对于实现高效系统性能至关重要。

例如，在 TPU Pods 中，收缩执行模型确保数据以结构化模式移动，减少不必要的芯片外传输。在多 GPU 推理中，异步数据获取和计算与通信重叠等技术有助于减轻芯片间延迟。

因此，虽然单芯片系统中的数据移动优化侧重于缓存局部性和瓦片化，但在多芯片架构中，主要挑战是减少芯片间通信开销以最大化效率。

#### 编译器和运行时适应性

随着 AI 加速扩展到单个芯片之外，编译器和运行时必须适应以管理多个加速器之间的计算放置、内存组织和执行调度。局部性、并行性和高效调度的基本原理仍然是基本的，但它们的实现需要分布式执行的新策略。

在扩展 AI 执行规模的主要挑战中，计算放置是一个关键问题。在单芯片加速器中，工作负载被映射到处理元素、向量单元和张量核心，重点在于最小化芯片内部数据移动并最大化并行执行。然而，在多芯片系统中，计算必须进行分层分区，工作负载不仅分布在芯片内部的各个核心之间，还分布在多个加速器之间。编译器通过实现互联感知调度，优化工作负载放置以最小化昂贵的芯片间通信来处理这个问题。

同样，随着扩展超出单个加速器，内存管理也随着扩展而发展。在单芯片系统中，本地缓存、HBM 重用和高效的分块策略确保频繁访问的数据保持靠近计算单元。然而，在多芯片系统中，每个加速器都有自己的独立内存，需要显式的内存分区和协调。编译器优化分布式执行的内存布局，而运行时引入数据预取和缓存机制以减少芯片间内存访问开销。

除了计算和内存之外，在扩展规模时，数据移动成为了一个主要的瓶颈。在单芯片加速器中，高效的芯片内部缓存和最小化的 DRAM 访问确保数据被高效地重用。然而，在多芯片系统中，通信感知执行变得至关重要，需要编译器生成执行计划，使计算与数据传输重叠。运行时处理芯片间同步，确保工作负载不会被等待从远程加速器到达的数据所阻塞。

最后，执行调度必须扩展以实现全局协调。在单芯片 AI 执行中，调度主要关注并行性和最大化加速器内的计算占用率。然而，在多芯片系统中，调度必须在考虑互联带宽和同步延迟的同时平衡加速器之间的工作负载分布。运行时通过实现自适应调度策略来管理这种复杂性，这些策略根据系统状态和网络拥塞动态调整执行计划。

表 11.21 总结了这些关键适应措施，突出了编译器和运行时如何扩展其功能以有效地支持多芯片 AI 执行。

因此，尽管 AI 加速的基本原理保持不变，但编译器和运行时必须扩展其功能以在分布式系统中高效运行。下一节将探讨映射策略如何演变以进一步优化多芯片 AI 执行。

表 11.21：**多芯片适应**：在多个加速器上高效执行 AI 需要协调调整计算放置、内存管理和调度，以平衡工作负载分布并最小化通信开销。编译器和运行时扩展其功能以动态适应系统状态和网络拥塞，从而实现可扩展和性能良好的多芯片 AI 系统。

| **方面** | **单芯片 AI 加速器** | **多芯片 AI 系统及编译器/运行时如何适应** |
| --- | --- | --- |
| **计算放置** | 本地 PE、张量核心、向量单元 | 分层映射、互联感知调度 |
| **内存管理** | 缓存、HBM 重用、本地分块 | 分布式分配、预取、缓存 |
| **数据移动** | 芯片内重用，最小 DRAM 访问 | 通信感知执行，重叠传输 |
| **执行调度** | 并行性，计算占用率 | 全局调度，互联感知平衡 |

### 执行模型适应

当 AI 加速器扩展到单芯片之外时，执行模型必须演变以应对分布式计算、内存分区和芯片间通信引入的复杂性。在单芯片加速器中，执行针对本地处理元素进行优化，采用平衡并行性、局部性和数据重用的调度策略。然而，在多芯片 AI 系统中，现在必须在多个加速器之间协调执行，这引入了新的工作负载调度、内存一致性和互联感知执行挑战。

本节探讨了随着 AI 加速的扩展，执行模型如何变化，重点关注多芯片系统中的调度、内存协调和运行时管理。

#### 跨加速器调度

在单芯片 AI 加速器中，执行调度主要针对优化处理器内部的并行性。这涉及到通过采用旨在增强数据局部性和资源利用率的技巧，确保工作负载有效地映射到张量核心、向量单元和特殊功能单元。例如，静态调度使用预先确定的执行顺序，该顺序经过精心优化以适应局部性和重用，而动态调度则实时适应工作负载需求的变化。此外，流水线执行将计算划分为阶段，通过保持操作的连续流动来最大化硬件利用率。

相比之下，多芯片架构中的调度必须解决芯片间依赖带来的额外挑战。在这样的系统中，工作负载分区涉及将任务分配到各种加速器，以便每个加速器都获得最佳的工作负载份额，同时最大限度地减少由过多通信造成的开销。感知互连的调度对于将执行时间与芯片间带宽约束对齐至关重要，从而防止性能停滞。延迟隐藏技术也发挥着关键作用，因为它们使计算与通信重叠，有效地减少了等待时间。

例如，在多 GPU 推理场景中，执行调度以允许数据与计算同时预取，从而减轻内存停滞。同样，TPU Pods 利用 systolic array 模型将执行调度与数据流紧密耦合，确保每个 TPU 核心在需要时精确地接收到所需数据。因此，虽然单芯片执行调度主要关注最大化内部并行性，但多芯片系统需要更全面的方法，该方法明确管理通信开销并同步加速器之间的工作负载分配。

#### 跨加速器协调

在单芯片 AI 加速器中，内存协调通过复杂的本地缓存策略来管理，这些策略将频繁使用的数据保留在执行单元附近。采用如分块、内核融合和数据重用等技术来减少对较慢的内存层次结构的依赖，从而提高性能并降低延迟。

相比之下，多芯片架构提出了分布式内存协调挑战，这需要更谨慎的管理。在这样的系统中，每个加速器都拥有自己的独立内存，必须通过显式的内存分区来组织，以最小化跨芯片数据访问。此外，确保加速器之间共享数据的一致性和同步对于保持计算正确性至关重要。还必须实施有效的通信机制，以便以限制与同步延迟相关的开销的方式进行数据传输。

例如，在分布式深度学习训练中，模型参数必须使用全归约等方法在多个 GPU 之间同步，这些方法可以在加速器之间聚合梯度同时减少通信延迟。在晶圆级 AI 中，内存协调必须进一步解决容错执行问题，确保损坏区域不会影响整体系统性能。因此，虽然单芯片系统中的内存协调主要关注缓存优化，但多芯片架构需要管理分布式内存访问、同步和通信以实现高效执行。

#### 跨加速器执行管理

单芯片人工智能加速器中的执行由处理工作负载调度、内存分配和硬件执行的 AI 运行时管理。这些运行时在内核级别优化执行，确保计算在可用资源内高效执行。

在多芯片人工智能系统中，运行时必须包含分布式执行编排的策略。这种方法确保计算和内存访问在多个加速器之间无缝协调，从而实现硬件资源的有效利用并最小化与数据传输相关的瓶颈。

这些系统需要强大的跨芯片工作负载同步机制。仔细管理依赖关系和加速器之间的及时协调对于防止由于芯片间通信延迟而可能出现的执行停滞至关重要。这种同步对于保持计算流程至关重要，尤其是在延迟可能显著影响整体性能的环境中。

最后，自适应执行模型在当代多芯片架构中扮演着关键角色。这些模型根据当前硬件可用性和通信约束动态调整执行计划，确保系统能够响应变化条件并在实时中优化性能。这些策略共同提供了一个有弹性的框架，用于管理分布式人工智能执行中的复杂性。

例如，在谷歌的 TPU Pods 中，TPU 运行时负责在多个 TPU 核心之间调度计算，确保工作负载以最小化通信瓶颈的方式执行。在 PyTorch 和 TensorFlow 等多 GPU 框架中，运行时执行必须同步 GPU 之间的操作，确保数据高效传输的同时保持执行顺序。

因此，虽然单芯片运行时专注于优化单个处理器内的执行，但多芯片运行时必须处理整个系统的执行，平衡计算、内存和互连性能。

#### 计算放置自适应

随着人工智能系统扩展到单芯片执行之外，计算放置必须适应考虑芯片间工作负载分布和互连效率。在单芯片加速器中，编译器通过将工作负载映射到张量核心、向量单元和 PE 来优化放置，确保最大并行性同时最小化芯片内数据移动。然而，在多芯片系统中，放置策略必须解决互连带宽约束、同步延迟以及多个加速器之间的分层工作负载分区问题。

表 11.22 突出了这些适应措施。为了减少昂贵的跨芯片通信，编译器现在实现感知互联的工作负载分区，根据通信成本战略性地将计算分配给加速器。例如，在多 GPU 训练中，编译器优化放置以最小化 NVLink 或 PCIe 流量，而 TPU Pods 利用环状互联拓扑结构来增强数据交换。

表 11.22：**计算放置策略**：多芯片 AI 系统需要分层工作负载映射以最小化通信开销；编译器通过考虑互联带宽和延迟，在分配计算到加速器时采用单芯片优化技术。此表对比了单芯片系统中的计算放置（本地到处理单元）与多芯片系统，其中放置策略优先考虑加速器之间高效的数据交换。

| **方面** | **单芯片 AI 加速器** | **多芯片 AI 系统及编译器/运行时的适应方式** |
| --- | --- | --- |
| **计算放置** | 本地 PE、张量核心、向量单元 | 分层映射，感知互联的调度 |
| **工作负载分布** | 单芯片内优化 | 横跨加速器分区，最小化芯片间通信 |
| **同步** | 在本地执行单元内管理 | 运行时动态平衡工作负载，调整执行计划 |

运行时通过动态管理执行工作负载，实时调整放置策略以平衡加速器之间的负载来补充这一点。与假设固定硬件拓扑的静态编译不同，AI 运行时持续监控系统条件，并根据需要迁移任务以防止瓶颈。这确保了即使在工作负载需求波动或硬件可用性变化的环境中也能高效执行。

因此，大规模计算放置建立在本地执行优化之上，同时引入了芯片间协调、通信感知执行和动态负载平衡的新挑战——这些挑战扩展到内存层次结构如何适应以支持多芯片架构上的高效执行。

### 探索多芯片 AI 的复杂性

从单芯片加速器到多芯片系统再到晶圆级集成，AI 硬件的演变突显了高效执行大规模机器学习工作负载的复杂性不断增加。扩展 AI 系统引入了计算放置、内存管理和数据移动的新挑战。虽然 AI 加速的基本原理保持一致，但它们的实现必须适应分布式执行、互联带宽限制和同步开销的限制。

多芯片人工智能架构在解决现代机器学习模型的计算需求方面迈出了重要一步。通过在多个加速器之间分配工作负载，这些系统提供了更高的性能、更大的内存容量和可扩展性。然而，实现这些好处需要仔细考虑如何将计算映射到硬件，如何划分和访问内存，以及如何在分布式系统中调度执行。

尽管我们已经概述了多芯片人工智能加速在超越单一系统时的关键概念和挑战，但仍有许多内容值得进一步探索。随着人工智能模型在规模和复杂性上的持续增长，需要新的架构创新、映射策略和运行时优化来维持高效的执行。这些新兴趋势和未来方向在该领域正迅速演变。人工智能硬件和软件的持续发展反映了计算领域的一个更广泛趋势，即针对新兴工作负载的独特需求，专业化和特定领域架构正变得越来越重要。

理解多芯片人工智能加速所涉及的原则和权衡，使机器学习工程师和系统设计师能够就如何最佳部署和优化他们的模型做出明智的决定。无论是在大规模语言模型 TPU 集群上训练还是将计算机视觉应用部署在多 GPU 系统上，将计算高效映射到硬件的能力将继续是实现人工智能全部潜力的关键因素。

## 异构 SoC 人工智能加速

在前几节中考察的多芯片架构主要关注最大化数据中心工作负载的计算吞吐量，其中电源预算扩展到千瓦级别，冷却基础设施支持机架级部署。然而，在移动和边缘环境中部署人工智能系统时，建立的硬件加速原则——专用计算单元、内存层次优化和工作负载映射策略——必须做出巨大调整。智能手机在 2 到 5 瓦特的电源预算内运行，自动驾驶汽车需要确定性的实时保证，而物联网传感器必须在电池供电下工作数年。这些限制需要异构系统级芯片（SoC）架构，在单个芯片内协调多个专用处理器，同时满足与数据中心部署根本不同的严格电源、热和延迟要求。

移动人工智能革命从根本上改变了我们对人工智能加速的看法，从同构数据中心架构转向异构片上系统（SoC）设计，这种设计协调多个专用处理器。现代智能手机、汽车系统和物联网设备在单个芯片内集成了 CPU 核心、GPU 着色器、数字信号处理器（DSP）和专用神经网络处理器（NPU），需要复杂的编排才能在严格的功率和热约束下实现最佳性能。

### 移动 SoC 架构演变

高通 Snapdragon AI Engine 是移动人工智能异构计算的典范，它在一个共享的内存层次结构中协调 Kryo CPU 核心、Adreno GPU、Hexagon DSP 和专用 NPU34。Snapdragon 8 Gen 3 通过智能的工作负载分配实现了 73 TOPS，计算机视觉内核在 GPU 的并行着色器上执行，音频处理利用 DSP 的专用算术单元，而变压器注意力机制利用 NPU 优化的矩阵引擎。这种协调需要毫秒级的调度精度以满足实时约束，同时管理热管理限制和电池寿命优化。

虽然高通的方法强调处理器多样化，但苹果的垂直整合策略展示了紧密的软硬件协同设计如何实现更复杂的异构执行。M2 芯片的 16 核心神经网络引擎（15.8 TOPS）通过统一的内存架构与 10 核心 GPU 和 8 核心 CPU 协调，消除了数据复制的开销。神经网络引擎的专用矩阵乘法单元处理变压器层，GPU 的 Metal 性能着色器加速卷积操作，而 CPU 管理控制流和动态层选择。这种精细的协调实现了实时语言翻译和设备上图像生成，同时保持毫秒级响应时间。

除了高通和苹果的垂直整合解决方案之外，ARM 的知识产权许可模式提供了一种根本不同的方法，使 SoC 设计者能够根据目标应用定制处理器组合。Mali-G78 GPU 的 24 个核心可以与 Ethos-N78 NPU 配对，以实现平衡的通用和人工智能加速，而 Cortex-M55 微控制器则集成了 Ethos-U55 微 NPU，用于超低功耗的边缘应用。这种模块化灵活性允许汽车 SoC 强调确定性实时处理，而智能手机 SoC 则优化交互性能和电池效率。

### 动态工作负载分配策略

在异构 SoC 上可用的多个专用处理器中，关键挑战成为智能地在这些资源之间分配神经网络操作，以最大化性能同时尊重功率和延迟约束。

现代神经网络需要根据操作特性和当前系统状态在异构处理器之间进行智能分区。具有常规数据访问模式的卷积层通常在 GPU 着色器核心上高效执行，而具有不规则稀疏模式的完全连接层可能在具有大缓存的通用 CPU 核心上表现更好。在序列较长时，变压器中的注意力机制受益于 NPU 矩阵引擎，但当序列长度较小时，由于 NPU 设置开销，它们可能在 CPU 上执行得更高效。

不同于静态的操作到处理器的映射，异构 SoC 根据多个约束条件实现动态处理器选择：

+   **功率预算**：在电池操作期间，系统可能会将计算路由到低功耗 DSP 核心，而不是高性能 GPU 核心

+   **热状态**：当接近热限制时，工作负载会从功耗较高的 NPU 转移到更高效的 CPU 执行

+   **延迟要求**：对安全至关重要的汽车应用优先考虑确定性的 CPU 执行，而不是可能更快但可变的 NPU 处理

+   **并发工作负载干扰**：多个 AI 应用可能需要在可用的处理器之间进行负载平衡，以保持服务质量

在处理器选择挑战的基础上，共享内存架构在多个处理器同时访问 LPDDR 时需要复杂的仲裁。Snapdragon 8 Gen 3 的内存控制器实现了基于优先级的调度，其中相机处理获得比后台 AI 任务更高的优先级，确保实时视频处理，同时后台神经网络调整它们的执行模式以适应可用的内存带宽。这种仲裁在内存密集型操作（如大型语言模型推理）期间变得至关重要，在这些操作中，从 DRAM 流出的参数必须在处理器之间仔细协调。

### 功率和热管理

移动 AI 工作负载必须在严格的功率预算和热 envelopes 内运行，同时保持高性能——这些约束需要在异构处理器之间进行复杂的协调。

异构 SoC 在多个处理器之间实现协调的动态电压频率调整（DVFS），以优化功率-性能包络。当一个处理器增加频率以满足延迟需求时，系统可能会降低其他处理器的电压以维持总功率预算。在 AI 工作负载中，计算阶段可能在处理器之间快速切换，这种协调变得复杂——系统必须预测即将到来的工作负载转换，以便预先调整工作点，同时避免电压/频率振荡，这些振荡会降低效率。

当仅 DVFS 无法维持功率包络时，移动 SoC 通过智能任务迁移而不是简单的频率降低来实现热管理。当 NPU 在密集的神经网络处理期间接近热极限时，运行时系统可以将层迁移到 GPU 或 CPU，同时保持计算吞吐量。这种方法在热事件期间保持性能，尽管它需要复杂的任务特征化来预测不同处理器的执行时间和功耗。

除了实时电源和热管理之外，移动 AI 系统还必须根据电池状态和充电状态调整其计算策略。在低电量条件下，系统可能从高精度模型切换到高效近似，将工作负载从能耗高的 NPU 迁移到节能的 DSP，或降低推理频率同时保持应用程序响应性。相反，在充电期间，系统可以启用更高性能的模型并增加处理频率，以提供增强的用户体验。

### 汽车异构 AI 系统

汽车应用引入了独特的异构计算挑战，这些挑战结合了移动风格的电源效率、硬实时保证和功能安全要求——这种组合需要根本不同的架构方法。

汽车 SoC 必须保证安全关键功能的确定性推理延迟，同时支持高级驾驶辅助系统（ADAS）。Snapdragon Ride 平台协调跨安全域的多个 AI 加速器——冗余处理元素确保功能安全合规性，而高性能加速器处理感知、规划和控制算法。这种架构需要在安全关键功能和便利功能之间实现时间隔离，通过硬件分区和时间触发的调度来实现。

当考虑到现代车辆集成多个 AI 启用 SoC 以处理不同领域时，这些安全要求变得更加复杂——视觉处理 SoC 处理基于摄像头的感知，雷达处理 SoC 管理射频传感器数据，而中央计算平台协调高级决策。这些分布式系统必须在传感器模态之间保持时间一致性，具有微秒级精度的定时，需要专门的跨 SoC 通信协议和分布式同步机制。

车辆到一切（V2X）通信超越了车辆内部传感器的范围，它增加了一层异构处理层，其中 AI 算法必须协调本地传感器处理与从其他车辆和基础设施接收到的信息。这需要超低延迟的处理链，其中 5G 调制解调器、AI 加速器和控制系统在毫秒级截止时间内运行，同时保持功能安全要求。

### 软件堆栈挑战

异构 SoC 的架构复杂性为软件开发带来了重大挑战，这些挑战跨越编程模型、内存管理和运行时优化。

编程异构 SoC 需要框架来抽象处理器差异，同时暴露性能关键优化机会。OpenCL 和 Vulkan 提供跨处理器执行，但要实现最佳性能，需要针对特定处理器的优化，这会复杂化可移植开发。现代 ML 框架如 TensorFlow Lite 和 PyTorch Mobile 实现了自动处理器选择，但开发者仍然需要理解异构执行模式以实现最佳结果。

进一步复杂化编程挑战，具有共享内存架构的异构 SoC 需要考虑处理器特定缓存行为、内存访问模式和一致性要求的复杂内存管理。CPU 缓存可能会干扰 GPU 内存访问模式，而 NPU 直接内存访问（DMA）操作必须与 CPU 缓存操作同步，以保持数据一致性。

为了解决这些维度上手动优化的复杂性，高级异构 SoC 实现了基于机器学习的运行时优化，它从执行模式中学习以改进处理器选择、热管理和电源优化。这些系统收集关于工作负载特性、处理器利用率和功耗的遥测数据，以构建预测新工作负载最佳执行策略的模型。

这种异构 AI 加速方法代表了计算的未来，其中没有单个处理器架构可以最优地处理现代 AI 应用中的各种计算模式。理解这些协调挑战对于开发高效移动 AI 系统至关重要，这些系统能够提供高性能，同时满足边缘部署场景严格的电源、热和实时约束。

然而，这些异构系统的复杂性为误解和次优设计决策创造了众多机会。以下谬误和陷阱突出了可能破坏加速策略的常见误解。

## 谬误和陷阱

硬件加速涉及专用架构、软件堆栈和工作负载特性之间的复杂交互，这为误解最佳部署策略创造了重大机会。与 AI 加速器相关联的令人印象深刻的性能数字往往掩盖了决定不同部署场景中实际效果的重要约束和权衡。

**谬误：** *更专业的硬件总是比通用替代品提供更好的性能。*

这种信念假设专用加速器自动优于通用处理器处理所有 AI 工作负载。专用硬件仅在工作负载与架构假设和优化目标匹配时才能达到峰值性能。具有不规则内存访问模式、小批量大小或动态计算图的模型，在灵活的通用处理器上可能比为密集、规则计算设计的专用加速器表现更好。数据移动、格式转换和同步的开销可能会消除专用计算的好处。有效的硬件选择需要将工作负载特征与架构优势相匹配，而不是假设专业化总是获胜。

**陷阱：** *在选择加速策略时忽略内存带宽限制。*

许多从业者关注计算吞吐量指标，而没有考虑到内存带宽约束，这些约束通常限制了实际性能。具有令人印象深刻的计算能力的 AI 加速器可能会因为内存带宽不足而严重瓶颈，导致硬件利用率低下。计算强度与内存访问需求之间的比率决定了加速器能否达到其理论性能。这种疏忽导致昂贵的硬件部署未能实现预期的性能提升，因为工作负载是内存受限而不是计算受限。

**谬误：** *硬件加速的好处与额外加速器的数量成线性增长。*

这种误解驱使团队期望在系统中添加更多加速器时能够获得成比例的性能提升。多加速器配置引入了通信开销、同步成本和负载平衡挑战，这些都可能严重限制扩展效率。小型模型可能无法提供足够的并行工作来有效利用多个加速器，而大型模型可能受到设备间通信带宽的限制。分布式训练和推理面临梯度聚合、模型分区和协调开销等额外挑战，这些挑战创造了非线性扩展关系。

**陷阱：** *不考虑长期可移植性和灵活性而进行的供应商特定优化。*

组织通常仅针对特定的硬件供应商进行优化，以实现最大性能，而不考虑系统灵活性和未来迁移的影响。与特定供应商库、定制内核和专有优化工具的深度集成，创造了锁定效应，使得硬件升级、供应商更换或多供应商部署变得复杂。虽然特定供应商的优化可以提供显著的性能优势，但它们应该与系统可移植性和适应不断变化的硬件景观的能力相平衡。保持一定程度的硬件抽象可以保持战略灵活性，同时仍然捕捉到大多数性能优势。

## 摘要

硬件加速已成为将机器学习从学术好奇心转变为实际现实的关键推动者，从根本上改变了我们设计计算系统和在其上运行的算法的方式。从通用处理器到专用人工智能加速器的演变，不仅仅是渐进式的改进——它反映了一种范式转变，即向特定领域计算转变，在这种计算中，硬件和软件是协同设计的，以优化特定的计算模式。从 CPU 到 GPU，再到专门的 TPU、NPU 和晶圆级系统的发展历程，展示了理解工作负载特征如何推动架构创新，通过针对性的专业化，创造了数量级性能提升的机会。

人工智能加速的技术挑战跨越了计算堆栈的多个层次，从低级内存层次结构优化到高级编译器转换和运行时编排。内存带宽限制创造了基本瓶颈，需要像数据分块、内核融合和层次感知调度这样的复杂技术来克服。将神经网络计算映射到硬件涉及在不同数据流模式、内存分配策略和执行调度方法之间进行复杂的权衡，这些方法必须在计算效率与资源利用之间取得平衡。

在这些基础概念的基础上，多芯片和分布式加速系统的出现引入了通信开销、内存一致性和工作负载分区等方面的额外复杂性，这些都需要仔细的系统级优化。

**关键要点**

+   专用人工智能加速器通过针对张量操作和数据流模式优化的特定领域架构实现性能提升。

+   内存层次结构管理通常是人工智能加速的主要瓶颈，需要复杂的数据移动优化策略。

+   硬件-软件协同设计通过将算法特征与架构能力对齐，实现了数量级的改进。

+   多芯片扩展引入了分布式计算挑战，需要新的通信、同步和资源管理方法。

这里确立的硬件加速原则为理解基准测试方法如何评估加速器性能以及部署策略如何考虑硬件约束和能力提供了基础。随着人工智能模型在复杂性和计算需求上的持续增长，有效地利用专用硬件的能力对于实际系统部署变得越来越关键，它影响着从能效和成本优化到跨不同应用领域实时推理和大规模训练的可行性等各个方面。

* * *
