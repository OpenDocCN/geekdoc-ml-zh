# 在设备上学习

*DALL·E 3 提示：一部智能手机的内部组件暴露的绘图，展示了不同性别和肤色的微型工程师正在积极工作在机器学习模型上。这些工程师包括男性、女性和非二元个体，他们正在调整参数、修复连接和即时增强网络。数据流入机器学习模型，实时处理，并生成输出推断。*

![图片](img/file219.png)

## 目的

*为什么在设备上学习代表了自训练和推理分离以来机器学习系统中最基本的架构转变，是什么使得这种能力对智能系统的未来至关重要？*

在设备上学习打破了数十年来主导机器学习架构的假设：模型训练和操作之间的分离。这通过在现实世界中实现持续适应而不是静态部署预训练模型来重新定义系统可以成为什么。从集中式训练到分布式、自适应学习的转变将系统从被动的推理引擎转变为能够在断开连接的环境中实现个性化、隐私保护和自主改进的智能代理。随着人工智能系统超越受控数据中心进入不可预测的环境，预训练无法预见每个场景或部署条件，这种架构革命变得至关重要。理解在设备上学习的原则使工程师能够设计突破静态模型限制的系统，在人类交互点创建能够学习和演化的自适应智能。

**学习目标**

+   通过比较计算分布、数据局部性和协调机制来区分在设备上学习与集中式训练方法

+   识别关键的动力驱动因素（个性化、延迟、隐私、基础设施效率）并评估在设备上学习何时比其他方法更合适

+   分析与推理相比，训练如何放大资源限制，量化内存（3-5<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>）、计算（2-3<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>）和能量开销对系统设计的影响

+   通过比较资源消耗、表达性和对不同设备类别的适用性来评估包括权重冻结、残差更新和稀疏更新在内的适应策略

+   检查在有限本地数据集上进行学习的效率技术，包括少样本学习、经验重放和数据压缩方法

+   应用联邦学习协议来协调异构设备群体中的隐私保护模型更新，同时管理通信效率和收敛挑战

+   设计集成热管理、内存层次优化和电源预算的设备学习系统，以保持可接受的用户体验

+   实施解决 MLOps 集成挑战的实用部署策略，包括设备感知管道、分布式监控和异构更新协调

## 分布式学习范式转变

操作框架（第十三章）通过集中编排、监控和部署管道，为大规模管理机器学习系统奠定基础。这些框架假设在受控的云环境中，计算资源丰富，网络连接可靠，系统行为可预测。然而，随着机器学习系统越来越多地超出数据中心，移动到边缘设备，这些基本假设开始崩溃。

一部学习预测用户文本输入的手机、一个适应家庭日常生活的智能家居设备，或一辆根据当地驾驶条件更新感知模型的自动驾驶汽车，都是传统集中式训练方法证明不足的场景。智能手机遇到了全球训练数据中不存在的、属于个别用户的独特语言模式。智能家居设备必须适应季节变化和家庭动态，这些变化在各个家庭之间差异很大。自动驾驶汽车面临着与原始训练环境不同的当地道路条件、天气模式和交通行为。

这些场景是设备学习的例子，其中模型必须在它们运行的设备上直接进行训练和适应 1。这种范式将机器学习从集中学科转变为分布式生态系统，学习发生在数百万种异构设备上，每个设备都处于独特的约束和本地条件下。

向设备学习过渡在机器学习系统设计中引入了基本紧张关系。虽然基于云的架构利用了丰富的计算资源和受控的操作环境，但边缘设备必须在严重受限的资源范围内运行，这些资源包括有限的内存容量、受限的计算吞吐量、受限的能源预算和间歇性的网络连接。这些使设备学习在技术上具有挑战性的约束同时使其最显著的优势得以实现：通过本地数据处理实现个性化适应，通过数据本地化保护隐私，以及通过独立于集中基础设施实现操作自主性。

本章探讨了导航这种架构紧张关系所需的理论基础和实践方法。基于计算效率原则（第九章）和运营框架（第十三章），我们研究了在极端资源约束下实现有效学习的专用算法技术、架构设计模式和系统级原则。挑战不仅超越了传统训练算法的优化，还要求对整个机器学习管道进行重新概念化，以适应传统计算假设失效的部署环境。

**设备端学习**是指在无服务器连接的情况下，直接在部署的硬件上对机器学习模型进行本地训练或适配。这种模式在资源受限的条件下，能够实现*个性化*、*隐私保护*和*自主操作*。

这种范式的影响远远超出了技术优化，挑战了关于机器学习系统开发、部署和维护生命周期的既定假设。模型从遵循可预测的版本模式转变为表现出连续的分歧和适应轨迹。性能评估方法从集中的监控仪表板转变为跨异构用户群体的分布式评估。隐私保护从监管合规的考虑转变为塑造系统设计决策的核心架构要求。

理解这些系统性的影响需要考察推动组织采用设备端学习的强大动机以及必须解决的重大技术挑战。这种分析建立了构建能够在网络边缘有效学习的同时，在严格约束下运行的系统的理论基础和实践方法。

## 动机与益处

传统的机器学习系统通常依赖于集中的训练流程，其中模型通过使用大型、精心挑选的数据集和强大的云基础设施进行开发和优化（Jeffrey Dean 和 Ghemawat 2008）。一旦训练完成，这些模型就会被部署到客户端设备上进行推理，从而在训练和部署阶段之间形成清晰的分离。虽然这种架构分离在大多数用例中都表现良好，但在现代应用中，本地数据是动态的、私密的或高度个性化的，它带来了显著的限制。

设备上学习通过允许系统在设备上直接进行训练或自适应，而不依赖于持续连接到云端，挑战了这一既定模型。这种转变不仅仅代表技术进步，它反映了不断变化的应用需求和用户期望，这些需求和期望要求响应性、个性化且保护隐私的人工智能系统。

考虑一个智能手机键盘适应用户的独特词汇和打字模式。为了个性化预测，系统必须在本地观察到的文本输入上对紧凑的语言模型执行梯度更新。即使是对于最小的语言模型，单个梯度更新也需要 50-100 MB 的内存用于激活和优化器状态。现代智能手机通常为键盘等后台应用程序分配 200-300 MB（根据操作系统和设备代系而异）。这个极薄的内存边界，其中单个训练步骤消耗了可用内存的 25%，体现了设备上学习的核心工程挑战。系统必须在如此严重的限制下实现有意义的个性化，而传统的训练方法在架构上变得不可行。这种定量现实推动了需要专门技术，这些技术使得在极端资源限制下实现自适应成为可能。

### 设备上学习的好处

理解设备上学习采用背后的驱动因素需要审视传统集中式方法固有的局限性。传统的机器学习系统依赖于模型训练和推理之间明确的劳动分工。训练在可访问高性能计算资源和大规模数据集的集中环境中进行。一旦训练完成，模型被分发到客户端设备，在那里它们以静态的推理模式运行。

虽然这种集中式范式在许多部署中已被证明是有效的，但它引入了在数据是用户特定、行为是动态的或连接是间歇性的场景中的基本局限性。随着机器学习超越受控环境进入具有多样化用户群体和部署背景的实际情况，这些局限性变得尤为突出。

设备上学习通过允许部署的设备使用本地数据执行模型自适应来克服这些局限性。设备上学习不仅仅是效率优化；它是构建可信赖人工智能系统的基石，开启第四部分：可信赖的系统。通过保持数据本地化，它为隐私提供了强大的基础。通过适应单个用户，它增强了公平性和实用性。通过支持离线操作，它提高了对网络故障和基础设施依赖的鲁棒性。本章探讨了构建这些可信赖、自适应系统所需的工程。

这种从集中式到分布式学习的转变是由四个关键考虑因素驱动的，这些因素反映了技术能力和不断变化的应用需求：个性化、延迟和可用性、隐私和基础设施效率 (T. Li et al. 2020)。

个性化代表了最具吸引力的动机，因为部署的模型经常遇到与它们的训练环境有显著差异的使用模式和数据分布。本地适应允许模型根据用户特定的数据来细化行为，捕捉语言偏好、生理基线、传感器特征或环境条件。这种能力在用户间差异大的应用中证明是至关重要的，在这些应用中，一个全局模型无法有效地服务于所有用户。

延迟和可用性限制为本地学习提供了额外的理由。在边缘计算场景中，连接到集中式基础设施可能不可靠、延迟或有意限制以保留带宽或减少能耗。设备上的学习允许模型在完全离线或对延迟敏感的上下文中自主改进，即使往返云端的更新在架构上不可行。

隐私考虑提供了一个第三大推动力。许多现代应用涉及敏感或受监管的数据，包括生物识别测量、输入文本、位置跟踪或健康信息。本地学习通过在设备上保留原始数据并在隐私保护边界内操作来减轻隐私担忧，可能有助于遵守 GDPR2、HIPAA (Tomes 1996)或特定区域的数据主权法律。

基础设施效率为分布式学习方法提供了经济动机。集中式训练管道需要大量的后端基础设施来收集、存储和处理来自数百万设备的用户数据。通过将学习转移到边缘，系统降低了通信成本，并将训练工作负载分布在部署舰队中，减轻了集中资源的压力，同时提高了可扩展性。

### 替代方法和决策标准

设备上的学习代表了一个重大的工程投资，其固有的复杂性可能无法由其带来的好处来证明。在承诺采用这种方法之前，团队应仔细评估是否更简单的替代方案可以以更低的运营成本实现类似的结果。了解何时不实施设备上的学习与了解其好处一样重要，因为过早采用可能会在不成比例的价值下引入不必要的复杂性。

几种替代方法通常足以满足个性化与适应需求，而不需要本地训练的复杂性：

+   **基于特征的个性化**：通过本地存储用户偏好、交互历史和行为特征来实现有效的定制。系统不是调整模型权重，而是将这些存储的特征输入到静态模型中，以实现个性化。新闻推荐系统通过本地存储用户主题偏好和阅读模式，然后将这些特征与集中式内容模型结合，提供个性化的推荐，而无需更新模型。

+   **基于云的微调与隐私控制**：通过集中式适应实现个性化，同时采用适当的隐私保护措施。用户数据在非高峰时段批量处理，使用差分隐私 3 或联邦分析等隐私保护技术。这种方法通常比资源受限的设备更新具有更高的准确性，同时保持许多应用的可接受的隐私属性。

+   **用户特定的查找表**：结合全局模型与个性化检索机制。系统维护一个轻量级的、针对特定用户的查找表，用于频繁访问的模式，同时使用共享的全局模型进行泛化。这种混合方法提供了个性化优势，同时计算和存储开销最小。

实施设备学习的决策应受可量化的需求驱动，这些需求排除了这些更简单的替代方案。真正的数据隐私限制，法律禁止云处理，真正的网络限制，防止可靠连接，定量延迟预算，禁止云往返，或可证明的性能改进，证明操作复杂性的合理性，这些是设备学习采用的有效驱动因素。

对于具有关键时间要求的（例如，33 ms 以下的相机处理，500 ms 以下的语音响应，20 ms 以下的 AR/VR 运动到光子延迟，或 10 ms 以下的安全关键控制），网络往返时间（通常为 50-200 ms）使得基于云的替代方案在架构上不可行。在这种情况下，无论考虑复杂性的因素如何，设备学习都成为必要。团队在承诺设备学习所需的重大工程投资之前，应彻底评估更简单的解决方案。

这些动机根植于更广泛的知识迁移概念，其中预训练模型将有用的表示迁移到新的任务或领域。这一基本原理使得设备上的学习既可行又有效，允许在最小本地资源下进行复杂的适应。如图 14.1 图 14.1 所示，知识迁移可以发生在紧密相关的任务之间（例如，玩不同的桌面游戏或乐器），或者跨越具有相似结构的领域（例如，从骑自行车到骑滑板）。在设备上学习的背景下，这意味着利用在云端预训练的模型，并仅使用本地数据和有限的更新来高效地将其适应到新的环境中。该图强调了关键思想：预训练的知识允许快速适应，无需从头开始重新学习，即使新任务在输入模式或目标上有所偏离。

![图片](img/file220.png)

图 14.1：知识迁移：预训练模型通过利用现有表示来加速新任务的学习，例如通过在相关桌面游戏或乐器之间调整技能。这种迁移扩展到自行车骑行和滑板操作等跨领域，其中共享的底层结构允许在有限的新数据下进行高效的调整。

这种由迁移学习和适应能力带来的概念转变，使得实际设备上的应用成为可能。无论是调整语言模型以适应个人打字偏好，调整手势识别以适应个人运动模式，还是在不断变化的环境中重新校准传感器模型，设备上的学习允许系统在时间上保持响应性、高效性和与用户的对齐。

### 实际应用领域

建立在这些既定动机（个性化、延迟、隐私和基础设施效率）的基础上，实际部署展示了设备上学习在众多应用领域中的实际影响。这些领域包括消费技术、医疗保健、工业系统和嵌入式应用，每个领域都展示了上述好处对于有效机器学习部署成为关键的场景。

移动输入预测代表了在设备上学习的最成熟和最广泛部署的例子。在智能手机键盘等系统中，预测文本和自动纠错功能从持续的本地调整中受益良多。用户的打字模式高度个性化且动态变化，使得集中式的静态模型不足以提供最佳用户体验。设备上的学习允许语言模型直接在设备上微调其预测，实现个性化同时保持数据本地性。

例如，Google 的 Gboard 使用联邦学习来改善大量用户之间的共享模型，同时保持每个设备的原始数据本地化(Hard 等人 2018)4。

如图 14.2 所示，不同的预测策略展示了本地适应在实时操作中的工作方式：下一词预测（NWP）基于先前的文本提出可能的延续，而智能完成则使用即时重新评分提供动态补全，展示了本地推理机制的复杂性。

![图片](img/file221.png)

图 14.2：设备上的预测策略：Gboard 同时使用下一词预测和智能完成以及即时重新评分来适应用户的本地打字模式，增强个性化并保护隐私。这些技术展示了机器学习模型如何在实时中细化预测，而不需要将数据传输到中央服务器，从而实现高效且私密的移动输入体验。

在消费应用的基础上，可穿戴设备和健康监测设备提供了同样有吸引力的用例，并增加了额外的监管约束。这些系统依赖于加速度计、心率传感器和皮肤电活动监测器的实时数据来跟踪用户健康和健身。生理基线在个体之间差异很大，这为静态模型无法有效解决的个性化挑战。设备上的学习允许模型随着时间的推移适应这些个体基线，显著提高活动识别、压力检测和睡眠阶段的准确性，同时满足数据本地化的监管要求。

语音交互技术呈现了另一个重要的应用领域，具有独特的声学挑战。唤醒词检测 5 以及智能音箱和耳机等设备中的语音界面必须快速准确地识别语音命令，即使在嘈杂或动态的声学环境中也是如此。

这些系统面临严格的延迟要求：语音界面必须保持端到端响应时间低于 500 毫秒，以保持自然对话流程，唤醒词检测需要低于 100 毫秒的响应时间，以避免用户感到沮丧。本地训练允许模型适应用户的独特语音特征和不断变化的周围环境，在满足这些性能约束的同时，减少误报和漏检。这种适应在远场音频设置中尤其有价值，因为麦克风配置和房间声学在部署中差异很大。

除了消费类应用之外，工业物联网和远程监控系统展示了在资源受限环境中设备上学习的价值。在农业传感、管道监控或环境监控等应用中，连接到集中式基础设施可能有限、昂贵或完全不可用。设备上的学习允许这些系统检测异常、调整阈值或适应季节性趋势，而无需与云持续通信。这种能力对于保持边缘部署的传感器网络的自主性和可靠性至关重要，因为系统停机或漏检可能产生重大的经济或安全后果。

最具挑战性的应用出现在嵌入式计算机视觉系统中，包括机器人、AR/VR 和智能摄像头等，这些系统将复杂的视觉处理与极端的时序约束相结合。摄像头应用必须在 33 毫秒内处理帧以保持 30 FPS 的实时性能，而 AR/VR 系统则要求运动到光子的延迟低于 20 毫秒，以防止恶心并保持沉浸感。安全关键的控制系统需要更严格的界限，通常低于 10 毫秒，因为延迟的决策可能产生严重的后果。这些系统在新的或快速变化的环境中运行，与它们的原始训练条件有显著差异。设备上的自适应允许模型重新校准以适应新的光照条件、物体外观或运动模式，同时满足这些关键的延迟预算，这些预算从根本上推动了设备上与基于云的处理之间的架构决策。

每个领域都揭示了一个共同的模式：部署环境引入了变化和特定上下文的要求，这些要求在集中式训练期间无法预料。这些应用展示了动机驱动因素（个性化、延迟、隐私和基础设施效率）如何表现为具体的工程约束。移动键盘面临存储用户特定模式的内存限制，可穿戴设备遇到限制训练频率的能量预算，语音界面必须满足低于 100 毫秒的延迟要求，这排除了云协调，而工业物联网系统在网络受限的环境中运行，需要自主适应。这种模式揭示了塑造所有后续技术决策的基本设计要求：学习必须在重大资源约束下高效、私密和可靠地进行，这些约束通过约束分析（第 14.3 节）、自适应技术（第 14.4 节）和联邦协调（第 14.6 节）来考察。

### 建筑权衡：集中式与去中心化训练

这些应用展示了在多个领域内设备端学习的实际价值。在此基础上，我们现在探讨设备端学习与传统机器学习架构的不同之处，揭示了训练生命周期的全面重新构想，这远远超出了简单的部署选择。

要理解设备端学习所代表的转变，需要检查传统机器学习系统的结构以及它们的局限性在哪里变得明显。如今，大多数机器学习系统遵循一个集中式学习范式，这个范式为该领域服务得很好，但随着现代部署要求的增加，它越来越显示出压力。模型在数据中心使用大规模、精心挑选的数据集进行训练，这些数据集来自许多来源。一旦训练完成，这些模型就以静态形式部署到客户端设备上，在那里它们执行推理而无需进一步修改。模型参数的更新，无论是为了整合新数据还是为了提高泛化能力，通常通过离线重新训练来处理，这通常使用从现场收集或标记的数据来完成。

这种既定的集中式模型提供了许多经过验证的优势：高性能计算基础设施、访问多样化的数据分布，以及强大的调试和验证管道。它还依赖于几个在现代部署场景中可能不成立的假设：可靠的数据传输、对数据保管人的信任，以及能够管理跨设备编队的全球更新的基础设施。随着机器学习被部署到越来越多样化、分布式的环境中，这种方法的优势变得更加明显，但同时也往往具有阻碍性。

与这种集中式方法相反，设备端学习拥抱了一种固有的去中心化范式，挑战了许多传统假设。每个设备都维护自己模型的副本，并使用通常无法提供给集中式基础设施的数据在本地对其进行调整。训练在设备上发生，通常是异步的，并在基于设备使用模式、电池水平和热状态变化的资源条件下进行。数据永远不会离开设备，这减少了隐私暴露，但也使得设备间的协调变得更加复杂。设备在硬件能力、运行环境和使用模式上可能存在显著差异，使得学习过程异构且难以标准化。这些硬件变化为系统设计带来了重大挑战。

这种去中心化架构引入了一类新的系统挑战，这些挑战远远超出了传统机器学习关注的范畴。设备可能运行不同版本的模型，导致部署集群中行为的不一致性。评估和验证变得更加复杂，因为没有中央点可以衡量所有设备的性能(McMahan 等人 2017c)。必须谨慎管理模型更新以防止退化，并且在缺乏集中测试和验证基础设施的情况下，安全保证变得更加难以执行。

管理数千个异构边缘设备超出了典型分布式系统的复杂性。设备异质性不仅包括硬件差异，还包括不同的操作系统版本、安全补丁、网络配置和电源管理策略。在任何给定时间，20-40% 的设备处于离线状态(Bonawitz 等人 2019)，而其他设备可能已经断开连接数周或数月，从而造成持续的协调挑战。

当断开连接的设备重新连接时，它们需要状态协调以避免版本冲突。更新验证变得至关重要，因为设备可能静默地未能应用更新或报告成功，同时运行着过时的模型。健壮的系统实施多阶段验证：加密签名确认更新完整性，功能测试验证模型行为，遥测确认部署成功。回滚策略必须处理部分部署的情况，其中一些设备已接收更新，而其他设备仍停留在之前的版本，这需要复杂的编排来在故障恢复期间保持系统一致性。

这些挑战需要与集中式机器学习系统相比不同的系统设计和运营管理方法，基于第十三章中分布式系统原则，同时引入边缘特定的复杂性。

尽管存在这些挑战，去中心化引入了经常足以证明额外复杂性的机会。它允许在没有集中监管的情况下进行深度个性化，支持在断开连接或带宽受限的环境中稳健学习，并降低模型更新的运营成本和基础设施负担。实现这些好处引发了如何有效协调设备间学习的问题，无论是通过定期同步、联邦聚合还是平衡本地和全局目标的混合方法。

从集中式学习到分布式学习的转变不仅仅代表了部署架构的转移。它重塑了机器学习系统的整个设计空间，需要新的方法来处理模型架构、训练算法、数据管理和系统验证。在集中式训练中，数据从许多来源汇总并在大型数据中心进行处理，在那里模型被训练、验证，然后以静态形式部署到边缘设备。相比之下，设备学习引入了一种根本不同的范式：模型直接在客户端设备上使用本地数据更新，通常是非同步的，并在不同的硬件条件下进行。这种架构转型引入了协调挑战，同时实现了自主的本地适应，需要仔细考虑验证、系统可靠性和跨异构设备群体的更新编排。

设备学习是对集中式机器学习工作流程局限性的回应。从集中式到分布式学习的转变创造了三个不同的操作阶段，每个阶段都有不同的特征和挑战。

传统的集中式范式从基于云的聚合数据训练开始，然后是静态模型部署到客户端设备。当数据收集可行、网络连接可靠且单个全局模型可以有效地为所有用户服务时，这种方法效果良好。然而，当数据变得个性化、对隐私敏感或在有限连接的环境中收集时，它就会崩溃。

一旦部署，随着每个设备遇到其独特的数据分布，本地差异开始出现。设备收集的数据反映了个人用户模式、环境条件和使用上下文。这些数据通常是非-IID（非独立同分布）6 且噪声大的，需要本地模型适应以保持性能。这一转变标志着从全局泛化到本地专业化的转变。

最后一个阶段引入了联邦协调，设备通过汇总模型更新而不是原始数据共享来定期同步其本地适应。这既实现了隐私保护的全局优化，又保持了本地个性化的好处。

这三个不同的阶段（集中式训练、本地适应和联邦协调）代表了架构的演变，重塑了机器学习生命周期的各个方面。图 14.3 展示了在这些阶段中数据流、计算分布和协调机制的不同，突出了每个阶段出现的日益复杂的复杂性以及增强的能力。理解这一进展有助于界定设备学习系统必须解决的挑战。

![图片](img/file222.svg)

图 14.3：从集中式云训练（区域 A）通过本地设备适应（区域 B）到联邦协调（区域 C）的演变代表了机器学习架构的根本性转变。每个阶段都引入了独特的操作特性，从统一的全球模型到个性化的本地适应，再到协调的分布式学习。

## 设计约束

第三部分确立了塑造所有机器学习系统的效率原则。第九章介绍了三个效率维度（算法效率、计算效率和数据效率），并通过扩展定律揭示了为什么蛮力方法会遇到基本限制。第十章开发了包括量化、剪枝和知识蒸馏在内的压缩技术，这些技术使得在资源受限的设备上部署成为可能。第十一章描述了从微控制器到移动加速器的边缘硬件能力，如硬件讨论中详细所述。这些章节主要关注推理工作负载：高效地运行预训练模型。

在设备上学习遵循相同的效率约束，但具有针对训练特定的放大，这使得优化变得更加具有挑战性。在推理阶段，网络只需要单次正向传递，而在训练阶段，则需要正向传播、通过反向传播进行梯度计算和权重更新，这增加了 3-5<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的内存需求以及 2-3<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的计算成本。能够实现高效推理的模型压缩技术，从优化变成了基本要求，因为没有激进的压缩，在边缘设备上的训练将是不可能的。

考虑到设备上学习的既定动机，我们现在来探讨塑造其实施的根本性工程挑战。在设备上实现学习需要完全重新思考关于机器学习系统运行位置和方式的传统假设。在集中式环境中，模型是在访问广泛的计算基础设施、大量精选的数据集以及慷慨的内存和能源预算的情况下进行训练的。在边缘，这些假设都不成立，从而创造了一个根本不同的设计空间。

设备上学习约束分为三个关键维度，这些维度与第三部分中的效率框架平行但扩展：模型压缩需求（扩展算法效率）、稀疏和非均匀数据特征（扩展数据效率），以及严重受限的计算资源（扩展计算效率）。这三个维度形成一个相互关联的约束空间，定义了设备上学习系统的可行区域，每个维度都施加独特的限制，影响算法选择、系统架构和部署策略。

### 边缘设备上训练开销的量化

从仅推理部署到设备上训练的转变，产生了乘法而非加法复杂性。这些约束相互作用并相互放大，以重塑系统设计要求，基于第九章的资源优化原则，同时引入了特定于分布式学习环境的新挑战。

第三部分引入的效率约束适用于推理和训练，但训练将每个约束维度放大了 3-10 倍。表 14.1 量化了训练工作负载如何加剧第九章（ch015.xhtml#sec-efficient-ai）、第十章（ch016.xhtml#sec-model-optimizations）和第十一章（ch017.xhtml#sec-ai-acceleration）中确立的挑战。

这些放大解释了为什么仅仅将第三部分优化技术应用于训练工作负载证明是不够的。每个约束类别都会影响设备上学习系统设计，需要基于但超越早期章节中推理方法的方法。

表 14.1：**训练放大推理约束**：设备上学习在相同的效率约束下运行，但具有特定于训练的放大，这使得优化变得极具挑战性。本表量化了从运行预训练模型到本地调整模型时，每个约束维度如何加剧。放大系数假设标准反向传播，不包含梯度检查点等优化。

| **约束维度** | **推理（第三部分）** | **训练放大** | **对设计的影响** |
| --- | --- | --- | --- |
| **内存占用** | 模型权重 + 单个激活图 | 权重 + 完整激活缓存 + 梯度 + 优化器状态 | 增加 3-5 倍；迫使进行激进压缩 |
| **计算操作** | 仅前向传递 | 前向 + 反向 + 权重更新 | 增加 2-3 倍；限制模型复杂性 |
| **内存带宽** | 顺序权重读取 | 梯度双向数据流 | 增加 5-10<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>；造成瓶颈 |
| **每样本能耗** | 单个推理操作 | 多个梯度步骤直至收敛 | 增加 10-50<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>；需要机会性调度 |
| **数据需求** | 预收集、整理的数据集 | 稀疏、噪声、流式本地数据 | 需要样本高效的方法 |
| **硬件利用率** | 优化正向传播 | 反向传播的不同访问模式 | 推理加速器可能对训练无帮助 |

图 14.4 展示了将离线预训练与资源受限的物联网设备上的在线自适应学习相结合的管道。系统首先使用通用数据进行元训练。在部署期间，设备特定的约束（如数据可用性、计算和内存）通过排名和选择要更新的层和通道来塑造适应策略。这允许在有限的资源范围内进行高效的设备学习。

![](img/file223.svg)

图 14.4：资源受限的设备使用两阶段学习过程：离线预训练建立初始模型权重，随后是在线自适应，根据可用的数据、计算和内存选择性地更新层。这种方法在模型性能与边缘部署的实际限制之间取得平衡，使现实世界环境中的持续学习成为可能。

### 模型约束

设备学习约束的第一维度集中在模型本身。其结构、大小和计算需求决定了部署的可行性。机器学习模型的结构和大小直接影响设备上训练是否可行，更不用说是否实用。与可以跨越数十亿参数并依赖于多吉字节内存预算的云部署模型不同，旨在设备上学习的模型必须符合对内存、存储和计算复杂性的严格限制。这些限制不仅适用于推理时间，在训练期间变得更加严格，因为需要额外的资源来进行梯度计算、参数更新和优化器状态管理。

当检查设备谱系中的具体示例时，这些约束的范围变得明显。MobileNetV2 架构，常用于移动视觉任务，在其标准配置下需要大约 14 MB 的存储空间。虽然这种内存需求对于具有数 GB 可用 RAM 的现代智能手机来说是完全可行的，但它远远超过了 Arduino Nano 33 BLE Sense7 等嵌入式微控制器上可用的内存，后者仅提供 256 KB 的 SRAM 和 1 MB 的闪存存储。这种可用资源的巨大差异需要采取激进的模型压缩技术。在这样的严重受限平台上，即使是典型的卷积神经网络的单层，在训练过程中也可能因为需要存储中间特征图和梯度信息而超出可用的 RAM。

除了静态存储需求之外，训练过程本身也会显著扩大有效内存占用，从而增加了一层额外的约束。标准的反向传播需要在正向传递期间缓存每一层的激活，然后在反向传递的梯度计算中重新使用这些激活。如上所述的放大分析所确立的，这种激活缓存将内存需求与仅推理部署相比增加了。对于一个看似朴素的 10 层卷积模型处理<semantics><mrow><mn>64</mn><mo>×</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">64 \times 64</annotation></semantics>图像，所需的内存可能超过 1 到 2 MB，远远超出大多数嵌入式系统的 SRAM 容量，突显了模型表达能力和资源可用性之间的基本矛盾。

这些内存约束的叠加，模型复杂性直接影响运行时能耗和热限，引入了额外的实际部署障碍。在智能手表或电池供电的可穿戴设备等系统中，持续的模型训练会迅速耗尽能源储备或触发性能下降的热管理。在这些设备上使用浮点运算进行完整模型的训练，从能源角度来看通常是不切实际的，即使内存约束得到满足。这些实际限制促使开发了超轻量级模型变体，如 MLPerf Tiny 基准网络(C. Banbury 等. 2021)，这些网络的大小在 100-200 KB 之间，并且可以通过仅使用部分梯度更新来适应。这些专用模型采用激进的量化和剪枝策略，在保持足够表达性以进行有意义适应的同时，实现了如此紧凑的表示。

电池和热限制的实用影响不仅限于限制训练时间。移动设备必须仔细平衡训练机会与用户体验。激进的设备端训练可能导致设备明显发热和快速耗电，导致用户不满甚至可能卸载应用程序。现代智能手机通常将 ML 工作负载的持续处理功率限制在 2-3 W，以防止热不适，尽管在热限制启动之前，它们可以短暂地达到 5-10 W。即使是训练简单的模型也可能轻易超过这些可持续的功率限制。这一现实需要智能的调度策略：在充电期间进行训练，以改善热散失，在可能的情况下使用低功耗核心进行梯度计算，并在温度阈值超过时实施热感知的轮换，暂停训练。一些系统甚至利用设备使用模式，仅在设备空闲且连接电源的夜间充电期间进行密集的适应调度。

考虑到这些多方面的限制，模型架构本身必须从一开始就从根本上设计，以考虑设备端学习的能力。许多传统的架构，如大型转换器或深度卷积网络，由于它们固有的规模和计算复杂性，在设备端适应方面根本不可行。相反，专门开发了一些轻量级架构，如 MobileNets8、SqueezeNet (Iandola et al. 2016) 和 EfficientNet (Tan and Le 2019a)，这些架构专门针对资源受限的环境。这些架构利用效率原则和架构优化，重新思考了神经网络的结构。这些专用模型采用深度可分离卷积 9、瓶颈层和激进的量化等技术，在显著降低内存和计算需求的同时，保持了足够的应用性能。

这些架构通常设计为模块化，便于适应和微调。例如，MobileNets (A. G. Howard et al. 2017) 可以配置不同的宽度乘数和分辨率设置，以平衡性能和资源使用。具体来说，α=1.0 的 MobileNetV2 需要 3.4 M 个参数（FP32 格式下为 13.6 MB），但 α=0.5 时参数减少到 0.7 M（2.8 MB），使得设备仅有的 4 MB 可用 RAM 也能部署。这种灵活性对于设备端学习非常重要，因为模型必须适应部署环境的特定限制。

虽然模型架构决定了设备上学习的内存和计算基线，但可用训练数据的特征引入了同样根本的限制，这些限制塑造了学习过程的各个方面。

### 数据限制

设备上学习限制的第二个维度集中在数据可用性和质量上。设备上机器学习系统可用的数据性质与在云训练中使用的庞大、精心策划和集中管理的数据集大相径庭。在边缘，数据是本地收集的，时间上稀疏的，通常是非结构化或未标记的，创造了一个不同的学习环境。这些特征在数量、质量和统计分布方面引入了多方面的挑战，所有这些都直接影响设备上学习的可靠性和泛化能力。

数据量代表第一个主要限制因素，受到存储限制和用户交互的偶然性质的双重限制。例如，一个智能健身追踪器可能仅在身体活动期间收集运动数据，每天产生的标记样本相对较少。如果用户仅佩戴设备进行 30 分钟的锻炼，可能只有几百个数据点可用于训练，与在受控环境中进行有效的监督学习通常所需的数千或数百万数据点相比。这种稀缺性改变了学习范式，从数据丰富的算法转变为数据高效的算法。

除了体积限制之外，设备上的数据通常是非独立同分布的（non-IID）(Y. Zhao et al. 2018)，这为云系统很少遇到的统计挑战。这种异质性在多个维度上表现出来：用户行为模式、环境条件、语言偏好和使用上下文。在多个家庭中部署的语音助手会遇到口音、语言、说话方式和命令模式的大幅变化。同样，智能手机键盘会适应个人的打字模式、自动纠错偏好和多语言使用，这些在用户之间差异很大。这种数据异质性既复杂了模型的收敛，也复杂了必须跨设备泛化同时保持个性化的更新机制的设计。

这些分布挑战的叠加，标签稀缺又提出了一个额外的关键障碍，严重限制了传统的学习方法。大多数边缘收集的数据默认情况下是无标签的，需要系统从弱或隐式监督信号中学习。例如，在智能手机摄像头中，设备可能一天中捕获成千上万张图片，但只有少数与有意义的用户行为（例如，标记、收藏或分享）相关，这些可以作为隐式标签。在许多应用中，包括检测传感器数据中的异常和调整手势识别模型，可能完全无法获得显式标签，这使得在没有开发弱监督或无监督适应的替代方法的情况下，传统的监督学习变得不可行。

数据质量问题给设备上的学习挑战增加了另一层复杂性。噪声和可变性进一步降低了用于训练的有限数据。嵌入式系统，如环境传感器或汽车 ECU，可能会经历传感器校准、环境干扰或机械磨损的波动，导致随时间推移输入信号损坏或漂移。没有集中式验证系统来检测和过滤这些错误，它们可能会默默地降低学习性能，创造一个可靠性挑战，云系统可以通过数据预处理管道更容易地解决。

最后，数据隐私和安全问题对所有约束施加了最严格的限制，通常使得数据共享在架构上变得不可能，而不仅仅是不可取。敏感信息，如健康数据、个人通讯或用户行为模式，必须在法律和伦理要求下受到保护，防止未经授权的访问。这种约束通常完全排除了使用传统数据共享方法，例如将原始数据上传到中央服务器进行训练。相反，设备上的学习必须依赖于复杂的技巧，这些技巧能够实现本地适应，而不会暴露任何敏感信息，从而改变了学习系统的设计和验证方式。

### 计算约束

第十一章描述了为机器学习提供计算基础的边缘硬件景观：在最受限的端点是微控制器如 STM32F4 和 ESP32，中间是具有专用 AI 加速器（苹果神经网络引擎、高通 Hexagon、谷歌 Tensor）的移动级处理器，在高端是高能力边缘设备。该章节重点介绍了推理能力——在执行预训练模型时可以达到的计算吞吐量、内存带宽和能效。

训练工作负载展现出根本不同的计算特性，这改变了硬件利用模式。基于第十一章中描述的边缘硬件景观，从微控制器到移动 AI 加速器，设备上的学习必须在严重受限的计算范围内运行，这与基于云的训练基础设施在原始计算能力上相差数百或数千倍。

关键区别在于：反向传播由于梯度计算和激活缓存需要显著更高的内存带宽，权重更新会创建不同于推理的只读操作的写密集型访问模式，而优化器状态管理需要额外的内存分配，这是推理从未遇到的。这些特定的训练需求意味着对于推理完全足够的硬件可能证明在适应方面完全不足，即使只是更新一个小参数子集。

在最受限的一端，STM32F410 或 ESP3211 等微控制器仅提供几百千字节的 SRAM，并且完全缺乏对浮点操作的硬件支持 (Warden and Situnayake 2020)。这些极端限制代表了边缘硬件的基本限制 (Chapter 11)。如此严重的限制排除了使用传统的深度学习库，并要求模型必须精心设计以适应整数算术和最小的运行时内存分配。在这些环境中，即使是看似简单的模型也需要高度专业化的技术，包括量化感知训练 12 和选择性参数更新，以在不超过内存或功耗预算的情况下执行训练循环。

实际影响是明显的：虽然 STM32F4 微控制器可以运行一个带有几百个参数的简单线性回归模型，但训练甚至一个小的卷积神经网络会立即超出其内存容量。在这些严重受限的环境中，训练通常仅限于简单的算法，如随机梯度下降（SGD）13 或 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>-means 聚类，这些算法可以使用整数算术和最小的内存开销实现，这代表了与现代机器学习实践的根本性转变。

在计算层次结构中向上移动，移动级硬件代表了一个显著的改进，但仍然受到大量约束。包括高通骁龙、苹果神经网络引擎 14 和谷歌 Tensor SoC15 在内的平台，比微控制器提供了显著更多的计算能力，通常具有专门的 AI 加速器和针对 8 位或混合精度 16 矩阵操作的优化支持。这些加速器、它们的性能和它们的编程模型在第十一章中详细说明。虽然这些平台可以支持更复杂的训练程序，包括对紧凑模型的完整反向传播，但它们在计算吞吐量和内存带宽方面仍然远远落后于集中式数据中心。例如，在智能手机上训练轻量级转换器 17 在技术上可行，但必须在时间和能耗上严格限制，以避免降低用户体验，突显了学习能力和实际部署约束之间的持续紧张关系。

这些计算限制在实时或电池供电系统中尤为突出，正如在相机处理需求中所展示的，特定的延迟预算创造了硬性架构约束。以 30 帧每秒（FPS）处理的相机应用，每帧不能超过 33 毫秒（ms），语音界面需要快速响应时间以实现自然交互，增强现实/虚拟现实（AR/VR）系统需要低于 20 毫秒的运动到光子（motion-to-photon）延迟以避免用户不适，而安全关键的控制系统必须在 10 毫秒内响应以确保操作安全。这些定量约束决定了设备上学习是否可行，或者基于云的替代方案在架构上是否必要。在基于智能手机的语音识别器中，设备上的自适应必须与主要推理工作负载无缝共存，而不干扰响应延迟或系统响应性。同样，在可穿戴医疗监测器中，训练必须在精心管理的时间窗口内偶然发生——通常在低活动期或充电期间——以保护电池寿命并避免热管理问题。

除了原始的计算能力之外，这些硬件限制在系统设计选择上的影响延伸到了根本性的系统设计。训练操作与推理工作负载在内存访问模式上存在根本性的不同：由于梯度计算和激活缓存，反向传播需要 3-5 倍更高的内存带宽，这创造了纯计算指标无法捕捉的瓶颈。现代边缘加速器试图通过越来越专业的硬件特性来应对这些挑战。自适应精度数据路径允许在正向传递使用 INT4 和梯度计算使用 FP16 之间动态切换，在能源预算内优化准确性和效率。稀疏计算单元通过跳过零梯度来加速选择性参数更新，这对于高效的偏差仅适应和 LoRA 适应是关键能力。近内存计算架构 18 通过在权重存储附近直接执行梯度更新来降低数据移动成本，解决内存带宽瓶颈。然而，大多数当前的边缘加速器仍然在本质上针对推理工作负载进行优化，为未来一代专门设计用于处理本地适应独特需求的设备上训练加速器创造了显著的硬件-软件协同设计机会。

### 边缘硬件集成挑战

超越模型、数据和计算的个人限制，设备上的学习系统必须应对这些元素与移动计算底层物理（如功耗、热限制和能源预算）之间的复杂交互。这些物理限制不仅仅是工程细节，它们是基本的设计驱动因素，决定了设备上学习算法的整个可行空间。理解这些定量限制能够使设计决策更加明智，在平衡学习能力和长期系统可持续性以及用户接受度之间取得平衡。

#### 能源和热限制分析

能量和热管理可能是设备上学习系统设计中最具挑战性的方面，因为它们直接影响用户体验和设备寿命。移动设备在严格的电源预算下运行，这从根本上决定了可行的模型复杂度和训练计划。移动处理器的热设计功耗（TDP）为设备上学习的各个方面创造了硬约束。现代智能手机通常将 ML 工作负载的持续处理能力维持在 2-3 W，以防止热不适，但在热管理显著降低性能 50%或更多之前，可以短暂地爆发出 5-10 W。这种热循环行为迫使训练算法在精心管理的高峰模式下运行，仅在 10-30 秒内利用峰值性能，然后降低到可持续的功率水平，这一约束从根本上改变了训练算法的设计方式。

移动电源预算层次结构揭示了设备上学习必须运行的严格约束。智能手机的持续处理能力限制在 2-3 W，以防止用户察觉到发热并保持一整天的可接受电池寿命。峰值训练突发模式可以达到 10 W，但这种功率水平只能持续 10-30 秒，然后热管理启动以保护硬件。专用神经网络单元用于 AI 工作负载，消耗 0.5-2 W，与通用处理器相比，提供优化的功耗效率。基于 CPU 的 AI 处理需要 3-5 W，并需要积极的温度管理以及轮换工作以防止过热，使其成为持续设备上学习最不节能的选项。

训练工作负载的功耗特性创造了额外的约束层，这些约束层超出了简单的计算能力。功耗随模型大小和训练复杂度的超线性增长，训练操作消耗的功率比等效推理工作负载多 10-50<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>，这是由于梯度计算（消耗 40-70%的训练功率）、权重更新（20-30%）和内存层次结构之间数据移动的显著增加（10-30%）的大量计算开销。为了保持可接受的用户体验，移动设备通常只为持续 ML 训练预留 500-1000 mW 的预算，实际上将实际训练时间限制在每天正常使用模式下的 10-100 分钟。这种严重的功耗约束从根本上改变了设计优先级，从最大化计算吞吐量转向优化功耗效率，需要仔细协同优化算法和硬件利用模式。

热管理挑战远远超出了简单的功率限制，创造了复杂且随环境条件和使用模式变化的动态约束。训练工作负载会产生局部热量，这可能会在特定的处理器核心或加速器单元中触发保护性降频，通常是以不可预测的方式，这取决于环境温度和设备设计。现代移动系统级芯片（SoC）实现了复杂的热管理系统，包括动态电压和频率缩放（DVFS）19、在效率集群和性能集群之间的核心迁移，以及选择性关闭非关键处理单元。成功部署在设备上的学习系统必须与这些热管理框架紧密集成，智能地安排在最佳热窗口期间进行训练爆发，并在接近热限制时优雅地降低性能，而不是简单地失败或导致用户可见的性能问题。

#### 内存层次结构优化

补充热能和功率挑战，内存层次结构的约束为设备上的学习系统设计创造了另一个基本瓶颈。如上所述的约束放大分析所确立的，这些限制影响静态模型存储和训练期间的动态内存需求，通常会将系统推向其实际极限之外。

设备内存层次结构跨越不同设备类别，数量级跨度很大，每个类别都为设备上的学习提供了独特的约束。iPhone 15 Pro 提供了总共 8 GB 的系统内存，但在考虑到操作系统需求和后台进程后，只有大约 2-4 GB 可用于应用工作负载。预算型安卓设备运行时拥有总共 4 GB 的系统内存，在操作系统开销消耗了大量资源后，仅剩下 1-2 GB 可用于机器学习工作负载。物联网嵌入式系统提供 64 MB-1 GB 的总内存，这些内存必须在系统任务和应用数据之间共享，为任何学习算法创造了严重的约束。微控制器仅提供 256 KB-2 MB 的 SRAM，这要求极端优化和仔细的内存管理，从根本上限制了可以在这些平台上适应的模型复杂性。

训练过程中的内存扩展带来了特别严峻的挑战，这些挑战往往决定了系统的可行性。标准的反向传播需要在正向传递过程中为每一层缓存中间激活，然后在反向传递的梯度计算中重新使用，从而产生大量的内存开销。一个仅需要 14 MB 进行推理的 MobileNetV2 模型在训练过程中膨胀到 50-70 MB，通常超过了许多移动设备上的可用内存预算，使得没有积极的优化就无法进行训练。这种戏剧性的膨胀需要复杂的模型压缩技术，这些技术必须以乘法方式组合：INT8 量化提供<semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">4\times</annotation></semantics>的内存减少，结构化剪枝实现<semantics><mrow><mn>10</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">10\times</annotation></semantics>的参数减少，而知识蒸馏使得模型大小减少<semantics><mrow><mn>5</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">5\times</annotation></semantics>，同时保持准确率在原始模型的 2-5%以内。这些技术必须谨慎组合，以实现实际部署所需的激进压缩比率。

在这些内存限制下，缓存优化对于在受限的内存池中实现可接受的性能变得绝对关键。现代移动 SoC 具有复杂的内存层次结构，包括 L1 缓存（32-64 KB）、L2 缓存（1-8 MB）和系统内存（4-16 GB），不同层级之间表现出 10-100 倍的延迟差异，当工作集超过缓存容量时，会形成严重的性能悬崖。超出缓存容量的训练工作负载由于内存带宽瓶颈而面临显著的性能下降，这可能导致训练速度降低几个数量级。成功的设备端学习系统必须精心设计数据访问模式以最大化缓存命中率，通常需要专门的内存布局来分组相关参数以实现空间局部性，精心设计的迷你批次以完全符合缓存限制，以及复杂的梯度累积策略以最小化昂贵的内存总线流量。

在训练过程中，内存带宽限制变得尤为突出。虽然推理工作负载主要按顺序读取模型权重，但训练需要双向数据流来进行梯度计算和权重更新。这种增加的内存流量可能会饱和内存子系统，形成瓶颈，限制训练吞吐量，无论计算能力如何。高级实现采用梯度检查点 20 等技术以计算换取内存，以及混合精度训练以降低带宽需求同时保持数值稳定性。

#### 移动 AI 加速器优化

不同的移动平台提供了不同的加速能力，这些能力不仅决定了可实现的模型复杂度，还决定了可行的学习范式。这些加速器之间的架构差异从根本上塑造了设备上训练算法的设计空间，影响着从数值精度选择到梯度计算策略的各个方面。

当前一代的移动加速器在能力和优化重点上表现出显著的多样性。苹果的 A17 Pro 中的 Neural Engine 提供了 35 TOPS 的峰值性能，专门针对 8 位和 16 位操作，主要针对 CoreML 推理模式进行优化，对训练的支持有限，使其成为推理密集型适应技术的理想选择。高通的 Snapdragon 8 Gen 3 中的 Hexagon DSP 通过灵活的精度支持和可编程向量单元实现了 45 TOPS，能够实现混合精度训练工作流程，可以根据训练阶段和内存约束动态调整精度。谷歌的 Pixel 8 中的 Tensor TPU 针对 TensorFlow Lite 操作进行了优化，具有强大的 INT8 性能，并与联邦学习框架紧密集成，反映了谷歌对分布式学习场景的战略关注。能效比较揭示了为什么专用神经网络单元是必不可少的：NPUs 在每瓦特实现 1-5 TOPS，而通用 CPU 在每瓦特仅实现 0.1-0.2 TOPS，这代表了 5-50<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的效率优势，这在设备上训练的可行性与不可行性之间划定了界限。

这些加速器不仅决定了原始性能，还决定了可行的学习范式和算法方法。苹果的神经网络引擎在固定精度推理工作负载方面表现出色，但为梯度计算的动态精度需求提供了有限的支持，使其更适合于推理密集型适应技术，如少样本学习。高通的 Hexagon DSP 通过其可编程向量单元和对混合精度算术的支持提供了更大的训练灵活性，使得在紧凑模型上实现更复杂的设备上训练成为可能，包括完整的反向传播。谷歌的 Tensor TPU 与联邦学习框架紧密集成，并为分布式训练场景提供了优化的通信原语。

架构的影响不仅限于计算吞吐量，还包括内存访问模式和数据处理优化。训练工作负载与推理具有根本不同的特性：由于上述讨论的放大效应，梯度计算需要显著更高的内存带宽，权重更新创建出写密集型的访问模式，而优化器状态管理需要额外的内存分配。现代边缘加速器正通过专用硬件特性来应对这些挑战，包括自适应精度数据路径，该路径可以动态地在 INT4（用于正向传递）和 FP16（用于梯度计算）之间切换，稀疏计算单元通过跳过零梯度来加速选择性参数更新，以及近内存计算架构，通过在权重存储附近直接执行梯度更新来降低数据移动成本。

然而，目前大多数边缘加速器主要针对推理工作负载进行优化，这为硬件和软件协同设计提供了巨大的机会。未来的设备上训练加速器需要高效地处理本地适应的独特需求，包括支持动态精度缩放、高效的梯度累积以及针对训练工作负载双向数据流模式优化的专用内存层次结构。架构选择影响着从模型量化策略和梯度计算方法到联邦通信协议和热管理策略的各个方面。

### 全面的资源管理策略

上述约束分析揭示了三个基本挑战类别，这些类别定义了设备上学习设计空间。每个约束类别直接驱动相应的解决方案支柱，形成了一种系统化的工程方法来应对这个复杂系统问题。约束到解决方案的映射自然地来自于理解特定限制如何需要特定的技术响应。

资源放大效应——训练使内存需求增加 3-10 倍，计算成本增加 2-3 倍，能耗成比例增加——直接需要模型适应方法。当由于资源限制而无法进行传统训练时，系统必须在保持学习能力的同时，从根本上减少参数更新的范围。

信息稀缺约束——局部数据集有限、非独立同分布分布、数据共享的隐私限制和最小监督——直接推动了数据效率解决方案。当由于局部信息不足而导致传统数据密集型方法失败时，系统必须从最少的示例中提取最大的学习信号。

协调挑战——设备异质性、间歇性连接、分布式验证复杂性和可扩展性要求——直接推动了联邦协调机制。当设备上的独立学习限制了集体智慧时，系统必须能够在设备群体之间实现隐私保护的协作。

如表 14.2 所示，这种约束到解决方案的映射创建了一个系统性的工程框架，其中每个支柱解决部署挑战的特定方面，同时与其他部分相整合。而不是将这些视为独立的技术，成功的设备上学习系统编排所有三种方法，以创建在边缘约束内有效运行的协调一致的自适应系统。

表 14.2：**约束-解决方案映射**：设备上学习的三个基本约束类别通过直接必要性驱动相应的解决方案。

| **约束类别** | **关键挑战** | **解决方案** |
| --- | --- | --- |
| **资源放大** | • 训练工作负载（3-10 倍内存） • 内存限制 • 功率限制 | **模型适应** • 参数高效更新 • 选择性层微调 • 低秩适应 |
| **信息稀缺** | • 局部数据集有限 • 非独立同分布分布 • 数据共享的隐私限制 | **数据效率**• 少样本学习• 元学习 • 迁移学习 |
| **协调挑战** | • 设备异质性 • 间歇性连接 • 分布式验证 | **联邦协调** • 隐私保护聚合 • 坚固的通信协议 • 异步参与 |

后续章节将系统地检查每个解决方案支柱，基于第十章（第十章）中的优化原则和第十三章（第十三章）中的分布式系统框架。每个支柱都提供了其他单个支柱无法提供的必要功能，但它们的集成创造了能够在边缘部署环境的严格约束下进行有意义适应的系统。

## 模型适应

上述的计算和内存限制为模型训练创造了一个具有挑战性的环境，但它们也揭示了当系统性地处理时，清晰的解决方案路径。模型适应代表了设备端学习系统工程的第一个支柱：通过减少参数更新的范围，在边缘约束内使训练可行，同时保持足够模型的表达性以实现有意义的个性化。

工程挑战集中在导航一个基本权衡空间：适应表达性与资源消耗。在一种极端情况下，更新所有参数提供最大灵活性，但超出了边缘设备的处理能力。在另一种极端情况下，不进行适应可以保留资源，但无法捕捉到用户特定的模式。有效的设备端学习系统必须在中间地带运行，根据三个关键工程标准选择适应策略。

首先，可用的内存、计算和能源决定了哪些适应方法是可行的。具有 1MB RAM 的智能手表需要与具有 8GB RAM 的手机采用完全不同的策略。其次，用户特定差异的程度驱动了适应复杂性的需求。简单的偏好学习可能只需要偏差更新，而复杂的领域迁移则需要更复杂的方法。第三，适应技术必须与第十三章（第十三章）中建立的现有推理管道、联邦协调协议和运营监控系统相集成。

这种系统视角指导了从轻量级方法（第 14.4.1 节）开始的选择和组合，并逐步过渡到更复杂的方法（第 14.4.3 节），从仅包含偏差的轻量级方法逐步过渡到更具表达性但资源密集型的方法。每种技术代表工程权衡空间中的不同点，而不是一个孤立的算法解决方案。

基于第十章中的压缩技术，设备上的学习将压缩从一次性优化转变为持续约束。驱动所有模型调整方法的中心洞察是，对于设备上的学习场景，完全重新训练模型既不必要也不可行。相反，系统可以战略性地利用预训练表示，并仅适应捕获局部变化所需的最小参数子集，遵循的原则是：保留全局有效的内容，适应局部重要的事项。

本节系统地检查了三种互补的调整策略，每种策略都是专门为解决不同的设备约束配置和应用需求而设计的。权重冻结通过仅更新偏置项或最终层来应对严重的内存限制，即使在其他情况下缺乏任何形式适应资源的严重受限的微控制器上也能实现学习。结构化更新使用低秩和残差调整来平衡模型的表达能力和计算效率，在保持可管理资源需求的同时，允许比仅偏置方法更复杂的学习。稀疏更新允许根据梯度重要性或层关键性进行选择性的参数修改，将学习能力集中在最有影响的参数上，而将不那么重要的权重冻结。

这些方法在既定的架构原则基础上，战略性地应用优化策略来解决边缘部署的独特挑战。每种技术都代表了在基本精度-效率权衡空间中经过仔细考虑的点，使得在边缘硬件能力的全范围内实现实用部署成为可能——从超受限的微控制器到功能强大的移动处理器。

### 权重冻结

使设备上的学习成为可能的最直接方法之一是显著减少需要更新的参数数量。实现这种减少的最简单和最有效的策略之一是冻结模型的大部分参数，并仅适应精心选择的极小子集。在这个家族中最广泛使用的方法是仅偏置调整，其中所有权重都保持固定，仅在训练期间更新偏置项（通常是线性或卷积层之后的标量偏移量）。这个简单的约束带来了显著的好处：它减少了可训练参数的数量（通常减少 100-1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>），简化了反向传播期间的内存管理，并在训练数据稀疏或噪声时有助于减轻过拟合。

考虑一个标准的神经网络层：<semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow> <annotation encoding="application/x-tex">y = W x + b</annotation></semantics> 其中 <semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W \in \mathbb{R}^{m \times n}</annotation></semantics> 是权重矩阵，<semantics><mrow><mi>b</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">b \in \mathbb{R}^m</annotation></semantics> 是偏置向量，<semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^n</annotation></semantics> 是输入。在完整训练中，计算了<semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics>和<semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics>的梯度。在仅偏置适应中，我们约束：<semantics><mrow><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><mi>W</mi></mrow></mfrac><mo>=</mo><mn>0</mn><mo>,</mo><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><mi>b</mi></mrow></mfrac><mo>≠</mo><mn>0</mn></mrow> <annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial W} = 0, \quad \frac{\partial \mathcal{L}}{\partial b} \neq 0</annotation></semantics> 因此，只有偏置通过梯度下降进行更新：<semantics><mrow><mi>b</mi><mo>←</mo><mi>b</mi><mo>−</mo><mi>η</mi><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><mi>b</mi></mrow></mfrac></mrow> <annotation encoding="application/x-tex">b \leftarrow b - \eta \frac{\partial \mathcal{L}}{\partial b}</annotation></semantics>

这减少了存储的梯度数和优化器状态数，使得在内存受限条件下也能进行训练。在缺少浮点单元的嵌入式设备上，这种减少使得设备上的学习成为可能。

列表 14.1 中的代码片段展示了如何在 PyTorch 中实现仅偏置适应。

列表 14.1：**仅偏置适应**：冻结模型参数中的偏置以减少内存使用，并允许设备上的学习。

```py
# Freeze all parameters
for name, param in model.named_parameters():
    param.requires_grad = False

# Enable gradients for bias parameters only
for name, param in model.named_parameters():
    if "bias" in name:
        param.requires_grad = True
```

这种模式确保只有偏置项参与反向传播和优化器更新，简化了训练过程同时保持适应能力。当将预训练模型适应于特定用户或设备本地数据时，其中核心表示仍然相关但需要校准，这非常有价值。

TinyTL 的实际有效性通过 TinyTL 框架得到证明，该框架专门设计用于在微控制器和其他严重内存受限平台上高效地适应深度神经网络。在训练期间，TinyTL 并不是更新所有网络参数（在如此受限的设备上不可能实现），而是战略性地冻结卷积权重和批量归一化统计信息，仅训练偏置项以及在某些情况下轻量级的残差组件。这种架构限制在反向传播期间对内存需求产生了深远的影响，因为最大的内存消耗者（中间激活）不再需要存储以进行冻结层之间的梯度计算。

当将标准训练与 TinyTL 方法进行比较时，这种方法的架构影响变得清晰。图 14.5 展示了传统模型与 TinyTL 方法在设备上适应的根本区别。考虑到之前建立的边缘设备内存限制，TinyTL 方法通过消除存储冻结层激活的需求，从根本上改变了内存方程，使得在边缘设备的严重内存限制下进行适应成为可能。

![图片](img/file224.svg)

图 14.5：TinyTL 通过冻结卷积权重和批量归一化，仅更新偏置项和轻量级残差连接来降低设备上训练的成本。这种方法允许在资源受限的边缘设备上部署深度神经网络，这些设备具有有限的 SRAM，从而在不要求完整参数更新的情况下实现高效的模型适应。

与之相反，TinyTL 架构冻结了所有权重，并且只更新在卷积层之后插入的偏置项。这些偏置模块轻量级且占用内存最小，使得在显著减少内存占用的情况下实现高效训练成为可能。冻结的卷积层充当固定的特征提取器，并且只有可训练的偏置组件参与适应。通过避免存储完整的激活图并限制更新参数的数量，TinyTL 允许在设备上在严重资源限制下进行训练。

由于基础模型保持不变，TinyTL 假设预训练的特征对于下游任务来说足够表达。偏置项允许模型行为发生微小但有意义的变化，尤其是在个性化任务中。当领域变化更为显著时，TinyTL 可以选择性地结合小的残差适配器来提高表达能力，同时保持系统的紧凑内存和能耗配置文件。

这些设计选择使得 TinyTL 能够将训练内存使用量减少 10 倍。例如，使用 TinyTL 对 MobileNetV2 模型进行自适应可以将更新参数的数量从超过 300 万个减少到少于 50,000 个 21。结合量化，这允许在只有几百千字节内存的设备上进行本地自适应——使得在受限环境中进行设备学习真正可行。

### 结构化参数更新

虽然权重冻结提供了计算效率和清晰的内存界限，但它通过限制自适应到一个小参数子集，严重限制了模型的表达能力。当仅偏置更新不足以捕捉复杂的领域变化或用户特定的模式时，残差和低秩技术提供了增强的自适应能力，同时保持了计算上的可处理性。这些方法代表了权重冻结的极端效率和无限制微调的完全表达能力之间的中间地带。

与修改现有参数不同，这些方法通过添加可训练组件——残差自适应模块(Houlsby 等人，2019)或低秩参数化(E. J. Hu 等人，2021)——来扩展冻结模型，这些组件提供了受控的模型容量增加。这种架构方法使得更复杂的自适应成为可能，同时保持了使设备学习可行的计算优势。

这些方法通过添加可训练层来扩展冻结模型，这些层通常很小且计算成本低，使得网络能够对新数据进行响应。网络的主要部分保持不变，而只有添加的组件被优化。这种模块化使得该方法非常适合在受限环境中进行设备上的自适应，在这种环境中，必须提供能够带来有意义变化的小更新。

#### 基于适配器的自适应

一种常见的实现方法是在预训练模型中的现有层之间插入适配器，这些适配器是小的残差瓶颈层。考虑在层之间传递的隐藏表示<semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics>。一个残差适配器引入了一种转换：<semantics><mrow><mi>h</mi><mi>′</mi><mo>=</mo><mi>h</mi><mo>+</mo><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">h' = h + A(h)</annotation></semantics> 其中 <semantics><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>⋅</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(\cdot)</annotation></semantics> 是一个可训练的函数，通常由两个带有非线性函数的线性层组成：<semantics><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>W</mi><mn>2</mn></msub><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>W</mi><mn>1</mn></msub><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">A(h) = W_2 \, \sigma(W_1 h)</annotation></semantics> 其中 <semantics><mrow><msub><mi>W</mi><mn>1</mn></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W_1 \in \mathbb{R}^{r \times d}</annotation></semantics> 和 <semantics><mrow><msub><mi>W</mi><mn>2</mn></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W_2 \in \mathbb{R}^{d \times r}</annotation></semantics>，其中 <semantics><mrow><mi>r</mi><mo>≪</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">r \ll d</annotation></semantics>。这种瓶颈设计确保每个层只引入少量参数。

适配器在冻结的骨干网络上充当可学习的扰动。由于它们体积小且应用稀疏，因此它们几乎不会增加内存开销，但它们允许模型根据新的输入调整其预测。

#### 低秩技术

另一种高效策略是将权重更新本身约束到低秩结构。而不是更新完整的矩阵 <semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics>，我们将更新近似为：<semantics><mrow><mi>Δ</mi><mi>W</mi><mo>≈</mo><mi>U</mi><msup><mi>V</mi><mi>⊤</mi></msup></mrow> <annotation encoding="application/x-tex">\Delta W \approx U V^\top</annotation></semantics> 其中 <semantics><mrow><mi>U</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation encoding="application/x-tex">U \in \mathbb{R}^{m \times r}</annotation></semantics> 和 <semantics><mrow><mi>V</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation encoding="application/x-tex">V \in \mathbb{R}^{n \times r}</annotation></semantics>，其中 <semantics><mrow><mi>r</mi><mo>≪</mo><mo>min</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">r \ll \min(m,n)</annotation></semantics>。这减少了可训练参数的数量，从 <semantics><mrow><mi>m</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">mn</annotation></semantics> 减少到 <semantics><mrow><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo>+</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">r(m + n)</annotation></semantics>。

这种分解背后的数学直觉与基本的线性代数原理相联系：任何矩阵都可以通过奇异值分解表示为秩为 1 的矩阵之和。通过将我们的更新约束到低秩（通常是 <semantics><mrow><mi>r</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">r = 4</annotation></semantics> 到 <semantics><mn>16</mn><annotation encoding="application/x-tex">16</annotation></semantics>），我们捕捉到最重要的变化模式，同时减少参数。对于一个典型的维度为 <semantics><mrow><mn>768</mn><mo>×</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">768 \times 768</annotation></semantics> 的 transformer 层，全微调需要更新 589,824 个参数。使用秩-4 分解，我们只更新 <semantics><mrow><mn>768</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>2</mn><mo>=</mo><mn>6</mn><mo>,</mo><mn>144</mn></mrow><annotation encoding="application/x-tex">768 \times 4 \times 2 = 6,144</annotation></semantics> 个参数，减少了 96%，同时在经验上保留了 85-90%的适应质量。

在自适应过程中，新的权重计算如下：<semantics><mrow><msub><mi>W</mi><mtext mathvariant="normal">adapted</mtext></msub><mo>=</mo><msub><mi>W</mi><mtext mathvariant="normal">frozen</mtext></msub><mo>+</mo><mi>U</mi><msup><mi>V</mi><mi>⊤</mi></msup></mrow> <annotation encoding="application/x-tex">W_{\text{adapted}} = W_{\text{frozen}} + U V^\top</annotation></semantics>

这种公式在 LoRA（低秩自适应）22 技术中常用，最初是为 transformer 模型开发的（E. J. Hu 等，2021），但适用于各种架构。从系统工程的角度来看，LoRA 解决了设备学习中部署的关键连接性和资源权衡问题。

考虑一个移动部署场景，其中 7B 参数的语言模型需要 14GB 进行完全微调——这在典型的 6-8GB 总内存的智能手机上是不可能的。使用 16 秩的 LoRA 可以将可训练参数减少到约 100MB（原始的 0.7%），从而在移动内存限制内实现本地自适应。

在间歇性连接场景中，LoRA 的效率变得至关重要。通过蜂窝网络进行完整模型更新需要 14GB 的下载（潜在成本 140+美元的移动数据费用），而 LoRA 适配器更新通常为 10-50MB。对于定期的联邦协调，设备可以在 3G 网络上在 30 秒内同步 LoRA 适配器，而完整模型传输可能需要数小时。这即使在网络条件较差的情况下也使得实用的联邦学习成为可能。

系统通常根据设备能力部署不同的 LoRA 配置——旗舰手机使用 32 秩适配器以实现更高的表达能力，中端设备使用 16 秩以实现平衡的性能，而预算设备使用 8 秩以保持在 2GB 内存限制内。低秩更新可以在边缘设备上高效实现，尤其是在<semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics>和<semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics>很小且支持定点表示的情况下（列表 14.2）。

列表 14.2：**低秩适配器**：该代码通过使用矩阵（u）和（v）近似权重更新来实现低秩适配器模块，减少参数数量，同时使边缘设备上的高效模型自适应成为可能。

```py
class Adapter(nn.Module):
    def __init__(self, dim, bottleneck_dim):
        super().__init__()
        self.down = nn.Linear(dim, bottleneck_dim)
        self.up = nn.Linear(bottleneck_dim, dim)
        self.activation = nn.ReLU()

    def forward(self, x):
        return x + self.up(self.activation(self.down(x)))
```

此适配器向一个冻结层添加了一个小的残差变换。当将其插入到更大的模型中时，只有适配器参数会被训练。

#### 边缘个性化

当全局模型部署到多个设备上并且必须适应特定于设备的输入分布时，适配器很有用。在智能手机摄像头管道中，环境光照、用户偏好或镜头畸变在不同用户之间会有所不同（Rebuffi、Bilen 和 Vedaldi 2017）。可以通过使用几个残差模块来冻结共享模型并针对每个设备进行微调，从而实现轻量级个性化，而不会导致灾难性遗忘。在基于语音的系统中，适配器模块已被证明可以降低个性化语音识别中的词错误率，而无需重新训练完整的声学模型。它们还允许轻松回滚或在不同用户特定版本之间切换。

#### 性能与资源权衡

残差和低秩更新在表达性和效率之间取得了平衡。与仅使用偏差的学习相比，它们可以模拟从预训练任务中更显著的偏差。然而，它们在训练和推理过程中需要更多的内存和计算资源。

在考虑设备上学习的残差和低秩更新时，出现了几个重要的权衡。首先，与仅使用偏差的方法相比，这些方法在适应性质量上始终表现出优越性，尤其是在部署在涉及从原始训练数据到显著分布变化的场景中（Quiñonero-Candela 等人 2008）。这种改进的适应性源于它们增加的参数容量和能够学习更复杂变换的能力。

这种增强的适应性是有代价的。引入额外的层或参数不可避免地会增加前向和反向传递期间的内存需求和计算延迟。虽然与完整模型训练相比，这些增加是微不足道的，但在部署到资源受限的设备时必须考虑这一点。

实施这些适应技术需要系统级支持动态计算图和选择性地注入可训练参数的能力。并非所有部署环境或推理引擎都默认支持这些功能。

残差适应技术在移动和边缘计算场景中已被证明是有价值的，在这些场景中，设备有足够的计算资源。现代智能手机和平板电脑可以适应这些更新，同时保持可接受的性能特征。这使得残差适应成为需要个性化但无需重新训练完整模型开销的应用的实际选择。

### 稀疏更新

随着我们从仅偏置更新通过低秩自适应到更复杂的技术，稀疏更新代表了我们模型自适应层次中最先进的方法。虽然前一种技术添加了新参数或限制更新到特定子集，但稀疏更新动态地识别出为每个特定任务或用户提供最大自适应益处的现有参数。这种方法在保持边缘部署所需的计算效率的同时，最大化了自适应的表达能力。

即使通过上述技术将自适应限制在少量参数上，在受限制的设备上进行训练仍然资源密集。稀疏更新通过仅选择性地更新与任务相关的模型参数子集，而不是修改整个网络或引入新模块，来解决这一挑战。这种方法被称为任务自适应稀疏更新(X. Zhang, Song, and Tao 2020)，代表了原则性参数选择策略的顶峰。

关键的洞察是，深度模型的不是所有层都对新的任务或数据集的性能提升做出同等贡献。如果我们能识别出对自适应影响最大的*最小参数子集*，我们就可以只训练这些参数，从而在降低内存和计算成本的同时，仍然实现有意义的个性化。

#### 稀疏更新设计

设一个神经网络由参数<semantics><mrow><mi>θ</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>θ</mi><mi>L</mi></msub><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\theta = \{\theta_1, \theta_2, \ldots, \theta_L\}</annotation></semantics>跨越<semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>层定义。在标准微调中，我们计算所有参数的梯度并进行更新：<semantics><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><mi>η</mi><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfrac><mo>,</mo><mrow><mtext mathvariant="normal">for</mtext></mrow> <mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>L</mi></mrow> <annotation encoding="application/x-tex">\theta_i \leftarrow \theta_i - \eta \frac{\partial \mathcal{L}}{\partial \theta_i}, \quad \text{for } i = 1, \ldots, L</annotation></semantics>

在任务自适应稀疏更新中，我们选择一个小的子集<semantics><mrow><mi>𝒮</mi><mo>⊂</mo><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>L</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{S} \subset \{1, \ldots, L\}</annotation></semantics>，使得只有<semantics><mi>𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics>中的参数被更新：<semantics><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><mi>η</mi><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfrac><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if</mtext></mrow> <mi>i</mi><mo>∈</mo><mi>𝒮</mi></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><msub><mi>θ</mi><mi>i</mi></msub><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">otherwise</mtext></mtd></mtr></mtable></mrow></mrow> <annotation encoding="application/x-tex">\theta_i \leftarrow \begin{cases} \theta_i - \eta \frac{\partial \mathcal{L}}{\partial \theta_i}, & \text{if } i \in \mathcal{S} \\ \theta_i, & \text{otherwise} \end{cases}</annotation></semantics>

挑战在于在内存和计算约束下选择最优子集<semantics><mi>𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics>。

#### 层选择

选择<semantics><mi>𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics>的一个原则性策略是使用贡献分析——这是一种经验方法，用于估计每个层对下游性能改进的贡献程度。例如，可以独立地测量更新每个层的边际增益：

1.  冻结整个模型。

1.  解冻一个候选层。

1.  简短微调并评估验证准确性的改进。

1.  按每单位成本（例如，每 KB 的可训练内存）的性能增益对层进行排名。

这种层级分析产生了一个排名，根据这个排名可以在内存预算的限制下构建<semantics><mi>𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics>。

一个具体的例子是 TinyTrain，这是一种旨在允许设备上快速适应的方法（C. Deng, Zhang, and Wu 2022）。TinyTrain 在预训练模型的同时，还预训练了元梯度，以捕捉哪些层对新的任务最为敏感。在运行时，系统根据任务特性和可用资源动态选择要更新的层。

#### 选择性层更新实现

这种模式可以通过分析逻辑扩展，根据贡献分数或硬件配置文件选择层，如列表 14.3 所示（ch020.xhtml#lst-selective-update）。

列表 14.3：**选择性层更新**：这项技术允许在保持其他层冻结的同时微调预训练模型的特定层，从而优化计算资源以实现针对性的改进。*来源：PyTorch 文档*

```py
# Assume model has named layers: ['conv1', 'conv2', 'fc']
# We selectively update only conv2 and fc

for name, param in model.named_parameters():
    if "conv2" in name or "fc" in name:
        param.requires_grad = True
    else:
        param.requires_grad = False
```

#### TinyTrain 个性化

考虑一个场景，用户佩戴一个执行实时物体识别的增强现实头盔。随着光照和环境的变化，系统必须适应以保持准确性——但训练必须在短暂的空闲期间或充电时进行。

TinyTrain 通过在离线准备期间使用元训练来实现这一点：模型不仅学习执行任务，还学习哪些参数对于适应最为重要。然后，在部署时，设备执行针对当前环境的任务自适应稀疏更新，仅修改与当前环境最相关的少数层。这保持了适应的快速性、节能性和内存感知性。

#### 适应策略权衡

任务自适应稀疏更新引入了几个重要的系统级考虑因素，必须仔细平衡。首先，贡献分析的开销，尽管主要发生在预训练或初始分析期间，代表了一个非微不足道的计算成本。这种开销通常是可以接受的，因为它发生在离线时，但它必须纳入整体系统设计和部署流程中。

其次，当使用稀疏更新时，适应过程的不稳定性变得重要。如果选定的参数太少，模型可能无法很好地拟合目标分布，无法捕捉到重要的局部变化。这表明在部署前需要对选定的参数子集进行仔细验证，可能需要结合适应能力的最小阈值。

第三，选择可更新参数时必须考虑目标平台的特定硬件特性。除了考虑梯度幅度之外，系统必须评估在部署硬件上更新特定层的实际执行成本。一些参数可能显示出高贡献分数，但在某些架构上更新成本高昂，需要一种更细致的选择策略，以平衡统计效用和运行时效率。

尽管存在这些权衡，任务自适应稀疏更新提供了一种强大的机制，可以将适应扩展到各种部署环境，从微控制器到移动设备（Levy 等人 2023）。

#### 适应策略比较

每种设备学习自适应策略都提供了表达性、资源效率和实现复杂度之间的独特平衡。在设计针对各种部署目标的系统时，理解这些权衡很重要——从超低功耗微控制器到功能丰富的移动处理器。

仅偏差的自适应是最轻量级的方法，它只更新每层的标量偏移量，同时冻结所有其他参数。这显著降低了内存需求和计算负担，使其适用于内存和能源预算紧张的设备。然而，由于其表达能力有限，它最适合于预训练模型已经捕捉到大多数相关任务特征，并且只需要进行少量局部校准的应用。

剩余自适应，通常通过适配器模块实现，向冻结的神经网络主干引入少量可训练参数。这比仅偏差更新提供了更大的灵活性，同时仍然保持对自适应成本的掌控。由于主干保持固定，训练可以在受限条件下高效且安全地进行。这种方法支持跨任务和用户的模块化个性化，使其成为需要适度自适应能力的移动环境中的优选选择。

任务自适应稀疏更新通过选择性地仅更新对下游性能有贡献的子集层或参数，为特定任务的微调提供了最大的潜力。虽然这种方法允许表达性的局部自适应，但它需要一个层选择机制，通过配置文件、贡献分析或元训练来实现，这引入了额外的复杂性。然而，当谨慎部署时，它允许在准确性和效率之间进行动态权衡，尤其是在经历大规模领域转移或输入条件演变的系统中。

这三种方法形成了一个权衡的谱系。它们的相对适用性取决于应用领域、可用硬件、延迟约束和预期的分布变化。表 14.3 总结了它们的特征：

表 14.3：**自适应策略权衡**：表条目通过量化其对可训练参数、内存开销、表达能力、对不同用例的适用性和系统要求的影响，来描述模型自适应的三种方法——仅偏差更新、选择性层更新和完全微调。这些特征揭示了在动态环境中部署机器学习系统时，模型灵活性、计算成本和性能之间的固有权衡。

| **技术** | **可训练参数** | **内存开销** | **表达能力** | **适用用例** | **系统要求** |
| --- | --- | --- | --- | --- | --- |
| **仅偏置更新** | 仅偏置项 | 最小 | 低 | 简单个性化；低方差 | 极端内存/计算限制 |
| **残差适配器** | 适配器模块 | 中等 | 中等到高 | 移动设备上的用户特定调整 | 带运行时支持的移动级 SoC |
| **稀疏层更新** | 选择性参数子集 | 变量 | 高（任务自适应） | 实时适应；领域迁移 | 需要配置文件或元训练 |

## 数据效率

通过模型技术建立了资源高效的适应机制后，我们遇到了设备上学习系统工程的第二个支柱：从严重受限的数据中最大化学习信号。这代表了从传统机器学习系统所假设的数据丰富环境到边缘部署信息稀缺现实的基本转变。

系统工程挑战集中在关键权衡上：数据收集成本与适应质量。边缘设备面临严重的数据获取约束，这以在集中式训练中未遇到的方式重塑了学习系统设计。理解和导航这些约束需要系统地分析四个相互关联的工程维度。

首先，每个数据点都有获取成本，包括用户摩擦、能耗、存储开销和隐私风险。一个从音频样本学习的语音助手必须平衡改进潜力与电池消耗和用户对始终开启录音的舒适度。其次，有限的数据收集能力迫使系统在广泛覆盖和深入示例之间做出选择。一个移动键盘可以收集许多浅层打字模式或较少的详细交互序列，每种策略都意味着不同的学习方法。第三，某些应用需要从最小示例中快速学习（紧急响应场景），而其他应用可以随着时间的推移积累数据（用户偏好学习）。这个时间维度驱动了基本架构选择。第四，数据效率技术必须与第 14.4 节中的模型适应方法、联邦协调(第 14.6 节)以及第十三章中建立的运营监控相结合。

这些工程约束创建了一个系统性的权衡空间，其中不同的数据效率方法服务于不同组合的约束。成功的设备上学习系统通常结合多种方法，每种方法都针对数据稀缺挑战的特定方面。

本节探讨了四种互补的数据效率策略，这些策略针对数据稀缺挑战的不同方面。少样本学习允许从最少的标记示例中进行适应，使系统可以根据用户提供的少量样本进行个性化，而不是需要大量的训练数据集。流式更新适应随着时间的推移逐渐到达的数据，使设备在正常操作中遇到新模式时能够持续适应，而无需收集和存储大量批次。经验重放通过智能重用最大化从有限数据中的学习，多次重放重要示例以从稀缺的训练数据中提取最大的学习信号。数据压缩在保留学习信号的同时减少内存需求，使系统能够在边缘设备的严格内存约束内维护重放缓冲区和训练历史。

每种技术都针对数据约束问题的不同方面，即使在传统监督学习失败的情况下也能实现稳健的学习。

### 少样本学习和数据流

在传统的机器学习工作流程中，有效的训练通常需要大量的标记数据集，这些数据集经过精心策划和预处理，以确保足够的多样性和平衡。相比之下，设备上的学习通常必须从少量本地示例开始——这些示例通过用户交互或环境感知被动收集，并且很少以监督方式标记。这些限制促使两种互补的适应策略：少样本学习，其中模型从一个小而静态的示例集中泛化，以及流式适应，其中更新随着数据的到达而持续进行。

少样本适应在设备观察少量标记或弱标记实例的新任务或用户条件下尤其相关 (Yaqing Wang 等人 2020)。在这种情况下，在不过度拟合的情况下进行所有模型参数的全量微调往往是不可行的。相反，采用如仅更新偏差、适配器模块或基于原型的分类等方法来利用有限的数据，同时最小化记忆能力。用 <semantics><mrow><mi>D</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">D = \{(x_i, y_i)\}_{i=1}^K</annotation></semantics> 表示在设备上收集的标记示例的 <semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics> 个少样本数据集。目标是更新模型参数 <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics> 以在以下约束条件下提高任务性能：

+   有限的梯度步数：<semantics><mrow><mi>T</mi><mo>≪</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">T \ll 100</annotation></semantics>

+   限制内存占用：<semantics><mrow><mo stretchy="false" form="postfix">∥</mo><msub><mi>θ</mi><mtext mathvariant="normal">updated</mtext></msub><mo stretchy="false" form="postfix">∥</mo><mo>≪</mo><mo stretchy="false" form="postfix">∥</mo><mi>θ</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">\|\theta_{\text{updated}}\| \ll \|\theta\|</annotation></semantics>

+   保持先前任务知识（以避免灾难性遗忘）

关键词检测 (KWS) 系统是少样本适应在现实世界、设备部署中的具体例子 (Warden 2018)。这些模型用于检测固定短语，包括“Hey Siri”23 或“OK Google”等短语，具有低延迟和高可靠性。一个典型的 KWS 模型由一个预训练的声学编码器（例如，一个小型卷积或循环网络，将输入音频转换为嵌入空间）和一个轻量级分类器组成。在商业系统中，编码器使用跨多种语言和说话人的数千小时标记语音进行集中训练。然而，由于数据稀缺和隐私问题，支持自定义唤醒词（例如，“Hey Jarvis”）或适应代表性不足的口音和方言通常通过集中训练是不可行的。

少样本自适应通过仅微调输出分类器或一小部分参数（包括偏差项），使用仅从设备上收集的几个示例语句来解决此问题。例如，用户可能提供 5-10 个自定义唤醒词的录音。然后，这些样本用于在本地更新模型，同时主编码器保持冻结以保持泛化并减少内存开销。这允许个性化，而无需额外的标记数据或向云端传输私人音频。

这种方法不仅计算效率高，而且与保护隐私的设计原则相一致。因为只有输出层被更新，通常涉及简单的梯度步骤或原型计算，总的内存占用和运行时计算与移动设备或甚至微控制器兼容。这使得关键词唤醒（KWS）成为边缘少样本学习的典型案例研究，在该研究中，系统必须在严格的约束下运行，同时提供用户特定的性能。

除了静态少样本学习之外，许多设备上的场景还可以从流式自适应中受益，其中模型必须随着新数据的到来逐步学习（Hayes 等人 2020）。流式自适应将这一想法推广到连续、异步的设置中，其中数据随时间逐步到达。用<semantics><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>x</mi><mi>t</mi></msub><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>∞</mi></msubsup></mrow><annotation encoding="application/x-tex">\{x_t\}_{t=1}^{\infty}</annotation></semantics>表示一个观察流。在流式设置中，模型必须在观察每个新输入后更新自己，通常没有访问先前数据，并且内存和计算有限。模型更新可以写成通用形式：<semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><msub><mi>η</mi><mi>t</mi></msub><mi>∇</mi><mi>ℒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>;</mo><msub><mi>θ</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\theta_{t+1} = \theta_t - \eta_t \nabla \mathcal{L}(x_t; \theta_t)</annotation></semantics> 其中 <semantics><msub><mi>η</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\eta_t</annotation></semantics> 是时间 <semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics> 的学习率。这种自适应形式对输入分布中的噪声和漂移敏感，因此通常采用学习率衰减、元学习初始化或更新门控等机制来提高稳定性。

除了关键词语音识别（KWS）之外，这些策略的实际例子比比皆是。在可穿戴健康设备中，一个用于分类身体活动的模型可能从一个通用分类器开始，仅使用少量标记的活动片段来适应用户的特定运动模式。在智能助手中，用户语音配置文件会随着时间的推移通过持续的语音输入进行微调，即使没有明确监督的情况下也是如此。在这种情况下，包括纠正、重复或下游任务成功在内的本地反馈可以作为指导学习的隐含信号。

少样本和流式适应突显了从传统训练流程向在不确定性下的数据高效、实时学习的转变。它们构成了更高级记忆和重放策略的基础，我们将在下一部分进行探讨。

### 经验重放

经验重放通过维护先前学习阶段中代表性示例的缓冲区，在连续学习场景中解决了灾难性遗忘的挑战——即学习新任务导致模型忘记先前学习的信息。这种技术最初是为强化学习开发的（Mnih 等人 2015），在设备端学习中证明至关重要，因为顺序数据流可能导致模型过度拟合到最近的示例。

与依赖于大量数据集和广泛计算资源的服务器端重放策略不同，设备端重放必须在极其有限的容量下运行，通常只有几十或几百个样本，并且必须避免干扰用户体验（Rolnick 等人 2019）。缓冲区可能只存储压缩特征或精炼摘要，并且更新必须具有机会性（例如，在空闲周期或充电期间）。这些系统级约束重塑了在嵌入式机器学习背景下重放的实施和评估方式。

用<semantics><mi>ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics>表示一个内存缓冲区，该缓冲区保留训练示例的固定大小子集。在时间步<semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics>，模型接收一个新的数据点<semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x_t, y_t)</annotation></semantics>并将其添加到<semantics><mi>ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics>中。然后，基于重放的自适应更新从<semantics><mi>ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics>中采样一个批次<semantics><mrow><mo stretchy="false" form="prefix">{</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup></mrow><annotation encoding="application/x-tex">\{(x_i, y_i)\}_{i=1}^{k}</annotation></semantics>，并应用梯度步：<semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><msub><mi>∇</mi><mi>θ</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>ℒ</mi><mrow><mo stretchy="true" form="postfix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo>;</mo><msub><mi>θ</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow> <annotation encoding="application/x-tex">\theta_{t+1} = \theta_t - \eta \nabla_\theta \left[ \frac{1}{k} \sum_{i=1}^{k} \mathcal{L}(x_i, y_i; \theta_t) \right]</annotation></semantics>其中<semantics><msub><mi>θ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\theta_t</annotation></semantics>是模型参数，<semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics>是学习率，<semantics><mi>ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics>是损失函数。随着时间的推移，这种重放机制允许模型在结合新信息的同时加强先前知识。

一种实用的设备上实现可能使用环形缓冲区来存储一小组压缩的特征向量，而不是完整的输入示例。如列表 14.4 所示，伪代码说明了为受限环境设计的最小重放缓冲区。

列表 14.4：**重放缓冲区**：实现了一种循环存储机制，用于在受限环境中进行高效的内存管理。这种方法允许模型有效地保留和从最近的数据点中采样，在利用历史信息的同时，结合新的见解。

```py
# Replay Buffer Techniques
class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []
        self.index = 0

    def store(self, feature_vec, label):
        if len(self.buffer) < self.capacity:
            self.buffer.append((feature_vec, label))
        else:
            self.buffer[self.index] = (feature_vec, label)
        self.index = (self.index + 1) % self.capacity

    def sample(self, k):
        return random.sample(self.buffer, min(k, len(self.buffer)))
```

此实现维护一个固定容量的循环缓冲区，存储压缩表示（例如，最后一层的嵌入）和相关标签。此类缓冲区对于在不违反内存或能耗预算的情况下重放适应更新非常有用。

在 TinyML 应用 24 中，经验重放已被应用于诸如手势识别等问题，在这些问题中，设备必须持续改进预测，同时每天观察少量事件。而不是直接在流数据上训练，设备存储最近手势的代表性特征向量，并定期使用它们来微调分类边界。同样，在设备端关键词检测中，重放过去的语音可以改善唤醒词检测的准确性，而无需将音频数据传输到设备外。

虽然经验重放提高了数据稀疏或非平稳环境中的稳定性，但它引入了几个权衡。存储原始输入可能会违反隐私限制或超出存储预算，尤其是在视觉和音频应用中。从特征向量重放可以减少内存使用，但可能会限制上游层的梯度丰富性。对持久性闪存的写周期，这对于嵌入式设备上的长期存储通常是必要的，也可能引起磨损均衡问题。这些限制要求仔细协同设计内存使用策略、重放频率和特征选择策略，尤其是在持续部署场景中。

### 数据压缩

在许多设备端学习场景中，原始的训练数据可能太大、噪声过多或冗余，难以有效存储和处理。这促使人们使用压缩数据表示，将原始输入转换为低维嵌入或紧凑编码，在最小化内存和计算成本的同时保留显著信息。

压缩表示实现了两个互补的目标。首先，它们减少了存储数据的占用空间，使得设备能够在紧张的内存预算下维持更长的历史记录或重放缓冲区（Sanh 等人 2019）。其次，通过将原始输入投影到更结构化的特征空间，简化了学习任务，这些特征空间通常通过预训练或元学习获得，其中在最小监督下可以进行有效的适应。

一种常见的方法是使用预训练的特征提取器对数据点进行编码，并丢弃原始的高维输入。例如，一个图像 <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics> 可能会通过一个卷积神经网络（CNN）来生成一个嵌入向量 <semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">z_i = f(x_i)</annotation></semantics>，其中 <semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>⋅</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation></semantics> 是一个固定的特征编码器。这个嵌入以紧凑的表示捕捉视觉结构（例如，形状、纹理或空间布局），通常在 64 到 512 维之间，适合轻量级下游适应。

从数学上讲，训练可以通过压缩样本进行，使用轻量级的解码器或投影头处理 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(z_i, y_i)</annotation></semantics>。让 <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics> 代表这个解码模型的可训练参数，这通常是一个小型神经网络，它将压缩表示映射到输出预测。每当呈现一个示例时，模型参数都会通过梯度下降进行更新：<semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><msub><mi>∇</mi><mi>θ</mi></msub><mi>ℒ</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>;</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow> <annotation encoding="application/x-tex">\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}\big(g(z_i; \theta), y_i\big)</annotation></semantics> 这里：

+   <semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics> 是第 <semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics> 个输入的压缩表示，

+   <semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics> 是相应的标签或监督信号，

+   <semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>;</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(z_i; \theta)</annotation></semantics> 是解码器的预测，

+   <semantics><mi>ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics> 是衡量预测误差的损失函数，

+   <semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics> 是学习率，并且

+   <semantics><msub><mi>∇</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\nabla_\theta</annotation></semantics> 表示相对于参数 <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics> 的梯度。

这种公式突出了为什么只需要训练一个紧凑的解码器模型，其参数集为 <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>，这使得即使在内存和计算有限的情况下，学习过程也是可行的。

高级方法通过学习表示数据使用低秩或稀疏系数矩阵的离散或稀疏字典，超越了固定编码器。一个传感器轨迹数据集可以被分解为 <semantics><mrow><mi>X</mi><mo>≈</mo><mi>D</mi><mi>C</mi></mrow><annotation encoding="application/x-tex">X \approx DC</annotation></semantics>，其中 <semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics> 是基模式字典，而 <semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics> 是指示每个示例中哪些模式是活跃的块稀疏系数矩阵。通过仅更新少量字典原子或系数，模型以最小的开销进行适应。

在隐私敏感的环境中，压缩表示被证明是有用的，因为它们允许在编码后丢弃或模糊原始数据。压缩作为一种隐式正则化器，在只有少量训练示例可用时，可以平滑学习过程并减轻过拟合。

在实践中，这些策略已经应用于诸如关键词检测等域，其中原始音频信号首先被转换为梅尔频率倒谱系数（MFCCs）25——这是语音功率谱的紧凑、有损表示。这些 MFCC 向量作为压缩输入用于下游模型，仅使用几 KB 的内存即可实现局部自适应。

### 数据效率策略比较

本节介绍的技术（少量样本学习、经验回放和压缩数据表示）为在数据稀缺或流式传输时在设备上适应模型提供了策略。它们在不同的假设和约束下运行，其有效性取决于系统级因素，如内存容量、数据可用性、任务结构和隐私要求。

当有少量但信息丰富的标记示例集可用时，少量样本适应表现出色，尤其是在需要个性化或快速任务特定调整时。它最小化了计算和数据需求，但它的有效性取决于预训练表示的质量以及初始模型与本地任务之间的对齐。

经验回放通过减轻遗忘和改善稳定性来处理持续适应，尤其是在非平稳环境中。它允许重用过去的数据，但需要存储示例的内存和周期性更新的计算周期。重放缓冲区也可能引起隐私或长期存储的担忧，尤其是在存储空间有限或闪存写入周期有限的设备上。

压缩数据表示通过将原始数据转换为紧凑的特征空间来减少学习占用的空间。这种方法支持更长时间的经验保留和高效的微调，尤其是在只有轻量级头部可训练的情况下。压缩可能会引入信息损失，如果固定编码器没有与部署条件良好对齐，它们可能无法捕捉到与任务相关的变化。表 14.4 总结了关键权衡：

表 14.4：**设备上学习权衡**：少量样本适应通过利用小型标记数据集在数据效率和模型个性化之间取得平衡，但需要在资源受限的设备上部署时仔细考虑内存和计算约束。该表总结了根据应用需求和可用资源选择适当的设备上学习技术时的关键考虑因素。

| **技术** | **数据需求** | **内存/计算开销** | **适用场景** |
| --- | --- | --- | --- |
| **少量样本适应** | 小型标记集（K 次射击） | 低 | 个性化，快速设备微调 |
| **经验回放** | 流数据 | 中等（缓冲区与更新） | 非平稳数据，漂移下的稳定性 |
| **压缩表示** | 未标记或编码数据 | 低到中等 | 内存受限设备，隐私敏感环境 |

在实践中，这些方法不是相互排斥的。许多现实世界的系统将它们结合起来，以实现稳健、高效的适应。例如，一个关键词检测系统可能使用压缩音频特征（例如，MFCCs），从一个小支持集中微调少量参数，并维护过去嵌入的重放缓冲区以进行持续优化。

这些策略共同体现了设备学习的核心挑战：在数据、计算和内存的持续约束下实现可靠的模型改进。

## 联邦学习

上述单个设备技术——从仅偏差更新到复杂的适配器模块——创造了强大的个性化能力，但在规模部署时揭示了根本性的限制。虽然每个设备可以有效地适应本地条件，但这些孤立的改进不能惠及更广泛的设备群体。关于模型鲁棒性、适应策略和故障模式的宝贵见解仍然被困在单个设备上，失去了使集中式训练有效的集体智慧。

这种限制在需要个人化和人口规模学习的场景中变得明显。模型适应和数据效率技术使单个设备能够在资源约束下有效地学习，但它们也揭示了一个基本的协调挑战，当复杂的本地学习遇到分布式部署的现实时出现。

考虑一个部署在 1000 万户家庭的语音助手。每个设备都根据其用户的语音、口音和词汇进行本地适应。设备 A 了解到“数据”发音为/ˈdeɪtə/，设备 B 了解到/ˈdætə/。设备 C 经常遇到罕见的短语“机器学习”（技术家庭），而设备 D 从未见过（非技术家庭）。经过六个月的本地适应：

+   每个设备擅长其特定用户的模式，但只有这些模式

+   罕见词汇在某些设备上学习，在其他设备上遗忘

+   本地偏差在没有更广泛的群体纠正的情况下积累

+   在一个设备上发现的宝贵见解对其他设备没有好处

虽然单个设备上的学习非常强大，但当设备在孤立状态下运行时，它面临着根本性的限制。每个设备只能观察到整个数据分布的一个狭窄切片，限制了泛化。设备的能力差异很大，在人口中造成了学习不平衡。在一个设备上学习的宝贵见解不能惠及其他设备，降低了整体系统智能。没有协调，模型可能会随着时间的推移而发散或退化，这是由于本地偏差造成的。

联邦学习成为解决分布式协调约束的解决方案。它实现了隐私保护的合作，其中设备在不共享原始数据的情况下为集体智慧做出贡献。与其将单个设备学习和协调学习视为不同的范例，联邦学习代表了当设备在规模上部署时的自然演变。这种方法将数据本地的约束从限制转变为隐私特性，允许系统从人口规模的数据中学习，同时保持个人信息的安全。

这里的隐私要求直接关联到安全和隐私原则，这些原则在生产部署中变得至关重要。与其将单个设备学习和协调学习视为不同的范例，不如说联邦学习是在大规模部署时设备系统的自然演变。

**联邦学习**是一种去中心化的训练方法，在这种方法中，分布式设备通过使用本地数据协同训练一个**共享模型**，而只交换**模型更新**，通过数据本地化来保护**隐私**。

为了更好地理解联邦学习的作用，将其与其他学习范例进行对比是有用的。图 14.6 展示了离线学习、设备学习和联邦学习之间的区别。在传统的离线学习中，所有数据都集中收集和处理。模型在云中使用精选的数据集进行训练，然后部署到边缘设备而无需进一步适应。相比之下，设备学习允许使用设备本身生成的数据进行本地模型适应，支持个性化但孤立——不与用户共享见解。联邦学习通过允许本地训练同时协调全球更新来弥合这两个极端。它通过保持原始数据本地化来保留数据隐私，同时通过聚合来自许多设备的更新来从分布式模型改进中受益。

![图片](img/file225.svg)

图 14.6：联邦学习通过协调分布式设备上的本地训练，在数据隐私和集体模型改进之间取得平衡，与离线学习的集中式方法或设备学习的孤立适应方法不同。此图对比了每种范例如何处理数据位置和模型更新策略，揭示了个性化、数据安全和全球知识共享之间的权衡。

本节探讨了在移动和嵌入式系统背景下联邦学习的原理和实际考虑。它首先概述了标准的 FL 协议及其系统影响。然后讨论设备参与约束、通信高效的更新机制以及个性化学习的策略。在整个过程中，重点始终是如何通过使分布式模型训练跨越各种资源和受限的硬件平台来扩展设备学习的范围。

### 隐私保护协同学习

联邦学习（FL）是一种去中心化的范式，用于在设备群体中训练机器学习模型，而不需要将原始数据传输到中央服务器（McMahan 等人 2017c）。与传统集中式训练流程不同，后者需要在单个位置聚合所有训练数据，联邦学习将训练过程本身进行分布。每个参与设备根据其本地数据计算更新，并通过聚合协议向全局模型做出贡献，通常由中央服务器协调。这种训练架构的转变与移动、边缘和嵌入式系统的需求紧密一致，在这些系统中，隐私、通信成本和系统异构性对集中式方法施加了重大限制。

如前所述的应用领域所示——从 Gboard 的键盘个性化到可穿戴健康监测再到语音界面——联邦学习弥合了模型改进与本章中建立的系统级约束之间的差距。它实现了设备学习的个性化、隐私和连接性优势，同时通过协调但分布的训练来应对资源约束。然而，这些优势也引入了新的挑战，包括客户端的可变性、通信效率和非-IID 数据分布，这些都需要专门的协议和协调机制。

在此基础上，本节剩余部分探讨了定义设备上联邦学习的核心技术和权衡，检查了治理跨设备协调的核心学习协议，并研究了调度、通信效率和个性化的策略。

### 学习协议

联邦学习协议定义了设备协作训练共享模型的规则和机制。这些协议规定了如何计算、聚合和通信本地更新，以及设备如何参与训练过程。协议的选择对系统性能、通信开销和模型收敛有重大影响。

在本节中，我们概述了联邦学习协议的核心组件，包括本地训练、聚合方法和通信策略。我们还讨论了不同方法相关的权衡及其对设备上机器学习系统的影响。

#### 本地训练

本地训练是指单个设备根据其本地数据计算模型更新的过程。在联邦学习中，这一步骤至关重要，因为它允许设备在不传输原始数据的情况下，将共享模型适应其特定环境。本地训练过程包括以下步骤：

1.  **模型初始化**：每个设备都会初始化其本地模型参数，通常是通过从服务器下载最新的全局模型来实现的。

1.  **本地数据采样**：设备为其本地数据采样一个子集进行训练。这些数据可能是非-IID 的，这意味着它们可能不在设备之间均匀分布。

1.  **本地训练**：设备在其本地数据上执行多次训练迭代，根据计算出的梯度更新模型参数。

1.  **模型更新**：在本地训练后，设备计算模型更新（例如，更新后的参数与初始参数之间的差异）并准备将其发送到服务器。

1.  **通信**：设备通过通常使用安全通信通道来保护用户隐私的方式将模型更新传输到服务器。

1.  **模型聚合**：服务器从多个设备聚合更新以生成一个新的全局模型，然后将其分发给参与设备。

此过程会迭代重复，设备定期下载最新的全局模型并进行本地训练。这些更新的频率可以根据系统约束、设备可用性和通信成本而变化。

#### 联邦聚合协议

联邦学习的核心是一个协调机制，它允许许多设备，每个设备只能访问一个小型本地数据集，共同训练一个共享模型。这是通过一个协议实现的，其中客户端设备在其本地数据上执行本地训练并将模型更新传输到中央服务器。服务器聚合这些更新以细化全局模型，然后将其重新分发给客户端进行下一轮训练。这种循环过程将学习过程与集中式数据收集解耦，非常适合本章中描述的移动和边缘环境，在这些环境中用户数据是私密的，带宽受限，设备参与是间歇性的。

此过程最广泛使用的基线是联邦平均（FedAvg）26，它已成为联邦学习的标准算法（McMahan 等人 2017c）。在 FedAvg 中，每个设备使用其私有数据上的随机梯度下降（SGD）训练其模型的本地副本。

形式上，用<semantics><msub><mi>𝒟</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathcal{D}_k</annotation></semantics>表示客户端<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>上的本地数据集，用<semantics><msubsup><mi>θ</mi><mi>k</mi><mi>t</mi></msubsup><annotation encoding="application/x-tex">\theta_k^t</annotation></semantics>表示客户端<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>在第<semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics>轮次的模型参数。每个客户端在其本地数据上执行<semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics>步 SGD，得到一个更新<semantics><msubsup><mi>θ</mi><mi>k</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\theta_k^{t+1}</annotation></semantics>。然后中心服务器将这些更新汇总为：<semantics><mrow><msup><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mfrac><msub><mi>n</mi><mi>k</mi></msub><mi>n</mi></mfrac><msubsup><mi>θ</mi><mi>k</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow> <annotation encoding="application/x-tex">\theta^{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} \theta_k^{t+1}</annotation></semantics> 其中 <semantics><mrow><msub><mi>n</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>𝒟</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">n_k = |\mathcal{D}_k|</annotation></semantics> 是设备<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>上的样本数量，<semantics><mrow><mi>n</mi><mo>=</mo><msub><mo>∑</mo><mi>k</mi></msub><msub><mi>n</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">n = \sum_k n_k</annotation></semantics> 是参与客户端的总样本数量，<semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>是当前轮次中活跃设备的数量。

这种循环协调协议构成了联邦学习的基础，如图图 14.7 所示，该图阐明了核心 FedAvg 过程：

![图片](img/file226.svg)

图 14.7：**联邦平均周期**：四个步骤的协调协议，在保持数据隐私的同时实现分布式训练。（1）服务器将全局模型分发到参与客户端，（2）客户端使用多个 SGD 步骤在其私有数据上本地训练，（3）客户端将更新的模型权重（而非原始数据）发送回服务器，（4）服务器对客户端的更新进行加权平均，以创建新的全局模型。

这种基本结构引入了许多设计选择和权衡。本地步骤的数量<semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics>影响计算和通信之间的平衡：较大的<semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics>会减少通信频率，但如果本地数据分布差异太大，则存在发散的风险。参与客户端的选择影响收敛稳定性和公平性。在实际部署中，并非所有设备在所有时间都可用，硬件能力可能差异很大，需要强大的参与调度和容错能力。

#### 客户端调度

联邦学习基于这样的假设：持有本地数据的客户端、设备，会定期可用以参与训练轮次。在现实世界的系统中，客户端的可用性是间歇性和变化的。设备可能会关闭，断开电源，缺乏网络访问，或者在其他情况下无法在任何给定时间参与。因此，客户端调度在分布式学习的有效性和效率中起着核心作用。

在基本水平上，联邦机器学习系统定义了参与资格标准。设备必须满足最低要求，如插电、连接到 Wi-Fi 和空闲，以避免干扰用户体验或耗尽电池资源。这些标准决定了在任意训练轮次中被认为是“可用”的总人口子集。

除了这些操作过滤器之外，设备在硬件能力、数据可用性和网络条件方面也存在差异。一些智能手机包含许多与当前任务相关的最新示例，而其他设备则包含过时或不相关的数据。网络带宽和上传速度可能因地理位置和运营商基础设施而大相径庭。因此，随机选择客户端可能导致对底层数据分布的覆盖不足和模型收敛不稳定。

可用性驱动的选择引入了参与偏差：具有有利条件（包括频繁充电、高端硬件和稳定的连接）的客户端更有可能反复参与，而其他客户端则可能系统地代表性不足。这可能导致生成的模型偏向于特权子集的行为和偏好，从而引发公平性和泛化问题。

当检查实际部署统计时，参与偏差的严重性变得明显。联邦学习的部署研究表明，最活跃的 10%的设备可以贡献超过 50%的训练轮次，而底部的 50%的设备可能根本不参与。这形成了一个反馈循环：模型变得越来越优化，以适应高端设备和稳定连接的用户，这可能会降低最需要适应的资源受限用户的性能。一个键盘预测模型可能会偏向于使用旗舰手机并夜间充电的用户的手指打字模式，从而忽略了来自预算设备或非规律充电模式用户的语言变化。

为了应对这些挑战，系统必须在调度效率和客户多样性之间取得平衡。一个关键的方法是采用分层或配额制的抽样，以确保不同群体中代表性的客户参与。一些系统实施了“公平预算”，以跟踪累积参与情况，并在可用时积极优先考虑代表性不足的设备。其他系统使用重要性抽样技术，根据估计的群体统计数据重新加权贡献，而不是根据原始参与率。例如，基于异步缓冲区的技术允许参与客户独立贡献模型更新，而不需要在每一轮中都进行同步协调（Nguyen 等人 2021）。该模型已被扩展以包含陈旧度感知（Rodio 和 Neglia 2024）和公平机制（J. Ma 等人 2024），防止过度活跃的客户在训练过程中可能产生的偏差。

为了应对这些挑战，联邦机器学习系统实施了自适应客户选择策略。这包括优先考虑代表性不足的数据类型的客户，针对较少采样的地理或人口统计特征，以及使用历史参与数据来强制执行公平约束。系统采用预测建模来预测未来客户的可用性或成功率，从而提高训练吞吐量。

被选中的客户在其私有数据上执行一个或多个本地训练步骤，并将他们的模型更新传输到中央服务器。这些更新被汇总形成一个新的全局模型。通常，这种汇总是加权的，其中每个客户的贡献按比例缩放，例如，在训练期间使用的本地示例数量，然后再进行平均。这确保了具有更代表性或更大数据集的客户对全局模型产生成比例的影响。

这些调度决策直接影响系统性能。它们影响收敛速度、模型泛化、能耗和整体用户体验。不良的调度可能导致过多的延迟者、过度拟合到狭窄的客户端段或浪费计算。因此，客户端调度不仅仅是物流问题；它是联邦学习系统设计中一个核心组件，需要算法洞察和基础设施级别的协调。

#### 带宽感知更新压缩

联邦机器学习系统中的一个主要瓶颈是边缘客户端和中央服务器之间通信的成本。在每轮训练后传输完整的模型权重或梯度可能会耗尽带宽和能源预算，尤其是对于在受限无线链路上运行的移动或嵌入式设备 27。为了解决这个问题，已经开发了一系列技术来减少通信开销，同时保持学习效率。

这些技术主要分为三个类别：模型压缩、选择性更新共享和架构分区。

模型压缩方法旨在通过量化 28、稀疏化或子采样来减少传输更新的大小。客户端不是发送全精度梯度，而是传输 8 位量化更新或仅通信具有最高幅度的前<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>梯度元素 29。

选择性更新共享通过仅传输模型参数或更新的子集进一步减少通信。在分层选择性共享中，客户端仅更新某些层，通常是最终的分类器或适配器模块，同时保持大多数骨干层冻结。这减少了上传成本和将共享表示过度拟合到非代表性客户端数据的风险。

分割模型和架构分区将模型分为共享的全局组件和私有的本地组件。客户端独立训练和维护其私有模块，同时仅与服务器同步共享部分。这允许进行用户特定的个性化，同时最小化通信和隐私泄露。

所有这些方法都在联邦聚合协议的背景下运行。聚合的标准基线是联邦平均（FedAvg），其中服务器通过计算在给定轮次中接收到的客户端更新的加权平均值来更新全局模型。令 <semantics><msub><mi>𝒦</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathcal{K}_t</annotation></semantics> 表示第 <semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics> 轮次中参与的客户端集合，并令 <semantics><msubsup><mi>θ</mi><mi>k</mi><mi>t</mi></msubsup><annotation encoding="application/x-tex">\theta_k^t</annotation></semantics> 表示客户端 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics> 的本地更新模型参数。服务器计算新的全局模型 <semantics><msup><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\theta^{t+1}</annotation></semantics> 如下： <semantics><mrow><msup><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><munder><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><msub><mi>𝒦</mi><mi>t</mi></msub></mrow></munder><mfrac><msub><mi>n</mi><mi>k</mi></msub><msub><mi>n</mi><msub><mi>𝒦</mi><mi>t</mi></msub></msub></mfrac><msubsup><mi>θ</mi><mi>k</mi><mi>t</mi></msubsup></mrow> <annotation encoding="application/x-tex">\theta^{t+1} = \sum_{k \in \mathcal{K}_t} \frac{n_k}{n_{\mathcal{K}_t}} \theta_k^t</annotation></semantics>

在这里，<semantics><msub><mi>n</mi><mi>k</mi></msub><annotation encoding="application/x-tex">n_k</annotation></semantics> 表示客户端 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics> 的本地训练样本数量，而 <semantics><mrow><msub><mi>n</mi><msub><mi>𝒦</mi><mi>t</mi></msub></msub><mo>=</mo><msub><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><msub><mi>𝒦</mi><mi>t</mi></msub></mrow></msub><msub><mi>n</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">n_{\mathcal{K}_t} = \sum_{k \in \mathcal{K}_t} n_k</annotation></semantics> 表示所有参与客户端的总训练样本数量。这种基于数据权重的聚合确保了拥有更多训练数据的客户端对全局模型产生成比例更大的影响，同时也考虑了部分参与和异构数据量。

然而，通信高效的更新可能会引入权衡。压缩可能会降低梯度精度，选择性更新可能会限制模型容量，而分割架构可能会复杂化协调。因此，有效的联邦学习需要仔细平衡带宽限制、隐私关注和收敛动态——这种平衡在很大程度上取决于客户端群体的能力和多样性。

#### 联邦个性化

虽然压缩和通信策略提高了可扩展性，但它们并没有解决全球联邦学习范式的一个重要限制——无法捕捉用户特定的变化。在实际部署中，设备通常会观察到不同且异构的数据分布。当统一应用于不同用户时，一个通用的全局模型可能会表现不佳。这促使了个性化联邦学习的需求，在这种学习中，本地模型会根据用户特定的数据进行调整，同时不损害全球协调的好处。

令 <semantics><msub><mi>θ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\theta_k</annotation></semantics> 表示客户端 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics> 上的模型参数，以及 <semantics><msub><mi>θ</mi><mtext mathvariant="normal">global</mtext></msub><annotation encoding="application/x-tex">\theta_{\text{global}}</annotation></semantics> 表示汇总的全局模型。传统的联邦学习旨在最小化全局目标：<semantics><mrow><munder><mo>min</mo><mi>θ</mi></munder><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>w</mi><mi>k</mi></msub><msub><mi>ℒ</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\min_\theta \sum_{k=1}^K w_k \mathcal{L}_k(\theta)</annotation></semantics> 其中 <semantics><mrow><msub><mi>ℒ</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_k(\theta)</annotation></semantics> 是客户端 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics> 上的局部损失，而 <semantics><msub><mi>w</mi><mi>k</mi></msub><annotation encoding="application/x-tex">w_k</annotation></semantics> 是一个权重因子（例如，与本地数据集大小成比例）。然而，这种公式假设单个模型 <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics> 可以很好地服务于所有用户。在实践中，局部损失景观 <semantics><msub><mi>ℒ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_k</annotation></semantics> 在客户端之间往往差异很大，反映了非独立同分布的数据分布和不同的任务需求。

个性化修改了这个目标，允许每个客户端维护自己的适应参数 <semantics><msub><mi>θ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\theta_k</annotation></semantics>，这些参数与全局模型和本地数据都进行了优化：<semantics><mrow><munder><mo>min</mo><mrow><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>θ</mi><mi>K</mi></msub></mrow></munder><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ℒ</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>λ</mi><mo>⋅</mo><mi>ℛ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>k</mi></msub><mo>,</mo><msub><mi>θ</mi><mtext mathvariant="normal">global</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\min_{\theta_1, \ldots, \theta_K} \sum_{k=1}^K \left( \mathcal{L}_k(\theta_k) + \lambda \cdot \mathcal{R}(\theta_k, \theta_{\text{global}}) \right)</annotation></semantics>

在这里，<semantics><mi>ℛ</mi><annotation encoding="application/x-tex">\mathcal{R}</annotation></semantics> 是一个正则化项，它惩罚与全局模型的偏差，而 <semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics> 控制这种惩罚的强度。这种公式允许局部模型根据需要偏离，同时仍然从全局协调中受益。

实际应用案例说明了这种方法的重要性。考虑一个可穿戴的健康监测器，它通过追踪生理信号来分类身体活动。虽然全局模型可能在整个人群中表现良好，但个别用户表现出独特的运动模式、步态特征或传感器放置。对最终分类层或低秩适配器的个性化微调可以提升准确性，尤其是在处理罕见或特定用户类别时。

出现了几种个性化策略来解决计算开销、隐私和适应速度之间的权衡。一种广泛使用的方法是本地微调，其中每个客户端下载最新的全局模型，并使用其私有数据执行少量梯度步骤。虽然这种方法简单且保护隐私，但当全局模型与客户端的数据分布不匹配或本地数据集极其有限时，它可能产生次优结果。

另一种有效技术涉及个性化层，其中模型被划分为共享骨干和轻量级、客户端特定的头部——通常是最终的分类层(Arivazhagan et al. 2019)。仅在设备上更新头部，显著减少了内存使用和训练时间。这种方法特别适合于客户端之间主要差异在于输出类别或决策边界的情况。

集群联邦学习通过根据客户端数据或性能特征的相似性对客户端进行分组，并为每个集群训练单独的模型，提供了一种替代方案。这种策略可以在同质子群体中提高准确性，但引入了额外的系统复杂性，可能需要交换元数据以确定群体成员资格。

最后，元学习方法，如模型无关元学习（MAML），旨在产生一个全局模型初始化，该初始化可以通过仅进行少量本地更新快速适应新任务(Finn, Abbeel, and Levine 2017)。当客户端数据有限或在频繁分布变化的环境中操作时，这项技术特别有用。这些策略反映了权衡空间中的不同点。这些策略在系统影响方面有所不同，包括计算开销、隐私保证和适应延迟。表 14.5 总结了权衡。

表 14.5：**个性化权衡**：联邦学习策略在个性化与系统成本之间取得平衡，影响计算开销、隐私保护和针对不同客户端群体的适应速度。本表总结了本地微调、集群学习和元学习如何各自在这个权衡空间中导航，在考虑实际部署约束的同时实现定制模型。

| **策略** | **个性化机制** | **计算开销** | **隐私保护** | **适应速度** |
| --- | --- | --- | --- | --- |
| **本地微调** | 在聚合后的本地损失上进行梯度下降 | 低到中等 | 高（无数据共享） | 快速（少量步骤） |
| **个性化层** | 分割模型：共享基础 + 用户特定头部 | 中等 | 高 | 快速（训练小型头部） |
| **集群联邦学习** | 根据数据相似性分组客户端，按组训练 | 中等到高 | 中等（组元数据） | 中等 |
| **元学习** | 训练以快速适应不同任务/设备 | 高（元目标） | 高 | 非常快（少样本） |

选择适当个性化方法取决于部署约束、数据特征以及所需在准确性、隐私和计算效率之间的平衡。在实践中，通常采用结合多种策略的混合方法，包括在个性化头部之上进行本地微调，以在异构设备上实现稳健的性能。

#### 联邦隐私

虽然联邦学习通常是由隐私关注所驱动，因为它涉及将原始数据本地化而不是传输到中央服务器，但这种范式引入了自己的一套安全和隐私风险。尽管设备不会共享它们的原始数据，但传输的模型更新（如梯度或权重变化）可能会无意中泄露关于底层私有数据的详细信息。模型反演攻击和成员推断攻击等技术表明，攻击者可能通过分析这些更新部分重建或推断本地数据集的属性。

为了减轻这些风险，现代联邦机器学习系统通常采用保护措施。安全聚合协议确保单个模型更新被加密并以一种方式聚合，使得服务器只能观察到综合结果，而不是任何单个客户端的贡献。差分隐私 30 技术将精心校准的噪声注入到更新中，以数学上限制可以推断关于任何单个客户端数据的任何信息。

虽然这些技术增强了隐私性，但它们引入了额外的系统复杂性，并在模型效用、通信成本和鲁棒性之间引入了权衡。对这些攻击、防御及其影响的更深入探索需要专门覆盖分布式机器学习系统中的安全原则。

### 大规模设备编排

联邦学习将机器学习转化为一个巨大的分布式系统挑战，这远远超出了传统的算法考虑。协调数千或数百万具有间歇性连接的异构设备需要复杂的分布式系统协议，这些协议能够处理拜占庭故障、网络分区和前所未有的通信效率。这些挑战与数据中心分布式训练的受控环境根本不同，在那里高带宽网络和可靠的基础设施使得简单的协调协议变得简单。

#### 网络和带宽优化

通信瓶颈是联邦学习系统中主要的可扩展性约束。理解定量传输需求使得关于模型架构、更新压缩策略和确定系统可行性的客户端参与策略的设计决策具有原则性。

联邦通信层次结构揭示了分布式学习必须运行的严重带宽限制。对于典型的深度学习模型，每次训练轮次需要 10-500 MB 的全模型同步——这在实际中对于平均上传带宽仅为 5-50 Mbps 的有限上传带宽的移动网络来说是不可行的。梯度压缩通过量化（将 FP32 减少到 INT8）、稀疏化（仅传输非零梯度）和选择性梯度传输（仅发送最重要的更新）实现了 10-100<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的压缩。实际部署需要更加激进的 100-1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>压缩比率，将 100 MB 的模型压缩到可管理的 100 KB-1 MB 更新，这样移动设备可以在合理的时间内传输，而不会耗尽数据计划。通信频率在模型更新新鲜度和网络效率限制之间引入了一个关键的权衡——更频繁的更新能够更快地适应变化条件，但同时也限制了可持续的带宽消耗。

网络基础设施的限制直接影响到参与率和整个系统的可行性。现代 4G 网络在最佳条件下通常提供 5-50 Mbps 的上传速度（存在显著的地理和运营商差异），这意味着 8 MB 的模型更新需要 1.3-13 秒的持续传输。然而，现实世界的移动网络表现出极大的可变性：农村地区可能只有 1 Mbps 的上传速度，而城市 5G 部署则能实现 100+ Mbps。这种 100<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的网络能力差异需要自适应的通信策略，以优化最低共同连接性，同时使高能力设备能够更有效地做出贡献。

通信需求与参与率之间的关系表现出明显的阈值效应。实证研究表明，需要超过 10 MB 模型转移的联邦学习系统，其持续客户端参与率低于 10%，而保持更新低于 1 MB 的系统可以在多样化的移动人群中维持 40-60%的参与率。这种通信效率直接转化为模型质量的提升：更高的参与率提供了更好的统计多样性和更稳健的全局模型更新梯度估计。

高级压缩技术对于实际部署变得至关重要。梯度量化将精度从 FP32 降低到 INT8 甚至二进制表示，实现了 4-32<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的压缩，同时最小化精度损失。稀疏化技术只传输最大的梯度分量，利用神经网络更新中的自然稀疏性。Top-k 梯度选择进一步减少通信，只传输最重要的参数更新，而误差累积确保小梯度不会永久丢失。

#### 异步设备同步

联邦学习在分布式系统和机器学习的复杂交叉点上运行，既继承了这两个领域的根本挑战，又引入了由边缘设备的移动性、异构性和不可靠性带来的独特复杂性。

联邦学习必须应对比典型分布式系统挑战更广泛的拜占庭容错要求。设备故障频繁发生，因为客户端在训练轮次中崩溃、断电或断开连接，这比传统分布式训练中的服务器故障更为常见。恶意更新带来了安全担忧，因为敌对客户端可以故意提供损坏的梯度，这些梯度旨在降低全局模型性能或从聚合过程中提取私人信息。实现拜占庭容错平均的鲁棒聚合协议确保了系统可靠性，尽管这些协议引入了显著的计算开销。共识机制必须在没有传统分布式共识协议（如 Paxos 或 Raft）开销的情况下协调数百万不可靠的参与者，这些协议是为小型可靠服务器集群设计的。

网络分区对联邦协调协议提出了特别严峻的挑战。与传统在可靠数据中心网络中运行的分布式系统不同，联邦学习必须优雅地处理长时间客户端断开事件，在这些事件中，设备可能因旅行、在覆盖较差的区域或简单断电而离线数小时或数天。异步协调协议能够在缺少参与者的情况下继续训练进度，但必须仔细平衡陈旧性（接受可能过时的贡献）与新鲜性（优先处理最近但可能稀疏的更新）。

故障恢复和弹性策略是联邦学习基础设施的一个基本层。通过定期全局模型快照进行检查点同步，可以在服务器故障时进行恢复，并在检测到损坏的训练轮次时提供回滚点，尽管在数百万设备上检查点同步大型模型会引入大量的存储和通信开销。部分更新处理确保系统在大量客户端在训练过程中失败或断开连接时能够优雅地处理不完整的训练轮次，需要仔细的加权策略来防止偏向更可靠的设备群体。状态协调协议使客户端在长时间离线后（可能是几天或几周）能够高效地与当前全局模型重新同步，同时最小化可能压倒带宽受限设备的通信开销。动态负载均衡解决不均匀的客户端可用性模式，这些模式会创建计算热点，需要智能地在可用的参与者之间重新分配负载，以维持训练吞吐量，尽管参与率随时间变化。

联邦协调的异步性质在保持训练收敛保证方面引入了额外的复杂性。传统的同步训练假设所有参与者都完成每一轮，但联邦系统必须优雅地处理落后者和退出者。例如，FedAsync31 这样的技术可以实现异步聚合，其中服务器在客户端更新到达时持续更新全局模型，而有限的陈旧机制防止过时的更新破坏最近的进展。

#### 管理百万设备异构性

真实世界的联邦学习部署在多个维度上同时表现出极端的异构性：硬件能力、网络条件、数据分布和可用性模式。这种多维异构性从根本上挑战了传统分布式机器学习关于在相似条件下操作的同质参与者假设。

现实世界的联邦学习部署面临着多维度的设备异质性，这导致每个系统维度都存在极端的差异性。计算差异跨越了 1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的差异，这是在 35 TOPS 运行的旗舰智能手机和仅运行在 0.03 TOPS 的物联网微控制器之间的处理能力差异，从根本上限制了可以在不同设备层级上训练的模型。内存限制在设备类别之间表现出更为显著的 100-10,000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的差异，从微控制器上的 256KB 到高端智能手机上的 16 GB 不等，这决定了设备是否可以进行任何本地训练，或者必须完全依赖推理。能量限制迫使训练会话必须围绕充电模式、热约束和电池保护要求进行仔细安排，移动设备通常将机器学习工作负载限制在 500-1000 mW 的持续功耗。网络多样性引入了数量级的性能差异，因为 WiFi、4G、5G 和卫星连接表现出截然不同的带宽（从 1 Mbps 到 1 Gbps）、延迟（10 ms 到 600 ms）和可靠性特征，这些特征决定了可行的更新频率和压缩要求。

自适应协调协议通过复杂的分层参与策略解决这种异质性，优化了整个设备谱系中的资源利用率。高能力设备，如旗舰智能手机，可以执行带有大批量数据和多个时代的复杂本地训练，而资源受限的物联网设备则通过轻量级更新、专用子任务或甚至简单的数据聚合来做出贡献。这创造了一个自然的计算层次结构，其中强大的设备充当“超级对等节点”，执行不成比例的计算，而边缘设备则贡献专门的本地知识和覆盖范围。

规模挑战远远超出了设备异质性，扩展到了基本的协调开销限制。传统的分布式共识算法，如 Raft 或 PBFT，是为受控环境中的数十个节点设计的，但联邦学习需要在不可靠网络上协调数百万参与者。这需要分层协调架构，其中区域聚合服务器通过在贡献全球聚合之前执行本地共识来减少通信开销。边缘计算基础设施提供了自然的分层协调点，使联邦学习系统能够利用现有的内容分发网络（CDNs）和移动边缘计算（MEC）部署，以实现高效的梯度聚合。

现代联邦系统实施复杂的客户端选择策略，在统计多样性和实际约束之间取得平衡。随机抽样确保了无偏代表性，但可能会选择许多低能力设备，而基于能力的选择可以提高训练效率，但存在统计偏差的风险。混合方法在设备层级之间使用分层抽样，确保了统计代表性以及计算效率。这些选择策略还必须考虑时间模式：办公室工作人员的设备可能在特定时间段内可用，而物联网传感器提供连续但有限的计算资源。

## 生产集成

之前建立的理论基础——模型适应性策略、数据效率技术和联邦协调算法——为设备上学习系统提供了构建块。然而，将这些单个组件转换为生产就绪系统需要解决跨越所有约束维度的集成挑战。

现实世界的部署引入了超过单个技术总和的系统复杂性。模型适应性、数据效率和联邦协调必须无缝协作，而不是作为独立的优化。不同的学习策略具有不同的计算和内存配置文件，必须在整体设备预算内进行协调。训练、推理和通信必须仔细安排，以避免干扰用户体验和系统稳定性。与具有可观察训练循环的集中式系统不同，设备上学习需要分布式验证和故障检测机制，这些机制在异构设备群体中运行。

这种从理论到实践的转变需要系统性的工程方法，在平衡相互竞争的约束条件的同时保持系统可靠性。成功的设备上学习部署不依赖于单个算法的改进，而依赖于整体系统设计，该设计在操作约束内协调多种技术。接下来的部分将探讨生产系统如何通过原则性的设计模式、操作实践和监控策略来应对这些集成挑战，这些策略使得可扩展、可靠的设备上学习部署成为可能。

### MLOps 集成挑战

将设备上学习集成到现有的 MLOps 工作流程中需要扩展第十三章中建立的运营框架，以处理分布式训练、异构设备和隐私保护协调。前一章中讨论的持续集成管道、模型版本控制系统和监控基础设施提供了基本基础，但必须适应以解决独特的边缘部署挑战。标准 MLOps 管道假设集中式数据访问、受控部署环境和统一的监控能力，这些能力不直接适用于边缘学习场景，需要新的方法来管理早期建立的债务管理和技术卓越原则。

#### 部署流程转换

传统 MLOps 部署流程来自第十三章，遵循标准化的 CI/CD 流程：单个模型实体的模型训练、验证、预部署和生产部署到统一的基础设施。设备上学习需要设备感知的部署流程，该流程在不同异构设备层级上分配不同的适应策略。微控制器接收仅偏置的更新，中端手机使用 LoRA 适配器，而旗舰设备执行选择性层更新。部署实体从静态模型文件演变为一系列适应策略、初始模型权重和设备特定的优化配置。

这种架构转变需要扩展传统部署流程，包括设备能力检测、策略选择逻辑和分层部署编排，以保持传统 MLOps 的可靠性保证，同时适应前所未有的部署多样性。

这种转变在版本管理中引入了新的复杂性。虽然集中式系统维护单个模型版本，但设备上的学习系统必须同时跟踪多个版本维度。分发到所有设备的预训练骨干代表基础模型版本，它作为所有本地适应的基础。针对每个设备类别部署的不同更新机制构成了适应策略，从微控制器上的简单偏差调整到旗舰设备上的完整层微调。随着设备遇到独特的数据分布，本地模型状态自然会与基础模型分离，创建反映个别适应历史的设备特定检查点。最后，定期同步设备群体的联邦学习轮次建立了聚合时期，标志着分布式知识汇聚到更新后的全局模型的离散点。成功的部署实施分层版本方案，其中基础模型缓慢演变——通常通过每月更新——而本地适应则持续进行，从而创建了一个类似于传统部署中熟悉的线性版本历史的高级版本空间。

#### 监控系统演变

第十三章 建立了从集中式推理服务器聚合指标的性能监控实践。设备上的学习监控必须在本质上不同的约束条件下运行，这些约束条件重塑了系统如何观察、测量以及对分布式设备群体中的模型行为做出反应的方式。

隐私保护遥测代表了与传统监控的第一个根本性区别。在不损害用户隐私的情况下收集性能指标需要联邦分析，其中设备仅共享汇总统计信息或差分隐私摘要。系统不能像集中式系统那样简单地记录单个预测或训练样本。相反，设备报告分布摘要，如平均准确性和置信度直方图，而不是每个实例的指标。所有报告的统计数据都必须包含差分隐私保证，这些保证通过精心校准的噪声添加来限制信息泄露。安全聚合协议防止服务器观察到单个设备的贡献，确保即使聚合过程本身也无法从任何单个设备的数据中重建隐私信息。

漂移检测在没有访问到地面实况标签的情况下提出了额外的挑战。传统的监控将模型预测与维护在集中式基础设施上的标记验证集进行比较。设备上的系统必须在部署期间仅使用本地信号来检测漂移。置信度校准跟踪预测概率是否与经验频率相匹配，当模型的置信度估计与实际结果校准不良时，检测到退化。输入分布监控通过不需要标签的统计技术检测特征分布从训练数据中发生的变化。任务性能代理利用隐式反馈，如用户更正或任务放弃，作为质量信号，指示模型未能满足用户需求。阴影基线比较在适配模型旁边运行冻结的基础模型，以测量偏差，标记出本地适应相对于已知良好的基线性能下降而不是提高的情况。

异构性能跟踪解决了一个第三大关键挑战：当设备群体表现出高方差时，全局平均值会掩盖关键故障。监控系统必须在多个维度上分割性能，以识别影响特定设备群体的系统性问题。基于能力的性能差距表明，旗舰设备与预算设备相比取得了显著更好的结果，这表明针对资源受限的硬件，适应性策略可能需要调整。当模型在某些地理市场上表现良好而在其他市场上表现不佳时，区域偏差问题就会显现出来，这可能反映了数据分布的变化或文化因素，这些因素在初始训练期间没有被捕捉到。当运行过时的基础模型且未从联邦聚合中接收最近更新的设备性能下降时，就会出现时间模式。当比较经常适应的设备与很少参与训练的设备时，参与不平等变得明显，这揭示了学习收益在用户群体中分配的潜在公平性问题。

#### 持续训练编排

在第十三章中介绍的传统的持续训练在具有可预测资源可用性和协调执行的集中式基础设施上执行计划中的重新训练作业。设备上的学习将此转变为持续分布式训练，其中数百万台设备独立训练，无需全局同步，从而产生了需要根本不同的协调策略的编排挑战。

异步设备协调代表了从集中式训练的第一个重大转变。数百万台设备在其本地数据上独立训练，但编排系统不能依赖于同步参与。由于网络连接限制、电池约束和不同的使用模式，通常只有 20-40%的设备在任何一轮训练中可用。系统必须表现出对落后者的容忍度，确保在有限硬件或较差网络连接上的慢速设备不会阻碍快速设备进行其本地适应。设备通常同时运行在不同的基础模型版本上，这会创建版本偏差，聚合协议必须优雅地处理，而无需强制所有设备保持相同的模型状态。当设备在长时间离线后重新连接时——可能是几天或几周——状态协调变得必要，需要系统整合它们累积的本地适应，尽管它们已经错过了多次联邦聚合轮次。

资源感知调度确保训练过程既尊重设备限制，也考虑用户体验。编排策略实施机会主义训练窗口，仅在设备空闲、充电且连接到 WiFi 时执行适应，避免干扰活跃用户任务或消耗计费蜂窝数据。当设备温度超过制造商指定的阈值时，热预算会暂停训练，防止因持续的计算负载导致用户不适和硬件损坏。电池保护策略将训练能耗限制在每天不超过电池容量的 5%，确保设备上的学习不会明显影响用户视角下的设备运行时间。网络感知通信在设备必须使用计费连接时，会积极压缩模型更新，以减少带宽消耗，从而降低用户数据费用，以换取计算开销的降低。

没有全局可见性的收敛评估构成了最终的编排挑战。传统的训练监控在集中式验证集上损失曲线，提供关于训练进度和收敛的明确信号。分布式训练必须通过跨设备群体汇总的间接信号来评估收敛。联邦评估从维护本地保留集的设备中汇总验证指标，尽管设备参与不完全，但提供对全局模型质量的近似度量。更新幅度跟踪监控每个聚合轮次中本地梯度对全局模型的影响程度，更新大小的减少表明可能已收敛。参与多样性确保在汇总更新中广泛代表设备，防止收敛指标仅反映部署环境的狭窄子集。时间一致性检测模型改进在多个聚合轮次中达到平台期，表明当前适应性策略已耗尽其潜在收益，可能需要调整。

#### 验证策略适应性

第十三章中的验证方法假设可以访问保留的测试集和集中式评估基础设施，可以直接将模型质量与已知的真实情况进行比较。设备上的学习需要分布式验证，在尊重隐私和资源限制的同时，仍能为异构设备群体提供可靠的质量信号。

影子模型评估通过在每个设备上维护多个模型变体并比较其行为来提供主要的验证机制。设备同时运行一个基线影子模型——最后已知良好基模型的冻结副本，提供一个稳定的参考点——以及当前本地适应的版本，该版本反映了最近的设备上训练。许多系统还维护最新的联邦聚合结果作为全局模型变体，使个人设备适应与从整个设备群体汇总的集体知识之间的比较成为可能。通过比较这些变体在传入数据流中的预测，系统检测到本地适应相对于既定的基线性能下降。这种比较在正常操作期间持续进行，无需额外的标记验证数据。当适应的模型持续低于基线影子模型时，系统触发自动回滚到已知良好的版本，防止性能下降在生产中持续存在。

基于置信度的质量门在标签验证数据不可用的情况下提供额外的验证信号。在没有地面真实标签的情况下，系统使用预测置信度作为质量代理，这与模型性能相关。校准良好的模型应在与其训练数据相似的分布样本上表现出高置信度，置信度得分应准确反映正确预测的概率。置信度下降表明可能是分布偏移——输入数据不再匹配训练分布——或者模型因问题性的局部适应而退化。基于阈值的门控通过持续监控平均预测置信度，并在置信度低于初始部署期间建立的基线水平时暂停适应来实现这种验证机制。这种方法可以捕捉许多故障模式，而无需标记的验证数据，尽管它不能检测到所有性能问题，因为过于自信但错误的预测可以保持高置信度得分。

联邦 A/B 测试允许在分布式设备群体中验证新的适应策略或模型架构。为了验证所提出的更改，系统实施分布式实验，随机将设备分配到治疗组和对照组，同时保持设备层级和用法模式之间的统计平衡。两组都使用隐私保护聚合协议收集联邦度量，这些协议防止个人设备数据泄露，同时允许进行人口水平比较。系统比较适应成功率——衡量局部适应如何频繁地超过基线模型——以及收敛速度，这表明设备达到最佳性能的速度有多快，以及最终性能指标，这些指标反映了适应完成后模型的最终质量。在治疗组中显示出明显改进的成功策略将逐步推广到设备群体中，从小的百分比开始，只有在确认好处可以推广到实验群体之外后才会扩大。

这些操作转换需要新的工具和基础设施，这些工具和基础设施系统地扩展了传统的 MLOps 实践，从第十三章开始。为集中部署建立的 CI/CD 管道、监控仪表板、A/B 测试框架和事件响应程序构成了设备学习操作的基础。联邦学习协议(第 14.6 节)提供了分布式训练的协调机制，而监控挑战(第 14.9.3 节)则解决了由去中心化适应产生的可观察性差距。

成功的设备上学习部署建立在经过验证的 MLOps 方法之上，同时适应了分布式、异构学习环境的独特挑战。这种进化方法确保了操作可靠性，同时实现了边缘学习的益处。

### 生物启发学习效率

设备上学习的约束反映了生物智能系统解决的基本挑战，为高效学习设计提供了理论见解。理解这些联系使得能够采用基于原理的方法来利用数十亿年的进化优化进行资源受限的机器学习。

#### 从生物神经效率中学习

人类大脑在持续从有限的监督中学习的同时，以大约 20 瓦的功率运行——这正是设备上学习系统的效率目标 32。这种非凡的效率源于几个直接影响边缘学习设计的架构原则，展示了理论上高度优化的学习系统可以达到什么水平。

大脑的效率特性揭示了设备上系统应针对的多个优化维度。从功耗角度来看，大脑总共消耗 20 W 的功率，其中大约 10 W 用于主动学习和记忆巩固——这种能量预算与移动设备在充电期间可持续分配给设备上学习的能量相当。内存效率来源于稀疏、分布式的表示，在任何认知任务中，只有 1-2%的神经元同时激活，与密集神经网络相比，大大减少了计算和存储需求。学习效率通过少样本学习能力体现，这种能力使得可以从单一暴露中适应，以及通过连续适应机制，在整合新知识时避免灾难性遗忘。分层处理在多个尺度上组织信息，从低级感官输入到高级抽象推理，使得在不同任务和环境中高效重用学习到的特征。

生物学习表现出几个特征，设备上系统必须复制以实现类似的效率。稀疏表示确保了有限的神经资源的有效利用——在任何认知任务中，只有大脑神经元的一小部分会激活。这种稀疏性直接对应于移动部署中必不可少的精选参数更新和剪枝架构。事件驱动处理通过仅在感官输入变化时激活计算来最小化能耗，类似于设备空闲期间的机会训练。

#### 未标记数据利用策略

移动设备持续收集丰富的传感器流，非常适合自监督学习：来自摄像头的视觉数据、来自加速度计的时间模式、来自 GPS 的空间模式以及来自触摸屏使用的交互模式。这大量的未标记数据使得在没有外部监督的情况下进行复杂的表示学习成为可能。

移动设备上传感器数据生成的规模为自监督学习创造了前所未有的机遇。每秒 30 帧的摄像头视觉流每天提供约 260 万帧图像，为通过比较相同图像的增强版本来学习视觉表示的对比学习方法提供了大量数据 33。以 100 Hz 采样率的加速度计生成的运动数据每天产生 860 万个数据点，捕捉适合学习人类活动和设备运动表示的时间模式。来自 GPS 传感器的位置轨迹通过捕捉运动模式和频繁访问的位置，无需显式标签，实现空间表示学习和行为预测。来自触摸事件、打字动态和应用使用序列的交互模式创建了丰富的行为嵌入，揭示了用户偏好和习惯，从而实现无需手动标注的个性化模型适应。

从时间相关性中进行对比学习为利用这些传感器数据提供了特别有前景的机遇。移动摄像头的连续帧自然为视觉表示学习提供正对，毫秒之差捕获的图像通常是从略微不同的角度看到的同一场景，而诸如颜色抖动和随机裁剪等增强技术则创建了负例。麦克风中的音频流通过掩码和预测任务实现自监督语音表示学习，其中模型学习预测音频频谱图的掩码部分。甚至设备方向和运动数据也可以用于活动识别模型的自我监督预训练，学习捕捉人类运动时间结构的表示，而无需标记的活动注释。

生物启发的思想扩展到持续学习而不遗忘。大脑通过突触巩固和重放等机制持续整合新的经验，同时保留数十年的记忆。设备上的系统必须实现类似机制：弹性权重巩固通过保护对先前任务重要的权重来防止灾难性遗忘，经验重放通过交替新训练和先前任务的回放示例来维持适应期间的稳定性，而渐进式神经网络架构随着新任务的涌现而扩展模型容量，而不是将所有知识都强制纳入固定容量的网络。

#### 永续适应而不遗忘

真实世界的设备端部署需要不断适应不断变化的环境、用户行为和任务需求。这提出了稳定性-塑性权衡的基本挑战：模型必须保持足够的稳定性以保留现有知识，同时足够塑性以学习新模式。

在边缘设备上进行持续学习面临几个相互关联的挑战，这些挑战加剧了分布式适应的难度。当新的学习覆盖了之前获得的知识时，会发生灾难性遗忘，导致模型在适应新任务时性能下降，尤其是在设备无法访问历史训练数据的情况下这是一个特别严重的问题。当多个学习目标争夺有限的模型容量时，会出现任务干扰，迫使模型在必须同时保持的不同能力之间做出艰难的权衡。数据分布变化表现为部署环境与训练条件显著不同，要求模型适应新模式的同时保持对原始分布的性能。

元学习方法通过学习学习算法本身来解决这些挑战，而不仅仅是学习特定任务。模型无关元学习（MAML）训练模型以使用最少的数据快速适应新任务——这正是个性化设备端适应所必需的，在收集大型用户特定数据集不切实际的情况下。少样本学习技术允许从小的用户特定数据集中快速专业化，使模型能够仅基于少量示例进行个性化，同时保持预训练期间学习到的通用能力。

理论基础表明，最优的设备端学习系统将结合稀疏表示、基于传感器数据的自监督预训练以及元学习以实现快速适应。这些原则直接影响实际系统设计：稀疏模型架构降低内存和计算需求，自监督目标利用丰富的未标记传感器数据，而元学习则使模型能够从有限的用户交互中实现高效个性化。

构建实用系统的关键原则是尽量减少适应足迹。在边缘平台上进行全模型微调通常不可行，因此，应优先考虑局部更新策略，包括仅优化偏差、残差适配器和轻量级任务特定头部。这些方法允许在资源受限的情况下实现模型专业化，同时减轻过拟合或不稳定的风险。

轻量级适应的可行性在很大程度上取决于离线预训练的强度 (Bommasani 等人 2021)。预训练模型应封装可泛化的特征表示，以便从有限的本地数据中高效地适应。将特征提取的负担转移到集中式训练可以降低设备更新的复杂性和能耗，同时在数据稀疏环境中提高收敛稳定性。

即使适应是轻量级的，机会性调度仍然很重要，以保持系统响应性和用户体验。本地更新应推迟到设备空闲、连接到外部电源并运行在可靠网络上的时段。此类策略最小化了背景训练对延迟、电池消耗和热性能的影响。

本地训练工件敏感性需要谨慎的数据安全措施。重放缓冲区、支持集、适应日志和模型更新元数据必须受到未经授权的访问或篡改的保护。轻量级加密或基于硬件的安全存储可以减轻这些风险，而不会对边缘平台施加过高的资源成本。

然而，仅靠安全措施并不能保证模型的鲁棒性。随着模型在本地适应，监控适应动态变得重要。轻量级验证技术，包括置信度评分、漂移检测启发式方法和影子模型评估，可以帮助早期识别偏差，使系统在发生严重退化之前触发回滚机制 (Gama 等人 2014)。

顽强的回滚程序依赖于保留可信的模型检查点。每次部署都应该保留一个已知的良好基线版本，以便在适应导致不可接受的行为时可以恢复。这一原则在安全重要和受监管的领域尤为重要，在这些领域，故障恢复必须是可证明且快速的。

在去中心化或联邦学习环境中，通信效率成为一级设计约束。必须采用压缩技术，如量化梯度更新、稀疏参数集和选择性模型传输，以允许在大规模、异构设备群之间进行可扩展的协调，而不会耗尽带宽或能源预算 (Konečný 等人 2016)。

当需要个性化时，系统应尽可能实现本地适应。将更新限制在轻量级组件中，包括最终的分类头或模块化适配器，可以限制灾难性遗忘的风险，减少内存开销，并加速适应而不会破坏核心模型表示的稳定性。

最后，在整个系统生命周期中，隐私和合规性要求必须被构建到适应管道中。支持用户同意、数据最小化、保留限制和删除权的机制必须被视为模型设计的基本方面，而不是事后调整。在规模上满足监管义务要求设备上学习工作流程与可审计的自主性原则内在一致。

图 14.8 中的流程图总结了设计实用、可扩展和弹性设备上机器学习系统时的关键决策点。

![图片](img/file227.svg)

图 14.8：此流程图通过概述与数据管理、模型选择和隐私考虑相关的关键决策点，指导实用设备上机器学习系统的系统化开发。将隐私和合规性要求（如用户同意和数据最小化）集成到设计过程中确保了可审计的自主性和设备智能的可扩展部署。

## 生产部署的系统集成

现实世界的设备上学习系统通过系统地结合所有三个解决方案支柱来实现有效性，而不是依赖于孤立的技术。这种集成需要仔细的系统工程来管理交互、解决冲突，并在部署约束下优化整体系统性能。

考虑在一个覆盖 5000 万台异构设备的生产级语音助手部署。该系统架构展示了在三个互补层之间的系统化集成，这三个层共同工作以在多种约束条件下实现有效的学习。

模型适应层根据设备能力分层技术，将复杂性匹配到可用资源。代表部署中前 20%的旗舰手机使用 LoRA rank-32 适配器，通过高维参数更新实现复杂的语音模式学习。占车队 60%的中端设备采用 rank-16 适配器，在适应表达性和主流智能手机典型的更紧的内存限制之间取得平衡。构成剩余 20%的预算设备依赖于仅偏置更新的方式，保持在 1 GB 内存限制内，同时仍然能够实现基本个性化。

数据效率层在整个设备群体中实施自适应策略，同时尊重个体资源限制。所有设备都实现经验重放，但具有设备适当的缓冲区大小——预算设备上为 10 MB，而旗舰机型上为 100 MB，确保内存受限的设备仍能从基于重放的学习中受益。少样本学习使用户在前 10 次交互中快速适应新用户，减少了困扰需要大量训练数据的系统的冷启动问题。流式更新适应用户说话风格随时间自然变化或在新声学环境中使用助手时的连续语音模式演变。

联邦协调层在设备群体中协调隐私保护的合作。设备根据连接状态和电池水平有选择地参与联邦训练轮次，确保协调不会降低用户体验。LoRA 适配器只需 50 MB 的更新即可高效聚合，而全模型同步需要 14 GB，这使得联邦学习在移动网络上变得可行。隐私保护聚合协议确保个人语音模式永远不会离开设备，同时仍能实现口音识别和语言理解方面的种群规模改进，使所有用户受益。

有效的系统集成需要遵守关键工程原则，以确保在异构设备群体中稳健运行：

1.  **分层能力匹配**：在具备能力的设备上部署更复杂的技巧，同时确保在整个设备范围内保持基本功能。永远不要假设能力统一。

1.  **优雅降级**：系统必须在单个组件失败时有效运行。不良的连接性不应阻止本地适应；低电量应触发最小适应模式。

1.  **冲突解决**：模型适应和数据效率技术可能存在冲突（内存限制与缓冲区大小）。通过预定义的优先级层次结构进行系统资源分配可以防止这些冲突。

1.  **性能验证**：集成会产生个体技术所不具备的涌现行为。系统需要在设备组合和网络条件下进行全面测试。

这种集成方法将设备上的学习从一系列技术转变为一个连贯的系统能力，在现实世界的部署限制内提供稳健的个性化。

## 持续的技术和操作挑战

上文探讨的解决方案技术——模型适应性、数据效率和联邦协调——解决了设备上学习的许多基本约束，但也揭示了从现实部署中产生的持续挑战。这些挑战代表了设备上学习研究的前沿，并突出了先前讨论的技术达到其极限或创造新的操作复杂性的领域。理解这些挑战为评估何时采用设备上学习方法以及何时可能需要替代策略提供了关键背景。

与在具有统一硬件和精选数据集的受控环境中进行训练的传统集中式系统不同，边缘系统必须应对设备异构性、数据碎片化和缺乏集中式验证基础设施。这些因素产生了新的系统级权衡，考验了我们考察的适应性策略、数据效率方法和协调机制的边界。

### 设备和数据异构性管理

联邦和设备上的机器学习系统必须在从智能手机和可穿戴设备到物联网传感器和微控制器的广泛且多样化的设备生态系统中运行。这种异构性跨越多个维度：硬件能力、软件堆栈、网络连接和电力可用性。与可以标准化和控制的云基础系统不同，边缘部署会遇到广泛的系统配置和约束。这些变化在算法设计、资源调度和模型部署方面引入了显著的复杂性。

在硬件层面，设备在内存容量、处理器架构（例如，ARM Cortex-M 与 A 系列 34）、指令集支持（例如，SIMD 或浮点单元的可用性）以及是否存在 AI 加速器方面存在差异。一些客户端可能拥有能够运行小型训练循环的强大 NPU，而其他客户端可能仅依赖低频 CPU 和最小内存。这些差异影响了模型的可实现大小、训练算法的选择以及更新的频率。

软件异构性加剧了挑战。设备可能运行不同的操作系统版本、内核级驱动程序和运行时库。一些环境支持优化的机器学习运行时，如 TensorFlow Lite35 Micro 或 ONNX Runtime Mobile，而其他环境则依赖于定制的推理堆栈或受限的 API。这些差异可能导致行为上的微妙不一致，尤其是在模型编译不同或平台间浮点精度不同的情况下。

除了计算异构性之外，设备在连接性和运行时间上表现出差异。有些设备是间歇性连接的，偶尔插电，或在严格的带宽限制下运行。其他设备可能具有连续的电源和可靠的联网，但仍然优先考虑用户界面的响应性，而不是后台学习。这些差异使得协调学习和更新调度复杂化。

最后，系统碎片化影响了可重复性和测试。在如此广泛的执行环境中，很难确保模型行为的一致性或可靠地调试故障。这使得监控、验证和回滚机制变得更加重要——但也更难以在所有设备上统一实施。

考虑一个用于移动键盘的联邦学习部署。高端智能手机可能配备 8 GB 的 RAM、专门的 AI 加速器和连续的 Wi-Fi 接入。相比之下，预算设备可能只有 2 GB 的 RAM、没有硬件加速，并依赖于间歇性的移动数据。这些差异影响了训练运行的时间长度、模型更新的频率，甚至是否可以进行训练。为了支持这种范围，系统必须动态调整训练计划、模型格式和压缩策略——确保用户之间模型改进的公平性，同时尊重每个设备的限制。

### 非 IID 数据分布挑战

在集中式机器学习中，数据可以被聚合、打乱和整理，以近似独立同分布（IID）样本——这是许多学习算法背后的关键假设。设备端和联邦学习系统从根本上挑战了这个假设，需要能够处理跨不同设备和环境的高度碎片化和非 IID 数据的算法。

这种碎片化的统计影响在整个学习过程中造成了级联挑战。在不同设备上计算的梯度可能冲突，减缓收敛或使训练不稳定。局部更新可能会过度拟合到单个客户端的独特性，当全局聚合时降低性能。客户端间数据的多样性也使得评估复杂化，因为没有任何单个测试集可以代表真实的部署分布。

这些挑战需要能够处理异构性和不平衡参与的鲁棒算法。如个性化层、重要性加权、自适应聚合方案等技术提供了部分解决方案，但最佳方法随着应用上下文和数据碎片化的具体性质而变化。如第 14.3.3 节所述，这种统计异构性代表了区分设备端学习与传统集中式方法的核心挑战之一。

### 分布式系统可观察性

来自第十三章的监控和可观察性框架必须为分布式边缘环境进行根本性的重新构想。当设备间歇性连接且数据无法集中时，依赖于统一数据收集和实时可见性的传统集中式监控方法变得不切实际。MLOps 中建立的漂移检测和性能监控技术提供了概念基础，但需要适应以处理设备上学习系统的分布式、隐私保护特性。

与可以持续对保留的验证集进行评估的集中式机器学习系统不同，设备上学习引入了可见性和可观察性的核心转变。一旦部署，模型在高度多样化和通常断开连接的环境中运行，内部更新可能在没有外部监控的情况下进行。这为确保模型适应既有益又安全带来了重大挑战。

核心困难在于缺乏集中的验证数据。在传统的工作流程中，模型使用作为部署条件代理的精选数据集进行训练和评估。相比之下，设备上学习者根据本地输入进行适应，这些输入很少标记，并且可能没有系统地收集。因此，更新质量和方向，无论是增强泛化还是导致漂移，都难以评估，而不会干扰用户体验或违反隐私限制。

在流式设置中，模型漂移的风险尤为明显，持续的适应可能导致性能缓慢下降。例如，一个对背景噪声适应过于激进的语音识别模型最终可能会过度拟合到瞬时的声学条件，降低目标任务的准确性。如果没有对模型参数或输出的演变有可见性，这种退化可能直到变得严重才被发现。

缓解这个问题需要设备上验证和更新门控的机制。一种方法是将适应步骤与轻量级性能检查交替进行——使用代理目标或自监督信号来近似模型置信度 (Y. Deng, Mokhtari, and Ozdaglar 2021)。例如，一个关键词检测系统可能会跟踪最近话语中的检测置信度，并在置信度持续低于阈值时暂停更新。或者，可以采用影子评估，在设备上维护多个模型变体，并在传入的数据流上并行评估，使系统能够将适应后的模型行为与稳定的基线进行比较。

另一种策略涉及周期性检查点和回滚，即在适应之前保存模型状态快照。如果后续性能下降，如通过下游指标或用户反馈确定，系统可以回滚到已知良好的状态。这种方法已在健康监测设备中使用，错误的预测可能导致用户不信任或安全担忧。然而，它引入了存储和计算开销，尤其是在内存受限的环境中。

在某些情况下，联邦验证提供了一种部分解决方案。设备可以与中央服务器共享匿名化的模型更新或汇总统计信息，该服务器汇总用户数据以识别全局漂移或故障模式。虽然这保留了一定程度的隐私，但它引入了通信开销，并且可能无法捕捉到罕见或特定于用户的故障。

最终，在设备上学习中的更新监控和验证需要重新思考传统的评估实践。与传统集中式测试集不同，系统必须依赖隐式信号、运行时反馈和保守的适应策略来确保鲁棒性。缺乏全局可观察性不仅仅是技术限制——它反映了更深层次的系统挑战，即在局部适应与全局可靠性之间进行协调。

#### 动态环境中的性能评估

第十二章建立了测量机器学习系统性能的系统方法：推理延迟、吞吐量、能效和准确度指标。这些基准测试方法为表征模型性能提供了基础，但它们是为静态推理工作负载设计的。设备上的学习需要扩展这些指标，通过特定的训练基准来捕捉适应质量和训练效率。

除了第十二章中提到的推理指标，自适应系统需要专门的训练指标来捕捉在边缘约束下的学习效率。适应效率衡量的是每个训练样本消耗后的准确度提升，量化为资源约束下的学习曲线斜率——一个每 100 个训练样本实现 2%准确度提升的系统比需要 500 个样本才能达到相同提升的系统具有更高的适应效率，这直接转化为更快的个性化服务和减少数据收集需求。内存约束下的收敛评估在指定的 RAM 预算内实现的验证损失，例如“在 512 KB 训练足迹内收敛”，捕捉了系统在固定内存分配下的学习效率——这对于比较从微控制器到智能手机的设备类别之间的适应策略至关重要。每更新一次能耗量量化了每次梯度更新所消耗的毫焦耳，这是一个对于电池供电设备至关重要的指标，因为训练能耗直接影响用户体验——移动设备通常为持续 ML 工作负载预留 500-1000 mW，这意味着每小时适应所需的能量仅为 1.8-3.6 焦耳，在明显影响电池寿命之前。

评估局部适应是否真正优于全局模型需要个性化增益指标来证明设备上学习的开销。每个用户的性能变化衡量的是适应模型与全局基线在用户特定保留数据上的准确度提升——系统应显示出具有统计学意义的改进，通常超过 2%的准确度提升，以证明计算开销、能耗和适应引入的复杂性是合理的。个性化-隐私权衡量化了每单位本地数据暴露的准确度提升，衡量从隐私敏感信息中提取的价值——这个指标有助于评估适应带来的好处是否超过了保留本地用户数据的隐私成本，这对于处理敏感信息如健康数据或个人通信的应用尤为重要。灾难性遗忘率衡量模型适应局部分布时原始任务的退化，通过保留测试来衡量——可接受的遗忘率取决于应用领域，但通常应保持在原始任务上 5%以下准确度损失，以确保个性化不会以牺牲模型的一般能力为代价。

当设备通过联邦学习（第 14.6 节）进行协调时，联邦协调成本指标对于评估系统可行性变得至关重要。通信效率衡量每传输一个字节模型精度提升的程度，捕捉梯度压缩和选择性更新策略的有效性——现代联邦系统通过量化和稀疏化技术实现了 10-100 倍的压缩，同时保持 95%或以上的未压缩精度，这是实际和不可行的移动部署之间的区别。落后者影响量化了由慢速或不稳定设备引起的收敛延迟，测量为有与无参与滤波器的收敛时间差异——通过异步聚合和选择性参与的有效落后者缓解将收敛时间减少了 30-50%，与同步方法相比，同步方法等待所有设备。聚合质量评估全局模型性能作为设备参与率的函数，揭示了联邦学习无法有效收敛的最小可行参与阈值——大多数联邦系统每轮需要 10-20%的设备参与以维持稳定的收敛，为客户端选择和可用性管理策略确立了明确的要求。

这些针对训练的基准测试补充了第十二章（ch018.xhtml#sec-benchmarking-ai）中的推理指标，为自适应系统提供了完整的性能描述。实际的基准测试必须衡量这两个维度：一个系统如果实现了快速的推理但适应缓慢，或者高效的适应但最终精度差，则无法满足现实世界的需求。推理和训练基准的集成使得可以对设备端学习系统在其整个操作生命周期内进行全面的评估。

### 资源管理

设备端学习引入了传统仅推理部署中不存在的资源竞争模式。许多边缘设备被配置为高效运行预训练模型，但很少考虑训练工作负载。因此，本地适应性与其他系统进程和面向用户的程序竞争有限的资源，包括计算周期、内存带宽、能源和热头房。

最直接的约束是计算可用性。训练涉及通过模型进行额外的正向和反向传递，这可能会超过推理的成本。即使只有一小部分参数被更新，例如在仅偏置或仅头部适应的情况下，反向传播仍然需要遍历相关层，从而触发指令计数和内存流量的增加。在具有共享计算单元的设备（例如，移动 SoC 或嵌入式 CPU）上，这种需求可能会延迟交互式任务，降低帧率或损害传感器处理。

能量消耗加剧了这个问题。适应通常涉及在多个输入样本上的持续计算，这会消耗电池供电系统的能量，并可能导致能量迅速耗尽。例如，在微控制器级设备上执行单个适应周期可能会消耗几毫焦耳 36——这对于一个基于收集能量的分时系统来说是一个相当大的能量预算部分。这需要仔细的调度，以确保学习仅在空闲期间进行，此时能量储备充足，用户延迟约束放松。

从内存的角度来看，由于需要缓存中间激活 37、梯度以及优化器状态(Ji Lin 等人 2020)，训练产生的峰值使用率高于推理。

这些资源需求也必须与服务质量（QoS）目标相平衡。用户期望边缘设备能够可靠且一致地响应，无论学习是否在后台进行。任何可观察到的退化，包括唤醒词检测器中的音频丢失或可穿戴显示器中的延迟，都可能侵蚀用户的信任。这些系统可靠性问题与第十三章中讨论的操作挑战相平行。因此，许多系统采用机会性学习策略，即在后台活动期间暂停适应，仅在系统负载低时恢复。

在某些部署中，适应还受到网络基础设施施加的成本约束的进一步限制。例如，设备可能将学习工作负载的一部分卸载到附近的网关或云小点，这引入了带宽和通信权衡。这些混合模型提出了关于任务放置和调度的额外问题：更新应该本地发生，还是推迟到高吞吐量链路可用时？

总结来说，设备上学习的成本并不仅仅以 FLOPs 或内存使用量来衡量。它表现为系统负载、用户体验、能量可用性和基础设施容量之间复杂交互的结果。解决这些挑战需要算法、运行时和硬件层的协同设计，确保适应在现实世界约束下保持不引人注目、高效和可持续。

### 识别和预防系统故障

理解设备上学习的潜在故障模式有助于防止昂贵的部署错误。基于联邦学习研究中的记录挑战（Kairouz 等人 2021）和自适应系统中已知的风险，几个类别的故障需要仔细考虑。

设备上学习最基本的风险是无界适应漂移，即无约束的持续学习导致模型逐渐偏离其预期行为。考虑一个假设的键盘预测系统，它从所有用户输入包括纠正中学习——它可能会开始将错误输入作为有效建议纳入，导致预测质量逐渐下降。在健康监测应用中，这种风险变得尤为严重，因为用户基线中的渐进变化可能会被学习为“正常”，可能导致系统错过静态模型能够检测到的重要异常。这种漂移的隐蔽性在于它缓慢且局部地发生，没有适当的监控基础设施很难检测到。

除了个别设备漂移之外，联邦学习系统在人口层面上面临着参与偏差放大的挑战。具有可靠电源和连接性的设备更频繁地参与联邦轮次（T. Li 等人 2020）。这种不均匀的参与创造了模型越来越优化高端设备用户而性能对资源有限的用户下降的场景。由此产生的反馈循环加剧了数字不平等：服务较好的用户获得越来越好的模型，而服务不足的人群则经历性能下降，减少他们的参与，进一步减少他们在训练轮次中的代表性（J. Wang 等人 2021）。这些公平性和偏差放大问题突出了分布式学习系统的伦理影响。

这些系统性偏差与数据质量问题相互作用，形成自纠正反馈循环，尤其是在基于文本的应用中。当系统无法区分预期输入和纠正时，它们可能会表现出意外的行为。经常被纠正的特定领域术语可能会被错误地学习为错误，导致在专业环境中提出不恰当的建议。这个问题加剧了漂移问题：不仅模型适应了个人的怪癖，而且当用户接受自动纠正而没有意识到系统正在从这些交互中学习时，它们也可能从自己的错误中学习。

这些故障模式之间的相互关联性，从个体漂移到群体偏差再到数据质量下降，强调了实施全面安全机制的重要性。成功的部署需要有限的适应范围以防止无限制的漂移，分层抽样以解决参与偏差，仔细的数据过滤以避免从校正中学习作为真实情况，以及与静态基线的影子评估以检测退化。尽管由于竞争和隐私问题，具体的生产事件很少公开，但研究界已将这些模式识别为需要系统缓解策略的关键领域(T. Li 等人 2020; Kairouz 等人 2021)。

### 生产部署风险评估

在边缘设备上部署自适应模型引入了超出技术可行性的挑战。在需要合规性、可审计性和监管批准的领域，包括医疗保健、金融和安全性重要的系统，设备上的学习在系统自主性和控制之间构成了核心紧张关系。

在传统的机器学习管道中，所有模型更新都是集中管理、版本控制和验证的。训练数据、模型检查点和评估指标通常记录在可重现的工作流程中，支持可追溯性。然而，当学习发生在设备本身时，这种可见性就会丧失。每个设备可能独立地发展其模型参数，受到开发者或系统维护者从未观察到的独特本地数据流的影响。

这种自主性产生了验证差距。没有访问输入数据或确切更新轨迹，很难验证学习到的模型是否仍然遵循其原始规范或性能保证。这在受监管的行业中尤其成问题，因为认证取决于证明系统在定义的操作边界内表现一致。一个响应现实世界使用而自行更新的设备可能会超出这些边界，在没有外部信号的情况下触发合规违规。

缺乏集中监管使得回滚和故障恢复变得复杂。如果模型更新降低了性能，可能不会立即被发现，尤其是在离线场景或没有遥测功能的系统中。当观察到故障时，系统的内部状态可能已经与任何已知的检查点有显著差异，这使得诊断和恢复比在静态部署中更为复杂。这需要强大的安全机制，例如保守的更新阈值、回滚缓存或保留经过验证基线的双模型架构。

除了合规挑战之外，设备上的学习还引入了新的安全漏洞。由于模型适应是本地发生的，并且依赖于特定于设备的、可能不可信的数据流，攻击者可能会通过篡改存储的数据（如重放缓冲区）或在适应过程中注入毒化示例来操纵学习过程，以降低模型性能或引入漏洞。任何本地存储的适应数据，如特征嵌入或少样本示例，都必须得到保护，防止未经授权的访问，以防止意外信息泄露。

在去中心化环境中维护模型完整性尤其困难，因为中央监控和验证有限。自主更新可能在没有外部可见性的情况下导致模型漂移到不安全或偏颇的状态。这些风险通过合规义务（如 GDPR 的删除权）进一步加剧：如果用户数据通过适应微妙地影响模型，跟踪和逆转这种影响变得复杂。

自适应模型，尤其是在边缘，的安全性和完整性提出了重要的开放挑战。对这些威胁和相应的缓解策略的全面处理需要为分布式机器学习系统提供专门的网络安全框架。

隐私法规也与设备上的学习以非平凡的方式相互作用。虽然本地适应可以减少传输敏感数据的需要，但它可能仍然需要在设备本身上存储和处理个人信息，包括传感器轨迹或行为日志。这些隐私考虑需要仔细关注安全框架和法规合规。根据司法管辖区，这可能会引发对数据保留、用户同意和可审计性的额外要求。系统必须设计得满足这些要求，而不损害适应的有效性，这通常涉及加密存储数据、实施保留限制或实施用户控制的重置机制。

最后，边缘学习的出现提出了关于问责制和责任（Brakerski 等人 2022）的开放性问题。当模型自主适应时，谁对其行为负责？如果适应后的模型做出了错误的决定，例如误诊健康状况或误解语音命令，其根本原因可能在于本地数据漂移、初始化不良或安全措施不足。如果没有标准化的机制来捕捉和分析这些故障模式，责任可能难以分配，监管批准也更难获得。

解决这些部署和合规风险需要新的工具、协议和设计实践，以支持可审计的自主性——系统在满足外部对可追溯性、可重复性和用户保护的要求的同时，能够就地适应。随着设备端学习变得更加普遍，这些挑战将成为系统架构和治理框架的核心。

### 工程挑战综合

设计设备端机器学习系统涉及在技术和实践约束的复杂景观中导航。虽然局部适应允许个性化、隐私和响应性，但它也引入了一系列挑战，这些挑战跨越了硬件异构性、数据碎片化、可观察性和法规遵从性。

系统异构性通过引入计算、内存和运行时环境的变化，使部署和优化复杂化。非独立同分布数据分布挑战学习稳定性和泛化能力，尤其是在模型在设备上训练且无法访问全局上下文的情况下。缺乏集中监控使得验证更新或检测性能退步变得困难，并且训练活动必须经常与核心设备功能竞争能源和计算资源。最后，部署后的学习在模型治理方面引入了复杂性，从可审计性和回滚到隐私保证。

这些挑战并非孤立存在——它们以影响不同适应策略可行性的方式相互作用。表 14.6 总结了主要挑战及其对边缘部署的机器学习系统的影响。

表 14.6：**设备端学习挑战**：系统异构性、非独立同分布数据以及有限的资源为在边缘设备上部署和适应机器学习模型带来了独特的挑战，影响了可移植性、稳定性和治理。该表详细说明了这些挑战的根本原因及其系统级影响，突出了模型性能与资源约束之间的权衡。

| **挑战** | **根本原因** | **系统级影响** |
| --- | --- | --- |
| **系统异构性** | 多样化的硬件、软件和工具链 | 限制了可移植性；需要针对特定平台进行调整 |
| **非独立同分布和碎片化数据** | 本地化、用户特定的数据分布 | 阻碍了泛化；增加了漂移风险 |
| **有限的可观察性和反馈** | 没有集中的测试或日志记录 | 使得更新验证和调试困难 |
| **资源竞争和调度** | 内存、计算和电池的竞争需求 | 需要动态调度和预算感知学习 |
| **部署和合规风险** | 部署后继续学习 | 复杂化模型版本控制、审计和回滚 |

### 稳健人工智能系统的基础

前几节探讨的操作挑战和故障模式揭示了超出部署担忧的系统基本可靠性方面的漏洞。当模型在数百万个异构设备上自主适应时，会出现三种传统集中式训练从未遇到的威胁类别。

第一，与故障局部化和可观察的集中式系统（如第十三章[sec-ml-operations]所述）不同，设备上的学习创造了本地故障可以无声传播到设备群体中的场景。如果一个设备上的损坏适应通过联邦学习聚合，可能会毒害全局模型。在集中式基础设施中会触发错误的硬件故障可能在边缘设备上静默地损坏梯度，而检测错误的能力最小。

第二，使协作学习成为可能的联邦协调机制也创造了新的攻击面。敌对客户端可以注入旨在降低全局模型性能的有毒梯度 38。即使聚合，模型反演攻击也可以从共享更新中提取私有信息。设备学习分布的特性使得这些攻击既容易执行（损害客户端设备）又难以检测（没有集中式验证）。

第三，设备上的系统必须在无法访问标记验证数据的情况下处理分布变化和环境变化。模型可能会自信地漂移到故障模式，适应局部偏差或暂时异常。设备间的非独立同分布数据分布意味着单个设备上的局部漂移可能不会触发全局警报，从而允许无声退化。

这些可靠性威胁需要系统性的方法来确保设备上的学习系统在自主适应、恶意操纵和环境不确定性下保持鲁棒性。第十六章全面考察了这些挑战，确立了容错 AI 系统的原则，这些系统能够在硬件故障、对抗性攻击和数据分布变化的情况下保持可靠性。在那里开发的拜占庭容错聚合、对抗性训练和漂移检测技术成为生产就绪的设备学习系统的基本组成部分，而不是可选的增强功能。

这些鲁棒性机制的隐私保护方面，包括安全聚合和差分隐私，直接关联到第十五章，该章节建立了部署大规模自学习系统所需的密码学基础和隐私保障，同时保持用户信任和合规性。

## 谬误和陷阱

设备上的学习在本质上与基于云的训练环境不同，面临着严重的资源限制和隐私要求，这挑战了传统的机器学习假设。本地适应和隐私保护的吸引力可能会掩盖决定设备学习是否比简单替代方案提供净收益的重大技术限制和实施挑战。

**谬误：** *设备学习提供了与基于云的训练相同的适应能力。*

这种误解导致团队期望本地学习能够实现与集中训练相同的模型改进，而集中训练拥有丰富的计算资源。设备学习在严重的限制下运行，包括有限的内存、受限的计算能力和最小的能源预算，这些从根本上限制了适应能力。本地数据集通常是小的、有偏见的和非代表性的，这使得无法实现与集中训练相同的泛化性能。有效的设备学习需要接受这些限制，并设计适应策略，在实用限制内提供有意义的改进，而不是试图复制云规模的学习能力。这需要一种以效率为先的思维方式和仔细的优化技术。

**陷阱：** *假设联邦学习自动保护隐私，无需额外的安全措施。*

许多从业者认为，在本地设备上保留数据本质上提供了隐私保护，而没有考虑到可以从模型更新中推断出的信息。梯度更新和参数更新可以通过各种推理攻击泄露关于本地训练数据的重大信息。设备参与模式、更新频率和模型收敛行为可以揭示关于用户及其活动的敏感信息。真正的隐私保护需要额外的机制，如差分隐私（数学上保证单个数据点不能从模型输出中推断出来）、防止参数检查的安全聚合协议以及仔细的通信协议，而不是仅仅依赖于数据本地性。

**谬误：** *资源受限的适应总是比通用模型产生更好的个性化模型。*

这种信念假设，无论可用的本地数据的质量或数量如何，任何本地适应都是有益的。在设备上使用不足、噪声或偏颇的本地数据进行学习实际上可能会降低模型性能，与训练良好的通用模型相比。小数据集可能无法提供足够的信息进行有意义的学习，而适应本地噪声可能会损害泛化能力。有效的设备学习系统必须包括机制来检测何时本地适应是有益的，并在本地数据不足以进行可靠学习时回退到通用模型。

**陷阱：** *忽视不同设备类型和能力之间的异质性挑战。*

团队通常在设计设备上学习系统时假设部署设备具有统一的硬件能力。现实世界的部署涵盖了具有不同计算能力、内存容量、能源限制和网络能力的各种硬件。一个在高端智能手机上表现良好的学习算法可能在资源受限的物联网设备上失败得非常严重 39。

**陷阱：** *低估在分布式边缘系统中协调学习的复杂性。*

许多团队专注于单个设备的优化，而没有考虑到协调数千或数百万边缘设备进行学习的系统级挑战。边缘系统编排必须处理间歇性连接、不同的电源状态、不同的时区和不可预测的设备可用性模式，这些都创造了复杂的调度和同步挑战。设备聚类、联邦轮次协调、跨不同部署环境中的模型版本控制以及处理不可靠设备的部分参与需要超越简单聚合服务器的复杂基础设施。此外，现实世界的边缘部署涉及多个利益相关者，他们有不同的激励措施、安全要求和操作程序，这些必须与学习目标相平衡。有效的边缘学习系统需要能够维持系统连贯性的强大编排框架，即使在设备不断更替、网络分区和操作中断的情况下也能保持。

## 摘要

设备上学习代表了从静态、集中式训练到动态、本地部署设备的直接适应的根本转变。这种范式使机器学习系统能够个性化体验同时保护隐私，减少网络依赖，并快速响应本地条件的变化。成功需要整合优化原则、理解硬件限制并应用合理的操作实践。从传统的基于云的训练到基于边缘的学习的转变需要克服严重的计算、内存和能源限制，这从根本上改变了模型的设计和适应方式。

能够实现设备上实际学习的专业技术策略涵盖了系统设计的多个维度。适应技术从轻量级的仅更新偏差到选择性的参数调整不等，每种方法都提供了在表达性和资源效率之间的不同权衡。当从有限的本地示例中进行学习时，数据效率变得至关重要，这推动了少量样本学习 40、流式适应和基于内存的重放机制 41 的创新。

**关键要点**

+   设备上学习将机器学习从静态部署转变为动态本地适应，在保留隐私的同时实现个性化

+   资源限制推动了专用技术的出现：仅更新偏差、适配器模块、稀疏参数更新和压缩数据表示

+   联邦学习协调异构设备上的分布式训练，同时保持隐私并处理非-IID 数据分布

+   成功需要与硬件约束协同设计算法，在适应能力与内存、能源和计算限制之间取得平衡

真实世界的应用展示了设备上学习的潜力和挑战，从适应用户声音的关键词检测系统到不传输用户数据的个性化推荐引擎。随着机器学习扩展到移动、嵌入式和可穿戴环境，在保持效率和可靠性的同时本地学习的能力对于无缝运行于各种部署环境中的下一代智能系统变得至关重要。

设备上学习的分布式特性引入了新的漏洞，这些漏洞不仅超越了单个设备限制。正是这些系统强大的能力——从用户数据中学习、适应本地模式、跨设备协调——也创造了新的攻击面和隐私风险。这些自适应系统不仅必须正确运行，还必须保护敏感用户信息并防御对抗性操纵。安全和隐私框架（第十五章）解决这些关键问题，展示了如何保护设备上学习系统免受隐私泄露和对抗性攻击。随后，鲁棒人工智能原则（第十六章）将这些保护扩展到涵盖系统级可靠性挑战，包括硬件故障和软件错误，而机器学习操作（第十三章）提供了部署和维护这些复杂自适应系统的全面框架。

* * *
