# 7.1 基础知识

> 原文：[`huyenchip.com/ml-interviews-book/contents/7.1-basics.html`](https://huyenchip.com/ml-interviews-book/contents/7.1-basics.html)

1.  [E] 解释监督学习、无监督学习、弱监督学习、半监督学习和主动学习。

1.  经验风险最小化。

    1.  [E] 经验风险最小化中的风险是什么？

    1.  [E] 为什么它是经验性的？

    1.  [E] 我们如何最小化那个风险？

1.  [E] 奥卡姆剃刀原则指出，当简单解释和复杂解释都能同样好地工作时，简单的解释通常是正确的。我们如何在机器学习中应用这个原则？

1.  [E] 允许深度学习在过去十年中流行的条件是什么？

1.  [M] 如果我们有一个具有相同参数数量的宽神经网络和深神经网络，哪一个更有表达力，为什么？

1.  [H] 广义逼近定理指出，具有 1 个隐藏层的神经网络可以逼近特定范围内的任何连续函数。那么为什么简单的神经网络不能达到任意小的正误差？

1.  [E] 什么是鞍点和局部最小值？它们被认为对训练大型神经网络造成更多问题的原因是什么？

1.  超参数。

    1.  [E] 参数和超参数之间的区别是什么？

    1.  [E] 为什么超参数调优很重要？

    1.  [M] 解释超参数调优的算法。

1.  分类与回归。

    1.  [E] 什么是分类问题和回归问题的区别？

    1.  [E] 一个分类问题能否变成回归问题，反之亦然？

1.  参数方法与非参数方法。

    1.  [E] 参数方法和非参数方法之间的区别是什么？给出每种方法的例子。

    1.  [H] 我们应该在什么时候使用一个，什么时候使用另一个？

1.  [M] 为什么将独立训练的模型进行集成通常能提高性能？

1.  [M] 为什么 L1 正则化倾向于导致稀疏性，而 L2 正则化将权重推向 0？

1.  [E] 为什么机器学习模型在生产中的性能会下降？

1.  [M] 在部署大型机器学习模型时，我们可能会遇到哪些问题？

1.  你的模型在测试集上表现很好，但在生产中表现很差。

    1.  [M] 你对原因的假设是什么？

    1.  [H] 你如何验证你的假设是否正确？

    1.  [M] 假设你对原因的假设是正确的。你会如何解决它们？
