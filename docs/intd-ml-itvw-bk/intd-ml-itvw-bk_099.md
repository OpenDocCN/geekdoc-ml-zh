# 8.3 训练神经网络

> 原文：[`huyenchip.com/ml-interviews-book/contents/8.3-training-neural-networks.html`](https://huyenchip.com/ml-interviews-book/contents/8.3-training-neural-networks.html)
> 
> 🌳 **提示** 🌳
> 
> 想了解更多关于训练神经网络的技巧，请查看：
> 
> +   [训练神经网络的秘籍](http://karpathy.github.io/2019/04/25/recipe/) (Karpathy 2019)
> +   
> +   [自然语言处理中的 Clever Hans 时刻已经到来](https://thegradient.pub/nlps-clever-hans-moment-has-arrived/) (Heinzerling 2019)：一篇关于尝试理解你的神经网络究竟学习了什么，以及确保你的模型在文本数据上正确工作的技术的优秀文章。
> +   
> +   [梯度下降优化算法概述](http://ruder.io/optimizing-gradient-descent/index.html) (Ruder 2016)

1.  [E] 当构建神经网络时，你应该先过度拟合还是欠拟合？

1.  [E] 写出 vanilla 梯度更新。

1.  简单 Numpy 中的神经网络。

    1.  [E] 用纯 NumPy 编写具有 ReLU 层的两层前馈神经网络的正向和反向传递。

    1.  [M] 在 NumPy 中实现 vanilla dropout 的前向和反向传递。

1.  激活函数。

    1.  [E] 绘制 sigmoid、tanh、ReLU 和 leaky ReLU 的图表。

    1.  [E] 每个激活函数的优缺点。

    1.  [E] ReLU 可微吗？当它不可微时怎么办？

    1.  [M] 当是向量时，推导 sigmoid 函数的导数。

1.  [E] 跳跃连接在神经网络中的动机是什么？

1.  消失和爆炸梯度。

    1.  [E] 我们如何知道梯度正在爆炸？我们如何防止它？

    1.  [E] 为什么 RNN 特别容易受到消失和爆炸梯度的影响？

1.  [M] 权重归一化将权重向量的范数与其梯度分开。这如何有助于训练？

1.  [M] 当训练一个大型神经网络，比如一个拥有十亿参数的语言模型时，你会在每个 epoch 结束时在验证集上评估你的模型。你发现验证损失通常低于训练损失。可能发生了什么？

1.  [E] 你会使用什么标准来决定提前停止？

1.  [E] 梯度下降 vs SGD vs 小批量 SGD。

1.  [H] 使用 epoch 来训练深度学习模型是一种常见做法：我们从数据中采样批次而不进行替换。为什么我们会使用 epoch 而不是仅仅用替换的方式来采样数据？

1.  [M] 你的模型权重在训练过程中波动很大。这如何影响你的模型性能？你该如何处理？

1.  学习率。

    1.  [E] 绘制当学习率为时的训练轮数与训练误差的图表。

        1.  太高

        1.  太低

        1.  可接受的。

    1.  [E] 学习率预热是什么？为什么我们需要它？

1.  [E] 比较批归一化和层归一化。

1.  [M] 为什么平方 L2 范数有时比 L2 范数更适合用于正则化神经网络？

1.  [E] 一些模型使用权重衰减：在每次梯度更新后，权重乘以一个略小于 1 的因子。这有什么用？

1.  在训练过程中降低学习率是一种常见做法。

    1.  [E] 有什么动机？

    1.  [M] 可能有哪些例外情况？

1.  批大小。

    1.  [E] 当你将批大小减少到 1 时，你的模型训练会发生什么？

    1.  [E] 当你在一个批中使用全部训练数据时会发生什么？

    1.  [M] 随着批大小的增加或减少，我们应该如何调整学习率？

1.  [M] 为什么 Adagrad 在稀疏梯度问题中有时更受欢迎？

1.  Adam 与 SGD。

    1.  [M] 你能对 Adam 与 SGD 的收敛能力和泛化能力说些什么？

    1.  [M] 你还能对这两种优化器的区别说些什么？

1.  [M] 在模型并行化中，你可能使用每个机器的梯度异步或同步地更新你的模型权重。异步 SGD 与同步 SGD 的优缺点是什么？

1.  [M] 为什么我们不应该在神经网络中有两个连续的线性层？

1.  [M] 只使用 RELU (非线性) 的神经网络能作为线性分类器吗？

1.  [M] 设计一个最小的神经网络，使其能作为 XOR 门使用。

1.  [E] 为什么我们不直接将神经网络中的所有权重初始化为零？

1.  随机性。

    1.  [M] 神经网络中的一些随机性来源有哪些？

    1.  [M] 在训练神经网络时，随机性有时是可取的。为什么？

1.  死神经元。

    1.  [E] 死神经元是什么？

    1.  [E] 我们如何在神经网络中检测它们？

    1.  [M] 如何防止它们？

1.  剪枝。

    1.  [M] 剪枝是一种流行的技术，其中神经网络的一些权重被设置为 0。为什么这是可取的？

    1.  [M] 你如何选择从神经网络中剪枝的内容？

1.  [H] 在什么条件下可以从权重检查点中恢复训练数据？

1.  [H] 为什么我们试图通过知识蒸馏等技术来减小大型训练模型的大小，而不是从一开始就训练一个小的模型？

* * *

*这本书是由 [Chip Huyen](https://huyenchip.com) 在众多朋友的帮助下创作的。对于反馈、勘误和建议，作者可以通过[这里](https://huyenchip.com/communication/)联系。版权©2021 Chip Huyen。*
