# 5.1.3 维度约简

> 原文：[`huyenchip.com/ml-interviews-book/contents/5.1.3-dimensionality-reduction.html`](https://huyenchip.com/ml-interviews-book/contents/5.1.3-dimensionality-reduction.html)

*如果某些字符似乎缺失，那是因为 MathJax 没有正确加载。刷新页面应该可以解决这个问题。*

1.  [E] 我们为什么需要维度约简？

1.  [E] 特征值分解是一种常用的因子分解技术，用于维度约简。矩阵的特征值分解总是唯一的吗？

1.  [M] 列举一些特征值和特征向量的应用。

1.  [M] 我们想在具有不同范围的多个特征的数据集上执行 PCA。例如，一个是 0-1 的范围，另一个是 10-1000 的范围。PCA 在这个数据集上会起作用吗？

1.  [H] 在什么条件下可以应用特征值分解？SVD 呢？

    1.  SVD 和特征值分解之间有什么关系？

    1.  PCA 和 SVD 之间有什么关系？

1.  [H] t-SNE（T-分布随机邻域嵌入）是如何工作的？为什么我们需要它？

> **如果您需要 PCA 的复习，这里有一个没有数学解释的解释。**
> 
> 假设您的祖母喜欢葡萄酒，并希望找到最能描述她地窖中葡萄酒瓶的特征。我们可以使用许多特征来描述一瓶葡萄酒，包括年龄、价格、颜色、酒精含量、甜度、酸度等。许多这些特征是相关的，因此是冗余的。我们有没有办法选择更少的特征来描述我们的葡萄酒，并回答像：哪两瓶葡萄酒差异最大这样的问题？
> 
> PCA 是一种从现有特征中构建新特征的技术。例如，一个新特征可能被计算为`年龄 - 酸度 + 价格`或类似的东西，我们称之为线性组合。
> 
> 为了区分我们的葡萄酒，我们希望找到在葡萄酒之间强烈差异的特征。如果我们找到一个对所有葡萄酒都相同的特征，那么它就不会很有用。PCA 寻找的是在所有现有特征的线性组合中，尽可能多地显示葡萄酒之间差异的特征。这些构建的特征是我们葡萄酒的主成分。
> 
> 如果您想看到一个更详细、直观的 PCA 解释，包括可视化，请查看[amoeba 在 StackOverflow 上的答案](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579)。这可能是我读过的最好的 PCA 解释。
