# å¤šé¡¹å¼å›å½’

> åŸæ–‡ï¼š[`geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_polynomial_regression.html`](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_polynomial_regression.html)

è¿ˆå…‹å°”Â·JÂ·çš®å°”èŒ¨ï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

ç”µå­ä¹¦â€œPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œâ€çš„ç« èŠ‚ã€‚

è¯·å°†æ­¤ç”µå­ä¹¦å¼•ç”¨å¦‚ä¸‹ï¼š

çš®å°”èŒ¨ï¼ŒM.J.ï¼Œ2024ï¼Œ*Python åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œ* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138 ![DOI](https://doi.org/10.5281/zenodo.15169138)

æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠæ›´å¤šå†…å®¹åœ¨æ­¤å¤„å¯ç”¨ï¼š

è¯·å°† MachineLearningDemos GitHub ä»“åº“å¼•ç”¨å¦‚ä¸‹ï¼š

çš®å°”èŒ¨ï¼ŒM.J.ï¼Œ2024ï¼Œ*MachineLearningDemos: Python æœºå™¨å­¦ä¹ æ¼”ç¤ºå·¥ä½œæµç¨‹ä»“åº“* (0.0.3) [è½¯ä»¶]. Zenodo. DOI: 10.5281/zenodo.13835312\. GitHub ä»“åº“ï¼š[GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos) ![DOI](https://zenodo.org/doi/10.5281/zenodo.13835312)

è¿ˆå…‹å°”Â·JÂ·çš®å°”èŒ¨

Â© ç‰ˆæƒæ‰€æœ‰ 2024ã€‚

æœ¬ç« æ˜¯å…³äº/æ¼”ç¤º**å¤šé¡¹å¼å›å½’**çš„æ•™ç¨‹ã€‚

**YouTube è®²åº§**ï¼šæŸ¥çœ‹æˆ‘å…³äºä»¥ä¸‹å†…å®¹çš„è®²åº§ï¼š

+   [æœºå™¨å­¦ä¹ ç®€ä»‹](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)

+   [çº¿æ€§å›å½’](https://youtu.be/0fzbyhWiP84?si=uRdmHOTzdnUvDPA9)

+   [å¤šé¡¹å¼å›å½’](https://youtu.be/z19Hs2HfO88?si=etUIb3LegiTigEio)

+   [æ•°å€¼ä¼˜åŒ–](https://youtu.be/4nYz5j0sAQs?si=n_553YQdh5grTquV)

è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘ YouTube ä¸Šçš„[æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«é“¾æ¥è‰¯å¥½çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºè·å–ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœä½ æƒ³äº†è§£æˆ‘çš„åŠ¨æœºï¼Œè¯·æŸ¥çœ‹[è¿ˆå…‹å°”çš„æ•…äº‹](https://michaelpyrcz.com/my-story)ã€‚

## å¤šé¡¹å¼å›å½’çš„åŠ¨æœº

é€šè¿‡ä»çº¿æ€§å›å½’è¿‡æ¸¡åˆ°å¤šé¡¹å¼å›å½’ï¼Œæˆ‘ä»¬ï¼Œ

+   é€šè¿‡åœ¨æ•°æ®ä¸­å»ºæ¨¡éçº¿æ€§æ¥å¢åŠ é¢„æµ‹çµæ´»æ€§

+   å»ºç«‹åœ¨ç‰¹å¾å·¥ç¨‹æ¦‚å¿µçš„ç‰¹å¾æ‰©å±•ä¹‹ä¸Š

åŒæ—¶ï¼Œä»çº¿æ€§å›å½’è®­ç»ƒæ¨¡å‹å‚æ•°çš„è§£æè§£ä¸­å—ç›Šã€‚

æˆ‘ä»¬é€šè¿‡åŸºå‡½æ•°å±•å¼€å®Œæˆæ‰€æœ‰è¿™äº›ï¼Œ

+   æˆ‘ä»¬å°†ç‰¹å¾è¿›è¡Œè½¬æ¢å’Œå±•å¼€ $\rightarrow$ å¼•å…¥åŸºå‡½æ•°å±•å¼€ï¼

+   æˆ‘ä»¬å¯ä»¥é€šè¿‡å¼•å…¥éçº¿æ€§åŸºå‡½æ•°æ¥å¢åŠ æˆ‘ä»¬çš„é¢„æµ‹æ¨¡å‹å¤æ‚æ€§å’Œçµæ´»æ€§ $\rightarrow$ éçº¿æ€§åŸºï¼

+   æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¶ˆé™¤å¤šé‡å…±çº¿æ€§ $\rightarrow$ æ­£äº¤åŸºæ¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ï¼

è®©æˆ‘ä»¬ä»çº¿æ€§å›å½’å¼€å§‹ï¼Œç„¶åé€æ­¥è¿‡æ¸¡åˆ°å¤šé¡¹å¼å›å½’ã€‚

## çº¿æ€§å›å½’

çº¿æ€§å›å½’é¢„æµ‹ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹ä¸€ç»„æ•°æ®æ‹Ÿåˆçš„çº¿æ€§æ¨¡å‹ã€‚

![å›¾ç‰‡](img/806bf5f702f9bb5a63e30d6e1f7969d9.png)

çº¿æ€§å›å½’æ¨¡å‹ç¤ºä¾‹ã€‚

è®©æˆ‘ä»¬å…ˆå®šä¹‰ä¸€äº›æœ¯è¯­ï¼Œ

+   **é¢„æµ‹ç‰¹å¾** - é¢„æµ‹æ¨¡å‹çš„è¾“å…¥ç‰¹å¾ï¼Œé‰´äºæˆ‘ä»¬åªè®¨è®ºçº¿æ€§å›å½’è€Œä¸æ˜¯å¤šå…ƒçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œ$x$ã€‚åœ¨æˆ‘ä»¬çš„å›¾è¡¨ï¼ˆåŒ…æ‹¬ä¸Šé¢çš„ï¼‰ä¸­ï¼Œé¢„æµ‹ç‰¹å¾ä½äº x è½´ä¸Šã€‚

+   **å“åº”ç‰¹å¾** - é¢„æµ‹æ¨¡å‹çš„è¾“å‡ºç‰¹å¾ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ$y$ã€‚åœ¨æˆ‘ä»¬çš„å›¾è¡¨ï¼ˆåŒ…æ‹¬ä¸Šé¢çš„ï¼‰ä¸­ï¼Œå“åº”ç‰¹å¾ä½äº y è½´ä¸Šã€‚

ç°åœ¨ï¼Œä»¥ä¸‹æ˜¯çº¿æ€§å›å½’çš„ä¸€äº›å…³é”®æ–¹é¢ï¼š

**å‚æ•°æ¨¡å‹**

è¿™æ˜¯ä¸€ä¸ªå‚æ•°é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬æ¥å—ä¸€ä¸ªå…ˆéªŒçš„çº¿æ€§å‡è®¾ï¼Œç„¶åè·å¾—ä¸€ä¸ªéå¸¸ä½çš„å‚æ•°è¡¨ç¤ºï¼Œæ˜“äºè®­ç»ƒï¼Œæ— éœ€å¤§é‡æ•°æ®ã€‚

+   æ‹Ÿåˆæ¨¡å‹æ˜¯ä¸€ä¸ªåŸºäºæ‰€æœ‰å¯ç”¨ç‰¹å¾ $x_1,\ldots,x_m$ çš„ç®€å•åŠ æƒçº¿æ€§åŠ æ€§æ¨¡å‹ã€‚

+   å‚æ•°æ¨¡å‹çš„å½¢å¼ä¸ºï¼š

$$ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 $$

è¿™æ˜¯çº¿æ€§æ¨¡å‹å‚æ•°çš„å¯è§†åŒ–ï¼Œ

![å›¾ç‰‡](img/ada2fcc2740c48478e79404563c91061.png)

çº¿æ€§æ¨¡å‹å‚æ•°ã€‚

**æœ€å°äºŒä¹˜æ³•**

å¯¹äº L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼Œæ¨¡å‹å‚æ•° $b_1,\ldots,b_m,b_0$ çš„è§£æè§£æ˜¯å¯ç”¨çš„ï¼Œè¯¯å·®æ˜¯æ±‚å’Œå¹¶å¹³æ–¹çš„ï¼Œå·²çŸ¥ä¸ºæœ€å°äºŒä¹˜æ³•ã€‚

+   æˆ‘ä»¬åœ¨è®­ç»ƒæ•°æ®ä¸Šæœ€å°åŒ–è¯¯å·®ï¼Œæ®‹å·®å¹³æ–¹å’Œï¼ˆRSSï¼‰ï¼š

$$ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)Â² $$

å…¶ä¸­ $y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾å€¼ï¼Œ$\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ æ˜¯æ¨¡å‹é¢„æµ‹ï¼Œåœ¨ $\alpha = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šã€‚

è¿™æ˜¯ L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼ŒMSE çš„å¯è§†åŒ–ï¼Œ

![å›¾ç‰‡](img/835541b16e1038a4606f7d97b628c4f9.png)

çº¿æ€§æ¨¡å‹çš„æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®ã€‚

+   è¿™å¯ä»¥ç®€åŒ–ä¸ºè®­ç»ƒæ•°æ®ä¸Šçš„å¹³æ–¹è¯¯å·®ä¹‹å’Œï¼Œ

$$ \sum_{i=1}^n (\Delta y_i)Â² $$

å…¶ä¸­ $\Delta y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾è§‚å¯Ÿ $y_i$ å‡å»æ¨¡å‹é¢„æµ‹ $\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ï¼Œåœ¨ $i = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šã€‚

**å‡è®¾**

æˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹æœ‰ä¸€äº›é‡è¦çš„å‡è®¾ï¼Œ

+   **æ— è¯¯å·®** - é¢„æµ‹å˜é‡æ˜¯æ— è¯¯å·®çš„ï¼Œä¸æ˜¯éšæœºå˜é‡

+   **çº¿æ€§** - å“åº”æ˜¯ç‰¹å¾ï¼ˆsï¼‰çš„çº¿æ€§ç»„åˆ

+   **å¸¸æ•°æ–¹å·®** - å“åº”è¯¯å·®åœ¨é¢„æµ‹å™¨ï¼ˆä»¬ï¼‰çš„å€¼ä¸Šä¿æŒæ’å®š

+   **è¯¯å·®ç‹¬ç«‹æ€§** - å“åº”è¯¯å·®ä¹‹é—´ç›¸äº’ä¸ç›¸å…³

+   **æ— å¤šé‡å…±çº¿æ€§** - æ²¡æœ‰ç‰¹å¾ä¸å…¶ä»–ç‰¹å¾å†—ä½™

## é¢„æµ‹å™¨ç‰¹å¾ / åŸºç¡€æ‰©å±•

æˆ‘ä»¬å¯ä»¥é€šè¿‡åº”ç”¨åŸºç¡€æ‰©å±•å’Œåº”ç”¨äºæˆ‘ä»¬çš„é¢„æµ‹å™¨ç‰¹å¾çš„åŸºç¡€å‡½æ•°æ¥æé«˜æ¨¡å‹çš„çµæ´»æ€§å’Œå¤æ‚æ€§ã€‚åŸºæœ¬æ€æƒ³æ˜¯åˆ©ç”¨ä¸€å¥—åŸºç¡€å‡½æ•°ï¼Œ$h_1, h_2, \ldots, h_k$ï¼Œæä¾›æ–°çš„é¢„æµ‹å™¨ç‰¹å¾ã€‚

$$ h(x_i) = (h_1(x_i),h_1(x_i),\ldots,h_k(x_i)) $$

æˆ‘ä»¬ä»å•ä¸ªç‰¹å¾ $X$ åˆ°æ‰©å±•çš„ $k$ ä¸ªç‰¹å¾åŸºï¼Œ$X_1, X_2,\ldots, X_k$ã€‚

+   å¦‚æœæˆ‘ä»¬çš„æ•°æ®è¡¨ä¸­å…·æœ‰ $m$ ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ $k \times m$ ä¸ªç‰¹å¾

![](img/3cf75cc4ca509f9dd86ecfb64061b7cf.png)

é¢„æµ‹å™¨ $m$ ä¸ªç‰¹å¾çš„åŸºç¡€æ‰©å±•ï¼Œä½¿ç”¨ $k$ ä¸ªåŸºç¡€å‡½æ•°åˆ° $m \times k$ æ‰©å±•ç‰¹å¾ã€‚

## å¤šé¡¹å¼å›å½’

å¯ä»¥è¯æ˜å¤šé¡¹å¼å›å½’åªæ˜¯å°†çº¿æ€§å›å½’åº”ç”¨äºé¢„æµ‹å™¨ç‰¹å¾çš„å¤šé¡¹å¼æ‰©å±•ã€‚

$$ X_{j} \rightarrow X_{j}, X_{j}Â², X_{j}Â³, \ldots X_{j}^k $$

å…¶ä¸­æˆ‘ä»¬æ‹¥æœ‰ $j = 1, \ldots, m$ ä¸ªåŸå§‹ç‰¹å¾ã€‚

æˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªæ‰©å±•çš„é¢„æµ‹å™¨ç‰¹å¾é›†ã€‚

$$ h_{j,k}(X_j) = X_j^k $$

åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰ $j = 1, \ldots, m$ ä¸ªåŸå§‹ç‰¹å¾å’Œ $k = 1, \ldots, K$ ä¸ªå¤šé¡¹å¼é˜¶æ•°ã€‚

æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†æˆ‘ä»¬çš„æ¨¡å‹è¡¨è¿°ä¸ºè½¬æ¢ç‰¹å¾çš„çº¿æ€§å›å½’ã€‚

$$ y = f(x) = \sum_{j=1}^{m} \sum_{k = 1}^{K} \beta_{j,k} h_{j,m}(X_j) + \beta_0 $$

ç»è¿‡ $h_l, l=1,\ldots,k$ è½¬æ¢åï¼Œåœ¨ $j=1,\ldots,m$ é¢„æµ‹å™¨ç‰¹å¾ä¸Šï¼Œæˆ‘ä»¬æœ‰ç›¸åŒçš„çº¿æ€§æ–¹ç¨‹å’Œåˆ©ç”¨å…ˆå‰è®¨è®ºçš„è§£æè§£çš„èƒ½åŠ›ï¼Œå‚è§çº¿æ€§å›å½’ç« èŠ‚ã€‚

æˆ‘ä»¬å‡è®¾åœ¨åº”ç”¨åŸºç¡€è½¬æ¢åæ˜¯çº¿æ€§çš„ã€‚

+   ç°åœ¨æ¨¡å‹ç³»æ•°ï¼Œ$\beta_{l,i}$ï¼Œä¸åˆå§‹é¢„æµ‹å™¨ç‰¹å¾çš„è½¬æ¢ç‰ˆæœ¬ç›¸å…³ï¼Œ$h_l(X_i)$ã€‚

+   ä½†æˆ‘ä»¬å¤±å»äº†è§£é‡Šç³»æ•°çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œ$\phiâ´$ åœ¨ $\phi$ æ˜¯å­”éš™ç‡çš„æƒ…å†µä¸‹æ˜¯ä»€ä¹ˆï¼Ÿ

ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªå•ä¸€çš„é¢„æµ‹å™¨ç‰¹å¾ï¼Œ$m = 1$ï¼Œå¹¶ä¸”ç›´åˆ° $4^{th}$ é˜¶ï¼Œæ¨¡å‹æ˜¯ï¼Œ

$$ y = \beta_{1,1}X_1 + \beta_{1,1}X_1Â² + \beta_{1,3}X_1Â³ + \beta_{1,4}X_1â´ + \beta_0 $$

å…¶ä¸­æ¨¡å‹å‚æ•°ç¬¦å·æ˜¯ $\beta_{m,k}$ï¼Œå…¶ä¸­ $m$ æ˜¯ç‰¹å¾ï¼Œ$k$ æ˜¯é˜¶æ•°ã€‚ä¸ºäº†æ¾„æ¸…ï¼Œè¿™é‡Œæ˜¯å¯¹ $m = 2$ çš„æƒ…å†µï¼Œ

$$ y = \beta_{1,1}X_1 + \beta_{1,2}X_1Â² + \beta_{1,3}X_1Â³ + \beta_{1,4}X_1â´ + \beta_{2,1}X_2 + \beta_{2,2}X_2Â² + \beta_{2,3}X_2Â³ + \beta_{2,4}X_2â´ + \beta_0 $$

å› æ­¤ï¼Œæˆ‘ä»¬çš„é¢„æµ‹å»ºæ¨¡å·¥ä½œæµç¨‹æ˜¯ï¼š

+   åº”ç”¨å¤šé¡¹å¼åŸºç¡€æ‰©å±•

+   åœ¨å¤šé¡¹å¼åŸºç¡€æ‰©å±•ä¸Šæ‰§è¡Œçº¿æ€§å›å½’

## å¤šé¡¹å¼å›å½’çš„ä¼˜ç‚¹å’Œç¼ºç‚¹

å¤šé¡¹å¼å›å½’ç›¸å¯¹äºçº¿æ€§å›å½’çš„ä¼˜ç‚¹åŒ…æ‹¬ï¼Œ

+   æé«˜çµæ´»æ€§ä»¥æ‹Ÿåˆéçº¿æ€§ç°è±¡ï¼Œä½¿ç”¨çº¿æ€§åˆ†æå’Œè§£æè§£æ¥è®­ç»ƒæ¨¡å‹å‚æ•°ã€‚

ç¼ºç‚¹

é€šå¸¸ï¼Œæ¨¡å‹æ–¹å·®æ˜¾è‘—æ›´é«˜ï¼å¯èƒ½å­˜åœ¨ä¸ç¨³å®šçš„æ’å€¼å’Œç‰¹åˆ«æ˜¯å¤–æ¨ã€‚

å¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼Œç‰¹åˆ«æ˜¯å½“ $â„_ğ‘˜ \left(ğ‘¥_{ğ‘–,ğ‘—}\right)=ğ‘¥_{ğ‘–,ğ‘—}^ğ‘˜$ ä¸” $ğ‘˜$ è¾ƒå¤§æ—¶ã€‚

æˆ‘ä»¬å¤±å»äº†æ¨¡å‹å‚æ•°çš„å¯è§£é‡Šæ€§ï¼Œ$ğ›½_{ğ‘—,ğ‘˜}$ ä¸ $â„_ğ‘˜ \left(ğ‘‹_j \right)$ ç›¸å…³ã€‚

## æ·»åŠ åŸºæœ¬å‡½æ•°

å¤šé¡¹å¼å›å½’çš„å¦ä¸€ç§è§£é‡Šæ˜¯é€šè¿‡æ·»åŠ åŸºæœ¬å‡½æ•°æ¥æ„å»ºå›å½’æ¨¡å‹ï¼Œå³åŸºå‡½æ•°ã€‚

è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå•é¢„æµ‹ç‰¹å¾å’Œ$K$åŸºå‡½æ•°å±•å¼€ã€‚

$$ y = \sum_{l=1}^{k} \beta_{1,k} h_k (X_j) $$

å¯¹äºæˆ‘ä»¬çš„ç®€å•å•é¢„æµ‹ç‰¹å¾$X$çš„å¤šé¡¹å¼é—®é¢˜ï¼Œè¿™æ˜¯ï¼Œ

$$ y = \beta_{1,K} X^K + \beta_{1,K-1} X^{K-1} + \dots + \beta_{1,2} XÂ² + \beta_{1,1} X + \beta_0 $$

è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª 4 é˜¶å¤šé¡¹å¼å±•å¼€ï¼Œ$K=4$ï¼Œæ¥æ ‡å‡†åŒ–æ·±åº¦ã€‚

![å›¾ç‰‡](img/ea64332d4805861caa74b4d26e6bd3f0.png)

å¤šé¡¹å¼åŸºå‡½æ•°è‡³$K=4$ã€‚

æ„å»ºæˆ‘ä»¬çš„å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬æ­£åœ¨ç§»åŠ¨ã€ç¼©æ”¾å’Œæ·»åŠ è¿™äº›åŸºæœ¬å‡½æ•°ã€‚è®©æˆ‘ä»¬é€šè¿‡$k=2$åŸºå‡½æ•°çš„ä¾‹å­ï¼Œå³æŠ›ç‰©çº¿ï¼Œ$h_2: ğ‘¦=ğ‘¥Â²$ï¼Œæ¥å›é¡¾å¦‚ä½•è¿›è¡Œç§»åŠ¨å’Œç¼©æ”¾ã€‚è€ƒè™‘ä»¥ä¸‹å˜åŒ–ï¼š

+   åœ¨ X è½´ä¸Šå¹³ç§»ã€‚

+   åœ¨ Y è½´ä¸Šå¹³ç§»

+   åœ¨ X è½´ä¸Šç¿»è½¬ã€‚

+   æ”¹å˜æ–œç‡

å¯¹äºæ¯ä¸€ä¸ªï¼Œæˆ‘éƒ½å±•ç¤ºäº†å˜åŒ–çš„å¯è§†åŒ–ï¼Œç„¶åæ˜¯å®ƒå¯¹å¤šé¡¹å¼æ–¹ç¨‹çš„å½±å“ã€‚

+   åœ¨ X è½´ä¸Šå¹³ç§»å‡½æ•°ï¼Œ

![å›¾ç‰‡](img/87df4ff1a6183394b90b31dfe989e9f7.png)

åœ¨ X è½´ä¸Šå¹³ç§»äºŒé˜¶åŸºæœ¬å‡½æ•°ã€‚

$$ y = (x - \Delta_x)Â² = xÂ² - 2\Delta_x x + \Delta_xÂ² $$

+   åœ¨ Y è½´ä¸Šå¹³ç§»å‡½æ•°ï¼Œ

![å›¾ç‰‡](img/87df4ff1a6183394b90b31dfe989e9f7.png)

åœ¨ Y è½´ä¸Šå¹³ç§»äºŒé˜¶åŸºæœ¬å‡½æ•°ã€‚

$$ y = xÂ² - \Delta_y $$

+   åœ¨ X è½´ä¸Šç¿»è½¬å‡½æ•°ï¼š

![å›¾ç‰‡](img/2e93ae27cb57ce4b016c4823c8e50642.png)

åœ¨ X è½´ä¸Šç¿»è½¬äºŒé˜¶åŸºæœ¬å‡½æ•°ã€‚

$$ y = \pm \beta_2 xÂ² $$

+   æ”¹å˜æ–œç‡ï¼š

![å›¾ç‰‡](img/63aa39b205aca7c3c08dd272484377e3.png)

æ”¹å˜äºŒé˜¶åŸºæœ¬å‡½æ•°çš„æ–œç‡ã€‚

$$ y = \downarrow \beta_2 xÂ², \text{æ›´å®½/æ›´æµ…} $$$$ y = \uparrow \beta_2 xÂ², \text{æ›´çª„/æ›´æ·±} $$

è®©æˆ‘ä»¬ä»ä¸Šé¢è§‚å¯Ÿä¸€äº›å†…å®¹ï¼Œ

+   åœ¨ Y è½´ä¸Šå¹³ç§»åªéœ€è¦ä¿®æ”¹å¤šé¡¹å¼æ–¹ç¨‹ä¸­æ¨¡å‹å‚æ•°çš„å¸¸æ•°é¡¹

+   åœ¨ X è½´ä¸Šå¹³ç§»éœ€è¦ä¿®æ”¹å¤šé¡¹å¼æ–¹ç¨‹ä¸­ä½é˜¶æ¨¡å‹å‚æ•°

+   åœ¨ X è½´ä¸Šç¿»è½¬éœ€è¦æ”¹å˜å¤šé¡¹å¼æ–¹ç¨‹ä¸­å½“å‰é˜¶æ¨¡å‹å‚æ•°çš„ç¬¦å·

+   å¢åŠ æ–œç‡éœ€è¦å¢åŠ å¤šé¡¹å¼æ–¹ç¨‹ä¸­å½“å‰é˜¶æ¨¡å‹å‚æ•°

## å¤šé¡¹å¼å›å½’çš„å‡è®¾

æˆ‘ä»¬çš„å¤šé¡¹å¼å›å½’æ¨¡å‹æœ‰ä¸€äº›é‡è¦çš„å‡è®¾ï¼Œè¿™äº›å‡è®¾æ‰©å±•äº†ä¸Šè¿°çº¿æ€§å›å½’çš„å‡è®¾ï¼Œ

+   **æ— è¯¯å·®** - é¢„æµ‹ç‰¹å¾åŸºå‡½æ•°å±•å¼€æ˜¯æ— è¯¯å·®çš„ï¼Œä¸æ˜¯éšæœºå˜é‡

+   **å¸¸æ•°æ–¹å·®** - å“åº”è¯¯å·®åœ¨é¢„æµ‹å€¼ä¸Šæ˜¯å¸¸æ•°

+   **çº¿æ€§** - å“åº”æ˜¯åŸºç‰¹å¾çš„çº¿æ€§ç»„åˆ

+   **å¤šé¡¹å¼** - X å’Œ Y ä¹‹é—´çš„å…³ç³»æ˜¯å¤šé¡¹å¼

+   **è¯¯å·®ç‹¬ç«‹æ€§** - å“åº”è¯¯å·®ä¹‹é—´ç›¸äº’ä¸ç›¸å…³

+   **æ— å…±çº¿æ€§** - åŸºç‰¹å¾å±•å¼€ä¸­çš„ä»»ä½•ä¸€ä¸ªéƒ½ä¸æ˜¯å…¶ä»–ç‰¹å¾çš„çº¿æ€§å†—ä½™

è€ƒè™‘ä¸Šè¿°å¤šé¡¹å¼åŸºå±•å¼€ï¼Œæ£€æŸ¥æˆ‘ä»¬åŸºä¹‹é—´çš„å…±çº¿æ€§ã€‚ä¸ºäº†æ£€æŸ¥ï¼Œæˆ‘è®¡ç®—äº†ä¸‹é¢æ¼”ç¤ºä¸­ä½¿ç”¨çš„åŸºç¡€å±•å¼€çš„ç›¸å…³çŸ©é˜µã€‚

![](img/08d2443894d5916687f1cf4785734bec.png)

$K=4$ çš„å¤šé¡¹å¼åŸºå±•å¼€çš„ç›¸å…³çŸ©é˜µã€‚

$K=1$ å’Œ $K=3$ çš„åŸºä¸ $k=2$ å’Œ $k=4$ çš„åŸºä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„å…±çº¿æ€§ã€‚

+   å›æƒ³ä¸€ä¸‹ï¼Œå…±çº¿æ€§å’Œå¤šå…±çº¿æ€§å¯èƒ½ä¼šå¢åŠ æ¨¡å‹æ–¹å·®

ä¸ºäº†æ¶ˆé™¤è¿™ç§å…±çº¿æ€§ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨å„ç±³å¤šé¡¹å¼ã€‚

## **å„ç±³å¤šé¡¹å¼**

æ˜¯å®æ•°çº¿ä¸Šçš„æ­£äº¤å¤šé¡¹å¼æ—ã€‚

| é˜¶æ•° | å„ç±³å¤šé¡¹å¼ $H_e(x)$ |
| --- | --- |
| é›¶é˜¶ | $H_{e_0}(x) = 1$ |
| ä¸€é˜¶ | $H_{e_1}(x) = x$ |
| äºŒé˜¶ | $H_{e_2}(x) = xÂ² - 1$ |
| ä¸‰é˜¶ | $H_{e_3}(x) = xÂ³ - 3x$ |
| å››é˜¶ | $H_{e_4}(x) = xâ´ - 6xÂ² + 3$ |

è¿™äº›å¤šé¡¹å¼ç›¸å¯¹äºåŠ æƒå‡½æ•°æ˜¯æ­£äº¤çš„ï¼Œ

$$ ğ‘¤(ğ‘¥)=ğ‘’^{âˆ’\frac{ğ‘¥Â²}{2}} $$

è¿™æ˜¯æ ‡å‡†é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œä¸å¸¦æ ‡åº¦ï¼Œ$\frac{1}{\sqrt{2\pi}}$ã€‚æ­£äº¤æ€§çš„å®šä¹‰å¦‚ä¸‹ï¼Œ

$$ \int_{-\infty}^{\infty} H_m(x) H_n(x) w(x) \, dx = 0 $$

å„ç±³å¤šé¡¹å¼åœ¨æ ‡å‡†æ­£æ€æ¦‚ç‡åˆ†å¸ƒçš„åŒºé—´ $[âˆ’\infty,\infty]$ ä¸Šæ˜¯æ­£äº¤çš„ã€‚

é€šè¿‡åœ¨å¤šé¡¹å¼å›å½’ä¸­åº”ç”¨å„ç±³å¤šé¡¹å¼è€Œä¸æ˜¯å¸¸è§„å¤šé¡¹å¼è¿›è¡Œå¤šé¡¹å¼åŸºå±•å¼€ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†é¢„æµ‹ç‰¹å¾ä¹‹é—´çš„å…±çº¿æ€§ï¼Œ

+   å›æƒ³ä¸€ä¸‹ï¼Œé¢„æµ‹ç‰¹å¾ç‹¬ç«‹æ€§æ˜¯å¤šé¡¹å¼åŸºå±•å¼€åœ¨å¤šé¡¹å¼å›å½’ä¸­åº”ç”¨çš„çº¿æ€§ç³»ç»Ÿçš„å‡è®¾

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = True
import os                                                     # to set current working directory 
import math                                                   # square root operator
import numpy as np                                            # arrays and matrix math
import scipy                                                  # Hermite polynomials
from scipy import stats                                       # statistical methods
import pandas as pd                                           # DataFrames
import pandas.plotting as pd_plot
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import seaborn as sns                                         # for matrix scatter plots
from sklearn.linear_model import LinearRegression             # linear regression with scikit learn
from sklearn.preprocessing import PolynomialFeatures          # polynomial basis expansion
from sklearn import metrics                                   # measures to check our models
from sklearn.preprocessing import (StandardScaler,PolynomialFeatures) # standardize the features, polynomial basis expansion
from sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning
from sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline
from sklearn import metrics                                   # measures to check our models
from sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation
from sklearn.model_selection import train_test_split          # train and test split
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # supress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½é¦–å…ˆéœ€è¦å®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£ï¼Œè¿˜æœ‰æ›´å¤šå¸®åŠ©å¯ç”¨ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæ–¹ä¾¿çš„å‡½æ•°æ¥æ·»åŠ ç½‘æ ¼çº¿åˆ°æˆ‘ä»¬çš„å›¾è¡¨ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³çŸ©é˜µã€‚

```py
def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks

def plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix 
    my_colormap = plt.get_cmap('RdBu_r', 256)          
    newcolors = my_colormap(np.linspace(0, 1, 256))
    white = np.array([256/256, 256/256, 256/256, 1])
    white_low = int(128 - mask*128); white_high = int(128+mask*128)
    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)
    newcmp = ListedColormap(newcolors)
    m = corr_matrix.shape[0]
    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)
    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()
    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()
    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
    plt.colorbar(im, orientation = 'vertical')
    plt.title(title)
    for i in range(0,m):
        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')
        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')
    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5]) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆæ¯æ¬¡é¿å…åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/PGE383")                                        # set the working directory 
```

æ‚¨å°†ä¸å¾—ä¸æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ä»¥åŒ…å«æ‚¨è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”æ ¼å¼åœ¨ Mac ä¸Šä¸åŒï¼ˆä¾‹å¦‚ï¼Œâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½æ•°æ®

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„äºŒå…ƒã€ç©ºé—´æ•°æ®é›†[Density_Por_data.csv](https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv)ï¼Œè¯¥æ•°æ®é›†åœ¨æˆ‘çš„ GeoDataSet ä»“åº“ä¸­å¯ç”¨ã€‚å®ƒæ˜¯ä¸€ä¸ªé€—å·åˆ†éš”çš„æ–‡ä»¶ï¼ŒåŒ…å«ï¼š

+   æ·±åº¦ï¼ˆç±³ï¼‰

+   é«˜æ–¯è½¬æ¢çš„å­”éš™ç‡ï¼ˆ%ï¼‰

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸ºâ€˜dfâ€™çš„æ•°æ®å¸§ä¸­ã€‚

```py
df = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/1D_Porosity.csv") # data from Dr. Pyrcz's github repository 
```

## å¯è§†åŒ–æ•°æ®å¸§

å¯è§†åŒ–è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®å¸§æ˜¯åœ¨æˆ‘ä»¬æ„å»ºæ¨¡å‹ä¹‹å‰çš„ä¸€ä¸ªæœ‰ç”¨çš„æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨â€˜headâ€™æ•°æ®å¸§æˆå‘˜å‡½æ•°æ¥é¢„è§ˆï¼ˆæ ¼å¼æ•´æ´ã€ç¾è§‚ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚

```py
df.head(n=13)                                                 # preview the data 
```

|  | Depth | Nporosity |
| --- | --- | --- |
| 0 | 0.25 | -1.37 |
| 1 | 0.50 | -2.08 |
| 2 | 0.75 | -1.67 |
| 3 | 1.00 | -1.16 |
| 4 | 1.25 | -0.24 |
| 5 | 1.50 | -0.36 |
| 6 | 1.75 | 0.44 |
| 7 | 2.00 | 0.36 |
| 8 | 2.25 | -0.02 |
| 9 | 2.50 | -0.63 |
| 10 | 2.75 | -1.26 |
| 11 | 3.00 | -1.03 |
| 12 | 3.25 | 0.88 |

## è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯

åœ¨ DataFrames ä¸­ï¼Œæœ‰è®¸å¤šæœ‰æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯ã€‚

+   describe å‘½ä»¤æä¾›äº†ä¸€ä¸ªç¾è§‚çš„æ•°æ®è¡¨ï¼Œå…¶ä¸­åŒ…æ‹¬è®¡æ•°ã€å¹³å‡å€¼ã€æ ‡å‡†å·®ã€ç™¾åˆ†ä½æ•°ã€æœ€å°å€¼å’Œæœ€å¤§å€¼ã€‚

+   æˆ‘å–œæ¬¢æŒ‡å®šç™¾åˆ†ä½æ•°ï¼Œå¦åˆ™é»˜è®¤ä¸º P25ã€P50 å’Œ P75 å››åˆ†ä½æ•°

```py
df.describe(percentiles=[0.1,0.9]).transpose()                # summary statistics 
```

|  | count | mean | std | min | 10% | 50% | 90% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Depth | 40.0 | 5.12500 | 2.922613 | 0.25 | 1.225 | 5.125 | 9.025 | 10.00 |
| Nporosity | 40.0 | 0.02225 | 0.992111 | -2.08 | -1.271 | 0.140 | 1.220 | 2.35 |

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æ·±åº¦å’Œé«˜æ–¯è½¬æ¢çš„å­”éš™ç‡ï¼ŒNporosityï¼Œä»æ•°æ®å¸§ä¸­æå–åˆ°å•ç‹¬çš„ 1D æ•°ç»„ä¸­ï¼Œåˆ†åˆ«ç§°ä¸ºâ€˜depthâ€™å’Œâ€˜NPorâ€™ï¼Œä»¥ä¾¿ä»£ç å¯è¯»ã€‚

+   è­¦å‘Šï¼Œè¿™æ˜¯ä¸€ä¸ªæµ…æ‹·è´ï¼Œå¦‚æœæˆ‘ä»¬æ›´æ”¹è¿™äº› 1D æ•°ç»„ï¼Œæ›´æ”¹å°†åæ˜ åœ¨åŸå§‹æ•°æ®å¸§ä¸­ã€‚

```py
Xname = ['Depth']; yname = ['Nporosity']                      # select the predictor and response feature

Xlabel = ['Depth']; ylabel = ['Gaussian Transformed Porosity'] # specify the feature labels for plotting
Xunit = ['m']; yunit = ['N[%]']
Xlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')']
ylabelunit = ylabel[0] + ' (' + yunit[0] + ')'

X = df[Xname[0]]                                              # extract the 1D ndarrays from the DataFrame
y = df[yname[0]]

Xmin = 0.0; Xmax = 10.0                                       # limits for plotting
ymin = -3.0; ymax = 3.0

X_values = np.linspace(Xmin,Xmax,100)                         # X intervals to visualize the model 
```

## çº¿æ€§å›å½’æ¨¡å‹

è®©æˆ‘ä»¬é¦–å…ˆä½¿ç”¨ scikit-learn çš„ LinearRegression ç±»è®¡ç®—çº¿æ€§å›å½’æ¨¡å‹ã€‚æ­¥éª¤åŒ…æ‹¬ï¼Œ

1.  **å®ä¾‹åŒ–** - çº¿æ€§å›å½’å¯¹è±¡ï¼Œæ³¨æ„æ²¡æœ‰è¶…å‚æ•°éœ€è¦æŒ‡å®šã€‚

1.  **æ‹Ÿåˆ** - ä½¿ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒå®ä¾‹åŒ–çš„çº¿æ€§å›å½’å¯¹è±¡

1.  **é¢„æµ‹** - ä½¿ç”¨è®­ç»ƒå¥½çš„çº¿æ€§å›å½’å¯¹è±¡

è¿™é‡Œæ˜¯çº¿æ€§å›å½’æ¨¡å‹çš„å®ä¾‹åŒ–å’Œæ‹Ÿåˆæ­¥éª¤ã€‚

+   æ³¨æ„ï¼Œæˆ‘ä»¬æ·»åŠ äº† reshape åˆ°æˆ‘ä»¬çš„é¢„æµ‹ç‰¹å¾ä¸­ï¼Œå› ä¸º scikit-learn å‡è®¾æœ‰å¤šä¸ªé¢„æµ‹ç‰¹å¾ï¼Œå¹¶æœŸæœ›ä¸€ä¸ªäºŒç»´æ•°ç»„ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„ 1D æ•°ç»„é‡å¡‘ä¸ºä¸€ä¸ªåªæœ‰ 1 åˆ—çš„äºŒç»´æ•°ç»„ã€‚

åœ¨æˆ‘ä»¬è®­ç»ƒæ¨¡å‹åï¼Œæˆ‘ä»¬ç”¨æ•°æ®ç»˜åˆ¶å®ƒä»¥è¿›è¡Œæ¨¡å‹çš„å¯è§†åŒ–æ£€æŸ¥ã€‚

```py
lin = LinearRegression()                                      # instantiate linear regression object, note no hyperparameters 
lin.fit(X.values.reshape(-1, 1), y)                           # train linear regression model

slope = lin.coef_[0]                                          # get the model parameters
intercept = lin.intercept_

plt.subplot(111)                                              # plot the data and the model
plt.scatter(X,y,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.plot(X_values,intercept + slope*X_values,label='model',color = 'black')
plt.title('Linear Regression Model, Regression of ' + yname[0] + ' on ' + Xname[0])
plt.xlabel(Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel(yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])
plt.annotate('Linear Regression Model',[4.5,-1.8])
plt.annotate(r'    $\beta_1$ :' + str(round(slope,2)),[6.8,-2.3])
plt.annotate(r'    $\beta_0$ :' + str(round(intercept,2)),[6.8,-2.7])
plt.annotate(r'$N[\phi] = \beta_1 \times z + \beta_0$',[4.0,-2.3])
plt.annotate(r'$N[\phi] = $' + str(round(slope,2)) + r' $\times$ $z$ + (' + str(round(intercept,2)) + ')',[4.0,-2.7])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/64b4519fff29b4b1c8eef0c0d94e3ceba809f3543abba1333ea33b4f4120ac4a.png](img/ba77774bef128a461422095cb22a2827.png)

## ä¸éå‚æ•°é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¯”è¾ƒ

è®©æˆ‘ä»¬è¿è¡Œå‡ ä¸ªéå‚æ•°é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»¥ä¸çº¿æ€§å‚æ•°æ¨¡å‹å’Œå¤šé¡¹å¼å‚æ•°æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚é¦–å…ˆæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªå¿«é€Ÿå†³ç­–æ ‘æ¨¡å‹ï¼Œç„¶åæ˜¯ä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹ã€‚

+   æˆ‘ä»¬è·å¾—äº†æ˜¾è‘—çš„çµæ´»æ€§æ¥æ‹Ÿåˆæ•°æ®ä¸­çš„ä»»ä½•æ¨¡å¼

+   éœ€è¦æ›´å¤šçš„æ¨ç†ï¼Œå› ä¸ºéå‚æ•°å®é™…ä¸Šæ˜¯å‚æ•°ä¸°å¯Œçš„ï¼

æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…å…³äºå†³ç­–æ ‘å’Œéšæœºæ£®æ—çš„ç« èŠ‚ã€‚

```py
from sklearn import tree                                      # tree program from scikit learn 

my_tree = tree.DecisionTreeRegressor(min_samples_leaf=5, max_depth = 20) # instantiate the decision tree model with hyperparameters
my_tree = my_tree.fit(X.values.reshape(-1, 1),y)              # fit the decision tree to the training data (all the data in this case)
DT_y = my_tree.predict(X_values.reshape(-1,1))                # predict at high resolution over the range of depths

plt.subplot(111)                                              # plot the model and data
plt.scatter(X,y,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.plot(X_values, DT_y, label='model', color = 'black')
plt.title('Decision Tree Model, ' + yname[0] + ' as a Function of ' + Xname[0])
plt.xlabel(Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel(yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/8e31233c62876ecb3c64296751df5ef5.png)

è¿™é‡Œæ˜¯ä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹ï¼š

```py
from sklearn.ensemble import RandomForestRegressor            # random forest method

max_depth = 5                                                 # set the random forest hyperparameters
num_tree = 1000
max_features = 1

my_forest = RandomForestRegressor(max_depth=max_depth,random_state=seed,n_estimators=num_tree,max_features=max_features)
my_forest.fit(X = X.values.reshape(-1, 1), y = y)  
RF_y = my_forest.predict(X_values.reshape(-1,1))
plt.subplot(111)
plt.scatter(X,y,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.plot(X_values, RF_y, label='model', color = 'black')
plt.title('Random Forest Tree Model, ' + yname[0] + ' as a Function of ' + Xname[0])
plt.xlabel(Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel(yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2)
plt.show() 
```

![å›¾ç‰‡](img/704a35303eabbbf03215f2c0a311653d.png)

æ³¨æ„ï¼Œæ²¡æœ‰å¯¹è¿™äº›å»ºæ¨¡çš„è¶…å‚æ•°è¿›è¡Œè°ƒæ•´ã€‚æˆ‘åªæ˜¯æƒ³å±•ç¤ºéå‚æ•°æ¨¡å‹å­¦ä¹ ç³»ç»Ÿå½¢çŠ¶çš„å·¨å¤§çµæ´»æ€§ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„å‚æ•°å¤šé¡¹å¼æ¨¡å‹ã€‚

+   è®©æˆ‘ä»¬é¦–å…ˆå°†æ•°æ®è½¬æ¢æˆæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œé«˜æ–¯åˆ†å¸ƒã€‚

+   æˆ‘ä»¬è¿™æ ·åšæ˜¯ä¸ºäº†æé«˜æ¨¡å‹æ‹Ÿåˆåº¦ï¼ˆå¤„ç†å¼‚å¸¸å€¼ï¼‰å¹¶ç¬¦åˆå³å°†ä»‹ç»çš„ Hermite å¤šé¡¹å¼çš„ç†è®ºã€‚

## é«˜æ–¯ç•¸å˜ \ é«˜æ–¯å˜æ¢

è®©æˆ‘ä»¬å°†ç‰¹å¾è½¬æ¢ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œ

+   é«˜æ–¯åˆ†å¸ƒ

+   å‡å€¼ä¸º 0.0

+   æ ‡å‡†å·®ä¸º 1.0

å­”éš™ç‡ç‰¹å¾ä¹‹å‰å·²ç»è¢«â€˜è½¬æ¢â€™æˆé«˜æ–¯åˆ†å¸ƒï¼Œä½†æœ‰æœºä¼šå¯¹å…¶è¿›è¡Œæ¸…ç†ã€‚

+   æ¯”è¾ƒä¸‹é¢çš„åŸå§‹å’Œè½¬æ¢åçš„æ•°æ®

+   æ³¨æ„ï¼Œæˆ‘ä½¿ç”¨äº†æˆ‘ä»åŸå§‹ GSLIBï¼ˆDeutsch and Journel, 1997ï¼‰ç§»æ¤çš„ GeostatsPy é«˜æ–¯å˜æ¢ï¼Œå› ä¸º scikit-learn çš„é«˜æ–¯å˜æ¢ä¼šåˆ›å»ºæˆªæ–­å°–å³°/å¼‚å¸¸å€¼ã€‚

```py
import geostatspy.geostats as geostats                        # for Gaussian transform from GSLIB

df_ns = pd.DataFrame()   
df_ns[Xname[0]], tvPor, tnsPor = geostats.nscore(df, Xname[0]) # nscore transform for all facies porosity 
df_ns[yname[0]], tvdepth, tnsdepth = geostats.nscore(df, yname[0]) # nscore transform for all facies permeability
X_ns = df_ns[Xname[0]]; y_ns = df_ns[yname[0]]
X_ns_values = np.linspace(-3.0,3.0,1000)                      # values to predict at in standard normal space 
```

è®©æˆ‘ä»¬ç»˜åˆ¶ä¸€äº›å¥½çš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°å›¾æ¥æ£€æŸ¥åŸå§‹å’Œè½¬æ¢åçš„å˜é‡ã€‚

+   ç»“æœçœ‹èµ·æ¥éå¸¸å¥½

æˆ‘ä»¬è¿™æ ·åšæ˜¯å› ä¸ºæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒçš„é¢„æµ‹ç‰¹å¾æ¥è¿›è¡Œæ­£äº¤æ€§ã€‚æ›´å¤šå†…å®¹å°†åœ¨åé¢ä»‹ç»ï¼

```py
plt.subplot(221)                                              # plot original sand and shale porosity histograms
plt.hist(df[Xname[0]], facecolor='red',bins=np.linspace(Xmin,Xmax,1000),histtype="stepfilled",alpha=0.2,density=True,
         cumulative=True,edgecolor='black',label='Original')
plt.xlim([0.0,10.0]); plt.ylim([0,1.0])
plt.xlabel(Xname[0] + ' (' + Xunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Original Depth')
plt.legend(loc='upper left')
plt.grid(True)

plt.subplot(222)  
plt.hist(df_ns[Xname[0]], facecolor='blue',bins=np.linspace(-3.0,3.0,1000),histtype="stepfilled",alpha=0.2,density=True,
         cumulative=True,edgecolor='black',label = 'NS')
plt.xlim([-3.0,3.0]); plt.ylim([0,1.0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Nscore ' + Xname[0])
plt.legend(loc='upper left')
plt.grid(True)

plt.subplot(223)                                        # plot nscore transformed sand and shale histograms
plt.hist(df[yname[0]], facecolor='red',bins=np.linspace(ymin,ymax,1000),histtype="stepfilled",alpha=0.2,density=True,
         cumulative=True,edgecolor='black',label='Original')
plt.xlim([-3.0,3.0]); plt.ylim([0,1.0])
plt.xlabel(yname[0] + ' (' + yunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Original Porosity')
plt.legend(loc='upper left')
plt.grid(True)

plt.subplot(224)                                        # plot nscore transformed sand and shale histograms
plt.hist(df_ns[yname[0]], facecolor='blue',bins=np.linspace(-3.0,3.0,1000),histtype="stepfilled",alpha=0.2,density=True,
         cumulative=True,edgecolor='black',label = 'NS')
plt.xlim([-3.0,3.0]); plt.ylim([0,1.0])
plt.xlabel('NS: ' + yname[0] + ' (' + yunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Nscore ' + yname[0])
plt.legend(loc='upper left')
plt.grid(True)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=2.0, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/7b0e4b346e5f29d5e18e8d52b82145f1.png)

## å¸¦æœ‰æ ‡å‡†åŒ–ç‰¹å¾çš„çº¿æ€§å›å½’æ¨¡å‹

è®©æˆ‘ä»¬é‡å¤çº¿æ€§å›å½’æ¨¡å‹ï¼Œè¿™æ¬¡ä½¿ç”¨æ ‡å‡†åŒ–ç‰¹å¾ã€‚

```py
lin_ns = LinearRegression()                                   # instantiate linear regression object, note no hyperparameters 
lin_ns.fit(X_ns.values.reshape(-1, 1), y_ns)                  # train linear regression model
slope_ns = lin_ns.coef_[0]                                    # get the model parameters
intercept_ns = lin_ns.intercept_

plt.subplot(111)                                              # plot the data and the model
plt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.plot(X_ns_values,intercept_ns + slope_ns*X_ns_values,label='model',color = 'black')
plt.title('Linear Regression Model, Regression of NS ' + yname[0] + ' on ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])
plt.annotate('Linear Regression Model',[0.8,-1.8])
plt.annotate(r'    $\beta_1$ :' + str(round(slope_ns,2)),[1.8,-2.3])
plt.annotate(r'    $\beta_0$ :' + str(round(intercept_ns,2)),[1.8,-2.7])
plt.annotate(r'$N[\phi] = \beta_1 \times z + \beta_0$',[0.5,-2.3])
plt.annotate(r'$N[\phi] = $' + str(round(slope_ns,2)) + r' $\times$ $z$ + (' + str(round(intercept_ns,2)) + ')',[0.5,-2.7])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/4c865fd8f2805646d61c8babce9fdbbd.png)

å†æ¬¡ï¼Œæ‹Ÿåˆåº¦ä¸ä½³ã€‚è®©æˆ‘ä»¬ä½¿ç”¨æ›´å¤æ‚ã€æ›´çµæ´»çš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

## å¤šé¡¹å¼å›å½’

æˆ‘ä»¬å°†é€šè¿‡æ‰‹å·¥è¿›è¡Œå¤šé¡¹å¼å›å½’ï¼š

+   åˆ›å»ºåŸå§‹é¢„æµ‹ç‰¹å¾çš„å¤šé¡¹å¼åŸºå±•å¼€

+   åœ¨å¤šé¡¹å¼åŸºå±•å¼€ä¸Šæ‰§è¡Œçº¿æ€§å›å½’

### å¤šé¡¹å¼åŸºå±•å¼€

è®©æˆ‘ä»¬ä»è®¡ç®— 1 ä¸ªé¢„æµ‹ç‰¹å¾çš„å¤šé¡¹å¼åŸºå±•å¼€å¼€å§‹ã€‚

```py
poly4 = PolynomialFeatures(degree = 4)                        # instantiate polynomial expansion 
X_ns_poly4 = poly4.fit_transform(X_ns.values.reshape(-1, 1))  # calculate the basis expansion for our dataset
df_X_ns_poly4 = pd.DataFrame({'Values':X_ns,'0th':X_ns_poly4[:,0],'1st':X_ns_poly4[:,1],'2nd':X_ns_poly4[:,2], 
                              '3rd':X_ns_poly4[:,3],'4th':X_ns_poly4[:,4]}) # make a new DataFrame from the vectors
df_X_ns_poly4 = pd.DataFrame({'Values':X_ns,'1st':X_ns_poly4[:,1],'2nd':X_ns_poly4[:,2], 
                              '3rd':X_ns_poly4[:,3],'4th':X_ns_poly4[:,4]}) # make a new DataFrame from the vectors
df_X_ns_poly4.head()                                          # preview the polynomial basis expansion with the original predictor feature 
```

|  | å€¼ | ç¬¬ 1 | ç¬¬ 2 | ç¬¬ 3 | ç¬¬ 4 |
| --- | --- | --- | --- | --- | --- |
| 0 | -2.026808 | -2.026808 | 4.107951 | -8.326029 | 16.875264 |
| 1 | -1.780464 | -1.780464 | 3.170053 | -5.644167 | 10.049238 |
| 2 | -1.534121 | -1.534121 | 2.353526 | -3.610592 | 5.539084 |
| 3 | -1.356312 | -1.356312 | 1.839582 | -2.495046 | 3.384060 |
| 4 | -1.213340 | -1.213340 | 1.472193 | -1.786270 | 2.167352 |

ç°åœ¨è®©æˆ‘ä»¬æ£€æŸ¥åŸå§‹é¢„æµ‹ç‰¹å¾æ•°æ®çš„å¤šé¡¹å¼åŸºå±•å¼€ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

+   å›æƒ³ä¸€ä¸‹ï¼Œé¢„æµ‹ç‰¹å¾ä¹‹é—´é«˜åº¦çš„ç›¸å…³æ€§ä¼šå¢åŠ æ¨¡å‹æ–¹å·®ã€‚

```py
corr_matrix = df_X_ns_poly4.iloc[:,1:].corr()                 # calculate the correlation matrix

plt.subplot(111)
plot_corr(corr_matrix,'Polynomial Expansion Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/9b9eee94daf8d4510c17a21728efa520.png)

æˆ‘ä»¬åœ¨ 1 é˜¶å’Œ 3 é˜¶ä»¥åŠ 2 é˜¶å’Œ 4 é˜¶ä¹‹é—´å­˜åœ¨é«˜åº¦ç›¸å…³æ€§ã€‚

+   è®©æˆ‘ä»¬ç”¨å¤šé¡¹å¼åŸºçš„çŸ©é˜µæ•£ç‚¹å›¾æ¥æ£€æŸ¥ã€‚

## å¯è§†åŒ–å¤šé¡¹å¼å±•å¼€ç‰¹å¾çš„æˆå¯¹å…³ç³»

```py
sns.pairplot(df_X_ns_poly4.iloc[:,1:],vars=['1st','2nd','3rd','4th'],markers='o', kind='reg',diag_kind='kde')
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/3ea96d491efa1ce020f1f121c4e5fc5c.png)

è®©æˆ‘ä»¬å¯è§†åŒ–é«˜æ–¯å˜æ¢æ·±åº¦ä¸Šçš„å¤šé¡¹å¼å±•å¼€ã€‚

```py
plt.subplot(111)                                              # plot the polynomial basis expansion
plt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,0],label='0th',color = 'black')
plt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,1],label='1th',color = 'blue')
plt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,2],label='2th',color = 'green')
plt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,3],label='3th',color = 'red')
plt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,4],label='4th',color = 'orange') 
plt.title('Polynomial Basis Expansion of ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('h[ NS: ' + Xname[0] + ' (' + Xunit[0] + ') ]')
plt.legend(); plt.xlim(-3,3); add_grid()
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/3f62f7e21b741e2cd86be2687d0f5fdb.png)

æˆ‘ä»¬è¿˜å¯ä»¥æ£€æŸ¥æ¯ä¸ªå¤šé¡¹å¼åŸºå±•å¼€çš„ç®—æœ¯å¹³å‡å€¼ã€‚

```py
print('The averages of each basis expansion, 0 - 4th order = ' + str(stats.describe(X_ns_poly4)[2]) + '.') 
```

```py
The averages of each basis expansion, 0 - 4th order = [1\.         0.00536486 0.9458762  0.07336308 2.31077802]. 
```

è®©æˆ‘ä»¬å°†çº¿æ€§å›å½’æ¨¡å‹æ‹Ÿåˆåˆ°å¤šé¡¹å¼åŸºå±•å¼€ã€‚

+   æ³¨æ„ï¼šæ¨¡å‹å¯¹æ‹Ÿåˆè¿™ç§å¤æ‚/éçº¿æ€§æ•°æ®ç›¸å½“çµæ´»

```py
lin_poly4 = LinearRegression()                                # instantiate new linear model 
lin_poly4.fit(df_X_ns_poly4.iloc[:,1:], y_ns)                 # train linear model with polynomial expansion, polynomial regression
b1,b2,b3,b4 = np.round(lin_poly4.coef_,3)                     # retrieve the model parameters
b0 = lin_poly4.intercept_

plt.subplot(111)
plt.plot(X_ns_values,lin_poly4.predict(poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,1:]),label='polynomial',color = 'red') 
plt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.title('Polynomial Regression Model, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])
plt.annotate('Polynomial Regression Model',[-2.8,2.6])
plt.annotate(r'    $\beta_4$ :' + str(round(b4,3)),[-2.8,2.1])
plt.annotate(r'    $\beta_3$ :' + str(round(b3,3)),[-2.8,1.7])
plt.annotate(r'    $\beta_2$ :' + str(round(b2,3)),[-2.8,1.3])
plt.annotate(r'    $\beta_1$ :' + str(round(b1,3)),[-2.8,0.9])
plt.annotate(r'    $\beta_0$ :' + str(round(b0,2)),[-2.8,0.5])
plt.annotate(r'$N[\phi] = \beta_4 \times N[z]â´ + \beta_3 \times N[z]Â³ + \beta_2 \times N[z]Â² + \beta_1 \times N[z] + \beta_0$',[-1.0,-2.0])
plt.annotate(r'$N[\phi] = $' + str(b4) + r' $\times N[z]â´ +$ ' + str(b3) + r' $\times N[z]Â³ +$ ' + str(b2) + r' $\times N[z]Â² +$ ' + 
             str(b1) + r' $\times N[z]$ + ' + str(round(b0,2)),[-1.0,-2.5])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/8544dd539b8a7b35cc6999a54d0fecb5.png)

## ä½¿ç”¨èµ«ç±³ç‰¹åŸºå±•å¼€çš„å›å½’

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨èµ«ç±³ç‰¹å¤šé¡¹å¼æ¥å‡å°‘åŸºé¢„æµ‹ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

+   æˆ‘ä»¬å°†é¢„æµ‹ç‰¹å¾ã€æ·±åº¦è½¬æ¢ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œå› ä¸ºèµ«ç±³ç‰¹å¤šé¡¹å¼å±•å¼€åœ¨æ ‡å‡†æ­£æ€æ¦‚ç‡å¯†åº¦å‡½æ•°çš„å‡è®¾ä¸‹ï¼Œåœ¨è´Ÿæ— ç©·åˆ°æ­£æ— ç©·çš„èŒƒå›´å†…å®ç°ç‹¬ç«‹æ€§ã€‚

```py
orders4 = [1,2,3,4]                                           # specify the orders for Hermite basis expansion
X_ns_hermite4 = scipy.special.eval_hermitenorm(orders4,X_ns.values.reshape(-1, 1), out=None) # Hermite polynomials for X 
df_X_ns_hermite4 = pd.DataFrame({'value':X_ns.values,'1st':X_ns_hermite4[:,0],'2nd':X_ns_hermite4[:,1], 
                                     '3rd':X_ns_hermite4[:,2],'4th':X_ns_hermite4[:,3]}) # make a new DataFrame from the vectors
df_X_ns_hermite4.head() 
```

|  | å€¼ | 1 é˜¶ | 2 é˜¶ | 3 é˜¶ | 4 é˜¶ |
| --- | --- | --- | --- | --- | --- |
| 0 | -2.026808 | -2.026808 | 3.107951 | -2.245605 | -4.772444 |
| 1 | -1.780464 | -1.780464 | 2.170053 | -0.302774 | -5.971082 |
| 2 | -1.534121 | -1.534121 | 1.353526 | 0.991769 | -5.582071 |
| 3 | -1.356312 | -1.356312 | 0.839582 | 1.573889 | -4.653429 |
| 4 | -1.213340 | -1.213340 | 0.472193 | 1.853749 | -3.665806 |

æ³¨æ„ï¼šæˆ‘å·²ç»çœç•¥äº†å¯¹äºæˆ‘ä»¬çš„æ•°æ®é›†å…·æœ‰æ›´é«˜ç›¸å…³æ€§çš„é˜¶æ•°ã€‚

è®©æˆ‘ä»¬æ£€æŸ¥èµ«ç±³ç‰¹é¢„æµ‹ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æœ‰æ‰€æ”¹è¿›ã€‚

```py
hermite_corr_matrix = df_X_ns_hermite4.iloc[:,1:].corr()      # calculate correlation matrix of Hermite basis expansion of X

plt.subplot(111)
plot_corr(hermite_corr_matrix,'Hermite Polynomial Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/f1b9e3f1eac053a1460fbb16b9d3ab5e.png)

ä¸å¤šé¡¹å¼åŸºç›¸æ¯”ï¼Œæˆå¯¹çº¿æ€§ç›¸å…³æ€§ç›¸å½“ä½ã€‚

è®©æˆ‘ä»¬å¯è§†åŒ–èµ«ç±³ç‰¹åŸºé˜¶æ•°çš„åŒå˜é‡å…³ç³»ã€‚

```py
sns.pairplot(df_X_ns_hermite4.iloc[:,1:],vars=['1st','2nd','3rd','4th'],markers='o', kind='reg',diag_kind='kde')
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/9fd622d54d360b533c8ba76f2bd6a2b6.png)

æˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ‰€æœ‰èµ«ç±³ç‰¹åŸºå±•å¼€çš„ç®—æœ¯å¹³å‡å€¼ã€‚

```py
print('The means of each basis expansion, 1 - 4th order = ' + str(stats.describe(X_ns_hermite4)[2]) + '.') 
```

```py
The means of each basis expansion, 1 - 4th order = [ 0.00536486 -0.0541238   0.05726848 -0.36447919]. 
```

è®©æˆ‘ä»¬å¯è§†åŒ–æ ‡å‡†åŒ–æ·±åº¦èŒƒå›´å†…çš„èµ«ç±³ç‰¹å¤šé¡¹å¼ã€‚

```py
plt.subplot(111)                                              # plot Hermite polynomials
plt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,0],label='1st',color = 'blue')
plt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,1],label='2nd',color = 'green')
plt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,2],label='3rd',color = 'red')
plt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,3],label='4th',color = 'orange')
plt.title('Hermite Polynomial Basis Expansion of ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('h[ NS: ' + Xname[0] + ' (' + Xunit[0] + ') ]')
plt.legend(); plt.xlim(-3,3); add_grid()
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/4014172673585ba0353f9f413f88bd94.png)

ç°åœ¨è®©æˆ‘ä»¬æ‹Ÿåˆæˆ‘ä»¬çš„èµ«ç±³ç‰¹åŸºå›å½’æ¨¡å‹ã€‚

```py
lin_herm4 = LinearRegression()                                # instantiate model
lin_herm4.fit(df_X_ns_hermite4.iloc[:,1:], y_ns)              # fit Hermite polynomials 
hb1,hb2,hb3,hb4 = np.round(lin_herm4.coef_,3)                 # retrieve the model parameters
hb0 = lin_herm4.intercept_
plt.subplot(111)                                              # plot data and model
plt.plot(X_ns_values, lin_herm4.predict(scipy.special.eval_hermitenorm(orders4,X_ns_values.reshape(-1, 1), out=None)), 
         label='4th order',color = 'red') 
plt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.title('Hermite Polynomial Regression Model, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])
plt.annotate('Hermite Polynomial Regression Model',[-2.8,2.6])
plt.annotate(r'    $\beta_4$ :' + str(round(hb4,3)),[-2.8,2.1])
plt.annotate(r'    $\beta_3$ :' + str(round(hb3,3)),[-2.8,1.7])
plt.annotate(r'    $\beta_2$ :' + str(round(hb2,3)),[-2.8,1.3])
plt.annotate(r'    $\beta_1$ :' + str(round(hb1,3)),[-2.8,0.9])
plt.annotate(r'    $\beta_0$ :' + str(round(hb0,2)),[-2.8,0.5])
plt.annotate(r'$N[\phi] = \beta_4 \times N[z]â´ + \beta_3 \times N[z]Â³ + \beta_2 \times N[z]Â² + \beta_1 \times N[z] + \beta_0$',[-1.0,-2.0])
plt.annotate(r'$N[\phi] = $' + str(hb4) + r' $\times N[z]â´ +$ ' + str(hb3) + r' $\times N[z]Â³ +$ ' + str(hb2) + r' $\times N[z]Â² +$ ' + 
             str(hb1) + r' $\times N[z]$ + ' + str(round(hb0,2)),[-1.0,-2.5])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/f42cc4ffefcd978188c7348b3d653b8b.png)

ç”±äºæˆ‘ä»¬æ‰©å±•çš„åŸºç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§è¾ƒä½ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ¨¡å‹ç³»æ•°å¹¶è§£é‡Šæ¯ä¸ªé˜¶æ•°çš„ç‹¬ç‰¹é‡è¦æ€§ã€‚

## æ­£äº¤å¤šé¡¹å¼

è®©æˆ‘ä»¬å°è¯• Dave Moore ç”¨ Python é‡æ–°å®ç°çš„æ­£äº¤å¤šé¡¹å¼åŸºå±•å¼€ï¼Œä»–ä» R è¯­è¨€çš„ poly()å‡½æ•°ä¸­è·å–ã€‚

+   ä»¥ä¸‹ fit å’Œ predict å‡½æ•°ç›´æ¥æ¥è‡ª Dave çš„[åšå®¢](http://davmre.github.io/blog/python/2013/12/15/orthogonal_poly)

+   åœ¨æ‹Ÿåˆè®­ç»ƒæ•°æ®æ—¶ï¼Œè®¡ç®—äº†èŒƒæ•° 2 å’Œ alpha æ¨¡å‹å‚æ•°

+   è¿™äº›å‚æ•°å¿…é¡»ä¼ é€’ç»™æ¯ä¸ªåç»­é¢„æµ‹ä»¥ç¡®ä¿ç»“æœä¸€è‡´

```py
# functions taken (without modification) from http://davmre.github.io/blog/python/2013/12/15/orthogonal_poly
# appreciation to Dave Moore for the great blog post on titled 'Orthogonal polynomial regression in Python'
# functions are Dave's reimplementation of poly() from R

def ortho_poly_fit(x, degree = 1):
    n = degree + 1
    x = np.asarray(x).flatten()
    if(degree >= len(np.unique(x))):
            stop("'degree' must be less than number of unique points")
    xbar = np.mean(x)
    x = x - xbar
    X = np.fliplr(np.vander(x, n))
    q,r = np.linalg.qr(X)

    z = np.diag(np.diag(r))
    raw = np.dot(q, z)

    norm2 = np.sum(raw**2, axis=0)
    alpha = (np.sum((raw**2)*np.reshape(x,(-1,1)), axis=0)/norm2 + xbar)[:degree]
    Z = raw / np.sqrt(norm2)
    return Z, norm2, alpha

def ortho_poly_predict(x, alpha, norm2, degree = 1):
    x = np.asarray(x).flatten()
    n = degree + 1
    Z = np.empty((len(x), n))
    Z[:,0] = 1
    if degree > 0:
        Z[:, 1] = x - alpha[0]
    if degree > 1:
        for i in np.arange(1,degree):
             Z[:, i+1] = (x - alpha[i]) * Z[:, i] - (norm2[i] / norm2[i-1]) * Z[:, i-1]
    Z /= np.sqrt(norm2)
    return Z 
```

è®©æˆ‘ä»¬è¯•ä¸€è¯•ï¼Œå¹¶å¯¹æˆ‘ä»¬çš„æ ‡å‡†æ­£æ€å˜æ¢æ·±åº¦è¿›è¡Œæ­£äº¤å¤šé¡¹å¼å±•å¼€ã€‚

```py
X_ns_ortho4, norm2, alpha = ortho_poly_fit(X_ns.values.reshape(-1, 1), degree = 4) # orthogonal polynomial expansion
df_X_ns_ortho4 = pd.DataFrame({'value':X_ns.values,'1st':X_ns_ortho4[:,1],'2nd':X_ns_ortho4[:,2],'3rd':X_ns_ortho4[:,3],
                               '4th':X_ns_ortho4[:,4]})       # make a new DataFrame from the vectors
df_X_ns_ortho4.head() 
```

|  | value | 1st | 2nd | 3rd | 4th |
| --- | --- | --- | --- | --- | --- |
| 0 | -2.026808 | -0.330385 | 0.440404 | -0.460160 | 0.420374 |
| 1 | -1.780464 | -0.290335 | 0.313201 | -0.207862 | 0.021278 |
| 2 | -1.534121 | -0.250285 | 0.202153 | -0.029761 | -0.172968 |
| 3 | -1.356312 | -0.221377 | 0.132038 | 0.058235 | -0.220834 |
| 4 | -1.213340 | -0.198133 | 0.081765 | 0.107183 | -0.219084 |

è®©æˆ‘ä»¬æ£€æŸ¥æ­£äº¤å¤šé¡¹å¼é¢„æµ‹ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æˆ‘å°è±¡æ·±åˆ»ï¼åŸºç‰¹å¾é˜¶æ•°ä¹‹é—´çš„ç›¸å…³æ€§éƒ½æ˜¯é›¶ï¼

```py
ortho_corr_matrix = df_X_ns_ortho4.iloc[:,1:].corr()          # calculate the correlation matrix

plt.subplot(111)
plot_corr(ortho_corr_matrix,'Orthogonal Polynomial Expansion Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/5c444109d15f6a2e24d2be84d8941629.png)

è®©æˆ‘ä»¬å¯è§†åŒ–æˆ‘ä»¬çš„æ­£äº¤å¤šé¡¹å¼åŸºé˜¶æ•°ä¹‹é—´çš„äºŒå…ƒå…³ç³»ã€‚

```py
sns.pairplot(df_X_ns_ortho4.iloc[:,1:],vars=['1st','2nd','3rd','4th'],markers='o',kind='reg',diag_kind='kde') 
```

```py
<seaborn.axisgrid.PairGrid at 0x1ed608d8370> 
```

![å›¾ç‰‡](img/5a2f9488237446e128cc99a668c78ad7.png)

è®©æˆ‘ä»¬å¯è§†åŒ–æ ‡å‡†åŒ–æ·±åº¦èŒƒå›´å†…çš„æ­£äº¤å¤šé¡¹å¼åŸºé˜¶æ•°ã€‚

```py
ortho_poly_ns_values = ortho_poly_predict(X_ns_values.reshape(-1, 1), alpha, norm2, degree = 4)

plt.subplot(111)
plt.plot(X_ns_values, ortho_poly_ns_values[:,0], label='0th', color = 'black')
plt.plot(X_ns_values, ortho_poly_ns_values[:,1], label='1st', color = 'blue')
plt.plot(X_ns_values, ortho_poly_ns_values[:,2], label='2nd', color = 'green')
plt.plot(X_ns_values, ortho_poly_ns_values[:,3], label='3rd', color = 'red')
plt.plot(X_ns_values, ortho_poly_ns_values[:,4], label='4th', color = 'orange')
plt.title('Orthogonal Polynomial Basis Expansion of ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('h[ NS: ' + Xname[0] + ' (' + Xunit[0] + ') ]')
plt.legend(); plt.xlim(-3,3); add_grid()
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/74a68b06994a2941d62a1c4da558780c.png)

æœ€åï¼Œè®©æˆ‘ä»¬æ‹Ÿåˆæˆ‘ä»¬çš„æ­£äº¤å¤šé¡¹å¼åŸºå±•å¼€å›å½’æ¨¡å‹ã€‚

```py
lin_ortho4 = LinearRegression()                               # instantiate model
lin_ortho4.fit(df_X_ns_ortho4.iloc[:,1:], y_ns)               # fit Hermite polynomials 
ob1,ob2,ob3,ob4 = np.round(lin_ortho4.coef_,3)                # retrieve the model parameters
ob0 = lin_ortho4.intercept_

plt.subplot(111)
plt.plot(X_ns_values,lin_ortho4.predict(ortho_poly_ns_values[:,1:]),label='orthogonal polynomial',color = 'red') 
plt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.title('Orthogonal Polynomial Regression Model, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])
plt.annotate('Orthogonal Polynomial Regression Model',[-2.8,2.6])
plt.annotate(r'    $\beta_4$ :' + str(round(ob4,3)),[-2.8,2.1])
plt.annotate(r'    $\beta_3$ :' + str(round(ob3,3)),[-2.8,1.7])
plt.annotate(r'    $\beta_2$ :' + str(round(ob2,3)),[-2.8,1.3])
plt.annotate(r'    $\beta_1$ :' + str(round(ob1,3)),[-2.8,0.9])
plt.annotate(r'    $\beta_0$ :' + str(round(ob0,2)),[-2.8,0.5])
plt.annotate(r'$N[\phi] = \beta_4 \times N[z]â´ + \beta_3 \times N[z]Â³ + \beta_2 \times N[z]Â² + \beta_1 \times N[z] + \beta_0$',[-1.0,-2.0])
plt.annotate(r'$N[\phi] = $' + str(ob4) + r' $\times N[z]â´ +$ ' + str(ob3) + r' $\times N[z]Â³ +$ ' + str(ob2) + r' $\times N[z]Â² +$ ' + 
             str(ob1) + r' $\times N[z]$ + ' + str(round(ob0,2)),[-1.0,-2.5])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/2a189618f9055efc6488a7eb46c5c41d.png)

## ä½¿ç”¨ Pipelines çš„ scikit-learn è¿›è¡Œå¤šé¡¹å¼å›å½’

é¦–å…ˆæ‰§è¡ŒåŸºå±•å¼€ç„¶åè®­ç»ƒç»“æœï¼ˆåŸºå˜æ¢åï¼‰çš„çº¿æ€§æ¨¡å‹å¯èƒ½çœ‹èµ·æ¥æœ‰ç‚¹å¤æ‚ã€‚

+   ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨ scikit-learn çš„ Pipeline å¯¹è±¡ã€‚ä»¥ä¸‹æ˜¯å…³äº Pipeline çš„ä¸€äº›äº®ç‚¹ã€‚

æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹å¯èƒ½å¾ˆå¤æ‚ï¼ŒåŒ…å«å„ç§æ­¥éª¤ï¼š

+   æ•°æ®å‡†å¤‡ã€ç‰¹å¾å·¥ç¨‹è½¬æ¢

+   æ¨¡å‹å‚æ•°æ‹Ÿåˆ

+   æ¨¡å‹è¶…å‚æ•°è°ƒæ•´

+   æ¨¡å‹æ–¹æ³•é€‰æ‹©

+   åœ¨å¤§é‡è¶…å‚æ•°ç»„åˆä¸­è¿›è¡Œæœç´¢

+   è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹è¿è¡Œ

ç®¡é“æ˜¯ scikit-learn ä¸­çš„ä¸€ä¸ªç±»ï¼Œå…è®¸å°è£…ä¸€ç³»åˆ—æ•°æ®å‡†å¤‡å’Œå»ºæ¨¡æ­¥éª¤

+   ç„¶åæˆ‘ä»¬å¯ä»¥å°†ç®¡é“è§†ä¸ºæˆ‘ä»¬é«˜åº¦ç²¾ç®€çš„å·¥ä½œæµç¨‹ä¸­çš„ä¸€ä¸ªå¯¹è±¡

ç®¡é“ç±»å…è®¸æˆ‘ä»¬ï¼š

+   æé«˜ä»£ç å¯è¯»æ€§å¹¶ä¿æŒä¸€åˆ‡äº•ç„¶æœ‰åº

+   é¿å…å¸¸è§çš„æµç¨‹é—®é¢˜ï¼Œå¦‚æ•°æ®æ³„éœ²ï¼Œæµ‹è¯•æ•°æ®å‘ŠçŸ¥æ¨¡å‹å‚æ•°è®­ç»ƒ

+   æ¦‚æ‹¬å¸¸è§çš„æœºå™¨å­¦ä¹ å»ºæ¨¡ï¼Œå¹¶ä¸“æ³¨äºæ„å»ºå°½å¯èƒ½å¥½çš„æ¨¡å‹

åŸºæœ¬å“²å­¦æ˜¯å°†æœºå™¨å­¦ä¹ è§†ä¸ºä¸€ç§ç»„åˆæœç´¢ï¼Œä»¥æ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼ˆAutoMLï¼‰

```py
order=4                                                       # set the polynomial order

polyreg_pipe=make_pipeline(PolynomialFeatures(order),LinearRegression()) # make the modeling pipeline
polyreg_pipe.fit(X_ns.values.reshape(-1, 1), y_ns)            # fit the model to the data
y_hat = polyreg_pipe.predict(X_ns_values.reshape(-1, 1))      # predict with the modeling pipeline
poly_reg_model = polyreg_pipe.named_steps['linearregression'] # retrieve the model from the pipeline
pb0a,pb1,pb2,pb3,pb4 = np.round(poly_reg_model.coef_,3)       # retrieve the model parameters
pb0b = poly_reg_model.intercept_
pb0 = pb0a + pb0b

plt.subplot(111)                                              # plot the data and model
plt.plot(X_ns_values,y_hat, label='4th order',color = 'red') 
plt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.title(str(order) + r'$^{th}$ Polynomial Regression Model with Pipelines, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])
plt.annotate('Orthogonal Polynomial Regression Model',[-2.8,2.6])
plt.annotate(r'    $\beta_4$ :' + str(round(pb4,3)),[-2.8,2.1])
plt.annotate(r'    $\beta_3$ :' + str(round(pb3,3)),[-2.8,1.7])
plt.annotate(r'    $\beta_2$ :' + str(round(pb2,3)),[-2.8,1.3])
plt.annotate(r'    $\beta_1$ :' + str(round(pb1,3)),[-2.8,0.9])
plt.annotate(r'    $\beta_0$ :' + str(round(pb0,2)),[-2.8,0.5])
plt.annotate(r'$N[\phi] = \beta_4 \times N[z]â´ + \beta_3 \times N[z]Â³ + \beta_2 \times N[z]Â² + \beta_1 \times N[z] + \beta_0$',[-1.0,-2.0])
plt.annotate(r'$N[\phi] = $' + str(pb4) + r' $\times N[z]â´ +$ ' + str(pb3) + r' $\times N[z]Â³ +$ ' + str(pb2) + r' $\times N[z]Â² +$ ' + 
             str(pb1) + r' $\times N[z]$ + ' + str(round(pb0,2)),[-1.0,-2.5])
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/4c8e565b643fa879eb74f2d3c49419386b5073f8c2dce53cd9dd9142465f16ee.png](img/e0ba7ea47dd6042f5976cb230740cb93.png)

## è¯„è®º

è¿™æ˜¯å¯¹å¤šé¡¹å¼å›å½’çš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„è§†é¢‘è®²åº§é“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼ˆMichael Pyrczï¼‰æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œåœ¨é‚£é‡Œä»–ç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°çƒç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜

+   [ã€Šè®¡ç®—æœºä¸åœ°çƒç§‘å­¦ã€‹](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[ã€Šæ•°å­¦åœ°çƒç§‘å­¦ã€‹](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼ˆMichael Pyrczï¼‰å·²æ’°å†™è¶…è¿‡ 70 ç¯‡[åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[ã€Šåœ°çƒç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡ã€‹](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[ã€ŠPython åº”ç”¨åœ°çƒç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—ã€‹](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[ã€ŠPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„å®è·µæŒ‡å—ã€‹](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å« 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ª GitHub ä»“åº“ä¸­çš„è¯¦ç»†è®°å½•çš„å·¥ä½œæµç¨‹é“¾æ¥ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚ä¸ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼Œæ³¨å†Œå·¥ç¨‹å¸ˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢æ•™æˆ

æ›´å¤šèµ„æºè¯·è®¿é—®ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## å¤šé¡¹å¼å›å½’çš„åŠ¨æœº

é€šè¿‡ä»çº¿æ€§å›å½’åˆ°å¤šé¡¹å¼å›å½’çš„è½¬å˜ï¼Œæˆ‘ä»¬ï¼Œ

+   é€šè¿‡æ¨¡æ‹Ÿæ•°æ®ä¸­çš„éçº¿æ€§æ¥å¢åŠ é¢„æµ‹çš„çµæ´»æ€§

+   å»ºç«‹åœ¨ç‰¹å¾å·¥ç¨‹æ¦‚å¿µçš„ç‰¹å¾æ‰©å±•ä¹‹ä¸Š

åœ¨ä»è®­ç»ƒæ¨¡å‹å‚æ•°å¦‚çº¿æ€§å›å½’çš„åˆ†æè§£ä¸­å—ç›Šçš„åŒæ—¶ã€‚

æˆ‘ä»¬é€šè¿‡åŸºå‡½æ•°æ‰©å±•å®Œæˆæ‰€æœ‰è¿™äº›ï¼Œ

+   æˆ‘ä»¬å°†ç‰¹å¾è¿›è¡Œè½¬æ¢å’Œæ‰©å±• $\rightarrow$ å¼•å…¥åŸºå‡½æ•°æ‰©å±•ï¼

+   æˆ‘ä»¬å¯ä»¥å¢åŠ æˆ‘ä»¬çš„é¢„æµ‹æ¨¡å‹å¤æ‚æ€§å’Œçµæ´»æ€§ $\rightarrow$ éçº¿æ€§åŸºï¼

+   æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¶ˆé™¤å¤šé‡å…±çº¿æ€§æ¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ $\rightarrow$ æ­£äº¤åŸºï¼

è®©æˆ‘ä»¬ä»çº¿æ€§å›å½’å¼€å§‹ï¼Œç„¶åè¿‡æ¸¡åˆ°å¤šé¡¹å¼å›å½’ã€‚

## çº¿æ€§å›å½’

ç”¨äºé¢„æµ‹çš„çº¿æ€§å›å½’ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹ä¸€ç»„æ•°æ®æ‹Ÿåˆçš„çº¿æ€§æ¨¡å‹ã€‚

![](img/806bf5f702f9bb5a63e30d6e1f7969d9.png)

ç¤ºä¾‹çº¿æ€§å›å½’æ¨¡å‹ã€‚

è®©æˆ‘ä»¬å…ˆå®šä¹‰ä¸€äº›æœ¯è¯­ï¼Œ

+   **é¢„æµ‹ç‰¹å¾** - é¢„æµ‹æ¨¡å‹çš„è¾“å…¥ç‰¹å¾ï¼Œé‰´äºæˆ‘ä»¬åªè®¨è®ºçº¿æ€§å›å½’è€Œä¸æ˜¯å¤šå…ƒçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªé¢„æµ‹ç‰¹å¾ $x$ã€‚åœ¨æˆ‘ä»¬çš„å›¾è¡¨ï¼ˆåŒ…æ‹¬ä¸Šé¢çš„ï¼‰ä¸­ï¼Œé¢„æµ‹ç‰¹å¾ä½äº x è½´ä¸Šã€‚

+   **å“åº”ç‰¹å¾** - é¢„æµ‹æ¨¡å‹çš„è¾“å‡ºç‰¹å¾ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ$y$ã€‚åœ¨æˆ‘ä»¬çš„å›¾è¡¨ï¼ˆåŒ…æ‹¬ä¸Šé¢çš„ï¼‰ä¸­ï¼Œå“åº”ç‰¹å¾ä½äº y è½´ä¸Šã€‚

ç°åœ¨ï¼Œä»¥ä¸‹æ˜¯çº¿æ€§å›å½’çš„ä¸€äº›å…³é”®æ–¹é¢ï¼š

**å‚æ•°æ¨¡å‹**

è¿™æ˜¯ä¸€ä¸ªå‚æ•°é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬æ¥å—ä¸€ä¸ªå…ˆéªŒçš„çº¿æ€§å‡è®¾ï¼Œç„¶åè·å¾—ä¸€ä¸ªéå¸¸ä½çš„å‚æ•°è¡¨ç¤ºï¼Œè¿™ä½¿å¾—åœ¨æ²¡æœ‰å¤§é‡æ•°æ®çš„æƒ…å†µä¸‹æ˜“äºè®­ç»ƒã€‚

+   é€‚åˆçš„æ¨¡å‹æ˜¯ä¸€ä¸ªåŸºäºæ‰€æœ‰å¯ç”¨ç‰¹å¾ $x_1,\ldots,x_m$ çš„ç®€å•åŠ æƒçº¿æ€§åŠ æ€§æ¨¡å‹ã€‚

+   å‚æ•°æ¨¡å‹é‡‡å–ä»¥ä¸‹å½¢å¼ï¼š

$$ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 $$

è¿™é‡Œæ˜¯çº¿æ€§æ¨¡å‹å‚æ•°çš„å¯è§†åŒ–ï¼Œ

![](img/ada2fcc2740c48478e79404563c91061.png)

çº¿æ€§æ¨¡å‹å‚æ•°ã€‚

**æœ€å°äºŒä¹˜æ³•**

å¯¹äºæ¨¡å‹å‚æ•° $b_1,\ldots,b_m,b_0$ çš„è§£æè§£åœ¨ L2 èŒƒæ•°æŸå¤±å‡½æ•°ä¸­æ˜¯å¯ç”¨çš„ï¼Œè¯¯å·®æ˜¯ç´¯åŠ å¹¶å¹³æ–¹çš„ï¼Œå·²çŸ¥ä¸ºæœ€å°äºŒä¹˜æ³•ã€‚

+   æˆ‘ä»¬åœ¨è®­ç»ƒæ•°æ®ä¸Šæœ€å°åŒ–è¯¯å·®ï¼Œæ®‹å·®å¹³æ–¹å’Œï¼ˆRSSï¼‰ï¼š

$$ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)Â² $$

å…¶ä¸­ $y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾å€¼ï¼Œè€Œ $\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ æ˜¯æ¨¡å‹é¢„æµ‹å€¼ï¼Œè¿™äº›é¢„æµ‹å€¼æ˜¯åŸºäº $\alpha = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ã€‚

è¿™é‡Œæ˜¯ L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®çš„å¯è§†åŒ–ï¼Œ

![](img/835541b16e1038a4606f7d97b628c4f9.png)

çº¿æ€§æ¨¡å‹æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®ã€‚

+   è¿™å¯ä»¥ç®€åŒ–ä¸ºè®­ç»ƒæ•°æ®ä¸Šçš„å¹³æ–¹è¯¯å·®ä¹‹å’Œï¼Œ

\begin{equation} \sum_{i=1}^n (\Delta y_i)Â² \end{equation}

å…¶ä¸­ $\Delta y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾è§‚å¯Ÿ $y_i$ å‡å»æ¨¡å‹é¢„æµ‹ $\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ï¼Œåœ¨ $i = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šã€‚

**å‡è®¾**

æˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹æœ‰ä¸€äº›é‡è¦çš„å‡è®¾ï¼Œ

+   **æ— è¯¯å·®** - é¢„æµ‹å˜é‡æ˜¯æ— è¯¯å·®çš„ï¼Œä¸æ˜¯éšæœºå˜é‡

+   **çº¿æ€§** - å“åº”æ˜¯ç‰¹å¾ï¼ˆçš„ï¼‰çº¿æ€§ç»„åˆ

+   **å¸¸æ•°æ–¹å·®** - å“åº”è¯¯å·®åœ¨é¢„æµ‹å€¼ä¸Šæ˜¯æ’å®šçš„

+   **è¯¯å·®ç‹¬ç«‹æ€§** - å“åº”è¯¯å·®å½¼æ­¤ä¸ç›¸å…³

+   **æ— å¤šé‡å…±çº¿æ€§** - æ²¡æœ‰ç‰¹å¾ä¸å…¶ä»–ç‰¹å¾å†—ä½™

## é¢„æµ‹ç‰¹å¾ / åŸºç¡€æ‰©å±•

æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹é¢„æµ‹ç‰¹å¾åº”ç”¨åŸºç¡€å‡½æ•°æ¥åº”ç”¨åŸºç¡€æ‰©å±•ï¼Œä»¥å¢åŠ æ¨¡å‹çµæ´»æ€§å’Œå¤æ‚æ€§ã€‚åŸºæœ¬æ€æƒ³æ˜¯åˆ©ç”¨ä¸€å¥—åŸºç¡€å‡½æ•° $h_1, h_2, \ldots, h_k$ï¼Œè¿™äº›å‡½æ•°æä¾›äº†æ–°çš„é¢„æµ‹ç‰¹å¾ã€‚

$$ h(x_i) = (h_1(x_i),h_1(x_i),\ldots,h_k(x_i)) $$

ä»ä¸€ä¸ªç‰¹å¾ $X$ åˆ° $k$ ä¸ªç‰¹å¾çš„æ‰©å±•åŸº $X_1, X_2,\ldots, X_k$ã€‚

+   å¦‚æœæˆ‘ä»¬çš„æ•°æ®è¡¨ä¸­å…·æœ‰ $m$ ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ $k \times m$ ä¸ªç‰¹å¾

![](img/3cf75cc4ca509f9dd86ecfb64061b7cf.png)

å°†é¢„æµ‹ç‰¹å¾ $m$ çš„åŸºå‡½æ•°æ‰©å±•åˆ° $m \times k$ ä¸ªæ‰©å±•ç‰¹å¾ã€‚

## å¤šé¡¹å¼å›å½’

å¯ä»¥è¯æ˜å¤šé¡¹å¼å›å½’åªæ˜¯å°†çº¿æ€§å›å½’åº”ç”¨äºé¢„æµ‹ç‰¹å¾çš„å¤šé¡¹å¼å±•å¼€ã€‚

$$ X_{j} \rightarrow X_{j}, X_{j}Â², X_{j}Â³, \ldots X_{j}^k $$

å…¶ä¸­æˆ‘ä»¬å…·æœ‰ $j = 1, \ldots, m$ ä¸ªåŸå§‹ç‰¹å¾ã€‚

æˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªæ‰©å±•çš„é¢„æµ‹ç‰¹å¾é›†ã€‚

$$ h_{j,k}(X_j) = X_j^k $$

å…¶ä¸­æˆ‘ä»¬å…·æœ‰ $j = 1, \ldots, m$ ä¸ªåŸå§‹ç‰¹å¾å’Œ $k = 1, \ldots, K$ ä¸ªå¤šé¡¹å¼é˜¶æ•°ã€‚

æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†æˆ‘ä»¬çš„æ¨¡å‹è¡¨è¿°ä¸ºè½¬æ¢ç‰¹å¾çš„çº¿æ€§å›å½’ã€‚

$$ y = f(x) = \sum_{j=1}^{m} \sum_{k = 1}^{K} \beta_{j,k} h_{j,m}(X_j) + \beta_0 $$

ç»è¿‡ $h_l, l=1,\ldots,k$ çš„è½¬æ¢åï¼Œå¯¹äº $j=1,\ldots,m$ ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œæˆ‘ä»¬æœ‰ç›¸åŒçš„çº¿æ€§æ–¹ç¨‹å’Œåˆ©ç”¨å…ˆå‰è®¨è®ºçš„è§£æè§£çš„èƒ½åŠ›ï¼Œå‚è§çº¿æ€§å›å½’ç« èŠ‚ã€‚

æˆ‘ä»¬å‡è®¾åœ¨åº”ç”¨åŸºå˜æ¢åçº¿æ€§æˆç«‹ã€‚

+   ç°åœ¨æ¨¡å‹ç³»æ•° $\beta_{l,i}$ ä¸åˆå§‹é¢„æµ‹ç‰¹å¾çš„è½¬æ¢ç‰ˆæœ¬ç›¸å…³ï¼Œ$h_l(X_i)$ã€‚

+   ä½†æˆ‘ä»¬å¤±å»äº†è§£é‡Šç³»æ•°çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œ$\phiâ´$ ä¸­ $\phi$ æ˜¯å­”éš™ç‡æ˜¯ä»€ä¹ˆï¼Ÿ

ä¾‹å¦‚ï¼Œå¯¹äºå•ä¸ªé¢„æµ‹ç‰¹å¾ $m = 1$ï¼Œå¹¶ä¸”æœ€é«˜åˆ° $4^{th}$ é˜¶ï¼Œæ¨¡å‹æ˜¯ï¼Œ

$$ y = \beta_{1,1}X_1 + \beta_{1,1}X_1Â² + \beta_{1,3}X_1Â³ + \beta_{1,4}X_1â´ + \beta_0 $$

å…¶ä¸­æ¨¡å‹å‚æ•°çš„è¡¨ç¤ºä¸º $\beta_{m,k}$ï¼Œå…¶ä¸­ $m$ æ˜¯ç‰¹å¾ï¼Œ$k$ æ˜¯é˜¶æ•°ã€‚ä¸ºäº†æ¾„æ¸…ï¼Œè¿™é‡Œä»¥ $m = 2$ ä¸ºä¾‹ï¼Œ

$$ y = \beta_{1,1}X_1 + \beta_{1,2}X_1Â² + \beta_{1,3}X_1Â³ + \beta_{1,4}X_1â´ + \beta_{2,1}X_2 + \beta_{2,2}X_2Â² + \beta_{2,3}X_2Â³ + \beta_{2,4}X_2â´ + \beta_0 $$

å› æ­¤ï¼Œæˆ‘ä»¬çš„é¢„æµ‹å»ºæ¨¡å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š

+   åº”ç”¨å¤šé¡¹å¼åŸºå±•å¼€

+   åœ¨å¤šé¡¹å¼åŸºå±•å¼€ä¸Šæ‰§è¡Œçº¿æ€§å›å½’

## å¤šé¡¹å¼å›å½’çš„ä¼˜ç‚¹å’Œç¼ºç‚¹

å¤šé¡¹å¼å›å½’ç›¸å¯¹äºçº¿æ€§å›å½’çš„ä¼˜ç‚¹åŒ…æ‹¬ï¼Œ

+   æé«˜äº†æ‹Ÿåˆéçº¿æ€§ç°è±¡çš„çµæ´»æ€§ï¼Œé€šè¿‡çº¿æ€§åˆ†æå’Œè§£æè§£æ¥è®­ç»ƒæ¨¡å‹å‚æ•°ã€‚

ç¼ºç‚¹

é€šå¸¸ï¼Œæ¨¡å‹æ–¹å·®æ˜¾è‘—æ›´é«˜ï¼å¯èƒ½å­˜åœ¨ä¸ç¨³å®šçš„æ’å€¼å’Œç‰¹åˆ«æ˜¯å¤–æ¨ã€‚

å¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼Œç‰¹åˆ«æ˜¯å½“ $â„_ğ‘˜ \left(ğ‘¥_{ğ‘–,ğ‘—}\right)=ğ‘¥_{ğ‘–,ğ‘—}^ğ‘˜$ ä¸” $ğ‘˜$ è¾ƒå¤§æ—¶

æˆ‘ä»¬å¤±å»äº†æ¨¡å‹å‚æ•°çš„å¯è§£é‡Šæ€§ï¼Œ$ğ›½_{ğ‘—,ğ‘˜}$ ä¸ $â„_ğ‘˜ \left(ğ‘‹_j \right)$ ç›¸å…³ã€‚

## æ·»åŠ åŸºæœ¬å‡½æ•°

å¤šé¡¹å¼å›å½’çš„å¦ä¸€ç§è§£é‡Šæ˜¯é€šè¿‡æ·»åŠ åŸºæœ¬å‡½æ•°ï¼ˆå³åŸºå‡½æ•°ï¼‰æ„å»ºå›å½’æ¨¡å‹ã€‚

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªé¢„æµ‹ç‰¹å¾å’Œ $K$ ä¸ªåŸºå±•å¼€æ¥å·¥ä½œã€‚

$$ y = \sum_{l=1}^{k} \beta_{1,k} h_k (X_j) $$

å¯¹äºæˆ‘ä»¬çš„ç®€å•å•é¢„æµ‹ç‰¹å¾ X çš„å¤šé¡¹å¼é—®é¢˜ï¼Œè¿™æ˜¯ï¼Œ

$$ y = \beta_{1,K} X^K + \beta_{1,K-1} X^{K-1} + \dots + \beta_{1,2} XÂ² + \beta_{1,1} X + \beta_0 $$

è®©æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†æ·±åº¦ 4 é˜¶å¤šé¡¹å¼å±•å¼€ï¼Œ$K=4$ï¼Œè¿›è¡Œå·¥ä½œã€‚

![å›¾ç‰‡](img/ea64332d4805861caa74b4d26e6bd3f0.png)

å¤šé¡¹å¼åŸºè‡³$K=4$ã€‚

ä¸ºäº†æ„å»ºæˆ‘ä»¬çš„å‡½æ•°ï¼Œæˆ‘ä»¬æ­£åœ¨ç§»åŠ¨ã€ç¼©æ”¾å’Œæ·»åŠ è¿™äº›åŸºæœ¬å‡½æ•°ã€‚è®©æˆ‘ä»¬é€šè¿‡$k=2$åŸºå‡½æ•°çš„ä¾‹å­ï¼Œå³æŠ›ç‰©çº¿$h_2: ğ‘¦=ğ‘¥Â²$ï¼Œå›é¡¾å¦‚ä½•è¿›è¡ŒåŸºæœ¬å‡½æ•°çš„ç§»åŠ¨å’Œç¼©æ”¾ã€‚è€ƒè™‘ä»¥ä¸‹å˜åŒ–ï¼š

+   åœ¨ X è½´ä¸Šå¹³ç§»

+   åœ¨ Y è½´ä¸Šå¹³ç§»

+   åœ¨ X è½´ä¸Šç¿»è½¬

+   æ”¹å˜æ–œç‡

å¯¹äºæ¯ä¸€ä¸ªï¼Œæˆ‘éƒ½å±•ç¤ºäº†å˜åŒ–çš„å¯è§†åŒ–ï¼Œç„¶åæ˜¯å®ƒå¯¹å¤šé¡¹å¼æ–¹ç¨‹çš„å½±å“ã€‚

+   åœ¨ X è½´ä¸Šå¹³ç§»å‡½æ•°ï¼Œ

![å›¾ç‰‡](img/87df4ff1a6183394b90b31dfe989e9f7.png)

åœ¨ X è½´ä¸Šå¹³ç§»äºŒé˜¶åŸºæœ¬å‡½æ•°ã€‚

$$ y = (x - \Delta_x)Â² = xÂ² - 2\Delta_x x + \Delta_xÂ² $$

+   åœ¨ Y è½´ä¸Šå¹³ç§»å‡½æ•°ï¼Œ

![å›¾ç‰‡](img/87df4ff1a6183394b90b31dfe989e9f7.png)

åœ¨ Y è½´ä¸Šå¹³ç§»äºŒé˜¶åŸºæœ¬å‡½æ•°ã€‚

$$ y = xÂ² - \Delta_y $$

+   åœ¨ X è½´ä¸Šç¿»è½¬å‡½æ•°ï¼š

![å›¾ç‰‡](img/2e93ae27cb57ce4b016c4823c8e50642.png)

åœ¨ X è½´ä¸Šç¿»è½¬äºŒé˜¶åŸºæœ¬å‡½æ•°ã€‚

$$ y = \pm \beta_2 xÂ² $$

+   æ”¹å˜æ–œç‡ï¼š

![å›¾ç‰‡](img/63aa39b205aca7c3c08dd272484377e3.png)

æ”¹å˜äºŒé˜¶åŸºæœ¬å‡½æ•°çš„æ–œç‡ã€‚

$$ y = \downarrow \beta_2 xÂ², \text{æ›´å®½/æ›´æµ…} $$$$ y = \uparrow \beta_2 xÂ², \text{æ›´çª„/æ›´æ·±} $$

è®©æˆ‘ä»¬ä»ä¸Šé¢çš„å†…å®¹ä¸­åšä¸€äº›è§‚å¯Ÿï¼Œ

+   ä»…åœ¨ Y è½´ä¸Šå¹³ç§»éœ€è¦ä¿®æ”¹å¤šé¡¹å¼æ–¹ç¨‹ä¸­æ¨¡å‹å‚æ•°çš„å¸¸æ•°é¡¹

+   åœ¨ X è½´ä¸Šå¹³ç§»éœ€è¦ä¿®æ”¹å¤šé¡¹å¼æ–¹ç¨‹ä¸­ä½é˜¶æ¨¡å‹å‚æ•°

+   åœ¨ X è½´ä¸Šç¿»è½¬éœ€è¦æ”¹å˜å¤šé¡¹å¼æ–¹ç¨‹ä¸­å½“å‰é˜¶æ•°æ¨¡å‹å‚æ•°çš„ç¬¦å·

+   å¢åŠ æ–œç‡éœ€è¦å¢åŠ å¤šé¡¹å¼æ–¹ç¨‹ä¸­å½“å‰é˜¶æ•°æ¨¡å‹å‚æ•°

## å¤šé¡¹å¼å›å½’çš„å‡è®¾

æˆ‘ä»¬çš„å¤šé¡¹å¼å›å½’æ¨¡å‹æœ‰ä¸€äº›é‡è¦çš„å‡è®¾ï¼Œè¿™äº›å‡è®¾æ˜¯ä»ä¸Šè¿°çº¿æ€§å›å½’çš„å‡è®¾ä¸­æ‰©å±•å‡ºæ¥çš„ï¼Œ

+   **æ— è¯¯å·®** - é¢„æµ‹ç‰¹å¾åŸºå‡½æ•°å±•å¼€æ˜¯æ— è¯¯å·®çš„ï¼Œä¸æ˜¯éšæœºå˜é‡

+   **å¸¸æ•°æ–¹å·®** - å“åº”è¯¯å·®åœ¨é¢„æµ‹å€¼ä¸Šæ˜¯æ’å®šçš„

+   **çº¿æ€§** - å“åº”æ˜¯åŸºç‰¹å¾çº¿æ€§ç»„åˆ

+   **å¤šé¡¹å¼** - X å’Œ Y ä¹‹é—´çš„å…³ç³»æ˜¯å¤šé¡¹å¼

+   **è¯¯å·®ç‹¬ç«‹æ€§** - å“åº”è¯¯å·®ä¹‹é—´ç›¸äº’ä¸ç›¸å…³

+   **æ— å¤šé‡å…±çº¿æ€§** - åŸºç‰¹å¾å±•å¼€ä¸­æ²¡æœ‰ä»»ä½•ä¸€ä¸ªä¸å…¶ä»–ç‰¹å¾çº¿æ€§å†—ä½™

è€ƒè™‘ä¸Šè¿°å¤šé¡¹å¼åŸºå±•å¼€ï¼Œæ£€æŸ¥æˆ‘ä»¬åŸºä¹‹é—´çš„å…±çº¿æ€§ã€‚ä¸ºäº†æ£€æŸ¥ï¼Œæˆ‘è®¡ç®—äº†ä»¥ä¸‹æ¼”ç¤ºä¸­ä½¿ç”¨çš„åŸºå±•å¼€çš„ç›¸å…³çŸ©é˜µã€‚

![å›¾ç‰‡](img/08d2443894d5916687f1cf4785734bec.png)

$K=4$ çš„å¤šé¡¹å¼åŸºå±•å¼€çš„ç›¸å…³çŸ©é˜µã€‚

$K=1$ å’Œ $K=3$ çš„åŸºä¸ $k=2$ å’Œ $k=4$ çš„åŸºä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„å…±çº¿æ€§ã€‚

+   å›æƒ³ä¸€ä¸‹ï¼Œå…±çº¿æ€§å¯èƒ½å¢åŠ æ¨¡å‹æ–¹å·®

ä¸ºäº†æ¶ˆé™¤è¿™ç§å…±çº¿æ€§ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨èµ«å°”ç±³ç‰¹å¤šé¡¹å¼ã€‚

## **èµ«å°”ç±³ç‰¹å¤šé¡¹å¼**

æ˜¯å®æ•°çº¿ä¸Šæ­£äº¤å¤šé¡¹å¼æ—ã€‚

| é˜¶æ•° | èµ«å°”ç±³ç‰¹å¤šé¡¹å¼ $H_e(x)$ |
| --- | --- |
| 0 é˜¶ | $H_{e_0}(x) = 1$ |
| 1 é˜¶ | $H_{e_1}(x) = x$ |
| 2 é˜¶ | $H_{e_2}(x) = xÂ² - 1$ |
| 3 é˜¶ | $H_{e_3}(x) = xÂ³ - 3x$ |
| 4 é˜¶ | $H_{e_4}(x) = xâ´ - 6xÂ² + 3$ |

è¿™äº›å¤šé¡¹å¼ç›¸å¯¹äºä¸€ä¸ªåŠ æƒå‡½æ•°æ˜¯æ­£äº¤çš„ï¼Œ

$$ ğ‘¤(ğ‘¥)=ğ‘’^{âˆ’\frac{ğ‘¥Â²}{2}} $$

è¿™æ˜¯æ ‡å‡†é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œæ²¡æœ‰ç¼©æ”¾å™¨ï¼Œ$\frac{1}{\sqrt{2\pi}}$ã€‚æ­£äº¤æ€§çš„å®šä¹‰å¦‚ä¸‹ï¼Œ

$$ \int_{-\infty}^{\infty} H_m(x) H_n(x) w(x) \, dx = 0 $$

èµ«å°”ç±³ç‰¹å¤šé¡¹å¼åœ¨æ ‡å‡†æ­£æ€æ¦‚ç‡åˆ†å¸ƒçš„åŒºé—´ $[âˆ’\infty,\infty]$ ä¸Šæ˜¯æ­£äº¤çš„ã€‚

é€šè¿‡åœ¨å¤šé¡¹å¼å›å½’ä¸­ç”¨èµ«å°”ç±³ç‰¹å¤šé¡¹å¼ä»£æ›¿å¸¸è§„å¤šé¡¹å¼è¿›è¡Œå¤šé¡¹å¼åŸºå±•å¼€ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†é¢„æµ‹ç‰¹å¾ä¹‹é—´çš„å¤šé‡å…±çº¿æ€§ï¼Œ

+   å›æƒ³ä¸€ä¸‹ï¼Œé¢„æµ‹ç‰¹å¾çš„ç‹¬ç«‹æ€§æ˜¯åº”ç”¨äºå¤šé¡¹å¼å›å½’ä¸­å¤šé¡¹å¼åŸºå±•å¼€çš„çº¿æ€§ç³»ç»Ÿçš„ä¸€ä¸ªå‡è®¾

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = True
import os                                                     # to set current working directory 
import math                                                   # square root operator
import numpy as np                                            # arrays and matrix math
import scipy                                                  # Hermite polynomials
from scipy import stats                                       # statistical methods
import pandas as pd                                           # DataFrames
import pandas.plotting as pd_plot
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import seaborn as sns                                         # for matrix scatter plots
from sklearn.linear_model import LinearRegression             # linear regression with scikit learn
from sklearn.preprocessing import PolynomialFeatures          # polynomial basis expansion
from sklearn import metrics                                   # measures to check our models
from sklearn.preprocessing import (StandardScaler,PolynomialFeatures) # standardize the features, polynomial basis expansion
from sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning
from sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline
from sklearn import metrics                                   # measures to check our models
from sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation
from sklearn.model_selection import train_test_split          # train and test split
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # supress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½é¦–å…ˆéœ€è¦å®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥ â€˜python -m pip install [package-name]â€™ æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£ï¼Œè¿˜æœ‰æ›´å¤šå¸®åŠ©ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæ–¹ä¾¿çš„å‡½æ•°æ¥ä¸ºæˆ‘ä»¬çš„å›¾è¡¨æ·»åŠ ç½‘æ ¼çº¿ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³çŸ©é˜µã€‚

```py
def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks

def plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix 
    my_colormap = plt.get_cmap('RdBu_r', 256)          
    newcolors = my_colormap(np.linspace(0, 1, 256))
    white = np.array([256/256, 256/256, 256/256, 1])
    white_low = int(128 - mask*128); white_high = int(128+mask*128)
    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)
    newcmp = ListedColormap(newcolors)
    m = corr_matrix.shape[0]
    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)
    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()
    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()
    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
    plt.colorbar(im, orientation = 'vertical')
    plt.title(title)
    for i in range(0,m):
        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')
        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')
    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5]) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/PGE383")                                        # set the working directory 
```

æ‚¨å°†éœ€è¦æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ä»¥ä½¿ç”¨æ‚¨è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”åœ¨ Mac ä¸Šæ ¼å¼ä¸åŒï¼ˆä¾‹å¦‚ï¼šâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½æ•°æ®

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„äºŒå…ƒï¼Œç©ºé—´æ•°æ®é›† [Density_Por_data.csv](https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv)ï¼Œå®ƒåœ¨æˆ‘çš„ GeoDataSet ä»“åº“ä¸­å¯ç”¨ã€‚å®ƒæ˜¯ä¸€ä¸ªé€—å·åˆ†éš”çš„æ–‡ä»¶ï¼ŒåŒ…å«ï¼š

+   æ·±åº¦ï¼ˆç±³ï¼‰

+   é«˜æ–¯è½¬æ¢å­”éš™ç‡ï¼ˆ%ï¼‰

æˆ‘ä»¬ä½¿ç”¨ pandas çš„ â€˜read_csvâ€™ å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸º â€˜dfâ€™ çš„æ•°æ®æ¡†ä¸­ã€‚

```py
df = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/1D_Porosity.csv") # data from Dr. Pyrcz's github repository 
```

## å¯è§†åŒ– DataFrame

å¯è§†åŒ–è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ DataFrame æ˜¯åœ¨æˆ‘ä»¬æ„å»ºæ¨¡å‹ä¹‹å‰çš„ä¸€ä¸ªæœ‰ç”¨çš„æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ©ç”¨ â€˜headâ€™ DataFrame æˆå‘˜å‡½æ•°æ¥é¢„è§ˆï¼ˆæ ¼å¼æ•´æ´ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚

```py
df.head(n=13)                                                 # preview the data 
```

|  | æ·±åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| 0 | 0.25 | -1.37 |
| 1 | 0.50 | -2.08 |
| 2 | 0.75 | -1.67 |
| 3 | 1.00 | -1.16 |
| 4 | 1.25 | -0.24 |
| 5 | 1.50 | -0.36 |
| 6 | 1.75 | 0.44 |
| 7 | 2.00 | 0.36 |
| 8 | 2.25 | -0.02 |
| 9 | 2.50 | -0.63 |
| 10 | 2.75 | -1.26 |
| 11 | 3.00 | -1.03 |
| 12 | 3.25 | 0.88 |

## è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯

åœ¨ DataFrames ä¸­ï¼Œæœ‰è®¸å¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯ã€‚

+   describe å‘½ä»¤æä¾›äº†ä¸€ä¸ªè®¡æ•°ã€å¹³å‡å€¼ã€æ ‡å‡†å·®ã€ç™¾åˆ†ä½æ•°ã€æœ€å°å€¼ã€æœ€å¤§å€¼çš„æ•°æ®è¡¨ã€‚

+   æˆ‘å–œæ¬¢æŒ‡å®šç™¾åˆ†ä½æ•°ï¼Œå¦åˆ™é»˜è®¤ä¸º P25ï¼ŒP50 å’Œ P75 å››åˆ†ä½æ•°

```py
df.describe(percentiles=[0.1,0.9]).transpose()                # summary statistics 
```

|  | count | mean | std | min | 10% | 50% | 90% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Depth | 40.0 | 5.12500 | 2.922613 | 0.25 | 1.225 | 5.125 | 9.025 | 10.00 |
| Nporosity | 40.0 | 0.02225 | 0.992111 | -2.08 | -1.271 | 0.140 | 1.220 | 2.35 |

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æ·±åº¦å’Œé«˜æ–¯å˜æ¢çš„å­”éš™ç‡ Nporosity ä» DataFrame ä¸­æå–åˆ°å•ç‹¬çš„ä¸€ç»´æ•°ç»„ä¸­ï¼Œåˆ†åˆ«ç§°ä¸º'depth'å’Œ'NPor'ï¼Œä»¥ä¾¿ä»£ç æ˜“äºé˜…è¯»ã€‚

+   è­¦å‘Šï¼Œè¿™æ˜¯ä¸€ä¸ªæµ…æ‹·è´ï¼Œå¦‚æœæˆ‘ä»¬æ›´æ”¹è¿™äº›ä¸€ç»´æ•°ç»„ï¼Œæ›´æ”¹å°†åæ˜ åœ¨åŸå§‹ DataFrame ä¸­

```py
Xname = ['Depth']; yname = ['Nporosity']                      # select the predictor and response feature

Xlabel = ['Depth']; ylabel = ['Gaussian Transformed Porosity'] # specify the feature labels for plotting
Xunit = ['m']; yunit = ['N[%]']
Xlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')']
ylabelunit = ylabel[0] + ' (' + yunit[0] + ')'

X = df[Xname[0]]                                              # extract the 1D ndarrays from the DataFrame
y = df[yname[0]]

Xmin = 0.0; Xmax = 10.0                                       # limits for plotting
ymin = -3.0; ymax = 3.0

X_values = np.linspace(Xmin,Xmax,100)                         # X intervals to visualize the model 
```

## çº¿æ€§å›å½’æ¨¡å‹

è®©æˆ‘ä»¬é¦–å…ˆä½¿ç”¨ scikit-learn çš„ LinearRegression ç±»è®¡ç®—çº¿æ€§å›å½’æ¨¡å‹ã€‚æ­¥éª¤åŒ…æ‹¬ï¼Œ

1.  **instantiate** - çº¿æ€§å›å½’å¯¹è±¡ï¼Œæ³¨æ„è¿™é‡Œæ²¡æœ‰éœ€è¦æŒ‡å®šçš„è¶…å‚æ•°ã€‚

1.  **fit** - ä½¿ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒå®ä¾‹åŒ–çš„çº¿æ€§å›å½’å¯¹è±¡

1.  **predict** - ä½¿ç”¨è®­ç»ƒå¥½çš„çº¿æ€§å›å½’å¯¹è±¡

è¿™é‡Œæ˜¯æˆ‘ä»¬çº¿æ€§å›å½’æ¨¡å‹çš„å®ä¾‹åŒ–å’Œæ‹Ÿåˆæ­¥éª¤ã€‚

+   æ³¨æ„ï¼Œæˆ‘ä»¬æ·»åŠ äº† reshape åˆ°æˆ‘ä»¬çš„é¢„æµ‹ç‰¹å¾ä¸­ï¼Œå› ä¸º scikit-learn å‡è®¾æœ‰å¤šä¸ªé¢„æµ‹ç‰¹å¾ï¼Œå¹¶æœŸæœ›ä¸€ä¸ªäºŒç»´æ•°ç»„ã€‚æˆ‘ä»¬å°†ä¸€ç»´ ndarray é‡å¡‘ä¸ºä¸€ä¸ªåªæœ‰ä¸€åˆ—çš„äºŒç»´æ•°ç»„ã€‚

è®­ç»ƒæ¨¡å‹åï¼Œæˆ‘ä»¬ç”¨æ•°æ®ç»˜åˆ¶æ¨¡å‹ï¼Œä»¥è¿›è¡Œå¯è§†æ¨¡å‹æ£€æŸ¥ã€‚

```py
lin = LinearRegression()                                      # instantiate linear regression object, note no hyperparameters 
lin.fit(X.values.reshape(-1, 1), y)                           # train linear regression model

slope = lin.coef_[0]                                          # get the model parameters
intercept = lin.intercept_

plt.subplot(111)                                              # plot the data and the model
plt.scatter(X,y,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.plot(X_values,intercept + slope*X_values,label='model',color = 'black')
plt.title('Linear Regression Model, Regression of ' + yname[0] + ' on ' + Xname[0])
plt.xlabel(Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel(yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])
plt.annotate('Linear Regression Model',[4.5,-1.8])
plt.annotate(r'    $\beta_1$ :' + str(round(slope,2)),[6.8,-2.3])
plt.annotate(r'    $\beta_0$ :' + str(round(intercept,2)),[6.8,-2.7])
plt.annotate(r'$N[\phi] = \beta_1 \times z + \beta_0$',[4.0,-2.3])
plt.annotate(r'$N[\phi] = $' + str(round(slope,2)) + r' $\times$ $z$ + (' + str(round(intercept,2)) + ')',[4.0,-2.7])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/ba77774bef128a461422095cb22a2827.png)

## ä¸éå‚æ•°é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¯”è¾ƒ

è®©æˆ‘ä»¬è¿è¡Œå‡ ä¸ªéå‚æ•°é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»¥ä¸çº¿æ€§å‚æ•°æ¨¡å‹å’Œå¤šé¡¹å¼å‚æ•°æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚é¦–å…ˆæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªå¿«é€Ÿå†³ç­–æ ‘æ¨¡å‹ï¼Œç„¶åæ˜¯ä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹ã€‚

+   æˆ‘ä»¬è·å¾—äº†æ˜¾è‘—çš„çµæ´»æ€§ï¼Œå¯ä»¥æ‹Ÿåˆæ•°æ®ä¸­çš„ä»»ä½•æ¨¡å¼

+   éœ€è¦æ›´å¤šçš„æ¨ç†ï¼Œå› ä¸ºéå‚æ•°å®é™…ä¸Šå‚æ•°ä¸°å¯Œï¼

æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…å…³äºå†³ç­–æ ‘å’Œéšæœºæ£®æ—çš„ç« èŠ‚ã€‚

```py
from sklearn import tree                                      # tree program from scikit learn 

my_tree = tree.DecisionTreeRegressor(min_samples_leaf=5, max_depth = 20) # instantiate the decision tree model with hyperparameters
my_tree = my_tree.fit(X.values.reshape(-1, 1),y)              # fit the decision tree to the training data (all the data in this case)
DT_y = my_tree.predict(X_values.reshape(-1,1))                # predict at high resolution over the range of depths

plt.subplot(111)                                              # plot the model and data
plt.scatter(X,y,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.plot(X_values, DT_y, label='model', color = 'black')
plt.title('Decision Tree Model, ' + yname[0] + ' as a Function of ' + Xname[0])
plt.xlabel(Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel(yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/8e31233c62876ecb3c64296751df5ef5.png)

è¿™é‡Œæ˜¯ä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹ï¼š

```py
from sklearn.ensemble import RandomForestRegressor            # random forest method

max_depth = 5                                                 # set the random forest hyperparameters
num_tree = 1000
max_features = 1

my_forest = RandomForestRegressor(max_depth=max_depth,random_state=seed,n_estimators=num_tree,max_features=max_features)
my_forest.fit(X = X.values.reshape(-1, 1), y = y)  
RF_y = my_forest.predict(X_values.reshape(-1,1))
plt.subplot(111)
plt.scatter(X,y,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.plot(X_values, RF_y, label='model', color = 'black')
plt.title('Random Forest Tree Model, ' + yname[0] + ' as a Function of ' + Xname[0])
plt.xlabel(Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel(yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2)
plt.show() 
```

![å›¾ç‰‡](img/704a35303eabbbf03215f2c0a311653d.png)

æ³¨æ„ï¼Œæ²¡æœ‰å¯¹è¿™äº›å»ºæ¨¡çš„è¶…å‚æ•°è¿›è¡Œè°ƒæ•´ã€‚æˆ‘åªæ˜¯æƒ³å±•ç¤ºéå‚æ•°æ¨¡å‹å­¦ä¹ ç³»ç»Ÿå½¢çŠ¶çš„å·¨å¤§çµæ´»æ€§ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„å‚æ•°å¤šé¡¹å¼æ¨¡å‹ã€‚

+   è®©æˆ‘ä»¬é¦–å…ˆå°†æ•°æ®è½¬æ¢ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œå³é«˜æ–¯åˆ†å¸ƒã€‚

+   æˆ‘ä»¬è¿™æ ·åšæ˜¯ä¸ºäº†æé«˜æ¨¡å‹æ‹Ÿåˆåº¦ï¼ˆå¤„ç†å¼‚å¸¸å€¼ï¼‰å¹¶ç¬¦åˆå³å°†ä»‹ç»çš„ Hermite å¤šé¡¹å¼ç†è®ºã€‚

## é«˜æ–¯ç•¸å˜ \ é«˜æ–¯å˜æ¢

è®©æˆ‘ä»¬å°†ç‰¹å¾è½¬æ¢ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œ

+   é«˜æ–¯åˆ†å¸ƒ

+   å‡å€¼ä¸º 0.0

+   æ ‡å‡†å·®ä¸º 1.0

å­”éš™ç‡ç‰¹å¾ä¹‹å‰å·²ç»è¢«â€œè½¬æ¢â€ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œä½†æœ‰æœºä¼šè¿›è¡Œæ¸…ç†ã€‚

+   æ¯”è¾ƒåŸå§‹å’Œè½¬æ¢åçš„ç»“æœ

+   æ³¨æ„ï¼Œæˆ‘ä½¿ç”¨äº†æˆ‘ä»åŸå§‹ GSLIB (Deutsch and Journel, 1997) ç«¯å£ç§»æ¤çš„ GeostatsPy é«˜æ–¯å˜æ¢ï¼Œå› ä¸º scikit-learn çš„é«˜æ–¯å˜æ¢ä¼šåˆ›å»ºæˆªæ–­å°–å³°/å¼‚å¸¸å€¼ã€‚

```py
import geostatspy.geostats as geostats                        # for Gaussian transform from GSLIB

df_ns = pd.DataFrame()   
df_ns[Xname[0]], tvPor, tnsPor = geostats.nscore(df, Xname[0]) # nscore transform for all facies porosity 
df_ns[yname[0]], tvdepth, tnsdepth = geostats.nscore(df, yname[0]) # nscore transform for all facies permeability
X_ns = df_ns[Xname[0]]; y_ns = df_ns[yname[0]]
X_ns_values = np.linspace(-3.0,3.0,1000)                      # values to predict at in standard normal space 
```

è®©æˆ‘ä»¬ç»˜åˆ¶ä¸€äº›å¥½çš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°å›¾æ¥æ£€æŸ¥åŸå§‹å’Œè½¬æ¢åçš„å˜é‡ã€‚

+   ç»“æœçœ‹èµ·æ¥éå¸¸å¥½

æˆ‘ä»¬è¿™æ ·åšæ˜¯å› ä¸ºæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒçš„é¢„æµ‹ç‰¹å¾æ¥å®ç°æ­£äº¤æ€§ã€‚æ›´å¤šå†…å®¹ç¨åæ­æ™“ï¼

```py
plt.subplot(221)                                              # plot original sand and shale porosity histograms
plt.hist(df[Xname[0]], facecolor='red',bins=np.linspace(Xmin,Xmax,1000),histtype="stepfilled",alpha=0.2,density=True,
         cumulative=True,edgecolor='black',label='Original')
plt.xlim([0.0,10.0]); plt.ylim([0,1.0])
plt.xlabel(Xname[0] + ' (' + Xunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Original Depth')
plt.legend(loc='upper left')
plt.grid(True)

plt.subplot(222)  
plt.hist(df_ns[Xname[0]], facecolor='blue',bins=np.linspace(-3.0,3.0,1000),histtype="stepfilled",alpha=0.2,density=True,
         cumulative=True,edgecolor='black',label = 'NS')
plt.xlim([-3.0,3.0]); plt.ylim([0,1.0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Nscore ' + Xname[0])
plt.legend(loc='upper left')
plt.grid(True)

plt.subplot(223)                                        # plot nscore transformed sand and shale histograms
plt.hist(df[yname[0]], facecolor='red',bins=np.linspace(ymin,ymax,1000),histtype="stepfilled",alpha=0.2,density=True,
         cumulative=True,edgecolor='black',label='Original')
plt.xlim([-3.0,3.0]); plt.ylim([0,1.0])
plt.xlabel(yname[0] + ' (' + yunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Original Porosity')
plt.legend(loc='upper left')
plt.grid(True)

plt.subplot(224)                                        # plot nscore transformed sand and shale histograms
plt.hist(df_ns[yname[0]], facecolor='blue',bins=np.linspace(-3.0,3.0,1000),histtype="stepfilled",alpha=0.2,density=True,
         cumulative=True,edgecolor='black',label = 'NS')
plt.xlim([-3.0,3.0]); plt.ylim([0,1.0])
plt.xlabel('NS: ' + yname[0] + ' (' + yunit[0] + ')'); plt.ylabel('Frequency'); plt.title('Nscore ' + yname[0])
plt.legend(loc='upper left')
plt.grid(True)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=2.0, wspace=0.2, hspace=0.3); plt.show() 
```

![_images/502106ad20a71cc9f6a412707dabe69539c7d2e42d6c72fa8b141c5695c13588.png](img/7b0e4b346e5f29d5e18e8d52b82145f1.png)

## å¸¦æœ‰æ ‡å‡†åŒ–ç‰¹å¾çš„çº¿æ€§å›å½’æ¨¡å‹

è®©æˆ‘ä»¬é‡å¤çº¿æ€§å›å½’æ¨¡å‹ï¼Œç°åœ¨ä½¿ç”¨æ ‡å‡†åŒ–ç‰¹å¾ã€‚

```py
lin_ns = LinearRegression()                                   # instantiate linear regression object, note no hyperparameters 
lin_ns.fit(X_ns.values.reshape(-1, 1), y_ns)                  # train linear regression model
slope_ns = lin_ns.coef_[0]                                    # get the model parameters
intercept_ns = lin_ns.intercept_

plt.subplot(111)                                              # plot the data and the model
plt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.plot(X_ns_values,intercept_ns + slope_ns*X_ns_values,label='model',color = 'black')
plt.title('Linear Regression Model, Regression of NS ' + yname[0] + ' on ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])
plt.annotate('Linear Regression Model',[0.8,-1.8])
plt.annotate(r'    $\beta_1$ :' + str(round(slope_ns,2)),[1.8,-2.3])
plt.annotate(r'    $\beta_0$ :' + str(round(intercept_ns,2)),[1.8,-2.7])
plt.annotate(r'$N[\phi] = \beta_1 \times z + \beta_0$',[0.5,-2.3])
plt.annotate(r'$N[\phi] = $' + str(round(slope_ns,2)) + r' $\times$ $z$ + (' + str(round(intercept_ns,2)) + ')',[0.5,-2.7])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/1966b7337c4a5b38596f989a8211aa0c1e8cfbab292369ed714bb5b7ebefb550.png](img/4c865fd8f2805646d61c8babce9fdbbd.png)

å†æ¬¡ï¼Œæ‹Ÿåˆåº¦ä¸ä½³ã€‚è®©æˆ‘ä»¬ä½¿ç”¨æ›´å¤æ‚ã€æ›´çµæ´»çš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

## å¤šé¡¹å¼å›å½’

æˆ‘ä»¬å°†æ‰‹åŠ¨è¿›è¡Œå¤šé¡¹å¼å›å½’ï¼š

+   åˆ›å»ºåŸå§‹é¢„æµ‹ç‰¹å¾çš„å¤šé¡¹å¼åŸºå±•å¼€

+   åœ¨å¤šé¡¹å¼åŸºå±•å¼€ä¸Šè¿›è¡Œçº¿æ€§å›å½’

### å¤šé¡¹å¼åŸºå±•å¼€

è®©æˆ‘ä»¬ä»è®¡ç®— 1 ä¸ªé¢„æµ‹ç‰¹å¾çš„å¤šé¡¹å¼åŸºå±•å¼€å¼€å§‹ã€‚

```py
poly4 = PolynomialFeatures(degree = 4)                        # instantiate polynomial expansion 
X_ns_poly4 = poly4.fit_transform(X_ns.values.reshape(-1, 1))  # calculate the basis expansion for our dataset
df_X_ns_poly4 = pd.DataFrame({'Values':X_ns,'0th':X_ns_poly4[:,0],'1st':X_ns_poly4[:,1],'2nd':X_ns_poly4[:,2], 
                              '3rd':X_ns_poly4[:,3],'4th':X_ns_poly4[:,4]}) # make a new DataFrame from the vectors
df_X_ns_poly4 = pd.DataFrame({'Values':X_ns,'1st':X_ns_poly4[:,1],'2nd':X_ns_poly4[:,2], 
                              '3rd':X_ns_poly4[:,3],'4th':X_ns_poly4[:,4]}) # make a new DataFrame from the vectors
df_X_ns_poly4.head()                                          # preview the polynomial basis expansion with the original predictor feature 
```

|  | å€¼ | ç¬¬ä¸€ | ç¬¬äºŒ | ç¬¬ä¸‰ | ç¬¬å›› |
| --- | --- | --- | --- | --- | --- |
| 0 | -2.026808 | -2.026808 | 4.107951 | -8.326029 | 16.875264 |
| 1 | -1.780464 | -1.780464 | 3.170053 | -5.644167 | 10.049238 |
| 2 | -1.534121 | -1.534121 | 2.353526 | -3.610592 | 5.539084 |
| 3 | -1.356312 | -1.356312 | 1.839582 | -2.495046 | 3.384060 |
| 4 | -1.213340 | -1.213340 | 1.472193 | -1.786270 | 2.167352 |

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥åŸå§‹é¢„æµ‹ç‰¹å¾æ•°æ®çš„å¤šé¡¹å¼åŸºå±•å¼€ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

+   å›æƒ³ä¸€ä¸‹ï¼Œé¢„æµ‹ç‰¹å¾ä¹‹é—´çš„é«˜åº¦ç›¸å…³æ€§ä¼šå¢åŠ æ¨¡å‹æ–¹å·®ã€‚

```py
corr_matrix = df_X_ns_poly4.iloc[:,1:].corr()                 # calculate the correlation matrix

plt.subplot(111)
plot_corr(corr_matrix,'Polynomial Expansion Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![_images/2f3f18b5d2988d034a420125ea0efceca61840db5af413c92d054ca206d52af6.png](img/9b9eee94daf8d4510c17a21728efa520.png)

ç¬¬ä¸€é˜¶å’Œç¬¬ä¸‰é˜¶ä»¥åŠç¬¬äºŒé˜¶å’Œç¬¬å››é˜¶ä¹‹é—´å­˜åœ¨é«˜åº¦ç›¸å…³æ€§ã€‚

+   è®©æˆ‘ä»¬é€šè¿‡å¤šé¡¹å¼åŸºçš„çŸ©é˜µæ•£ç‚¹å›¾æ¥æ£€æŸ¥è¿™ä¸€ç‚¹ã€‚

### å¤šé¡¹å¼åŸºå±•å¼€

è®©æˆ‘ä»¬ä»è®¡ç®— 1 ä¸ªé¢„æµ‹ç‰¹å¾çš„å¤šé¡¹å¼åŸºå±•å¼€å¼€å§‹ã€‚

```py
poly4 = PolynomialFeatures(degree = 4)                        # instantiate polynomial expansion 
X_ns_poly4 = poly4.fit_transform(X_ns.values.reshape(-1, 1))  # calculate the basis expansion for our dataset
df_X_ns_poly4 = pd.DataFrame({'Values':X_ns,'0th':X_ns_poly4[:,0],'1st':X_ns_poly4[:,1],'2nd':X_ns_poly4[:,2], 
                              '3rd':X_ns_poly4[:,3],'4th':X_ns_poly4[:,4]}) # make a new DataFrame from the vectors
df_X_ns_poly4 = pd.DataFrame({'Values':X_ns,'1st':X_ns_poly4[:,1],'2nd':X_ns_poly4[:,2], 
                              '3rd':X_ns_poly4[:,3],'4th':X_ns_poly4[:,4]}) # make a new DataFrame from the vectors
df_X_ns_poly4.head()                                          # preview the polynomial basis expansion with the original predictor feature 
```

|  | å€¼ | ç¬¬ä¸€ | ç¬¬äºŒ | ç¬¬ä¸‰ | ç¬¬å›› |
| --- | --- | --- | --- | --- | --- |
| 0 | -2.026808 | -2.026808 | 4.107951 | -8.326029 | 16.875264 |
| 1 | -1.780464 | -1.780464 | 3.170053 | -5.644167 | 10.049238 |
| 2 | -1.534121 | -1.534121 | 2.353526 | -3.610592 | 5.539084 |
| 3 | -1.356312 | -1.356312 | 1.839582 | -2.495046 | 3.384060 |
| 4 | -1.213340 | -1.213340 | 1.472193 | -1.786270 | 2.167352 |

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥åŸå§‹é¢„æµ‹ç‰¹å¾æ•°æ®çš„å¤šé¡¹å¼åŸºå±•å¼€çš„ç›¸å…³æ€§ã€‚

+   å›æƒ³ä¸€ä¸‹ï¼Œé¢„æµ‹ç‰¹å¾ä¹‹é—´é«˜åº¦çš„ç›¸å…³æ€§ä¼šå¢åŠ æ¨¡å‹æ–¹å·®ã€‚

```py
corr_matrix = df_X_ns_poly4.iloc[:,1:].corr()                 # calculate the correlation matrix

plt.subplot(111)
plot_corr(corr_matrix,'Polynomial Expansion Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/9b9eee94daf8d4510c17a21728efa520.png)

æˆ‘ä»¬åœ¨é˜¶æ•° 1 å’Œ 3 ä»¥åŠé˜¶æ•° 2 å’Œ 4 ä¹‹é—´å­˜åœ¨é«˜åº¦ç›¸å…³æ€§ã€‚

+   è®©æˆ‘ä»¬é€šè¿‡å¤šé¡¹å¼åŸºçš„çŸ©é˜µæ•£ç‚¹å›¾æ¥éªŒè¯è¿™ä¸€ç‚¹ã€‚

## å¯è§†åŒ–å¤šé¡¹å¼å±•å¼€ç‰¹å¾çš„æˆå¯¹å…³ç³»

```py
sns.pairplot(df_X_ns_poly4.iloc[:,1:],vars=['1st','2nd','3rd','4th'],markers='o', kind='reg',diag_kind='kde')
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/3ea96d491efa1ce020f1f121c4e5fc5c.png)

è®©æˆ‘ä»¬åœ¨é«˜æ–¯å˜æ¢æ·±åº¦ä¸Šå¯è§†åŒ–å¤šé¡¹å¼å±•å¼€ã€‚

```py
plt.subplot(111)                                              # plot the polynomial basis expansion
plt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,0],label='0th',color = 'black')
plt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,1],label='1th',color = 'blue')
plt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,2],label='2th',color = 'green')
plt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,3],label='3th',color = 'red')
plt.plot(X_ns_values,poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,4],label='4th',color = 'orange') 
plt.title('Polynomial Basis Expansion of ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('h[ NS: ' + Xname[0] + ' (' + Xunit[0] + ') ]')
plt.legend(); plt.xlim(-3,3); add_grid()
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/3f62f7e21b741e2cd86be2687d0f5fdb.png)

æˆ‘ä»¬è¿˜å¯ä»¥æ£€æŸ¥æ¯ä¸ªå¤šé¡¹å¼åŸºå±•å¼€çš„ç®—æœ¯å¹³å‡å€¼ã€‚

```py
print('The averages of each basis expansion, 0 - 4th order = ' + str(stats.describe(X_ns_poly4)[2]) + '.') 
```

```py
The averages of each basis expansion, 0 - 4th order = [1\.         0.00536486 0.9458762  0.07336308 2.31077802]. 
```

è®©æˆ‘ä»¬å°†çº¿æ€§å›å½’æ¨¡å‹æ‹Ÿåˆåˆ°å¤šé¡¹å¼åŸºå±•å¼€ã€‚

+   æ³¨æ„ï¼Œæ¨¡å‹å¯¹è¿™ç§å¤æ‚/éçº¿æ€§æ•°æ®æ‹Ÿåˆå¾—ç›¸å½“çµæ´»

```py
lin_poly4 = LinearRegression()                                # instantiate new linear model 
lin_poly4.fit(df_X_ns_poly4.iloc[:,1:], y_ns)                 # train linear model with polynomial expansion, polynomial regression
b1,b2,b3,b4 = np.round(lin_poly4.coef_,3)                     # retrieve the model parameters
b0 = lin_poly4.intercept_

plt.subplot(111)
plt.plot(X_ns_values,lin_poly4.predict(poly4.fit_transform(X_ns_values.reshape(-1, 1))[:,1:]),label='polynomial',color = 'red') 
plt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.title('Polynomial Regression Model, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])
plt.annotate('Polynomial Regression Model',[-2.8,2.6])
plt.annotate(r'    $\beta_4$ :' + str(round(b4,3)),[-2.8,2.1])
plt.annotate(r'    $\beta_3$ :' + str(round(b3,3)),[-2.8,1.7])
plt.annotate(r'    $\beta_2$ :' + str(round(b2,3)),[-2.8,1.3])
plt.annotate(r'    $\beta_1$ :' + str(round(b1,3)),[-2.8,0.9])
plt.annotate(r'    $\beta_0$ :' + str(round(b0,2)),[-2.8,0.5])
plt.annotate(r'$N[\phi] = \beta_4 \times N[z]â´ + \beta_3 \times N[z]Â³ + \beta_2 \times N[z]Â² + \beta_1 \times N[z] + \beta_0$',[-1.0,-2.0])
plt.annotate(r'$N[\phi] = $' + str(b4) + r' $\times N[z]â´ +$ ' + str(b3) + r' $\times N[z]Â³ +$ ' + str(b2) + r' $\times N[z]Â² +$ ' + 
             str(b1) + r' $\times N[z]$ + ' + str(round(b0,2)),[-1.0,-2.5])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/8544dd539b8a7b35cc6999a54d0fecb5.png)

## åŸºäºå„ç±³åŸºå±•å¼€çš„å›å½’

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å„ç±³å¤šé¡¹å¼æ¥å‡å°‘åŸºé¢„æµ‹ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

+   æˆ‘ä»¬å°†é¢„æµ‹ç‰¹å¾ï¼Œæ·±åº¦ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œå› ä¸ºå„ç±³å¤šé¡¹å¼å±•å¼€æ–¹æ³•åœ¨å‡è®¾æ ‡å‡†æ­£æ€æ¦‚ç‡å¯†åº¦å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œåœ¨è´Ÿæ— ç©·åˆ°æ­£æ— ç©·çš„èŒƒå›´å†…å®ç°ç‹¬ç«‹æ€§ã€‚

```py
orders4 = [1,2,3,4]                                           # specify the orders for Hermite basis expansion
X_ns_hermite4 = scipy.special.eval_hermitenorm(orders4,X_ns.values.reshape(-1, 1), out=None) # Hermite polynomials for X 
df_X_ns_hermite4 = pd.DataFrame({'value':X_ns.values,'1st':X_ns_hermite4[:,0],'2nd':X_ns_hermite4[:,1], 
                                     '3rd':X_ns_hermite4[:,2],'4th':X_ns_hermite4[:,3]}) # make a new DataFrame from the vectors
df_X_ns_hermite4.head() 
```

|  | value | 1st | 2nd | 3rd | 4th |
| --- | --- | --- | --- | --- | --- |
| 0 | -2.026808 | -2.026808 | 3.107951 | -2.245605 | -4.772444 |
| 1 | -1.780464 | -1.780464 | 2.170053 | -0.302774 | -5.971082 |
| 2 | -1.534121 | -1.534121 | 1.353526 | 0.991769 | -5.582071 |
| 3 | -1.356312 | -1.356312 | 0.839582 | 1.573889 | -4.653429 |
| 4 | -1.213340 | -1.213340 | 0.472193 | 1.853749 | -3.665806 |

æ³¨æ„ï¼šæˆ‘å·²ç»çœç•¥äº†ä¸æˆ‘ä»¬æ•°æ®é›†å…·æœ‰æ›´é«˜ç›¸å…³æ€§çš„é˜¶æ•°ã€‚

è®©æˆ‘ä»¬æ£€æŸ¥å„ç±³é¢„æµ‹ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æœ‰æ‰€æ”¹è¿›ã€‚

```py
hermite_corr_matrix = df_X_ns_hermite4.iloc[:,1:].corr()      # calculate correlation matrix of Hermite basis expansion of X

plt.subplot(111)
plot_corr(hermite_corr_matrix,'Hermite Polynomial Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/f1b9e3f1eac053a1460fbb16b9d3ab5e.png)

ä¸å¤šé¡¹å¼åŸºç›¸æ¯”ï¼Œæˆå¯¹çº¿æ€§ç›¸å…³æ€§ç›¸å½“ä½ã€‚

è®©æˆ‘ä»¬å¯è§†åŒ–æˆ‘ä»¬å„ç±³åŸºé˜¶æ•°çš„åŒå˜é‡å…³ç³»ã€‚

```py
sns.pairplot(df_X_ns_hermite4.iloc[:,1:],vars=['1st','2nd','3rd','4th'],markers='o', kind='reg',diag_kind='kde')
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/9fd622d54d360b533c8ba76f2bd6a2b6.png)

æˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ‰€æœ‰å„ç±³åŸºå±•å¼€çš„ç®—æœ¯å¹³å‡å€¼ã€‚

```py
print('The means of each basis expansion, 1 - 4th order = ' + str(stats.describe(X_ns_hermite4)[2]) + '.') 
```

```py
The means of each basis expansion, 1 - 4th order = [ 0.00536486 -0.0541238   0.05726848 -0.36447919]. 
```

è®©æˆ‘ä»¬åœ¨æ ‡å‡†åŒ–æ·±åº¦çš„èŒƒå›´å†…å¯è§†åŒ–å„ç±³å¤šé¡¹å¼ã€‚

```py
plt.subplot(111)                                              # plot Hermite polynomials
plt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,0],label='1st',color = 'blue')
plt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,1],label='2nd',color = 'green')
plt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,2],label='3rd',color = 'red')
plt.plot(X_ns_values,scipy.special.eval_hermite(orders4,X_ns_values.reshape(-1, 1))[:,3],label='4th',color = 'orange')
plt.title('Hermite Polynomial Basis Expansion of ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('h[ NS: ' + Xname[0] + ' (' + Xunit[0] + ') ]')
plt.legend(); plt.xlim(-3,3); add_grid()
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/4014172673585ba0353f9f413f88bd94.png)

ç°åœ¨è®©æˆ‘ä»¬æ‹Ÿåˆæˆ‘ä»¬çš„ Hermite åŸºå›å½’æ¨¡å‹ã€‚

```py
lin_herm4 = LinearRegression()                                # instantiate model
lin_herm4.fit(df_X_ns_hermite4.iloc[:,1:], y_ns)              # fit Hermite polynomials 
hb1,hb2,hb3,hb4 = np.round(lin_herm4.coef_,3)                 # retrieve the model parameters
hb0 = lin_herm4.intercept_
plt.subplot(111)                                              # plot data and model
plt.plot(X_ns_values, lin_herm4.predict(scipy.special.eval_hermitenorm(orders4,X_ns_values.reshape(-1, 1), out=None)), 
         label='4th order',color = 'red') 
plt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.title('Hermite Polynomial Regression Model, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])
plt.annotate('Hermite Polynomial Regression Model',[-2.8,2.6])
plt.annotate(r'    $\beta_4$ :' + str(round(hb4,3)),[-2.8,2.1])
plt.annotate(r'    $\beta_3$ :' + str(round(hb3,3)),[-2.8,1.7])
plt.annotate(r'    $\beta_2$ :' + str(round(hb2,3)),[-2.8,1.3])
plt.annotate(r'    $\beta_1$ :' + str(round(hb1,3)),[-2.8,0.9])
plt.annotate(r'    $\beta_0$ :' + str(round(hb0,2)),[-2.8,0.5])
plt.annotate(r'$N[\phi] = \beta_4 \times N[z]â´ + \beta_3 \times N[z]Â³ + \beta_2 \times N[z]Â² + \beta_1 \times N[z] + \beta_0$',[-1.0,-2.0])
plt.annotate(r'$N[\phi] = $' + str(hb4) + r' $\times N[z]â´ +$ ' + str(hb3) + r' $\times N[z]Â³ +$ ' + str(hb2) + r' $\times N[z]Â² +$ ' + 
             str(hb1) + r' $\times N[z]$ + ' + str(round(hb0,2)),[-1.0,-2.5])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/f42cc4ffefcd978188c7348b3d653b8b.png)

ç”±äºæˆ‘ä»¬å±•å¼€çš„åŸºç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§è¾ƒä½ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ¨¡å‹ç³»æ•°å¹¶è§£é‡Šæ¯ä¸ªé¡ºåºçš„ç‹¬ç‰¹é‡è¦æ€§ã€‚

## æ­£äº¤å¤šé¡¹å¼

è®©æˆ‘ä»¬å°è¯•ç”± Dave Moore åœ¨ Python ä¸­é‡æ–°å®ç°çš„æ­£äº¤å¤šé¡¹å¼åŸºå±•å¼€ï¼Œä»–ä» R ä¸­çš„ poly()å‡½æ•°å¼€å§‹ã€‚

+   ä¸‹é¢çš„ fit å’Œ predict å‡½æ•°ç›´æ¥æ¥è‡ª Dave çš„[åšå®¢](http://davmre.github.io/blog/python/2013/12/15/orthogonal_poly)

+   æ³¨æ„åœ¨æ‹Ÿåˆè®­ç»ƒæ•°æ®æ—¶ï¼Œè®¡ç®—äº† norm2 å’Œ alpha æ¨¡å‹å‚æ•°

+   è¿™äº›å‚æ•°å¿…é¡»ä¼ é€’ç»™æ¯ä¸ªåç»­é¢„æµ‹ä»¥ç¡®ä¿ç»“æœä¸€è‡´

```py
# functions taken (without modification) from http://davmre.github.io/blog/python/2013/12/15/orthogonal_poly
# appreciation to Dave Moore for the great blog post on titled 'Orthogonal polynomial regression in Python'
# functions are Dave's reimplementation of poly() from R

def ortho_poly_fit(x, degree = 1):
    n = degree + 1
    x = np.asarray(x).flatten()
    if(degree >= len(np.unique(x))):
            stop("'degree' must be less than number of unique points")
    xbar = np.mean(x)
    x = x - xbar
    X = np.fliplr(np.vander(x, n))
    q,r = np.linalg.qr(X)

    z = np.diag(np.diag(r))
    raw = np.dot(q, z)

    norm2 = np.sum(raw**2, axis=0)
    alpha = (np.sum((raw**2)*np.reshape(x,(-1,1)), axis=0)/norm2 + xbar)[:degree]
    Z = raw / np.sqrt(norm2)
    return Z, norm2, alpha

def ortho_poly_predict(x, alpha, norm2, degree = 1):
    x = np.asarray(x).flatten()
    n = degree + 1
    Z = np.empty((len(x), n))
    Z[:,0] = 1
    if degree > 0:
        Z[:, 1] = x - alpha[0]
    if degree > 1:
        for i in np.arange(1,degree):
             Z[:, i+1] = (x - alpha[i]) * Z[:, i] - (norm2[i] / norm2[i-1]) * Z[:, i-1]
    Z /= np.sqrt(norm2)
    return Z 
```

è®©æˆ‘ä»¬è¯•ä¸€è¯•ï¼Œå¹¶æ‰§è¡Œæˆ‘ä»¬æ ‡å‡†æ­£æ€å˜æ¢æ·±åº¦çš„æ­£äº¤å¤šé¡¹å¼å±•å¼€ã€‚

```py
X_ns_ortho4, norm2, alpha = ortho_poly_fit(X_ns.values.reshape(-1, 1), degree = 4) # orthogonal polynomial expansion
df_X_ns_ortho4 = pd.DataFrame({'value':X_ns.values,'1st':X_ns_ortho4[:,1],'2nd':X_ns_ortho4[:,2],'3rd':X_ns_ortho4[:,3],
                               '4th':X_ns_ortho4[:,4]})       # make a new DataFrame from the vectors
df_X_ns_ortho4.head() 
```

|  | å€¼ | 1st | 2nd | 3rd | 4th |
| --- | --- | --- | --- | --- | --- |
| 0 | -2.026808 | -0.330385 | 0.440404 | -0.460160 | 0.420374 |
| 1 | -1.780464 | -0.290335 | 0.313201 | -0.207862 | 0.021278 |
| 2 | -1.534121 | -0.250285 | 0.202153 | -0.029761 | -0.172968 |
| 3 | -1.356312 | -0.221377 | 0.132038 | 0.058235 | -0.220834 |
| 4 | -1.213340 | -0.198133 | 0.081765 | 0.107183 | -0.219084 |

è®©æˆ‘ä»¬æ£€æŸ¥æ­£äº¤å¤šé¡¹å¼é¢„æµ‹ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æˆ‘å°è±¡æ·±åˆ»ï¼åŸºç¡€ç‰¹å¾é¡ºåºä¹‹é—´çš„ç›¸å…³æ€§éƒ½æ˜¯é›¶ï¼

```py
ortho_corr_matrix = df_X_ns_ortho4.iloc[:,1:].corr()          # calculate the correlation matrix

plt.subplot(111)
plot_corr(ortho_corr_matrix,'Orthogonal Polynomial Expansion Correlation Matrix',1.0,0.1) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/5c444109d15f6a2e24d2be84d8941629.png)

è®©æˆ‘ä»¬å¯è§†åŒ–æˆ‘ä»¬çš„æ­£äº¤å¤šé¡¹å¼åŸºé¡ºåºä¹‹é—´çš„äºŒå…ƒå…³ç³»ã€‚

```py
sns.pairplot(df_X_ns_ortho4.iloc[:,1:],vars=['1st','2nd','3rd','4th'],markers='o',kind='reg',diag_kind='kde') 
```

```py
<seaborn.axisgrid.PairGrid at 0x1ed608d8370> 
```

![å›¾ç‰‡](img/5a2f9488237446e128cc99a668c78ad7.png)

è®©æˆ‘ä»¬å¯è§†åŒ–æ ‡å‡†åŒ–æ·±åº¦èŒƒå›´å†…çš„æ­£äº¤å¤šé¡¹å¼åŸºé¡ºåºã€‚

```py
ortho_poly_ns_values = ortho_poly_predict(X_ns_values.reshape(-1, 1), alpha, norm2, degree = 4)

plt.subplot(111)
plt.plot(X_ns_values, ortho_poly_ns_values[:,0], label='0th', color = 'black')
plt.plot(X_ns_values, ortho_poly_ns_values[:,1], label='1st', color = 'blue')
plt.plot(X_ns_values, ortho_poly_ns_values[:,2], label='2nd', color = 'green')
plt.plot(X_ns_values, ortho_poly_ns_values[:,3], label='3rd', color = 'red')
plt.plot(X_ns_values, ortho_poly_ns_values[:,4], label='4th', color = 'orange')
plt.title('Orthogonal Polynomial Basis Expansion of ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('h[ NS: ' + Xname[0] + ' (' + Xunit[0] + ') ]')
plt.legend(); plt.xlim(-3,3); add_grid()
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/74a68b06994a2941d62a1c4da558780c.png)

æœ€åï¼Œè®©æˆ‘ä»¬æ‹Ÿåˆæˆ‘ä»¬çš„æ­£äº¤å¤šé¡¹å¼åŸºå±•å¼€å›å½’æ¨¡å‹ã€‚

```py
lin_ortho4 = LinearRegression()                               # instantiate model
lin_ortho4.fit(df_X_ns_ortho4.iloc[:,1:], y_ns)               # fit Hermite polynomials 
ob1,ob2,ob3,ob4 = np.round(lin_ortho4.coef_,3)                # retrieve the model parameters
ob0 = lin_ortho4.intercept_

plt.subplot(111)
plt.plot(X_ns_values,lin_ortho4.predict(ortho_poly_ns_values[:,1:]),label='orthogonal polynomial',color = 'red') 
plt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.title('Orthogonal Polynomial Regression Model, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])
plt.annotate('Orthogonal Polynomial Regression Model',[-2.8,2.6])
plt.annotate(r'    $\beta_4$ :' + str(round(ob4,3)),[-2.8,2.1])
plt.annotate(r'    $\beta_3$ :' + str(round(ob3,3)),[-2.8,1.7])
plt.annotate(r'    $\beta_2$ :' + str(round(ob2,3)),[-2.8,1.3])
plt.annotate(r'    $\beta_1$ :' + str(round(ob1,3)),[-2.8,0.9])
plt.annotate(r'    $\beta_0$ :' + str(round(ob0,2)),[-2.8,0.5])
plt.annotate(r'$N[\phi] = \beta_4 \times N[z]â´ + \beta_3 \times N[z]Â³ + \beta_2 \times N[z]Â² + \beta_1 \times N[z] + \beta_0$',[-1.0,-2.0])
plt.annotate(r'$N[\phi] = $' + str(ob4) + r' $\times N[z]â´ +$ ' + str(ob3) + r' $\times N[z]Â³ +$ ' + str(ob2) + r' $\times N[z]Â² +$ ' + 
             str(ob1) + r' $\times N[z]$ + ' + str(round(ob0,2)),[-1.0,-2.5])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.4, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/2a189618f9055efc6488a7eb46c5c41d.png)

## ä½¿ç”¨ Pipeline åœ¨ scikit-learn ä¸­è¿›è¡Œå¤šé¡¹å¼å›å½’

é¦–å…ˆæ‰§è¡ŒåŸºå±•å¼€ç„¶åè®­ç»ƒç»“æœï¼ˆåœ¨åŸºå˜æ¢åï¼‰çš„çº¿æ€§æ¨¡å‹å¯èƒ½çœ‹èµ·æ¥æœ‰ç‚¹å¤æ‚ã€‚

+   ä¸€ä¸ªè§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨ scikit-learn ä¸­çš„ Pipeline å¯¹è±¡ã€‚ä»¥ä¸‹æ˜¯å…³äº Pipeline çš„ä¸€äº›äº®ç‚¹ã€‚

æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹å¯èƒ½å¾ˆå¤æ‚ï¼Œæœ‰å„ç§æ­¥éª¤ï¼š

+   æ•°æ®å‡†å¤‡ï¼Œç‰¹å¾å·¥ç¨‹è½¬æ¢

+   æ¨¡å‹å‚æ•°æ‹Ÿåˆ

+   æ¨¡å‹è¶…å‚æ•°è°ƒæ•´

+   æ¨¡å‹æ–¹æ³•é€‰æ‹©

+   åœ¨å¤§é‡è¶…å‚æ•°ç»„åˆä¸­è¿›è¡Œæœç´¢

+   è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹è¿è¡Œ

ç®¡é“æ˜¯ scikit-learn ä¸­çš„ä¸€ä¸ªç±»ï¼Œå…è®¸å°è£…ä¸€ç³»åˆ—æ•°æ®å‡†å¤‡å’Œå»ºæ¨¡æ­¥éª¤

+   ç„¶åæˆ‘ä»¬å¯ä»¥å°†ç®¡é“è§†ä¸ºæˆ‘ä»¬ç®€åŒ–å·¥ä½œæµç¨‹ä¸­çš„ä¸€ä¸ªå¯¹è±¡

ç®¡é“è¯¾ç¨‹ä½¿æˆ‘ä»¬èƒ½å¤Ÿï¼š

+   æé«˜ä»£ç å¯è¯»æ€§å¹¶ä¿æŒä¸€åˆ‡æ¸…æ™°

+   é¿å…å¸¸è§çš„æµç¨‹é—®é¢˜ï¼Œå¦‚æ•°æ®æ³„éœ²ã€æµ‹è¯•æ•°æ®å½±å“æ¨¡å‹å‚æ•°è®­ç»ƒ

+   æ¦‚è¿°å¸¸è§çš„æœºå™¨å­¦ä¹ å»ºæ¨¡å¹¶ä¸“æ³¨äºæ„å»ºæœ€ä½³æ¨¡å‹

åŸºæœ¬å“²å­¦æ˜¯å°†æœºå™¨å­¦ä¹ è§†ä¸ºä¸€ç§ç»„åˆæœç´¢ï¼Œä»¥æ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼ˆAutoMLï¼‰

```py
order=4                                                       # set the polynomial order

polyreg_pipe=make_pipeline(PolynomialFeatures(order),LinearRegression()) # make the modeling pipeline
polyreg_pipe.fit(X_ns.values.reshape(-1, 1), y_ns)            # fit the model to the data
y_hat = polyreg_pipe.predict(X_ns_values.reshape(-1, 1))      # predict with the modeling pipeline
poly_reg_model = polyreg_pipe.named_steps['linearregression'] # retrieve the model from the pipeline
pb0a,pb1,pb2,pb3,pb4 = np.round(poly_reg_model.coef_,3)       # retrieve the model parameters
pb0b = poly_reg_model.intercept_
pb0 = pb0a + pb0b

plt.subplot(111)                                              # plot the data and model
plt.plot(X_ns_values,y_hat, label='4th order',color = 'red') 
plt.scatter(X_ns,y_ns,marker='o',label='data',color = 'darkorange',alpha = 0.8,edgecolor = 'black')
plt.title(str(order) + r'$^{th}$ Polynomial Regression Model with Pipelines, Regression of NS ' + yname[0] + ' on NS ' + Xname[0])
plt.xlabel('NS: ' + Xname[0] + ' (' + Xunit[0] + ')')
plt.ylabel('NS: ' + yname[0] + ' (' + yunit[0] + ')')
plt.legend(); add_grid(); plt.xlim([-3.0,3.0]); plt.ylim([ymin,ymax])
plt.annotate('Orthogonal Polynomial Regression Model',[-2.8,2.6])
plt.annotate(r'    $\beta_4$ :' + str(round(pb4,3)),[-2.8,2.1])
plt.annotate(r'    $\beta_3$ :' + str(round(pb3,3)),[-2.8,1.7])
plt.annotate(r'    $\beta_2$ :' + str(round(pb2,3)),[-2.8,1.3])
plt.annotate(r'    $\beta_1$ :' + str(round(pb1,3)),[-2.8,0.9])
plt.annotate(r'    $\beta_0$ :' + str(round(pb0,2)),[-2.8,0.5])
plt.annotate(r'$N[\phi] = \beta_4 \times N[z]â´ + \beta_3 \times N[z]Â³ + \beta_2 \times N[z]Â² + \beta_1 \times N[z] + \beta_0$',[-1.0,-2.0])
plt.annotate(r'$N[\phi] = $' + str(pb4) + r' $\times N[z]â´ +$ ' + str(pb3) + r' $\times N[z]Â³ +$ ' + str(pb2) + r' $\times N[z]Â² +$ ' + 
             str(pb1) + r' $\times N[z]$ + ' + str(round(pb0,2)),[-1.0,-2.5])
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/e0ba7ea47dd6042f5976cb230740cb93.png)

## è¯„è®º

è¿™æ˜¯å¯¹å¤šé¡¹å¼å›å½’çš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„ YouTube è®²åº§é“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![å›¾ç‰‡](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ˜¯[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œåœ¨[å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡](https://www.utexas.edu/)è¿›è¡Œåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°è´¨ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ çš„ç ”ç©¶å’Œæ•™å­¦ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜

+   [è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº†è¶…è¿‡ 70 ç¯‡[åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[åœ°è´¨ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[Python åº”ç”¨åœ°è´¨ç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[Python åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå®è·µæŒ‡å—ä¸ä»£ç ](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è¯¾ç¨‹éƒ½å¯åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å« 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†è®°å½•å·¥ä½œæµç¨‹ï¼Œè¿™äº›å­˜å‚¨åº“å¯åœ¨ä»–çš„[GitHub è´¦æˆ·](https://github.com/GeostatsGuy)ä¸Šæ‰¾åˆ°ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›å¸¸é’å†…å®¹ã€‚äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™ä»½å†…å®¹å¯¹é‚£äº›æƒ³è¦äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æä»¥åŠæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚ä¸ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œï¼Œæ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»æˆ‘ã€‚

æˆ‘æ€»æ˜¯ä¹äºè®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼Œåšå£«ï¼Œæ³¨å†Œå·¥ç¨‹å¸ˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢æ•™æˆ

æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹ä½ç½®è·å–ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°çƒç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°çƒç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
