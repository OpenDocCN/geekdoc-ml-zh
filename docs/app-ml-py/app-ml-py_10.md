# ç‰¹å¾æ’åº

> åŸæ–‡ï¼š[`geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_feature_ranking.html`](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_feature_ranking.html)

Michael J. Pyrczï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

ç”µå­ä¹¦â€œPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„å®æˆ˜æŒ‡å—â€çš„ç« èŠ‚ã€‚

è¯·å¼•ç”¨æ­¤ç”µå­ä¹¦ä¸ºï¼š

Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with Code* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138 ![DOI](https://doi.org/10.5281/zenodo.15169138)

æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠå…¶ä»–å·¥ä½œæµç¨‹éƒ½å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š

è¯·å°† MachineLearningDemos GitHub ä»“åº“å¼•ç”¨å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration Workflows Repository* (0.0.3) [è½¯ä»¶]. Zenodo. DOI: 10.5281/zenodo.13835312\. GitHub ä»“åº“ï¼š[GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos) ![DOI](https://zenodo.org/doi/10.5281/zenodo.13835312)

ä½œè€…ï¼šMichael J. Pyrcz

Â© ç‰ˆæƒæ‰€æœ‰ 2024ã€‚

æœ¬ç« æ˜¯å…³äº**ç‰¹å¾æ’åº**çš„æ•™ç¨‹/æ¼”ç¤ºã€‚

**YouTube è®²åº§**ï¼šæŸ¥çœ‹æˆ‘å…³äºä»¥ä¸‹å†…å®¹çš„è®²åº§ï¼š

+   [æœºå™¨å­¦ä¹ ç®€ä»‹](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)

+   [ç»´åº¦è¯…å’’ã€é™ç»´ã€ä¸»æˆåˆ†åˆ†æ](https://youtu.be/embks9p4pb8?si=B2HXm_i0oMSWkBhN)

+   [å¤šç»´å°ºåº¦åˆ†æå’ŒéšæœºæŠ•å½±](https://youtu.be/Yt0o8ukIOKU?si=_ri1NPwKVdhYzgO3)

+   [ç‰¹å¾è½¬æ¢](https://youtu.be/6QJjZoWknEI?si=p6vp811xWAmzWY3r)

+   [ç‰¹å¾é€‰æ‹©](https://youtu.be/5Q0gemu-h3Q?si=ATG-ue0i2qcc-IVx)

è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘ YouTube ä¸Šçš„[æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«æœ‰è‰¯å¥½æ–‡æ¡£è®°å½•çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºè·å–ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœä½ æƒ³çŸ¥é“æˆ‘çš„åŠ¨æœºï¼Œè¯·æŸ¥çœ‹[Michael çš„æ•…äº‹](https://michaelpyrcz.com/my-story)ã€‚

## ç‰¹å¾æ’åºçš„åŠ¨æœº

é€šå¸¸æœ‰å¾ˆå¤šé¢„æµ‹ç‰¹å¾ï¼ˆè¾“å…¥å˜é‡ï¼‰å¯ä¾›æˆ‘ä»¬ç”¨äºæ„å»ºé¢„æµ‹æ¨¡å‹ã€‚

+   æœ‰å……åˆ†çš„ç†ç”±è¦æœ‰æ‰€é€‰æ‹©ï¼Œå°†æ¯ä¸ªå¯èƒ½çš„ç‰¹å¾éƒ½åŠ å…¥è¿›æ¥å¹¶ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼

é€šå¸¸ï¼Œå¯¹äºæœ€ä½³çš„é¢„æµ‹æ¨¡å‹ï¼Œä»”ç»†é€‰æ‹©æä¾›æœ€å¤šä¿¡æ¯çš„å°‘æ•°ç‰¹å¾æ˜¯æœ€ä½³å®è·µã€‚

åŸå› å¦‚ä¸‹ï¼š

+   **é”™è¯¯** - æ›´å¤šçš„é¢„æµ‹ç‰¹å¾å¯¼è‡´æ›´å¤æ‚çš„æµç¨‹ï¼Œéœ€è¦æ›´å¤šä¸“ä¸šæ—¶é—´ï¼Œå¹¶ä¸”åœ¨æµç¨‹ä¸­å‡ºé”™çš„æœºä¼šå¢åŠ 

+   **éš¾ä»¥å¯è§†åŒ–** - é«˜ç»´æ¨¡å‹ï¼Œå³æ›´å¤šé¢„æµ‹ç‰¹å¾çš„æ¨¡å‹ï¼Œæ›´éš¾ä»¥å¯è§†åŒ–

+   **æ¨¡å‹æ£€æŸ¥** - æ›´å¤æ‚çš„æ¨¡å‹å¯èƒ½æ›´éš¾ä»¥è°ƒæŸ¥ã€è§£é‡Šå’Œè¿›è¡Œè´¨é‡æ§åˆ¶

+   **é¢„æµ‹ç‰¹å¾å†—ä½™** - æ›´æœ‰å¯èƒ½å­˜åœ¨å†—ä½™çš„é¢„æµ‹ç‰¹å¾ã€‚åŒ…å«é«˜åº¦å†—ä½™å’Œå…±çº¿æ€§æˆ–å¤šå…±çº¿æ€§çš„ç‰¹å¾ä¼šå¢åŠ æ¨¡å‹æ–¹å·®ï¼Œå¢åŠ æ¨¡å‹ä¸ç¨³å®šæ€§ï¼Œå¹¶é™ä½æµ‹è¯•é¢„æµ‹çš„å‡†ç¡®æ€§

+   **è®¡ç®—æ—¶é—´** - é€šå¸¸ï¼Œæ›´å¤šçš„é¢„æµ‹ç‰¹å¾ä¼šå¢åŠ è®­ç»ƒæ¨¡å‹æ‰€éœ€çš„è®¡ç®—æ—¶é—´å’Œè®¡ç®—å­˜å‚¨ï¼Œå³æ¨¡å‹å¯èƒ½ä¸å¤ªç´§å‡‘ä¸”ä¸ä¾¿äºæºå¸¦

+   **æ¨¡å‹è¿‡æ‹Ÿåˆ** - éšç€ç‰¹å¾æ•°é‡çš„å¢åŠ ï¼Œè¿‡æ‹Ÿåˆçš„é£é™©å¢åŠ ï¼Œå› ä¸ºæ¨¡å‹å¤æ‚æ€§å¢åŠ 

+   **æ¨¡å‹å¤–æ¨** - è®¸å¤šé¢„æµ‹ç‰¹å¾å¯¼è‡´é«˜ç»´æ¨¡å‹ç©ºé—´æ•°æ®è¦†ç›–åº¦ä½ï¼Œæ¨¡å‹å¤–æ¨å¯èƒ½ä¸å‡†ç¡®çš„å¯èƒ½æ€§æ›´é«˜

è®¸å¤šé¢„æµ‹ç‰¹å¾çš„ä¸»è¦é—®é¢˜æ˜¯ç»´åº¦è¯…å’’ã€‚è®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹è¿™ä¸ªè¯…å’’ï¼

## ç»´åº¦è¯…å’’

1.  **æ•°æ®å’Œæ¨¡å‹å¯è§†åŒ–** - æˆ‘ä»¬æ— æ³•å¯è§†åŒ–è¶…è¿‡ä¸‰ç»´ï¼Œå³æ— æ³•è®¿é—®æ•°æ®æ‹Ÿåˆæ¨¡å‹ï¼Œè¯„ä¼°å†…æ’ä¸å¤–æ¨ã€‚

+   è€ƒè™‘ä¸€ä¸ª 5D ç¤ºä¾‹ï¼Œå¦‚å›¾æ‰€ç¤ºä¸ºçŸ©é˜µæ•£ç‚¹å›¾ï¼Œå³ä½¿åœ¨è¿™ç§æƒ…å†µä¸­ï¼Œæ¯ä¸ªå›¾ä¹Ÿæœ‰æç«¯çš„è¾¹é™…åŒ–åˆ°äºŒç»´ï¼Œ

![](img/ecf50f66114aec17ea35fde1342d66c4.png)

ç¤ºä¾‹ï¼š5D æ•°æ®ä½œä¸ºçŸ©é˜µæ•£ç‚¹å›¾ã€‚

1.  **é‡‡æ ·** - è¶³å¤Ÿçš„æ ·æœ¬æ•°é‡ä»¥æ¨æ–­è¯¸å¦‚è”åˆæ¦‚ç‡$P(x_1,\ldots,x_m)$ä¹‹ç±»çš„ç»Ÿè®¡ä¿¡æ¯ã€‚

+   å›å¿†ä¸€ä¸‹ç›´æ–¹å›¾æˆ–å½’ä¸€åŒ–ç›´æ–¹å›¾çš„è®¡ç®—ï¼šæˆ‘ä»¬å»ºç«‹ç®±å­å¹¶è®¡ç®—æ¯ä¸ªç®±å­ä¸­çš„é¢‘ç‡æˆ–æ¦‚ç‡ã€‚

+   æˆ‘ä»¬éœ€è¦æ¯ä¸ªç®±å­çš„åä¹‰æ•°æ®æ ·æœ¬æ•°ï¼Œå› æ­¤åœ¨ä¸€ç»´ä¸­æˆ‘ä»¬éœ€è¦$ğ‘›=ğ‘›_{ğ‘ /ğ‘ğ‘–ğ‘›} \cdot ğ‘›_{ğ‘ğ‘–ğ‘›ğ‘ }$ä¸ªæ ·æœ¬

+   ä½†åœ¨ mD ä¸­ï¼Œæˆ‘ä»¬éœ€è¦$n$ä¸ªæ ·æœ¬æ¥è®¡ç®—ç¦»æ•£åŒ–è”åˆæ¦‚ç‡ï¼Œ

$$ ğ‘›=ğ‘›_{ğ‘ /ğ‘ğ‘–ğ‘›} \cdot ğ‘›_{ğ‘ğ‘–ğ‘›ğ‘ }^m $$

+   ä¾‹å¦‚ï¼Œæ¯ä¸ªç®±å­ 10 ä¸ªæ ·æœ¬ï¼Œ35 ä¸ªç®±å­éœ€è¦ 2D ä¸­çš„ 12,250 ä¸ªæ ·æœ¬ï¼Œ3D ä¸­çš„ 428,750 ä¸ªæ ·æœ¬

![](img/bc8823819263f4497ef6baab93a9ee38.png)

ç¤ºä¾‹ï¼šå…·æœ‰æ¯ä¸ªç‰¹å¾ 35 ä¸ªç®±å­çš„ 2D æ•°æ®ã€‚

1.  **æ ·æœ¬è¦†ç›–** - æ ·æœ¬å€¼èŒƒå›´è¦†ç›–é¢„æµ‹ç‰¹å¾ç©ºé—´ã€‚

+   æ ·æœ¬ç©ºé—´ä¸­å¯èƒ½è§£ç©ºé—´çš„åˆ†æ•°ï¼Œå¯¹äº 1 ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬å‡è®¾ 80%çš„è¦†ç›–ç‡

+   è®°ä½ï¼Œæˆ‘ä»¬é€šå¸¸ç›´æ¥é‡‡æ ·åªæœ‰åœ°ä¸‹ä½“ç§¯çš„$\frac{1}{10â·}$ã€‚

+   æ˜¯çš„ï¼Œè¦†ç›–çš„æ¦‚å¿µæ˜¯ä¸»è§‚çš„ï¼Œéœ€è¦è¦†ç›–å¤šå°‘æ•°æ®ï¼Ÿå…³äºç¼ºå£æ€ä¹ˆåŠç­‰é—®é¢˜ã€‚

![](img/d8058511a88a482ed34b0cbd9eb34fec.png)

æ¯ä¸ªç‰¹å¾æœ‰ 35 ä¸ªæ¡¶çš„ 2D æ•°æ®çš„ç¤ºä¾‹ã€‚

+   å¦‚æœæœ‰ä¸¤ä¸ªç‰¹å¾çš„ 80%è¦†ç›–ç‡ï¼Œåˆ™äºŒç»´è¦†ç›–ç‡æ˜¯ 64%ã€‚

![](img/8d96453b3f6c2a92a160fe4329a13d4a.png)

æ¯ä¸ªç‰¹å¾æœ‰ 35 ä¸ªæ¡¶çš„ 2D æ•°æ®çš„ç¤ºä¾‹ã€‚

+   è¦†ç›–ç‡æ˜¯ï¼Œ

$$ c = c_1^m $$

1.  **æ‰­æ›²ç©ºé—´** - é«˜ç»´ç©ºé—´è¢«æ‰­æ›²ã€‚

+   å–è¶…ç«‹æ–¹ä½“å†…å†…æ¥è¶…çƒä½“çš„ä½“ç§¯æ¯”ï¼Œ

$$ \frac{\pi^{\frac{m}{2}}}{m 2^{m-1} \Gamma\left(\frac{m}{2}\right)} \to 0 \quad \text{as} \quad m \to \infty $$

+   å›å¿†ï¼Œ$\Gamma(ğ‘›)=(ğ‘›âˆ’1)!$.

+   é«˜ç»´ç©ºé—´å…¨æ˜¯è§’è€Œæ²¡æœ‰ä¸­é—´éƒ¨åˆ†ï¼Œè€Œä¸”å¤§å¤šæ•°é«˜ç»´ç©ºé—´ç¦»ä¸­é—´éƒ¨åˆ†å¾ˆè¿œï¼ˆå…¨æ˜¯è§’ï¼ï¼‰ã€‚

+   å› æ­¤ï¼Œé«˜ç»´ç©ºé—´ä¸­çš„è·ç¦»å¤±å»äº†æ•æ„Ÿæ€§ï¼Œå³å¯¹äºç©ºé—´ä¸­çš„ä»»ä½•éšæœºç‚¹ï¼Œé¢„æœŸçš„æˆå¯¹è·ç¦»éƒ½å˜å¾—ç›¸åŒï¼Œ

$$ \lim_{m \to \infty} \left( \mathbb{E}\left[\text{dist}_{\text{max}}(m) - \text{dist}_{\text{min}}(m)\right] \right) \to 0 $$

+   è¶…é«˜ç»´ç©ºé—´ä¸­éšæœºç‚¹æˆå¯¹è·ç¦»èŒƒå›´çš„æœŸæœ›æé™è¶‹äºé›¶ã€‚å¦‚æœè·ç¦»å‡ ä¹éƒ½ç›¸åŒï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»å°±ä¸å†æœ‰æ„ä¹‰äº†ï¼

![](img/8c8d512cca4eb330150d1ba298831543.png)

è¶…ç«‹æ–¹ä½“å†…è¶…çƒä½“çš„ä½“ç§¯æ¯”ã€‚

+   è¿™é‡Œæ˜¯å„ç§ç»´åº¦çš„æ‰­æ›²ä¸¥é‡ç¨‹åº¦ï¼Œ

| m | nD / 2D |
| --- | --- |
| 2 | 1.0 |
| 5 | 0.28 |
| 10 | 0.003 |
| 20 | 0.00000003 |

1.  **å¤šé‡å…±çº¿æ€§** - é«˜ç»´æ•°æ®é›†æ›´æœ‰å¯èƒ½å‡ºç°å…±çº¿æ€§æˆ–å¤šé‡å…±çº¿æ€§ã€‚

+   ç”±å…¶ä»–ç‰¹å¾çº¿æ€§æè¿°çš„ç‰¹å¾å¯¼è‡´æ¨¡å‹æ–¹å·®é«˜ã€‚

## ä»€ä¹ˆæ˜¯ç‰¹å¾æ’åï¼Ÿ

ç‰¹å¾æ’åæ˜¯ä¸€ç»„åº¦é‡ï¼Œå®ƒæ ¹æ®åŒ…å«åœ¨æ¨ç†ä¸­çš„ä¿¡æ¯å’Œé¢„æµ‹å“åº”ç‰¹å¾çš„é‡è¦æ€§ï¼Œä¸ºæ¯ä¸ªé¢„æµ‹ç‰¹å¾åˆ†é…ç›¸å¯¹é‡è¦æ€§æˆ–ä»·å€¼ã€‚æœ‰å„ç§å„æ ·çš„å¯èƒ½æ–¹æ³•æ¥å®Œæˆè¿™é¡¹ä»»åŠ¡ã€‚æˆ‘çš„å»ºè®®æ˜¯é‡‡ç”¨**â€œå®½æ•°ç»„â€**æ–¹æ³•ï¼Œç»“åˆå¤šç§åˆ†æå’Œåº¦é‡ï¼ŒåŒæ—¶ç†è§£æ¯ç§æ–¹æ³•çš„å‡è®¾å’Œé™åˆ¶ã€‚

è¿™é‡Œæ˜¯æˆ‘ä»¬è€ƒè™‘çš„ç‰¹å¾æ’åçš„ä¸€èˆ¬ç±»å‹ã€‚

1.  æ•°æ®åˆ†å¸ƒå’Œæ•£ç‚¹å›¾çš„è§†è§‰æ£€æŸ¥

1.  ç»Ÿè®¡æ‘˜è¦

1.  åŸºäºæ¨¡å‹

1.  é€’å½’ç‰¹å¾æ¶ˆé™¤

æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸åº”å¿½è§†ä¸“å®¶çŸ¥è¯†ã€‚å¦‚æœå…³äºç‰©ç†è¿‡ç¨‹ã€å› æœå…³ç³»ã€é¢„æµ‹ç‰¹å¾çš„å¯é æ€§å’Œå¯ç”¨æ€§æœ‰é¢å¤–çš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯åº”æ•´åˆåˆ°åˆ†é…ç‰¹å¾æ’åä¸­ã€‚

## åŠ è½½æ‰€éœ€çš„åº“

ä»¥ä¸‹ä»£ç åŠ è½½æ‰€éœ€çš„åº“ã€‚

```py
import geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper
import geostatspy.geostats as geostats                        # GSLIB methods convert to Python 
import geostatspy
print('GeostatsPy version: ' + str(geostatspy.__version__)) 
```

```py
GeostatsPy version: 0.0.71 
```

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
ignore_warnings = True                                        # ignore warnings?
import numpy as np                                            # ndarrays for gridded data
import pandas as pd                                           # DataFrames for tabular data
from sklearn import preprocessing                             # remove encoding error
from sklearn.feature_selection import RFE                     # for recursive feature selection
from sklearn.feature_selection import mutual_info_regression  # mutual information
from sklearn.linear_model import LinearRegression             # linear regression model
from sklearn.ensemble import RandomForestRegressor            # model-based feature importance
from sklearn import metrics                                   # measures to check our models
from statsmodels.stats.outliers_influence import variance_inflation_factor # variance inflation factor
import os                                                     # set working directory, run executables
import math                                                   # basic math operations
import random                                                 # for random numbers
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import matplotlib.ticker as mtick                             # control tick label formatting
import seaborn as sns                                         # for matrix scatter plots
from scipy import stats                                       # summary statistics
import numpy.linalg as linalg                                 # for linear algebra
import scipy.spatial as sp                                    # for fast nearest neighbor search
import scipy.signal as signal                                 # kernel for moving window calculation
from numba import jit                                         # for numerical speed up
from statsmodels.stats.weightstats import DescrStatsW
plt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements
if ignore_warnings == True:                                   
    import warnings
    warnings.filterwarnings('ignore')
cmap = plt.cm.inferno                                         # color map 
```

å¯¹äºç‰¹å¾æ’åçš„ Shapley å€¼æ–¹æ³•ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé¢å¤–çš„åŒ…ä»¥åŠå¯åŠ¨ javascript æ”¯æŒã€‚

+   è¿è¡Œæ­¤ä»£ç å—åï¼Œä½ åº”è¯¥çœ‹åˆ°ä¸€ä¸ªå¸¦æœ‰æ–‡æœ¬â€˜jsâ€™çš„å…­è¾¹å½¢ï¼Œä»¥è¡¨ç¤º javascript å·²å‡†å¤‡å¥½ã€‚

```py
import sys
#!{sys.executable} -m pip install shap
import shap
shap.initjs() 
```

![](img/70b822753245ba6bb888425de8eb62b5.png)

å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½é¦–å…ˆéœ€è¦å®‰è£…è¿™äº›åŒ…ä¸­çš„æŸäº›åŒ…ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ï¼Œç„¶åè¾“å…¥ â€˜python -m pip install [package-name]â€™ æ¥å®Œæˆã€‚æ›´å¤šå¸®åŠ©å¯ä»¥åœ¨ç›¸åº”åŒ…çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ã€‚

## è®¾è®¡è‡ªå®šä¹‰é¢œè‰²å›¾

é€šè¿‡å±è”½éæ˜¾è‘—å€¼æ¥è€ƒè™‘æ˜¾è‘—æ€§

+   ç›®å‰ä»…ç”¨äºæ¼”ç¤ºï¼Œå¯ä»¥æ ¹æ®ç»“æœç½®ä¿¡åº¦å’Œä¸ç¡®å®šæ€§æ›´æ–°æ¯ä¸ªå›¾è¡¨

```py
my_colormap = plt.cm.get_cmap('RdBu_r', 256)                  # make a custom colormap
newcolors = my_colormap(np.linspace(0, 1, 256))               # define colormap space
white = np.array([250/256, 250/256, 250/256, 1])              # define white color (4 channel)
#newcolors[26:230, :] = white                                 # mask all correlations less than abs(0.8)
#newcolors[56:200, :] = white                                 # mask all correlations less than abs(0.6)
newcolors[76:180, :] = white                                  # mask all correlations less than abs(0.4)
signif = ListedColormap(newcolors)                            # assign as listed colormap

my_colormap = plt.cm.get_cmap('inferno', 256)                 # make a custom colormap
newcolors = my_colormap(np.linspace(0, 1, 256))               # define colormap space
white = np.array([250/256, 250/256, 250/256, 1])              # define white color (4 channel)
#newcolors[26:230, :] = white                                 # mask all correlations less than abs(0.8)
newcolors[0:12, :] = white                                    # mask all correlations less than abs(0.6)
#newcolors[86:170, :] = white                                 # mask all correlations less than abs(0.4)
sign1 = ListedColormap(newcolors)                             # assign as listed colormap 
```

## å£°æ˜å‡½æ•°

è¿™é‡Œæœ‰ä¸€äº›å‡½æ•°å¯ä»¥å¸®åŠ©è®¡ç®—ç”¨äºæ’åå’Œå…¶ä»–å›¾è¡¨çš„æŒ‡æ ‡ï¼š

+   **plot_corr** - ç»˜åˆ¶ç›¸å…³çŸ©é˜µ

+   **partial_corr** - éƒ¨åˆ†ç›¸å…³ç³»æ•°

+   **semipar_corr** - åŠéƒ¨åˆ†ç›¸å…³ç³»æ•°

+   **mutual_matrix** - äº’ä¿¡æ¯çŸ©é˜µï¼Œæ‰€æœ‰æˆå¯¹äº’ä¿¡æ¯çš„çŸ©é˜µ

+   **mutual_information_objective** - æˆ‘ä¿®æ”¹çš„ MRMR æŸå¤±å‡½æ•°ç‰ˆæœ¬ï¼ˆIxy - å¹³å‡(Ixx)ï¼‰ç”¨äºç‰¹å¾æ’åï¼ˆä½¿ç”¨æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾ï¼‰

+   **delta_mutual_information_quotient** - é€šè¿‡æ·»åŠ å’Œåˆ é™¤ç‰¹å®šç‰¹å¾æ¥æ”¹å˜äº’ä¿¡æ¯å•†çš„å˜åŒ–ï¼ˆä½¿ç”¨æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾è¿›è¡Œæ¯”è¾ƒï¼‰

+   **weighted_avg_and_std** - å¹³å‡å€¼å’Œæ ‡å‡†å·®è€ƒè™‘æ•°æ®æƒé‡

+   **weighted_percentile** - è€ƒè™‘æ•°æ®æƒé‡çš„ç™¾åˆ†ä½æ•°

+   **histogram_bounds** - å‘ç›´æ–¹å›¾æ·»åŠ ç½®ä¿¡åŒºé—´

+   **add_grid** - æ·»åŠ ä¸»è¦å’Œæ¬¡è¦ç½‘æ ¼çº¿ä»¥æ”¹å–„ç»˜å›¾å¯è§£é‡Šæ€§çš„ä¾¿åˆ©å‡½æ•°

è¿™é‡Œæ˜¯è¿™äº›å‡½æ•°ï¼š

```py
def feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot
    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal)
    plt.plot(pred,metric,color='black',zorder=20)
    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)
    plt.plot([-0.5,mpred-0.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  
    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)
    plt.ylim(mmin,mmax); plt.xlim([-0.5,mpred-0.5]); add_grid();
    plt.xticks(rotation=270.0)
    return

def plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix 
    my_colormap = plt.cm.get_cmap('RdBu_r', 256)          
    newcolors = my_colormap(np.linspace(0, 1, 256))
    white = np.array([256/256, 256/256, 256/256, 1])
    white_low = int(128 - mask*128); white_high = int(128+mask*128)
    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)
    newcmp = ListedColormap(newcolors)
    m = corr_matrix.shape[0]
    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)
    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()
    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()
    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
    plt.colorbar(im, orientation = 'vertical')
    plt.title(title)
    for i in range(0,m):
        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')
        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')
    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])
    plt.xticks(rotation=270.0)

def partial_corr(C):                                          # partial correlation by Fabian Pedregosa-Izquierdo, f@bianp.net
    C = np.asarray(C)
    p = C.shape[1]
    P_corr = np.zeros((p, p), dtype=float)
    for i in range(p):
        P_corr[i, i] = 1
        for j in range(i+1, p):
            idx = np.ones(p, dtype=bool)
            idx[i] = False
            idx[j] = False
            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]
            beta_j = linalg.lstsq(C[:, idx], C[:, i])[0]
            res_j = C[:, j] - C[:, idx].dot( beta_i)
            res_i = C[:, i] - C[:, idx].dot(beta_j)
            corr = stats.pearsonr(res_i, res_j)[0]
            P_corr[i, j] = corr
            P_corr[j, i] = corr
    return P_corr

def semipartial_corr(C):                                      # Michael Pyrcz modified the function above by Fabian Pedregosa-Izquierdo, f@bianp.net for semipartial correlation

    C = np.asarray(C)
    p = C.shape[1]
    P_corr = np.zeros((p, p), dtype=float)
    for i in range(p):
        P_corr[i, i] = 1
        for j in range(i+1, p):
            idx = np.ones(p, dtype=bool)
            idx[i] = False
            idx[j] = False
            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]
            res_j = C[:, j] - C[:, idx].dot( beta_i)
            res_i = C[:, i] 
            corr = stats.pearsonr(res_i, res_j)[0]
            P_corr[i, j] = corr
            P_corr[j, i] = corr
    return P_corr

def mutual_matrix(df,features):                               # calculate mutual information matrix
    mutual = np.zeros([len(features),len(features)])
    for i, ifeature in enumerate(features):
        for j, jfeature in enumerate(features):
            if i != j:
                mutual[i,j] = mutual_info_regression(df.iloc[:,i].values.reshape(-1, 1),np.ravel(df.iloc[:,j].values))[0]
    mutual /= np.max(mutual) 
    for i, ifeature in enumerate(features):
        mutual[i,i] = 1.0
    return mutual

def mutual_information_objective(x,y):                        # modified from MRMR loss function, Ixy - average(Ixx)
    mutual_information_quotient = []
    for i, icol in enumerate(x.columns):
        Vx = mutual_info_regression(x.iloc[:,i].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1)))
        Ixx_mat = []
        for m, mcol in enumerate(x.columns):
            if i != m:
                Ixx_mat.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,i].values.reshape(-1, 1))))
        Wx = np.average(Ixx_mat)
        mutual_information_quotient.append(Vx/Wx)
    mutual_information_quotient  = np.asarray(mutual_information_quotient).reshape(-1)
    return mutual_information_quotient

def delta_mutual_information_quotient(x,y):                   # standard mutual information quotient
    delta_mutual_information_quotient = []               

    Ixy = []
    for m, mcol in enumerate(x.columns):
        Ixy.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1))))
    Vs = np.average(Ixy)
    Ixx = []
    for m, mcol in enumerate(x.columns):
        for n, ncol in enumerate(x.columns):
            Ixx.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,n].values.reshape(-1, 1))))
    Ws = np.average(Ixx) 

    for i, icol in enumerate(x.columns):          
        Ixy_s = []                                          
        for m, mcol in enumerate(x.columns):
            if m != i:
                Ixy_s.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1))))
        Vs_s = np.average(Ixy_s)
        Ixx_s = []
        for m, mcol in enumerate(x.columns):
            if m != i:
                for n, ncol in enumerate(x.columns):
                    if n != i:
                        Ixx_s.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,n].values.reshape(-1, 1))))                  
        Ws_s = np.average(Ixx_s)
        delta_mutual_information_quotient.append((Vs/Ws)-(Vs_s/Ws_s))

    delta_mutual_information_quotient  = np.asarray(delta_mutual_information_quotient).reshape(-1)  
    return delta_mutual_information_quotient

def weighted_avg_and_std(values, weights):                    # calculate weighted statistics (Eric O Lebigot, stack overflow)
    average = np.average(values, weights=weights)
    variance = np.average((values-average)**2, weights=weights)
    return (average, math.sqrt(variance))

def weighted_percentile(data, weights, perc):                 # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) 
    ix = np.argsort(data)
    data = data[ix] 
    weights = weights[ix] 
    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) 
    return np.interp(perc, cdf, data)

def histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram 
    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)
    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')
    plt.plot([avg,avg],[0.0,45],color = color)
    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')

def add_grid():                                               # add major and minor gridlines
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("d:/PGE383")                                   # set the working directory 
```

æ‚¨å°†ä¸å¾—ä¸æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ä»¥åŒ¹é…æ‚¨è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”åœ¨ Mac ä¸Šæ ¼å¼ä¸åŒï¼ˆä¾‹å¦‚ï¼šâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½è¡¨æ ¼æ•°æ®

è¿™é‡Œæ˜¯åŠ è½½æˆ‘ä»¬çš„é€—å·åˆ†éš”æ•°æ®æ–‡ä»¶åˆ° Pandas DataFrame å¯¹è±¡çš„å‘½ä»¤ã€‚

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›† â€˜unconv_MV.csvâ€™ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…å«æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   äº•å¹³å‡å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   å‰ªåˆ‡æ¯”ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   ç»ç’ƒè´¨åå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹ç”Ÿäº§ 90 å¤©å¹³å‡ï¼ˆMCFPDï¼‰ã€‚

æ³¨æ„ï¼Œæ•°æ®é›†æ˜¯åˆæˆçš„ã€‚

æˆ‘ä»¬ä½¿ç”¨ pandas çš„ â€˜read_csvâ€™ å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸º â€˜my_dataâ€™ çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

```py
idata = 0
if idata == 0:
    df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository 

    response = 'Prod'                                             # specify the response feature
    x = df.copy(deep = True); x = x.drop(['Well',response],axis='columns') # make predictor and response DataFrames
    Y = df.loc[:,response]

    features = x.columns.values.tolist() + [Y.name]               # store the names of the features
    pred = x.columns.values.tolist()
    resp = Y.name

    xmin = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting
    Ymin = 500.0; Ymax = 9000.0

    predlabel = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Brittleness Ratio (%)', # set the names for plotting
                 'Total Organic Carbon (%)','Vitrinite Reflectance (%)']
    resplabel = 'Normalized Initial Production (MCFPD)'

    predtitle = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting
                 'Total Organic Carbon','Vitrinite Reflectance']
    resptitle = 'Normalized Initial Production'

    featurelabel = predlabel + [resplabel]                        # make feature labels and titles for concise code
    featuretitle = predtitle + [resptitle]

    m = len(pred) + 1
    mpred = len(pred)

# elif idata == 1:
#     names = {'Porosity':'Por'}

#     df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository 
#     df = df.rename(columns=names)
#     df['Por'] = df['Por'] * 100.0; df['AI'] = df['AI'] / 1000.0; 
#     df.drop('Unnamed: 0',axis=1,inplace=True) 

#     features = df.columns.values.tolist()                          # store the names of the features

#     xmin = [0.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax = [10000.0,10000.0,1.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting

#     flabel = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Facies (categorical)',
#               'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting

#     ftitle = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',
#               'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']

elif idata == 2:  
    df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository 

    response = 'CumulativeOil'                                             # specify the response feature
    x = df.copy(deep = True); x = x.drop(['Well_ID','X','Y',response],axis='columns') # make predictor and response DataFrames
    Y = df.loc[:,response]

    features = x.columns.values.tolist() + [Y.name]               # store the names of the features
    pred = x.columns.values.tolist()
    resp = Y.name

    xmin = [1.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax = [75.0,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting
    Ymin = 0.0; Ymax = 3000.0

    predlabel = ['Well (ID)','X (m)','Y (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)',
              'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] 
    resplabel = 'Cumulative Production (MSTB)'

    predtitle = ['Well','X','Y','Porosity','Permeability','Acoustic Impedance',
              'Density (g/cmÂ³)','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus'] 
    resptitle = 'Cumulative Production'

    featurelabel = predlabel + [resplabel]                        # make feature labels and titles for concise code
    featuretitle = predtitle + [resptitle]

    m = len(pred) + 1
    mpred = len(pred) 
```

```py
---------------------------------------------------------------------------
SSLCertVerificationError  Traceback (most recent call last)
File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:1317, in AbstractHTTPHandler.do_open(self, http_class, req, **http_conn_args)
  1316 try:
-> 1317     h.request(req.get_method(), req.selector, req.data, headers,
  1318               encode_chunked=req.has_header('Transfer-encoding'))
  1319 except OSError as err: # timeout error

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1230, in HTTPConnection.request(self, method, url, body, headers, encode_chunked)
  1229  """Send a complete request to the server."""
-> 1230 self._send_request(method, url, body, headers, encode_chunked)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1276, in HTTPConnection._send_request(self, method, url, body, headers, encode_chunked)
  1275     body = _encode(body, 'body')
-> 1276 self.endheaders(body, encode_chunked=encode_chunked)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1225, in HTTPConnection.endheaders(self, message_body, encode_chunked)
  1224     raise CannotSendHeader()
-> 1225 self._send_output(message_body, encode_chunked=encode_chunked)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1004, in HTTPConnection._send_output(self, message_body, encode_chunked)
  1003 del self._buffer[:]
-> 1004 self.send(msg)
  1006 if message_body is not None:
  1007 
  1008     # create a consistent interface to message_body

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:944, in HTTPConnection.send(self, data)
  943 if self.auto_open:
--> 944     self.connect()
  945 else:

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1399, in HTTPSConnection.connect(self)
  1397     server_hostname = self.host
-> 1399 self.sock = self._context.wrap_socket(self.sock,
  1400                                       server_hostname=server_hostname)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\ssl.py:500, in SSLContext.wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)
  494 def wrap_socket(self, sock, server_side=False,
  495                 do_handshake_on_connect=True,
  496                 suppress_ragged_eofs=True,
  497                 server_hostname=None, session=None):
  498     # SSLSocket class handles server_hostname encoding before it calls
  499     # ctx._wrap_socket()
--> 500     return self.sslsocket_class._create(
  501         sock=sock,
  502         server_side=server_side,
  503         do_handshake_on_connect=do_handshake_on_connect,
  504         suppress_ragged_eofs=suppress_ragged_eofs,
  505         server_hostname=server_hostname,
  506         context=self,
  507         session=session
  508     )

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\ssl.py:1040, in SSLSocket._create(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)
  1039             raise ValueError("do_handshake_on_connect should not be specified for non-blocking sockets")
-> 1040         self.do_handshake()
  1041 except (OSError, ValueError):

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\ssl.py:1309, in SSLSocket.do_handshake(self, block)
  1308         self.settimeout(None)
-> 1309     self._sslobj.do_handshake()
  1310 finally:

SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)

During handling of the above exception, another exception occurred:

URLError  Traceback (most recent call last)
Cell In[7], line 3
  1 idata = 0
  2 if idata == 0:
----> 3     df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository 
  5     response = 'Prod'                                             # specify the response feature
  6     x = df.copy(deep = True); x = x.drop(['Well',response],axis='columns') # make predictor and response DataFrames

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:912, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)
  899 kwds_defaults = _refine_defaults_read(
  900     dialect,
  901     delimiter,
   (...)
  908     dtype_backend=dtype_backend,
  909 )
  910 kwds.update(kwds_defaults)
--> 912 return _read(filepath_or_buffer, kwds)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:577, in _read(filepath_or_buffer, kwds)
  574 _validate_names(kwds.get("names", None))
  576 # Create the parser.
--> 577 parser = TextFileReader(filepath_or_buffer, **kwds)
  579 if chunksize or iterator:
  580     return parser

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:1407, in TextFileReader.__init__(self, f, engine, **kwds)
  1404     self.options["has_index_names"] = kwds["has_index_names"]
  1406 self.handles: IOHandles | None = None
-> 1407 self._engine = self._make_engine(f, self.engine)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:1661, in TextFileReader._make_engine(self, f, engine)
  1659     if "b" not in mode:
  1660         mode += "b"
-> 1661 self.handles = get_handle(
  1662     f,
  1663     mode,
  1664     encoding=self.options.get("encoding", None),
  1665     compression=self.options.get("compression", None),
  1666     memory_map=self.options.get("memory_map", False),
  1667     is_text=is_text,
  1668     errors=self.options.get("encoding_errors", "strict"),
  1669     storage_options=self.options.get("storage_options", None),
  1670 )
  1671 assert self.handles is not None
  1672 f = self.handles.handle

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:716, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)
  713     codecs.lookup_error(errors)
  715 # open URLs
--> 716 ioargs = _get_filepath_or_buffer(
  717     path_or_buf,
  718     encoding=encoding,
  719     compression=compression,
  720     mode=mode,
  721     storage_options=storage_options,
  722 )
  724 handle = ioargs.filepath_or_buffer
  725 handles: list[BaseBuffer]

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:368, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)
  366 # assuming storage_options is to be interpreted as headers
  367 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)
--> 368 with urlopen(req_info) as req:
  369     content_encoding = req.headers.get("Content-Encoding", None)
  370     if content_encoding == "gzip":
  371         # Override compression based on Content-Encoding header

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:270, in urlopen(*args, **kwargs)
  264  """
  265 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of
  266 the stdlib.
  267 """
  268 import urllib.request
--> 270 return urllib.request.urlopen(*args, **kwargs)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:222, in urlopen(url, data, timeout, cafile, capath, cadefault, context)
  220 else:
  221     opener = _opener
--> 222 return opener.open(url, data, timeout)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)
  522     req = meth(req)
  524 sys.audit('urllib.Request', req.full_url, req.data, req.headers, req.get_method())
--> 525 response = self._open(req, data)
  527 # post-process response
  528 meth_name = protocol+"_response"

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:542, in OpenerDirector._open(self, req, data)
  539     return result
  541 protocol = req.type
--> 542 result = self._call_chain(self.handle_open, protocol, protocol +
  543                           '_open', req)
  544 if result:
  545     return result

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:502, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)
  500 for handler in handlers:
  501     func = getattr(handler, meth_name)
--> 502     result = func(*args)
  503     if result is not None:
  504         return result

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:1360, in HTTPSHandler.https_open(self, req)
  1359 def https_open(self, req):
-> 1360     return self.do_open(http.client.HTTPSConnection, req,
  1361         context=self._context, check_hostname=self._check_hostname)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:1320, in AbstractHTTPHandler.do_open(self, http_class, req, **http_conn_args)
  1317         h.request(req.get_method(), req.selector, req.data, headers,
  1318                   encode_chunked=req.has_header('Transfer-encoding'))
  1319     except OSError as err: # timeout error
-> 1320         raise URLError(err)
  1321     r = h.getresponse()
  1322 except:

URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)> 
```

+   æˆ‘ä»¬è¿˜å¯ä»¥ä¸ºç»˜å›¾å»ºç«‹ç‰¹å¾èŒƒå›´ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¦‚ä¸‹ä»£ç ç›´æ¥ä»æ•°æ®ä¸­è®¡ç®—ç‰¹å¾èŒƒå›´ï¼š

```py
Pormin = np.min(df['Por'].values)                             # extract ndarray of data table column
Pormax = np.max(df['Por'].values)                             # and calculate min and max 
```

ä½†æ˜¯ï¼Œè¿™ä¸ä¼šå¯¼è‡´æ˜“äºç†è§£çš„è‰²å½©æ¡å’Œè½´åˆ»åº¦ï¼Œè®©æˆ‘ä»¬é€‰æ‹©æ–¹ä¾¿çš„æ•´æ•°ã€‚æˆ‘ä»¬è¿˜å°†å£°æ˜ç‰¹å¾æ ‡ç­¾ä»¥æ–¹ä¾¿ç»˜å›¾ã€‚

## å¯è§†åŒ– DataFrame

å¯è§†åŒ– DataFrame æ˜¯æ•°æ®çš„ç¬¬ä¸€æ­¥æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ï¼Œç­‰ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ DataFrame çš„â€˜headâ€™æˆå‘˜å‡½æ•°æ¥é¢„è§ˆï¼ˆæ ¼å¼æ•´æ´ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚

+   æ·»åŠ å‚æ•°â€˜n=13â€™ä»¥æŸ¥çœ‹æ•°æ®é›†çš„å‰ 13 è¡Œã€‚

```py
df.head(n=13)                                                 # we could also use this command for a table preview 
```

|  | Well | Por | Perm | AI | Brittle | TOC | VR | Prod |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 1 | 12.08 | 2.92 | 2.80 | 81.40 | 1.16 | 2.31 | 1695.360819 |
| 1 | 2 | 12.38 | 3.53 | 3.22 | 46.17 | 0.89 | 1.88 | 3007.096063 |
| 2 | 3 | 14.02 | 2.59 | 4.01 | 72.80 | 0.89 | 2.72 | 2531.938259 |
| 3 | 4 | 17.67 | 6.75 | 2.63 | 39.81 | 1.08 | 1.88 | 5288.514854 |
| 4 | 5 | 17.52 | 4.57 | 3.18 | 10.94 | 1.51 | 1.90 | 2859.469624 |
| 5 | 6 | 14.53 | 4.81 | 2.69 | 53.60 | 0.94 | 1.67 | 4017.374438 |
| 6 | 7 | 13.49 | 3.60 | 2.93 | 63.71 | 0.80 | 1.85 | 2952.812773 |
| 7 | 8 | 11.58 | 3.03 | 3.25 | 53.00 | 0.69 | 1.93 | 2670.933846 |
| 8 | 9 | 12.52 | 2.72 | 2.43 | 65.77 | 0.95 | 1.98 | 2474.048178 |
| 9 | 10 | 13.25 | 3.94 | 3.71 | 66.20 | 1.14 | 2.65 | 2722.893266 |
| 10 | 11 | 15.04 | 4.39 | 2.22 | 61.11 | 1.08 | 1.77 | 3828.247174 |
| 11 | 12 | 16.19 | 6.30 | 2.29 | 49.10 | 1.53 | 1.86 | 5095.810104 |
| 12 | 13 | 16.82 | 5.42 | 2.80 | 66.65 | 1.17 | 1.98 | 4091.637316 |

## è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡

åœ¨ DataFrames ä¸­ï¼Œæœ‰è®¸å¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡ã€‚`describe`å‘½ä»¤æä¾›äº†è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼å’Œå››åˆ†ä½æ•°ï¼Œå…¨éƒ¨åœ¨ä¸€ä¸ªæ¼‚äº®çš„æ•°æ®è¡¨ä¸­ã€‚

+   æˆ‘ä»¬ä½¿ç”¨è½¬ç½®åªæ˜¯ä¸ºäº†è®©è¡¨æ ¼ç¿»è½¬ï¼Œä½¿å¾—ç‰¹å¾åœ¨è¡Œä¸Šï¼Œè€Œç»Ÿè®¡ä¿¡æ¯åœ¨åˆ—ä¸Šã€‚

```py
df.describe().transpose()                                     # calculate summary statistics for the data 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Well | 200.0 | 100.500000 | 57.879185 | 1.000000 | 50.750000 | 100.500000 | 150.250000 | 200.000000 |
| Por | 200.0 | 14.991150 | 2.971176 | 6.550000 | 12.912500 | 15.070000 | 17.402500 | 23.550000 |
| Perm | 200.0 | 4.330750 | 1.731014 | 1.130000 | 3.122500 | 4.035000 | 5.287500 | 9.870000 |
| AI | 200.0 | 2.968850 | 0.566885 | 1.280000 | 2.547500 | 2.955000 | 3.345000 | 4.630000 |
| Brittle | 200.0 | 48.161950 | 14.129455 | 10.940000 | 37.755000 | 49.510000 | 58.262500 | 84.330000 |
| TOC | 200.0 | 0.990450 | 0.481588 | -0.190000 | 0.617500 | 1.030000 | 1.350000 | 2.180000 |
| VR | 200.0 | 1.964300 | 0.300827 | 0.930000 | 1.770000 | 1.960000 | 2.142500 | 2.870000 |
| Prod | 200.0 | 3864.407081 | 1553.277558 | 839.822063 | 2686.227611 | 3604.303506 | 4752.637555 | 8590.384044 |

æ’åºç‰¹å¾å®é™…ä¸Šæ˜¯ä¸€é¡¹åŠªåŠ›ï¼Œæ—¨åœ¨ç†è§£ç‰¹å¾åŠå…¶ç›¸äº’ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬å°†ä»åŸºæœ¬çš„æ•°æ®å¯è§†åŒ–å¼€å§‹ï¼Œè¿‡æ¸¡åˆ°æ›´å¤æ‚çš„æ–¹æ³•ï¼Œä¾‹å¦‚åç›¸å…³å’Œé€’å½’ç‰¹å¾æ¶ˆé™¤ã€‚

## è¦†ç›–ç‡

è®©æˆ‘ä»¬ä»ç‰¹å¾è¦†ç›–çš„æ¦‚å¿µå¼€å§‹ã€‚

+   å¦‚æœä¸€ä¸ªç‰¹å¾åªåœ¨å°‘é‡æ ·æœ¬ä¸­å¯ç”¨ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯èƒ½ä¸æƒ³å°†å…¶åŒ…æ‹¬åœ¨å†…ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´ç‰¹å¾æ’è¡¥å’Œç¼ºå¤±æ•°æ®ä¼°è®¡çš„é—®é¢˜ã€‚

+   é€šè¿‡åˆ é™¤ä¸€äº›è¦†ç›–ç‡è¾ƒå·®çš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šæé«˜æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå› ä¸ºç‰¹å¾æ’è¡¥å­˜åœ¨å±€é™æ€§ï¼Œç‰¹å¾æ’è¡¥å®é™…ä¸Šä¼šåœ¨ç»Ÿè®¡æ•°æ®å’Œæˆ‘ä»¬çš„é¢„æµ‹æ¨¡å‹ä¸­å¼•å…¥åå·®å’Œé¢å¤–çš„è¯¯å·®ã€‚

+   å¦‚æœåº”ç”¨ç±»ä¼¼åˆ é™¤æ¥å¤„ç†ç¼ºå¤±å€¼ï¼Œä½è¦†ç›–ç‡çš„ç‰¹å¾ä¼šå¯¼è‡´å¤§é‡æ•°æ®è¢«åˆ é™¤ï¼

è®©æˆ‘ä»¬ä»æ¡å½¢å›¾å¼€å§‹ï¼Œå±•ç¤ºç¼ºå¤±è®°å½•çš„æ¯”ä¾‹ï¼š

```py
plt.subplot(111)
(df.isnull().sum()/len(df)).plot(kind = 'bar')                # calculate DataFrame with percentage missing by feature
plt.xlabel('Feature'); plt.ylabel('Percentage of Missing Values'); plt.title('Data Completeness'); plt.ylim([0.0,1.0])
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.2); add_grid(); plt.show() 
```

![å›¾ç‰‡](img/e5a834b70ca47ad151aee5749adc53ce.png)

å¯¹äºæä¾›çš„ç¤ºä¾‹æ•°æ®é›†ï¼Œå›¾è¡¨åº”ä¸ºç©ºã€‚æ²¡æœ‰ç¼ºå¤±æ•°æ®ï¼Œå› æ­¤æ‰€æœ‰ç‰¹å¾çš„â€œç¼ºå¤±è®°å½•æ¯”ä¾‹â€å‡ä¸º 0.0ã€‚

å¦‚æœä½ æƒ³ç”¨ä¸€äº›ç¼ºå¤±æ•°æ®æµ‹è¯•æ­¤å›¾è¡¨ï¼Œè¯·å…ˆè¿è¡Œæ­¤ä»£ç ï¼š

```py
proportion_NaN = 0.1                                    # proportion of values in DataFrame to remove

remove = np.random.random(df.shape) < proportion_NaN    # make the boolean array for removal
print('Fraction of removed values in mask ndarray = ' + str(round(remove.sum()/remove.size,3)) + '.')

df_mask = df.mask(remove)                               # make a new DataFrame with specified proportion removed 
```

åˆ é™¤æ­¤ä»£ç å¹¶é‡æ–°åŠ è½½æ•°æ®ï¼Œä»¥ç»§ç»­è·å¾—ä¸ä»¥ä¸‹è®¨è®ºä¸€è‡´çš„ç»“æœã€‚

è¿™å¹¶ä¸èƒ½è¯´æ˜å…¨éƒ¨æƒ…å†µã€‚ä¾‹å¦‚ï¼Œå¦‚æœç‰¹å¾ A çš„ 20%ç¼ºå¤±ï¼Œç‰¹å¾ B çš„ 20%ç¼ºå¤±ï¼Œè¿™äº›æ˜¯å¦æ˜¯ç›¸åŒçš„æ ·æœ¬æˆ–ä¸åŒçš„æ ·æœ¬ã€‚å¦‚æœä½ æ‰§è¡Œç±»ä¼¼çš„åˆ é™¤ï¼Œè¿™ä¼šäº§ç”Ÿå·¨å¤§çš„å½±å“ã€‚

+   å¦‚æœæ•°æ®é‡ä¸æ˜¯å¤ªå¤šï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥åœ¨è¿™æ ·çš„å¸ƒå°”è¡¨ä¸­å¯è§†åŒ–æ‰€æœ‰æ ·æœ¬å’Œç‰¹å¾çš„è¦†ç›–æƒ…å†µã€‚

+   æ­¤æ–¹æ³•å¯èƒ½è¯†åˆ«å‡ºå…·æœ‰è®¸å¤šç¼ºå¤±ç‰¹å¾çš„å…·ä½“æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬å¯èƒ½è¢«åˆ é™¤ä»¥æé«˜æ•´ä½“è¦†ç›–æˆ–ç¼ºå¤±æ•°æ®ä¸­çš„å…¶ä»–è¶‹åŠ¿æˆ–ç»“æ„ï¼Œè¿™å¯èƒ½å¯¼è‡´é‡‡æ ·åå·®ã€‚

```py
df_temp = df.copy(deep=True)                                  # make a deep copy of the DataFrame
df_bool = df_temp.isnull()                                    # true is value, false if NaN
#df_bool = df_bool.set_index(df_temp.pop('UWI'))              # set the index / feature for the heat map y column
heat = sns.heatmap(df_bool, cmap=['r','w'], annot=False, fmt='.0f',cbar=False,linecolor='black',linewidth=0.1) # make the binary heat map, no bins
heat.set_xticklabels(heat.get_xticklabels(), rotation=90, fontsize=8)
heat.set_yticklabels(heat.get_yticklabels(), rotation=0, fontsize=8)
heat.set_title('Data Completeness Heatmap',fontsize=16); heat.set_xlabel('Feature',fontsize=12); heat.set_ylabel('Sample (Index)',fontsize=12)
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.8, top=1.6, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/9b8b2b5fe0360995f89df5950e3ed23d.png)

å†æ¬¡å¼ºè°ƒï¼Œå¯¹äºæä¾›çš„å…·æœ‰å®Œç¾è¦†ç›–çš„æ•°æ®é›†ï¼Œæ­¤å›¾è¡¨åº”è¯¥ç›¸å½“æ— èŠï¼Œæ¯ä¸ªå•å…ƒæ ¼éƒ½åº”å¡«å……çº¢è‰²ã€‚

+   æ·»åŠ ä»£ç ä»¥åˆ é™¤ä¸€äº›è®°å½•ä»¥æµ‹è¯•æ­¤å›¾è¡¨ã€‚ç™½è‰²å•å…ƒæ ¼è¡¨ç¤ºç¼ºå¤±è®°å½•ã€‚

### ç‰¹å¾æ’è¡¥

è¯·å‚é˜…å…³äºç‰¹å¾æ’è¡¥çš„ç« èŠ‚ï¼Œäº†è§£å¦‚ä½•å¤„ç†ç¼ºå¤±æ•°æ®ã€‚

ç›®å‰åœ¨æ­¤å¤„è¿›è¡Œç®€è¦å¤„ç†ï¼Œæˆ‘ä»¬åªéœ€åŒæ ·åˆ é™¤å¹¶ç»§ç»­è¿›è¡Œã€‚

+   æˆ‘ä»¬ç§»é™¤æ‰€æœ‰å…·æœ‰ä»»ä½•ç¼ºå¤±ç‰¹å¾å€¼çš„æ ·æœ¬ã€‚è™½ç„¶è¿™å¾ˆç®€å•ï¼Œä½†è¿™æ˜¯ç¡®ä¿æˆ‘ä»¬å³å°†å±•ç¤ºçš„ç‰¹å¾æ’åæ–¹æ³•æ‰€éœ€å®Œç¾è¦†ç›–çš„ä¸€ç§ç®€å•ç²—æš´çš„æ–¹æ³•ã€‚è¯·æŸ¥çœ‹ä¸Šé¢é“¾æ¥çš„å·¥ä½œæµç¨‹ä¸­çš„å…¶ä»–æ–¹æ³•ã€‚

```py
df.dropna(axis=0,how='any',inplace=True)                      # likewise deletion 
```

## æ‘˜è¦ç»Ÿè®¡

åœ¨ä»»ä½•å¤šå…ƒå·¥ä½œä¸­ï¼Œæˆ‘ä»¬åº”è¯¥ä»å•å˜é‡åˆ†æå¼€å§‹ï¼Œä¸€æ¬¡åªåˆ†æä¸€ä¸ªå˜é‡çš„æ‘˜è¦ç»Ÿè®¡ã€‚æ‘˜è¦ç»Ÿè®¡æ’åæ–¹æ³•æ˜¯å®šæ€§çš„ï¼Œæˆ‘ä»¬æ˜¯åœ¨é—®ï¼š

+   æ˜¯å¦å­˜åœ¨æ•°æ®é—®é¢˜ï¼Ÿ

+   æˆ‘ä»¬æ˜¯å¦ä¿¡ä»»ç‰¹å¾ï¼Ÿæˆ‘ä»¬æ˜¯å¦åŒç­‰ä¿¡ä»»æ‰€æœ‰ç‰¹å¾ï¼Ÿ

+   åœ¨æˆ‘ä»¬å¼€å‘ä»»ä½•å¤šå…ƒå·¥ä½œæµç¨‹ä¹‹å‰ï¼Œéœ€è¦å¤„ç†å“ªäº›é—®é¢˜ï¼Ÿ

åœ¨ DataFrames ä¸­ï¼Œæœ‰è®¸å¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ã€‚describe å‘½ä»¤æä¾›äº†ä¸€ä¸ªç´§å‡‘çš„æ•°æ®è¡¨ï¼Œæä¾›äº†è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼å’Œå››åˆ†ä½æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨ transpose()å‘½ä»¤ç¿»è½¬è¡¨æ ¼ï¼Œä½¿ç‰¹å¾ä½äºè¡Œä¸Šï¼Œè€Œç»Ÿè®¡å€¼ä½äºåˆ—ä¸Šã€‚

```py
df.describe().transpose()                                     # DataFrame summary statistics 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Well | 200.0 | 100.500000 | 57.879185 | 1.000000 | 50.750000 | 100.500000 | 150.250000 | 200.000000 |
| Por | 200.0 | 14.991150 | 2.971176 | 6.550000 | 12.912500 | 15.070000 | 17.402500 | 23.550000 |
| Perm | 200.0 | 4.330750 | 1.731014 | 1.130000 | 3.122500 | 4.035000 | 5.287500 | 9.870000 |
| AI | 200.0 | 2.968850 | 0.566885 | 1.280000 | 2.547500 | 2.955000 | 3.345000 | 4.630000 |
| Brittle | 200.0 | 48.161950 | 14.129455 | 10.940000 | 37.755000 | 49.510000 | 58.262500 | 84.330000 |
| TOC | 200.0 | 0.990450 | 0.481588 | -0.190000 | 0.617500 | 1.030000 | 1.350000 | 2.180000 |
| VR | 200.0 | 1.964300 | 0.300827 | 0.930000 | 1.770000 | 1.960000 | 2.142500 | 2.870000 |
| Prod | 200.0 | 3864.407081 | 1553.277558 | 839.822063 | 2686.227611 | 3604.303506 | 4752.637555 | 8590.384044 |

æ‘˜è¦ç»Ÿè®¡æ˜¯æ•°æ®æ£€æŸ¥çš„å…³é”®ç¬¬ä¸€æ­¥ã€‚

+   è¿™åŒ…æ‹¬æ¯ä¸ªç‰¹å¾çš„æœ‰æ•ˆï¼ˆéç©ºï¼‰å€¼çš„æ•°é‡ï¼ˆè®¡æ•°ä¼šä»æ¯ä¸ªå˜é‡çš„æ€»æ•°ä¸­ç§»é™¤æ‰€æœ‰ np.NaNï¼‰ã€‚

+   æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€èˆ¬çš„è¡Œä¸ºï¼Œå¦‚ä¸­å¿ƒè¶‹åŠ¿ã€å‡å€¼å’Œåˆ†æ•£åº¦ã€æ–¹å·®ã€‚

+   æˆ‘ä»¬å¯ä»¥è¯†åˆ«å‡ºè´Ÿå€¼ã€æç«¯å€¼ä»¥åŠè¶…å‡ºæ¯ä¸ªå±æ€§å¯èƒ½å€¼èŒƒå›´çš„å€¼çš„é—®é¢˜ã€‚

+   æ•°æ®çœ‹èµ·æ¥ç›¸å½“è‰¯å¥½ï¼Œä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬è·³è¿‡å¼‚å¸¸å€¼æ£€æµ‹ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å•å˜é‡åˆ†å¸ƒã€‚

## å•å˜é‡åˆ†å¸ƒ

ä¸æ‘˜è¦ç»Ÿè®¡ä¸€æ ·ï¼Œè¿™ç§æ–¹æ³•æ˜¯å¯¹æ•°æ®é—®é¢˜è¿›è¡Œå®šæ€§æ£€æŸ¥ï¼Œå¹¶è¯„ä¼°æˆ‘ä»¬å¯¹æ¯ä¸ªç‰¹å¾çš„ä¿¡å¿ƒã€‚æœ€å¥½ä¸åŒ…å«è´¨é‡ä¿¡å¿ƒä½çš„ç‰¹å¾ï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šè¯¯å¯¼ï¼ˆå¦‚å‰æ‰€è¿°ï¼ŒåŒæ—¶å¢åŠ æ¨¡å‹å¤æ‚æ€§ï¼‰ã€‚

```py
nbins = 20                                                    # number of histogram bins
for i, feature in enumerate(features):                        # plot histograms with central tendency and P10 and P90 labeled
    plt.subplot(4,3,i+1)
    y,_,_ = plt.hist(x=df[feature],weights=None,bins=nbins,alpha = 0.8,edgecolor='black',color='darkorange',density=True)
    # histogram_bounds(values=df[feature].values,weights=np.ones(len(df)),color='red')
    plt.xlabel(feature); plt.ylabel('Frequency'); plt.ylim([0.0,y.max()*1.10]); plt.title(featuretitle[i]); add_grid() 
    # if feature == resp: 
    #     plt.xlim([Ymin,Ymax]) 
    # else:
    #     plt.xlim([xmin[i],xmax[i]]) 

plt.subplots_adjust(left=0.0, bottom=0.0, right=3., top=4.1, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/8a0c89fa0d885a7643839d04c23ff8be.png)

å•å˜é‡åˆ†å¸ƒçœ‹èµ·æ¥å¾ˆå¥½ï¼š

+   æ²¡æœ‰æ˜æ˜¾çš„å¼‚å¸¸å€¼

+   æ¸—é€ç‡æ˜¯æ­£åæ–œçš„ï¼Œè¿™æ˜¯å¸¸è§çš„ç°è±¡

+   ä¿®æ­£åçš„ç›®å½•è¡¨æœ‰ä¸€ä¸ªå°å³°å€¼ï¼Œä½†è¿™æ˜¯åˆç†çš„

## åŒå˜é‡åˆ†å¸ƒ

çŸ©é˜µæ•£ç‚¹å›¾æ˜¯è§‚å¯Ÿå˜é‡ä¹‹é—´åŒå˜é‡å…³ç³»çš„ä¸€ç§éå¸¸æœ‰æ•ˆçš„æ–¹æ³•ã€‚

+   è¿™åˆæ˜¯é€šè¿‡æ•°æ®å¯è§†åŒ–æ¥è¯†åˆ«æ•°æ®é—®é¢˜çš„å¦ä¸€ä¸ªæœºä¼š

+   æˆ‘ä»¬å¯ä»¥è¯„ä¼°æ˜¯å¦å­˜åœ¨å¤šé‡å…±çº¿æ€§ï¼Œç‰¹åˆ«æ˜¯æ¯æ¬¡è¯„ä¼°ä¸¤ä¸ªç‰¹å¾ä¹‹é—´çš„ç®€å•å½¢å¼ã€‚

```py
pairgrid = sns.PairGrid(df) # matrix scatter plots
pairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)
pairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle
pairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, 
                              shade = False, shade_lowest = False, alpha = 1.0, n_levels = 10)
pairgrid.add_legend()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/eb1c7e7e9920c8c27be71cd70df81661.png)

è¿™ä¸ªå›¾è¡¨ä¼ è¾¾äº†å¤§é‡çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨è¿™ä¸ªå›¾è¡¨è¿›è¡Œå˜é‡æ’åï¼Ÿ

+   æˆ‘ä»¬å¯ä»¥è¯†åˆ«å‡ºå½¼æ­¤ç´§å¯†ç›¸å…³çš„ç‰¹å¾ï¼Œä¾‹å¦‚ï¼Œå¦‚æœä¸¤ä¸ªç‰¹å¾å‡ ä¹æœ‰å®Œç¾çš„å•è°ƒçº¿æ€§æˆ–è¿‘çº¿æ€§å…³ç³»ï¼Œæˆ‘ä»¬åº”è¯¥ç«‹å³åˆ é™¤ä¸€ä¸ªã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¤šé‡å…±çº¿æ€§æ¡ˆä¾‹ï¼Œå¦‚ä¸Šæ‰€è¿°ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹ä¸ç¨³å®šã€‚

+   æˆ‘ä»¬å¯ä»¥æ£€æŸ¥çº¿æ€§ä¸éçº¿æ€§å…³ç³»ã€‚å¦‚æœæˆ‘ä»¬è§‚å¯Ÿåˆ°éçº¿æ€§åŒå˜é‡å…³ç³»ï¼Œè¿™å°†å½±å“æ–¹æ³•çš„é€‰æ‹©ï¼Œä»¥åŠå˜é‡æ’åæ–¹æ³•å‡è®¾çº¿æ€§å…³ç³»æ—¶çš„ç»“æœè´¨é‡ã€‚

+   æˆ‘ä»¬å¯ä»¥è¯†åˆ«å˜é‡ä¹‹é—´çš„çº¦æŸå…³ç³»å’Œå¼‚æ–¹å·®æ€§ã€‚å†æ¬¡å¼ºè°ƒï¼Œè¿™äº›å¯èƒ½ä¼šé™åˆ¶æˆ‘ä»¬çš„æ’åæ–¹æ³•ï¼Œå¹¶é¼“åŠ±æˆ‘ä»¬ä¿ç•™ç‰¹å®šç‰¹å¾ä»¥åœ¨ç»“æœæ¨¡å‹ä¸­ä¿ç•™è¿™äº›ç‰¹å¾ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬å¿…é¡»è®°ä½ï¼ŒåŒå˜é‡å¯è§†åŒ–å’Œåˆ†æä¸è¶³ä»¥ç†è§£æ•°æ®ä¸­çš„æ‰€æœ‰å¤šå…ƒå…³ç³»ï¼Œä¾‹å¦‚ï¼Œå¤šé‡å…±çº¿æ€§åŒ…æ‹¬ä¸¤ä¸ªæˆ–æ›´å¤šç‰¹å¾ä¹‹é—´å¼ºçƒˆçš„çº¿æ€§å…³ç³»ã€‚è¿™äº›å¯èƒ½ä»…é€šè¿‡åŒå˜é‡å›¾éš¾ä»¥çœ‹åˆ°ã€‚

## å¯¹æ•°åæ–¹å·®

å¯¹æ•°åæ–¹å·®æä¾›äº†è¡¡é‡æ¯ä¸ªé¢„æµ‹ç‰¹å¾ä¸å“åº”ç‰¹å¾ä¹‹é—´çº¿æ€§å…³ç³»å¼ºåº¦çš„åº¦é‡ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬æŒ‡å®šæœ¬ç ”ç©¶çš„ç›®çš„æ˜¯ä»å…¶ä»–å¯ç”¨çš„é¢„æµ‹ç‰¹å¾ä¸­é¢„æµ‹äº§é‡ï¼Œæˆ‘ä»¬çš„å“åº”å˜é‡ã€‚æˆ‘ä»¬ç°åœ¨æ­£åœ¨é¢„æµ‹æ€§æ€è€ƒï¼Œè€Œä¸æ˜¯æ¨æ–­æ€§æ€è€ƒï¼Œæˆ‘ä»¬æƒ³è¦ä¼°è®¡å‡½æ•° $\hat{f}$ æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ï¼š

$$ Y = \hat{f}(X_1,\ldots,X_n) $$

å…¶ä¸­ $Y$ æ˜¯æˆ‘ä»¬çš„å“åº”ç‰¹å¾ï¼Œ$X_1,\ldots,X_n$ æ˜¯æˆ‘ä»¬çš„é¢„æµ‹ç‰¹å¾ã€‚å¦‚æœæˆ‘ä»¬ä¿ç•™äº†æ‰€æœ‰é¢„æµ‹ç‰¹å¾æ¥é¢„æµ‹å“åº”ï¼Œæˆ‘ä»¬å°±ä¼šæœ‰ï¼š

$$ äº§é‡ = \hat{f}(å­”éš™ç‡ï¼Œæ¸—é€ç‡ï¼ŒAIï¼Œè„†æ€§ï¼ŒTOCï¼ŒVR) $$

ç°åœ¨å›åˆ°åæ–¹å·®ï¼Œåæ–¹å·®å®šä¹‰ä¸ºï¼š

$$ C_{xy} = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{(n-1)} $$

åæ–¹å·®ï¼š

+   è¡¡é‡çº¿æ€§å…³ç³»

+   å¯¹é¢„æµ‹å’Œå“åº”çš„åˆ†æ•£/æ–¹å·®æ•æ„Ÿ

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥æ„å»ºåæ–¹å·®çŸ©é˜µï¼š

```py
df.iloc[:,1:8].cov()                                    # covariance matrix sliced predictors vs. response 
```

è¾“å‡ºæ˜¯ä¸€ä¸ªæ–°çš„ Pandas DataFrameï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åˆ‡ç‰‡æœ€åä¸€åˆ—ä»¥è·å–ä¸€ä¸ª Pandas ç³»åˆ—ï¼ˆå…·æœ‰åç§°çš„ ndarrayï¼‰ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰é¢„æµ‹ç‰¹å¾ä¸å“åº”ä¹‹é—´çš„åæ–¹å·®ã€‚

```py
covariance = df.iloc[:,df.columns.get_indexer(features)].cov().iloc[len(features)-1,:len(features)] # calculate covariance matrix and slice for only pred - resp
cov_matrix = df.iloc[:,df.columns.get_indexer(features)].cov()
plt.subplot(121)
plot_corr(cov_matrix,'Covariance Matrix',4000.0,0.1)          # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features') 

plt.subplot(122)
feature_rank_plot(features,covariance,-20000.0,20000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.1)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/89e89e3a083da28954418714216aa9d0.png)

åæ–¹å·®æ˜¯æœ‰ç”¨çš„ï¼Œä½†å¦‚æ‚¨æ‰€è§ï¼Œå…¶å¤§å°ç›¸å½“å¤šå˜ã€‚

+   åæ–¹å·®çš„å¤§å°æ˜¯æ¯ä¸ªç‰¹å¾çš„ç‰¹æ€§åŠå…¶ç‰¹å¾æ–¹å·®æ˜¯ç›¸å½“ä»»æ„çš„ã€‚

+   ä¾‹å¦‚ï¼Œå­”éš™ç‡çš„æ–¹å·®åœ¨åˆ†æ•°ä¸ç™¾åˆ†æ¯”ä¹‹é—´æˆ–æ¸—é€ç‡åœ¨è¾¾è¥¿ä¸æ¯«è¾¾è¥¿ä¹‹é—´çš„æ–¹å·®æ˜¯å¤šå°‘ã€‚æˆ‘ä»¬å¯ä»¥è¡¨æ˜ï¼Œå¦‚æœæˆ‘ä»¬å¯¹ä¸€ä¸ªç‰¹å¾ $X$ åº”ç”¨ä¸€ä¸ªå¸¸æ•°ä¹˜æ•° $c$ï¼Œæ–¹å·®å°†æ ¹æ®æ­¤å…³ç³»å˜åŒ–ï¼ˆè¯æ˜åŸºäºæ–¹å·®çš„æœŸæœ›å…¬å¼ï¼‰ï¼š

$$ \sigma_{cX}Â² = cÂ² \cdot \sigma_{X}Â² $$

é€šè¿‡ä»ç™¾åˆ†æ¯”è½¬æ¢ä¸ºåˆ†æ•°ï¼Œæˆ‘ä»¬ä½¿å­”éš™ç‡çš„æ–¹å·®é™ä½äº† 10000 å€ï¼æ¯ä¸ªç‰¹å¾çš„æ–¹å·®å¯èƒ½æ˜¯ä»»æ„çš„ï¼Œé™¤éæ‰€æœ‰ç‰¹å¾éƒ½åœ¨ç›¸åŒçš„å•ä½ä¸­ã€‚

å¯¹æ•°ç›¸å…³ç³»æ•°æ˜¯æ ‡å‡†åŒ–çš„åæ–¹å·®ï¼›å› æ­¤ï¼Œé¿å…äº†è¿™ç§ä»»æ„å¤§å°çš„é—®é¢˜ã€‚

## å¯¹æ•°ç›¸å…³ç³»æ•°

é…å¯¹ç›¸å…³ç³»æ•°æä¾›äº†è¡¡é‡æ¯ä¸ªé¢„æµ‹ç‰¹å¾ä¸å“åº”ç‰¹å¾ä¹‹é—´çº¿æ€§å…³ç³»å¼ºåº¦çš„åº¦é‡ã€‚

$$ \rho_{xy} = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{(n-1)\sigma_x \sigma_y}, \, -1.0 \le \rho_{xy} \le 1.0 $$

ç›¸å…³ç³»æ•°ï¼š

+   è¡¡é‡çº¿æ€§å…³ç³»

+   é€šè¿‡å°†æ¯ä¸ªç‰¹å¾çš„æ ‡å‡†å·®ä¹˜ç§¯è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ¶ˆé™¤äº†å¯¹é¢„æµ‹ç‰¹å¾å’Œå“åº”ç‰¹å¾åˆ†æ•£/æ–¹å·®çš„æ•æ„Ÿæ€§

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥æ„å»ºä¸€ä¸ªç›¸å…³çŸ©é˜µï¼š

```py
df.iloc[:,1:8].corr() 
```

è¾“å‡ºæ˜¯ä¸€ä¸ªæ–°çš„ Pandas DataFrameï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åˆ‡ç‰‡æœ€åä¸€åˆ—ä»¥è·å–ä¸€ä¸ª Pandas ç³»åˆ—ï¼ˆå…·æœ‰åç§°çš„ ndarrayï¼‰ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰é¢„æµ‹ç‰¹å¾ä¸å“åº”ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

```py
correlation = df.iloc[:,df.columns.get_indexer(features)].corr().iloc[len(features)-1,:len(features)] # calculate covariance matrix and slice for only pred - resp
corr_matrix = df.iloc[:,df.columns.get_indexer(features)].corr()

plt.subplot(121)
plot_corr(corr_matrix,'Correlation Matrix',1.0,0.5)           # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(features,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![_images/8de550142d1a57e5f8c2443095641ca1f3d8907168b1d668112afc3f7f49b625.png](img/cf2f8ebbbb9381f4ae232eefb7ce2e7a.png)

ä»ç›¸å…³çŸ©é˜µä¸­æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼š

+   æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å­”éš™ç‡ã€æ¸—é€ç‡å’Œæ€»æœ‰æœºç¢³ä¸äº§é‡ä¹‹é—´æœ‰æœ€å¼ºçš„çº¿æ€§å…³ç³»ã€‚

+   å£°æ³¢é˜»æŠ—ä¸äº§é‡å‘ˆå¼±è´Ÿç›¸å…³ã€‚

+   å»¶å±•æ€§éå¸¸æ¥è¿‘ 0.0ã€‚å¦‚æœä½ æŸ¥çœ‹å»¶å±•æ€§ vs. äº§é‡æ•£ç‚¹å›¾ï¼Œä½ ä¼šè§‚å¯Ÿåˆ°ä¸€ç§å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚å¯¹äºäº§é‡æ¥è¯´ï¼Œæœ‰ä¸€ä¸ªå»¶å±•æ€§æ¯”ç‡æœ€ä½³ç‚¹ï¼ˆæ—¢ä¸å¤ªè½¯ä¹Ÿä¸å¤ªç¡¬çš„å²©çŸ³ï¼‰ï¼

æˆ‘ä»¬ä¹Ÿå¯ä»¥æŸ¥çœ‹å®Œæ•´çš„ç›¸å…³çŸ©é˜µæ¥è¯„ä¼°é¢„æµ‹ç‰¹å¾ä¹‹é—´å†—ä½™çš„æ½œåŠ›ã€‚

+   å­”éš™ç‡å’Œæ¸—é€ç‡ä»¥åŠå­”éš™ç‡å’Œ TOC ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„å…³è”åº¦

+   TOC å’Œå£°æ³¢é˜»æŠ—ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„è´Ÿç›¸å…³åº¦

æˆ‘ä»¬ä»ç„¶å±€é™äºä¸¥æ ¼çš„çº¿æ€§å…³ç³»ã€‚æ’åºç›¸å…³ç³»æ•°ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ”¾å®½è¿™ä¸ªå‡è®¾ã€‚

## é…å¯¹æ–¯çš®å°”æ›¼ç§©ç›¸å…³ç³»æ•°

æ’åºç›¸å…³ç³»æ•°åœ¨è®¡ç®—ç›¸å…³ç³»æ•°ä¹‹å‰å¯¹æ•°æ®è¿›è¡Œç§©è½¬æ¢ã€‚è¦è®¡ç®—ç§©è½¬æ¢ï¼Œåªéœ€å°†æ•°æ®å€¼æ›¿æ¢ä¸ºç§© $R_x = 1,\dots,n$ï¼Œå…¶ä¸­ $n$ æ˜¯æœ€å¤§å€¼ï¼Œ$1$ æ˜¯æœ€å°å€¼ã€‚

$$ \rho_{R_x R_y} = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i} - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le 1.0 $$$$ x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall \, i \gt j $$$$ R_{x_i} = i $$

æ’åºç›¸å…³ç³»æ•°ï¼š

+   è¡¡é‡å•è°ƒå…³ç³»ï¼Œæ”¾å®½äº†çº¿æ€§å‡è®¾

+   é€šè¿‡å°†æ¯ä¸ªç‰¹å¾çš„æ ‡å‡†å·®ä¹˜ç§¯è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ¶ˆé™¤äº†å¯¹é¢„æµ‹ç‰¹å¾å’Œå“åº”çš„æ•æ„Ÿæ€§

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥æ„å»ºæ’åºç›¸å…³çŸ©é˜µå¹¶è®¡ç®— p å€¼ï¼š

```py
stats.spearmanr(df.iloc[:,1:8]) 
```

è¾“å‡ºæ˜¯ä¸€ä¸ªæ–°çš„ Pandas DataFrameï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åˆ‡ç‰‡æœ€åä¸€åˆ—ä»¥è·å–ä¸€ä¸ª Pandas ç³»åˆ—ï¼ˆå…·æœ‰åç§°çš„ ndarrayï¼‰ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰é¢„æµ‹ç‰¹å¾ä¸å“åº”ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¾—åˆ°äº†ä¸€ä¸ªéå¸¸æ–¹ä¾¿çš„ *pval* 2D ndarrayï¼Œå®ƒå…·æœ‰å‡è®¾æ£€éªŒçš„åŒä¾§ï¼ˆä¸¤å°¾æ±‚å’Œå¯¹ç§°åœ°è·¨è¶Šä¸¤ç«¯ï¼‰p å€¼ï¼š

$$ H_o: \rho_{R_x R_y} = 0 $$$$ H_1: \rho_{R_x R_y} \ne 0 $$

è®©æˆ‘ä»¬ä¿ç•™æ‰€æœ‰é¢„æµ‹ç‰¹å¾å’Œå“åº”ç‰¹å¾ä¹‹é—´çš„ p å€¼ã€‚

```py
rank_correlation, rank_correlation_pval = stats.spearmanr(df.iloc[:,df.columns.get_indexer(features)]) # calculate the rank correlation coefficient
rank_matrix = pd.DataFrame(rank_correlation,columns=corr_matrix.columns)
rank_correlation = rank_correlation[:,len(features)-1][:len(features)]
rank_correlation_pval = rank_correlation_pval[:,len(pred)-1][:len(features)]
print("\nRank Correlation p-value:\n"); print(rank_correlation_pval)

plt.subplot(121)
plot_corr(rank_matrix,'Rank Correlation Matrix',1.0,0.5)      # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

```py
Rank Correlation p-value:

[2.43279911e-02 1.34135205e-01 1.18844068e-10 2.71646948e-04
 2.11367755e-06 0.00000000e+00 3.29170847e-04] 
```

![å›¾ç‰‡](img/609c07d08205a2c92204da55d19ad62a.png)

è¿™äº›çŸ©é˜µå’Œçº¿å›¾è¡¨æ˜ï¼Œç§©ç›¸å…³ç³»æ•°ä¸ç›¸å…³ç³»æ•°ç›¸ä¼¼ï¼Œè¡¨æ˜éçº¿æ€§å¼‚å¸¸å€¼ä¸å¤ªå¯èƒ½å½±å“åŸºäºç›¸å…³æ€§çš„ç‰¹å¾æ’åºã€‚

å…³äºç§©ç›¸å…³ p å€¼ï¼Œ

+   åœ¨å…¸å‹çš„ alpha å€¼ä¸º 0.05 çš„æƒ…å†µä¸‹ï¼Œåªæœ‰è„†æ€§ä¸äº§é‡çš„ç§©ç›¸å…³æ²¡æœ‰é€šè¿‡å‡è®¾æ£€éªŒï¼›å› æ­¤ï¼Œä¸ 0.0 å¹¶æ— æ˜¾è‘—å·®å¼‚ã€‚

æŸ¥çœ‹ç›¸å…³ç³»æ•°å’Œç§©ç›¸å…³ç³»æ•°ä¹‹é—´çš„å·®å¼‚æ˜¯æœ‰ç”¨çš„ã€‚

```py
plt.subplot(121)                                              # plot correlation matrix with significance colormap
diff = corr_matrix.values - rank_matrix.values
diff_matrix = pd.DataFrame(diff,columns=corr_matrix.columns)
plot_corr(diff_matrix,'Correlation - Rank Correlation',0.1,0.1) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

corr_diff = correlation - rank_correlation

plt.subplot(122)
feature_rank_plot(features,corr_diff,-0.20,0.20,0.0,'Correlation Coefficient - Rank Correlation Coefficient','Correlation Diffference',0.1)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/9010e510d3b644ed069447aad1564797.png)

è¿™é‡Œæœ‰ä¸€äº›æœ‰è¶£çš„è§‚å¯Ÿï¼š

+   å½“æˆ‘ä»¬å‡å°‘çº¿æ€§å’Œå¼‚å¸¸å€¼çš„å½±å“æ—¶ï¼Œå­”éš™ç‡å’Œç»ç’ƒå…‰æ³½åå°„ç‡ä¸äº§é‡çš„ç›¸å…³æ€§æé«˜

+   å½“æˆ‘ä»¬å‡å°‘çº¿æ€§å’Œå¼‚å¸¸å€¼çš„å½±å“æ—¶ï¼Œè„†æ€§ä¸äº§é‡çš„ç›¸å…³æ€§å˜å·®

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‰€æœ‰è¿™äº›æ–¹æ³•éƒ½æ˜¯ä¸€æ¬¡è€ƒè™‘ä¸€ä¸ªç‰¹å¾ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥è€ƒè™‘åŒæ—¶è€ƒè™‘æ‰€æœ‰ç‰¹å¾çš„æ–¹æ³•ï¼Œä»¥â€œéš”ç¦»â€æ¯ä¸ªç‰¹å¾çš„å½±å“ã€‚

## åç›¸å…³ç³»æ•°

è¿™æ˜¯ä¸€ä¸ªæ§åˆ¶æ‰€æœ‰å‰©ä½™å˜é‡å½±å“çš„çº¿æ€§ç›¸å…³ç³»æ•°ï¼Œ$\rho_{XY.Z}$ å’Œ $\rho_{YX.Z}$ æ˜¯åœ¨æ§åˆ¶ $Z$ å $X$ å’Œ $Y$ã€$Y$ å’Œ $X$ ä¹‹é—´çš„åç›¸å…³ã€‚

è¦è®¡ç®—åœ¨ç»™å®š $Z_i, \forall \quad i = 1,\ldots, m-1$ å‰©ä½™ç‰¹å¾çš„æƒ…å†µä¸‹ $X$ å’Œ $Y$ ä¹‹é—´çš„åç›¸å…³ç³»æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æ­¥éª¤ï¼š

1.  æ‰§è¡Œçº¿æ€§ã€æœ€å°äºŒä¹˜å›å½’ä»¥ä» $Z_i, \forall \quad i = 1,\ldots, m-1$ é¢„æµ‹ $X$ã€‚$X$ é€šè¿‡é¢„æµ‹å˜é‡è¿›è¡Œå›å½’ä»¥è®¡ç®—ä¼°è®¡å€¼ï¼Œ$X^*$

1.  åœ¨æ­¥éª¤ #1 ä¸­è®¡ç®—æ®‹å·®ï¼Œ$X-X^*$ï¼Œå…¶ä¸­ $X^* = f(Z_{1,\ldots,m-1})$ï¼Œçº¿æ€§å›å½’æ¨¡å‹

1.  æ‰§è¡Œçº¿æ€§ã€æœ€å°äºŒä¹˜å›å½’ä»¥ä» $Z_i, \forall \quad i = 1,\ldots, m-1$ é¢„æµ‹ $Y$ã€‚$Y$ é€šè¿‡é¢„æµ‹å˜é‡è¿›è¡Œå›å½’ä»¥è®¡ç®—ä¼°è®¡å€¼ï¼Œ$Y^*$

1.  åœ¨æ­¥éª¤ #3 ä¸­è®¡ç®—æ®‹å·®ï¼Œ$Y-Y^*$ï¼Œå…¶ä¸­ $Y^* = f(Z_{1,\ldots,m-1})$ï¼Œçº¿æ€§å›å½’æ¨¡å‹

1.  è®¡ç®—æ­¥éª¤ #2 å’Œ #4 çš„æ®‹å·®ä¹‹é—´çš„ç›¸å…³ç³»æ•°ï¼Œ$\rho_{X-X^*,Y-Y^*}$

åç›¸å…³ç³»æ•°æä¾›äº†åœ¨æ§åˆ¶ $Z$ ç­‰å…¶ä»–ç‰¹å¾å¯¹ $X$ å’Œ $Y$ çš„å½±å“ä¸‹ï¼Œ$X$ å’Œ $Y$ ä¹‹é—´çº¿æ€§å…³ç³»çš„åº¦é‡ã€‚æˆ‘ä»¬ä½¿ç”¨ä¹‹å‰å£°æ˜çš„å‡½æ•°ï¼Œæ¥è‡ª Fabian Pedregosa-Izquierdoï¼Œf@bianp.netã€‚åŸå§‹ä»£ç åœ¨ GitHub ä¸Šï¼Œ[`git.io/fhyHB`](https://git.io/fhyHB)ã€‚

è¦ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œæˆ‘ä»¬å¿…é¡»å‡è®¾ï¼š

1.  éœ€è¦æ¯”è¾ƒçš„ä¸¤ä¸ªå˜é‡ï¼Œ$X$ å’Œ $Y$

1.  éœ€è¦æ§åˆ¶çš„å…¶å®ƒå˜é‡ï¼Œ$Z_{1,\ldots,m-2}$

1.  æ‰€æœ‰å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»

1.  æ²¡æœ‰æ˜¾è‘—å¼‚å¸¸å€¼

1.  å˜é‡ä¹‹é—´çš„å¤§è‡´åŒå˜é‡æ­£æ€æ€§

æˆ‘ä»¬çš„æƒ…å†µç›¸å½“ä¸é”™ï¼Œä½†æœ‰ä¸€äº›åç¦»åŒå˜é‡æ­£æ€æ€§çš„æƒ…å†µã€‚æˆ‘ä»¬å¯ä»¥è€ƒè™‘é«˜æ–¯å•å˜é‡å˜æ¢æ¥æ”¹è¿›è¿™ä¸€ç‚¹ã€‚æ­¤é€‰é¡¹å°†åœ¨ç¨åæä¾›ã€‚

```py
partial_correlation = partial_corr(df.iloc[:,df.columns.get_indexer(features)]) # calculate the partial correlation coefficients
partial_matrix = pd.DataFrame(partial_correlation,columns=corr_matrix.columns)
partial_correlation = partial_correlation[:,len(features)-1][:len(features)] # extract a single row and remove production with itself 

plt.subplot(121)
plot_corr(partial_matrix,'Partial Correlation Matrix',1.0,0.5) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/11649099249c19a8d0134ee44bd96661.png)

ç°åœ¨ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å…³äºæ¯ä¸ªé¢„æµ‹ç‰¹å¾ç‹¬ç‰¹è´¡çŒ®çš„è®¸å¤šæ–°äº‹ç‰©ï¼

+   å­”éš™ç‡å’Œæ¸—é€ç‡å½¼æ­¤ä¹‹é—´é«˜åº¦ç›¸å…³ï¼Œå› æ­¤å®ƒä»¬å—åˆ°ä¸¥é‡æƒ©ç½š

+   å£°æ³¢é˜»æŠ—å’Œé•œè´¨ä½“åå°„ç‡çš„ç»å¯¹å…³è”æ€§å¢åŠ ï¼Œåæ˜ äº†å®ƒä»¬ç‹¬ç‰¹çš„è´¡çŒ®

+   æ€»æœ‰æœºç¢³ç¿»è½¬äº†ç¬¦å·ï¼å½“æˆ‘ä»¬æ§åˆ¶æ‰€æœ‰å…¶ä»–å˜é‡æ—¶ï¼Œå®ƒä¸äº§é‡å‘ˆè´Ÿç›¸å…³å…³ç³»ã€‚

é€šè¿‡åç›¸å…³ç³»æ•°ï¼Œæˆ‘ä»¬å·²ç»æ§åˆ¶äº†æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾å¯¹ç‰¹å®šé¢„æµ‹ç‰¹å¾å’Œå“åº”ç‰¹å¾çš„å½±å“ã€‚åŠåç›¸å…³ç³»æ•°è¿‡æ»¤æ‰æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾å¯¹åŸå§‹å“åº”å˜é‡çš„å½±å“ã€‚

## åŠåç›¸å…³ç³»æ•°

è¿™æ˜¯ä¸€ä¸ªæ§åˆ¶æ‰€æœ‰å‰©ä½™ç‰¹å¾ $Z$ å¯¹ $X$ çš„å½±å“çš„çº¿æ€§ç›¸å…³ç³»æ•°ï¼Œç„¶åè®¡ç®—æ®‹å·® $X^*-X$ å’Œ $Y$ ä¹‹é—´çš„ç›¸å…³ç³»æ•°ã€‚æ³¨æ„ï¼šæˆ‘ä»¬ä¸æ§åˆ¶ $Z$ ç‰¹å¾å¯¹å“åº”ç‰¹å¾ $Y$ çš„å½±å“ã€‚

è¦è®¡ç®—ç»™å®š $Z_i, \forall \quad i = 1,\ldots, m-1$ å‰©ä½™ç‰¹å¾ä¸‹çš„ $X$ å’Œ $Y$ ä¹‹é—´çš„åŠåç›¸å…³ç³»æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æ­¥éª¤ï¼š

1.  æ‰§è¡Œçº¿æ€§ã€æœ€å°äºŒä¹˜å›å½’ä»¥é¢„æµ‹ $X$ æ¥è‡ª $Z_i, \forall \quad i = 1,\ldots, m-1$ã€‚$X$ é€šè¿‡å‰©ä½™çš„é¢„æµ‹ç‰¹å¾è¿›è¡Œå›å½’ä»¥è®¡ç®—ä¼°è®¡å€¼ï¼Œ$X^*$

1.  åœ¨æ­¥éª¤ #1 ä¸­è®¡ç®—æ®‹å·®ï¼Œ$X-X^*$ï¼Œå…¶ä¸­ $X^* = f(Z_{1,\ldots,m-1})$ï¼Œçº¿æ€§å›å½’æ¨¡å‹

1.  è®¡ç®—æ­¥éª¤ #2 ä¸­æ®‹å·®ä¸ $Y$ å“åº”ç‰¹å¾çš„å…³è”ç³»æ•°ï¼Œ$\rho_{X-X^*,Y}$

åŠéƒ¨åˆ†ç›¸å…³ç³»æ•°æä¾›äº†åœ¨æ§åˆ¶ $Z$ï¼ˆå…¶ä»–é¢„æµ‹ç‰¹å¾ï¼‰å¯¹é¢„æµ‹ç‰¹å¾ $X$ çš„å½±å“çš„æƒ…å†µä¸‹ï¼Œ$X$ å’Œ $Y$ ä¹‹é—´çº¿æ€§å…³ç³»çš„åº¦é‡ã€‚æˆ‘ä»¬ä½¿ç”¨ä¹‹å‰å£°æ˜çš„éƒ¨åˆ†ç›¸å…³å‡½æ•°çš„ä¿®æ”¹ç‰ˆã€‚åŸå§‹ä»£ç åœ¨ GitHub ä¸Šï¼Œè§ [`git.io/fhyHB`](https://git.io/fhyHB)ã€‚

```py
semipartial_correlation = semipartial_corr(df.iloc[:,df.columns.get_indexer(features)])    # calculate the semi-partial correlation coefficients
semipartial_matrix = pd.DataFrame(semipartial_correlation,columns=corr_matrix.columns)
semipartial_correlation = semipartial_correlation[:,len(features)-1][:len(features)]    # extract a single row and remove production with itself

plt.subplot(121)
plot_corr(semipartial_matrix,'Semi-partial Correlation Matrix',1.0,0.5) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![_images/8288d5546785bae96f5c850214ddd5be4c148d6210b1b7d05ca2f86640b6b443.png](img/9bc790699675e3bdbd9e8ed625051fa1.png)

éœ€è¦è€ƒè™‘çš„æ›´å¤šä¿¡æ¯ï¼š

+   å­”éš™ç‡ã€æ¸—é€ç‡å’Œé•œè´¨ä½“åå°„ç‡æ˜¯æŒ‰æ­¤ç‰¹å¾æ’åæ–¹æ³•æœ€é‡è¦çš„

+   å…¶ä»–é¢„æµ‹ç‰¹å¾çš„ç›¸å…³æ€§éƒ½ç›¸å½“ä½

è¿™æ˜¯åœä¸‹æ¥æ€»ç»“æ‰€æœ‰å®šé‡æ–¹æ³•ç»“æœçš„ä¸é”™æ—¶æœºã€‚æˆ‘ä»¬å°†ä¸€èµ·ç»˜åˆ¶å®ƒä»¬ã€‚

```py
# plt.subplot(151)
# feature_rank_plot(features,covariance,-5000.0,5000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.1)

plt.subplot(131)
feature_rank_plot(features,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation',0.5)

plt.subplot(132)
feature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)

plt.subplot(133)
feature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)

# plt.subplot(155)
# feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.2, wspace=0.3, hspace=0.2); plt.show() 
```

![_images/8e1ed4eede67a45f3fd9cf848e281724927b517c683e5207ab574696b9952bde.png](img/9b87cdad7bc65f987a9f65bfe7febc3b.png)

æˆ‘è®¤ä¸ºæˆ‘ä»¬æ­£åœ¨æ”¶æ•›äºå­”éš™ç‡ã€æ¸—é€ç‡å’Œé•œè´¨ä½“åå°„ç‡ä½œä¸ºä¸ç”Ÿäº§ç›¸å…³çš„çº¿æ€§å…³ç³»ä¸­æœ€é‡è¦çš„å˜é‡ã€‚

## å¸¦æœ‰ç‰¹å¾è½¬æ¢çš„ç‰¹å¾æ’å

æœ‰è®¸å¤šåŸå› éœ€è¦è¿›è¡Œç‰¹å¾è½¬æ¢ï¼ˆå‚è§ç›¸å…³ç« èŠ‚ï¼‰ï¼Œå¦‚ä¸Šæ‰€è¿°ï¼Œå¯¹äºéƒ¨åˆ†å’ŒåŠéƒ¨åˆ†ç›¸å…³ï¼Œåˆ†å¸ƒè½¬æ¢å¯èƒ½æœ‰åŠ©äºç¬¦åˆåº¦é‡å‡è®¾ã€‚

+   ä½œä¸ºä¸€é¡¹ç»ƒä¹ å’Œæ£€æŸ¥ï¼Œè®©æˆ‘ä»¬å°†æ‰€æœ‰ç‰¹å¾æ ‡å‡†åŒ–å¹¶é‡å¤ä¹‹å‰è®¡ç®—å‡ºçš„å®šé‡æ–¹æ³•ã€‚æˆ‘ä»¬çŸ¥é“è¿™å°†å¯¹åæ–¹å·®äº§ç”Ÿå½±å“ï¼Œé‚£ä¹ˆå…¶ä»–æŒ‡æ ‡å‘¢ï¼Ÿ

æœ‰ä¸€äº›ä»£ç æ¥å®Œæˆè¿™é¡¹å·¥ä½œï¼Œä½†å¹¶ä¸å¤æ‚ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„åŒ…å«æ‰€æœ‰æ ‡å‡†åŒ–å˜é‡çš„ DataFrameã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸€äº›å°çš„ç¼–è¾‘ï¼ˆæ›´æ”¹ DataFrame åç§°ï¼‰å¹¶é‡ç”¨ä¸Šé¢çš„ä»£ç ã€‚æ‚¨å¯ä»¥é€‰æ‹©ï¼š

1.  æ ‡å‡†åŒ– - å¯¹åˆ†å¸ƒè¿›è¡Œä»¿å°„æ ¡æ­£ä»¥ä½¿å…¶å…·æœ‰ $\overline{x} = 0$ å’Œ $\sigma_x = 1.0$ã€‚

1.  æ­£æ€åˆ†æ•°è½¬æ¢ - å°†æ¯ä¸ªç‰¹å¾çš„åˆ†å¸ƒè½¬æ¢ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œå…·æœ‰ $\overline{x} = 0$ å’Œ $\sigma_x = 1.0$ çš„é«˜æ–¯å½¢çŠ¶ã€‚

ä½¿ç”¨æ­¤å—æ‰§è¡Œç‰¹å¾çš„ä»¿å°„æ ¡æ­£ï¼š

```py
# dfS = pd.DataFrame()                                        # affine correction, standardization to a mean of 0 and variance of 1 
# dfS['Well'] = df['Well'].values
# dfS['Por'] = GSLIB.affine(df['Por'].values,0.0,1.0)
# dfS['Perm'] = GSLIB.affine(df['Perm'].values,0.0,1.0)
# dfS['AI'] = GSLIB.affine(df['AI'].values,0.0,1.0)
# dfS['Brittle'] = GSLIB.affine(df['Brittle'].values,0.0,1.0)
# dfS['TOC'] = GSLIB.affine(df['TOC'].values,0.0,1.0)
# dfS['VR'] = GSLIB.affine(df['VR'].values,0.0,1.0)
# dfS['Prod'] = GSLIB.affine(df['Prod'].values,0.0,1.0)
# dfS.head() 
```

ä½¿ç”¨æ­¤å—æ‰§è¡Œç‰¹å¾çš„æ­£å¸¸åˆ†æ•°è½¬æ¢ï¼š

```py
dfS = pd.DataFrame()                                          # Gaussian transform, standardization to a mean of 0 and variance of 1 

for feature in features:
    dfS[feature],d1,d2 = geostats.nscore(df,feature)

dfS.head() 
```

|  | Por | Perm | AI | Brittle | TOC | VR | Prod |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | -0.964092 | -0.780664 | -0.285841 | 2.432379 | 0.312053 | 1.114651 | -1.780464 |
| 1 | -0.832725 | -0.378580 | 0.446827 | -0.195502 | -0.272809 | -0.325239 | -0.392079 |
| 2 | -0.312053 | -1.069155 | 1.722384 | 2.004654 | -0.272809 | 2.241403 | -0.832725 |
| 3 | 0.730638 | 1.325516 | -0.531604 | -0.590284 | 0.131980 | -0.325239 | 0.815126 |
| 4 | 0.698283 | 0.298921 | 0.365149 | -2.870033 | 1.047216 | -0.259823 | -0.531604 |

æ— è®ºé€‰æ‹©å“ªç§è½¬æ¢ï¼Œæ£€æŸ¥æ€»ç»“ç»Ÿè®¡ä¿¡æ¯éƒ½æ˜¯æœ€ä½³å®è·µã€‚

```py
dfS.describe()                                                # check the summary statistics 
```

|  | Por | Perm | AI | Brittle | TOC | VR | Prod |
| --- | --- | --- | --- | --- | --- | --- | --- |
| count | 200.000000 | 200.000000 | 2.000000e+02 | 2.000000e+02 | 200.000000 | 200.000000 | 2.000000e+02 |
| mean | -0.009700 | 0.010306 | 9.732356e-03 | 8.028717e-05 | 0.014152 | 0.017360 | 1.617223e-03 |
| std | 1.040456 | 1.005488 | 1.000221e+00 | 1.000278e+00 | 0.989223 | 1.000401 | 9.949811e-01 |
| min | -4.991462 | -3.355431 | -2.782502e+00 | -2.870033e+00 | -2.336891 | -2.899210 | -2.483589e+00 |
| 25% | -0.670577 | -0.647337 | -6.588985e-01 | -6.705770e-01 | -0.670577 | -0.651072 | -6.705770e-01 |
| 50% | 0.006267 | 0.006267 | 8.881784e-16 | 8.881784e-16 | 0.018807 | 0.006267 | 8.881784e-16 |
| 75% | 0.670577 | 0.678574 | 6.705770e-01 | 6.705770e-01 | 0.682378 | 0.682642 | 6.705770e-01 |
| max | 2.807034 | 2.807034 | 2.807034e+00 | 2.807034e+00 | 2.807034 | 2.807034 | 2.807034e+00 |

æˆ‘ä»¬è¿˜åº”è¯¥å†æ¬¡æ£€æŸ¥çŸ©é˜µæ•£ç‚¹å›¾ã€‚

+   å¦‚æœä½ è¿›è¡Œäº†æ­£æ€å¾—åˆ†è½¬æ¢ï¼Œä½ å·²ç»æ ‡å‡†åŒ–äº†å‡å€¼å’Œæ–¹å·®ï¼Œå¹¶çº æ­£äº†åˆ†å¸ƒçš„å•å˜é‡å½¢çŠ¶ï¼Œä½†åŒå˜é‡å…³ç³»ä»ç„¶åç¦»é«˜æ–¯ã€‚

```py
pairgrid = sns.PairGrid(dfS) # matrix scatter plots
pairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)
pairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle
pairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, 
                              shade = False, shade_lowest = False, alpha = 1.0, n_levels = 10)
pairgrid.add_legend()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/884680b3106f0f9bc10b64a1888d213dedcd55860acea49e6f5bd179d1604868.png](img/8568ba1fa581044cc577242f378dd1db.png)

è¿™æ˜¯å¸¦æœ‰æ ‡å‡†åŒ–å˜é‡çš„æ–° DataFrameã€‚ç°åœ¨æˆ‘ä»¬é‡å¤ä¹‹å‰çš„è®¡ç®—ã€‚

+   æˆ‘ä»¬è¿™æ¬¡å°†æ›´åŠ é«˜æ•ˆï¼Œå¹¶ä½¿ç”¨ç›¸å½“ç´§å‡‘çš„ä»£ç ã€‚

```py
stand_covariance = dfS.iloc[:,dfS.columns.get_indexer(features)].cov().iloc[len(features)-1,:len(features)]
stand_correlation = dfS.iloc[:,dfS.columns.get_indexer(features)].corr().iloc[len(features)-1,:len(features)]

stand_rank_correlation, stand_rank_correlation_pval = stats.spearmanr(dfS.iloc[:,dfS.columns.get_indexer(features)])
stand_rank_correlation = stand_rank_correlation[:,len(features)-1][:len(features)]
stand_partial_correlation = partial_corr(dfS.iloc[:,dfS.columns.get_indexer(features)]) # calculate the partial correlation coefficients
stand_partial_correlation = stand_partial_correlation[:,len(features)-1][:len(features)]
stand_semipartial_correlation = semipartial_corr(dfS.iloc[:,dfS.columns.get_indexer(features)])    # calculate the semi-partial correlation coefficients
stand_semipartial_correlation = stand_semipartial_correlation[:,len(features)-1][:len(features)] 
```

å¹¶é‡å¤ä¹‹å‰çš„æ±‡æ€»å›¾ã€‚

```py
# plt.subplot(2,5,1)
# feature_rank_plot(features,covariance,-5000.0,5000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.5)

plt.subplot(2,3,1)
feature_rank_plot(features,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation',0.5)

plt.subplot(2,3,2)
feature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)

plt.subplot(2,3,3)
feature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)

# plt.subplot(2,5,5)
# feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)

# plt.subplot(2,5,6)
# feature_rank_plot(features,stand_covariance,-1.0,1.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance of Standardized',0.5)

plt.subplot(2,3,4)
feature_rank_plot(features,stand_correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation of Standardized',0.5)

plt.subplot(2,3,5)
feature_rank_plot(features,stand_rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation of Standardized',0.5)

plt.subplot(2,3,6)
feature_rank_plot(features,stand_partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation of Standardized',0.5)

# plt.subplot(2,5,10)
# feature_rank_plot(features,stand_semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation of Standardized',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=2.2, wspace=0.3, hspace=0.3); plt.show() 
```

![_images/e6320b5768883e13feea467d4888f04edce7671ecd0ba7a92874bc94656cd1a2.png](img/8eeed3569969856a648e8f7bf97ddb8b.png)

ä½ å¯ä»¥è§‚å¯Ÿåˆ°ä»€ä¹ˆï¼š

+   åæ–¹å·®ç°åœ¨ç­‰äºç›¸å…³ç³»æ•°

+   åŠåç›¸å…³å¯¹ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆä»¿å°„ç›¸å…³æˆ–æ­£æ€å¾—åˆ†è½¬æ¢ï¼‰æ•æ„Ÿã€‚

## æ¡ä»¶ç»Ÿè®¡é‡

æˆ‘ä»¬å°†æŠŠäº•åˆ†æˆä½ã€ä¸­ã€é«˜äº§é‡ï¼Œå¹¶æ£€æŸ¥æ¡ä»¶ç»Ÿè®¡é‡çš„å·®å¼‚ã€‚

+   è¿™å°†æä¾›ä¸€ç§æ›´çµæ´»çš„æ–¹æ³•æ¥æ¯”è¾ƒæ¯ä¸ªç‰¹å¾ä¸ç”Ÿäº§ä¹‹é—´çš„å…³ç³»

+   å¦‚æœæ¡ä»¶ç»Ÿè®¡é‡å‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œé‚£ä¹ˆè¯¥ç‰¹å¾æ˜¯æœ‰ä¿¡æ¯çš„

æˆ‘ä»¬å°†åˆ¶ä½œä¸€ä¸ªåŒ…å«æ‰€æœ‰ç‰¹å¾çš„å•ä¸€å°æç´å›¾

+   æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç”¨äºç”Ÿäº§çš„åˆ†ç±»ç‰¹å¾ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨æ­¤ä»£ç å°†ç”Ÿäº§æˆªæ–­ä¸ºé«˜æˆ–ä½ï¼Œ

```py
df['tProd'] = np.where(df['Prod']>=4000, 'High', 'Low') 
```

+   æˆ‘ä»¬éœ€è¦æ ‡å‡†åŒ–æ‰€æœ‰ç‰¹å¾ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿå®ƒä»¬çš„ç›¸å¯¹å·®å¼‚

```py
x = df[['Por','Perm','AI','Brittle','TOC','VR']]
x_stand = (x - x.mean()) / (x.std()) 
```

+   æ­¤ä»£ç å°†ç‰¹å¾æå–åˆ°æ–°çš„ DataFrame â€˜xâ€™ ä¸­ï¼Œç„¶åå¯¹æ¯ä¸ªåˆ—ï¼ˆç‰¹å¾ï¼‰åº”ç”¨æ ‡å‡†åŒ–æ“ä½œ

+   ç„¶åæˆ‘ä»¬å°†æˆªæ–­çš„ç”Ÿäº§ç‰¹å¾æ·»åŠ åˆ°æ ‡å‡†åŒ–ç‰¹å¾ä¸­

```py
x = pd.concat([df['tProd'],x_stand.iloc[:,0:6]],axis=1) 
```

+   æˆ‘ä»¬å¯ä»¥åº”ç”¨ melt å‘½ä»¤æ¥é€†è½¬ DataFrame

```py
x = pd.melt(x,id_vars="tProd",var_name="Predictors",value_name='Standardized_Value') 
```

+   æˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªé•¿ DataFrameï¼ˆ6 ä¸ªç‰¹å¾ x 200 ä¸ªæ ·æœ¬ = 12000 è¡Œï¼‰ï¼ŒåŒ…å«ï¼š

    +   ç”Ÿäº§ï¼šä½æˆ–é«˜

    +   ç‰¹å¾ï¼šPor, Perm, AI, Brittle, TOC æˆ– VR

    +   æ ‡å‡†åŒ–ç‰¹å¾å€¼

æˆ‘ä»¬å¯ä»¥æ„å»ºæˆ‘ä»¬çš„å°æç´å›¾

+   x æ˜¯æˆ‘ä»¬çš„é¢„æµ‹ç‰¹å¾

+   y æ˜¯é¢„æµ‹ç‰¹å¾çš„æ ‡å‡†åŒ–å€¼ï¼ˆç°åœ¨éƒ½åœ¨ä¸€åˆ—ä¸­ï¼‰

+   hue æ˜¯ç”Ÿäº§æ°´å¹³é«˜æˆ–ä½

+   split æ˜¯ Trueï¼Œå› æ­¤å°æç´å›¾è¢«åˆ†æˆä¸¤åŠ

+   å†…éƒ¨æ˜¯ P25ã€P50 å’Œ P75 çš„å››åˆ†ä½æ•°ï¼Œç”¨è™šçº¿è¡¨ç¤º

```py
threshold = 2000.0

df['tProd'] = np.where(df[resp]>=threshold, 'High', 'Low')       # make a high and low production categorical feature

x_temp = df[pred]
x_temp_stand = (x_temp - x_temp.mean()) / (x_temp.std())      # standardization by feature
x_temp = pd.concat([df['tProd'],x_temp_stand.iloc[:,0:len(pred)]],axis=1) # add the production categorical feature to the DataFrame
x_temp = pd.melt(x_temp,id_vars="tProd",var_name="Predictor Feature",value_name='Standardized Predictor Feature') # unpivot the DataFrame

plt.subplot(111)
sns.violinplot(x="Predictor Feature", y="Standardized Predictor Feature", hue="tProd", data=x_temp,split=True, inner="quart", palette="Set2")
plt.xticks(rotation=90); plt.title('Conditional Distributions by Production')
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/39636127cd144d1449668e2c0eee3c8e122afe9891a46cc3e4610d38ff16390a.png](img/97e5b28483156d276669ee0ba4b67c7a.png)

ä»å°æç´å›¾ä¸­æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°å­”éš™ç‡ã€æ¸—é€ç‡å’Œæ€»æœ‰æœºç¢³ï¼ˆTOCï¼‰åœ¨ä½äº§é‡äº•å’Œé«˜äº§é‡äº•ä¹‹é—´çš„æ¡ä»¶åˆ†å¸ƒå˜åŒ–æœ€å¤§ã€‚

æˆ‘ä»¬å¯ä»¥ç”¨æ¡ä»¶åˆ†å¸ƒçš„ç®±çº¿å›¾æ¥æ›¿æ¢è¿™ä¸ªå›¾ã€‚

+   ç®±çº¿å›¾å’Œé¡»å›¾æé«˜äº†æˆ‘ä»¬è§‚å¯Ÿæ¡ä»¶ P25ã€P75 ä»¥åŠ Tukey å¼‚å¸¸å€¼æµ‹è¯•ä¸Šä¸‹é™çš„èƒ½åŠ›ã€‚

```py
plt.subplot(111)
sns.boxplot(x="Predictor Feature", y="Standardized Predictor Feature", hue="tProd", data=x_temp)
plt.xticks(rotation=90)
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2)
plt.show()

df = df.drop(['tProd'], axis = 1) 
```

![_images/2121ce938f6088b2c90acec866c9ff94b488c33826983e13a20d4e1721016352.png](img/5cb6b0c21042f1de1888484ca5e64a81.png)

ä»æ¡ä»¶ç®±çº¿å›¾ä¸­æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°å­”éš™ç‡ã€æ¸—é€ç‡å’Œæ€»æœ‰æœºç¢³ï¼ˆTOCï¼‰åœ¨ä½äº§é‡äº•å’Œé«˜äº§é‡äº•ä¹‹é—´çš„æ¡ä»¶åˆ†å¸ƒå˜åŒ–æœ€å¤§ã€‚

+   æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°å­”éš™ç‡ã€æ¸—é€ç‡ï¼ˆä¸Šå°¾ï¼‰ã€æ€»æœ‰æœºç¢³ï¼ˆä¸‹å°¾ï¼‰å’Œç»ç’ƒè´¨åå°„ç‡çš„å¼‚å¸¸å€¼ã€‚

## æ–¹å·®è†¨èƒ€å› å­ï¼ˆVIFï¼‰

è¿™æ˜¯ä¸€ä¸ªè¡¡é‡é¢„æµ‹ç‰¹å¾ï¼ˆ$X_i$ï¼‰ä¸æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾ï¼ˆ$X_j, \forall j \ne i$ï¼‰ä¹‹é—´çº¿æ€§å¤šé‡å…±çº¿æ€§ç¨‹åº¦çš„æŒ‡æ ‡ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬é’ˆå¯¹æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾ï¼Œå¯¹ç»™å®šé¢„æµ‹ç‰¹å¾è¿›è¡Œçº¿æ€§å›å½’è®¡ç®—ã€‚

$$ X_i = \sum_{j, j \ne i}^m X_j + \epsilon $$

ä»è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šç¡®å®šç³»æ•° $RÂ²$ï¼Œä¹Ÿç§°ä¸ºæ–¹å·®è§£é‡Šã€‚

ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—æ–¹å·®è†¨èƒ€å› å­ï¼ˆVIFï¼‰å¦‚ä¸‹ï¼š

$$ VIF = \frac{1}{1 - RÂ²} $$

```py
vif_values = []
for i in range(df[pred].values.shape[1]):
    vif_values.append(variance_inflation_factor(df[pred].values, i))

vif_values = np.asarray(vif_values)
indices = np.argsort(vif_values)[::-1]                  # find indices for descending order

plt.subplot(111)                                        # plot the feature importance 
plt.title("Variance Inflation Factor")
plt.bar(range(df[pred].values.shape[1]), vif_values[indices],edgecolor = 'black',
       color="darkorange",alpha=0.6, align="center")
plt.xticks(np.linspace(0,len(pred)-1,len(pred)), np.array(pred)[indices].tolist(),rotation=90); 

plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks

plt.xlim([-0.5, x.shape[1]-0.5]); plt.yscale('log');
plt.xlabel('Predictor Feature'); plt.ylabel('Variance Inflation Factor')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)
plt.show() 
```

![_images/eaaa1c71dedcaed7ee83a0dfce8fdfb50ad17facc2a2c38a51caf6c6481cb547.png](img/5692efa445f9167d3d0d5a6d8f3e8bcb.png)

ç»ç’ƒè´¨åå°„ç‡å…·æœ‰æœ€å¤šçš„çº¿æ€§å†—ä½™ï¼Œè€Œæ¸—é€ç‡ä¸å…¶ä»–é¢„æµ‹ç‰¹å¾çš„çº¿æ€§å†—ä½™æœ€å°‘ã€‚

+   è®°ä½ï¼Œé«˜æ–¹å·®è†¨èƒ€å› å­æ˜¯ä¸å¥½çš„ã€‚

+   è®°ä½ï¼Œæ–¹å·®è†¨èƒ€å› å­ï¼ˆVIFï¼‰å¹¶ä¸æ•´åˆæ¯ä¸ªé¢„æµ‹ç‰¹å¾ä¸å“åº”ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚

+   é€šå¸¸ï¼Œæ–¹å·®è†¨èƒ€å› å­è¢«ç”¨ä½œç­›é€‰å·¥å…·ï¼Œä»¥å»é™¤ä¸å…¶ä»–é¢„æµ‹ç‰¹å¾æœ‰è¿‡å¤šå†—ä½™çš„ç‰¹å¾ã€‚

ç°åœ¨æˆ‘ä»¬æ¥ä»‹ç»åŸºäºæ¨¡å‹çš„ç‰¹å¾æ’åºæ–¹æ³•ã€‚

## $B$ ç³»æ•° / è´å¡”æƒé‡

æˆ‘ä»¬è¿˜å¯ä»¥è€ƒè™‘ $B$ ç³»æ•°ã€‚è¿™äº›æ˜¯ä¸å¯¹å˜é‡è¿›è¡Œæ ‡å‡†åŒ–çš„çº¿æ€§å›å½’ç³»æ•°ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ SciPy åŒ…ä¸­å¯ç”¨çš„çº¿æ€§å›å½’æ–¹æ³•ã€‚

$Y$ çš„ä¼°è®¡å™¨ç®€å•åœ°æ˜¯çº¿æ€§æ–¹ç¨‹ï¼š

\begin{equation} Y^* = \sum_{i=1}^{m} b_i X_i + c \end{equation}

$b_i$ ç³»æ•°é€šè¿‡æœ€å°åŒ–ä¼°è®¡å€¼ $Y^*$ ä¸è®­ç»ƒæ•°æ®é›†ä¸­å€¼ $Y$ ä¹‹é—´çš„å¹³æ–¹è¯¯å·®æ¥æ±‚è§£ã€‚

```py
reg = LinearRegression()                                      # instantiate a linear regression model 
reg.fit(df[pred],df[resp])                                    # train the model
b = reg.coef_

plt.subplot(111)
feature_rank_plot(pred,b,-1000.0,1000.0,0.0,'Feature Ranking, B Coefficients with ' + resp,r'Linear Regression Slope, $b_1$',0.5) 
```

![_images/ddb18df2725eafa50c95853e6bec78f8aa249726ae339ee0d874966578afdf95.png](img/7da7b523b4ae881b20a379148ea78eea.png)

è¾“å‡ºæ˜¯ $b$ ç³»æ•°ï¼ŒæŒ‰æˆ‘ä»¬çš„ç‰¹å¾ä» $b_i, i = 1,\ldots,n$ æ’åºï¼Œç„¶åæ˜¯æˆªè· $c$ï¼Œæˆ‘å·²ç»ç§»é™¤ä»¥é¿å…æ··æ·†ã€‚

+   æˆ‘ä»¬çœ‹åˆ°äº†äººå·¥æ™ºèƒ½å’Œ TOC çš„è´Ÿè´¡çŒ®

+   ç»“æœå¯¹é¢„æµ‹ç‰¹å¾æ–¹å·®çš„å¹…åº¦éå¸¸æ•æ„Ÿã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡å¤„ç†æ ‡å‡†åŒ–ç‰¹å¾æ¥æ¶ˆé™¤è¿™ç§æ•æ„Ÿæ€§ã€‚

## $\beta$ ç³»æ•° / Beta Weights

$\beta$ ç³»æ•°æ˜¯åœ¨æˆ‘ä»¬å°†é¢„æµ‹å’Œå“åº”ç‰¹å¾æ ‡å‡†åŒ–ä¸ºæ–¹å·®ä¸º 1 ä¹‹åè®¡ç®—çš„çº¿æ€§å›å½’ç³»æ•°ã€‚

$$ \sigmaÂ²_{X^s_i} = 1.0 \quad \forall \quad i = 1,\ldots,m, \quad \sigmaÂ²_{Y^s} = 1.0 $$

$Y^s$ æ ‡å‡†åŒ–çš„ä¼°è®¡å™¨åªæ˜¯ä¸€ä¸ªç®€å•çš„çº¿æ€§æ–¹ç¨‹ï¼š

$$ Y^{s*} = \sum_{i=1}^{m} \beta_i X^s_i + c $$

å¾ˆæ–¹ä¾¿çš„æ˜¯ï¼Œæˆ‘ä»¬åˆšåˆšåˆšåˆšå°†æ‰€æœ‰å˜é‡æ ‡å‡†åŒ–ï¼Œä½¿å…¶æ–¹å·®ä¸º 1.0ï¼ˆè§ä¸Šæ–‡ï¼‰ã€‚è®©æˆ‘ä»¬å†æ¬¡ä½¿ç”¨ç›¸åŒçš„çº¿æ€§å›å½’æ–¹æ³•åœ¨æ ‡å‡†åŒ–ç‰¹å¾ä¸Šå¾—åˆ° $\beta$ ç³»æ•°ã€‚

```py
reg = LinearRegression()
reg.fit(dfS[pred],dfS[resp])
beta = reg.coef_

plt.subplot(111)
feature_rank_plot(pred,beta,-1.0,1.0,0.0,r'Feature Ranking, $\beta$ Coefficients with ' + resp,r'Standardized Linear Regression Slope, $b_1$',0.5) 
```

![å›¾ç‰‡](img/63a59eb20ac3161240452ef8f6fcd97a.png)

ä¸€äº›è§‚å¯Ÿç»“æœï¼š

+   $b$ å’Œ $\beta$ ç³»æ•°çš„å·®å¼‚å¹¶ä¸ä»…ä»…æ˜¯æ’åæŒ‡æ ‡ä¸Šçš„å¸¸æ•°ç¼©æ”¾ï¼Œå› ä¸ºçº¿æ€§æ¨¡å‹ç³»æ•°å¯¹ç‰¹å¾çš„å–å€¼èŒƒå›´å’Œå¤§å°ä¹Ÿå¾ˆæ•æ„Ÿã€‚

+   åœ¨ä¼°è®¡ç”Ÿäº§æ—¶ï¼Œ$\beta$ ç³»æ•°ã€å­”éš™ç‡ã€å£°é˜»æŠ—å’Œæ€»æœ‰æœºç¢³å…·æœ‰æ›´é«˜çš„æ’å

## ç‰¹å¾é‡è¦æ€§

ä¸åŒçš„æœºå™¨å­¦ä¹ æ–¹æ³•æä¾›äº†ç‰¹å¾é‡è¦æ€§çš„åº¦é‡ï¼Œä¾‹å¦‚å†³ç­–æ ‘é€šè¿‡åŒ…å«æ¯ä¸ªç‰¹å¾æ¥å‡å°‘å‡æ–¹è¯¯å·®ï¼Œæ€»ç»“å¦‚ä¸‹ï¼š

$$ FI(x) = \sum_{t \in T_f} \frac{N_t}{N} \Delta_{MSE_t} $$

å…¶ä¸­ $T_f$ æ˜¯æ‰€æœ‰ä»¥ç‰¹å¾ $x$ ä½œä¸ºåˆ†å‰²çš„èŠ‚ç‚¹ï¼Œ$N_t$ æ˜¯è¾¾åˆ°èŠ‚ç‚¹ $t$ çš„è®­ç»ƒæ ·æœ¬æ•°é‡ï¼Œ$N$ æ˜¯æ•°æ®é›†ä¸­æ ·æœ¬çš„æ€»æ•°ï¼Œ$\Delta_{MSE_t}$ æ˜¯ $t$ åˆ†å‰²å¸¦æ¥çš„ MSE å‡å°‘ã€‚

æ³¨æ„ï¼Œç‰¹å¾é‡è¦æ€§å¯ä»¥åƒä¸Šé¢çš„ MSE ä¸€æ ·ï¼Œä»¥ç±»ä¼¼çš„æ–¹å¼è®¡ç®—åˆ†ç±»æ ‘ä¸­çš„ **Gini ä¸çº¯åº¦** çš„æƒ…å†µã€‚

è®©æˆ‘ä»¬çœ‹çœ‹æ‹Ÿåˆåˆ°æˆ‘ä»¬æ•°æ®çš„éšæœºæ£®æ—å›å½’æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ã€‚

+   æˆ‘ä»¬ä½¿ç”¨é»˜è®¤è¶…å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªéšæœºæ£®æ—ã€‚è¿™å¯¼è‡´æˆ‘ä»¬çš„æ£®æ—ä¸­æ— é™å¤æ‚ï¼Œè¿‡æ‹Ÿåˆçš„æ ‘ã€‚è¿™äº›æ ‘çš„å¹³å‡å¤„ç†äº†è¿‡æ‹Ÿåˆé—®é¢˜ã€‚

+   ç„¶åï¼Œæˆ‘ä»¬è®­ç»ƒæˆ‘ä»¬çš„éšæœºæ£®æ—å¹¶æå–ç‰¹å¾é‡è¦æ€§ï¼Œè®¡ç®—ä¸ºæ£®æ—ä¸­æ‰€æœ‰æ ‘ä¸Šçš„é¢„æœŸç‰¹å¾é‡è¦æ€§ã€‚

+   æˆ‘ä»¬è¿˜å¯ä»¥ä»æ£®æ—ä¸­çš„æ‰€æœ‰æ ‘ä¸­æå–ç‰¹å¾é‡è¦æ€§ï¼Œå¹¶ä½¿ç”¨æ ‡å‡†å·®æ¥æ€»ç»“ï¼Œä»¥è¯„ä¼°æˆ‘ä»¬ç‰¹å¾é‡è¦æ€§åº¦é‡çš„é²æ£’æ€§å’Œä¸ç¡®å®šæ€§ã€‚

æƒ³äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æˆ‘å…³äº[éšæœºæ£®æ—](https://www.youtube.com/watch?v=m5_wk310fho&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=39)é¢„æµ‹æœºå™¨å­¦ä¹ çš„è®²åº§ã€‚

```py
# Code modified from https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization
lab_enc = preprocessing.LabelEncoder(); Y_encoded = lab_enc.fit_transform(Y) # this removes an encoding error 

random_forest = RandomForestRegressor()                 # instantiate the random forest 
random_forest = random_forest.fit(x,np.ravel(Y_encoded)) # fit the random forest
importance_rank = random_forest.feature_importances_    # extract the expected feature importances

importance_rank_stand = importance_rank/np.max(importance_rank)                          # calculate relative mutual information

std = np.std([tree.feature_importances_ for tree in random_forest.estimators_],axis=0) # calculate stdev over trees
indices = np.argsort(importance_rank)[::-1]             # find indices for descending order

plt.subplot(111)                                        # plot the feature importance 
plt.title("Random Forest-based Feature importances")
plt.bar(range(x.shape[1]), importance_rank[indices],edgecolor = 'black',
       color="darkorange",alpha = 0.6, yerr=std[indices], align="center")
plt.xticks(range(x.shape[1]), x.columns[indices],rotation=90)
plt.xlim([-0.5, x.shape[1]-0.5]); 
plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks
plt.ylim([0.,1.0])
plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)
plt.show() 
```

![å›¾ç‰‡](img/4221de8488fdf2ce8f7716c22e1ea9d8.png)

æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨åŸºäºæ¨¡å‹çš„æ–¹æ³•åšæ›´å¤šçš„äº‹æƒ…ã€‚æˆ‘ä»¬å°†å®é™…æµ‹è¯•æ¨¡å‹ä»¥è¯„ä¼°æ¯ä¸ªé¢„æµ‹ç‰¹å¾å¢é‡å½±å“ï¼æˆ‘ä»¬å°†å°è¯•ä½¿ç”¨é€’å½’ç‰¹å¾æ¶ˆé™¤æ³•æ¥åšè¿™ä¸ªå®éªŒã€‚

è®©æˆ‘ä»¬ç»˜åˆ¶$B$å’Œ$\beta$ç³»æ•°çš„ç»“æœï¼Œå¹¶ä¸ä¹‹å‰çš„ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚

```py
plt.subplot(231)
feature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)

plt.subplot(232)
feature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)

plt.subplot(234)
feature_rank_plot(pred,b[0:len(pred)],-1000.0,1000.0,0.0,'Feature Ranking, B Coefficients with ' + resp,'B Coefficients',0.5)

plt.subplot(235)
feature_rank_plot(pred,beta[0:len(pred)],-1.0,1.0,0.0,'Feature Ranking, Beta Coefficients with ' + resp,'Beta Coefficients',0.5)

plt.subplot(236)
feature_rank_plot(pred,importance_rank_stand,0.0,1.0,0.0,'Feature Ranking, Feature Importance with ' + resp,'Standardized Feature Importance',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=2.2, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/9b736db6a7b146115943a1775ce2cc0d.png)

## äº’ä¿¡æ¯

äº’ä¿¡æ¯æ˜¯ä¸€ç§æ³›åŒ–æ–¹æ³•ï¼Œé‡åŒ–äº†ä¸¤ä¸ªç‰¹å¾ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ã€‚

+   é‡åŒ–äº†ä»è§‚å¯Ÿä¸€ä¸ªç‰¹å¾ä¸­è·å–å…³äºå¦ä¸€ä¸ªç‰¹å¾çš„ä¿¡æ¯é‡

+   é¿å…äº†å¯¹å…³ç³»å½¢å¼çš„ä»»ä½•å‡è®¾ï¼ˆä¾‹å¦‚ï¼Œæ²¡æœ‰çº¿æ€§å…³ç³»çš„å‡è®¾ï¼‰

+   å°†è”åˆæ¦‚ç‡ä¸è¾¹ç¼˜æ¦‚ç‡çš„ä¹˜ç§¯è¿›è¡Œæ¯”è¾ƒ

å¯¹äºç¦»æ•£æˆ–åˆ†ç®±çš„è¿ç»­ç‰¹å¾$X$å’Œ$Y$ï¼Œäº’ä¿¡æ¯è®¡ç®—å¦‚ä¸‹ï¼š

$$ I(X;Y) = \sum_{y \in Y} \sum_{x \in X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) $$

å›å¿†ï¼Œç»™å®š$X$å’Œ$Y$ä¹‹é—´çš„ç‹¬ç«‹æ€§ï¼š

$$ P_{X,Y}(x,y) = P_X(x) \cdot P_Y(y) $$

å› æ­¤ï¼Œå¦‚æœä¸¤ä¸ªç‰¹å¾æ˜¯ç‹¬ç«‹çš„ï¼Œé‚£ä¹ˆ$log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) = 0$

è”åˆæ¦‚ç‡$P_{X,Y}(x,y)$æ˜¯æ€»å’Œçš„åŠ æƒé¡¹ï¼Œå¹¶å¼ºåˆ¶å°é—­ã€‚

+   è”åˆåˆ†å¸ƒä¸­å¯†åº¦æ›´å¤§çš„éƒ¨åˆ†å¯¹äº’ä¿¡æ¯åº¦é‡æœ‰æ›´å¤§çš„å½±å“

å¯¹äºè¿ç»­ï¼ˆä¸”æœªåˆ†ç®±ï¼‰çš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨ç§¯åˆ†å½¢å¼ã€‚

$$ I(X;Y) = \int_{Y} \int_{X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) dx dy $$

æˆ‘ä»¬å¯ä»¥é€šè¿‡å‘½ä»¤è·å–æŒ‰é‡è¦æ€§é™åºæ’åˆ—çš„ç´¢å¼•åˆ—è¡¨

```py
indices = np.argsort(importances)[::-1] 
```

åˆ‡ç‰‡ä¼šåè½¬é¡ºåºï¼Œä»¥ç‰¹å¾é‡è¦æ€§çš„é™åºæ’åˆ—ã€‚

```py
x_df = df.loc[:,pred]                            # separate DataFrames for predictor and response features
y_df = df.loc[:,resp]

mi = mutual_info_regression(x_df,np.ravel(y_df))              # calculate mutual information
mi /= np.max(mi)                                        # calculate relative mutual information

indices = np.argsort(mi)[::-1]                          # find indices for descending order

print("Feature ranking:")                               # write out the feature importances
for f in range(x.shape[1]):
    print("%d. feature %s = %f" % (f + 1, x.columns[indices][f], mi[indices[f]]))

plt.subplot(111)                                        # plot the relative mutual information 
plt.title("Mutual Information")
plt.bar(range(x.shape[1]), mi[indices],edgecolor = 'black',
       color="darkorange",alpha=0.6,align="center")
plt.xticks(range(x.shape[1]), x.columns[indices],rotation=90)
plt.xlim([-0.5, x.shape[1]-0.5]); plt.ylim([0,1.3])
plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks
plt.xlabel('Predictor Feature'); plt.ylabel('Mutual Information')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)
plt.show() 
```

```py
Feature ranking:
1\. feature Por = 1.000000
2\. feature Perm = 0.345842
3\. feature TOC = 0.272418
4\. feature Brittle = 0.073310
5\. feature AI = 0.059024
6\. feature VR = 0.000000 
```

![å›¾ç‰‡](img/d12dd6dd61aa76402a54c096acd0768b.png)

### è€ƒè™‘åˆ°ç›¸å…³æ€§å’Œå†—ä½™çš„äº’ä¿¡æ¯

æ ‡å‡†çš„æœ€å¤§ç›¸å…³æ€§-æœ€å°å†—ä½™ï¼ˆMRMRï¼‰ç›®æ ‡å‡½æ•°è€ƒè™‘é¢„æµ‹ç‰¹å¾çš„ä¸€ä¸ªå­é›†ï¼Œå³ï¼Œå°†é¢„æµ‹ç‰¹å¾å­é›†ä½œä¸ºåº¦é‡æ ‡å‡†ï¼Œä»¥è¯†åˆ«æœ€å…·æœ‰ä¿¡æ¯é‡çš„é¢„æµ‹ç‰¹å¾å­é›†ã€‚

+   è¯¥æ–¹æ³•è®¡ç®—é¢„æµ‹ç‰¹å¾å­é›†ä¸å“åº”ç‰¹å¾ä¹‹é—´çš„å¹³å‡äº’ä¿¡æ¯å‡å»é¢„æµ‹ç‰¹å¾å­é›†ä¹‹é—´çš„å¹³å‡äº’ä¿¡æ¯ã€‚

$$ MID = \frac{1}{|S|}{\sum_{\alpha \in S} I(X_{\alpha},Y) } - \frac{1}{|S|Â²} {\sum_{\alpha \in S}^m \sum_{\beta \in S}^m I(X_{\alpha},X_{\beta})} $$

ä½œä¸º$ç›¸å…³æ€§ - å†—ä½™$çš„åº¦é‡æˆ–

$\begin{equation} MIQ = \frac{ \frac{1}{|S|}{\sum_{\alpha \in S}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|Â²} {\sum_{\alpha \in S}^m \sum_{\beta \in S}^m I(X_{\alpha},X_{\beta})} } \end{equation}$

+   ä½œä¸º$\frac{ç›¸å…³æ€§}{å†—ä½™}$çš„åº¦é‡ã€‚

## è€ƒè™‘ç›¸å…³æ€§å’Œå†—ä½™çš„äº’ä¿¡æ¯ OFAT å˜ä½“

æˆ‘å»ºè®®å¯¹äºä¸€æ¬¡ä¸€ä¸ªç‰¹å¾ï¼ˆOFATï¼‰é¢„æµ‹ç‰¹å¾æ’åºï¼ˆé¢„æµ‹ç‰¹å¾å­é›†ï¼Œ$S = [X_i]$ å’Œ $|S| = 1$ï¼‰ï¼Œæˆ‘ä»¬å°†æ­¤ä¿®æ”¹ä¸ºä»¥ä¸‹è®¡ç®—ï¼š

+   **ç›¸å…³æ€§** - é€‰å®šçš„é¢„æµ‹ç‰¹å¾ $X_i$ ä¸å“åº”ç‰¹å¾ $Y$ ä¹‹é—´çš„äº’ä¿¡æ¯

+   **å†—ä½™** - é€‰å®šçš„é¢„æµ‹ç‰¹å¾ $X_i$ ä¸å‰©ä½™é¢„æµ‹ç‰¹å¾ $X_{\alpha}, \alpha \ne i$ ä¹‹é—´çš„å¹³å‡äº’ä¿¡æ¯ã€‚

+   æˆ‘ä»¬ä½¿ç”¨ Gulgezen, Cataltepe å’Œ Yu (2009) çš„è®¡ç®—å…¬å¼çš„å•†å½¢å¼ã€‚

æˆ‘ä»¬å¯¹æœ€å¤§ç›¸å…³æ€§-æœ€å°å†—ä½™ï¼ˆMRMRï¼‰ç›®æ ‡å‡½æ•°è¿›è¡Œäº†ä¿®æ”¹ï¼Œç”¨äº OFAT æ’åºåˆ†æ•°ï¼Œå°†é€‰å®šçš„é¢„æµ‹ç‰¹å¾ $X_i$ çš„**ç›¸å…³æ€§**ä½œä¸ºå…¶ä¸å“åº”ç‰¹å¾ä¹‹é—´çš„äº’ä¿¡æ¯ï¼š

$\begin{equation} I(X_i,Y) \end{equation}$

ä»¥åŠé€‰å®šçš„é¢„æµ‹ç‰¹å¾ $X_i$ ä¸å‰©ä½™é¢„æµ‹ç‰¹å¾ä¹‹é—´çš„**å†—ä½™**ï¼š

$\begin{equation} \frac{1}{|S|-1} \sum_{\alpha=1, \alpha \ne i}^m I(X_i,X_{\alpha}) \end{equation}$

å…¶ä¸­ $X$ æ˜¯é¢„æµ‹ç‰¹å¾ï¼Œ$Y$ æ˜¯å“åº”ç‰¹å¾ï¼Œ$X_i$ æ˜¯è¢«è¯„åˆ†çš„å…·ä½“é¢„æµ‹ç‰¹å¾ï¼Œ$|S|$ æ˜¯é¢„æµ‹ç‰¹å¾çš„æ•°é‡ï¼Œ$I()$ æ˜¯ç‰¹å¾ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚ä¸€ç§å½¢å¼æ˜¯ç®€å•çš„å·®å€¼ï¼Œç›¸å…³æ€§å‡å»å†—ä½™ï¼Œ

$$ \Phi_{\Delta}(X_i,Y) = I(X_{\alpha},Y) - \frac{1}{|S|-1} \sum_{\beta=1, \alpha \ne \beta}^m I( X_{\alpha},X_{\beta} ) $$

å¦ä¸€ä¸ªé€‰æ‹©æ˜¯æ¯”ç‡ï¼Œ

$$ \Phi_{r}(X_i,Y) = \frac{ I(X_i,Y) }{ \frac{1}{|S|-1} \sum_{\alpha=1, \alpha \ne i}^m I(X_i,X_{\alpha})} $$

åœ¨è¿™é‡Œï¼Œç‰¹å¾æ’åä¸ºäº’ä¿¡æ¯ç›¸å…³æ€§å‡å»å†—ä½™ï¼Œ$\Phi_{\Delta}(X_i,Y)$æ–¹æ³•ã€‚

```py
obj_mutual = mutual_information_objective(x_df,y_df)
indices_obj = np.argsort(obj_mutual)[::-1]              # find indices for descending order

plt.subplot(111)                                        # plot the relative mutual information 
plt.title("One-at-a-Time MRMR Objective Function for Mutual Information-based Feature Selection")
plt.bar(range(x.shape[1]), obj_mutual[indices_obj],
       color="darkorange",alpha = 0.6, align="center",edgecolor="black")
plt.xticks(range(x.shape[1]), x.columns[indices_obj],rotation=90)
plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks
plt.xlim([-0.5, x.shape[1]-0.5]); plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)
plt.show() 
```

![å›¾ç‰‡](img/ce31fdb26712840a6f374091a92555c3.png)

### è€ƒè™‘ç›¸å…³æ€§å’Œå†—ä½™çš„ Delta äº’ä¿¡æ¯å•†

æˆ‘ä»¬å°† Gulgezen, Cataltepe å’Œ Yu (2009) çš„äº’ä¿¡æ¯å•†åº”ç”¨äºå¼€å‘ä¸€ä¸ª OFAT æ’åºåº¦é‡ã€‚

æ ‡å‡†çš„ MRMR ç›®æ ‡å‡½æ•°ï¼Œç”¨äºè¯„ä¼°é¢„æµ‹ç‰¹å¾å­é›†ä¸å“åº”ç‰¹å¾ä¹‹é—´çš„**ç›¸å…³æ€§**ï¼š

$\begin{equation} \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } \end{equation}$

ä»¥åŠé¢„æµ‹ç‰¹å¾å­é›†ä¹‹é—´çš„**å†—ä½™**ï¼š

$\begin{equation} \frac{1}{|S|Â²} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} \end{equation}$

ä¸ºäº†æ‰¾åˆ°æœ€æœ‰ä¿¡æ¯çš„é¢„æµ‹ç‰¹å¾å­é›†ï¼Œæˆ‘ä»¬å¿…é¡»æ‰¾åˆ°æœ€å¤§åŒ–ç›¸å…³æ€§åŒæ—¶æœ€å°åŒ–å†—ä½™çš„ç‰¹å¾å­é›†ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æœ€å¤§åŒ–è¿™ä¸¤ä¸ªå…¬å¼ä¸­çš„ä»»ä½•ä¸€ä¸ªæ¥å®ç°è¿™ä¸€ç‚¹ï¼Œ

\begin{equation} MID = \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } - \frac{1}{|S|Â²} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} \end{equation}

æˆ–è€…

\begin{equation} MIQ = \frac{ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|Â²} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} } \end{equation}

æˆ‘å»ºè®®é€šè¿‡è®¡ç®—åŒ…å«å’Œç§»é™¤ç‰¹å®šé¢„æµ‹ç‰¹å¾ï¼ˆ$X_i$ï¼‰çš„$MIQ$å˜åŒ–æ¥è¿›è¡Œç‰¹å¾æ’åºã€‚

\begin{equation} \Delta MIQ_i = \frac{ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|Â²} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} } - \frac{ \frac{1}{|S|}{\sum_{\alpha=1,\alpha \ne i}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|Â²} {\sum_{\alpha=1,\alpha \ne i}^m \sum_{\beta=1,\beta \ne i}^m I(X_{\alpha},X_{\beta})} } \end{equation}

```py
delta_mutual_information = delta_mutual_information_quotient(x_df,y_df)

indices_delta_mutual_information = np.argsort(delta_mutual_information)[::-1] # find indices for descending order

plt.subplot(111)                                              # plot the relative mutual information 
plt.title("Delta Mutual Information Quotient")
plt.bar(range(x.shape[1]), delta_mutual_information[indices_delta_mutual_information],
       color="darkorange",alpha = 0.6,align="center",edgecolor = 'black')
plt.xticks(range(x.shape[1]), x.columns[indices_delta_mutual_information],rotation=90)
plt.xlim([-0.5, x.shape[1]-0.5])
plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks
plt.plot([-0.5,x.shape[1]-0.5],[0,0],color='black',lw=3); plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)
plt.show() 
```

![_images/03d7d53a3d4abf4c562eeb53cbb1d74427a39a38a569d1e60c953a3ce9fe55a8.png](img/21f9cee66dbe4d7f6bc8cac154c1ad85.png)

æ¯”è¾ƒå¢é‡äº’ä¿¡æ¯å’Œæ–¹å·®è†¨èƒ€å› å­æ’åºæ˜¯æœ‰æ•™è‚²æ„ä¹‰çš„ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½è€ƒè™‘äº†é¢„æµ‹ç‰¹å¾å†—ä½™ã€‚

+   ä½† VIF å‡è®¾çº¿æ€§å…³ç³»ï¼Œå¹¶ä¸”ä¸è€ƒè™‘ç›¸å…³æ€§

```py
plt.scatter(stats.rankdata(delta_mutual_information),stats.rankdata(-vif_values),c='black',edgecolor='black')
for i, feature in enumerate(x.columns):
    plt.annotate(feature, (stats.rankdata(delta_mutual_information)[i]-0.2,stats.rankdata(-vif_values)[i]+0.1))
plt.xlabel('Delta Mutual Information Rank'); plt.ylabel('Variance Inflation Factor Rank'); plt.title('Variance Inflation Factor vs. Delta Mutual Information Ranking')
plt.xlim(0,len(pred)+0.1); plt.ylim(0,len(pred)+0.1)
plt.plot([2,len(pred)],[0,len(pred)-2],color='black',alpha=0.5,ls='--'); 
plt.plot([0,len(pred)-2],[2,len(pred)],color='black',alpha=0.5,ls='--')
plt.fill_between([0,len(pred)-2], [2,len(pred)], [len(pred),len(pred)], color='coral',alpha=0.2,zorder=1)
plt.fill_between([2,len(pred)], [0,len(pred)-2], [0,0], color='dodgerblue',alpha=0.2,zorder=1)
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/753de82205baf137ac8f92671732ef8708fc686e982b161ca2b05ba1095ee90e.png](img/3028e58a56f928ebb26de84cfb6e1f54.png)

ä»äº’ä¿¡æ¯ä¸­æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå­”éš™ç‡ã€æ¸—é€ç‡ç„¶åæ˜¯æ€»æœ‰æœºç¢³å’Œè„†æ€§åœ¨ä¸€èˆ¬ç‹¬ç«‹æ€§æ–¹é¢æœ‰æœ€å¤§çš„åç¦»ã€‚

## æ‰€æœ‰åŒå˜é‡æŒ‡æ ‡çš„æ€»ç»“

æˆ‘ä»¬æœ‰ä¸€ç³»åˆ—å¹¿æ³›çš„å‡†åˆ™æ¥å¯¹ç‰¹å¾è¿›è¡Œæ’åºã€‚

+   $B$ç³»æ•°ä¸åæ–¹å·®æœ‰ç›¸åŒçš„é—®é¢˜ï¼Œå¯¹å•å˜é‡æ–¹å·®æ•æ„Ÿ

+   $\beta$ç³»æ•°æ¶ˆé™¤äº†è¿™ç§æ•æ„Ÿæ€§ï¼Œå¹¶ä¸å…ˆå‰ç»“æœä¸€è‡´ã€‚

è€ƒè™‘åˆ°æ‰€æœ‰è¿™äº›æ–¹æ³•ï¼Œæˆ‘å°†å˜é‡æ’åºå¦‚ä¸‹ï¼š

1.  å­”éš™ç‡

1.  ç»ç’ƒå…‰æ³½åå°„ç‡

1.  å£°é˜»æŠ—

1.  æ¸—é€ç‡

1.  æ€»æœ‰æœºç¢³

1.  è„†æ€§

æˆ‘é€šè¿‡è§‚å¯Ÿè¿™äº›æŒ‡æ ‡çš„ä¸€èˆ¬è¶‹åŠ¿æ¥åˆ†é…è¿™äº›ç­‰çº§ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åŠ æƒæ¯ç§æ–¹æ³•æ¥åˆ¶ä½œä¸€ä¸ªæ›´å®šé‡çš„åˆ†æ•°å¹¶æ’åºã€‚

å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬ä¸åº”å¿½è§†ä¸“å®¶çŸ¥è¯†ã€‚å¦‚æœå…³äºç‰©ç†è¿‡ç¨‹ã€å› æœå…³ç³»ä»¥åŠå˜é‡çš„å¯é æ€§å’Œå¯ç”¨æ€§æœ‰æ›´å¤šä¿¡æ¯ï¼Œåˆ™åº”å°†è¿™äº›ä¿¡æ¯æ•´åˆåˆ°åˆ†é…ç­‰çº§ä¸­ã€‚

æˆ‘ä»¬åœ¨è¿™é‡ŒåŒ…æ‹¬ä¸€ä¸ªé™„åŠ æ–¹æ³•ï¼Œé€’å½’ç‰¹å¾æ¶ˆé™¤ï¼Œä½†åªæä¾›äº†ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹ç¤ºä¾‹ã€‚å¯¹äºæ›´å¤æ‚çš„æ¨¡å‹ï¼Œå¯ä»¥åšæ›´å¤šçš„äº‹æƒ…ã€‚

## é€’å½’ç‰¹å¾æ¶ˆé™¤

é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰æ–¹æ³•é€šè¿‡é€’å½’åœ°ç§»é™¤ç‰¹å¾å¹¶ä½¿ç”¨å‰©ä½™ç‰¹å¾æ„å»ºæ¨¡å‹æ¥å·¥ä½œã€‚

+   åœ¨ç¬¬ä¸€æ­¥ä¸­ï¼Œä½¿ç”¨æ‰€æœ‰ç‰¹å¾æ„å»ºæ¨¡å‹ï¼Œå¹¶æ ¹æ®ç‰¹å¾é‡è¦æ€§æˆ–$\beta$ç³»æ•°å¯¹ç‰¹å¾è¿›è¡Œæ’åº

+   æœ€ä¸é‡è¦çš„ç‰¹å¾è¢«å‰ªæï¼Œå¹¶é‡æ–°æ„å»ºæ¨¡å‹

+   è¿™ä¼šä¸€ç›´é‡å¤ï¼Œç›´åˆ°åªå‰©ä¸‹ä¸€ä¸ªç‰¹å¾

åœ¨æ­¤ä»£ç ä¸­ï¼Œæˆ‘ä»¬åŸºäºå¤šå…ƒå›å½’æ„å»ºé¢„æµ‹æ¨¡å‹ï¼Œå¹¶æŒ‡å‡ºæˆ‘ä»¬æƒ³è¦æ ¹æ®é€’å½’ç‰¹å¾æ¶ˆé™¤æ‰¾åˆ°æœ€ä½³ç‰¹å¾ã€‚ç®—æ³•ä¸ºæ‰€æœ‰ç‰¹å¾åˆ†é…æ’å $1,\ldots,m$ã€‚

```py
rfe_linear = RFE(LinearRegression(),n_features_to_select=1,verbose=0) # set up RFE linear regression model
df['const'] = np.ones(len(df))                                # let's add one's for the constant term
rfe_linear = rfe_linear.fit(df[pred].values,np.ravel(df[resp])) # recursive elimination
dfS = df.drop('const',axis = 1)                               # remove the ones
print('Recursive Feature Elimination: Multilinear Regression')
for i in range(0,len(pred)):
    print('Rank #' + str(i+1) + ' ' + pred[rfe_linear.ranking_[i]-1]) 
```

```py
Recursive Feature Elimination: Multilinear Regression
Rank #1 Brittle
Rank #2 TOC
Rank #3 AI
Rank #4 VR
Rank #5 Por
Rank #6 Perm 
```

é€’å½’æ¶ˆé™¤æ–¹æ³•çš„ä¼˜ç‚¹ï¼š

+   å®é™…æ¨¡å‹å¯ç”¨äºè¯„ä¼°ç‰¹å¾æ’å

+   æ’ååŸºäºä¼°è®¡çš„å‡†ç¡®æ€§

ä½†è¿™ç§æ–¹æ³•å¯¹ä»¥ä¸‹å› ç´ æ•æ„Ÿï¼š

+   æ¨¡å‹çš„é€‰æ‹©

+   è®­ç»ƒæ•°æ®é›†

ç‰¹å¾æ’åä¸æˆ‘ä»¬ä¹‹å‰çš„æ–¹æ³•ç›¸å½“ä¸åŒã€‚è®¸å¤šå·²ç»ä»ä¹‹å‰çš„è¯„ä¼°ä¸­ç§»åŠ¨ã€‚ä¹Ÿè®¸æˆ‘ä»¬åº”è¯¥ä½¿ç”¨æ›´çµæ´»çš„å»ºæ¨¡æ–¹æ³•ã€‚

è®©æˆ‘ä»¬ç”¨ä¸€ç§æ›´çµæ´»çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå³å†³ç­–æ ‘å›å½’æ¨¡å‹ï¼Œé‡å¤è¿™ç§æ–¹æ³•ã€‚

```py
from sklearn.ensemble import RandomForestRegressor
import warnings
warnings.filterwarnings('ignore')            
import geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper
rfe_rf = RFE(RandomForestRegressor(max_depth=3),n_features_to_select=1,verbose=0) # set up RFE linear regression model
df['const'] = np.ones(len(df))                                # let's add one's for the constant term

lab_enc = preprocessing.LabelEncoder(); Y_encoded = lab_enc.fit_transform(Y)

rfe_rf = rfe_rf.fit(x,np.ravel(Y_encoded))                    # recursive elimination
dfS = df.drop('const',axis = 1)                               # remove the ones
print('Recursive Feature Elimination: Random Forest Regression')
for i in range(0,len(pred)):
    print('Rank #' + str(i+1) + ' ' + pred[rfe_rf.ranking_[i]-1]) 
```

```py
Recursive Feature Elimination: Random Forest Regression
Rank #1 Por
Rank #2 VR
Rank #3 Brittle
Rank #4 Perm
Rank #5 TOC
Rank #6 AI 
```

å†æ¬¡å¼ºè°ƒï¼Œé€’å½’ç‰¹å¾æ¶ˆé™¤åœ¨ç‰¹å¾æ’åä¸­å¯¹æ¨¡å‹çš„å‡†ç¡®æ€§æ•æ„Ÿã€‚

+   å®é™…çš„é¢„æµ‹æ¨¡å‹å¿…é¡»è°ƒæ•´å…¶å…³è”çš„è¶…å‚æ•°å¹¶æ£€æŸ¥æ¨¡å‹ç²¾åº¦ã€‚

+   ä¾‹å¦‚ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºçº¿æ€§æ¨¡å‹çš„å‡†ç¡®æ€§å·®ï¼Œå¤šå…ƒå›å½’çš„ç‰¹å¾æ’åä¸å¯é ã€‚

## ç‰¹å¾æ’åçš„ Shapley å€¼

è®©æˆ‘ä»¬å–æ•°æ®çš„ä¸€ä¸ªéšæœºå­é›†ï¼Œä½œä¸ºèƒŒæ™¯å€¼æ¥è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚

+   æˆ‘ä»¬ä¸ºäº†æ›´å¿«åœ°è®¡ç®—è€Œè¿›è¡Œå­é›†åˆ’åˆ†

+   æˆ‘ä»¬åº”è¯¥è¯„ä¼°/å¼ºåˆ¶é¢„æµ‹ç‰¹å¾ç©ºé—´çš„æ•ˆç‡è¦†ç›–

ç”±äº Shapley å€¼æ˜¯åŸºäºæ¨¡å‹çš„ï¼Œæˆ‘ä»¬å¿…é¡»ä»æ„å»ºæ¨¡å‹å¼€å§‹

### æ„å»ºéšæœºæ£®æ—æ¨¡å‹

ç”±äº Shapley æ˜¯åŸºäºæ¨¡å‹çš„ï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªæ¨¡å‹

+   è®©æˆ‘ä»¬ä»ä¸€ä¸ªå¥½çš„éšæœºæ£®æ—æ¨¡å‹å¼€å§‹ï¼Œè§‚å¯Ÿ Shapleyï¼Œç„¶åè¿”å›è¿™é‡Œå¹¶ä¿®æ”¹æ¨¡å‹

```py
seed = 73093                                                  # set the random forest hyperparameters

# #Underfit random forest
max_leaf_nodes = 2
num_tree = 10
max_features = 2

#Overfit random forest
max_leaf_nodes = 50
num_tree = 1
max_features = 6

# #Good random forest
max_leaf_nodes = 5
num_tree = 300
max_features = 2

rfr = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=seed,n_estimators=num_tree, max_features=max_features)
rfr.fit(X = x, y = Y)

Y_hat = predict_train = rfr.predict(x)

MSE = metrics.mean_squared_error(Y,Y_hat)
Var_Explained = metrics.explained_variance_score(Y,Y_hat)
print('Mean Squared Error on Training = ', round(MSE,2),', Variance Explained =', round(Var_Explained,2))

importances = rfr.feature_importances_               # expected (global) importance over the forest fore each predictor feature
std = np.std([rfr.feature_importances_ for tree in rfr.estimators_],axis=0)
indices = np.argsort(importances)[::-1].tolist()

plt.subplot(121)
plt.scatter(Y,Y_hat,s=None, c='darkorange',marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors="black")
plt.title('Random Forest Model'); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')
plt.xlim(0,7000); plt.ylim(0,7000)
plt.arrow(0,0,7000,7000,width=0.02,color='black',head_length=0.0,head_width=0.0)

plt.subplot(122)
plt.title("Feature Importances")
plt.bar([pred[i] for i in indices],rfr.feature_importances_[indices],color="darkorange", alpha = 0.8, edgecolor = 'black', yerr=std[indices], align="center")
#plt.xticks(range(X.shape[1]), indices)
plt.ylim(0,1), plt.xlabel('Predictor Features'); plt.ylabel('Feature Importance')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

```py
Mean Squared Error on Training =  428100.87 , Variance Explained = 0.82 
```

![å›¾ç‰‡](img/6262d930974113cf78a782fd287a8df6.png)

## è®¡ç®— Shapley å€¼

è®©æˆ‘ä»¬éšæœºé€‰æ‹©ä¸€äº›èƒŒæ™¯æ•°æ®æ¥è®¡ç®—å±€éƒ¨ Shapley å€¼ï¼Œç„¶åç”¨å…¨å±€ Shapley åº¦é‡æ¥æ€»ç»“ã€‚

èƒŒæ™¯æ ·æœ¬æ˜¯ä»æ‰€æœ‰æ•°æ®ä¸­éšæœºé€‰æ‹©çš„å­é›†ã€‚ä¸ºä»€ä¹ˆä¸ç›´æ¥ä½¿ç”¨æ‰€æœ‰æ•°æ®ä½œä¸ºèƒŒæ™¯ï¼Ÿ

+   **Shapley å€¼çš„è®¡ç®—å¯èƒ½å¾ˆæ˜‚è´µ**ï¼Œæˆ‘ä»¬éœ€è¦æ‰€æœ‰æ¨¡å‹çš„ç»„åˆæ¥è·å–æ‰€æœ‰è¾¹é™…è´¡çŒ®çš„é¢„æµ‹ï¼Œè¿™äº›è¾¹é™…è´¡çŒ®è¢«æ€»ç»“ä¸º Shapley å€¼

+   **åŸå§‹æ•°æ®å¯èƒ½ä»¥æœ‰åçš„æ–¹å¼é‡‡æ ·**ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ä¼šæƒ³ç¡®ä¿èƒŒæ™¯æ•°æ®å…·æœ‰ä»£è¡¨æ€§ï¼Œå³ä»åŸå§‹æ•°æ®ä¸­é‡‡æ ·ä»¥å‡å°‘åå·®ï¼Œé¿å…åœ¨ç‰¹å¾é‡è¦æ€§è¯„ä¼°ä¸­çš„åå·®

+   **æ³›åŒ–ä¸ç‰¹å®šé¢„æµ‹æ¡ˆä¾‹**ï¼Œå¦‚æœæ‰€æœ‰æ•°æ®éƒ½ç”¨ä½œèƒŒæ™¯ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ç‰¹å¾é‡è¦æ€§çš„æ€»ä½“æ•°æ®è¯„ä¼°ï¼Œä½†æˆ‘ä»¬å¯èƒ½å¸Œæœ›ä»”ç»†é€‰æ‹©æ•°æ®æ¥æ¢ç´¢ç‰¹å®šçš„é¢„æµ‹æ¡ˆä¾‹

ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åªæ˜¯éšæœºé€‰æ‹© $n$ ä¸ªæ•°æ®ã€‚

```py
background = shap.sample(x,nsamples=50,random_state=73073) 
model_explainer = shap.TreeExplainer(rfr)
shap_values = model_explainer.shap_values(background) # global Shapley Measures 
```

## å±€éƒ¨ Shapley å€¼

è®©æˆ‘ä»¬å…ˆçœ‹çœ‹å±€éƒ¨ Shapley å€¼ï¼Œä»¥å±•ç¤ºæ•ˆç‡çš„æ¦‚å¿µã€‚

+   é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç¡®è®¤ shap å‡½æ•°çš„è¾“å‡ºæ˜¯ä¸€ä¸ª $\left[n_{background}, m\right]$ çš„ nd æ•°ç»„ã€‚

```py
shap_values.shape 
```

```py
(50, 6) 
```

æˆ‘ä»¬ä¸ºèƒŒæ™¯æ¡ˆä¾‹ä¸­çš„æ¯ä¸ªé¢„æµ‹éƒ½è®¡ç®—äº†å±€éƒ¨ Shapley å€¼ã€‚è®©æˆ‘ä»¬å¯è§†åŒ–ä¸€ä¸ªæ¥å±•ç¤ºè¿™ä¸€ç‚¹ã€‚

+   æˆ‘ç¼–å†™äº†è¿™ä¸ªè‡ªå®šä¹‰å¯è§†åŒ–ï¼Œä»¥æ¸…æ¥šåœ°ä¼ è¾¾å±€éƒ¨ Shapley å€¼å’Œæ•ˆç‡çš„æ¦‚å¿µã€‚

+   æˆ‘ä»¬ä»è®­ç»ƒå“åº”ç‰¹å¾çš„å‡å€¼å¼€å§‹ï¼Œä¸ºæ¯ä¸ªé¢„æµ‹ç‰¹å¾æ·»åŠ å±€éƒ¨ Shapley å€¼ï¼Œä»¥è¾¾åˆ°é¢„æµ‹ã€‚

```py
nback = 7

resp_avg  = np.average(Y_hat)
yhat = rfr.predict(background.iloc[[nback]])

current = resp_avg

plt.subplot(111)

plt.plot([current,current],[0,0.3],color='black',lw=2,zorder=1)
plt.plot([current-2,current],[0.2,0.3],color='black',lw=2,zorder=1)
plt.plot([current,current+2],[0.3,0.2],color='black',lw=2,zorder=1)
for i in range(len(pred)+1):
    plt.scatter(current,i+0.5,color='grey',edgecolor='black',zorder=10)
    if i < len(pred):
        if shap_values[nback,i] > 0.0:
            color = 'red'
        else:
            color = 'blue'
        plt.plot([current,current + shap_values[nback,i]],[i+1,i+1],color=color,lw=2,zorder=1)
        plt.plot([current,current],[i+0.6,i+1],color=color,lw=2,zorder=1)
        plt.plot([current + shap_values[nback,i],current + shap_values[nback,i]],[i+1,i+1.3],color=color,lw=2,zorder=1)
        plt.plot([current + shap_values[nback,i]-2,current + shap_values[nback,i]],[i+1.2,i+1.3],color=color,lw=2,zorder=1)
        plt.plot([current + shap_values[nback,i],current + shap_values[nback,i]+2],[i+1.3,i+1.2],color=color,lw=2,zorder=1)
        if shap_values[nback,i] > 0.0:
            plt.annotate('+ ' + str(np.round(shap_values[nback,i],0)),[current + shap_values[nback,i]*0.5,i+1.1],ha='center')
        else:
            plt.annotate('- ' + str(np.round(abs(shap_values[nback,i]),0)),[current + shap_values[nback,i]*0.5,i+1.1],ha='center')
        current = current + shap_values[nback,i]

plt.plot([current,current],[i+0.7,i+1],color='black',lw=2,zorder=1)
plt.plot([current-2,current],[i+0.9,i+1],color='black',lw=2,zorder=1)
plt.plot([current,current+2],[i+1,i+0.9],color='black',lw=2,zorder=1)

plt.plot([resp_avg,resp_avg],[-0.5,len(pred)+1.5],color='black',ls='--',zorder=1)
plt.plot([yhat,yhat],[-0.5,len(pred)+1.5],color='black',ls='--',zorder=1)
plt.annotate('Response Feature, Training Average',[resp_avg-8,1.0],rotation=90.0)
plt.annotate('Model Prediction',[yhat-8,1.0],rotation=90.0)

plt.yticks(ticks=np.arange(len(pred)+2), labels=[r'None / $\overline{y}$'] + pred + [r'$\hat{y}=f(X)$'])
add_grid(); plt.ylim([-0.5,len(pred)+1.5])
plt.xlabel('Production (MCFPD)'); plt.ylabel('Feature'); plt.title('Local Shapley Values, Background Index: ' + str(nback))
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/223f16f325ce6407f83bcbcad31a87a9.png)

ç°åœ¨æˆ‘å‘æ‚¨å±•ç¤ºä½¿ç”¨ shap Python åŒ…å†…ç½®çš„ç»˜å›¾æ–¹æ³•æ¥ä¼ è¾¾ç›¸åŒçš„ä¿¡æ¯ã€‚

## Shapley åŠ›å›¾

æˆ‘ä»¬å¯ä»¥åŒæ—¶å¯è§†åŒ–æ‰€æœ‰æ ·æœ¬æ•°æ®ä¸­çš„æ‰€æœ‰ Shapley å€¼ï¼ŒæŒ‰èƒŒæ™¯æ•°æ®é›†çš„é¡ºåºæ’åˆ—ã€‚

+   è“è‰²è¡¨ç¤ºé¢„æµ‹äº§é‡çš„å‡å°‘ï¼Œçº¢è‰²è¡¨ç¤ºé¢„æµ‹äº§é‡çš„å¢åŠ 

æˆ‘ä»¬æ­£åœ¨ä¸€æ¬¡æ€§å¯è§†åŒ–æ‰€æœ‰èƒŒæ™¯æ ·æœ¬æ•°æ®ã€‚æŒ‰åŸå§‹æ ·æœ¬é¡ºåºé‡æ–°æ’åºï¼Œå¹¶é€‰æ‹© nback ç´¢å¼•ä»¥ä¸ä¸Šé¢çš„å›¾è¡¨è¿›è¡Œæ¯”è¾ƒã€‚

```py
shap.force_plot(model_explainer.expected_value,shap_values,background,out_names = ['Production'],feature_names=pred,) 
```

**å¯è§†åŒ–è¢«çœç•¥ï¼ŒJavaScript åº“æœªåŠ è½½ï¼**

æ‚¨åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­è¿è¡Œäº†`initjs()`å—ï¼Ÿå¦‚æœè¿™ä¸ªç¬”è®°æœ¬æ¥è‡ªå¦ä¸€ä¸ªç”¨æˆ·ï¼Œæ‚¨è¿˜å¿…é¡»ä¿¡ä»»è¿™ä¸ªç¬”è®°æœ¬ï¼ˆæ–‡ä»¶ -> ä¿¡ä»»ç¬”è®°æœ¬ï¼‰ã€‚å¦‚æœæ‚¨åœ¨ GitHub ä¸ŠæŸ¥çœ‹æ­¤ç¬”è®°æœ¬ï¼ŒJavaScript å·²è¢«ç§»é™¤ä»¥å¢å¼ºå®‰å…¨æ€§ã€‚å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨ JupyterLabï¼Œæ­¤é”™è¯¯æ˜¯å› ä¸ºå°šæœªç¼–å†™ JupyterLab æ‰©å±•ã€‚

## å±€éƒ¨åŠ›å›¾

æˆ‘ä»¬ä»èƒŒæ™¯ä¸­é€‰å–ä¸€ä¸ªç‰¹å®šçš„æ ·æœ¬ï¼Œå¹¶å¯è§†åŒ–å…¶åŠ›å›¾ã€‚

+   æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸Šè¿°å›¾è¡¨çš„èµ·æºï¼Œç»™å®šæ ·æœ¬$i$ï¼ˆ$x_i$ï¼‰çš„å±€éƒ¨å€¼é›†çš„æ‰€æœ‰ç‰¹å¾çš„ Shapley å€¼ã€‚

å°†æ­¤ç»“æœä¸ä¸Šé¢æˆ‘åˆ¶ä½œçš„è‡ªå®šä¹‰å›¾è¡¨è¿›è¡Œæ¯”è¾ƒï¼Œæ‚¨å°†çœ‹åˆ°å®ƒä»¬ä¼ è¾¾äº†ç›¸åŒçš„ä¿¡æ¯ã€‚

```py
shap.force_plot(model_explainer.expected_value,shap_values[nback],background.iloc[[nback]],show=False,feature_names = pred) 
```

**å¯è§†åŒ–è¢«çœç•¥ï¼ŒJavaScript åº“æœªåŠ è½½ï¼**

æ‚¨åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­è¿è¡Œäº†`initjs()`å—ï¼Ÿå¦‚æœè¿™ä¸ªç¬”è®°æœ¬æ¥è‡ªå¦ä¸€ä¸ªç”¨æˆ·ï¼Œæ‚¨è¿˜å¿…é¡»ä¿¡ä»»è¿™ä¸ªç¬”è®°æœ¬ï¼ˆæ–‡ä»¶ -> ä¿¡ä»»ç¬”è®°æœ¬ï¼‰ã€‚å¦‚æœæ‚¨åœ¨ GitHub ä¸ŠæŸ¥çœ‹æ­¤ç¬”è®°æœ¬ï¼ŒJavaScript å·²è¢«ç§»é™¤ä»¥å¢å¼ºå®‰å…¨æ€§ã€‚å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨ JupyterLabï¼Œæ­¤é”™è¯¯æ˜¯å› ä¸ºå°šæœªç¼–å†™ JupyterLab æ‰©å±•ã€‚

æ„Ÿè°¢è–›æ¾å¯¹æ”¹è¿›ä¸Šè¿°å±€éƒ¨ Shapley å€¼å†…å®¹å’Œå¯è§†åŒ–çš„å»ºè®®ã€‚

## å…¨å±€ Shapley å€¼

è®©æˆ‘ä»¬å›é¡¾å…¨å±€ Shapley åº¦é‡ã€‚

+   èƒŒæ™¯æ•°æ®ä¸Šç»å¯¹ SHAP å€¼çš„ç®—æœ¯å¹³å‡å€¼çš„æ’åºæ¡å½¢å›¾

+   èƒŒæ™¯æ•°æ®ä¸Š SHAP å€¼çš„æ’åºå›¾

+   èƒŒæ™¯æ•°æ®ä¸Š SHAP å€¼çš„ violin å›¾

æ³¨æ„ï¼šæ‰€æœ‰è¿™äº›æ–¹æ³•éƒ½æ˜¯å°†æ¯ä¸ªç‰¹å¾çš„å…¨çƒå¹³å‡å€¼ï¼ˆ$E[X_i]$ï¼‰åº”ç”¨äºé‚£äº›ä¸åŒ…æ‹¬ç‰¹å¾$i$çš„æ¡ˆä¾‹ã€‚

```py
plt.subplot(131)
shap.summary_plot(show=False,feature_names = pred, shap_values = shap_values, features = background, plot_type="bar",color = "darkorange",cmap = plt.cm.inferno)
plt.ylabel('Predictor Features')

plt.subplot(132)
shap.summary_plot(show=False,feature_names = pred, shap_values = shap_values, features = background,cmap = plt.cm.inferno)

plt.subplot(133)
shap.summary_plot(show=False,feature_names = pred, shap_values = shap_values, features = background,plot_type = "violin")

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2)
plt.show() 
```

![å›¾ç‰‡](img/e3945b1cd0e515c4f1adfcc33e2436d8.png)

ä¸­é—´å’Œå³ä¾§çš„å›¾æ˜¾ç¤ºäº†æ‰€æœ‰éšæœºé€‰æ‹©çš„èƒŒæ™¯æ ·æœ¬ä¸­æ¯ä¸ªç‰¹å¾çš„ Shapley å€¼ï¼Œè€Œå·¦ä¾§çš„å›¾æ˜¯å¹³å‡ç»å¯¹ Shapley å€¼çš„æ¡å½¢å›¾ã€‚

+   å­”éš™ç‡ã€æ¸—é€ç‡å’Œ TOC æ˜¯é¡¶çº§ç‰¹å¾

## è¯„è®º

è¿™æ˜¯å¯¹ç‰¹å¾æ’åçš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´ YouTube è®²åº§ä¸­çš„èµ„æºé“¾æ¥ï¼Œè§†é¢‘æè¿°ä¸­åŒ…å«èµ„æºé“¾æ¥ã€‚

æˆ‘å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[Cockrell å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œä»–åœ¨è¯¥æ ¡ä»äº‹å’Œæ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚

+   [è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº† 70 å¤šç¯‡[åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[åœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[Python ä¸­çš„åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPy åŠ¨æ‰‹æŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[Python ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„åŠ¨æ‰‹æŒ‡å—](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œé™„æœ‰ 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ª GitHub ä»“åº“ä¸­çš„è¯¦ç»†æ–‡æ¡£å·¥ä½œæµç¨‹é“¾æ¥ï¼Œè¿™äº›ä»“åº“åœ¨ä»–çš„[GitHub è´¦æˆ·](https://github.com/GeostatsGuy)ä¸Šï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›å¸¸é’å†…å®¹ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³ä¸€èµ·å·¥ä½œï¼Ÿ

æˆ‘å¸Œæœ›è¿™ä¸ªå†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£äºåˆä½œï¼Œæ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹æ¥å¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢

æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥è·å–ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## ç‰¹å¾æ’åºçš„åŠ¨æœº

é€šå¸¸æœ‰å¾ˆå¤šé¢„æµ‹ç‰¹å¾ï¼ˆè¾“å…¥å˜é‡ï¼‰å¯ä¾›æˆ‘ä»¬ç”¨äºæ„å»ºé¢„æµ‹æ¨¡å‹ã€‚

+   æœ‰å……åˆ†çš„ç†ç”±è¿›è¡Œé€‰æ‹©ï¼Œå°†æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾éƒ½åŠ å…¥è¿›æ¥å¹¶ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼

é€šå¸¸ï¼Œå¯¹äºæœ€ä½³çš„é¢„æµ‹æ¨¡å‹ï¼Œä»”ç»†é€‰æ‹©æä¾›æœ€å¤šä¿¡æ¯çš„å°‘æ•°ç‰¹å¾æ˜¯æœ€ä½³å®è·µã€‚

åŸå› å¦‚ä¸‹ï¼š

+   **é”™è¯¯** - æ›´å¤šçš„é¢„æµ‹ç‰¹å¾ä¼šå¯¼è‡´æ›´å¤æ‚çš„æµç¨‹ï¼Œéœ€è¦æ›´å¤šä¸“ä¸šæ—¶é—´ï¼Œå¹¶ä¸”åœ¨å·¥ä½œæµç¨‹ä¸­å‡ºé”™çš„æœºä¼šå¢åŠ 

+   **éš¾ä»¥å¯è§†åŒ–** - é«˜ç»´æ¨¡å‹ï¼Œå³é¢„æµ‹ç‰¹å¾æ•°é‡æ›´å¤šï¼Œæ›´éš¾ä»¥å¯è§†åŒ–

+   **æ¨¡å‹æ£€æŸ¥** - æ›´å¤æ‚çš„æ¨¡å‹å¯èƒ½æ›´éš¾ä»¥è°ƒæŸ¥ã€è§£é‡Šå’Œè¿›è¡Œè´¨é‡æ§åˆ¶

+   **é¢„æµ‹ç‰¹å¾å†—ä½™** - æ›´æœ‰å¯èƒ½å­˜åœ¨å†—ä½™çš„é¢„æµ‹ç‰¹å¾ã€‚åŒ…å«é«˜åº¦å†—ä½™å’Œå…±çº¿æ€§æˆ–å¤šå…±çº¿æ€§çš„ç‰¹å¾ä¼šå¢åŠ æ¨¡å‹æ–¹å·®ï¼Œå¢åŠ æ¨¡å‹çš„ä¸ç¨³å®šæ€§ï¼Œå¹¶é™ä½æµ‹è¯•é¢„æµ‹çš„å‡†ç¡®æ€§

+   **è®¡ç®—æ—¶é—´** - æ›´å¤šçš„é¢„æµ‹ç‰¹å¾é€šå¸¸ä¼šå¢åŠ è®­ç»ƒæ¨¡å‹æ‰€éœ€çš„è®¡ç®—æ—¶é—´å’Œè®¡ç®—å­˜å‚¨ï¼Œå³æ¨¡å‹å¯èƒ½ä¸å¤Ÿç´§å‡‘ä¸”ä¸ä¾¿äºæºå¸¦

+   **æ¨¡å‹è¿‡æ‹Ÿåˆ** - éšç€ç‰¹å¾æ•°é‡çš„å¢åŠ ï¼Œæ¨¡å‹å¤æ‚åº¦çš„æé«˜ï¼Œè¿‡æ‹Ÿåˆçš„é£é™©ä¹Ÿéšä¹‹å¢åŠ 

+   **æ¨¡å‹å¤–æ¨** - è®¸å¤šé¢„æµ‹ç‰¹å¾å¯¼è‡´é«˜ç»´æ¨¡å‹ç©ºé—´æ•°æ®è¦†ç›–åº¦ä½ï¼Œä¸”æ¨¡å‹å¤–æ¨å¯èƒ½ä¸å‡†ç¡®

å¯¹äºè®¸å¤šé¢„æµ‹ç‰¹å¾çš„ä¸»è¦é—®é¢˜æ˜¯ç»´åº¦ç¾éš¾ã€‚è®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹è¿™ä¸ªç¾éš¾ï¼

## ç»´åº¦ç¾éš¾

1.  **æ•°æ®å’Œæ¨¡å‹å¯è§†åŒ–** - æˆ‘ä»¬æ— æ³•å¯è§†åŒ–è¶…è¿‡ä¸‰ç»´ï¼Œå³æ— æ³•è®¿é—®æ•°æ®æ‹Ÿåˆæ¨¡å‹ï¼Œè¯„ä¼°å†…æ’ä¸å¤–æ¨ã€‚

+   è€ƒè™‘ä¸€ä¸ª 5 ç»´ç¤ºä¾‹ï¼Œå¦‚å›¾æ‰€ç¤ºä¸ºçŸ©é˜µæ•£ç‚¹å›¾ï¼Œå³ä½¿åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªå›¾ä¹Ÿæç«¯åœ°è¾¹ç¼˜åŒ–åˆ°äºŒç»´ï¼Œ

![](img/ecf50f66114aec17ea35fde1342d66c4.png)

ç¤ºä¾‹ 5D æ•°æ®ä½œä¸ºçŸ©é˜µæ•£ç‚¹å›¾ã€‚

1.  **é‡‡æ ·** - è¶³å¤Ÿçš„æ ·æœ¬æ•°é‡ä»¥æ¨æ–­è¯¸å¦‚è”åˆæ¦‚ç‡ $P(x_1,\ldots,x_m)$ è¿™æ ·çš„ç»Ÿè®¡é‡ã€‚

+   å›å¿†ç›´æ–¹å›¾æˆ–å½’ä¸€åŒ–ç›´æ–¹å›¾çš„è®¡ç®—ï¼šæˆ‘ä»¬å»ºç«‹åŒºé—´å¹¶è®¡ç®—æ¯ä¸ªåŒºé—´çš„é¢‘ç‡æˆ–æ¦‚ç‡ã€‚

+   æˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªåŒºé—´æä¾›åä¹‰ä¸Šçš„æ•°æ®æ ·æœ¬æ•°é‡ï¼Œå› æ­¤åœ¨ä¸€ç»´ä¸­æˆ‘ä»¬éœ€è¦ $ğ‘›=ğ‘›_{ğ‘ /ğ‘ğ‘–ğ‘›} \cdot ğ‘›_{ğ‘ğ‘–ğ‘›ğ‘ }$ ä¸ªæ ·æœ¬

+   ä½†åœ¨ mD ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ $n$ ä¸ªæ ·æœ¬æ¥è®¡ç®—ç¦»æ•£åŒ–çš„è”åˆæ¦‚ç‡ï¼Œ

$$ ğ‘›=ğ‘›_{ğ‘ /ğ‘ğ‘–ğ‘›} \cdot ğ‘›_{ğ‘ğ‘–ğ‘›ğ‘ }^m $$

+   ä¾‹å¦‚ï¼Œæ¯ä¸ªåŒºé—´ 10 ä¸ªæ ·æœ¬ï¼Œ35 ä¸ªåŒºé—´ï¼Œåœ¨äºŒç»´ä¸­éœ€è¦ 12,250 ä¸ªæ ·æœ¬ï¼Œåœ¨ä¸‰ç»´ä¸­éœ€è¦ 428,750 ä¸ªæ ·æœ¬

![](img/bc8823819263f4497ef6baab93a9ee38.png)

ç¤ºä¾‹ 2D æ•°æ®ï¼Œæ¯ä¸ªç‰¹å¾æœ‰ 35 ä¸ªåŒºé—´ã€‚

1.  **æ ·æœ¬è¦†ç›–åº¦** - æ ·æœ¬å€¼èŒƒå›´è¦†ç›–é¢„æµ‹ç‰¹å¾ç©ºé—´ã€‚

+   æ ·æœ¬ç©ºé—´ä¸­å¯èƒ½è§£çš„åˆ†æ•°ï¼Œå¯¹äº 1 ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬å‡è®¾ 80%çš„è¦†ç›–åº¦

+   è®°ä½ï¼Œæˆ‘ä»¬é€šå¸¸åªç›´æ¥é‡‡æ ·åœ°ä¸‹ä½“ç§¯çš„ $\frac{1}{10â·}$ã€‚

+   æ˜¯çš„ï¼Œè¦†ç›–åº¦çš„æ¦‚å¿µæ˜¯ä¸»è§‚çš„ï¼Œéœ€è¦è¦†ç›–å¤šå°‘æ•°æ®ï¼Ÿå…³äºé—´éš™æ€ä¹ˆåŠï¼Ÿç­‰ç­‰ã€‚

![](img/d8058511a88a482ed34b0cbd9eb34fec.png)

ç¤ºä¾‹ 2D æ•°æ®ï¼Œæ¯ä¸ªç‰¹å¾æœ‰ 35 ä¸ªåŒºé—´ã€‚

+   ç°åœ¨å¦‚æœæœ‰ä¸¤ä¸ªç‰¹å¾çš„ 80%è¦†ç›–åº¦ï¼ŒäºŒç»´è¦†ç›–åº¦ä¸º 64%

![](img/8d96453b3f6c2a92a160fe4329a13d4a.png)

ç¤ºä¾‹ 2D æ•°æ®ï¼Œæ¯ä¸ªç‰¹å¾æœ‰ 35 ä¸ªåŒºé—´ã€‚

+   è¦†ç›–åº¦ï¼Œ

$$ c = c_1^m $$

1.  **æ‰­æ›²ç©ºé—´** - é«˜ç»´ç©ºé—´æ˜¯æ‰­æ›²çš„ã€‚

+   å–å†…åµŒè¶…çƒä½“çš„ä½“ç§¯ä¸è¶…ç«‹æ–¹ä½“çš„ä½“ç§¯ä¹‹æ¯”ï¼Œ

$$ \frac{\pi^{\frac{m}{2}}}{m 2^{m-1} \Gamma\left(\frac{m}{2}\right)} \to 0 \quad \text{as} \quad m \to \infty $$

+   å›å¿†ï¼Œ$\Gamma(ğ‘›)=(ğ‘›âˆ’1)!$.

+   é«˜ç»´ç©ºé—´å…¨æ˜¯è§’è½è€Œæ²¡æœ‰ä¸­é—´éƒ¨åˆ†ï¼Œå¤§å¤šæ•°é«˜ç»´ç©ºé—´ç¦»ä¸­é—´éƒ¨åˆ†å¾ˆè¿œï¼ˆå…¨æ˜¯è§’è½ï¼ï¼‰ã€‚

+   å› æ­¤ï¼Œåœ¨å¤šç»´ç©ºé—´ä¸­ï¼Œè·ç¦»å¤±å»äº†æ•æ„Ÿæ€§ï¼Œå³å¯¹äºç©ºé—´ä¸­çš„ä»»ä½•éšæœºç‚¹ï¼Œé¢„æœŸçš„æˆå¯¹è·ç¦»éƒ½å˜å¾—ç›¸åŒï¼Œ

$$ \lim_{m \to \infty} \left( \mathbb{E}\left[\text{dist}_{\text{max}}(m) - \text{dist}_{\text{min}}(m)\right] \right) \to 0 $$

+   éšæœºç‚¹åœ¨è¶…ç©ºé—´ä¸­æˆå¯¹è·ç¦»èŒƒå›´çš„æœŸæœ›æé™è¶‹äºé›¶ã€‚å¦‚æœè·ç¦»å‡ ä¹éƒ½ç›¸åŒï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»å°±ä¸å†æœ‰æ„ä¹‰äº†ï¼

![](img/8c8d512cca4eb330150d1ba298831543.png)

è¶…ç«‹æ–¹ä½“å†…è¶…çƒä½“ä½“ç§¯çš„æ¯”ç‡ã€‚

+   è¿™é‡Œæ˜¯å„ç§ç»´åº¦ä¸‹çš„æ‰­æ›²ç¨‹åº¦ï¼Œ

| m | nD / 2D |
| --- | --- |
| 2 | 1.0 |
| 5 | 0.28 |
| 10 | 0.003 |
| 20 | 0.00000003 |

1.  **Multicollinearity** - é«˜ç»´æ•°æ®é›†æ›´æœ‰å¯èƒ½å‡ºç°å…±çº¿æ€§æˆ–å¤šé‡å…±çº¿æ€§ã€‚

+   ç”±å…¶ä»–ç‰¹å¾çº¿æ€§æè¿°çš„ç‰¹å¾å¯¼è‡´æ¨¡å‹æ–¹å·®é«˜ã€‚

## ä»€ä¹ˆæ˜¯ç‰¹å¾æ’åï¼Ÿ

ç‰¹å¾æ’åæ˜¯ä¸€ç»„æŒ‡æ ‡ï¼Œå®ƒä»¬æ ¹æ®åŒ…å«åœ¨æ¨ç†ä¸­çš„ä¿¡æ¯å’Œé¢„æµ‹å“åº”ç‰¹å¾çš„é‡è¦æ€§ï¼Œä¸ºæ¯ä¸ªé¢„æµ‹ç‰¹å¾åˆ†é…ç›¸å¯¹é‡è¦æ€§æˆ–ä»·å€¼ã€‚æœ‰å„ç§å„æ ·çš„å¯èƒ½æ–¹æ³•æ¥å®Œæˆè¿™é¡¹ä»»åŠ¡ã€‚æˆ‘çš„å»ºè®®æ˜¯é‡‡ç”¨**â€œå®½æ•°ç»„â€**æ–¹æ³•ï¼Œç»“åˆå¤šç§åˆ†æå’ŒæŒ‡æ ‡ï¼ŒåŒæ—¶ç†è§£æ¯ç§æ–¹æ³•çš„å‡è®¾å’Œå±€é™æ€§ã€‚

è¿™é‡Œæ˜¯æˆ‘ä»¬è€ƒè™‘ç”¨äºç‰¹å¾æ’åçš„ä¸€èˆ¬ç±»å‹æŒ‡æ ‡ã€‚

1.  æ•°æ®åˆ†å¸ƒå’Œæ•£ç‚¹å›¾çš„è§†è§‰æ£€æŸ¥

1.  ç»Ÿè®¡æ‘˜è¦

1.  åŸºäºæ¨¡å‹

1.  é€’å½’ç‰¹å¾æ¶ˆé™¤

æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸åº”å¿½è§†ä¸“å®¶çŸ¥è¯†ã€‚å¦‚æœå…³äºç‰©ç†è¿‡ç¨‹ã€å› æœå…³ç³»ã€é¢„æµ‹ç‰¹å¾çš„å¯ä¿¡åº¦å’Œå¯ç”¨æ€§æœ‰é¢å¤–çš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯åº”æ•´åˆåˆ°åˆ†é…ç‰¹å¾æ’åä¸­ã€‚

## åŠ è½½æ‰€éœ€çš„åº“

ä»¥ä¸‹ä»£ç åŠ è½½äº†æ‰€éœ€çš„åº“ã€‚

```py
import geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper
import geostatspy.geostats as geostats                        # GSLIB methods convert to Python 
import geostatspy
print('GeostatsPy version: ' + str(geostatspy.__version__)) 
```

```py
GeostatsPy version: 0.0.71 
```

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åŒ…åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
ignore_warnings = True                                        # ignore warnings?
import numpy as np                                            # ndarrays for gridded data
import pandas as pd                                           # DataFrames for tabular data
from sklearn import preprocessing                             # remove encoding error
from sklearn.feature_selection import RFE                     # for recursive feature selection
from sklearn.feature_selection import mutual_info_regression  # mutual information
from sklearn.linear_model import LinearRegression             # linear regression model
from sklearn.ensemble import RandomForestRegressor            # model-based feature importance
from sklearn import metrics                                   # measures to check our models
from statsmodels.stats.outliers_influence import variance_inflation_factor # variance inflation factor
import os                                                     # set working directory, run executables
import math                                                   # basic math operations
import random                                                 # for random numbers
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import matplotlib.ticker as mtick                             # control tick label formatting
import seaborn as sns                                         # for matrix scatter plots
from scipy import stats                                       # summary statistics
import numpy.linalg as linalg                                 # for linear algebra
import scipy.spatial as sp                                    # for fast nearest neighbor search
import scipy.signal as signal                                 # kernel for moving window calculation
from numba import jit                                         # for numerical speed up
from statsmodels.stats.weightstats import DescrStatsW
plt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements
if ignore_warnings == True:                                   
    import warnings
    warnings.filterwarnings('ignore')
cmap = plt.cm.inferno                                         # color map 
```

å¯¹äºç‰¹å¾æ’åçš„ Shapley å€¼æ–¹æ³•ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé¢å¤–çš„åŒ…ä»¥åŠå¯åŠ¨ JavaScript æ”¯æŒã€‚

+   è¿è¡Œæ­¤ä»£ç å—åï¼Œä½ åº”è¯¥ä¼šçœ‹åˆ°ä¸€ä¸ªå¸¦æœ‰æ–‡æœ¬â€˜jsâ€™çš„å…­è¾¹å½¢ï¼Œä»¥è¡¨ç¤º JavaScript å·²å°±ç»ª

```py
import sys
#!{sys.executable} -m pip install shap
import shap
shap.initjs() 
```

![](img/70b822753245ba6bb888425de8eb62b5.png)

å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½éœ€è¦é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ï¼Œç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£å¯ä»¥æä¾›æ›´å¤šå¸®åŠ©ã€‚

## è®¾è®¡è‡ªå®šä¹‰é¢œè‰²å›¾

é€šè¿‡å±è”½éæ˜¾è‘—å€¼æ¥è€ƒè™‘æ˜¾è‘—æ€§

+   ç›®å‰ä»…ç”¨äºæ¼”ç¤ºï¼Œå¯ä»¥æ ¹æ®ç»“æœç½®ä¿¡åº¦å’Œä¸ç¡®å®šæ€§æ›´æ–°æ¯ä¸ªå›¾è¡¨

```py
my_colormap = plt.cm.get_cmap('RdBu_r', 256)                  # make a custom colormap
newcolors = my_colormap(np.linspace(0, 1, 256))               # define colormap space
white = np.array([250/256, 250/256, 250/256, 1])              # define white color (4 channel)
#newcolors[26:230, :] = white                                 # mask all correlations less than abs(0.8)
#newcolors[56:200, :] = white                                 # mask all correlations less than abs(0.6)
newcolors[76:180, :] = white                                  # mask all correlations less than abs(0.4)
signif = ListedColormap(newcolors)                            # assign as listed colormap

my_colormap = plt.cm.get_cmap('inferno', 256)                 # make a custom colormap
newcolors = my_colormap(np.linspace(0, 1, 256))               # define colormap space
white = np.array([250/256, 250/256, 250/256, 1])              # define white color (4 channel)
#newcolors[26:230, :] = white                                 # mask all correlations less than abs(0.8)
newcolors[0:12, :] = white                                    # mask all correlations less than abs(0.6)
#newcolors[86:170, :] = white                                 # mask all correlations less than abs(0.4)
sign1 = ListedColormap(newcolors)                             # assign as listed colormap 
```

## å£°æ˜å‡½æ•°

è¿™é‡Œæœ‰ä¸€äº›å‡½æ•°å¯ä»¥å¸®åŠ©è®¡ç®—æ’åå’Œå…¶ä»–å›¾è¡¨çš„æŒ‡æ ‡ï¼š

+   **plot_corr** - ç»˜åˆ¶ç›¸å…³çŸ©é˜µ

+   **partial_corr** - éƒ¨åˆ†ç›¸å…³ç³»æ•°

+   **semipar_corr** - åŠéƒ¨åˆ†ç›¸å…³ç³»æ•°

+   **mutual_matrix** - äº’ä¿¡æ¯çŸ©é˜µï¼Œæ‰€æœ‰æˆå¯¹äº’ä¿¡æ¯çš„çŸ©é˜µ

+   **mutual_information_objective** - æˆ‘ä¿®æ”¹çš„ MRMR æŸå¤±å‡½æ•°ç‰ˆæœ¬ï¼ˆIxy - Ixx çš„å¹³å‡å€¼ï¼‰ç”¨äºç‰¹å¾æ’åï¼ˆä½¿ç”¨æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾ï¼‰

+   **delta_mutual_information_quotient** - é€šè¿‡æ·»åŠ å’Œåˆ é™¤ç‰¹å®šç‰¹å¾æ¥æ”¹å˜äº’ä¿¡æ¯å•†çš„å˜åŒ–ï¼ˆç”¨äºæ¯”è¾ƒä½¿ç”¨æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾ï¼‰

+   **weighted_avg_and_std** - å¹³å‡å€¼å’Œæ ‡å‡†å·®è€ƒè™‘æ•°æ®æƒé‡

+   **weighted_percentile** - è€ƒè™‘æ•°æ®æƒé‡çš„ç™¾åˆ†ä½æ•°

+   **histogram_bounds** - å‘ç›´æ–¹å›¾æ·»åŠ ç½®ä¿¡åŒºé—´

+   **add_grid** - æ·»åŠ ä¸»è¦å’Œæ¬¡è¦ç½‘æ ¼çº¿ä»¥å¢å¼ºç»˜å›¾å¯è§£é‡Šæ€§çš„ä¾¿åˆ©å‡½æ•°

è¿™é‡Œæ˜¯å‡½æ•°ï¼š

```py
def feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot
    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal)
    plt.plot(pred,metric,color='black',zorder=20)
    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)
    plt.plot([-0.5,mpred-0.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  
    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)
    plt.ylim(mmin,mmax); plt.xlim([-0.5,mpred-0.5]); add_grid();
    plt.xticks(rotation=270.0)
    return

def plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix 
    my_colormap = plt.cm.get_cmap('RdBu_r', 256)          
    newcolors = my_colormap(np.linspace(0, 1, 256))
    white = np.array([256/256, 256/256, 256/256, 1])
    white_low = int(128 - mask*128); white_high = int(128+mask*128)
    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)
    newcmp = ListedColormap(newcolors)
    m = corr_matrix.shape[0]
    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)
    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()
    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()
    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
    plt.colorbar(im, orientation = 'vertical')
    plt.title(title)
    for i in range(0,m):
        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')
        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')
    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])
    plt.xticks(rotation=270.0)

def partial_corr(C):                                          # partial correlation by Fabian Pedregosa-Izquierdo, f@bianp.net
    C = np.asarray(C)
    p = C.shape[1]
    P_corr = np.zeros((p, p), dtype=float)
    for i in range(p):
        P_corr[i, i] = 1
        for j in range(i+1, p):
            idx = np.ones(p, dtype=bool)
            idx[i] = False
            idx[j] = False
            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]
            beta_j = linalg.lstsq(C[:, idx], C[:, i])[0]
            res_j = C[:, j] - C[:, idx].dot( beta_i)
            res_i = C[:, i] - C[:, idx].dot(beta_j)
            corr = stats.pearsonr(res_i, res_j)[0]
            P_corr[i, j] = corr
            P_corr[j, i] = corr
    return P_corr

def semipartial_corr(C):                                      # Michael Pyrcz modified the function above by Fabian Pedregosa-Izquierdo, f@bianp.net for semipartial correlation

    C = np.asarray(C)
    p = C.shape[1]
    P_corr = np.zeros((p, p), dtype=float)
    for i in range(p):
        P_corr[i, i] = 1
        for j in range(i+1, p):
            idx = np.ones(p, dtype=bool)
            idx[i] = False
            idx[j] = False
            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]
            res_j = C[:, j] - C[:, idx].dot( beta_i)
            res_i = C[:, i] 
            corr = stats.pearsonr(res_i, res_j)[0]
            P_corr[i, j] = corr
            P_corr[j, i] = corr
    return P_corr

def mutual_matrix(df,features):                               # calculate mutual information matrix
    mutual = np.zeros([len(features),len(features)])
    for i, ifeature in enumerate(features):
        for j, jfeature in enumerate(features):
            if i != j:
                mutual[i,j] = mutual_info_regression(df.iloc[:,i].values.reshape(-1, 1),np.ravel(df.iloc[:,j].values))[0]
    mutual /= np.max(mutual) 
    for i, ifeature in enumerate(features):
        mutual[i,i] = 1.0
    return mutual

def mutual_information_objective(x,y):                        # modified from MRMR loss function, Ixy - average(Ixx)
    mutual_information_quotient = []
    for i, icol in enumerate(x.columns):
        Vx = mutual_info_regression(x.iloc[:,i].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1)))
        Ixx_mat = []
        for m, mcol in enumerate(x.columns):
            if i != m:
                Ixx_mat.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,i].values.reshape(-1, 1))))
        Wx = np.average(Ixx_mat)
        mutual_information_quotient.append(Vx/Wx)
    mutual_information_quotient  = np.asarray(mutual_information_quotient).reshape(-1)
    return mutual_information_quotient

def delta_mutual_information_quotient(x,y):                   # standard mutual information quotient
    delta_mutual_information_quotient = []               

    Ixy = []
    for m, mcol in enumerate(x.columns):
        Ixy.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1))))
    Vs = np.average(Ixy)
    Ixx = []
    for m, mcol in enumerate(x.columns):
        for n, ncol in enumerate(x.columns):
            Ixx.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,n].values.reshape(-1, 1))))
    Ws = np.average(Ixx) 

    for i, icol in enumerate(x.columns):          
        Ixy_s = []                                          
        for m, mcol in enumerate(x.columns):
            if m != i:
                Ixy_s.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1))))
        Vs_s = np.average(Ixy_s)
        Ixx_s = []
        for m, mcol in enumerate(x.columns):
            if m != i:
                for n, ncol in enumerate(x.columns):
                    if n != i:
                        Ixx_s.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,n].values.reshape(-1, 1))))                  
        Ws_s = np.average(Ixx_s)
        delta_mutual_information_quotient.append((Vs/Ws)-(Vs_s/Ws_s))

    delta_mutual_information_quotient  = np.asarray(delta_mutual_information_quotient).reshape(-1)  
    return delta_mutual_information_quotient

def weighted_avg_and_std(values, weights):                    # calculate weighted statistics (Eric O Lebigot, stack overflow)
    average = np.average(values, weights=weights)
    variance = np.average((values-average)**2, weights=weights)
    return (average, math.sqrt(variance))

def weighted_percentile(data, weights, perc):                 # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) 
    ix = np.argsort(data)
    data = data[ix] 
    weights = weights[ix] 
    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) 
    return np.interp(perc, cdf, data)

def histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram 
    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)
    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')
    plt.plot([avg,avg],[0.0,45],color = color)
    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')

def add_grid():                                               # add major and minor gridlines
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("d:/PGE383")                                   # set the working directory 
```

æ‚¨å°†ä¸å¾—ä¸æ›´æ–°å¼•å·ä¸­çš„éƒ¨åˆ†ä»¥åŒ…å«æ‚¨è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”æ ¼å¼åœ¨ Mac ä¸Šä¸åŒï¼ˆä¾‹å¦‚ï¼Œâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½è¡¨æ ¼æ•°æ®

è¿™æ˜¯å°†æˆ‘ä»¬çš„é€—å·åˆ†éš”æ•°æ®æ–‡ä»¶åŠ è½½åˆ° Pandas DataFrame å¯¹è±¡çš„å‘½ä»¤ã€‚

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒç©ºé—´æ•°æ®é›†â€˜unconv_MV.csvâ€™ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…æ‹¬æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   äº•å¹³å‡å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   å²©çŸ³è„†æ€§æ¯”ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   ç»ç’ƒè´¨åå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹ç”Ÿäº§ 90 å¤©å¹³å‡ï¼ˆMCFPDï¼‰ã€‚

æ³¨æ„ï¼Œæ•°æ®é›†æ˜¯åˆæˆçš„ã€‚

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†å…¶åŠ è½½åˆ°åä¸ºâ€˜my_dataâ€™çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

```py
idata = 0
if idata == 0:
    df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository 

    response = 'Prod'                                             # specify the response feature
    x = df.copy(deep = True); x = x.drop(['Well',response],axis='columns') # make predictor and response DataFrames
    Y = df.loc[:,response]

    features = x.columns.values.tolist() + [Y.name]               # store the names of the features
    pred = x.columns.values.tolist()
    resp = Y.name

    xmin = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting
    Ymin = 500.0; Ymax = 9000.0

    predlabel = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Brittleness Ratio (%)', # set the names for plotting
                 'Total Organic Carbon (%)','Vitrinite Reflectance (%)']
    resplabel = 'Normalized Initial Production (MCFPD)'

    predtitle = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting
                 'Total Organic Carbon','Vitrinite Reflectance']
    resptitle = 'Normalized Initial Production'

    featurelabel = predlabel + [resplabel]                        # make feature labels and titles for concise code
    featuretitle = predtitle + [resptitle]

    m = len(pred) + 1
    mpred = len(pred)

# elif idata == 1:
#     names = {'Porosity':'Por'}

#     df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository 
#     df = df.rename(columns=names)
#     df['Por'] = df['Por'] * 100.0; df['AI'] = df['AI'] / 1000.0; 
#     df.drop('Unnamed: 0',axis=1,inplace=True) 

#     features = df.columns.values.tolist()                          # store the names of the features

#     xmin = [0.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax = [10000.0,10000.0,1.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting

#     flabel = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Facies (categorical)',
#               'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting

#     ftitle = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',
#               'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']

elif idata == 2:  
    df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository 

    response = 'CumulativeOil'                                             # specify the response feature
    x = df.copy(deep = True); x = x.drop(['Well_ID','X','Y',response],axis='columns') # make predictor and response DataFrames
    Y = df.loc[:,response]

    features = x.columns.values.tolist() + [Y.name]               # store the names of the features
    pred = x.columns.values.tolist()
    resp = Y.name

    xmin = [1.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax = [75.0,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting
    Ymin = 0.0; Ymax = 3000.0

    predlabel = ['Well (ID)','X (m)','Y (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)',
              'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] 
    resplabel = 'Cumulative Production (MSTB)'

    predtitle = ['Well','X','Y','Porosity','Permeability','Acoustic Impedance',
              'Density (g/cmÂ³)','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus'] 
    resptitle = 'Cumulative Production'

    featurelabel = predlabel + [resplabel]                        # make feature labels and titles for concise code
    featuretitle = predtitle + [resptitle]

    m = len(pred) + 1
    mpred = len(pred) 
```

```py
---------------------------------------------------------------------------
SSLCertVerificationError  Traceback (most recent call last)
File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:1317, in AbstractHTTPHandler.do_open(self, http_class, req, **http_conn_args)
  1316 try:
-> 1317     h.request(req.get_method(), req.selector, req.data, headers,
  1318               encode_chunked=req.has_header('Transfer-encoding'))
  1319 except OSError as err: # timeout error

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1230, in HTTPConnection.request(self, method, url, body, headers, encode_chunked)
  1229  """Send a complete request to the server."""
-> 1230 self._send_request(method, url, body, headers, encode_chunked)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1276, in HTTPConnection._send_request(self, method, url, body, headers, encode_chunked)
  1275     body = _encode(body, 'body')
-> 1276 self.endheaders(body, encode_chunked=encode_chunked)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1225, in HTTPConnection.endheaders(self, message_body, encode_chunked)
  1224     raise CannotSendHeader()
-> 1225 self._send_output(message_body, encode_chunked=encode_chunked)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1004, in HTTPConnection._send_output(self, message_body, encode_chunked)
  1003 del self._buffer[:]
-> 1004 self.send(msg)
  1006 if message_body is not None:
  1007 
  1008     # create a consistent interface to message_body

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:944, in HTTPConnection.send(self, data)
  943 if self.auto_open:
--> 944     self.connect()
  945 else:

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\http\client.py:1399, in HTTPSConnection.connect(self)
  1397     server_hostname = self.host
-> 1399 self.sock = self._context.wrap_socket(self.sock,
  1400                                       server_hostname=server_hostname)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\ssl.py:500, in SSLContext.wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)
  494 def wrap_socket(self, sock, server_side=False,
  495                 do_handshake_on_connect=True,
  496                 suppress_ragged_eofs=True,
  497                 server_hostname=None, session=None):
  498     # SSLSocket class handles server_hostname encoding before it calls
  499     # ctx._wrap_socket()
--> 500     return self.sslsocket_class._create(
  501         sock=sock,
  502         server_side=server_side,
  503         do_handshake_on_connect=do_handshake_on_connect,
  504         suppress_ragged_eofs=suppress_ragged_eofs,
  505         server_hostname=server_hostname,
  506         context=self,
  507         session=session
  508     )

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\ssl.py:1040, in SSLSocket._create(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)
  1039             raise ValueError("do_handshake_on_connect should not be specified for non-blocking sockets")
-> 1040         self.do_handshake()
  1041 except (OSError, ValueError):

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\ssl.py:1309, in SSLSocket.do_handshake(self, block)
  1308         self.settimeout(None)
-> 1309     self._sslobj.do_handshake()
  1310 finally:

SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)

During handling of the above exception, another exception occurred:

URLError  Traceback (most recent call last)
Cell In[7], line 3
  1 idata = 0
  2 if idata == 0:
----> 3     df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository 
  5     response = 'Prod'                                             # specify the response feature
  6     x = df.copy(deep = True); x = x.drop(['Well',response],axis='columns') # make predictor and response DataFrames

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:912, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)
  899 kwds_defaults = _refine_defaults_read(
  900     dialect,
  901     delimiter,
   (...)
  908     dtype_backend=dtype_backend,
  909 )
  910 kwds.update(kwds_defaults)
--> 912 return _read(filepath_or_buffer, kwds)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:577, in _read(filepath_or_buffer, kwds)
  574 _validate_names(kwds.get("names", None))
  576 # Create the parser.
--> 577 parser = TextFileReader(filepath_or_buffer, **kwds)
  579 if chunksize or iterator:
  580     return parser

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:1407, in TextFileReader.__init__(self, f, engine, **kwds)
  1404     self.options["has_index_names"] = kwds["has_index_names"]
  1406 self.handles: IOHandles | None = None
-> 1407 self._engine = self._make_engine(f, self.engine)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\parsers\readers.py:1661, in TextFileReader._make_engine(self, f, engine)
  1659     if "b" not in mode:
  1660         mode += "b"
-> 1661 self.handles = get_handle(
  1662     f,
  1663     mode,
  1664     encoding=self.options.get("encoding", None),
  1665     compression=self.options.get("compression", None),
  1666     memory_map=self.options.get("memory_map", False),
  1667     is_text=is_text,
  1668     errors=self.options.get("encoding_errors", "strict"),
  1669     storage_options=self.options.get("storage_options", None),
  1670 )
  1671 assert self.handles is not None
  1672 f = self.handles.handle

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:716, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)
  713     codecs.lookup_error(errors)
  715 # open URLs
--> 716 ioargs = _get_filepath_or_buffer(
  717     path_or_buf,
  718     encoding=encoding,
  719     compression=compression,
  720     mode=mode,
  721     storage_options=storage_options,
  722 )
  724 handle = ioargs.filepath_or_buffer
  725 handles: list[BaseBuffer]

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:368, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)
  366 # assuming storage_options is to be interpreted as headers
  367 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)
--> 368 with urlopen(req_info) as req:
  369     content_encoding = req.headers.get("Content-Encoding", None)
  370     if content_encoding == "gzip":
  371         # Override compression based on Content-Encoding header

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:270, in urlopen(*args, **kwargs)
  264  """
  265 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of
  266 the stdlib.
  267 """
  268 import urllib.request
--> 270 return urllib.request.urlopen(*args, **kwargs)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:222, in urlopen(url, data, timeout, cafile, capath, cadefault, context)
  220 else:
  221     opener = _opener
--> 222 return opener.open(url, data, timeout)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)
  522     req = meth(req)
  524 sys.audit('urllib.Request', req.full_url, req.data, req.headers, req.get_method())
--> 525 response = self._open(req, data)
  527 # post-process response
  528 meth_name = protocol+"_response"

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:542, in OpenerDirector._open(self, req, data)
  539     return result
  541 protocol = req.type
--> 542 result = self._call_chain(self.handle_open, protocol, protocol +
  543                           '_open', req)
  544 if result:
  545     return result

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:502, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)
  500 for handler in handlers:
  501     func = getattr(handler, meth_name)
--> 502     result = func(*args)
  503     if result is not None:
  504         return result

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:1360, in HTTPSHandler.https_open(self, req)
  1359 def https_open(self, req):
-> 1360     return self.do_open(http.client.HTTPSConnection, req,
  1361         context=self._context, check_hostname=self._check_hostname)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\urllib\request.py:1320, in AbstractHTTPHandler.do_open(self, http_class, req, **http_conn_args)
  1317         h.request(req.get_method(), req.selector, req.data, headers,
  1318                   encode_chunked=req.has_header('Transfer-encoding'))
  1319     except OSError as err: # timeout error
-> 1320         raise URLError(err)
  1321     r = h.getresponse()
  1322 except:

URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)> 
```

+   æˆ‘ä»¬è¿˜å¯ä»¥ä¸ºç»˜å›¾å»ºç«‹ç‰¹å¾èŒƒå›´ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¦‚ä¸‹ä»£ç ç›´æ¥ä»æ•°æ®ä¸­è®¡ç®—ç‰¹å¾èŒƒå›´ï¼š

```py
Pormin = np.min(df['Por'].values)                             # extract ndarray of data table column
Pormax = np.max(df['Por'].values)                             # and calculate min and max 
```

ä½†æ˜¯ï¼Œè¿™ä¸ä¼šå¯¼è‡´æ˜“äºç†è§£çš„è‰²æ¡å’Œåæ ‡è½´åˆ»åº¦ï¼Œè®©æˆ‘ä»¬é€‰æ‹©æ–¹ä¾¿çš„æ•´æ•°ã€‚æˆ‘ä»¬è¿˜å°†å£°æ˜ç‰¹å¾æ ‡ç­¾ä»¥æ–¹ä¾¿ç»˜å›¾ã€‚

## å¯è§†åŒ– DataFrame

å¯è§†åŒ– DataFrame æ˜¯æ•°æ®çš„ç¬¬ä¸€æ­¥æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ï¼Œç­‰ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨â€˜headâ€™ DataFrame æˆå‘˜å‡½æ•°æ¥é¢„è§ˆï¼ˆæ ¼å¼æ•´æ´ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚

+   æ·»åŠ å‚æ•°â€˜n=13â€™ä»¥æŸ¥çœ‹æ•°æ®é›†çš„å‰ 13 è¡Œã€‚

```py
df.head(n=13)                                                 # we could also use this command for a table preview 
```

|  | äº• | å­” | æ¸— | AI | è„†æ€§ | TOC | VR | äº§ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 1 | 12.08 | 2.92 | 2.80 | 81.40 | 1.16 | 2.31 | 1695.360819 |
| 1 | 2 | 12.38 | 3.53 | 3.22 | 46.17 | 0.89 | 1.88 | 3007.096063 |
| 2 | 3 | 14.02 | 2.59 | 4.01 | 72.80 | 0.89 | 2.72 | 2531.938259 |
| 3 | 4 | 17.67 | 6.75 | 2.63 | 39.81 | 1.08 | 1.88 | 5288.514854 |
| 4 | 5 | 17.52 | 4.57 | 3.18 | 10.94 | 1.51 | 1.90 | 2859.469624 |
| 5 | 6 | 14.53 | 4.81 | 2.69 | 53.60 | 0.94 | 1.67 | 4017.374438 |
| 6 | 7 | 13.49 | 3.60 | 2.93 | 63.71 | 0.80 | 1.85 | 2952.812773 |
| 7 | 8 | 11.58 | 3.03 | 3.25 | 53.00 | 0.69 | 1.93 | 2670.933846 |
| 8 | 9 | 12.52 | 2.72 | 2.43 | 65.77 | 0.95 | 1.98 | 2474.048178 |
| 9 | 10 | 13.25 | 3.94 | 3.71 | 66.20 | 1.14 | 2.65 | 2722.893266 |
| 10 | 11 | 15.04 | 4.39 | 2.22 | 61.11 | 1.08 | 1.77 | 3828.247174 |
| 11 | 12 | 16.19 | 6.30 | 2.29 | 49.10 | 1.53 | 1.86 | 5095.810104 |
| 12 | 13 | 16.82 | 5.42 | 2.80 | 66.65 | 1.17 | 1.98 | 4091.637316 |

## è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡

åœ¨ DataFrames ä¸­ï¼Œæœ‰å¾ˆå¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ã€‚describe å‘½ä»¤æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„æ•°æ®è¡¨ï¼Œæä¾›äº†è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼å’Œå››åˆ†ä½æ•°ã€‚

+   æˆ‘ä»¬ä½¿ç”¨è½¬ç½®åªæ˜¯ä¸ºäº†è®©è¡¨æ ¼ç¿»è½¬ï¼Œä½¿å¾—ç‰¹å¾åœ¨è¡Œä¸Šï¼Œç»Ÿè®¡ä¿¡æ¯åœ¨åˆ—ä¸Šã€‚

```py
df.describe().transpose()                                     # calculate summary statistics for the data 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Well | 200.0 | 100.500000 | 57.879185 | 1.000000 | 50.750000 | 100.500000 | 150.250000 | 200.000000 |
| Por | 200.0 | 14.991150 | 2.971176 | 6.550000 | 12.912500 | 15.070000 | 17.402500 | 23.550000 |
| Perm | 200.0 | 4.330750 | 1.731014 | 1.130000 | 3.122500 | 4.035000 | 5.287500 | 9.870000 |
| AI | 200.0 | 2.968850 | 0.566885 | 1.280000 | 2.547500 | 2.955000 | 3.345000 | 4.630000 |
| Brittle | 200.0 | 48.161950 | 14.129455 | 10.940000 | 37.755000 | 49.510000 | 58.262500 | 84.330000 |
| TOC | 200.0 | 0.990450 | 0.481588 | -0.190000 | 0.617500 | 1.030000 | 1.350000 | 2.180000 |
| VR | 200.0 | 1.964300 | 0.300827 | 0.930000 | 1.770000 | 1.960000 | 2.142500 | 2.870000 |
| Prod | 200.0 | 3864.407081 | 1553.277558 | 839.822063 | 2686.227611 | 3604.303506 | 4752.637555 | 8590.384044 |

ç‰¹å¾æ’åå®é™…ä¸Šæ˜¯ä¸€é¡¹ç†è§£ç‰¹å¾åŠå…¶ç›¸äº’å…³ç³»çš„åŠªåŠ›ã€‚æˆ‘ä»¬å°†ä»åŸºæœ¬çš„æ•°æ®å¯è§†åŒ–å¼€å§‹ï¼Œç„¶åè½¬å‘æ›´å¤æ‚çš„æ–¹æ³•ï¼Œå¦‚åç›¸å…³å’Œé€’å½’ç‰¹å¾æ¶ˆé™¤ã€‚

## è¦†ç›–ç‡

è®©æˆ‘ä»¬ä»ç‰¹å¾è¦†ç›–çš„æ¦‚å¿µå¼€å§‹ã€‚

+   å¦‚æœä¸€ä¸ªç‰¹å¾åœ¨æ ·æœ¬ä¸­çš„æ¯”ä¾‹å¾ˆå°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯èƒ½ä¸æƒ³åŒ…å«å®ƒï¼Œå› ä¸ºå®ƒä¼šå¯¼è‡´ç‰¹å¾æ’è¡¥ã€ç¼ºå¤±æ•°æ®ä¼°è®¡ç­‰é—®é¢˜ã€‚

+   é€šè¿‡ç§»é™¤ä¸€äº›è¦†ç›–ç‡è¾ƒå·®çš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šæ”¹è¿›æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå› ä¸ºç‰¹å¾æ’è¡¥å­˜åœ¨å±€é™æ€§ï¼Œç‰¹å¾æ’è¡¥å®é™…ä¸Šä¼šåœ¨ç»Ÿè®¡æ•°æ®å’Œæˆ‘ä»¬çš„é¢„æµ‹æ¨¡å‹ä¸­å¼•å…¥åå·®å’Œé¢å¤–çš„è¯¯å·®

+   å¦‚æœåº”ç”¨ç±»ä¼¼çš„åˆ é™¤æ¥å¤„ç†ç¼ºå¤±å€¼ï¼Œè¦†ç›–ç‡ä½çš„ç‰¹å¾ä¼šå¯¼è‡´å¤§é‡æ•°æ®è¢«åˆ é™¤ï¼

è®©æˆ‘ä»¬ä»å¸¦æœ‰ç¼ºå¤±è®°å½•æ¯”ä¾‹çš„æŸ±çŠ¶å›¾å¼€å§‹ï¼š

```py
plt.subplot(111)
(df.isnull().sum()/len(df)).plot(kind = 'bar')                # calculate DataFrame with percentage missing by feature
plt.xlabel('Feature'); plt.ylabel('Percentage of Missing Values'); plt.title('Data Completeness'); plt.ylim([0.0,1.0])
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.2); add_grid(); plt.show() 
```

![å›¾ç‰‡](img/e5a834b70ca47ad151aee5749adc53ce.png)

å¯¹äºæä¾›çš„ç¤ºä¾‹æ•°æ®é›†ï¼Œå›¾è¡¨åº”è¯¥æ˜¯ç©ºçš„ã€‚æ²¡æœ‰ç¼ºå¤±æ•°æ®ï¼Œå› æ­¤æ‰€æœ‰ç‰¹å¾çš„â€œç¼ºå¤±è®°å½•æ¯”ä¾‹â€éƒ½æ˜¯ 0.0ã€‚

å¦‚æœä½ æƒ³æµ‹è¯•è¿™ä¸ªå›¾è¡¨å¹¶åŒ…å«ä¸€äº›ç¼ºå¤±æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```py
proportion_NaN = 0.1                                    # proportion of values in DataFrame to remove

remove = np.random.random(df.shape) < proportion_NaN    # make the boolean array for removal
print('Fraction of removed values in mask ndarray = ' + str(round(remove.sum()/remove.size,3)) + '.')

df_mask = df.mask(remove)                               # make a new DataFrame with specified proportion removed 
```

åˆ é™¤æ­¤ä»£ç å¹¶é‡æ–°åŠ è½½æ•°æ®ï¼Œä»¥ç»§ç»­è·å¾—ä¸ä»¥ä¸‹è®¨è®ºä¸€è‡´çš„ç»“æœã€‚

è¿™å¹¶æ²¡æœ‰è®²è¿°æ•´ä¸ªæ•…äº‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç‰¹å¾ A çš„ 20%ç¼ºå¤±ï¼Œç‰¹å¾ B çš„ 20%ç¼ºå¤±ï¼Œè¿™äº›æ˜¯å¦æ˜¯ç›¸åŒçš„æ ·æœ¬å’Œä¸åŒçš„æ ·æœ¬ã€‚å¦‚æœä½ æ‰§è¡Œç±»ä¼¼çš„åˆ é™¤ï¼Œè¿™ä¼šæœ‰å·¨å¤§çš„å½±å“ã€‚

+   å¦‚æœæ•°æ®ä¸æ˜¯å¤ªå¤šï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥åœ¨è¿™æ ·çš„å¸ƒå°”è¡¨ä¸­å¯è§†åŒ–æ‰€æœ‰æ ·æœ¬å’Œç‰¹å¾çš„æ•°æ®è¦†ç›–æƒ…å†µã€‚

+   æ­¤æ–¹æ³•å¯èƒ½è¯†åˆ«å‡ºå…·æœ‰è®¸å¤šç¼ºå¤±ç‰¹å¾çš„ç‰¹å®šæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬å¯èƒ½è¢«ç§»é™¤ä»¥æé«˜æ•´ä½“è¦†ç›–èŒƒå›´æˆ–å¯¼è‡´é‡‡æ ·åå·®çš„ç¼ºå¤±æ•°æ®ä¸­çš„å…¶ä»–è¶‹åŠ¿æˆ–ç»“æ„ã€‚

```py
df_temp = df.copy(deep=True)                                  # make a deep copy of the DataFrame
df_bool = df_temp.isnull()                                    # true is value, false if NaN
#df_bool = df_bool.set_index(df_temp.pop('UWI'))              # set the index / feature for the heat map y column
heat = sns.heatmap(df_bool, cmap=['r','w'], annot=False, fmt='.0f',cbar=False,linecolor='black',linewidth=0.1) # make the binary heat map, no bins
heat.set_xticklabels(heat.get_xticklabels(), rotation=90, fontsize=8)
heat.set_yticklabels(heat.get_yticklabels(), rotation=0, fontsize=8)
heat.set_title('Data Completeness Heatmap',fontsize=16); heat.set_xlabel('Feature',fontsize=12); heat.set_ylabel('Sample (Index)',fontsize=12)
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.8, top=1.6, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/5849f3ca4f8301f49db8799952b591b62475a866fcaa2646497f042118e36aa0.png](img/9b8b2b5fe0360995f89df5950e3ed23d.png)

å†æ¬¡å¼ºè°ƒï¼Œå¯¹äºå…·æœ‰å®Œç¾è¦†ç›–çš„æä¾›æ•°æ®é›†ï¼Œæ­¤å›¾åº”è¯¥ç›¸å½“æ— èŠï¼Œæ¯ä¸ªå•å…ƒæ ¼éƒ½åº”è¯¥ç”¨çº¢è‰²å¡«å……ã€‚

+   æ·»åŠ ä»£ç ä»¥åˆ é™¤ä¸€äº›è®°å½•ä»¥æµ‹è¯•æ­¤å›¾ã€‚ç©ºç™½å•å…ƒæ ¼è¡¨ç¤ºç¼ºå¤±è®°å½•ã€‚

### ç‰¹å¾æ’è¡¥

è¯·å‚é˜…å…³äºç‰¹å¾æ’è¡¥çš„ç« èŠ‚ï¼Œäº†è§£å¦‚ä½•å¤„ç†ç¼ºå¤±æ•°æ®ã€‚

ç›®å‰åœ¨è¿™é‡Œè¿›è¡Œç®€è¦å¤„ç†ï¼Œæˆ‘ä»¬åªæ˜¯åº”ç”¨ç±»ä¼¼çš„åˆ é™¤å¹¶ç»§ç»­ã€‚

+   æˆ‘ä»¬ç§»é™¤æ‰€æœ‰å…·æœ‰ä»»ä½•ç¼ºå¤±ç‰¹å¾å€¼çš„æ ·æœ¬ã€‚è™½ç„¶è¿™å¾ˆç®€å•ï¼Œä½†è¿™æ˜¯ç¡®ä¿æˆ‘ä»¬å³å°†å±•ç¤ºçš„ç‰¹å¾æ’åæ–¹æ³•æ‰€éœ€å®Œç¾è¦†ç›–çš„ä¸€ç§ç®€å•ç²—æš´çš„æ–¹æ³•ã€‚è¯·æŸ¥çœ‹ä¸Šé¢é“¾æ¥çš„å·¥ä½œæµç¨‹ä¸­çš„å…¶ä»–æ–¹æ³•ã€‚

```py
df.dropna(axis=0,how='any',inplace=True)                      # likewise deletion 
```

### ç‰¹å¾æ’è¡¥

è¯·å‚é˜…å…³äºç‰¹å¾æ’è¡¥çš„ç« èŠ‚ï¼Œäº†è§£å¦‚ä½•å¤„ç†ç¼ºå¤±æ•°æ®ã€‚

ç›®å‰åœ¨è¿™é‡Œè¿›è¡Œç®€è¦å¤„ç†ï¼Œæˆ‘ä»¬åªæ˜¯åº”ç”¨ç±»ä¼¼çš„åˆ é™¤å¹¶ç»§ç»­ã€‚

+   æˆ‘ä»¬ç§»é™¤æ‰€æœ‰å…·æœ‰ä»»ä½•ç¼ºå¤±ç‰¹å¾å€¼çš„æ ·æœ¬ã€‚è™½ç„¶è¿™å¾ˆç®€å•ï¼Œä½†è¿™æ˜¯ç¡®ä¿æˆ‘ä»¬å³å°†å±•ç¤ºçš„ç‰¹å¾æ’åæ–¹æ³•æ‰€éœ€å®Œç¾è¦†ç›–çš„ä¸€ç§ç®€å•ç²—æš´çš„æ–¹æ³•ã€‚è¯·æŸ¥çœ‹ä¸Šé¢é“¾æ¥çš„å·¥ä½œæµç¨‹ä¸­çš„å…¶ä»–æ–¹æ³•ã€‚

```py
df.dropna(axis=0,how='any',inplace=True)                      # likewise deletion 
```

## æ‘˜è¦ç»Ÿè®¡

åœ¨ä»»ä½•å¤šå…ƒåˆ†æå·¥ä½œä¸­ï¼Œæˆ‘ä»¬åº”è¯¥ä»å•å˜é‡åˆ†æå¼€å§‹ï¼Œä¸€æ¬¡åˆ†æä¸€ä¸ªå˜é‡çš„æ‘˜è¦ç»Ÿè®¡ã€‚æ‘˜è¦ç»Ÿè®¡æ’åæ–¹æ³•æ˜¯å®šæ€§çš„ï¼Œæˆ‘ä»¬æ­£åœ¨è¯¢é—®ï¼š

+   æ˜¯å¦å­˜åœ¨æ•°æ®é—®é¢˜ï¼Ÿ

+   æˆ‘ä»¬æ˜¯å¦ä¿¡ä»»è¿™äº›ç‰¹å¾ï¼Ÿæˆ‘ä»¬æ˜¯å¦åŒç­‰ä¿¡ä»»æ‰€æœ‰ç‰¹å¾ï¼Ÿ

+   åœ¨æˆ‘ä»¬å¼€å‘ä»»ä½•å¤šå…ƒå·¥ä½œæµç¨‹ä¹‹å‰ï¼Œæ˜¯å¦æœ‰éœ€è¦å¤„ç†çš„é—®é¢˜ï¼Ÿ

åœ¨ DataFrames ä¸­ï¼Œæœ‰è®¸å¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ã€‚`describe`å‘½ä»¤æä¾›äº†ä¸€ä¸ªç´§å‡‘çš„æ•°æ®è¡¨ï¼Œå…¶ä¸­åŒ…æ‹¬è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼å’Œå››åˆ†ä½æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨`transpose()`å‘½ä»¤ç¿»è½¬è¡¨æ ¼ï¼Œä½¿å¾—ç‰¹å¾ä½äºè¡Œä¸Šï¼Œè€Œç»Ÿè®¡å€¼ä½äºåˆ—ä¸Šã€‚

```py
df.describe().transpose()                                     # DataFrame summary statistics 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Well | 200.0 | 100.500000 | 57.879185 | 1.000000 | 50.750000 | 100.500000 | 150.250000 | 200.000000 |
| Por | 200.0 | 14.991150 | 2.971176 | 6.550000 | 12.912500 | 15.070000 | 17.402500 | 23.550000 |
| Perm | 200.0 | 4.330750 | 1.731014 | 1.130000 | 3.122500 | 4.035000 | 5.287500 | 9.870000 |
| AI | 200.0 | 2.968850 | 0.566885 | 1.280000 | 2.547500 | 2.955000 | 3.345000 | 4.630000 |
| Brittle | 200.0 | 48.161950 | 14.129455 | 10.940000 | 37.755000 | 49.510000 | 58.262500 | 84.330000 |
| TOC | 200.0 | 0.990450 | 0.481588 | -0.190000 | 0.617500 | 1.030000 | 1.350000 | 2.180000 |
| VR | 200.0 | 1.964300 | 0.300827 | 0.930000 | 1.770000 | 1.960000 | 2.142500 | 2.870000 |
| Prod | 200.0 | 3864.407081 | 1553.277558 | 839.822063 | 2686.227611 | 3604.303506 | 4752.637555 | 8590.384044 |

æ‘˜è¦ç»Ÿè®¡æ˜¯æ•°æ®æ£€æŸ¥çš„å…³é”®ç¬¬ä¸€æ­¥ã€‚

+   è¿™åŒ…æ‹¬æ¯ä¸ªç‰¹å¾çš„æœ‰æ•ˆï¼ˆéç©ºï¼‰å€¼çš„æ•°é‡ï¼ˆè®¡æ•°ä»æ¯ä¸ªå˜é‡çš„æ€»æ•°ä¸­ç§»é™¤äº†æ‰€æœ‰ np.NaNï¼‰ã€‚

+   æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€èˆ¬è¡Œä¸ºï¼Œå¦‚ä¸­å¿ƒè¶‹åŠ¿ã€å‡å€¼å’Œåˆ†æ•£æ€§ã€æ–¹å·®ã€‚

+   æˆ‘ä»¬å¯ä»¥è¯†åˆ«å‡ºä¸æ¯ä¸ªå±æ€§å¯èƒ½çš„å€¼èŒƒå›´ä¹‹å¤–çš„è´Ÿå€¼ã€æç«¯å€¼å’Œå€¼çš„é—®é¢˜ã€‚

+   æ•°æ®çœ‹èµ·æ¥ç›¸å½“è‰¯å¥½ï¼Œä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬è·³è¿‡å¼‚å¸¸å€¼æ£€æµ‹ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å•å˜é‡åˆ†å¸ƒã€‚

## å•å˜é‡åˆ†å¸ƒ

ä¸æ‘˜è¦ç»Ÿè®¡ä¸€æ ·ï¼Œè¿™ç§æ’åºæ–¹æ³•æ˜¯å¯¹æ•°æ®é—®é¢˜çš„å®šæ€§æ£€æŸ¥ï¼Œä»¥åŠè¯„ä¼°æˆ‘ä»¬å¯¹æ¯ä¸ªç‰¹å¾çš„ä¿¡å¿ƒã€‚æœ€å¥½ä¸åŒ…å«è´¨é‡ä¿¡å¿ƒä½çš„ç‰¹å¾ï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šäº§ç”Ÿè¯¯å¯¼ï¼ˆåŒæ—¶å¦‚å‰æ‰€è¿°å¢åŠ äº†æ¨¡å‹å¤æ‚æ€§ï¼‰ã€‚

```py
nbins = 20                                                    # number of histogram bins
for i, feature in enumerate(features):                        # plot histograms with central tendency and P10 and P90 labeled
    plt.subplot(4,3,i+1)
    y,_,_ = plt.hist(x=df[feature],weights=None,bins=nbins,alpha = 0.8,edgecolor='black',color='darkorange',density=True)
    # histogram_bounds(values=df[feature].values,weights=np.ones(len(df)),color='red')
    plt.xlabel(feature); plt.ylabel('Frequency'); plt.ylim([0.0,y.max()*1.10]); plt.title(featuretitle[i]); add_grid() 
    # if feature == resp: 
    #     plt.xlim([Ymin,Ymax]) 
    # else:
    #     plt.xlim([xmin[i],xmax[i]]) 

plt.subplots_adjust(left=0.0, bottom=0.0, right=3., top=4.1, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/8a0c89fa0d885a7643839d04c23ff8be.png)

å•å˜é‡åˆ†å¸ƒçœ‹èµ·æ¥å¾ˆå¥½ï¼š

+   æ²¡æœ‰æ˜æ˜¾çš„å¼‚å¸¸å€¼ã€‚

+   ç©¿é€ç‡å‘ˆæ­£åæ€ï¼Œè¿™æ˜¯å¸¸è§çš„ç°è±¡ã€‚

+   æ ¡æ­£åçš„ç›®å½•è¡¨æœ‰ä¸€ä¸ªå°å³°å€¼ï¼Œä½†è¿™æ˜¯åˆç†çš„ã€‚

## åŒå˜é‡åˆ†å¸ƒ

çŸ©é˜µæ•£ç‚¹å›¾æ˜¯è§‚å¯Ÿå˜é‡ä¹‹é—´åŒå˜é‡å…³ç³»çš„ä¸€ç§éå¸¸æœ‰æ•ˆçš„æ–¹æ³•ã€‚

+   è¿™æ˜¯é€šè¿‡æ•°æ®å¯è§†åŒ–è¯†åˆ«æ•°æ®é—®é¢˜çš„å¦ä¸€ä¸ªæœºä¼š

+   æˆ‘ä»¬å¯ä»¥è¯„ä¼°æ˜¯å¦å­˜åœ¨å…±çº¿æ€§ï¼Œç‰¹åˆ«æ˜¯æ¯æ¬¡è¯„ä¼°ä¸¤ä¸ªç‰¹å¾ä¹‹é—´çš„ç®€å•å½¢å¼ã€‚

```py
pairgrid = sns.PairGrid(df) # matrix scatter plots
pairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)
pairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle
pairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, 
                              shade = False, shade_lowest = False, alpha = 1.0, n_levels = 10)
pairgrid.add_legend()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/eb1c7e7e9920c8c27be71cd70df81661.png)

è¿™ä¸ªå›¾ä¼ è¾¾äº†å¤§é‡çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨è¿™ä¸ªå›¾è¿›è¡Œå˜é‡æ’åºï¼Ÿ

+   æˆ‘ä»¬å¯ä»¥è¯†åˆ«å‡ºå½¼æ­¤ç´§å¯†ç›¸å…³çš„ç‰¹å¾ï¼Œä¾‹å¦‚ï¼Œå¦‚æœä¸¤ä¸ªç‰¹å¾å‡ ä¹å…·æœ‰å®Œç¾çš„å•è°ƒçº¿æ€§æˆ–è¿‘çº¿æ€§å…³ç³»ï¼Œæˆ‘ä»¬åº”è¯¥ç«‹å³åˆ é™¤å…¶ä¸­ä¸€ä¸ªã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å…±çº¿æ€§æ¡ˆä¾‹ï¼Œå¦‚ä¸Šæ‰€è¿°ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹ä¸ç¨³å®šã€‚

+   æˆ‘ä»¬å¯ä»¥æ£€æŸ¥çº¿æ€§ä¸éçº¿æ€§å…³ç³»ã€‚å¦‚æœæˆ‘ä»¬è§‚å¯Ÿåˆ°éçº¿æ€§åŒå˜é‡å…³ç³»ï¼Œè¿™å°†å½±å“æ–¹æ³•çš„é€‰æ‹©ï¼Œä»¥åŠå‡è®¾å˜é‡æ’åºä¸ºçº¿æ€§å…³ç³»çš„æ–¹æ³•çš„è´¨é‡ã€‚

+   æˆ‘ä»¬å¯ä»¥è¯†åˆ«å˜é‡ä¹‹é—´çš„çº¦æŸå…³ç³»å’Œå¼‚æ–¹å·®æ€§ã€‚å†æ¬¡å¼ºè°ƒï¼Œè¿™äº›å¯èƒ½ä¼šé™åˆ¶æˆ‘ä»¬çš„æ’åºæ–¹æ³•ï¼Œå¹¶é¼“åŠ±æˆ‘ä»¬ä¿ç•™ç‰¹å®šç‰¹å¾ä»¥åœ¨ç»“æœæ¨¡å‹ä¸­ä¿ç•™è¿™äº›ç‰¹å¾ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬å¿…é¡»è®°ä½ï¼ŒåŒå˜é‡å¯è§†åŒ–å’Œåˆ†æä¸è¶³ä»¥ç†è§£æ•°æ®ä¸­çš„æ‰€æœ‰å¤šå˜é‡å…³ç³»ï¼Œä¾‹å¦‚ï¼Œå¤šé‡å…±çº¿æ€§åŒ…æ‹¬ä¸¤ä¸ªæˆ–æ›´å¤šç‰¹å¾ä¹‹é—´å¼ºçƒˆçš„çº¿æ€§å…³ç³»ã€‚è¿™äº›å…³ç³»ä»…é€šè¿‡åŒå˜é‡å›¾å¯èƒ½éš¾ä»¥çœ‹åˆ°ã€‚

## å¯¹åº”åæ–¹å·®

é…å¯¹åæ–¹å·®æä¾›äº†æ¯ä¸ªé¢„æµ‹ç‰¹å¾ä¸å“åº”ç‰¹å¾ä¹‹é—´çº¿æ€§å…³ç³»å¼ºåº¦çš„åº¦é‡ã€‚åœ¨æ­¤é˜¶æ®µï¼Œæˆ‘ä»¬æŒ‡å®šæœ¬ç ”ç©¶çš„ç›®çš„æ˜¯ä»å…¶ä»–å¯ç”¨çš„é¢„æµ‹ç‰¹å¾ä¸­é¢„æµ‹äº§é‡ï¼Œæˆ‘ä»¬çš„å“åº”å˜é‡ã€‚æˆ‘ä»¬ç°åœ¨å¤„äºé¢„æµ‹æ€ç»´çŠ¶æ€ï¼Œè€Œä¸æ˜¯æ¨æ–­æ€ç»´çŠ¶æ€ï¼Œæˆ‘ä»¬æƒ³è¦ä¼°è®¡å‡½æ•° $\hat{f}$ ä»¥å®Œæˆæ­¤ä»»åŠ¡ï¼š

$$ Y = \hat{f}(X_1,\ldots,X_n) $$

å…¶ä¸­ $Y$ æ˜¯æˆ‘ä»¬çš„å“åº”ç‰¹å¾ï¼Œ$X_1,\ldots,X_n$ æ˜¯æˆ‘ä»¬çš„é¢„æµ‹ç‰¹å¾ã€‚å¦‚æœæˆ‘ä»¬ä¿ç•™æ‰€æœ‰é¢„æµ‹ç‰¹å¾æ¥é¢„æµ‹å“åº”ï¼Œæˆ‘ä»¬ä¼šæœ‰ï¼š

$$ Prod = \hat{f}(Por,Perm,AI,Brittle,TOC,VR) $$

ç°åœ¨å›åˆ°åæ–¹å·®ï¼Œåæ–¹å·®å®šä¹‰ä¸ºï¼š

$$ C_{xy} = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{(n-1)} $$

åæ–¹å·®ï¼š

+   è¡¡é‡çº¿æ€§å…³ç³»

+   å¯¹é¢„æµ‹å˜é‡å’Œå“åº”å˜é‡çš„åˆ†æ•£/æ–¹å·®éƒ½å¾ˆæ•æ„Ÿ

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥æ„å»ºåæ–¹å·®çŸ©é˜µï¼š

```py
df.iloc[:,1:8].cov()                                    # covariance matrix sliced predictors vs. response 
```

è¾“å‡ºæ˜¯ä¸€ä¸ªæ–°çš„ Pandas DataFrameï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åˆ‡ç‰‡æœ€åä¸€åˆ—ä»¥è·å–åŒ…å«æ‰€æœ‰é¢„æµ‹ç‰¹å¾ä¸å“åº”ä¹‹é—´åæ–¹å·®çš„ Pandas ç³»åˆ—ï¼ˆå…·æœ‰åç§°çš„ ndarrayï¼‰ã€‚

```py
covariance = df.iloc[:,df.columns.get_indexer(features)].cov().iloc[len(features)-1,:len(features)] # calculate covariance matrix and slice for only pred - resp
cov_matrix = df.iloc[:,df.columns.get_indexer(features)].cov()
plt.subplot(121)
plot_corr(cov_matrix,'Covariance Matrix',4000.0,0.1)          # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features') 

plt.subplot(122)
feature_rank_plot(features,covariance,-20000.0,20000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.1)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![_images/ff711b73e2baba3be2993b907d5023462afb3ab86692953cb31870078fc6969f.png](img/89e89e3a083da28954418714216aa9d0.png)

åæ–¹å·®æ˜¯æœ‰ç”¨çš„ï¼Œä½†æ­£å¦‚ä½ æ‰€è§ï¼Œå…¶å¤§å°ç›¸å½“å¤šå˜ã€‚

+   åæ–¹å·®çš„å¤§å°æ˜¯æ¯ä¸ªç‰¹å¾çš„å‡½æ•°ï¼Œå¹¶ä¸”ç‰¹å¾æ–¹å·®æ˜¯æœ‰äº›ä»»æ„çš„ã€‚

+   ä¾‹å¦‚ï¼Œå­”éš™ç‡çš„æ–¹å·®åœ¨åˆ†æ•°ä¸ç™¾åˆ†æ¯”ä¹‹é—´ï¼Œæˆ–è€…æ¸—é€ç‡åœ¨è¾¾è¥¿ä¸æ¯«è¾¾è¥¿ä¹‹é—´çš„æ–¹å·®æ˜¯å¤šå°‘ã€‚æˆ‘ä»¬å¯ä»¥è¡¨æ˜ï¼Œå¦‚æœæˆ‘ä»¬å¯¹ä¸€ä¸ªç‰¹å¾ $X$ åº”ç”¨ä¸€ä¸ªå¸¸æ•°ä¹˜æ•° $c$ï¼Œæ–¹å·®å°†æ ¹æ®æ­¤å…³ç³»å˜åŒ–ï¼ˆè¯æ˜åŸºäºæ–¹å·®çš„æœŸæœ›å…¬å¼ï¼‰ï¼š

$$ \sigma_{cX}Â² = cÂ² \cdot \sigma_{X}Â² $$

é€šè¿‡ä»ç™¾åˆ†æ¯”è½¬æ¢ä¸ºåˆ†æ•°ï¼Œæˆ‘ä»¬é™ä½äº†å­”éš™ç‡çš„æ–¹å·®å› å­ä¸º 10,000ï¼æ¯ä¸ªç‰¹å¾çš„æ–¹å·®å¯èƒ½æ˜¯ä»»æ„çš„ï¼Œé™¤äº†å½“æ‰€æœ‰ç‰¹å¾éƒ½åœ¨ç›¸åŒçš„å•ä½æ—¶ã€‚

é…å¯¹ç›¸å…³ç³»æ•°æ˜¯æ ‡å‡†åŒ–çš„åæ–¹å·®ï¼›å› æ­¤ï¼Œé¿å…äº†è¿™ç§ä»»æ„å¤§å°çš„é—®é¢˜ã€‚

## é…å¯¹ç›¸å…³ç³»æ•°

é…å¯¹ç›¸å…³ç³»æ•°æä¾›äº†æ¯ä¸ªé¢„æµ‹ç‰¹å¾ä¸å“åº”ç‰¹å¾ä¹‹é—´çº¿æ€§å…³ç³»å¼ºåº¦çš„åº¦é‡ã€‚

$$ \rho_{xy} = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{(n-1)\sigma_x \sigma_y}, \, -1.0 \le \rho_{xy} \le 1.0 $$

ç›¸å…³ç³»æ•°ï¼š

+   è¡¡é‡çº¿æ€§å…³ç³»

+   é€šè¿‡å°†æ¯ä¸ªç‰¹å¾çš„æ–¹å·®ä¹˜ç§¯è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ¶ˆé™¤äº†å¯¹é¢„æµ‹å˜é‡å’Œå“åº”å˜é‡åˆ†æ•£/æ–¹å·®çš„æ•æ„Ÿæ€§ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥æ„å»ºç›¸å…³çŸ©é˜µï¼š

```py
df.iloc[:,1:8].corr() 
```

è¾“å‡ºæ˜¯ä¸€ä¸ªæ–°çš„ Pandas DataFrameï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åˆ‡ç‰‡æœ€åä¸€åˆ—ä»¥è·å–åŒ…å«æ‰€æœ‰é¢„æµ‹ç‰¹å¾ä¸å“åº”ä¹‹é—´ç›¸å…³æ€§çš„ Pandas ç³»åˆ—ï¼ˆå…·æœ‰åç§°çš„ ndarrayï¼‰ã€‚

```py
correlation = df.iloc[:,df.columns.get_indexer(features)].corr().iloc[len(features)-1,:len(features)] # calculate covariance matrix and slice for only pred - resp
corr_matrix = df.iloc[:,df.columns.get_indexer(features)].corr()

plt.subplot(121)
plot_corr(corr_matrix,'Correlation Matrix',1.0,0.5)           # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(features,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/cf2f8ebbbb9381f4ae232eefb7ce2e7a.png)

ä»ç›¸å…³çŸ©é˜µä¸­æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼š

+   æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å­”éš™ç‡ã€æ¸—é€ç‡å’Œæ€»æœ‰æœºç¢³ä¸ç”Ÿäº§ä¹‹é—´æœ‰æœ€å¼ºçš„çº¿æ€§å…³ç³»ã€‚

+   å£°æ³¢é˜»æŠ—ä¸ç”Ÿäº§æœ‰å¼±è´Ÿç›¸å…³å…³ç³»ã€‚

+   è„†æ€§éå¸¸æ¥è¿‘ 0.0ã€‚å¦‚æœä½ å›é¡¾è„†æ€§ä¸ç”Ÿäº§çš„æ•£ç‚¹å›¾ï¼Œä½ ä¼šè§‚å¯Ÿåˆ°ä¸€ç§å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚å¯¹äºç”Ÿäº§æ¥è¯´ï¼Œæœ‰ä¸€ä¸ªè„†æ€§æ¯”ç‡æœ€ä½³ç‚¹ï¼ˆæ—¢ä¸å¤ªè½¯ä¹Ÿä¸å¤ªç¡¬çš„å²©çŸ³ï¼‰ï¼

æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹å®Œæ•´çš„ç›¸å…³çŸ©é˜µæ¥è¯„ä¼°é¢„æµ‹ç‰¹å¾ä¹‹é—´å†—ä½™çš„æ½œåŠ›ã€‚

+   å­”éš™ç‡ä¸æ¸—é€ç‡ä»¥åŠå­”éš™ç‡ä¸ TOC ä¹‹é—´æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§

+   TOC ä¸å£°æ³¢é˜»æŠ—ä¹‹é—´æœ‰å¾ˆå¼ºçš„è´Ÿç›¸å…³æ€§

æˆ‘ä»¬ä»ç„¶å±€é™äºä¸¥æ ¼çš„çº¿æ€§å…³ç³»ã€‚ç§©ç›¸å…³ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ”¾å®½è¿™ä¸ªå‡è®¾ã€‚

## é…å¯¹ Spearman ç§©ç›¸å…³ç³»æ•°

ç§©ç›¸å…³ç³»æ•°åœ¨è®¡ç®—ç›¸å…³ç³»æ•°ä¹‹å‰å¯¹æ•°æ®è¿›è¡Œç§©è½¬æ¢ã€‚è¦è®¡ç®—ç§©è½¬æ¢ï¼Œåªéœ€å°†æ•°æ®å€¼æ›¿æ¢ä¸ºç§© $R_x = 1,\dots,n$ï¼Œå…¶ä¸­ $n$ æ˜¯æœ€å¤§å€¼ï¼Œ1 æ˜¯æœ€å°å€¼ã€‚

$$ \rho_{R_x R_y} = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i} - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le 1.0 $$$$ x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall \, i \gt j $$$$ R_{x_i} = i $$

ç§©ç›¸å…³ï¼š

+   è¡¡é‡å•è°ƒå…³ç³»ï¼Œæ”¾å®½çº¿æ€§å‡è®¾

+   é€šè¿‡å°†æ¯ä¸ªæ ‡å‡†å·®çš„ä¹˜ç§¯è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ¶ˆé™¤äº†å¯¹é¢„æµ‹å’Œå“åº”çš„åˆ†æ•£/æ–¹å·®çš„æ•æ„Ÿæ€§ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ„å»ºç§©ç›¸å…³çŸ©é˜µå¹¶è®¡ç®— p å€¼ï¼š

```py
stats.spearmanr(df.iloc[:,1:8]) 
```

è¾“å‡ºæ˜¯ä¸€ä¸ªæ–°çš„ Pandas DataFrameï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åˆ‡ç‰‡æœ€åä¸€åˆ—ä»¥è·å–ä¸€ä¸ª Pandas ç³»åˆ—ï¼ˆå…·æœ‰åç§°çš„ ndarrayï¼‰ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰é¢„æµ‹ç‰¹å¾ä¸å“åº”ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¾—åˆ°äº†ä¸€ä¸ªéå¸¸æ–¹ä¾¿çš„*pval* 2D ndarrayï¼Œå…¶ä¸­åŒ…å«å‡è®¾æ£€éªŒçš„åŒä¾§ï¼ˆä¸¤å°¾æ±‚å’Œå¯¹ç§°åœ°è¦†ç›–ä¸¤å°¾ï¼‰p å€¼ï¼š

$$ H_0: \rho_{R_x R_y} = 0 $$$$ H_1: \rho_{R_x R_y} \ne 0 $$

è®©æˆ‘ä»¬ä¿ç•™æ‰€æœ‰é¢„æµ‹ç‰¹å¾ä¸å“åº”ç‰¹å¾ä¹‹é—´çš„ p å€¼ã€‚

```py
rank_correlation, rank_correlation_pval = stats.spearmanr(df.iloc[:,df.columns.get_indexer(features)]) # calculate the rank correlation coefficient
rank_matrix = pd.DataFrame(rank_correlation,columns=corr_matrix.columns)
rank_correlation = rank_correlation[:,len(features)-1][:len(features)]
rank_correlation_pval = rank_correlation_pval[:,len(pred)-1][:len(features)]
print("\nRank Correlation p-value:\n"); print(rank_correlation_pval)

plt.subplot(121)
plot_corr(rank_matrix,'Rank Correlation Matrix',1.0,0.5)      # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

```py
Rank Correlation p-value:

[2.43279911e-02 1.34135205e-01 1.18844068e-10 2.71646948e-04
 2.11367755e-06 0.00000000e+00 3.29170847e-04] 
```

![å›¾ç‰‡](img/609c07d08205a2c92204da55d19ad62a.png)

è¯¥çŸ©é˜µå’Œçº¿å›¾è¡¨æ˜ï¼Œç§©ç›¸å…³ç³»æ•°ä¸ç›¸å…³ç³»æ•°ç›¸ä¼¼ï¼Œè¡¨æ˜éçº¿æ€§æˆ–å¼‚å¸¸å€¼ä¸å¤ªå¯èƒ½å½±å“åŸºäºç›¸å…³æ€§çš„ç‰¹å¾æ’åã€‚

å…³äºç§©ç›¸å…³ p å€¼ï¼Œ

+   åœ¨å…¸å‹çš„Î±å€¼ä¸º 0.05 æ—¶ï¼Œåªæœ‰è„†æ€§ä¸ç”Ÿäº§ä¹‹é—´çš„ç§©ç›¸å…³å…³ç³»æ²¡æœ‰é€šè¿‡å‡è®¾æ£€éªŒï¼›å› æ­¤ï¼Œä¸ 0.0 æ²¡æœ‰æ˜¾è‘—å·®å¼‚ã€‚

æŸ¥çœ‹ç›¸å…³ç³»æ•°å’Œç§©ç›¸å…³ç³»æ•°ä¹‹é—´çš„å·®å¼‚æ˜¯æœ‰ç”¨çš„ã€‚

```py
plt.subplot(121)                                              # plot correlation matrix with significance colormap
diff = corr_matrix.values - rank_matrix.values
diff_matrix = pd.DataFrame(diff,columns=corr_matrix.columns)
plot_corr(diff_matrix,'Correlation - Rank Correlation',0.1,0.1) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

corr_diff = correlation - rank_correlation

plt.subplot(122)
feature_rank_plot(features,corr_diff,-0.20,0.20,0.0,'Correlation Coefficient - Rank Correlation Coefficient','Correlation Diffference',0.1)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/9010e510d3b644ed069447aad1564797.png)

è¿™é‡Œæœ‰ä¸€äº›æœ‰è¶£çš„è§‚å¯Ÿï¼š

+   å½“æˆ‘ä»¬å‡å°‘çº¿æ€§æ€§å’Œå¼‚å¸¸å€¼çš„å½±å“æ—¶ï¼Œå­”éš™ç‡å’Œé•œè´¨ä½“åå°„ç‡ä¸ç”Ÿäº§çš„å…³è”æ€§ä¼šæ”¹å–„

+   å½“æˆ‘ä»¬å‡å°‘çº¿æ€§æ€§å’Œå¼‚å¸¸å€¼çš„å½±å“æ—¶ï¼Œè„†æ€§ä¸ç”Ÿäº§çš„å…³è”æ€§ä¼šæ¶åŒ–

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‰€æœ‰è¿™äº›æ–¹æ³•éƒ½è€ƒè™‘äº†ä¸€ä¸ªç‰¹å¾ã€‚æˆ‘ä»¬è¿˜å¯ä»¥è€ƒè™‘è€ƒè™‘æ‰€æœ‰ç‰¹å¾çš„æ–¹æ³•ï¼Œä»¥â€œéš”ç¦»â€æ¯ä¸ªç‰¹å¾çš„å½±å“ã€‚

## éƒ¨åˆ†ç›¸å…³ç³»æ•°

è¿™æ˜¯ä¸€ä¸ªæ§åˆ¶æ‰€æœ‰å‰©ä½™å˜é‡å½±å“çš„çº¿æ€§ç›¸å…³ç³»æ•°ï¼Œ$\rho_{XY.Z}$ å’Œ $\rho_{YX.Z}$ æ˜¯åœ¨æ§åˆ¶ $Z$ å $X$ å’Œ $Y$ã€$Y$ å’Œ $X$ ä¹‹é—´çš„éƒ¨åˆ†ç›¸å…³

åœ¨ç»™å®š $Z_i, \forall \quad i = 1,\ldots, m-1$ å‰©ä½™ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œè®¡ç®— $X$ å’Œ $Y$ ä¹‹é—´çš„éƒ¨åˆ†ç›¸å…³ç³»æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æ­¥éª¤ï¼š

1.  æ‰§è¡Œçº¿æ€§ã€æœ€å°äºŒä¹˜å›å½’ï¼Œä» $Z_i, \forall \quad i = 1,\ldots, m-1$ é¢„æµ‹ $X$ã€‚$X$ é€šè¿‡é¢„æµ‹å› å­å›å½’ä»¥è®¡ç®—ä¼°è®¡å€¼ï¼Œ$X^*$

1.  åœ¨æ­¥éª¤ #1 ä¸­è®¡ç®—æ®‹å·®ï¼Œ$X-X^*$ï¼Œå…¶ä¸­ $X^* = f(Z_{1,\ldots,m-1})$ï¼Œçº¿æ€§å›å½’æ¨¡å‹

1.  æ‰§è¡Œçº¿æ€§ã€æœ€å°äºŒä¹˜å›å½’ï¼Œä» $Z_i, \forall \quad i = 1,\ldots, m-1$ é¢„æµ‹ $Y$ã€‚$Y$ é€šè¿‡é¢„æµ‹å› å­å›å½’ä»¥è®¡ç®—ä¼°è®¡å€¼ï¼Œ$Y^*$

1.  åœ¨æ­¥éª¤ #3 ä¸­è®¡ç®—æ®‹å·®ï¼Œ$Y-Y^*$ï¼Œå…¶ä¸­ $Y^* = f(Z_{1,\ldots,m-1})$ï¼Œçº¿æ€§å›å½’æ¨¡å‹

1.  è®¡ç®—æ­¥éª¤ #2 å’Œ #4 çš„æ®‹å·®ä¹‹é—´çš„ç›¸å…³ç³»æ•°ï¼Œ$\rho_{X-X^*,Y-Y^*}$

éƒ¨åˆ†ç›¸å…³æä¾›äº†åœ¨æ§åˆ¶ $Z$ ä»¥åŠå…¶ä»–ç‰¹å¾å¯¹ $X$ å’Œ $Y$ çš„å½±å“çš„æƒ…å†µä¸‹ï¼Œ$X$ å’Œ $Y$ ä¹‹é—´çº¿æ€§å…³ç³»çš„åº¦é‡ã€‚æˆ‘ä»¬ä½¿ç”¨ä¹‹å‰å£°æ˜çš„å‡½æ•°ï¼Œæ¥è‡ª Fabian Pedregosa-Izquierdoï¼Œf@bianp.netã€‚åŸå§‹ä»£ç åœ¨ GitHub ä¸Šï¼Œ[`git.io/fhyHB`](https://git.io/fhyHB)ã€‚

è¦ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å¿…é¡»å‡è®¾ï¼š

1.  æ¯”è¾ƒçš„ä¸¤ä¸ªå˜é‡ï¼Œ$X$ å’Œ $Y$

1.  éœ€è¦æ§åˆ¶çš„å…¶ä»–å˜é‡ï¼Œ$Z_{1,\ldots,m-2}$

1.  æ‰€æœ‰å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»

1.  æ²¡æœ‰æ˜¾è‘—çš„å¼‚å¸¸å€¼

1.  å˜é‡ä¹‹é—´çš„å¤§çº¦åŒå˜é‡æ­£æ€æ€§

æˆ‘ä»¬çš„æƒ…å†µç›¸å½“ä¸é”™ï¼Œä½†æœ‰ä¸€äº›ä¸åŒå˜é‡æ­£æ€æ€§çš„åå·®ã€‚æˆ‘ä»¬å¯ä»¥è€ƒè™‘é«˜æ–¯å•å˜é‡å˜æ¢æ¥æ”¹è¿›è¿™ä¸€ç‚¹ã€‚æ­¤é€‰é¡¹å°†åœ¨ç¨åæä¾›ã€‚

```py
partial_correlation = partial_corr(df.iloc[:,df.columns.get_indexer(features)]) # calculate the partial correlation coefficients
partial_matrix = pd.DataFrame(partial_correlation,columns=corr_matrix.columns)
partial_correlation = partial_correlation[:,len(features)-1][:len(features)] # extract a single row and remove production with itself 

plt.subplot(121)
plot_corr(partial_matrix,'Partial Correlation Matrix',1.0,0.5) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/11649099249c19a8d0134ee44bd96661.png)

ç°åœ¨ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å…³äºæ¯ä¸ªé¢„æµ‹ç‰¹å¾ç‹¬ç‰¹è´¡çŒ®çš„è®¸å¤šæ–°äº‹ç‰©ï¼

+   å­”éš™ç‡å’Œæ¸—é€ç‡å½¼æ­¤ä¹‹é—´é«˜åº¦ç›¸å…³ï¼Œå› æ­¤å®ƒä»¬å—åˆ°ä¸¥é‡æƒ©ç½š

+   å£°æ³¢é˜»æŠ—å’Œé•œè´¨ä½“åå°„ç‡çš„ç»å¯¹ç›¸å…³æ€§å¢åŠ ï¼Œåæ˜ äº†å®ƒä»¬ç‹¬ç‰¹çš„è´¡çŒ®

+   æ€»æœ‰æœºç¢³ç¿»è½¬äº†ç¬¦å·ï¼å½“æˆ‘ä»¬æ§åˆ¶æ‰€æœ‰å…¶ä»–å˜é‡æ—¶ï¼Œå®ƒä¸ç”Ÿäº§å‘ˆè´Ÿç›¸å…³å…³ç³»ã€‚

é€šè¿‡åç›¸å…³ç³»æ•°ï¼Œæˆ‘ä»¬å·²ç»æ§åˆ¶äº†æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾å¯¹ç‰¹å®šé¢„æµ‹ç‰¹å¾å’Œå“åº”ç‰¹å¾çš„å½±å“ã€‚åŠåç›¸å…³ç³»æ•°è¿‡æ»¤æ‰äº†æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾å¯¹åŸå§‹å“åº”å˜é‡çš„å½±å“ã€‚

## åŠåç›¸å…³ç³»æ•°

è¿™æ˜¯ä¸€ä¸ªçº¿æ€§ç›¸å…³ç³»æ•°ï¼Œå®ƒæ§åˆ¶äº†æ‰€æœ‰å‰©ä½™ç‰¹å¾ $Z$ å¯¹ $X$ çš„å½±å“ï¼Œç„¶åè®¡ç®—æ®‹å·® $X^*-X$ ä¸ $Y$ ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æ³¨æ„ï¼šæˆ‘ä»¬æ²¡æœ‰æ§åˆ¶ $Z$ ç‰¹å¾å¯¹å“åº”ç‰¹å¾ $Y$ çš„å½±å“ã€‚

è¦è®¡ç®—ç»™å®š $Z_i, \forall \quad i = 1,\ldots, m-1$ å‰©ä½™ç‰¹å¾çš„ $X$ å’Œ $Y$ ä¹‹é—´çš„åŠåç›¸å…³ç³»æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æ­¥éª¤ï¼š

1.  æ‰§è¡Œçº¿æ€§ã€æœ€å°äºŒä¹˜å›å½’ä»¥é¢„æµ‹ $X$ æ¥è‡ª $Z_i, \forall \quad i = 1,\ldots, m-1$ã€‚$X$ é€šè¿‡å‰©ä½™é¢„æµ‹ç‰¹å¾å›å½’ä»¥è®¡ç®—ä¼°è®¡å€¼ï¼Œ$X^*$

1.  åœ¨æ­¥éª¤ #1 ä¸­è®¡ç®—æ®‹å·®ï¼Œ$X-X^*$ï¼Œå…¶ä¸­ $X^* = f(Z_{1,\ldots,m-1})$ï¼Œçº¿æ€§å›å½’æ¨¡å‹

1.  è®¡ç®—æ­¥éª¤ #2 ä¸­æ®‹å·®ä¸å“åº”ç‰¹å¾ $Y$ ä¹‹é—´çš„ç›¸å…³ç³»æ•°ï¼Œ$\rho_{X-X^*,Y}$

åŠåç›¸å…³ç³»æ•°æä¾›äº† $X$ å’Œ $Y$ ä¹‹é—´çº¿æ€§å…³ç³»çš„åº¦é‡ï¼ŒåŒæ—¶æ§åˆ¶äº† $Z$ å…¶ä»–é¢„æµ‹ç‰¹å¾å¯¹é¢„æµ‹ç‰¹å¾ $X$ çš„å½±å“ï¼Œä»¥è·å¾— $X$ ç›¸å¯¹äº $Y$ çš„ç‹¬ç‰¹è´¡çŒ®ã€‚æˆ‘ä»¬ä½¿ç”¨ä¹‹å‰å£°æ˜çš„åç›¸å…³å‡½æ•°çš„ä¿®æ”¹ç‰ˆã€‚åŸå§‹ä»£ç åœ¨ GitHub ä¸Šï¼Œ[`git.io/fhyHB`](https://git.io/fhyHB)ã€‚

```py
semipartial_correlation = semipartial_corr(df.iloc[:,df.columns.get_indexer(features)])    # calculate the semi-partial correlation coefficients
semipartial_matrix = pd.DataFrame(semipartial_correlation,columns=corr_matrix.columns)
semipartial_correlation = semipartial_correlation[:,len(features)-1][:len(features)]    # extract a single row and remove production with itself

plt.subplot(121)
plot_corr(semipartial_matrix,'Semi-partial Correlation Matrix',1.0,0.5) # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/9bc790699675e3bdbd9e8ed625051fa1.png)

éœ€è¦è€ƒè™‘çš„æ›´å¤šä¿¡æ¯ï¼š

+   å­”éš™ç‡ã€æ¸—é€ç‡å’Œé•œè´¨ä½“åå°„ç‡æ˜¯æŒ‰æ­¤ç‰¹å¾æ’åºæ–¹æ³•æœ€é‡è¦çš„

+   æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾çš„ç›¸å…³æ€§éƒ½ç›¸å½“ä½

è¿™æ˜¯ä¸ªå¥½æ—¶æœºåœä¸‹æ¥ï¼Œæ€»ç»“æ‰€æœ‰å®šé‡æ–¹æ³•çš„ç»“æœã€‚æˆ‘ä»¬å°†æŠŠå®ƒä»¬å…¨éƒ¨ä¸€èµ·ç»˜åˆ¶å‡ºæ¥ã€‚

```py
# plt.subplot(151)
# feature_rank_plot(features,covariance,-5000.0,5000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.1)

plt.subplot(131)
feature_rank_plot(features,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation',0.5)

plt.subplot(132)
feature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)

plt.subplot(133)
feature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)

# plt.subplot(155)
# feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.2, wspace=0.3, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/9b87cdad7bc65f987a9f65bfe7febc3b.png)

æˆ‘è®¤ä¸ºæˆ‘ä»¬æ­£åœ¨æ”¶æ•›åˆ°å­”éš™ç‡ã€æ¸—é€ç‡å’Œé•œè´¨ä½“åå°„ç‡ä½œä¸ºä¸ç”Ÿäº§çº¿æ€§å…³ç³»æœ€é‡è¦çš„å˜é‡ã€‚

## åŸºäºç‰¹å¾å˜æ¢çš„ç‰¹å¾æ’åº

æœ‰è®¸å¤šåŸå› éœ€è¦è¿›è¡Œç‰¹å¾å˜æ¢ï¼ˆå‚è§ç›¸å…³ç« èŠ‚ï¼‰ï¼Œå¦‚ä¸Šæ‰€è¿°ï¼Œå¯¹äºåç›¸å…³å’ŒåŠåç›¸å…³ï¼Œåˆ†å¸ƒå˜æ¢å¯èƒ½æœ‰åŠ©äºç¬¦åˆåº¦é‡å‡è®¾ã€‚

+   ä½œä¸ºä¸€é¡¹ç»ƒä¹ å’Œæ£€æŸ¥ï¼Œè®©æˆ‘ä»¬æ ‡å‡†åŒ–æ‰€æœ‰ç‰¹å¾å¹¶é‡å¤ä¹‹å‰è®¡ç®—çš„å®šé‡æ–¹æ³•ã€‚æˆ‘ä»¬çŸ¥é“è¿™å°†å¯¹åæ–¹å·®äº§ç”Ÿå½±å“ï¼Œé‚£ä¹ˆå…¶ä»–æŒ‡æ ‡å‘¢ï¼Ÿ

å®Œæˆè¿™é¡¹å·¥ä½œæœ‰ä¸€å †ä»£ç ï¼Œä½†å¹¶ä¸å¤æ‚ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ DataFrameï¼Œå…¶ä¸­æ‰€æœ‰å˜é‡éƒ½å·²æ ‡å‡†åŒ–ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¿›è¡Œå¾®å°çš„ç¼–è¾‘ï¼ˆæ›´æ”¹ DataFrame åç§°ï¼‰å¹¶é‡ç”¨ä¸Šé¢çš„ä»£ç ã€‚ä½ å¯ä»¥é€‰æ‹©ï¼š

1.  æ ‡å‡†åŒ– - ä»¿å°„æ ¡æ­£ä»¥å°†åˆ†å¸ƒç¼©æ”¾åˆ° $\overline{x} = 0$ å’Œ $\sigma_x = 1.0$ã€‚

1.  æ­£æ€åˆ†æ•°å˜æ¢ - å°†æ¯ä¸ªç‰¹å¾çš„åˆ†å¸ƒè½¬æ¢ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œé«˜æ–¯å½¢çŠ¶ï¼Œå‡å€¼ä¸º $\overline{x} = 0$ï¼Œæ ‡å‡†å·®ä¸º $\sigma_x = 1.0$ã€‚

ä½¿ç”¨æ­¤å—å¯¹ç‰¹å¾è¿›è¡Œä»¿å°„æ ¡æ­£ï¼š

```py
# dfS = pd.DataFrame()                                        # affine correction, standardization to a mean of 0 and variance of 1 
# dfS['Well'] = df['Well'].values
# dfS['Por'] = GSLIB.affine(df['Por'].values,0.0,1.0)
# dfS['Perm'] = GSLIB.affine(df['Perm'].values,0.0,1.0)
# dfS['AI'] = GSLIB.affine(df['AI'].values,0.0,1.0)
# dfS['Brittle'] = GSLIB.affine(df['Brittle'].values,0.0,1.0)
# dfS['TOC'] = GSLIB.affine(df['TOC'].values,0.0,1.0)
# dfS['VR'] = GSLIB.affine(df['VR'].values,0.0,1.0)
# dfS['Prod'] = GSLIB.affine(df['Prod'].values,0.0,1.0)
# dfS.head() 
```

ä½¿ç”¨æ­¤å—å¯¹ç‰¹å¾è¿›è¡Œæ­£æ€åˆ†æ•°å˜æ¢ï¼š

```py
dfS = pd.DataFrame()                                          # Gaussian transform, standardization to a mean of 0 and variance of 1 

for feature in features:
    dfS[feature],d1,d2 = geostats.nscore(df,feature)

dfS.head() 
```

|  | Por | Perm | AI | Brittle | TOC | VR | Prod |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | -0.964092 | -0.780664 | -0.285841 | 2.432379 | 0.312053 | 1.114651 | -1.780464 |
| 1 | -0.832725 | -0.378580 | 0.446827 | -0.195502 | -0.272809 | -0.325239 | -0.392079 |
| 2 | -0.312053 | -1.069155 | 1.722384 | 2.004654 | -0.272809 | 2.241403 | -0.832725 |
| 3 | 0.730638 | 1.325516 | -0.531604 | -0.590284 | 0.131980 | -0.325239 | 0.815126 |
| 4 | 0.698283 | 0.298921 | 0.365149 | -2.870033 | 1.047216 | -0.259823 | -0.531604 |

æ— è®ºä½ é€‰æ‹©äº†å“ªç§å˜æ¢ï¼Œæ£€æŸ¥æ€»ç»“ç»Ÿè®¡ä¿¡æ¯éƒ½æ˜¯æœ€ä½³å®è·µã€‚

```py
dfS.describe()                                                # check the summary statistics 
```

|  | Por | Perm | AI | Brittle | TOC | VR | Prod |
| --- | --- | --- | --- | --- | --- | --- | --- |
| count | 200.000000 | 200.000000 | 2.000000e+02 | 2.000000e+02 | 200.000000 | 200.000000 | 2.000000e+02 |
| mean | -0.009700 | 0.010306 | 9.732356e-03 | 8.028717e-05 | 0.014152 | 0.017360 | 1.617223e-03 |
| std | 1.040456 | 1.005488 | 1.000221e+00 | 1.000278e+00 | 0.989223 | 1.000401 | 9.949811e-01 |
| min | -4.991462 | -3.355431 | -2.782502e+00 | -2.870033e+00 | -2.336891 | -2.899210 | -2.483589e+00 |
| 25% | -0.670577 | -0.647337 | -6.588985e-01 | -6.705770e-01 | -0.670577 | -0.651072 | -6.705770e-01 |
| 50% | 0.006267 | 0.006267 | 8.881784e-16 | 8.881784e-16 | 0.018807 | 0.006267 | 8.881784e-16 |
| 75% | 0.670577 | 0.678574 | 6.705770e-01 | 6.705770e-01 | 0.682378 | 0.682642 | 6.705770e-01 |
| max | 2.807034 | 2.807034 | 2.807034e+00 | 2.807034e+00 | 2.807034 | 2.807034 | 2.807034e+00 |

æˆ‘ä»¬è¿˜åº”è¯¥å†æ¬¡æ£€æŸ¥çŸ©é˜µæ•£ç‚¹å›¾ã€‚

+   å¦‚æœä½ æ‰§è¡Œäº†æ­£æ€åˆ†æ•°å˜æ¢ï¼Œä½ å·²ç»æ ‡å‡†åŒ–äº†å‡å€¼å’Œæ–¹å·®ï¼Œå¹¶çº æ­£äº†åˆ†å¸ƒçš„å•å˜é‡å½¢çŠ¶ï¼Œä½†åŒå˜é‡å…³ç³»ä»ç„¶åç¦»é«˜æ–¯ã€‚

```py
pairgrid = sns.PairGrid(dfS) # matrix scatter plots
pairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)
pairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle
pairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, 
                              shade = False, shade_lowest = False, alpha = 1.0, n_levels = 10)
pairgrid.add_legend()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/884680b3106f0f9bc10b64a1888d213dedcd55860acea49e6f5bd179d1604868.png](img/8568ba1fa581044cc577242f378dd1db.png)

è¿™æ˜¯å¸¦æœ‰æ ‡å‡†åŒ–å˜é‡çš„æ–° DataFrameã€‚ç°åœ¨æˆ‘ä»¬é‡å¤ä¹‹å‰çš„è®¡ç®—ã€‚

+   æˆ‘ä»¬è¿™æ¬¡å°†æ›´æœ‰æ•ˆç‡ï¼Œå¹¶ä½¿ç”¨ç›¸å½“ç´§å‡‘çš„ä»£ç ã€‚

```py
stand_covariance = dfS.iloc[:,dfS.columns.get_indexer(features)].cov().iloc[len(features)-1,:len(features)]
stand_correlation = dfS.iloc[:,dfS.columns.get_indexer(features)].corr().iloc[len(features)-1,:len(features)]

stand_rank_correlation, stand_rank_correlation_pval = stats.spearmanr(dfS.iloc[:,dfS.columns.get_indexer(features)])
stand_rank_correlation = stand_rank_correlation[:,len(features)-1][:len(features)]
stand_partial_correlation = partial_corr(dfS.iloc[:,dfS.columns.get_indexer(features)]) # calculate the partial correlation coefficients
stand_partial_correlation = stand_partial_correlation[:,len(features)-1][:len(features)]
stand_semipartial_correlation = semipartial_corr(dfS.iloc[:,dfS.columns.get_indexer(features)])    # calculate the semi-partial correlation coefficients
stand_semipartial_correlation = stand_semipartial_correlation[:,len(features)-1][:len(features)] 
```

å¹¶é‡å¤ä¹‹å‰çš„æ€»ç»“å›¾ã€‚

```py
# plt.subplot(2,5,1)
# feature_rank_plot(features,covariance,-5000.0,5000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.5)

plt.subplot(2,3,1)
feature_rank_plot(features,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation',0.5)

plt.subplot(2,3,2)
feature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)

plt.subplot(2,3,3)
feature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)

# plt.subplot(2,5,5)
# feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)

# plt.subplot(2,5,6)
# feature_rank_plot(features,stand_covariance,-1.0,1.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance of Standardized',0.5)

plt.subplot(2,3,4)
feature_rank_plot(features,stand_correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation of Standardized',0.5)

plt.subplot(2,3,5)
feature_rank_plot(features,stand_rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation of Standardized',0.5)

plt.subplot(2,3,6)
feature_rank_plot(features,stand_partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation of Standardized',0.5)

# plt.subplot(2,5,10)
# feature_rank_plot(features,stand_semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation of Standardized',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=2.2, wspace=0.3, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/8eeed3569969856a648e8f7bf97ddb8b.png)

ä½ å¯ä»¥è§‚å¯Ÿåˆ°ä»€ä¹ˆï¼š

+   åæ–¹å·®ç°åœ¨ç­‰äºç›¸å…³ç³»æ•°

+   åŠåç›¸å…³å¯¹ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆä»¿å°„ç›¸å…³æˆ–æ­£æ€å¾—åˆ†è½¬æ¢ï¼‰æ•æ„Ÿã€‚

## æ¡ä»¶ç»Ÿè®¡

æˆ‘ä»¬å°†äº•åˆ†ä¸ºä½ã€ä¸­ã€é«˜äº§é‡ï¼Œå¹¶æ£€æŸ¥æ¡ä»¶ç»Ÿè®¡çš„å·®å¼‚ã€‚

+   è¿™å°†æä¾›ä¸€ç§æ›´çµæ´»çš„æ–¹æ³•æ¥æ¯”è¾ƒæ¯ä¸ªç‰¹å¾ä¸ç”Ÿäº§ä¹‹é—´çš„å…³ç³»

+   å¦‚æœæ¡ä»¶ç»Ÿè®¡å‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œåˆ™è¯¥ç‰¹å¾æ˜¯æœ‰ä¿¡æ¯çš„

æˆ‘ä»¬å°†åœ¨æ‰€æœ‰ç‰¹å¾ä¸Šåˆ¶ä½œä¸€ä¸ªå•ç‹¬çš„å°æç´å›¾

+   æˆ‘ä»¬éœ€è¦ä¸€ä¸ªåˆ†ç±»ç‰¹å¾æ¥è¡¨ç¤ºç”Ÿäº§ï¼Œå› æ­¤ä½¿ç”¨æ­¤ä»£ç å°†ç”Ÿäº§æˆªæ–­ä¸ºé«˜æˆ–ä½ï¼Œ

```py
df['tProd'] = np.where(df['Prod']>=4000, 'High', 'Low') 
```

+   æˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰ç‰¹å¾æ ‡å‡†åŒ–ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥ä¸€èµ·è§‚å¯Ÿå®ƒä»¬çš„ç›¸å¯¹å·®å¼‚

```py
x = df[['Por','Perm','AI','Brittle','TOC','VR']]
x_stand = (x - x.mean()) / (x.std()) 
```

+   æ­¤ä»£ç å°†ç‰¹å¾æå–åˆ°æ–°çš„ DataFrame 'x'ä¸­ï¼Œç„¶åå¯¹æ¯ä¸ªåˆ—ï¼ˆç‰¹å¾ï¼‰åº”ç”¨æ ‡å‡†åŒ–æ“ä½œ

+   ç„¶åï¼Œæˆ‘ä»¬å°†æˆªæ–­çš„ç”Ÿäº§ç‰¹å¾æ·»åŠ åˆ°æ ‡å‡†åŒ–ç‰¹å¾ä¸­

```py
x = pd.concat([df['tProd'],x_stand.iloc[:,0:6]],axis=1) 
```

+   ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨ melt å‘½ä»¤æ¥é€†ç½® DataFrame

```py
x = pd.melt(x,id_vars="tProd",var_name="Predictors",value_name='Standardized_Value') 
```

+   ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªé•¿ DataFrameï¼ˆ6 ä¸ªç‰¹å¾ x 200 ä¸ªæ ·æœ¬ = 12000 è¡Œï¼‰ï¼ŒåŒ…å«ï¼š

    +   ç”Ÿäº§ï¼šä½æˆ–é«˜

    +   ç‰¹å¾ï¼šPorã€Permã€AIã€Brittleã€TOC æˆ– VR

    +   æ ‡å‡†åŒ–ç‰¹å¾å€¼

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºæˆ‘ä»¬çš„å°æç´å›¾

+   x æ˜¯æˆ‘ä»¬çš„é¢„æµ‹ç‰¹å¾

+   y æ˜¯é¢„æµ‹ç‰¹å¾çš„æ ‡å‡†åŒ–å€¼ï¼ˆç°åœ¨éƒ½åœ¨ä¸€åˆ—ä¸­ï¼‰

+   è‰²è°ƒæ˜¯ç”Ÿäº§æ°´å¹³çš„é«˜æˆ–ä½

+   split ä¸º Trueï¼Œå› æ­¤å°æç´å›¾è¢«åˆ†æˆä¸¤åŠ

+   å†…éƒ¨æ˜¯ P25ã€P50 å’Œ P75 çš„å››åˆ†ä½æ•°ï¼Œä»¥è™šçº¿ç»˜åˆ¶

```py
threshold = 2000.0

df['tProd'] = np.where(df[resp]>=threshold, 'High', 'Low')       # make a high and low production categorical feature

x_temp = df[pred]
x_temp_stand = (x_temp - x_temp.mean()) / (x_temp.std())      # standardization by feature
x_temp = pd.concat([df['tProd'],x_temp_stand.iloc[:,0:len(pred)]],axis=1) # add the production categorical feature to the DataFrame
x_temp = pd.melt(x_temp,id_vars="tProd",var_name="Predictor Feature",value_name='Standardized Predictor Feature') # unpivot the DataFrame

plt.subplot(111)
sns.violinplot(x="Predictor Feature", y="Standardized Predictor Feature", hue="tProd", data=x_temp,split=True, inner="quart", palette="Set2")
plt.xticks(rotation=90); plt.title('Conditional Distributions by Production')
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/97e5b28483156d276669ee0ba4b67c7a.png)

ä»å°æç´å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°å­”éš™ç‡ã€æ¸—é€ç‡ã€TOC åœ¨ä½å’Œé«˜äº§é‡äº•ä¹‹é—´çš„æ¡ä»¶åˆ†å¸ƒå˜åŒ–æœ€å¤§ã€‚

æˆ‘ä»¬å¯ä»¥å°†å›¾æ›¿æ¢ä¸ºæ¡ä»¶åˆ†å¸ƒçš„ç®±çº¿å›¾

+   ç®±çº¿å›¾æé«˜äº†æˆ‘ä»¬è§‚å¯Ÿæ¡ä»¶ P25ã€P75 ä»¥åŠ Tukey å¼‚å¸¸å€¼æµ‹è¯•ä¸Šä¸‹é™çš„èƒ½åŠ›ã€‚

```py
plt.subplot(111)
sns.boxplot(x="Predictor Feature", y="Standardized Predictor Feature", hue="tProd", data=x_temp)
plt.xticks(rotation=90)
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2)
plt.show()

df = df.drop(['tProd'], axis = 1) 
```

![å›¾ç‰‡](img/5cb6b0c21042f1de1888484ca5e64a81.png)

ä»æ¡ä»¶ç®±çº¿å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°å­”éš™ç‡ã€æ¸—é€ç‡ã€TOC åœ¨ä½å’Œé«˜äº§é‡äº•ä¹‹é—´çš„æ¡ä»¶åˆ†å¸ƒå˜åŒ–æœ€å¤§ã€‚

+   æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°å­”éš™ç‡ã€æ¸—é€ç‡ï¼ˆä¸Šå°¾ï¼‰ã€æ€»æœ‰æœºç¢³ï¼ˆä¸‹å°¾ï¼‰å’Œé•œè´¨ä½“åå°„ç‡çš„å¼‚å¸¸å€¼ã€‚

## æ–¹å·®è†¨èƒ€å› å­ï¼ˆVIFï¼‰

é¢„æµ‹ç‰¹å¾ï¼ˆ$X_i$ï¼‰ä¸æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾ï¼ˆ$X_j, \forall j \ne i$ï¼‰ä¹‹é—´çš„çº¿æ€§å¤šé‡å…±çº¿æ€§åº¦é‡ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬é’ˆå¯¹æ‰€æœ‰å…¶ä»–é¢„æµ‹ç‰¹å¾è®¡ç®—ä¸€ä¸ªé¢„æµ‹ç‰¹å¾çš„çº¿æ€§å›å½’ã€‚

$$ X_i = \sum_{j, j \ne i}^m X_j + \epsilon $$

ä»è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šç¡®å®šç³»æ•° $RÂ²$ï¼Œä¹Ÿç§°ä¸ºæ–¹å·®è§£é‡Šã€‚

ç„¶åæˆ‘ä»¬è®¡ç®—æ–¹å·®è†¨èƒ€å› å­å¦‚ä¸‹ï¼š

$$ VIF = \frac{1}{1 - RÂ²} $$

```py
vif_values = []
for i in range(df[pred].values.shape[1]):
    vif_values.append(variance_inflation_factor(df[pred].values, i))

vif_values = np.asarray(vif_values)
indices = np.argsort(vif_values)[::-1]                  # find indices for descending order

plt.subplot(111)                                        # plot the feature importance 
plt.title("Variance Inflation Factor")
plt.bar(range(df[pred].values.shape[1]), vif_values[indices],edgecolor = 'black',
       color="darkorange",alpha=0.6, align="center")
plt.xticks(np.linspace(0,len(pred)-1,len(pred)), np.array(pred)[indices].tolist(),rotation=90); 

plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks

plt.xlim([-0.5, x.shape[1]-0.5]); plt.yscale('log');
plt.xlabel('Predictor Feature'); plt.ylabel('Variance Inflation Factor')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)
plt.show() 
```

![å›¾ç‰‡](img/5692efa445f9167d3d0d5a6d8f3e8bcb.png)

çƒŸç…¤åå°„ç‡å…·æœ‰æœ€å¤šçš„çº¿æ€§å†—ä½™ï¼Œè€Œæ¸—é€ç‡ä¸å…¶ä»–é¢„æµ‹ç‰¹å¾å…·æœ‰æœ€å°‘çš„çº¿æ€§å†—ä½™ã€‚

+   è®°ä½ï¼Œé«˜æ–¹å·®è†¨èƒ€å› å­æ˜¯ä¸å¥½çš„ã€‚

+   è®°ä½ï¼Œæ–¹å·®è†¨èƒ€å› å­ä¸æ•´åˆæ¯ä¸ªé¢„æµ‹ç‰¹å¾ä¸å“åº”ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚

+   é€šå¸¸ï¼Œæ–¹å·®è†¨èƒ€å› å­è¢«ç”¨ä½œç­›é€‰å·¥å…·ï¼Œä»¥å»é™¤ä¸å…¶ä»–é¢„æµ‹ç‰¹å¾æœ‰è¿‡å¤šå†—ä½™çš„ç‰¹å¾ã€‚

ç°åœ¨æˆ‘ä»¬æ¥ä»‹ç»åŸºäºæ¨¡å‹çš„ç‰¹å¾æ’åºæ–¹æ³•ã€‚

## $B$ ç³»æ•° / Beta Weights

æˆ‘ä»¬è¿˜å¯ä»¥è€ƒè™‘ $B$ ç³»æ•°ã€‚è¿™äº›æ˜¯åœ¨å˜é‡æœªæ ‡å‡†åŒ–æ—¶çš„çº¿æ€§å›å½’ç³»æ•°ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ SciPy åŒ…ä¸­å¯ç”¨çš„çº¿æ€§å›å½’æ–¹æ³•ã€‚

$Y$ çš„ä¼°è®¡é‡åªæ˜¯ä¸€ä¸ªçº¿æ€§æ–¹ç¨‹ï¼š

$$ Y^* = \sum_{i=1}^{m} b_i X_i + c $$

$b_i$ ç³»æ•°æ˜¯é€šè¿‡æœ€å°åŒ–ä¼°è®¡å€¼ $Y^*$ ä¸è®­ç»ƒæ•°æ®é›†ä¸­å€¼ $Y$ ä¹‹é—´çš„å¹³æ–¹è¯¯å·®æ¥è§£å†³çš„ã€‚

```py
reg = LinearRegression()                                      # instantiate a linear regression model 
reg.fit(df[pred],df[resp])                                    # train the model
b = reg.coef_

plt.subplot(111)
feature_rank_plot(pred,b,-1000.0,1000.0,0.0,'Feature Ranking, B Coefficients with ' + resp,r'Linear Regression Slope, $b_1$',0.5) 
```

![å›¾ç‰‡](img/7da7b523b4ae881b20a379148ea78eea.png)

è¾“å‡ºæ˜¯ $b$ ç³»æ•°ï¼ŒæŒ‰æˆ‘ä»¬çš„ç‰¹å¾ä» $b_i, i = 1,\ldots,n$ æ’åºï¼Œç„¶åæ˜¯æˆªè· $c$ï¼Œæˆ‘å·²å°†å…¶ç§»é™¤ä»¥é¿å…æ··æ·†ã€‚

+   æˆ‘ä»¬çœ‹åˆ°äº†äººå·¥æ™ºèƒ½å’Œ TOC çš„è´Ÿé¢å½±å“ã€‚

+   ç»“æœå¯¹é¢„æµ‹ç‰¹å¾æ–¹å·®çš„å¹…åº¦éå¸¸æ•æ„Ÿã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡å¤„ç†æ ‡å‡†åŒ–ç‰¹å¾æ¥æ¶ˆé™¤è¿™ç§æ•æ„Ÿæ€§ã€‚

## $\beta$ ç³»æ•° / Beta Weights

$\beta$ ç³»æ•°æ˜¯åœ¨æˆ‘ä»¬å°†é¢„æµ‹å’Œå“åº”ç‰¹å¾æ ‡å‡†åŒ–ä¸ºæ–¹å·®ä¸º 1 ä¹‹åè®¡ç®—çº¿æ€§å›å½’ç³»æ•°çš„ã€‚

$$ \sigmaÂ²_{X^s_i} = 1.0 \quad \forall \quad i = 1,\ldots,m, \quad \sigmaÂ²_{Y^s} = 1.0 $$

$Y^s$ çš„ä¼°è®¡é‡åªæ˜¯ä¸€ä¸ªçº¿æ€§æ–¹ç¨‹ï¼š

$$ Y^{s*} = \sum_{i=1}^{m} \beta_i X^s_i + c $$

å¾ˆæ–¹ä¾¿çš„æ˜¯ï¼Œæˆ‘ä»¬åˆšåˆšåˆšåˆšå°†æ‰€æœ‰å˜é‡æ ‡å‡†åŒ–ï¼Œä½¿å…¶æ–¹å·®ä¸º 1.0ï¼ˆè§ä¸Šæ–‡ï¼‰ã€‚è®©æˆ‘ä»¬å†æ¬¡ä½¿ç”¨ç›¸åŒçš„çº¿æ€§å›å½’æ–¹æ³•åœ¨æ ‡å‡†åŒ–ç‰¹å¾ä¸Šå¾—åˆ° $\beta$ ç³»æ•°ã€‚

```py
reg = LinearRegression()
reg.fit(dfS[pred],dfS[resp])
beta = reg.coef_

plt.subplot(111)
feature_rank_plot(pred,beta,-1.0,1.0,0.0,r'Feature Ranking, $\beta$ Coefficients with ' + resp,r'Standardized Linear Regression Slope, $b_1$',0.5) 
```

![å›¾ç‰‡](img/63a59eb20ac3161240452ef8f6fcd97a.png)

ä¸€äº›è§‚å¯Ÿï¼š

+   $b$ å’Œ $\beta$ ç³»æ•°ä¹‹é—´çš„å˜åŒ–ä¸ä»…ä»…æ˜¯æ’åæŒ‡æ ‡ä¸Šçš„å¸¸æ•°ç¼©æ”¾ï¼Œå› ä¸ºçº¿æ€§æ¨¡å‹ç³»æ•°å¯¹ç‰¹å¾çš„å–å€¼èŒƒå›´å’Œå¹…åº¦ä¹Ÿå¾ˆæ•æ„Ÿã€‚

+   ä½¿ç”¨è´å¡”ç³»æ•°å­”éš™ç‡ã€å£°é˜»æŠ—å’Œæ€»æœ‰æœºç¢³åœ¨ä¼°è®¡äº§é‡æ–¹é¢å…·æœ‰æ›´é«˜çš„æ’å

## ç‰¹å¾é‡è¦æ€§

ä¸åŒçš„æœºå™¨å­¦ä¹ æ–¹æ³•æä¾›äº†ç‰¹å¾é‡è¦æ€§çš„åº¦é‡ï¼Œä¾‹å¦‚å†³ç­–æ ‘é€šè¿‡åŒ…å«æ¯ä¸ªç‰¹å¾æ¥å‡å°‘å‡æ–¹è¯¯å·®ï¼Œå¹¶æ€»ç»“å¦‚ä¸‹ï¼š

$$ FI(x) = \sum_{t \in T_f} \frac{N_t}{N} \Delta_{MSE_t} $$

å…¶ä¸­ $T_f$ æ˜¯æ‰€æœ‰ä»¥ç‰¹å¾ $x$ ä¸ºåˆ†å‰²ç‚¹çš„èŠ‚ç‚¹ï¼Œ$N_t$ æ˜¯è¾¾åˆ°èŠ‚ç‚¹ $t$ çš„è®­ç»ƒæ ·æœ¬æ•°é‡ï¼Œ$N$ æ˜¯æ•°æ®é›†ä¸­æ ·æœ¬çš„æ€»æ•°ï¼Œ$\Delta_{MSE_t}$ æ˜¯ $t$ åˆ†å‰²ç‚¹çš„ MSE å‡å°‘é‡ã€‚

æ³¨æ„ï¼Œç‰¹å¾é‡è¦æ€§å¯ä»¥åƒä¸Šé¢çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä¸€æ ·è®¡ç®—ï¼Œé€‚ç”¨äºå…·æœ‰ **Gini ä¸çº¯åº¦** çš„åˆ†ç±»æ ‘ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹ä»é€‚åˆæˆ‘ä»¬æ•°æ®çš„éšæœºæ£®æ—å›å½’æ¨¡å‹ä¸­å¾—åˆ°çš„ç‰¹å¾é‡è¦æ€§ã€‚

+   æˆ‘ä»¬ä½¿ç”¨é»˜è®¤çš„è¶…å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªéšæœºæ£®æ—ã€‚è¿™å¯¼è‡´æˆ‘ä»¬çš„æ£®æ—ä¸­å­˜åœ¨æ— é™å¤æ‚æ€§å’Œè¿‡æ‹Ÿåˆçš„æ ‘ã€‚è¿™äº›æ ‘çš„å¹³å‡å¤„ç†è´Ÿè´£è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ã€‚

+   ç„¶åï¼Œæˆ‘ä»¬è®­ç»ƒæˆ‘ä»¬çš„éšæœºæ£®æ—å¹¶æå–ç‰¹å¾é‡è¦æ€§ï¼Œè¿™æ˜¯é€šè¿‡è®¡ç®—æ£®æ—ä¸­æ‰€æœ‰æ ‘çš„æœŸæœ›ç‰¹å¾é‡è¦æ€§æ¥è®¡ç®—çš„ã€‚

+   æˆ‘ä»¬ä¹Ÿå¯ä»¥ä»æ£®æ—ä¸­çš„æ‰€æœ‰æ ‘ä¸­æå–ç‰¹å¾é‡è¦æ€§ï¼Œå¹¶ç”¨æ ‡å‡†å·®æ¥æ€»ç»“ï¼Œä»¥è¯„ä¼°æˆ‘ä»¬ç‰¹å¾é‡è¦æ€§æµ‹é‡çš„é²æ£’æ€§å’Œä¸ç¡®å®šæ€§ã€‚

æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹æˆ‘çš„å…³äº [éšæœºæ£®æ—](https://www.youtube.com/watch?v=m5_wk310fho&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=39) é¢„æµ‹æœºå™¨å­¦ä¹ çš„è®²åº§ã€‚

```py
# Code modified from https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization
lab_enc = preprocessing.LabelEncoder(); Y_encoded = lab_enc.fit_transform(Y) # this removes an encoding error 

random_forest = RandomForestRegressor()                 # instantiate the random forest 
random_forest = random_forest.fit(x,np.ravel(Y_encoded)) # fit the random forest
importance_rank = random_forest.feature_importances_    # extract the expected feature importances

importance_rank_stand = importance_rank/np.max(importance_rank)                          # calculate relative mutual information

std = np.std([tree.feature_importances_ for tree in random_forest.estimators_],axis=0) # calculate stdev over trees
indices = np.argsort(importance_rank)[::-1]             # find indices for descending order

plt.subplot(111)                                        # plot the feature importance 
plt.title("Random Forest-based Feature importances")
plt.bar(range(x.shape[1]), importance_rank[indices],edgecolor = 'black',
       color="darkorange",alpha = 0.6, yerr=std[indices], align="center")
plt.xticks(range(x.shape[1]), x.columns[indices],rotation=90)
plt.xlim([-0.5, x.shape[1]-0.5]); 
plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks
plt.ylim([0.,1.0])
plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)
plt.show() 
```

![_images/633da2b9b4e395da57c78aa4f82399344b016071733ef9fb8569d30c70d92604.png](img/4221de8488fdf2ce8f7716c22e1ea9d8.png)

æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨åŸºäºæ¨¡å‹çš„æ–¹æ³•åšæ›´å¤šçš„äº‹æƒ…ã€‚æˆ‘ä»¬å°†å®é™…æµ‹è¯•æ¨¡å‹ä»¥è¯„ä¼°æ¯ä¸ªé¢„æµ‹ç‰¹å¾çš„å¢åŠ å½±å“ï¼æˆ‘ä»¬å°†å°è¯•ä½¿ç”¨é€’å½’ç‰¹å¾æ¶ˆé™¤æ³•æ¥åšè¿™ä¸ªå®éªŒã€‚

è®©æˆ‘ä»¬ç»˜åˆ¶ $B$ å’Œ $\beta$ ç³»æ•°çš„ç»“æœï¼Œå¹¶ä¸ä¹‹å‰çš„ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚

```py
plt.subplot(231)
feature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)

plt.subplot(232)
feature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)

plt.subplot(234)
feature_rank_plot(pred,b[0:len(pred)],-1000.0,1000.0,0.0,'Feature Ranking, B Coefficients with ' + resp,'B Coefficients',0.5)

plt.subplot(235)
feature_rank_plot(pred,beta[0:len(pred)],-1.0,1.0,0.0,'Feature Ranking, Beta Coefficients with ' + resp,'Beta Coefficients',0.5)

plt.subplot(236)
feature_rank_plot(pred,importance_rank_stand,0.0,1.0,0.0,'Feature Ranking, Feature Importance with ' + resp,'Standardized Feature Importance',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=2.2, wspace=0.2, hspace=0.3); plt.show() 
```

![_images/28592d6d0a57887d32a07cc893157689a5862f636dd4b208cc9b42b3dc9dd964.png](img/9b736db6a7b146115943a1775ce2cc0d.png)

## äº’ä¿¡æ¯

äº’ä¿¡æ¯æ˜¯ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œå®ƒé‡åŒ–äº†ä¸¤ä¸ªç‰¹å¾ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ã€‚

+   é‡åŒ–ä»è§‚å¯Ÿä¸€ä¸ªç‰¹å¾å…³äºå¦ä¸€ä¸ªç‰¹å¾è·å¾—çš„ä¿¡æ¯é‡

+   é¿å…å¯¹å…³ç³»çš„å½¢çŠ¶åšå‡ºä»»ä½•å‡è®¾ï¼ˆä¾‹å¦‚ï¼Œæ²¡æœ‰çº¿æ€§å…³ç³»çš„å‡è®¾ï¼‰

+   å°†è”åˆæ¦‚ç‡ä¸è¾¹ç¼˜æ¦‚ç‡çš„ä¹˜ç§¯è¿›è¡Œæ¯”è¾ƒ

å¯¹äºç¦»æ•£æˆ–åˆ†ç®±çš„è¿ç»­ç‰¹å¾ $X$ å’Œ $Y$ï¼Œäº’ä¿¡æ¯è®¡ç®—å¦‚ä¸‹ï¼š

$$ I(X;Y) = \sum_{y \in Y} \sum_{x \in X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) $$

å›æƒ³ä¸€ä¸‹ï¼Œç»™å®š $X$ å’Œ $Y$ ä¹‹é—´çš„ç‹¬ç«‹æ€§ï¼š

$$ P_{X,Y}(x,y) = P_X(x) \cdot P_Y(y) $$

å› æ­¤ï¼Œå¦‚æœä¸¤ä¸ªç‰¹å¾æ˜¯ç‹¬ç«‹çš„ï¼Œé‚£ä¹ˆ $\log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) = 0$

è”åˆæ¦‚ç‡ $P_{X,Y}(x,y)$ æ˜¯åŠ æƒé¡¹ï¼Œå¯¹æ€»å’Œè¿›è¡Œçº¦æŸï¼Œå¹¶å¼ºåˆ¶å°é—­ã€‚

+   è”åˆåˆ†å¸ƒä¸­å¯†åº¦è¾ƒå¤§çš„éƒ¨åˆ†å¯¹äº’ä¿¡æ¯åº¦é‡æœ‰æ›´å¤§çš„å½±å“

å¯¹äºè¿ç»­ï¼ˆå’Œéåˆ†ç®±ï¼‰ç‰¹å¾ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨ç§¯åˆ†å½¢å¼ã€‚

$$ I(X;Y) = \int_{Y} \int_{X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) dx dy $$

æˆ‘ä»¬é€šè¿‡å‘½ä»¤è·å¾—æŒ‰é‡è¦æ€§é€’å‡é¡ºåºæ’åºçš„ç´¢å¼•åˆ—è¡¨ã€‚

```py
indices = np.argsort(importances)[::-1] 
```

åˆ‡ç‰‡åè½¬äº†é¡ºåºï¼Œä»¥æŒ‰ç‰¹å¾é‡è¦æ€§é™åºæ’åˆ—ã€‚

```py
x_df = df.loc[:,pred]                            # separate DataFrames for predictor and response features
y_df = df.loc[:,resp]

mi = mutual_info_regression(x_df,np.ravel(y_df))              # calculate mutual information
mi /= np.max(mi)                                        # calculate relative mutual information

indices = np.argsort(mi)[::-1]                          # find indices for descending order

print("Feature ranking:")                               # write out the feature importances
for f in range(x.shape[1]):
    print("%d. feature %s = %f" % (f + 1, x.columns[indices][f], mi[indices[f]]))

plt.subplot(111)                                        # plot the relative mutual information 
plt.title("Mutual Information")
plt.bar(range(x.shape[1]), mi[indices],edgecolor = 'black',
       color="darkorange",alpha=0.6,align="center")
plt.xticks(range(x.shape[1]), x.columns[indices],rotation=90)
plt.xlim([-0.5, x.shape[1]-0.5]); plt.ylim([0,1.3])
plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks
plt.xlabel('Predictor Feature'); plt.ylabel('Mutual Information')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)
plt.show() 
```

```py
Feature ranking:
1\. feature Por = 1.000000
2\. feature Perm = 0.345842
3\. feature TOC = 0.272418
4\. feature Brittle = 0.073310
5\. feature AI = 0.059024
6\. feature VR = 0.000000 
```

![å›¾ç‰‡](img/d12dd6dd61aa76402a54c096acd0768b.png)

### è€ƒè™‘ç›¸å…³æ€§å’Œå†—ä½™çš„äº’ä¿¡æ¯

æ ‡å‡†çš„æœ€å¤§ç›¸å…³æ€§-æœ€å°å†—ä½™ï¼ˆMRMRï¼‰ç›®æ ‡å‡½æ•°è€ƒè™‘é¢„æµ‹ç‰¹å¾çš„å­é›†ï¼Œå³ï¼Œå°†é¢„æµ‹ç‰¹å¾å­é›†ä½œä¸ºåº¦é‡æ ‡å‡†æ¥è¯†åˆ«æœ€å…·æœ‰ä¿¡æ¯é‡çš„é¢„æµ‹ç‰¹å¾å­é›†ã€‚

+   è¯¥æ–¹æ³•è®¡ç®—é¢„æµ‹ç‰¹å¾å­é›†ä¸å“åº”ç‰¹å¾ä¹‹é—´çš„å¹³å‡äº’ä¿¡æ¯å‡å»é¢„æµ‹ç‰¹å¾å­é›†ä¹‹é—´çš„å¹³å‡äº’ä¿¡æ¯ã€‚

$$ MID = \frac{1}{|S|}{\sum_{\alpha \in S} I(X_{\alpha},Y) } - \frac{1}{|S|Â²} {\sum_{\alpha \in S}^m \sum_{\beta \in S}^m I(X_{\alpha},X_{\beta})} $$

ä½œä¸º $ç›¸å…³æ€§ - ä½™åº¦$ çš„åº¦é‡æˆ–

$$ MIQ = \frac{ \frac{1}{|S|}{\sum_{\alpha \in S}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|Â²} {\sum_{\alpha \in S}^m \sum_{\beta \in S}^m I(X_{\alpha},X_{\beta})} } $$

+   ä½œä¸º $\frac{ç›¸å…³æ€§}{å†—ä½™}$ çš„åº¦é‡ã€‚

### è€ƒè™‘ç›¸å…³æ€§å’Œå†—ä½™çš„äº’ä¿¡æ¯

æ ‡å‡†çš„æœ€å¤§ç›¸å…³æ€§-æœ€å°å†—ä½™ï¼ˆMRMRï¼‰ç›®æ ‡å‡½æ•°è€ƒè™‘é¢„æµ‹ç‰¹å¾çš„å­é›†ï¼Œå³ï¼Œå°†é¢„æµ‹ç‰¹å¾å­é›†ä½œä¸ºåº¦é‡æ ‡å‡†æ¥è¯†åˆ«æœ€å…·æœ‰ä¿¡æ¯é‡çš„é¢„æµ‹ç‰¹å¾å­é›†ã€‚

+   è¯¥æ–¹æ³•è®¡ç®—é¢„æµ‹ç‰¹å¾å­é›†ä¸å“åº”ç‰¹å¾ä¹‹é—´çš„å¹³å‡äº’ä¿¡æ¯å‡å»é¢„æµ‹ç‰¹å¾å­é›†ä¹‹é—´çš„å¹³å‡äº’ä¿¡æ¯ã€‚

$$ MID = \frac{1}{|S|}{\sum_{\alpha \in S} I(X_{\alpha},Y) } - \frac{1}{|S|Â²} {\sum_{\alpha \in S}^m \sum_{\beta \in S}^m I(X_{\alpha},X_{\beta})} $$

ä½œä¸º $ç›¸å…³æ€§ - ä½™åº¦$ çš„åº¦é‡æˆ–

$$ MIQ = \frac{ \frac{1}{|S|}{\sum_{\alpha \in S}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|Â²} {\sum_{\alpha \in S}^m \sum_{\beta \in S}^m I(X_{\alpha},X_{\beta})} } $$

+   ä½œä¸º $\frac{ç›¸å…³æ€§}{å†—ä½™}$ çš„åº¦é‡ã€‚

## è€ƒè™‘ç›¸å…³æ€§å’Œå†—ä½™çš„äº’ä¿¡æ¯ OFAT å˜ä½“

æˆ‘å»ºè®®å¯¹äºä¸€æ¬¡ä¸€ä¸ªç‰¹å¾çš„é¢„æµ‹ç‰¹å¾æ’åºï¼ˆé¢„æµ‹ç‰¹å¾å­é›†ï¼Œ$S = [X_i]$ ä¸” $|S| = 1$ï¼‰ï¼Œæˆ‘ä»¬å°†æ­¤ä¿®æ”¹ä¸ºä»¥ä¸‹è®¡ç®—ï¼š

+   **ç›¸å…³æ€§** - é€‰å®šçš„é¢„æµ‹ç‰¹å¾ $X_i$ ä¸å“åº”ç‰¹å¾ $Y$ ä¹‹é—´çš„äº’ä¿¡æ¯

+   **å†—ä½™** - æ‰€é€‰é¢„æµ‹ç‰¹å¾ $X_i$ ä¸å‰©ä½™é¢„æµ‹ç‰¹å¾ $X_{\alpha}, \alpha \ne i$ ä¹‹é—´çš„å¹³å‡äº’ä¿¡æ¯ã€‚

+   æˆ‘ä»¬ä½¿ç”¨ Gulgezenã€Cataltepe å’Œ Yu (2009) çš„è®¡ç®—å…¬å¼çš„å•†å½¢å¼ã€‚

æˆ‘ä»¬ä¿®æ”¹äº†æœ€å¤§ç›¸å…³æ€§-æœ€å°å†—ä½™ï¼ˆMRMRï¼‰ç›®æ ‡å‡½æ•°ï¼Œç”¨äº OFAT æ’åï¼Œå®ƒå°†æ‰€é€‰é¢„æµ‹ç‰¹å¾ $X_i$ çš„**ç›¸å…³æ€§**è§†ä¸ºå…¶ä¸å“åº”ç‰¹å¾çš„äº’ä¿¡æ¯ï¼š

\begin{equation} I(X_i,Y) \end{equation}

ä»¥åŠæ‰€é€‰é¢„æµ‹ç‰¹å¾ $X_i$ ä¸å‰©ä½™é¢„æµ‹ç‰¹å¾ä¹‹é—´çš„**å†—ä½™**ï¼š

\begin{equation} \frac{1}{|S|-1} \sum_{\alpha=1, \alpha \ne i}^m I(X_i,X_{\alpha}) \end{equation}

å…¶ä¸­ $X$ æ˜¯é¢„æµ‹ç‰¹å¾ï¼Œ$Y$ æ˜¯å“åº”ç‰¹å¾ï¼Œ$X_i$ æ˜¯è¢«è¯„åˆ†çš„å…·ä½“é¢„æµ‹ç‰¹å¾ï¼Œ$|S|$ æ˜¯é¢„æµ‹ç‰¹å¾çš„æ•°é‡ï¼Œ$I()$ æ˜¯æŒ‡ç¤ºç‰¹å¾ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚ä¸€ç§å…¬å¼æ˜¯ç®€å•çš„å·®å€¼ï¼Œç›¸å…³æ€§å‡å»å†—ä½™ï¼Œ

$$ \Phi_{\Delta}(X_i,Y) = I(X_{\alpha},Y) - \frac{1}{|S|-1} \sum_{\beta=1, \alpha \ne \beta}^m I( X_{\alpha},X_{\beta} ) $$

å¦ä¸€ä¸ªé€‰æ‹©æ˜¯æ¯”ç‡ï¼Œ

$$ \Phi_{r}(X_i,Y) = \frac{ I(X_i,Y) }{ \frac{1}{|S|-1} \sum_{\alpha=1, \alpha \ne i}^m I(X_i,X_{\alpha})} $$

åœ¨è¿™é‡Œï¼Œç‰¹å¾æ’åæ˜¯é€šè¿‡äº’ä¿¡æ¯ç›¸å…³å‡å»å†—ä½™çš„ $\Phi_{\Delta}(X_i,Y)$ æ–¹æ³•æ¥å®ç°çš„ã€‚

```py
obj_mutual = mutual_information_objective(x_df,y_df)
indices_obj = np.argsort(obj_mutual)[::-1]              # find indices for descending order

plt.subplot(111)                                        # plot the relative mutual information 
plt.title("One-at-a-Time MRMR Objective Function for Mutual Information-based Feature Selection")
plt.bar(range(x.shape[1]), obj_mutual[indices_obj],
       color="darkorange",alpha = 0.6, align="center",edgecolor="black")
plt.xticks(range(x.shape[1]), x.columns[indices_obj],rotation=90)
plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks
plt.xlim([-0.5, x.shape[1]-0.5]); plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)
plt.show() 
```

![_images/c0b176e4990bdb326b638222358d06af771613f6ae47153d42a7e2614c7d3c28.png](img/ce31fdb26712840a6f374091a92555c3.png)

### è€ƒè™‘ç›¸å…³æ€§å’Œå†—ä½™çš„ Delta äº’ä¿¡æ¯å•†

æˆ‘ä»¬é‡‡ç”¨ Gulgezenã€Cataltepe å’Œ Yu (2009) çš„äº’ä¿¡æ¯å•†æ¥å¼€å‘ä¸€ä¸ª OFAT æ’åæŒ‡æ ‡ã€‚

æ ‡å‡†çš„ MRMR ç›®æ ‡å‡½æ•°è¯„ä¼°é¢„æµ‹ç‰¹å¾å­é›†ä¸å“åº”ç‰¹å¾ä¹‹é—´çš„**ç›¸å…³æ€§**ï¼š

\begin{equation} \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } \end{equation}

ä»¥åŠé¢„æµ‹ç‰¹å¾å­é›†ä¹‹é—´çš„**å†—ä½™**ï¼š

\begin{equation} \frac{1}{|S|Â²} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} \end{equation}

ä¸ºäº†æ‰¾åˆ°æœ€æœ‰ä¿¡æ¯é‡çš„é¢„æµ‹ç‰¹å¾å­é›†ï¼Œæˆ‘ä»¬å¿…é¡»æ‰¾åˆ°æœ€å¤§åŒ–ç›¸å…³æ€§åŒæ—¶æœ€å°åŒ–å†—ä½™çš„ç‰¹å¾å­é›†ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æœ€å¤§åŒ–ä»¥ä¸‹ä¸¤ç§å…¬å¼ä¸­çš„ä»»ä½•ä¸€ä¸ªæ¥å®ç°è¿™ä¸€ç‚¹ï¼Œ

\begin{equation} MID = \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } - \frac{1}{|S|Â²} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} \end{equation}

æˆ–è€…

\begin{equation} MIQ = \frac{ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|Â²} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} } \end{equation}

æˆ‘å»ºè®®é€šè¿‡åŒ…å«å’Œåˆ é™¤ç‰¹å®šé¢„æµ‹ç‰¹å¾ï¼ˆ$X_i$ï¼‰æ¥è®¡ç®— $MIQ$ çš„å˜åŒ–æ¥è¿›è¡Œç‰¹å¾æ’åã€‚

$$ \Delta MIQ_i = \frac{ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|Â²} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} } - \frac{ \frac{1}{|S|}{\sum_{\alpha=1,\alpha \ne i}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|Â²} {\sum_{\alpha=1,\alpha \ne i}^m \sum_{\beta=1,\beta \ne i}^m I(X_{\alpha},X_{\beta})} } $$

```py
delta_mutual_information = delta_mutual_information_quotient(x_df,y_df)

indices_delta_mutual_information = np.argsort(delta_mutual_information)[::-1] # find indices for descending order

plt.subplot(111)                                              # plot the relative mutual information 
plt.title("Delta Mutual Information Quotient")
plt.bar(range(x.shape[1]), delta_mutual_information[indices_delta_mutual_information],
       color="darkorange",alpha = 0.6,align="center",edgecolor = 'black')
plt.xticks(range(x.shape[1]), x.columns[indices_delta_mutual_information],rotation=90)
plt.xlim([-0.5, x.shape[1]-0.5])
plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks
plt.plot([-0.5,x.shape[1]-0.5],[0,0],color='black',lw=3); plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)
plt.show() 
```

![å›¾ç‰‡](img/21f9cee66dbe4d7f6bc8cac154c1ad85.png)

æ¯”è¾ƒ Delta äº’ä¿¡æ¯å’Œæ–¹å·®è†¨èƒ€å› å­æ’åºæ˜¯æœ‰å¯å‘æ€§çš„ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½è€ƒè™‘äº†é¢„æµ‹ç‰¹å¾å†—ä½™ã€‚

+   ä½† VIF å‡è®¾çº¿æ€§ä¸”ä¸è€ƒè™‘ç›¸å…³æ€§

```py
plt.scatter(stats.rankdata(delta_mutual_information),stats.rankdata(-vif_values),c='black',edgecolor='black')
for i, feature in enumerate(x.columns):
    plt.annotate(feature, (stats.rankdata(delta_mutual_information)[i]-0.2,stats.rankdata(-vif_values)[i]+0.1))
plt.xlabel('Delta Mutual Information Rank'); plt.ylabel('Variance Inflation Factor Rank'); plt.title('Variance Inflation Factor vs. Delta Mutual Information Ranking')
plt.xlim(0,len(pred)+0.1); plt.ylim(0,len(pred)+0.1)
plt.plot([2,len(pred)],[0,len(pred)-2],color='black',alpha=0.5,ls='--'); 
plt.plot([0,len(pred)-2],[2,len(pred)],color='black',alpha=0.5,ls='--')
plt.fill_between([0,len(pred)-2], [2,len(pred)], [len(pred),len(pred)], color='coral',alpha=0.2,zorder=1)
plt.fill_between([2,len(pred)], [0,len(pred)-2], [0,0], color='dodgerblue',alpha=0.2,zorder=1)
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/3028e58a56f928ebb26de84cfb6e1f54.png)

ä»äº’ä¿¡æ¯ä¸­æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå­”éš™ç‡ã€æ¸—é€ç‡ç„¶åæ€»æœ‰æœºç¢³å’Œè„†æ€§æœ€åç¦»ä¸€èˆ¬ç‹¬ç«‹æ€§ã€‚

### è€ƒè™‘ç›¸å…³æ€§å’Œå†—ä½™çš„ Delta äº’ä¿¡æ¯å•†

æˆ‘ä»¬é‡‡ç”¨ Gulgezenã€Cataltepe å’Œ Yuï¼ˆ2009ï¼‰çš„äº’ä¿¡æ¯å•†æ¥å¼€å‘ä¸€ä¸ª OFAT æ’åºæŒ‡æ ‡ã€‚

æ ‡å‡†çš„ MRMR ç›®æ ‡å‡½æ•°å¯¹é¢„æµ‹ç‰¹å¾å­é›†å’Œå“åº”ç‰¹å¾ä¹‹é—´çš„**ç›¸å…³æ€§**å­é›†è¿›è¡Œè¯„åˆ†ï¼š

$$ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } $$

ä»¥åŠé¢„æµ‹ç‰¹å¾å­é›†ä¹‹é—´çš„**å†—ä½™**ï¼š

$$ \frac{1}{|S|Â²} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} $$

ä¸ºäº†æ‰¾åˆ°æœ€æœ‰ä¿¡æ¯çš„é¢„æµ‹ç‰¹å¾å­é›†ï¼Œæˆ‘ä»¬å¿…é¡»æ‰¾åˆ°æœ€å¤§åŒ–ç›¸å…³æ€§åŒæ—¶æœ€å°åŒ–å†—ä½™çš„ç‰¹å¾å­é›†ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æœ€å¤§åŒ–ä»¥ä¸‹ä¸¤ç§å…¬å¼ä¸­çš„ä»»ä½•ä¸€ä¸ªæ¥å®ç°è¿™ä¸€ç‚¹ï¼Œ

$$ MID = \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } - \frac{1}{|S|Â²} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} $$

æˆ–è€…

$$ MIQ = \frac{ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|Â²} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} } $$

æˆ‘å»ºè®®é€šè¿‡è®¡ç®—åŒ…å«å’Œåˆ é™¤ç‰¹å®šé¢„æµ‹ç‰¹å¾ï¼ˆ$X_i$ï¼‰æ—¶ $MIQ$ çš„å˜åŒ–æ¥è¿›è¡Œç‰¹å¾æ’åºã€‚

$$ \Delta MIQ_i = \frac{ \frac{1}{|S|}{\sum_{\alpha=1}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|Â²} {\sum_{\alpha=1}^m \sum_{\beta=1}^m I(X_{\alpha},X_{\beta})} } - \frac{ \frac{1}{|S|}{\sum_{\alpha=1,\alpha \ne i}^m I(X_{\alpha},Y) } }{ \frac{1}{|S|Â²} {\sum_{\alpha=1,\alpha \ne i}^m \sum_{\beta=1,\beta \ne i}^m I(X_{\alpha},X_{\beta})} } $$

```py
delta_mutual_information = delta_mutual_information_quotient(x_df,y_df)

indices_delta_mutual_information = np.argsort(delta_mutual_information)[::-1] # find indices for descending order

plt.subplot(111)                                              # plot the relative mutual information 
plt.title("Delta Mutual Information Quotient")
plt.bar(range(x.shape[1]), delta_mutual_information[indices_delta_mutual_information],
       color="darkorange",alpha = 0.6,align="center",edgecolor = 'black')
plt.xticks(range(x.shape[1]), x.columns[indices_delta_mutual_information],rotation=90)
plt.xlim([-0.5, x.shape[1]-0.5])
plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks
plt.plot([-0.5,x.shape[1]-0.5],[0,0],color='black',lw=3); plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)
plt.show() 
```

![å›¾ç‰‡](img/21f9cee66dbe4d7f6bc8cac154c1ad85.png)

æ¯”è¾ƒ Delta äº’ä¿¡æ¯å’Œæ–¹å·®è†¨èƒ€å› å­æ’åºæ˜¯æœ‰å¯å‘æ€§çš„ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½è€ƒè™‘äº†é¢„æµ‹ç‰¹å¾å†—ä½™ã€‚

+   ä½† VIF å‡è®¾çº¿æ€§å…³ç³»ï¼Œå¹¶ä¸”æ²¡æœ‰è€ƒè™‘ç›¸å…³æ€§ã€‚

```py
plt.scatter(stats.rankdata(delta_mutual_information),stats.rankdata(-vif_values),c='black',edgecolor='black')
for i, feature in enumerate(x.columns):
    plt.annotate(feature, (stats.rankdata(delta_mutual_information)[i]-0.2,stats.rankdata(-vif_values)[i]+0.1))
plt.xlabel('Delta Mutual Information Rank'); plt.ylabel('Variance Inflation Factor Rank'); plt.title('Variance Inflation Factor vs. Delta Mutual Information Ranking')
plt.xlim(0,len(pred)+0.1); plt.ylim(0,len(pred)+0.1)
plt.plot([2,len(pred)],[0,len(pred)-2],color='black',alpha=0.5,ls='--'); 
plt.plot([0,len(pred)-2],[2,len(pred)],color='black',alpha=0.5,ls='--')
plt.fill_between([0,len(pred)-2], [2,len(pred)], [len(pred),len(pred)], color='coral',alpha=0.2,zorder=1)
plt.fill_between([2,len(pred)], [0,len(pred)-2], [0,0], color='dodgerblue',alpha=0.2,zorder=1)
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/3028e58a56f928ebb26de84cfb6e1f54.png)

ä»äº’ä¿¡æ¯ä¸­æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå­”éš™ç‡ã€æ¸—é€ç‡ç„¶åæ€»æœ‰æœºç¢³å’Œè„†æ€§åœ¨ä¸€èˆ¬ç‹¬ç«‹æ€§æ–¹é¢æœ‰æœ€å¤§çš„åç¦»ã€‚

## æ‰€æœ‰åŒå˜é‡æŒ‡æ ‡çš„æ€»ç»“

æˆ‘ä»¬æœ‰ä¸€ç³»åˆ—å¹¿æ³›çš„å‡†åˆ™æ¥æ’åºæˆ‘ä»¬çš„ç‰¹å¾ã€‚

+   $B$ ç³»æ•°ä¸åæ–¹å·®æœ‰åŒæ ·çš„é—®é¢˜ï¼Œå¯¹å•å˜é‡æ–¹å·®æ•æ„Ÿã€‚

+   $\beta$ ç³»æ•°æ¶ˆé™¤äº†è¿™ç§æ•æ„Ÿæ€§ï¼Œå¹¶ä¸å…ˆå‰ç»“æœä¸€è‡´ã€‚

è€ƒè™‘åˆ°æ‰€æœ‰è¿™äº›æ–¹æ³•ï¼Œæˆ‘ä¼šæŒ‰ä»¥ä¸‹é¡ºåºæ’åˆ—å˜é‡ï¼š

1.  å­”éš™ç‡

1.  ç»ç’ƒè´¨åå°„ç‡

1.  å£°é˜»æŠ—

1.  æ¸—é€ç‡

1.  æ€»æœ‰æœºç¢³

1.  è„†æ€§

æˆ‘é€šè¿‡è§‚å¯Ÿè¿™äº›æŒ‡æ ‡çš„ä¸€èˆ¬è¶‹åŠ¿æ¥åˆ†é…è¿™äº›ç­‰çº§ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åŠ æƒæ¯ç§æ–¹æ³•æ¥åˆ¶ä½œä¸€ä¸ªæ›´é‡åŒ–çš„åˆ†æ•°å¹¶æ’åºã€‚

å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬ä¸åº”å¿½è§†ä¸“å®¶çŸ¥è¯†ã€‚å¦‚æœå…³äºç‰©ç†è¿‡ç¨‹ã€å› æœå…³ç³»ä»¥åŠå˜é‡çš„å¯é æ€§å’Œå¯ç”¨æ€§æœ‰æ›´å¤šä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯åº”æ•´åˆåˆ°åˆ†é…ç­‰çº§ä¸­ã€‚

æˆ‘ä»¬åœ¨è¿™é‡ŒåŒ…æ‹¬ä¸€ä¸ªé¢å¤–çš„æ–¹æ³•ï¼Œé€’å½’ç‰¹å¾æ¶ˆé™¤ï¼Œä½†åªæä¾›äº†ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹ç¤ºä¾‹ã€‚ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹å¯ä»¥åšæ›´å¤šçš„äº‹æƒ…ã€‚

## é€’å½’ç‰¹å¾æ¶ˆé™¤

é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰æ–¹æ³•é€šè¿‡é€’å½’åˆ é™¤ç‰¹å¾å¹¶ä½¿ç”¨å‰©ä½™ç‰¹å¾æ„å»ºæ¨¡å‹æ¥å·¥ä½œã€‚

+   åœ¨ç¬¬ä¸€æ­¥ä¸­ï¼Œä½¿ç”¨æ‰€æœ‰ç‰¹å¾æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶æ ¹æ®ç‰¹å¾é‡è¦æ€§æˆ– $\beta$ ç³»æ•°å¯¹ç‰¹å¾è¿›è¡Œæ’åºã€‚

+   æœ€ä¸é‡è¦çš„ç‰¹å¾è¢«å‰ªæï¼Œæ¨¡å‹è¢«é‡å»º

+   é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°åªå‰©ä¸‹ä¸€ä¸ªç‰¹å¾ã€‚

åœ¨æ­¤ä»£ç ä¸­ï¼Œæˆ‘ä»¬åŸºäºå¤šå…ƒå›å½’å»ºç«‹ä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œå¹¶æŒ‡å‡ºæˆ‘ä»¬æƒ³è¦æ ¹æ®é€’å½’ç‰¹å¾æ¶ˆé™¤æ‰¾åˆ°æœ€ä½³ç‰¹å¾ã€‚ç®—æ³•ä¸ºæ‰€æœ‰ç‰¹å¾åˆ†é… $1,\ldots,m$ çš„ç­‰çº§ã€‚

```py
rfe_linear = RFE(LinearRegression(),n_features_to_select=1,verbose=0) # set up RFE linear regression model
df['const'] = np.ones(len(df))                                # let's add one's for the constant term
rfe_linear = rfe_linear.fit(df[pred].values,np.ravel(df[resp])) # recursive elimination
dfS = df.drop('const',axis = 1)                               # remove the ones
print('Recursive Feature Elimination: Multilinear Regression')
for i in range(0,len(pred)):
    print('Rank #' + str(i+1) + ' ' + pred[rfe_linear.ranking_[i]-1]) 
```

```py
Recursive Feature Elimination: Multilinear Regression
Rank #1 Brittle
Rank #2 TOC
Rank #3 AI
Rank #4 VR
Rank #5 Por
Rank #6 Perm 
```

é€’å½’æ¶ˆé™¤æ–¹æ³•çš„ä¼˜ç‚¹ï¼š

+   å®é™…æ¨¡å‹å¯ç”¨äºè¯„ä¼°ç‰¹å¾ç­‰çº§

+   æ’åºåŸºäºä¼°è®¡çš„å‡†ç¡®æ€§

ä½†è¿™ç§æ–¹æ³•å¯¹ä»¥ä¸‹å› ç´ æ•æ„Ÿï¼š

+   æ¨¡å‹çš„é€‰æ‹©

+   è®­ç»ƒæ•°æ®é›†

ç‰¹å¾ç­‰çº§ä¸æˆ‘ä»¬å…ˆå‰çš„æ–¹æ³•ç›¸å½“ä¸åŒã€‚è®¸å¤šç‰¹å¾ä»å…ˆå‰çš„è¯„ä¼°ä¸­ç§»åŠ¨ã€‚ä¹Ÿè®¸æˆ‘ä»¬åº”è¯¥ä½¿ç”¨æ›´çµæ´»çš„å»ºæ¨¡æ–¹æ³•ã€‚

è®©æˆ‘ä»¬ç”¨æ›´çµæ´»çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå†³ç­–æ ‘å›å½’æ¨¡å‹ï¼Œé‡å¤è¿™ç§æ–¹æ³•ã€‚

```py
from sklearn.ensemble import RandomForestRegressor
import warnings
warnings.filterwarnings('ignore')            
import geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper
rfe_rf = RFE(RandomForestRegressor(max_depth=3),n_features_to_select=1,verbose=0) # set up RFE linear regression model
df['const'] = np.ones(len(df))                                # let's add one's for the constant term

lab_enc = preprocessing.LabelEncoder(); Y_encoded = lab_enc.fit_transform(Y)

rfe_rf = rfe_rf.fit(x,np.ravel(Y_encoded))                    # recursive elimination
dfS = df.drop('const',axis = 1)                               # remove the ones
print('Recursive Feature Elimination: Random Forest Regression')
for i in range(0,len(pred)):
    print('Rank #' + str(i+1) + ' ' + pred[rfe_rf.ranking_[i]-1]) 
```

```py
Recursive Feature Elimination: Random Forest Regression
Rank #1 Por
Rank #2 VR
Rank #3 Brittle
Rank #4 Perm
Rank #5 TOC
Rank #6 AI 
```

å†æ¬¡å¼ºè°ƒï¼Œç‰¹å¾æ’åºçš„é€’å½’ç‰¹å¾æ¶ˆé™¤å¯¹æ¨¡å‹çš„å‡†ç¡®æ€§æ•æ„Ÿã€‚

+   å®é™…é¢„æµ‹æ¨¡å‹å¿…é¡»è°ƒæ•´å…¶å…³è”çš„è¶…å‚æ•°å¹¶æ£€æŸ¥æ¨¡å‹å‡†ç¡®æ€§ã€‚

+   ä¾‹å¦‚ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºçº¿æ€§æ¨¡å‹çš„å‡†ç¡®æ€§å·®ï¼Œå¤šå…ƒçº¿æ€§å›å½’ç‰¹å¾æ’åºä¸å¯é ã€‚

## ç‰¹å¾æ’åºçš„ Shapley å€¼

è®©æˆ‘ä»¬éšæœºé€‰å–æ•°æ®çš„ä¸€ä¸ªå­é›†ï¼Œä½œä¸ºèƒŒæ™¯å€¼æ¥è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚

+   æˆ‘ä»¬å¯¹å­é›†è¿›è¡Œåˆ’åˆ†ä»¥åŠ å¿«è®¡ç®—ã€‚

+   æˆ‘ä»¬åº”è¯¥è¯„ä¼°/å¼ºåˆ¶æ‰§è¡Œé¢„æµ‹ç‰¹å¾ç©ºé—´çš„æ•ˆç‡è¦†ç›–

ç”±äº Shapley å€¼æ˜¯åŸºäºæ¨¡å‹çš„ï¼Œæˆ‘ä»¬å¿…é¡»å…ˆæ„å»ºä¸€ä¸ªæ¨¡å‹

### æ„å»ºéšæœºæ£®æ—æ¨¡å‹

ç”±äº Shapley æ˜¯åŸºäºæ¨¡å‹çš„ï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªæ¨¡å‹

+   è®©æˆ‘ä»¬ä»ä¸€ä¸ªå¥½çš„éšæœºæ£®æ—æ¨¡å‹å¼€å§‹ï¼Œè§‚å¯Ÿ Shapley å€¼ï¼Œç„¶åè¿”å›è¿™é‡Œå¹¶ä¿®æ”¹æ¨¡å‹

```py
seed = 73093                                                  # set the random forest hyperparameters

# #Underfit random forest
max_leaf_nodes = 2
num_tree = 10
max_features = 2

#Overfit random forest
max_leaf_nodes = 50
num_tree = 1
max_features = 6

# #Good random forest
max_leaf_nodes = 5
num_tree = 300
max_features = 2

rfr = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=seed,n_estimators=num_tree, max_features=max_features)
rfr.fit(X = x, y = Y)

Y_hat = predict_train = rfr.predict(x)

MSE = metrics.mean_squared_error(Y,Y_hat)
Var_Explained = metrics.explained_variance_score(Y,Y_hat)
print('Mean Squared Error on Training = ', round(MSE,2),', Variance Explained =', round(Var_Explained,2))

importances = rfr.feature_importances_               # expected (global) importance over the forest fore each predictor feature
std = np.std([rfr.feature_importances_ for tree in rfr.estimators_],axis=0)
indices = np.argsort(importances)[::-1].tolist()

plt.subplot(121)
plt.scatter(Y,Y_hat,s=None, c='darkorange',marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors="black")
plt.title('Random Forest Model'); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')
plt.xlim(0,7000); plt.ylim(0,7000)
plt.arrow(0,0,7000,7000,width=0.02,color='black',head_length=0.0,head_width=0.0)

plt.subplot(122)
plt.title("Feature Importances")
plt.bar([pred[i] for i in indices],rfr.feature_importances_[indices],color="darkorange", alpha = 0.8, edgecolor = 'black', yerr=std[indices], align="center")
#plt.xticks(range(X.shape[1]), indices)
plt.ylim(0,1), plt.xlabel('Predictor Features'); plt.ylabel('Feature Importance')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

```py
Mean Squared Error on Training =  428100.87 , Variance Explained = 0.82 
```

![å›¾ç‰‡](img/6262d930974113cf78a782fd287a8df6.png)

### æ„å»ºéšæœºæ£®æ—æ¨¡å‹

ç”±äº Shapley æ˜¯åŸºäºæ¨¡å‹çš„ï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªæ¨¡å‹

+   è®©æˆ‘ä»¬ä»ä¸€ä¸ªå¥½çš„éšæœºæ£®æ—æ¨¡å‹å¼€å§‹ï¼Œè§‚å¯Ÿ Shapley å€¼ï¼Œç„¶åè¿”å›è¿™é‡Œå¹¶ä¿®æ”¹æ¨¡å‹

```py
seed = 73093                                                  # set the random forest hyperparameters

# #Underfit random forest
max_leaf_nodes = 2
num_tree = 10
max_features = 2

#Overfit random forest
max_leaf_nodes = 50
num_tree = 1
max_features = 6

# #Good random forest
max_leaf_nodes = 5
num_tree = 300
max_features = 2

rfr = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=seed,n_estimators=num_tree, max_features=max_features)
rfr.fit(X = x, y = Y)

Y_hat = predict_train = rfr.predict(x)

MSE = metrics.mean_squared_error(Y,Y_hat)
Var_Explained = metrics.explained_variance_score(Y,Y_hat)
print('Mean Squared Error on Training = ', round(MSE,2),', Variance Explained =', round(Var_Explained,2))

importances = rfr.feature_importances_               # expected (global) importance over the forest fore each predictor feature
std = np.std([rfr.feature_importances_ for tree in rfr.estimators_],axis=0)
indices = np.argsort(importances)[::-1].tolist()

plt.subplot(121)
plt.scatter(Y,Y_hat,s=None, c='darkorange',marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors="black")
plt.title('Random Forest Model'); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')
plt.xlim(0,7000); plt.ylim(0,7000)
plt.arrow(0,0,7000,7000,width=0.02,color='black',head_length=0.0,head_width=0.0)

plt.subplot(122)
plt.title("Feature Importances")
plt.bar([pred[i] for i in indices],rfr.feature_importances_[indices],color="darkorange", alpha = 0.8, edgecolor = 'black', yerr=std[indices], align="center")
#plt.xticks(range(X.shape[1]), indices)
plt.ylim(0,1), plt.xlabel('Predictor Features'); plt.ylabel('Feature Importance')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

```py
Mean Squared Error on Training =  428100.87 , Variance Explained = 0.82 
```

![å›¾ç‰‡](img/6262d930974113cf78a782fd287a8df6.png)

## è®¡ç®— Shapley å€¼

è®©æˆ‘ä»¬éšæœºé€‰æ‹©ä¸€äº›èƒŒæ™¯æ•°æ®æ¥è®¡ç®—å±€éƒ¨çš„ Shapley å€¼ï¼Œç„¶åç”¨å…¨å±€ Shapley åº¦é‡æ¥æ€»ç»“ã€‚

èƒŒæ™¯æ ·æœ¬æ˜¯ä»æ‰€æœ‰æ•°æ®ä¸­éšæœºé€‰æ‹©çš„å­é›†ã€‚ä¸ºä»€ä¹ˆä¸ç”¨æ‰€æœ‰æ•°æ®ä½œä¸ºèƒŒæ™¯ï¼Ÿ

+   **Shapley å€¼çš„è®¡ç®—å¯èƒ½å¾ˆæ˜‚è´µ**ï¼Œæˆ‘ä»¬éœ€è¦æ‰€æœ‰æ¨¡å‹çš„ç»„åˆæ¥è·å–æ‰€æœ‰é¢„æµ‹çš„è¾¹é™…è´¡çŒ®ï¼Œè¿™äº›è´¡çŒ®è¢«æ€»ç»“ä¸º Shapley å€¼

+   **åŸå§‹æ•°æ®å¯èƒ½ä»¥æœ‰åçš„æ–¹å¼é‡‡æ ·**ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¸Œæœ›ç¡®ä¿èƒŒæ™¯æ•°æ®å…·æœ‰ä»£è¡¨æ€§ï¼Œå³ä»åŸå§‹æ•°æ®ä¸­é‡‡æ ·ä»¥å‡å°‘åå·®ï¼Œé¿å…åœ¨ç‰¹å¾é‡è¦æ€§è¯„ä¼°ä¸­çš„åå·®

+   **æ³›åŒ–ä¸ç‰¹å®šé¢„æµ‹æ¡ˆä¾‹**ï¼Œå¦‚æœæ‰€æœ‰æ•°æ®éƒ½ç”¨ä½œèƒŒæ™¯ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªæ•´ä½“çš„æ•°æ®ç‰¹å¾é‡è¦æ€§è¯„ä¼°ï¼Œä½†æˆ‘ä»¬å¯èƒ½å¸Œæœ›ä»”ç»†é€‰æ‹©æ•°æ®æ¥æ¢ç´¢ç‰¹å®šçš„é¢„æµ‹æ¡ˆä¾‹

ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åªæ˜¯éšæœºé€‰æ‹©$n$ä¸ªæ•°æ®ã€‚

```py
background = shap.sample(x,nsamples=50,random_state=73073) 
model_explainer = shap.TreeExplainer(rfr)
shap_values = model_explainer.shap_values(background) # global Shapley Measures 
```

## å±€éƒ¨ Shapley å€¼

è®©æˆ‘ä»¬å…ˆçœ‹çœ‹å±€éƒ¨çš„ Shapley å€¼æ¥å±•ç¤ºæ•ˆç‡çš„æ¦‚å¿µã€‚

+   é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç¡®è®¤ shap å‡½æ•°çš„è¾“å‡ºæ˜¯ä¸€ä¸ª$\left[n_{background}, m\right]$çš„ nd æ•°ç»„ã€‚

```py
shap_values.shape 
```

```py
(50, 6) 
```

æˆ‘ä»¬ä¸ºèƒŒæ™¯æ¡ˆä¾‹ä¸­çš„æ¯ä¸ªé¢„æµ‹éƒ½æ‹¥æœ‰å±€éƒ¨çš„ Shapley å€¼ã€‚è®©æˆ‘ä»¬å¯è§†åŒ–ä¸€ä¸ªæ¥å±•ç¤ºè¿™ä¸€ç‚¹ã€‚

+   æˆ‘ç¼–å†™äº†è¿™ä¸ªè‡ªå®šä¹‰çš„å¯è§†åŒ–ä»£ç ï¼Œä»¥æ¸…æ¥šåœ°ä¼ è¾¾å±€éƒ¨çš„ Shapley å€¼å’Œæ•ˆç‡çš„æ¦‚å¿µã€‚

+   æˆ‘ä»¬ä»è®­ç»ƒå“åº”ç‰¹å¾çš„å‡å€¼å¼€å§‹ï¼Œä¸ºæ¯ä¸ªé¢„æµ‹ç‰¹å¾æ·»åŠ å±€éƒ¨çš„ Shapley å€¼ï¼Œä»¥è¾¾åˆ°é¢„æµ‹ã€‚

```py
nback = 7

resp_avg  = np.average(Y_hat)
yhat = rfr.predict(background.iloc[[nback]])

current = resp_avg

plt.subplot(111)

plt.plot([current,current],[0,0.3],color='black',lw=2,zorder=1)
plt.plot([current-2,current],[0.2,0.3],color='black',lw=2,zorder=1)
plt.plot([current,current+2],[0.3,0.2],color='black',lw=2,zorder=1)
for i in range(len(pred)+1):
    plt.scatter(current,i+0.5,color='grey',edgecolor='black',zorder=10)
    if i < len(pred):
        if shap_values[nback,i] > 0.0:
            color = 'red'
        else:
            color = 'blue'
        plt.plot([current,current + shap_values[nback,i]],[i+1,i+1],color=color,lw=2,zorder=1)
        plt.plot([current,current],[i+0.6,i+1],color=color,lw=2,zorder=1)
        plt.plot([current + shap_values[nback,i],current + shap_values[nback,i]],[i+1,i+1.3],color=color,lw=2,zorder=1)
        plt.plot([current + shap_values[nback,i]-2,current + shap_values[nback,i]],[i+1.2,i+1.3],color=color,lw=2,zorder=1)
        plt.plot([current + shap_values[nback,i],current + shap_values[nback,i]+2],[i+1.3,i+1.2],color=color,lw=2,zorder=1)
        if shap_values[nback,i] > 0.0:
            plt.annotate('+ ' + str(np.round(shap_values[nback,i],0)),[current + shap_values[nback,i]*0.5,i+1.1],ha='center')
        else:
            plt.annotate('- ' + str(np.round(abs(shap_values[nback,i]),0)),[current + shap_values[nback,i]*0.5,i+1.1],ha='center')
        current = current + shap_values[nback,i]

plt.plot([current,current],[i+0.7,i+1],color='black',lw=2,zorder=1)
plt.plot([current-2,current],[i+0.9,i+1],color='black',lw=2,zorder=1)
plt.plot([current,current+2],[i+1,i+0.9],color='black',lw=2,zorder=1)

plt.plot([resp_avg,resp_avg],[-0.5,len(pred)+1.5],color='black',ls='--',zorder=1)
plt.plot([yhat,yhat],[-0.5,len(pred)+1.5],color='black',ls='--',zorder=1)
plt.annotate('Response Feature, Training Average',[resp_avg-8,1.0],rotation=90.0)
plt.annotate('Model Prediction',[yhat-8,1.0],rotation=90.0)

plt.yticks(ticks=np.arange(len(pred)+2), labels=[r'None / $\overline{y}$'] + pred + [r'$\hat{y}=f(X)$'])
add_grid(); plt.ylim([-0.5,len(pred)+1.5])
plt.xlabel('Production (MCFPD)'); plt.ylabel('Feature'); plt.title('Local Shapley Values, Background Index: ' + str(nback))
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=0.8, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/223f16f325ce6407f83bcbcad31a87a9.png)

ç°åœ¨ï¼Œæˆ‘å‘æ‚¨å±•ç¤ºå†…ç½®çš„ç»˜å›¾æ–¹æ³•ï¼Œä½¿ç”¨ shap Python åŒ…ä»¥ç›¸åŒçš„æ–¹å¼ä¼ è¾¾ç›¸åŒçš„ä¿¡æ¯ã€‚

## Shapley åŠ›å›¾

æˆ‘ä»¬å¯ä»¥åŒæ—¶å¯è§†åŒ–æ‰€æœ‰æ ·æœ¬æ•°æ®ä¸­çš„æ‰€æœ‰ Shapley å€¼ï¼ŒæŒ‰èƒŒæ™¯æ•°æ®é›†çš„é¡ºåºæ’åˆ—ã€‚

+   è“è‰²è¡¨ç¤ºé¢„æµ‹äº§é‡çš„å‡å°‘ï¼Œçº¢è‰²è¡¨ç¤ºé¢„æµ‹äº§é‡çš„å¢åŠ 

æˆ‘ä»¬æ­£åœ¨ä¸€æ¬¡æ€§å¯è§†åŒ–æ‰€æœ‰èƒŒæ™¯æ ·æœ¬æ•°æ®ã€‚æŒ‰åŸå§‹æ ·æœ¬é¡ºåºé‡æ–°æ’åºï¼Œå¹¶é€‰æ‹© nback ç´¢å¼•ä¸ä¸Šé¢çš„å›¾è¿›è¡Œæ¯”è¾ƒã€‚

```py
shap.force_plot(model_explainer.expected_value,shap_values,background,out_names = ['Production'],feature_names=pred,) 
```

**å¯è§†åŒ–çœç•¥ï¼ŒJavaScript åº“æœªåŠ è½½ï¼**

ä½ åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­è¿è¡Œäº†`initjs()`å—ï¼Ÿå¦‚æœè¿™ä¸ªç¬”è®°æœ¬æ˜¯ä»å¦ä¸€ä¸ªç”¨æˆ·é‚£é‡Œæ¥çš„ï¼Œä½ å¿…é¡»ä¹Ÿä¿¡ä»»è¿™ä¸ªç¬”è®°æœ¬ï¼ˆæ–‡ä»¶ -> ä¿¡ä»»ç¬”è®°æœ¬ï¼‰ã€‚å¦‚æœä½ åœ¨ github ä¸ŠæŸ¥çœ‹è¿™ä¸ªç¬”è®°æœ¬ï¼ŒJavaScript å·²ç»è¢«ç§»é™¤ä»¥ä¿éšœå®‰å…¨ã€‚å¦‚æœä½ åœ¨ä½¿ç”¨ JupyterLabï¼Œè¿™ä¸ªé”™è¯¯æ˜¯å› ä¸ºè¿˜æ²¡æœ‰ç¼–å†™ JupyterLab æ‰©å±•ã€‚

## å±€éƒ¨åŠ›å›¾

æˆ‘ä»¬ä»èƒŒæ™¯ä¸­é€‰å–ä¸€ä¸ªç‰¹å®šçš„æ ·æœ¬å¹¶å¯è§†åŒ–åŠ›å›¾ã€‚

+   æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸Šè¿°å›¾è¡¨çš„èµ·æºï¼Œå³ç»™å®šæ ·æœ¬$i$ä¸­çš„å±€éƒ¨å€¼é›†çš„æ‰€æœ‰ç‰¹å¾çš„ Shapley å€¼ï¼ˆ$x_i$ï¼‰ã€‚

å°†è¿™ä¸ªç»“æœä¸ä¸Šé¢æˆ‘åˆ¶ä½œçš„è‡ªå®šä¹‰å›¾è¡¨è¿›è¡Œæ¯”è¾ƒï¼Œä½ ä¼šçœ‹åˆ°å®ƒä¼ è¾¾äº†ç›¸åŒçš„ä¿¡æ¯ã€‚

```py
shap.force_plot(model_explainer.expected_value,shap_values[nback],background.iloc[[nback]],show=False,feature_names = pred) 
```

**å¯è§†åŒ–çœç•¥ï¼ŒJavaScript åº“æœªåŠ è½½ï¼**

ä½ åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­è¿è¡Œäº†`initjs()`å—ï¼Ÿå¦‚æœè¿™ä¸ªç¬”è®°æœ¬æ˜¯ä»å¦ä¸€ä¸ªç”¨æˆ·é‚£é‡Œæ¥çš„ï¼Œä½ å¿…é¡»ä¹Ÿä¿¡ä»»è¿™ä¸ªç¬”è®°æœ¬ï¼ˆæ–‡ä»¶ -> ä¿¡ä»»ç¬”è®°æœ¬ï¼‰ã€‚å¦‚æœä½ åœ¨ github ä¸ŠæŸ¥çœ‹è¿™ä¸ªç¬”è®°æœ¬ï¼ŒJavaScript å·²ç»è¢«ç§»é™¤ä»¥ä¿éšœå®‰å…¨ã€‚å¦‚æœä½ åœ¨ä½¿ç”¨ JupyterLabï¼Œè¿™ä¸ªé”™è¯¯æ˜¯å› ä¸ºè¿˜æ²¡æœ‰ç¼–å†™ JupyterLab æ‰©å±•ã€‚

æ„Ÿè°¢è–›æ¾Â·é©¬å¯¹æ”¹è¿›ä¸Šè¿°å±€éƒ¨ Shapley å€¼å†…å®¹å’Œå¯è§†åŒ–çš„å»ºè®®ã€‚

## å…¨å±€ Shapley å€¼

è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹å…¨å±€ Shapley åº¦é‡ã€‚

+   èƒŒæ™¯æ•°æ®ä¸Šç»å¯¹ SHAP å€¼çš„ç®—æœ¯å¹³å‡å€¼çš„æ’åºæ¡å½¢å›¾

+   èƒŒæ™¯æ•°æ®ä¸Š SHAP å€¼çš„æ’åºå›¾

+   èƒŒæ™¯æ•°æ®ä¸Š SHAP å€¼çš„ violin å›¾

æ³¨æ„ï¼šæ‰€æœ‰è¿™äº›æ–¹æ³•éƒ½æ˜¯åº”ç”¨æ¯ä¸ªç‰¹å¾çš„å…¨çƒå¹³å‡å€¼ï¼ˆ$E[X_i]$ï¼‰æ¥å¡«è¡¥ä¸åŒ…å«ç‰¹å¾$i$çš„æ¡ˆä¾‹ã€‚

```py
plt.subplot(131)
shap.summary_plot(show=False,feature_names = pred, shap_values = shap_values, features = background, plot_type="bar",color = "darkorange",cmap = plt.cm.inferno)
plt.ylabel('Predictor Features')

plt.subplot(132)
shap.summary_plot(show=False,feature_names = pred, shap_values = shap_values, features = background,cmap = plt.cm.inferno)

plt.subplot(133)
shap.summary_plot(show=False,feature_names = pred, shap_values = shap_values, features = background,plot_type = "violin")

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2)
plt.show() 
```

![å›¾ç‰‡](img/e3945b1cd0e515c4f1adfcc33e2436d8.png)

ä¸­å¿ƒå›¾å’Œå³ä¾§å›¾æ˜¾ç¤ºäº†æ‰€æœ‰ç‰¹å¾åœ¨éšæœºé€‰æ‹©çš„èƒŒæ™¯æ ·æœ¬ä¸Šçš„ Shapley å€¼ï¼Œè€Œå·¦ä¾§å›¾æ˜¯å¹³å‡ç»å¯¹ Shapley å€¼çš„æ¡å½¢å›¾ã€‚

+   å­”éš™ç‡ã€æ¸—é€ç‡å’Œ TOC æ˜¯é¡¶çº§ç‰¹å¾

## è¯„è®º

è¿™æ˜¯å¯¹ç‰¹å¾æ’åçš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„ YouTube è®²åº§é“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![å›¾ç‰‡](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[Cockrell å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[Jackson åœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œä»–åœ¨é‚£é‡Œç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜

+   [è®¡ç®—æœºä¸åœ°å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°å­¦åä¼šçš„[æ•°å­¦åœ°å­¦](https://link.springer.com/journal/11004/editorial-board)è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²æ’°å†™è¶…è¿‡ 70 ç¯‡[åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[åœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[Python ä¸­çš„åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[Python ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šä»£ç å®è·µæŒ‡å—](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„ [YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å« 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†å·¥ä½œæµç¨‹é“¾æ¥ï¼Œè¿™äº›å­˜å‚¨åº“ä½äºä»–çš„ [GitHub è´¦æˆ·](https://github.com/GeostatsGuy)ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›å¸¸é’å†…å®¹ã€‚äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³è¦ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™ä¸ªå†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°å­¦å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œï¼Œæ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°å­¦æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹æ¥å¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°å­¦é—®é¢˜ï¼

+   æˆ‘å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»ã€‚

æˆ‘æ€»æ˜¯ä¹äºè®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼Œåšå£«ï¼Œæ³¨å†Œå·¥ç¨‹å¸ˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢æ•™æˆ

æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
