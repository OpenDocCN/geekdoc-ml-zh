# å²­å›å½’

> åŸæ–‡ï¼š[`geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ridge_regression.html`](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ridge_regression.html)

Michael J. Pyrczï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

ç”µå­ä¹¦â€œPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œâ€çš„ç« èŠ‚ã€‚

å¼•ç”¨æ­¤ç”µå­ä¹¦å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *ã€ŠPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œã€‹* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138 ![DOI](https://doi.org/10.5281/zenodo.15169138)

æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠæ›´å¤šå†…å®¹å‡å¯åœ¨æ­¤å¤„è·å¾—ï¼š

å¼•ç”¨ MachineLearningDemos GitHub å­˜å‚¨åº“å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *MachineLearningDemos: Python æœºå™¨å­¦ä¹ æ¼”ç¤ºå·¥ä½œæµç¨‹å­˜å‚¨åº“* (0.0.3) [è½¯ä»¶]. Zenodo. DOI: 10.5281/zenodo.13835312\. GitHub å­˜å‚¨åº“: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos) ![DOI](https://zenodo.org/doi/10.5281/zenodo.13835312)

ä½œè€…ï¼šMichael J. Pyrcz

Â© ç‰ˆæƒæ‰€æœ‰ 2024.

æœ¬ç« æ˜¯å…³äº**å²­å›å½’**çš„æ•™ç¨‹å’Œæ¼”ç¤ºã€‚

**YouTube è®²åº§**ï¼šæŸ¥çœ‹æˆ‘å…³äºä»¥ä¸‹å†…å®¹çš„è®²åº§ï¼š

+   [æœºå™¨å­¦ä¹ ç®€ä»‹](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)

+   [çº¿æ€§å›å½’](https://youtu.be/0fzbyhWiP84)

+   [å²­å›å½’](https://youtu.be/pMGO40yXZ5Y?si=ygJAheyX-v2BmSiR)

+   [LASSO å›å½’](https://youtu.be/cVFYhlCCI_8?si=NbwIDaZj30vxezn2)

+   [èŒƒæ•°](https://youtu.be/JmxGlrurQp0?si=vuF1TXDbZkyRC1j-)

è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘ YouTube ä¸Šçš„[æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«é“¾æ¥è‰¯å¥½çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºè·å–ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœä½ æƒ³çŸ¥é“æˆ‘çš„åŠ¨æœºï¼Œè¯·æŸ¥çœ‹[Michael çš„æ•…äº‹](https://michaelpyrcz.com/my-story)ã€‚

## å²­å›å½’çš„åŠ¨æœº

è¿™é‡Œæœ‰ä¸€ä¸ªç®€å•çš„æµç¨‹ï¼Œæ¼”ç¤ºäº†å²­å›å½’ä¸çº¿æ€§å›å½’åœ¨åŸºäºæœºå™¨å­¦ä¹ çš„é¢„æµ‹ä¸­çš„æ¯”è¾ƒã€‚ä¸ºä»€ä¹ˆä»çº¿æ€§å›å½’å¼€å§‹ï¼Ÿ

+   çº¿æ€§å›å½’æ˜¯æœ€ç®€å•çš„å‚æ•°åŒ–é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹

+   æˆ‘ä»¬é€šè¿‡ä»è®­ç»ƒå‡æ–¹è¯¯å·®çš„å¯¼æ•°è®¡ç®—å‡ºçš„è§£æè§£æ¥å­¦ä¹ è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹

+   è¿™ä½¿æˆ‘ä»¬å¼€å§‹äº†æŸå¤±å‡½æ•°å’ŒèŒƒæ•°æ¦‚å¿µçš„å­¦ä¹ 

+   æˆ‘ä»¬å¯ä»¥è®¿é—®æ¨¡å‹ä¸ç¡®å®šæ€§çš„ç½®ä¿¡åŒºé—´å’Œå‚æ•°æ˜¾è‘—æ€§çš„å‡è®¾æ£€éªŒçš„è§£æè¡¨è¾¾å¼

ä¸ºä»€ä¹ˆæ¥ä¸‹æ¥è¦ä»‹ç»å²­å›å½’ï¼Ÿ

+   æœ‰æ—¶çº¿æ€§å›å½’å¹¶ä¸è¶³å¤Ÿç®€å•ï¼Œæˆ‘ä»¬å®é™…ä¸Šéœ€è¦ä¸€ä¸ªæ›´ç®€å•çš„æ¨¡å‹ï¼

+   ä»‹ç»æ¨¡å‹æ­£åˆ™åŒ–å’Œè¶…å‚æ•°è°ƒæ•´çš„æ¦‚å¿µ

è¿™é‡Œæœ‰ä¸€äº›å…³äºé¢„æµ‹æœºå™¨å­¦ä¹ å²­å›å½’æ¨¡å‹çš„åŸºæœ¬ç»†èŠ‚ï¼Œæˆ‘ä»¬å…ˆä»çº¿æ€§å›å½’å¼€å§‹ï¼Œç„¶åè¿‡æ¸¡åˆ°å²­å›å½’ï¼š

## çº¿æ€§å›å½’

çº¿æ€§å›å½’ç”¨äºé¢„æµ‹ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹ä¸€ç»„æ•°æ®æ‹Ÿåˆçš„çº¿æ€§æ¨¡å‹ã€‚

![](img/d82dccdcda485413554e49d73e4d1fc8.png)

ä¸¾ä¾‹è¯´æ˜çº¿æ€§å›å½’æ¨¡å‹ã€‚

è®©æˆ‘ä»¬å…ˆå®šä¹‰ä¸€äº›æœ¯è¯­ï¼Œ

+   **é¢„æµ‹ç‰¹å¾** - é¢„æµ‹æ¨¡å‹çš„è¾“å…¥ç‰¹å¾ï¼Œé‰´äºæˆ‘ä»¬åªè®¨è®ºçº¿æ€§å›å½’è€Œä¸è®¨è®ºå¤šå…ƒçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªé¢„æµ‹ç‰¹å¾ $x$ã€‚åœ¨æˆ‘ä»¬çš„å›¾è¡¨ï¼ˆåŒ…æ‹¬ä¸Šé¢çš„ï¼‰ä¸­ï¼Œé¢„æµ‹ç‰¹å¾ä½äº x è½´ä¸Šã€‚

+   **å“åº”ç‰¹å¾** - é¢„æµ‹æ¨¡å‹çš„è¾“å‡ºç‰¹å¾ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ$y$ã€‚åœ¨æˆ‘ä»¬çš„å›¾è¡¨ï¼ˆåŒ…æ‹¬ä¸Šé¢çš„ï¼‰ä¸­ï¼Œå“åº”ç‰¹å¾ä½äº y è½´ä¸Šã€‚

ç°åœ¨ï¼Œä»¥ä¸‹æ˜¯çº¿æ€§å›å½’çš„ä¸€äº›å…³é”®æ–¹é¢ï¼š

**å‚æ•°åŒ–æ¨¡å‹**

è¿™æ˜¯ä¸€ä¸ªå‚æ•°åŒ–çš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬æ¥å—ä¸€ä¸ªå…ˆéªŒçš„çº¿æ€§å‡è®¾ï¼Œç„¶åè·å¾—ä¸€ä¸ªéå¸¸ä½çš„å‚æ•°åŒ–è¡¨ç¤ºï¼Œè¿™ä½¿å¾—åœ¨æ²¡æœ‰å¤§é‡æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿå®¹æ˜“è®­ç»ƒã€‚

+   æ‹Ÿåˆæ¨¡å‹æ˜¯ä¸€ä¸ªåŸºäºæ‰€æœ‰å¯ç”¨ç‰¹å¾ $x_1,\ldots,x_m$ çš„ç®€å•åŠ æƒçº¿æ€§åŠ æ€§æ¨¡å‹ã€‚

+   å‚æ•°åŒ–æ¨¡å‹çš„å½¢å¼ä¸ºï¼š

$$ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 $$

è¿™æ˜¯çº¿æ€§æ¨¡å‹å‚æ•°çš„å¯è§†åŒ–ï¼Œ

![](img/175e41e10e74a46b4a56258ccdfb94c0.png)

çº¿æ€§æ¨¡å‹çš„å‚æ•°ã€‚

**æœ€å°äºŒä¹˜æ³•**

å¯¹äº L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼Œæ¨¡å‹å‚æ•° $b_1,\ldots,b_m,b_0$ çš„è§£æè§£æ˜¯å¯ç”¨çš„ï¼Œè¯¯å·®è¢«æ±‚å’Œå¹¶å¹³æ–¹ï¼Œè¿™è¢«ç§°ä¸ºæœ€å°äºŒä¹˜æ³•ã€‚

+   æˆ‘ä»¬æœ€å°åŒ–è®­ç»ƒæ•°æ®ä¸Šçš„è¯¯å·®ï¼Œæ®‹å·®å¹³æ–¹å’Œï¼ˆRSSï¼‰ï¼š

$$ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)Â² $$

å…¶ä¸­ $y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾å€¼ï¼Œè€Œ $\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ æ˜¯æ¨¡å‹çš„é¢„æµ‹å€¼ï¼Œè¿™äº›é¢„æµ‹å€¼è¦†ç›–äº† $\alpha = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ã€‚

è¿™æ˜¯ L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼ˆMSEï¼‰çš„å¯è§†åŒ–ï¼Œ

![](img/b25cdf17fb10f18b362f50ba655df92b.png)

çº¿æ€§æ¨¡å‹çš„æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®ã€‚

+   è¿™å¯ä»¥ç®€åŒ–ä¸ºè®­ç»ƒæ•°æ®ä¸Šçš„å¹³æ–¹è¯¯å·®ä¹‹å’Œï¼Œ

\begin{equation} \sum_{i=1}^n (\Delta y_i)Â² \end{equation}

å…¶ä¸­ $\Delta y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾è§‚å¯Ÿå€¼ $y_i$ å‡å»æ¨¡å‹é¢„æµ‹ $\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ï¼Œè¿™äº›é¢„æµ‹å€¼è¦†ç›–äº† $i = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ã€‚

**å‡è®¾**

æˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹æœ‰ä¸€äº›é‡è¦çš„å‡è®¾ï¼Œ

+   **æ— è¯¯å·®** - é¢„æµ‹å˜é‡æ— è¯¯å·®ï¼Œä¸æ˜¯éšæœºå˜é‡

+   **çº¿æ€§** - å“åº”æ˜¯ç‰¹å¾ï¼ˆsï¼‰çš„çº¿æ€§ç»„åˆ

+   **å¸¸æ•°æ–¹å·®** - å“åº”è¯¯å·®åœ¨é¢„æµ‹å˜é‡å€¼ä¸Šæ˜¯å¸¸æ•°

+   **è¯¯å·®ç‹¬ç«‹æ€§** - å“åº”è¯¯å·®ä¹‹é—´ä¸ç›¸å…³

+   **æ— å¤šé‡å…±çº¿æ€§** - æ²¡æœ‰ç‰¹å¾ä¸å…¶ä»–ç‰¹å¾å†—ä½™

## å²­å›å½’

åœ¨å²­å›å½’ä¸­ï¼Œæˆ‘ä»¬å‘æœ€å°åŒ–ä¸­æ·»åŠ äº†ä¸€ä¸ªè¶…å‚æ•° $\lambda$ï¼Œä»¥åŠä¸€ä¸ªæ”¶ç¼©æƒ©ç½šé¡¹ï¼Œ$\sum_{j=1}^m b_{\alpha}Â²$ã€‚

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)Â² + \lambda \sum_{j=1}^m b_{\alpha}Â² $$

å› æ­¤ï¼Œå²­å›å½’è®­ç»ƒæ•´åˆäº†ä¸¤ä¸ªç»å¸¸æ˜¯ç›¸äº’ç«äº‰çš„ç›®æ ‡æ¥æ‰¾åˆ°æ¨¡å‹å‚æ•°ï¼Œ

+   æ‰¾åˆ°æ¨¡å‹å‚æ•°ï¼Œä»¥æœ€å°åŒ–è®­ç»ƒæ•°æ®ä¸­çš„è¯¯å·®

+   æœ€å°åŒ–æ–œç‡å‚æ•°è¶‹å‘äºé›¶

æ³¨æ„ï¼šlambda ä¸åŒ…æ‹¬æˆªè·ï¼Œ$b_0$ã€‚

$\lambda$ æ˜¯ä¸€ä¸ªæ§åˆ¶æ¨¡å‹æ‹Ÿåˆç¨‹åº¦çš„è¶…å‚æ•°ï¼Œå¯èƒ½ä¸æ¨¡å‹çš„åå·®-æ–¹å·®æƒè¡¡æœ‰å…³ã€‚

![](img/2a291289b4f4b55ba9adbb7538140a67.png)

å²­å›å½’çš„æµ‹è¯•è¯¯å·®åˆ†é‡å’Œé¢„æœŸæµ‹è¯•å¹³æ–¹è¯¯å·®ã€‚

+   å½“ $\lambda \rightarrow 0$ æ—¶ï¼Œè§£è¶‹è¿‘äºçº¿æ€§å›å½’ï¼Œæ²¡æœ‰åå·®ï¼ˆç›¸å¯¹äºçº¿æ€§æ¨¡å‹æ‹Ÿåˆï¼‰ï¼Œä½†æ¨¡å‹æ–¹å·®å¯èƒ½æ›´é«˜

+   éšç€ $\lambda$ çš„å¢åŠ ï¼Œæ¨¡å‹æ–¹å·®é™ä½ï¼Œæ¨¡å‹åå·®å¢åŠ ï¼Œæ¨¡å‹å˜å¾—ç®€å•

+   å½“ $\lambda \rightarrow \infty$ æ—¶ï¼Œæ¨¡å‹å‚æ•° $b_1,\ldots,b_m$ æ”¶ç¼©åˆ° 0.0ï¼Œæ¨¡å‹é¢„æµ‹è¶‹è¿‘äºè®­ç»ƒæ•°æ®å“åº”ç‰¹å¾å‡å€¼

åœ¨ä¸‹é¢çš„å·¥ä½œæµç¨‹ä¸­ï¼Œå°†è®¡ç®—å„ç§ $\lamda$ å€¼çš„å¤šç§å²­å›å½’æ¨¡å‹ï¼Œ

![](img/4f26df3583b9f434159b8bbb15cc292b.png)

å²­å›å½’æ¨¡å‹å…·æœ‰ä»ä½åˆ°é«˜çš„ $\lambda$ è¶…å‚æ•°å€¼ã€‚

## ç”¨äºåŸºäºäº¤å‰éªŒè¯çš„è¶…å‚æ•°è°ƒæ•´çš„è®­ç»ƒ/æµ‹è¯•æ•°æ®åˆ†å‰²

å¯ç”¨æ•°æ®è¢«åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

+   é€šå¸¸ï¼Œ15-30% çš„æ•°æ®è¢«ä¿ç•™ç”¨äºè®­ç»ƒï¼Œä½œä¸ºæµ‹è¯•æ•°æ®

+   æµ‹è¯•æ•°æ®çš„é€‰æ‹©åº”è¯¥æ˜¯å…¬å¹³çš„ï¼Œé¢„æµ‹éš¾åº¦ï¼ˆåç§»/ä¸è®­ç»ƒæ•°æ®ä¸åŒï¼‰ç›¸åŒ [å…¬å¹³è®­ç»ƒ-æµ‹è¯•åˆ†å‰²è®ºæ–‡](https://www.sciencedirect.com/science/article/pii/S0920410521015023) (Salazar et al., 2022)ã€‚

+   è¿˜åŒ…æ‹¬è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•åˆ†å‰²ï¼Œä»¥åŠ k æŠ˜äº¤å‰éªŒè¯

åŸºæœ¬ä¸Šï¼Œæ‰€æœ‰è¿™äº›æ–¹æ³•éƒ½æ˜¯é€šè¿‡ä½¿ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—è¶…å‚æ•°ä¸Šæµ‹è¯•æ¨¡å‹åœ¨ä¿ç•™çš„æµ‹è¯•æ•°æ®ä¸Šçš„å‡†ç¡®æ€§æ¥è¿›è¡Œçš„ã€‚ç„¶åé€‰æ‹©æœ€å°åŒ–ä¿ç•™æµ‹è¯•æ•°æ®é›†é”™è¯¯çš„è¶…å‚æ•°ï¼Œè¿™å°±æ˜¯è¶…å‚æ•°è°ƒæ•´ã€‚

+   åœ¨è¶…å‚æ•°è°ƒæ•´åï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®ä»¥åŠé€‰å®šçš„è¶…å‚æ•°é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚

+   è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ–¹æ³•éšåä½¿ç”¨æ•°æ®é›†çš„ç¬¬äºŒä¸ªä¿ç•™å­é›†æ¥æ£€æŸ¥è°ƒä¼˜æ¨¡å‹ï¼Œè¿™äº›æ•°æ®æ—¢æœªç”¨äºæ¨¡å‹è®­ç»ƒä¹Ÿæœªç”¨äºæ¨¡å‹è°ƒä¼˜ã€‚

åœ¨ä»¥ä¸‹å·¥ä½œæµç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è®­ç»ƒå’Œæµ‹è¯•æ–¹æ³•è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ã€‚ä»¥ä¸‹æ˜¯æ›´å¤šç»†èŠ‚å’Œç›¸å…³æ¦‚å¿µçš„æ€»ç»“ã€‚

## æ¨¡å‹å‚æ•°è®­ç»ƒ

è®­ç»ƒæ•°æ®è¢«åº”ç”¨äºè®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œä½¿å¾—æ¨¡å‹æœ€å°åŒ–ä¸è®­ç»ƒæ•°æ®çš„å¤±é…

+   é€šå¸¸ä½¿ç”¨**å‡æ–¹è¯¯å·®**ï¼ˆç§°ä¸º**L2 èŒƒæ•°**ï¼‰ä½œä¸ºæŸå¤±å‡½æ•°æ¥æ€»ç»“æ¨¡å‹å¤±é…

+   **æœ€å°åŒ–æŸå¤±å‡½æ•°**å¯¹äºç®€å•æ¨¡å‹ï¼Œè§£æè§£å¯èƒ½æ˜¯å¯ç”¨çš„ï¼Œä½†å¯¹äºå¤§å¤šæ•°æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œè¿™éœ€è¦è¿­ä»£ä¼˜åŒ–æ–¹æ³•æ¥æ‰¾åˆ°æœ€ä½³æ¨¡å‹å‚æ•°

è¿™é‡Œæ˜¯è§£æè§£çš„æ¨å¯¼è¿‡ç¨‹ï¼Œä»å¸¦æœ‰æœ€å°äºŒä¹˜ï¼ˆå·¦ï¼‰å’Œæ­£åˆ™åŒ–ï¼ˆå³ï¼‰é¡¹çš„æŸå¤±å‡½æ•°å¼€å§‹ï¼Œ

$$ \left( \text{RSS} + \text{shrinkage} \right) = \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)Â² + \lambda \sum_{j=1}^m b_{\alpha}Â² $$

è®©æˆ‘ä»¬å°†è¿™ä¸ªè¡¨è¾¾å¼è½¬æ¢ä¸ºçŸ©é˜µè¡¨ç¤ºä»¥æ–¹ä¾¿è®¡ç®—ï¼Œ

$$ \left( \text{RSS} + \text{shrinkage} \right) = (ğ‘¦âˆ’ğ‘‹\beta)^ğ‘‡ (ğ‘¦âˆ’ğ‘‹\beta)+\lambda \beta^ğ‘‡ \beta $$

å…¶ä¸­ $\beta$ æ˜¯æ¨¡å‹å‚æ•°çš„å‘é‡ï¼Œ$y$ æ˜¯å“åº”ç‰¹å¾å€¼çš„å‘é‡ï¼Œ$X$ æ˜¯é¢„æµ‹ç‰¹å¾å€¼çš„çŸ©é˜µï¼Œè¿™äº›éƒ½æ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸Šã€‚

æˆ‘ä»¬å¯ä»¥å±•å¼€äºŒæ¬¡é¡¹ï¼Œ

$$ \left( \text{RSS} + \text{shrinkage} \right) = ğ‘¦âˆ’2ğ‘‹^ğ‘‡ ğ‘¦\beta+(ğ‘‹^ğ‘‡ ğ‘‹) \beta^ğ‘‡ \beta+\lambda \beta^ğ‘‡ \beta $$

æˆ‘ä»¬å¯¹æ¨¡å‹å‚æ•°æ±‚åå¯¼æ•°ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸º 0.0ï¼Œ

$$ \frac{\partial}{\partial \beta} \left( \text{RSS} + \text{shrinkage} \right) = -2X^T y + 2(X^T X) \beta + 2\lambda \beta = 0 $$

ç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€äº›å°çš„è°ƒæ•´è¿›ä¸€æ­¥ç®€åŒ–ã€‚

$$ 2(ğ‘‹^ğ‘‡ ğ‘‹) \beta + 2 \lambda \beta = 2ğ‘‹^ğ‘‡ ğ‘¦ $$

ä¸¤è¾¹åŒæ—¶é™¤ä»¥ 2ï¼Œ

$$ (ğ‘‹^ğ‘‡ ğ‘‹) \beta + \lambda \beta = ğ‘‹^ğ‘‡ ğ‘¦ $$

å°†å…¬å…±é¡¹åˆ†ç»„ï¼Œ

$$ (ğ‘‹^ğ‘‡ ğ‘‹+ \lambda I) \beta =ğ‘‹^ğ‘‡ ğ‘¦ $$

å…¶ä¸­ $I$ æ˜¯å•ä½çŸ©é˜µã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥æ±‚è§£å²­å›å½’å‚æ•°ï¼Œ

$$ ğ›½=\left(ğ‘‹^ğ‘‡ ğ‘‹+\lambda I \right)^{-1} ğ‘‹^ğ‘‡ ğ‘¦ $$

æ³¨æ„ï¼Œ$ğ‘‹^ğ‘‡ ğ‘‹+\lambda I$ é€šå¸¸æ˜¯å¯ä»¥é€†çš„ï¼Œæ‰€ä»¥è¿™æ˜¯å¯è§£çš„ã€‚

è¿™ä¸ªè¿‡ç¨‹åœ¨ç”±è¶…å‚æ•°æŒ‡å®šçš„æ¨¡å‹å¤æ‚åº¦èŒƒå›´å†…é‡å¤è¿›è¡Œã€‚

## æ­£åˆ™åŒ–çš„è§£é‡Š

å²­å›å½’çš„å¦ä¸€ç§è§£é‡Šå’ŒåŠ¨æœºï¼Œ

+   å½“ $ğ‘š \ge ğ‘›$ æ—¶ï¼Œçº¿æ€§å›å½’æ˜¯ç—…æ€é—®é¢˜ï¼Œå³å…·æœ‰å¤šäº 1 ä¸ªè§£çš„é—®é¢˜

+   å¯¹äºç—…æ€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€äº›çº¦æŸæˆ–æ­£åˆ™åŒ–æ¥é™åˆ¶è§£çš„èŒƒå›´

![å›¾ç‰‡](img/3b29636566b2ebc1625773201be9a303.png)

ç—…æ€çº¿æ€§å›å½’æ¨¡å‹ï¼ˆå·¦ï¼‰ï¼Œä»¥åŠå…·æœ‰é¢å¤–æ”¶ç¼©çº¦æŸçš„è‰¯æ€å²­å›å½’ï¼ˆå³ï¼‰ã€‚

## æ¨¡å‹è¶…å‚æ•°è°ƒä¼˜

ä¿ç•™çš„æµ‹è¯•æ•°æ®è¢«æ£€ç´¢å‡ºæ¥ï¼Œå¹¶è®¡ç®—æŸå¤±å‡½æ•°ï¼ˆé€šå¸¸æ˜¯**L2 èŒƒæ•°**ï¼‰æ¥æ€»ç»“æµ‹è¯•æ•°æ®ä¸Šçš„è¯¯å·®

+   è¿™æ˜¯åœ¨æŒ‡å®šçš„è¶…å‚æ•°èŒƒå›´å†…é‡å¤çš„ã€‚

+   é€‰æ‹©æœ€å°åŒ–æµ‹è¯•ä¸­æŸå¤±å‡½æ•°/é”™è¯¯æ‘˜è¦çš„æ¨¡å‹å¤æ‚åº¦/è¶…å‚æ•°ã€‚

è¿™å°±æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„æ¨¡å‹è¶…å‚æ•°è°ƒæ•´ã€‚

## æ¨¡å‹è¿‡æ‹Ÿåˆ

æ¨¡å‹å¤æ‚åº¦/çµæ´»æ€§è¶…è¿‡å¯ç”¨æ•°æ®ã€æ•°æ®å‡†ç¡®æ€§ã€é¢‘ç‡å’Œè¦†ç›–èŒƒå›´å¯ä»¥è¯æ˜çš„ã€‚

+   æ¨¡å‹è§£é‡Šæ•°æ®çš„â€œç‰¹æ€§â€ï¼Œåœ¨æ¨¡å‹ä¸­æ•æ‰æ•°æ®å™ªå£°/é”™è¯¯ã€‚

+   è®­ç»ƒæ—¶ç²¾åº¦é«˜ï¼Œä½†åœ¨æµ‹è¯•/å®é™…åº”ç”¨ä¸­è¿œç¦»è®­ç»ƒæ•°æ®æ¡ˆä¾‹æ—¶ç²¾åº¦ä½ - æ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®ã€‚

å¦‚æœæ¨¡å‹åœ¨æµ‹è¯•ä¸­çš„é”™è¯¯åœ¨å¢åŠ ï¼Œè€Œåœ¨è®­ç»ƒä¸­çš„é”™è¯¯åœ¨å‡å°‘ï¼Œè¿™æ˜¯è¿‡æ‹Ÿåˆæ¨¡å‹çš„æŒ‡æ ‡ã€‚

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = False
import os                                                     # to set current working directory 
import os                                                     # to set current working directory 
import numpy as np                                            # ndarrays and matrix math
import scipy.stats as st                                      # statistical methods
import pandas as pd                                           # DataFrames
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks
from sklearn.metrics import mean_squared_error, r2_score      # specific measures to check our models
from sklearn import linear_model                              # linear regression
from sklearn.linear_model import Ridge                        # ridge regression implemented in scikit learn
from sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation
from sklearn.model_selection import train_test_split          # train and test split
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # supress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
```

å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½å¿…é¡»é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥ â€˜python -m pip install [package-name]â€™ æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£ï¼Œè¿˜æœ‰æ›´å¤šå¸®åŠ©å¯ç”¨ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥ç®€åŒ–å‘æˆ‘ä»¬çš„å›¾è¡¨æ·»åŠ æŒ‡å®šçš„ç™¾åˆ†ä½æ•°å’Œä¸»æ¬¡ç½‘æ ¼çº¿ã€‚

```py
def weighted_percentile(data, weights, perc):                 # calculate weighted percentile 
    ix = np.argsort(data)
    data = data[ix] 
    weights = weights[ix] 
    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) 
    return np.interp(perc, cdf, data)
# Function from iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049

def histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram 
    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)
    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')
    plt.plot([avg,avg],[0.0,45],color = color)
    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)
    html_str = ''
    for df in args:
        html_str += df.head().to_html()  # Using .head() for the first few rows
    display(HTML(f'<div style="display: flex;">{html_str}</div>')) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œä»¥å…ä¸¢å¤±æ–‡ä»¶å¹¶ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥æ“ä½œï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯·ç¡®ä¿å°†æ‰€éœ€çš„æ•°æ®æ–‡ä»¶ï¼ˆè§ä¸‹æ–‡ï¼‰æ”¾ç½®åœ¨æ­¤å·¥ä½œç›®å½•ä¸­ã€‚

```py
#os.chdir("C:\PGE337")                                        # set the working directory 
```

æ‚¨å°†éœ€è¦æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ä»¥åŒ…å«æ‚¨è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”åœ¨ Mac ä¸Šæ ¼å¼ä¸åŒï¼ˆä¾‹å¦‚ï¼šâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½è¡¨æ ¼æ•°æ®

è¿™æ˜¯å°†æˆ‘ä»¬çš„é€—å·åˆ†éš”æ•°æ®æ–‡ä»¶åŠ è½½åˆ° Pandas DataFrame å¯¹è±¡ä¸­çš„å‘½ä»¤ã€‚

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›† â€˜unconv_MV.csvâ€™ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…å«æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   å¯†åº¦ ($g/cm^{3}$)

+   å­”éš™ç‡ï¼ˆä½“ç§¯%ï¼‰

æ³¨æ„ï¼Œæ•°æ®é›†æ˜¯åˆæˆçš„ã€‚

æˆ‘ä»¬ä½¿ç”¨ pandas çš„ â€˜read_csvâ€™ å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸º â€˜my_dataâ€™ çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

```py
add_error = True                                              # add random error to the response feature
std_error = 1.0; seed = 71071

yname = 'Porosity'; xname = 'Density'                         # specify the predictor features (x2) and response feature (x1)
xmin = 1.0; xmax = 2.5                                        # set minimums and maximums for visualization 
ymin = 0.0; ymax = 25.0    
xlabel = 'Porosity'; ylabel = 'Density'                       # specify the feature labels for plotting
yunit = '%'; xunit = '$g/cm^{3}$'    
Xlabelunit = xlabel + ' (' + xunit + ')'
ylabelunit = ylabel + ' (' + yunit + ')'

#df = pd.read_csv("Density_Por_data.csv")                     # load the data from local current directory
df = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv") # load the data from my github repo
df = df.sample(frac=1.0, random_state = 73073); df = df.reset_index() # extract 30% random to reduce the number of data

if add_error == True:                                         # method to add error
    np.random.seed(seed=seed)                                 # set random number seed
    df[yname] = df[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df)) # add noise
    values = df._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray 
```

## è®­ç»ƒ-æµ‹è¯•æ‹†åˆ†

ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn åŒ…ä¸­çš„ train_test_split å‡½æ•°åº”ç”¨éšæœºè®­ç»ƒ-æµ‹è¯•æ‹†åˆ†ã€‚

```py
x_train, x_test, y_train, y_test = train_test_split(df[xname],df[yname],test_size=0.25,random_state=73073) # train and test split
# y_train = pd.DataFrame({yname:y_train.values}); y_test = pd.DataFrame({yname:y_test.values}) # optional to ensure response is a DataFrame

y = df[yname].values.reshape(len(df))                         # features as 1D vectors
x = df[xname].values.reshape(len(df))

df_train = pd.concat([x_train,y_train],axis=1)                # features as train and test DataFrames
df_test = pd.concat([x_test,y_test],axis=1) 
```

## å¯è§†åŒ– DataFrame

å¯è§†åŒ– DataFrame æ˜¯æ£€æŸ¥æ•°æ®çš„ç¬¬ä¸€æ­¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½å‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ©ç”¨ â€˜headâ€™ DataFrame æˆå‘˜å‡½æ•°æ¥é¢„è§ˆï¼ˆæ ¼å¼æ•´æ´ã€ç¾è§‚ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚

+   æˆ‘ä»¬æœ‰ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°æ¥å¹¶æ’é¢„è§ˆè®­ç»ƒå’Œæµ‹è¯• DataFrameã€‚

```py
print('   Training DataFrame      Testing DataFrame')
display_sidebyside(df_train,df_test) 
```

```py
 Training DataFrame      Testing DataFrame 
```

|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| 24 | 1.778580 | 11.426485 |
| 101 | 2.410560 | 8.488544 |
| 88 | 2.216014 | 10.133693 |
| 79 | 1.631896 | 12.712326 |
| 58 | 1.528019 | 16.129542 |
|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| 59 | 1.742534 | 15.380154 |
| 1 | 1.404932 | 13.710628 |
| 35 | 1.552713 | 14.131878 |
| 92 | 1.762359 | 11.154896 |
| 22 | 1.885087 | 9.403056 |

## Summary Statistics for Tabular Data

There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames. The describe command provides count, mean, minimum, maximum in a nice data table.

```py
print('     Training DataFrame         Testing DataFrame')
display_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) 
```

```py
 Training DataFrame         Testing DataFrame 
```

|  | Density | Porosity |
| --- | --- | --- |
| count | 78.000000 | 78.000000 |
| mean | 1.739027 | 12.501465 |
| std | 0.302510 | 3.428260 |
| min | 0.996736 | 3.276449 |
| max | 2.410560 | 21.660179 |
|  | Density | Porosity |
| --- | --- | --- |
| count | 27.000000 | 27.000000 |
| mean | 1.734710 | 12.380796 |
| std | 0.247761 | 2.916045 |
| min | 1.067960 | 7.894595 |
| max | 2.119652 | 18.133771 |

## Visualize the Data

Letâ€™s check the consistency and coverage of training and testing with histograms and scatter plots.

+   check to make sure the train and test data cover the range of possible feature combinations

+   ensure we are not extrapolating beyond the training data with the testing cases

```py
nbins = 20                                                    # number of histogram bins

plt.subplot(221)
freq1,_,_ = plt.hist(x=df_train[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=True,label='Train')
freq2,_,_ = plt.hist(x=df_test[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=True,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(xname + ' (' + xunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  
plt.xlim([xmin,xmax]); plt.legend(loc='upper right')   

plt.subplot(222)
freq1,_,_ = plt.hist(x=df_train[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=True,label='Train')
freq2,_,_ = plt.hist(x=df_test[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=True,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(yname + ' (' + yunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  
plt.xlim([ymin,ymax]); plt.legend(loc='upper right')   

plt.subplot(223)                                              # plot the model
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.title('Porosity vs Density')
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![_images/c4e4535126491e78be365e71b1c9a03a2d1b7af5ddc9027eed6eced1b9c42f72.png](img/019f46cd3338e55a8b84bcd57a899945.png)

## Linear Regression Model

Letâ€™s first calculate the linear regression model. We use scikit learn and then extend the same workflow to ridge regression.

+   we are building a model, $\phi = f(\rho)$, where $\phi$ is porosity and $\rho$ is density.

+   we could also say, we have â€œporosity regressed on densityâ€.

Our model has this specific equation,

$$ \phi = b_1 \times \rho + b_0 $$

```py
linear_reg = linear_model.LinearRegression()                  # instantiate the model

linear_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters
x_model = np.linspace(xmin,xmax,10)
y_model_linear = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data 

plt.subplot(111)                                              # plot the data, model with model parameters
plt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])
plt.title('Linear Regression Model, Porosity = f(Density)')
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/9068634e78d0710f8ba9632a527aa661f2ca3527b1121d2db728317558851841.png](img/5bc6600437501d419bf56af2e1dc727d.png)

You may have noticed the additional reshape operation applied to the predictor feature in the predict function.

```py
y_linear_model = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data 
```

This is needed because scikit-learn assumes more than one predictor feature; therefore, expects a 2D array of samples (rows) and features (columns), but we have only a 1D vector.

+   the reshape operation turns the 1D vector into a 2D vector with only 1 column

## Linear Regression Model Checks

Letâ€™s run some quick model checks. Much more could be done, but I limit this for brevity here.

+   see the Linear Regression chapter for more information and checks

```py
y_pred_linear = linear_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data
r_squared_linear = r2_score(df_test[yname].values, y_pred_linear)

plt.subplot(121)                                              # plot testing diagnostics 
plt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
# plt.scatter(df_test[xname], y_pred,color='grey',edgecolor='black',s = 40, alpha = 1.0, label = 'predictions',zorder=100)
plt.scatter(df_test[xname], y_pred_linear,color='white',s=120,marker='o',linewidths=1.0, edgecolors="black",zorder=300)
plt.scatter(df_test[xname], y_pred_linear,color='red',s=90,marker='*',linewidths=0.5, edgecolors="black",zorder=320,label='Predictions')

for idata in range(0,len(df_test)):
    if idata == 0:
        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],
                        y_pred_linear[idata]],color='grey',label=r'test $\Delta_{y_i}$',zorder=1)
    else:  
        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],
                        y_pred_linear[idata]],color='grey',zorder=1)

plt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])
plt.annotate(r'$rÂ²$ :' + str(np.round(r_squared_linear,2)),[1.97,15])
plt.title('Linear Regression Model, Porosity = f(Density)')
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

y_res_linear = y_pred_linear - df_test['Porosity'].values     # calculate the test residual

plt.subplot(122)
plt.hist(y_res_linear, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))
plt.title("Error Residual at Testing Data"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')
plt.vlines(0,0,5.5,color='black',ls='--',lw=2)
plt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics
plt.annotate(r'$\overline{\Delta{y}}$: ' + str(round(np.average(y_res_linear),2)),[-4,4.4])
plt.annotate(r'$\sigma_{\Delta{y}}$: ' + str(np.round(np.std(y_res_linear),2)),[-4,4.1])
add_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/d10062259023c26d588429dde18deab7d42abe383ae44ad7fa6d4c7b96ef620e.png](img/208a933176542d3185b7f4f4c9a1bf61.png)

## Ridge Regression Model

Letâ€™s replace the scikit-learn linear regression method with the scikit-learn ridge regression method.

+   note, we must now set the $\lambda$ hyperparameter.

+   in scikit-learn the hyperparameter(s) is(are) set with the instantiation of the model

```py
lam = 13.0                                                     # lambda hyperparameter

ridge_reg = Ridge(alpha=lam)                                  # instantiate the model

ridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters
x_model = np.linspace(xmin,xmax,10)
y_model_ridge = ridge_reg.predict(x_model.reshape(10,1))      # predict with the fit model

plt.subplot(111)                                              # plot the data, model with model parameters
plt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Linear Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])
plt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(lam))
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/61654e5169f1e8d42162279fae72b88a9d9108ce66bb01b9fd227b101c160daf.png](img/05ecf578788df580a4c1f1f25e9ffcb9.png)

Letâ€™s repeat the simple model checks that we applied with our linear regression model.

```py
y_pred_ridge = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data
r_squared = r2_score(df_test[yname].values, y_pred_ridge)

plt.subplot(121)                                              # plot testing diagnostics 
plt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Linear Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='test')
plt.scatter(df_test[xname], y_pred_ridge,color='white',s=120,marker='o',linewidths=1.0, edgecolors="black",zorder=300)
plt.scatter(df_test[xname], y_pred_ridge,color='red',s=90,marker='*',linewidths=0.5, edgecolors="black",zorder=320,label='predictions')

for idata in range(0,len(df_test)):
    if idata == 0:
        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],
                        y_pred_ridge[idata]],color='grey',label=r'test $\Delta_{y_i}$',zorder=1)
    else:  
        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],
                        y_pred_ridge[idata]],color='grey',zorder=1)

plt.annotate('Ridge Regression Model Parameters:',[1.81,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])
plt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(lam))
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

y_res_ridge = y_pred_ridge - df_test['Porosity'].values       # calculate the test residual

plt.subplot(122)
plt.hist(y_res_ridge, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))
plt.title("Error Residual at Testing Data"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')
plt.vlines(0,0,5.5,color='red',lw=2,zorder=1); plt.vlines(np.average(y_res_ridge),0,5.5,color='black',ls='--',zorder=10)
plt.annotate('Residual Average = ' + str(np.round(np.average(y_res_ridge),2)),[np.average(y_res_ridge)+0.2,2.5],rotation=90.0)
plt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics
plt.annotate(r'$\overline{\Delta{y}}$: ' + str(round(np.average(y_res_ridge),2)),[-4,4.4])
plt.annotate(r'$\sigma_{\Delta{y}}$: ' + str(np.round(np.std(y_res_ridge),2)),[-4,4.1])
add_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/bd6d04e063283738954b9b88fa6ed77d11eeec140126dcf4052ef6d9523d2c48.png](img/2e6dac5a06cdc0e91c454e5758d2a7af.png)

Interesting, we explained less variance and have a larger residual standard deviation (more error).

+   å¯¹äºæˆ‘ä»¬ä»»æ„é€‰æ‹©çš„è¶…å‚æ•°$\lambda$ï¼Œå²­å›å½’å®é™…ä¸Šé™ä½äº†æµ‹è¯•ä¸­è§£é‡Šçš„æ–¹å·®å’Œå‡†ç¡®æ€§

+   è¿™å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¹¶æ²¡æœ‰è°ƒæ•´è¶…å‚æ•°ä»¥è·å¾—æœ€ä½³æ¨¡å‹ï¼

## è°ƒæŸ¥ Lambda è¶…å‚æ•°

è®©æˆ‘ä»¬å¾ªç¯éå†å¤šä¸ª lambda å€¼ - ä» 0 åˆ° 100ï¼Œå¹¶è§‚å¯Ÿä»¥ä¸‹å˜åŒ–ï¼š

+   è®­ç»ƒå’Œæµ‹è¯•ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰å’Œè§£é‡Šçš„æ–¹å·®

```py
# Arrays to store the results
ncases = 6
lamd_mat = np.logspace(-3,4,ncases)
x_model = np.linspace(xmin,xmax,10)
var_explained_train = np.zeros(ncases); var_explained_test = np.zeros(ncases)
mse_train = np.zeros(ncases); mse_test = np.zeros(ncases)

for ilam in range(0,len(lamd_mat)):                           # Loop over all lambda values
    ridge_reg = Ridge(alpha=lamd_mat[ilam])
    ridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # fit model

    y_model = ridge_reg.predict(x_model.reshape(10,1))        # predict with the fit model 
    y_pred_train = ridge_reg.predict(df_train[xname].values.reshape(len(df_train),1)) # predict with the fit model 
    var_explained_train[ilam] = r2_score(df_train[yname].values, y_pred_train)
    mse_train[ilam] = mean_squared_error(df_train[yname].values, y_pred_train) 

    y_pred_test = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1))
    var_explained_test[ilam] = r2_score(df_test[yname].values, y_pred_test)
    mse_test[ilam] = mean_squared_error(df_test[yname].values, y_pred_test)    

    if ilam <= 7:
        plt.subplot(4,2,ilam+1)
        plt.plot(x_model,y_model, color='red', linewidth=2,label='Linear Regression',zorder=100)
        plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
        plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
        plt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot
        plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])
        plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])
        plt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(round(lamd_mat[ilam],4)))
        plt.xlabel(xname + ' (' + xunit + ')')
        plt.ylabel(yname + ' (' + yunit + ')')
        plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=4.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/5a585689234bfbaf29513933357bc5cb.png)

æˆ‘ä»¬å¯ä»¥ä»è®­ç»ƒçš„å²­å›å½’æ¨¡å‹çš„ä¸Šè¿°å‰ 8 ä¸ªæ¡ˆä¾‹ä¸­è§‚å¯Ÿåˆ°ï¼Œå¢åŠ $\lambda$è¶…å‚æ•°ä¼šé™ä½çº¿æ€§æ‹Ÿåˆçš„æ–œç‡ã€‚

è®©æˆ‘ä»¬åœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä¸Šç»˜åˆ¶å‡æ–¹è¯¯å·®å’Œè§£é‡Šçš„æ–¹å·®å›¾ã€‚

å›æƒ³ä¸€ä¸‹ï¼Œè§£é‡Šçš„æ–¹å·®ï¼Œ$RÂ²$ï¼Œç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼Œ

$$ RÂ² = 1 - \frac{\text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}} $$

å…¶ä¸­$SS_{\text{residual}}$æ˜¯æ®‹å·®ï¼ˆæˆ–è¯¯å·®ï¼‰çš„å¹³æ–¹å’Œï¼Œ$SS_{\text{total}}$æ˜¯æ€»å¹³æ–¹å’Œï¼ˆè§‚å¯Ÿæ•°æ®çš„æ–¹å·®ï¼‰ã€‚

å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼Œ

$$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)Â² $$

å…¶ä¸­$y_i$æ˜¯å®é™…å€¼ï¼Œ$\hat{y}_i$æ˜¯é¢„æµ‹å€¼ï¼Œ$n$æ˜¯æ•°æ®ç‚¹çš„æ•°é‡ã€‚

```py
ncases = 100
lamd_mat = np.logspace(-3,4,ncases)
x_model = np.linspace(xmin,xmax,10)
var_explained_train = np.zeros(ncases); var_explained_test = np.zeros(ncases)
mse_train = np.zeros(ncases); mse_test = np.zeros(ncases)

for ilam in range(0,len(lamd_mat)):                           # Loop over all lambda values
    ridge_reg = Ridge(alpha=lamd_mat[ilam])
    ridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # fit model

    y_model = ridge_reg.predict(x_model.reshape(10,1))        # predict with the fit model 
    y_pred_train = ridge_reg.predict(df_train[xname].values.reshape(len(df_train),1)) # predict with the fit model 
    var_explained_train[ilam] = r2_score(df_train[yname].values, y_pred_train)
    mse_train[ilam] = mean_squared_error(df_train[yname].values, y_pred_train) 

    y_pred_test = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1))
    var_explained_test[ilam] = r2_score(df_test[yname].values, y_pred_test)
    mse_test[ilam] = mean_squared_error(df_test[yname].values, y_pred_test) 

plt.subplot(121)
plt.plot(lamd_mat, var_explained_train,  color='darkorange', linewidth = 2, label = 'Train')
plt.plot(lamd_mat, var_explained_test,  color='red', linewidth = 2, label = 'Test')
plt.title('Variance Explained vs. Lambda'); plt.xlabel('Lambda'); plt.ylabel('Variance Explained')
plt.xlim(0.001,10000.); plt.xscale('log'); plt.ylim(0,1.0); 
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linestyle=':', linewidth=0.5); plt.legend()  

plt.subplot(122)
plt.plot(lamd_mat, mse_train,  color='darkorange', linewidth = 2, label = 'Train')
plt.plot(lamd_mat, mse_test,  color='red', linewidth = 2, label = 'Test')
plt.title('MSE vs. Lambda'); plt.xlabel('Lambda'); plt.ylabel('Mean Square Error')
plt.xlim(0.001,10000.); plt.xscale('log'); plt.ylim(0,15.0); plt.legend()
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linestyle=':', linewidth=0.5); plt.legend()  

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/cebb95f748df4c664c9a127975a7c87b.png)

æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œéšç€ lambda å‚æ•°çš„å¢åŠ ï¼Œè§£é‡Šçš„æ–¹å·®å‡å°‘ï¼Œå‡æ–¹è¯¯å·®å¢åŠ ã€‚

+   è¿™æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºæ•°æ®æœ‰ä¸€ä¸ªä¸€è‡´çš„çº¿æ€§è¶‹åŠ¿ï¼Œå¹¶ä¸”éšç€æ–œç‡â€˜ç¼©å°â€™åˆ°é›¶ï¼Œè¯¯å·®å¢åŠ ï¼Œè§£é‡Šçš„æ–¹å·®å‡å°‘

+   å¯èƒ½è¿˜æœ‰å…¶ä»–æƒ…å†µï¼Œå…¶ä¸­å‡å°‘çš„æ–œç‡å®é™…ä¸Šåœ¨æµ‹è¯•ä¸­è¡¨ç°æ›´å¥½ã€‚ä¾‹å¦‚ï¼Œå¯¹äºç¨€ç–å’Œæœ‰å™ªå£°çš„æ•°æ®ã€‚

## æ¨¡å‹æ–¹å·®

ç°åœ¨è®©æˆ‘ä»¬æ¢è®¨æ¨¡å‹æ–¹å·®çš„æ¦‚å¿µï¼Œè¿™æ˜¯æµ‹è¯•ä¸­æœºå™¨å­¦ä¹ å‡†ç¡®æ€§çš„é‡è¦éƒ¨åˆ†ã€‚

+   æ¨¡å‹å¯¹ç‰¹å®šè®­ç»ƒæ•°æ®çš„æ•æ„Ÿæ€§

+   éšç€$\lambda$çš„å¢åŠ ï¼Œå¯¹è®­ç»ƒæ•°æ®çš„æ•æ„Ÿæ€§å¢åŠ ï¼Œæ¨¡å‹æ–¹å·®å‡å°‘

è®©æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹å·¥ä½œæµç¨‹æ¥æ¼”ç¤ºè¿™ä¸€ç‚¹ï¼š

+   å¾ªç¯éå†å¤šä¸ª lambda å€¼

    +   å¾ªç¯éå†æ•°æ®çš„å¤šé‡è‡ªåŠ©æ ·æœ¬

        +   è®¡ç®—å²­å›å½’æ‹Ÿåˆï¼ˆæ–œç‡ï¼‰

    +   è®¡ç®—è¿™äº›è‡ªåŠ©ç»“æœçš„æ–¹å·®

è­¦å‘Šï¼šè¿™å°†éœ€è¦å‡ åˆ†é’Ÿæ‰èƒ½è¿è¡Œ

```py
L = 200                                                       # the number of bootstrap realizations 

nsamples = 20                                                 # the number of samples in each bootstrap realization
nlambda = 100                                                 # number of lambda values to evaluate

coef_mat = np.zeros(L)                                        # declare arrays to store the results
variance_coef = np.zeros(nlambda)

#lamd_mat = np.linspace(0.0,100.0,nlambda) 
lambda_mat = np.logspace(-2,5,nlambda)
for ilam in range(0,len(lambda_mat)):                         # loop over all lambda values 
    for l in range(0, L):                                     # loop over all bootstrap realizations
        df_sample = df.sample(n = nsamples)                   # random sample (1 bootstrap)
        ridge_reg = Ridge(alpha=lambda_mat[ilam])             # instantiate model
        ridge_reg.fit(df_sample[xname].values.reshape(nsamples,1), df_sample[yname]) # fit model
        coef_mat[l] = ridge_reg.coef_[0]                      # get the slope parameter
    variance_coef[ilam] = np.var(coef_mat)                    # calculate the variance of the slopes over the L bootstraps

plt.subplot(111)
plt.plot(lambda_mat, variance_coef,  color='black', linewidth = 3, label = 'Slope Variance')
plt.title('Model Variance vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Model Variance - Variance of the Slope Parameter, $b_1$')
plt.xlim(0.01,100000.); plt.ylim(0.0,5.0); plt.xscale('log')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2)
plt.grid(which='both')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/42462583008a06affc490158f6f84d94.png)

ç»“æœæ­£å¦‚é¢„æœŸï¼Œéšç€$\lambda$è¶…å‚æ•°çš„å¢åŠ ï¼Œæ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„æ•æ„Ÿæ€§é™ä½ã€‚

## k æŠ˜äº¤å‰éªŒè¯

è¿›è¡Œå®Œæ•´çš„ k æŠ˜éªŒè¯ä»¥è¯„ä¼°æµ‹è¯•è¯¯å·®ä¸è¶…å‚æ•° lambda ä¹‹é—´çš„æ¯”è¾ƒï¼Œä»¥è¯„ä¼°æ¨¡å‹è°ƒä¼˜æ˜¯æœ‰ç”¨çš„ã€‚

+   ä»¥ä¸‹ä»£ç æä¾›ç”¨äºæ­¤ç›®çš„

+   å†æ¬¡å¼ºè°ƒï¼Œå¯¹äºå•ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œæˆ‘ä»¬å¿…é¡»å°†å…¶é‡å¡‘ä¸ºäºŒç»´æ•°ç»„

æˆ‘ä»¬å¾ªç¯éå†ä» 0.01 åˆ° 100,000 çš„ 100 ä¸ª$\lambda$å€¼ï¼Œ

+   ä¸º 4 ä¸ª k æŠ˜ä¸­çš„æ¯ä¸ªè·å–è´Ÿå‡æ–¹è¯¯å·®

+   ç„¶åæˆ‘ä»¬å–å¹³å‡å€¼å¹¶åº”ç”¨ç»å¯¹å€¼

ä¸ºä»€ä¹ˆä½¿ç”¨è´Ÿå‡æ–¹è¯¯å·®ï¼Ÿç®€å•æ¥è¯´ï¼Œä¸ºäº†ä½¿ç”¨ scikit-learn ä¸­çš„ä¼˜åŒ–åŠŸèƒ½ï¼Œè¯¥åŠŸèƒ½é€šè¿‡æœ€å¤§åŒ–è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ä¸”ä¸å…¶ä»–è¯„åˆ†ï¼ˆå¦‚$rÂ²$ï¼‰ä¿æŒä¸€è‡´æ€§ï¼Œå…¶ä¸­è¾ƒå¤§çš„å€¼æ›´å¥½ã€‚

+   æˆ‘è§‰å¾—è´Ÿå‡æ–¹è¯¯å·®å¾ˆä»¤äººå›°æƒ‘ï¼Œæ‰€ä»¥åœ¨ç»˜å›¾æ—¶ï¼Œæˆ‘ä½¿ç”¨ç»å¯¹å€¼å°†å€¼è½¬æ¢ä¸ºä¸¥æ ¼æ­£å€¼ã€‚

```py
score = []                                                    # code modified from StackOverFlow by Dimosthenis

nlambda = 100
lambda_mat = np.logspace(-2,5,nlambda)
for ilam in range(0,nlambda):
    ridge_reg = Ridge(alpha=lambda_mat[ilam])
    scores = cross_val_score(estimator=ridge_reg, X= df['Density'].values.reshape(-1, 1), y=df['Porosity'].values, cv=10, n_jobs=4, scoring = "neg_mean_squared_error") # Perform 10-fold cross validation
    score.append(abs(scores.mean()))

plt.subplot(111)
plt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)
plt.title('Ridge Regression Test Mean Square Error vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')
plt.xlim(1.0e-2,1.0e5); plt.ylim(0.001,20.0); plt.xscale('log')
plt.vlines(0.5,0,20,color='red',lw=2); plt.vlines(1000,0,20,color='red',lw=2,zorder=10)
plt.annotate('Linear Regression',[0.4,12.5],rotation=90.0,color='red',zorder=10)
plt.annotate('Mean',[1100,14.5],rotation=90.0,color='red',zorder=10)
plt.fill_between([0.01,0.5],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.fill_between([1000,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.grid(which='both')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/2e65418d331d427736b40372288b8cec15d2e1bd38e995d14e8e51014c0e10d7.png](img/8eb67ef97a23db1eb0ad78518b9e99c2.png)

ä»ä¸Šè¿°å†…å®¹ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ° 3 ä¸ªé˜¶æ®µï¼Œ

+   å·¦ä¾§ - æµ‹è¯•å‡æ–¹è¯¯å·®è¾¾åˆ°æœ€å°å€¼ï¼Œæ¨¡å‹æ”¶æ•›åˆ°çº¿æ€§å›å½’ã€‚

+   å³ä¾§ - æµ‹è¯•å‡æ–¹è¯¯å·®è¾¾åˆ°æœ€å¤§å€¼ï¼Œæ¨¡å‹æ”¶æ•›åˆ°é¢„æµ‹ç‰¹å¾å‡å€¼ï¼Œå³æ¨¡å‹å‚æ•°æ–œç‡ä¸º 0.0ã€‚

+   ä¸­å¿ƒ - ä»‹äºä¸¤ç§æƒ…å†µä¹‹é—´

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çœ‹åˆ°çº¿æ€§å›å½’æ¨¡å‹ï¼ˆ$\lambda = 0.0$ï¼‰æ˜¯æœ€ä½³æ¨¡å‹ï¼å¦‚æœå²­å›å½’æ˜¯æœ€ä½³æ¨¡å‹ï¼Œåˆ™æµ‹è¯•å‡æ–¹è¯¯å·®å°†åœ¨çº¿æ€§å›å½’å’Œå‡å€¼ä¹‹é—´æœ€å°åŒ–ã€‚

+   è¿™é€šå¸¸å‘ç”Ÿåœ¨æœ‰å™ªå£°ã€æ•°æ®ç¨€ç¼ºå’Œé«˜ç»´æ€§é—®é¢˜çš„æ•°æ®é›†ä¸­ã€‚

æˆ‘ä»¬çš„æ¨¡å¼ä¸çº¿æ€§å›å½’ç›¸åŒã€‚

+   æˆ‘ä»¬èƒ½å¦åˆ›é€ ä¸€ä¸ªæœ€ä½³æ¨¡å‹ä¸æ˜¯çº¿æ€§å›å½’çš„æƒ…å†µï¼Ÿå³æ­£åˆ™åŒ–æœ‰å¸®åŠ©çš„æƒ…å†µï¼Ÿ

+   æ˜¯çš„ï¼Œæˆ‘ä»¬å¯ä»¥ã€‚è®©æˆ‘ä»¬åˆ é™¤å¤§éƒ¨åˆ†æ ·æœ¬ä»¥åˆ›å»ºæ•°æ®ç¨€ç¼ºæ€§ï¼Œå¹¶æ·»åŠ å¤§é‡å™ªå£°ï¼

è¯šç„¶ï¼Œæˆ‘è¿­ä»£äº†æ ·æœ¬å’Œå™ªå£°çš„éšæœºç§å­ä»¥å¾—åˆ°è¿™ä¸ªç»“æœã€‚

+   å°‘é‡æ•°æ®ï¼ˆä½$n$ï¼‰å’Œé«˜ç»´æ€§ï¼ˆé«˜$m$ï¼‰é€šå¸¸ä¼šå¯¼è‡´ LASSO ä¼˜äºçº¿æ€§å›å½’

```py
df_sample = df.copy(deep=True).sample(n=10,random_state=11)
noise_stdev = 3.0
np.random.seed(seed=15)
df_sample['Porosity'] = df_sample['Porosity'] + np.random.normal(0.0, noise_stdev, size=len(df_sample))

score = []                                                    # code modified from StackOverFlow by Dimosthenis

nlambda = 100
lambda_mat = np.logspace(-4,3,nlambda)
for ilam in range(0,nlambda):
    ridge_reg = Ridge(alpha=lambda_mat[ilam])
    scores = cross_val_score(estimator=ridge_reg, X= df_sample['Density'].values.reshape(-1, 1), 
                             y=df_sample['Porosity'].values, cv=2, n_jobs=4, scoring = "neg_mean_squared_error") # Perform 10-fold cross validation
    score.append(abs(scores.mean()))

plt.subplot(111)
plt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)
plt.title('Ridge Regression Test Mean Square Error vs. Ridge Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')
plt.xlim(1.0e-4,1.0e3); plt.ylim(0.001,20.0); 
plt.xscale('log')
plt.vlines(0.001,0,20,color='red',lw=2); plt.vlines(10,0,20,color='red',lw=2,zorder=10); 
plt.vlines(0.07,0,20,color='red',lw=2,zorder=10);
plt.annotate('Linear Regression',[0.0007,12.5],rotation=90.0,color='red',zorder=10)
plt.annotate(r'Ridge Tuned $\lambda$',[0.055,12.5],rotation=90.0,color='red',zorder=10)
plt.annotate('Mean',[7.4,14.5],rotation=90.0,color='red',zorder=10)
plt.fill_between([0.0001,0.001],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.fill_between([10,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.grid(which='both')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/ac7782f4fe7198210e3ac58ca9cbea5c3c4e0c30335c332fac08ce3ae63077a9.png](img/b011c5d0305db3ebe120ee6826faed3c.png)

## è¯„è®º

è¿™æ˜¯å¯¹å²­å›å½’çš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´çš„ YouTube è®²åº§é“¾æ¥ï¼Œè§†é¢‘æè¿°ä¸­åŒ…å«èµ„æºé“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œä»–åœ¨é‚£é‡Œç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢çš„æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚

+   [ã€Šè®¡ç®—æœºä¸åœ°çƒç§‘å­¦ã€‹](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š [ã€Šæ•°å­¦åœ°çƒç§‘å­¦ã€‹](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²æ’°å†™è¶…è¿‡ 70 ç¯‡ [åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„ [Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦ [ã€Šåœ°çƒç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡ã€‹](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œåˆ†åˆ«æ˜¯ [ã€ŠPython ä¸­çš„åº”ç”¨åœ°çƒç»Ÿè®¡å­¦ï¼šGeostatsPy å®æˆ˜æŒ‡å—ã€‹](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) å’Œ [ã€ŠPython ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„å®æˆ˜æŒ‡å—ã€‹](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è¯¾ç¨‹éƒ½å¯åœ¨ä»–çš„ [YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å«é“¾æ¥åˆ° 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œåœ¨ä»– [GitHub è´¦æˆ·](https://github.com/GeostatsGuy)ä¸Šè¶…è¿‡ 40 ä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†æ–‡æ¡£å·¥ä½œæµç¨‹ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›å¸¸é’å†…å®¹ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³è¦ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œå­¦ä¹ æœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æƒ³è¦åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢

æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥è·å–ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## å²­å›å½’çš„åŠ¨æœº

è¿™é‡Œæœ‰ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’å’Œå²­å›å½’æ¼”ç¤ºçš„å·¥ä½œæµç¨‹ï¼Œä»¥åŠä¸åŸºäºæœºå™¨å­¦ä¹ çš„é¢„æµ‹çš„çº¿æ€§å›å½’çš„æ¯”è¾ƒã€‚ä¸ºä»€ä¹ˆä»çº¿æ€§å›å½’å¼€å§‹ï¼Ÿ

+   çº¿æ€§å›å½’æ˜¯æœ€ç®€å•çš„å‚æ•°åŒ–é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹

+   æˆ‘ä»¬é€šè¿‡ä»è®­ç»ƒ MSE çš„å¯¼æ•°è®¡ç®—å‡ºçš„è§£æè§£æ¥å­¦ä¹ è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹

+   è®©æˆ‘ä»¬å¼€å§‹äº†è§£æŸå¤±å‡½æ•°å’ŒèŒƒæ•°çš„æ¦‚å¿µ

+   æˆ‘ä»¬å¯ä»¥è®¿é—®æ¨¡å‹ä¸ç¡®å®šæ€§çš„ç½®ä¿¡åŒºé—´å’Œå‚æ•°æ˜¾è‘—æ€§çš„å‡è®¾æ£€éªŒçš„è§£æè¡¨è¾¾å¼

ä¸ºä»€ä¹ˆæ¥ä¸‹æ¥è¦ä»‹ç»å²­å›å½’ï¼Ÿ

+   æœ‰æ—¶çº¿æ€§å›å½’å¹¶ä¸è¶³å¤Ÿç®€å•ï¼Œæˆ‘ä»¬å®é™…ä¸Šéœ€è¦ä¸€ä¸ªæ›´ç®€å•çš„æ¨¡å‹ï¼

+   ä»‹ç»æ¨¡å‹æ­£åˆ™åŒ–å’Œè¶…å‚æ•°è°ƒæ•´çš„æ¦‚å¿µ

è¿™é‡Œæœ‰ä¸€äº›å…³äºé¢„æµ‹æœºå™¨å­¦ä¹ å²­å›å½’æ¨¡å‹çš„åŸºæœ¬ç»†èŠ‚ï¼Œè®©æˆ‘ä»¬å…ˆä»çº¿æ€§å›å½’å¼€å§‹ï¼Œç„¶åè¿‡æ¸¡åˆ°å²­å›å½’ï¼š

## çº¿æ€§å›å½’

é¢„æµ‹çš„çº¿æ€§å›å½’ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹ä¸€ç»„æ•°æ®æ‹Ÿåˆçš„çº¿æ€§æ¨¡å‹ã€‚

![](img/d82dccdcda485413554e49d73e4d1fc8.png)

ä¸¾ä¾‹è¯´æ˜çº¿æ€§å›å½’æ¨¡å‹ã€‚

è®©æˆ‘ä»¬å…ˆå®šä¹‰ä¸€äº›æœ¯è¯­ï¼Œ

+   **é¢„æµ‹ç‰¹å¾** - é¢„æµ‹æ¨¡å‹çš„è¾“å…¥ç‰¹å¾ï¼Œé‰´äºæˆ‘ä»¬åªè®¨è®ºçº¿æ€§å›å½’è€Œä¸è®¨è®ºå¤šå…ƒçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œ$x$ã€‚åœ¨æˆ‘ä»¬çš„å›¾è¡¨ï¼ˆåŒ…æ‹¬ä¸Šé¢çš„ï¼‰ä¸­ï¼Œé¢„æµ‹ç‰¹å¾ä½äº x è½´ä¸Šã€‚

+   **å“åº”ç‰¹å¾** - é¢„æµ‹æ¨¡å‹çš„è¾“å‡ºç‰¹å¾ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ$y$ã€‚åœ¨æˆ‘ä»¬çš„å›¾è¡¨ï¼ˆåŒ…æ‹¬ä¸Šé¢çš„ï¼‰ä¸­ï¼Œå“åº”ç‰¹å¾ä½äº y è½´ä¸Šã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹çº¿æ€§å›å½’çš„ä¸€äº›å…³é”®æ–¹é¢ï¼š

**å‚æ•°æ¨¡å‹**

è¿™æ˜¯ä¸€ä¸ªå‚æ•°åŒ–çš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬æ¥å—ä¸€ä¸ªå…ˆéªŒçš„çº¿æ€§å‡è®¾ï¼Œç„¶åè·å¾—ä¸€ä¸ªéå¸¸ä½å‚æ•°åŒ–çš„è¡¨ç¤ºï¼Œè¿™ä½¿å¾—åœ¨æ²¡æœ‰å¤§é‡æ•°æ®çš„æƒ…å†µä¸‹å®¹æ˜“è®­ç»ƒã€‚

+   é€‚é…æ¨¡å‹æ˜¯ä¸€ä¸ªåŸºäºæ‰€æœ‰å¯ç”¨ç‰¹å¾ï¼ˆ$x_1,\ldots,x_m$ï¼‰çš„ç®€å•åŠ æƒçº¿æ€§åŠ æ€§æ¨¡å‹ã€‚

+   å‚æ•°æ¨¡å‹çš„å½¢å¼ä¸ºï¼š

$$ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 $$

è¿™é‡Œæ˜¯çº¿æ€§æ¨¡å‹å‚æ•°çš„å¯è§†åŒ–ï¼Œ

![](img/175e41e10e74a46b4a56258ccdfb94c0.png)

çº¿æ€§æ¨¡å‹å‚æ•°ã€‚

**æœ€å°äºŒä¹˜æ³•**

å¯¹äº L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼Œæ¨¡å‹å‚æ•° $b_1,\ldots,b_m,b_0$ çš„è§£æè§£æ˜¯å¯ç”¨çš„ï¼Œè¯¯å·®æ˜¯æ€»å’Œå¹¶å¹³æ–¹ï¼Œå·²çŸ¥ä¸ºæœ€å°äºŒä¹˜æ³•ã€‚

+   æˆ‘ä»¬æœ€å°åŒ–è®­ç»ƒæ•°æ®ä¸Šçš„è¯¯å·®ï¼Œæ®‹å·®å¹³æ–¹å’Œ (RSS)ï¼š

$$ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)Â² $$

å…¶ä¸­ $y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾å€¼ï¼Œ$\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ æ˜¯æ¨¡å‹é¢„æµ‹ï¼Œåœ¨ $\alpha = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šã€‚

è¿™æ˜¯å¯¹ L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®çš„è§†è§‰è¡¨ç¤ºï¼Œ

![å›¾ç‰‡](img/b25cdf17fb10f18b362f50ba655df92b.png)

çº¿æ€§æ¨¡å‹æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®ã€‚

+   è¿™å¯ä»¥ç®€åŒ–ä¸ºè®­ç»ƒæ•°æ®ä¸Šçš„å¹³æ–¹è¯¯å·®ä¹‹å’Œï¼Œ

\begin{equation} \sum_{i=1}^n (\Delta y_i)Â² \end{equation}

å…¶ä¸­ $\Delta y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾è§‚å¯Ÿ $y_i$ å‡å»æ¨¡å‹é¢„æµ‹ $\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ï¼Œåœ¨ $i = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šã€‚

**å‡è®¾**

æˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹æœ‰ä¸€äº›é‡è¦çš„å‡è®¾ï¼Œ

+   **æ— è¯¯å·®** - é¢„æµ‹å˜é‡æ— è¯¯å·®ï¼Œä¸æ˜¯éšæœºå˜é‡

+   **çº¿æ€§** - å“åº”æ˜¯ç‰¹å¾ï¼ˆçš„ï¼‰çº¿æ€§ç»„åˆ

+   **æ’å®šæ–¹å·®** - å“åº”è¯¯å·®åœ¨é¢„æµ‹å˜é‡å€¼ä¸Šæ’å®š

+   **è¯¯å·®ç‹¬ç«‹æ€§** - å“åº”è¯¯å·®ä¹‹é—´ç›¸äº’ä¸ç›¸å…³

+   **æ— å¤šé‡å…±çº¿æ€§** - æ²¡æœ‰ç‰¹å¾ä¸å…¶ä»–ç‰¹å¾å†—ä½™

## å²­å›å½’

åœ¨å²­å›å½’ä¸­ï¼Œæˆ‘ä»¬å‘æœ€å°åŒ–è¿‡ç¨‹ä¸­æ·»åŠ ä¸€ä¸ªè¶…å‚æ•° $\lambda$ï¼Œä»¥åŠä¸€ä¸ªæ”¶ç¼©æƒ©ç½šé¡¹ï¼Œ$\sum_{j=1}^m b_{\alpha}Â²$ã€‚

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)Â² + \lambda \sum_{j=1}^m b_{\alpha}Â² $$

å› æ­¤ï¼Œå²­å›å½’è®­ç»ƒå°†ä¸¤ä¸ªç»å¸¸ç›¸äº’ç«äº‰çš„ç›®æ ‡æ•´åˆåœ¨ä¸€èµ·ï¼Œä»¥æ‰¾åˆ°æ¨¡å‹å‚æ•°ï¼Œ

+   æ‰¾åˆ°æ¨¡å‹å‚æ•°ï¼Œä»¥æœ€å°åŒ–è®­ç»ƒæ•°æ®ä¸­çš„è¯¯å·®

+   æœ€å°åŒ–æ–œç‡å‚æ•°è¶‹å‘äºé›¶

æ³¨æ„ï¼š$\lambda$ ä¸åŒ…æ‹¬æˆªè·ï¼Œ$b_0$ã€‚

$\lambda$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œå®ƒæ§åˆ¶æ¨¡å‹çš„æ‹Ÿåˆç¨‹åº¦ï¼Œå¯èƒ½ä¸æ¨¡å‹çš„åå·®-æ–¹å·®æƒè¡¡æœ‰å…³ã€‚

![å›¾ç‰‡](img/2a291289b4f4b55ba9adbb7538140a67.png)

å²­å›å½’çš„æµ‹è¯•è¯¯å·®åˆ†é‡å’Œé¢„æœŸæµ‹è¯•å¹³æ–¹è¯¯å·®ã€‚

+   å½“ $\lambda \rightarrow 0$ æ—¶ï¼Œè§£è¶‹è¿‘äºçº¿æ€§å›å½’ï¼Œæ²¡æœ‰åå·®ï¼ˆç›¸å¯¹äºçº¿æ€§æ¨¡å‹æ‹Ÿåˆï¼‰ï¼Œä½†æ¨¡å‹æ–¹å·®å¯èƒ½æ›´é«˜

+   éšç€ $\lambda$ çš„å¢åŠ ï¼Œæ¨¡å‹æ–¹å·®å‡å°ï¼Œæ¨¡å‹åå·®å¢åŠ ï¼Œæ¨¡å‹å˜å¾—ç®€å•

+   å½“ $\lambda \rightarrow \infty$ æ—¶ï¼Œæ¨¡å‹å‚æ•° $b_1,\ldots,b_m$ æ”¶ç¼©åˆ° 0.0ï¼Œæ¨¡å‹é¢„æµ‹è¶‹è¿‘äºè®­ç»ƒæ•°æ®å“åº”ç‰¹å¾å‡å€¼

ä¸‹é¢å°†å±•ç¤ºå„ç§ $\lambda$ å€¼çš„å²­å›å½’æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å°†åœ¨ä¸‹é¢çš„å·¥ä½œæµç¨‹ä¸­è®¡ç®—ï¼Œ

![å›¾ç‰‡](img/4f26df3583b9f434159b8bbb15cc292b.png)

å…·æœ‰ä»ä½åˆ°é«˜ $\lambda$ è¶…å‚æ•°å€¼çš„å²­å›å½’æ¨¡å‹ã€‚

## åŸºäºäº¤å‰éªŒè¯çš„è¶…å‚æ•°è°ƒæ•´çš„è®­ç»ƒ/æµ‹è¯•æ•°æ®åˆ†å‰²

å¯ç”¨æ•°æ®è¢«åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

+   é€šå¸¸ï¼Œ15-30%çš„æ•°æ®è¢«ä¿ç•™ä½œä¸ºæµ‹è¯•æ•°æ®

+   æµ‹è¯•æ•°æ®çš„é€‰æ‹©åº”è¯¥æ˜¯å…¬å¹³çš„ï¼Œé¢„æµ‹éš¾åº¦ç›¸åŒï¼ˆä¸è®­ç»ƒæ•°æ®åç§»/ä¸åŒï¼‰[å…¬å¹³çš„è®­ç»ƒ-æµ‹è¯•åˆ†å‰²è®ºæ–‡](https://www.sciencedirect.com/science/article/pii/S0920410521015023)ï¼ˆSalazar ç­‰äººï¼Œ2022 å¹´ï¼‰ã€‚

+   è¿˜æœ‰å…¶ä»–å„ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•åˆ†å‰²ï¼Œä»¥åŠ k æŠ˜äº¤å‰éªŒè¯

åŸºæœ¬ä¸Šï¼Œæ‰€æœ‰è¿™äº›æ–¹æ³•éƒ½æ˜¯é€šè¿‡ä½¿ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨ä¿ç•™çš„æµ‹è¯•æ•°æ®ä¸Šæµ‹è¯•æ¨¡å‹å‡†ç¡®ç‡æ¥è¿›è¡Œçš„ï¼Œå¯¹äºä¸€ç³»åˆ—è¶…å‚æ•°ã€‚ç„¶åé€‰æ‹©æœ€å°åŒ–ä¿ç•™æµ‹è¯•æ•°æ®é›†é”™è¯¯çš„è¶…å‚æ•°ï¼Œè¿™å°±æ˜¯è¶…å‚æ•°è°ƒæ•´ã€‚

+   è¶…å‚æ•°è°ƒæ•´åï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®ä»¥åŠé€‰å®šçš„è¶…å‚æ•°é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚

+   ç„¶åï¼Œè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ–¹æ³•ä½¿ç”¨æ•°æ®é›†çš„ç¬¬äºŒä¸ªä¿ç•™å­é›†æ¥æ£€æŸ¥è°ƒæ•´åçš„æ¨¡å‹ï¼Œè¿™äº›æ•°æ®æ—¢æœªç”¨äºæ¨¡å‹è®­ç»ƒä¹Ÿæœªç”¨äºæ¨¡å‹è°ƒæ•´ã€‚

åœ¨ä»¥ä¸‹å·¥ä½œæµç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è®­ç»ƒå’Œæµ‹è¯•æ–¹æ³•è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ã€‚ä»¥ä¸‹æ˜¯æ›´å¤šç»†èŠ‚å’Œç›¸å…³æ¦‚å¿µçš„æ€»ç»“ã€‚

## æ¨¡å‹å‚æ•°è®­ç»ƒ

è®­ç»ƒæ•°æ®è¢«ç”¨äºè®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œä½¿å¾—æ¨¡å‹æœ€å°åŒ–ä¸è®­ç»ƒæ•°æ®çš„ä¸åŒ¹é…

+   é€šå¸¸ä½¿ç”¨**å‡æ–¹è¯¯å·®**ï¼ˆä¹Ÿç§°ä¸º**L2 èŒƒæ•°**ï¼‰ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œæ€»ç»“æ¨¡å‹ä¸åŒ¹é…

+   **æœ€å°åŒ–æŸå¤±å‡½æ•°**å¯¹äºç®€å•æ¨¡å‹ï¼Œå¯èƒ½å­˜åœ¨è§£æè§£ï¼Œä½†å¯¹äºå¤§å¤šæ•°æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œè¿™éœ€è¦è¿­ä»£ä¼˜åŒ–æ–¹æ³•æ¥æ‰¾åˆ°æœ€ä½³æ¨¡å‹å‚æ•°

è¿™é‡Œæ˜¯è§£æè§£çš„æ¨å¯¼è¿‡ç¨‹ï¼Œä»åŒ…å«æœ€å°äºŒä¹˜ï¼ˆå·¦ï¼‰å’Œæ­£åˆ™åŒ–ï¼ˆå³ï¼‰é¡¹çš„æŸå¤±å‡½æ•°å¼€å§‹ï¼Œ

$$ \left( \text{RSS} + \text{shrinkage} \right) = \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)Â² + \lambda \sum_{j=1}^m b_{\alpha}Â² $$

è®©æˆ‘ä»¬å°†æ­¤è½¬æ¢ä¸ºçŸ©é˜µè¡¨ç¤ºä»¥æ–¹ä¾¿èµ·è§ï¼Œ

$$ \left( \text{RSS} + \text{shrinkage} \right) = (ğ‘¦âˆ’ğ‘‹\beta)^ğ‘‡ (ğ‘¦âˆ’ğ‘‹\beta)+\lambda \beta^ğ‘‡ \beta $$

å…¶ä¸­ $\beta$ æ˜¯æ¨¡å‹å‚æ•°çš„å‘é‡ï¼Œ$y$ æ˜¯å“åº”ç‰¹å¾å€¼çš„å‘é‡ï¼Œ$X$ æ˜¯é¢„æµ‹ç‰¹å¾å€¼çš„çŸ©é˜µï¼Œè¿™äº›éƒ½æ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸Šã€‚

æˆ‘ä»¬å¯ä»¥å±•å¼€äºŒæ¬¡é¡¹ï¼Œ

$$ \left( \text{RSS} + \text{shrinkage} \right) = ğ‘¦âˆ’2ğ‘‹^ğ‘‡ ğ‘¦\beta+(ğ‘‹^ğ‘‡ ğ‘‹) \beta^ğ‘‡ \beta+\lambda \beta^ğ‘‡ \beta $$

æˆ‘ä»¬å¯¹æ¨¡å‹å‚æ•°æ±‚åå¯¼æ•°ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸º 0.0ï¼Œ

$$ \frac{\partial}{\partial \beta} \left( \text{RSS} + \text{shrinkage} \right) = -2X^T y + 2(X^T X) \beta + 2\lambda \beta = 0 $$

ç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€äº›å°çš„è°ƒæ•´è¿›ä¸€æ­¥ç®€åŒ–ã€‚

$$ 2(ğ‘‹^ğ‘‡ ğ‘‹) \beta + 2 \lambda \beta = 2ğ‘‹^ğ‘‡ ğ‘¦ $$

ä¸¤è¾¹åŒæ—¶é™¤ä»¥ 2ï¼Œ

$$ (ğ‘‹^ğ‘‡ ğ‘‹) \beta + \lambda \beta = ğ‘‹^ğ‘‡ ğ‘¦ $$

åˆå¹¶å…¬å…±é¡¹ï¼Œ

$$ (ğ‘‹^ğ‘‡ ğ‘‹+ \lambda I) \beta =ğ‘‹^ğ‘‡ ğ‘¦ $$

å…¶ä¸­ $I$ æ˜¯ä¸€ä¸ªå•ä½çŸ©é˜µã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥æ±‚è§£å²­å›å½’å‚æ•°ï¼Œ

$$ ğ›½=\left(ğ‘‹^ğ‘‡ ğ‘‹+\lambda I \right)^{-1} ğ‘‹^ğ‘‡ ğ‘¦ $$

æ³¨æ„ï¼Œ$ğ‘‹^ğ‘‡ ğ‘‹+\lambda I$ é€šå¸¸æ˜¯å¯ä»¥é€†çš„ï¼Œæ‰€ä»¥è¿™æ˜¯å¯è§£çš„ã€‚

è¿™ä¸ªè¿‡ç¨‹å°†åœ¨ç”±è¶…å‚æ•°æŒ‡å®šçš„æ¨¡å‹å¤æ‚åº¦èŒƒå›´å†…é‡å¤è¿›è¡Œã€‚

## æ­£åˆ™åŒ–çš„è§£é‡Š

å¦ä¸€ç§å²­å›å½’çš„è§£é‡Šå’ŒåŠ¨æœºï¼Œ

+   å½“ $ğ‘š \ge ğ‘›$ æ—¶ï¼Œçº¿æ€§å›å½’æ˜¯ç—…æ€é—®é¢˜ï¼Œå³å…·æœ‰å¤šä¸ªè§£çš„é—®é¢˜ã€‚

+   å¯¹äºç—…æ€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€äº›çº¦æŸæˆ–æ­£åˆ™åŒ–æ¥é™åˆ¶è§£çš„èŒƒå›´ã€‚

![](img/3b29636566b2ebc1625773201be9a303.png)

ç—…æ€çº¿æ€§å›å½’æ¨¡å‹ï¼ˆå·¦ï¼‰ï¼Œä»¥åŠå…·æœ‰é¢å¤–æ”¶ç¼©çº¦æŸçš„è‰¯æ€å²­å›å½’ï¼ˆå³ï¼‰ã€‚

## æ¨¡å‹è¶…å‚æ•°è°ƒæ•´

ä¿ç•™çš„æµ‹è¯•æ•°æ®è¢«æ£€ç´¢å‡ºæ¥ï¼Œå¹¶è®¡ç®—æŸå¤±å‡½æ•°ï¼ˆé€šå¸¸æ˜¯**L2 èŒƒæ•°**ï¼‰æ¥æ€»ç»“æµ‹è¯•æ•°æ®ä¸­çš„é”™è¯¯ã€‚

+   è¿™å°†åœ¨æŒ‡å®šçš„è¶…å‚æ•°èŒƒå›´å†…é‡å¤è¿›è¡Œã€‚

+   é€‰æ‹©æ¨¡å‹å¤æ‚åº¦/è¶…å‚æ•°ï¼Œä»¥æœ€å°åŒ–æµ‹è¯•ä¸­çš„æŸå¤±å‡½æ•°/é”™è¯¯æ‘˜è¦ã€‚

è¿™å°±æ˜¯æ‰€è°“çš„æ¨¡å‹è¶…å‚æ•°è°ƒæ•´ã€‚

## æ¨¡å‹è¿‡æ‹Ÿåˆ

æ¨¡å‹å¤æ‚åº¦/çµæ´»æ€§æ¯”å¯ç”¨æ•°æ®ã€æ•°æ®å‡†ç¡®æ€§ã€é¢‘ç‡å’Œè¦†ç›–èŒƒå›´æ‰€èƒ½è¯æ˜çš„æ›´å¤šã€‚

+   æ¨¡å‹è§£é‡Šæ•°æ®çš„â€œç‰¹æ®Šæ€§â€ï¼Œåœ¨æ¨¡å‹ä¸­æ•æ‰æ•°æ®å™ªå£°/é”™è¯¯ã€‚

+   è®­ç»ƒä¸­çš„é«˜ç²¾åº¦ï¼Œä½†åœ¨æµ‹è¯•/å®é™…åº”ç”¨ä¸­çš„ä½ç²¾åº¦ï¼ˆè¿œç¦»è®­ç»ƒæ•°æ®æ¡ˆä¾‹ï¼‰- æ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®

å¦‚æœæµ‹è¯•ä¸­çš„æ¨¡å‹è¯¯å·®åœ¨å¢åŠ ï¼Œè€Œè®­ç»ƒä¸­çš„æ¨¡å‹è¯¯å·®åœ¨å‡å°‘ï¼Œè¿™æ˜¯è¿‡åº¦æ‹Ÿåˆæ¨¡å‹çš„æŒ‡æ ‡ã€‚

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = False
import os                                                     # to set current working directory 
import os                                                     # to set current working directory 
import numpy as np                                            # ndarrays and matrix math
import scipy.stats as st                                      # statistical methods
import pandas as pd                                           # DataFrames
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks
from sklearn.metrics import mean_squared_error, r2_score      # specific measures to check our models
from sklearn import linear_model                              # linear regression
from sklearn.linear_model import Ridge                        # ridge regression implemented in scikit learn
from sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation
from sklearn.model_selection import train_test_split          # train and test split
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # supress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
```

å¦‚æœä½ é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½éœ€è¦é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ï¼Œç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æ›´å¤šå¸®åŠ©å¯ä»¥åœ¨ç›¸åº”çš„åŒ…æ–‡æ¡£ä¸­æ‰¾åˆ°ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä»¥ä¾¿å°†æŒ‡å®šçš„ç™¾åˆ†ä½æ•°å’Œä¸»æ¬¡ç½‘æ ¼çº¿æ·»åŠ åˆ°æˆ‘ä»¬çš„å›¾ä¸­ã€‚

```py
def weighted_percentile(data, weights, perc):                 # calculate weighted percentile 
    ix = np.argsort(data)
    data = data[ix] 
    weights = weights[ix] 
    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) 
    return np.interp(perc, cdf, data)
# Function from iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049

def histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram 
    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)
    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')
    plt.plot([avg,avg],[0.0,45],color = color)
    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)
    html_str = ''
    for df in args:
        html_str += df.head().to_html()  # Using .head() for the first few rows
    display(HTML(f'<div style="display: flex;">{html_str}</div>')) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯·ç¡®ä¿å°†æ‰€éœ€çš„æ•°æ®æ–‡ä»¶ï¼ˆè§ä¸‹æ–‡ï¼‰æ”¾ç½®åœ¨æ­¤å·¥ä½œç›®å½•ä¸­ã€‚

```py
#os.chdir("C:\PGE337")                                        # set the working directory 
```

ä½ å°†ä¸å¾—ä¸æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ï¼Œä»¥åŒ…å«ä½ è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”æ ¼å¼åœ¨ Mac ä¸Šä¸åŒï¼ˆä¾‹å¦‚ï¼Œâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½è¡¨æ ¼æ•°æ®

è¿™æ˜¯å°†æˆ‘ä»¬çš„é€—å·åˆ†éš”æ•°æ®æ–‡ä»¶åŠ è½½åˆ° Pandas DataFrame å¯¹è±¡çš„å‘½ä»¤ã€‚

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›†â€˜unconv_MV.csvâ€™ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…æ‹¬æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼š

+   å¯†åº¦ï¼ˆ$g/cm^{3}$ï¼‰

+   å­”éš™ç‡ï¼ˆä½“ç§¯%ï¼‰

æ³¨æ„ï¼Œæ•°æ®é›†æ˜¯åˆæˆçš„ã€‚

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸ºâ€˜my_dataâ€™çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

```py
add_error = True                                              # add random error to the response feature
std_error = 1.0; seed = 71071

yname = 'Porosity'; xname = 'Density'                         # specify the predictor features (x2) and response feature (x1)
xmin = 1.0; xmax = 2.5                                        # set minimums and maximums for visualization 
ymin = 0.0; ymax = 25.0    
xlabel = 'Porosity'; ylabel = 'Density'                       # specify the feature labels for plotting
yunit = '%'; xunit = '$g/cm^{3}$'    
Xlabelunit = xlabel + ' (' + xunit + ')'
ylabelunit = ylabel + ' (' + yunit + ')'

#df = pd.read_csv("Density_Por_data.csv")                     # load the data from local current directory
df = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv") # load the data from my github repo
df = df.sample(frac=1.0, random_state = 73073); df = df.reset_index() # extract 30% random to reduce the number of data

if add_error == True:                                         # method to add error
    np.random.seed(seed=seed)                                 # set random number seed
    df[yname] = df[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df)) # add noise
    values = df._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray 
```

## è®­ç»ƒ-æµ‹è¯•åˆ†å‰²

ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn åŒ…ä¸­çš„ model_selection æ¨¡å—çš„ train_test_split å‡½æ•°è¿›è¡Œéšæœºè®­ç»ƒ-æµ‹è¯•åˆ†å‰²ã€‚

```py
x_train, x_test, y_train, y_test = train_test_split(df[xname],df[yname],test_size=0.25,random_state=73073) # train and test split
# y_train = pd.DataFrame({yname:y_train.values}); y_test = pd.DataFrame({yname:y_test.values}) # optional to ensure response is a DataFrame

y = df[yname].values.reshape(len(df))                         # features as 1D vectors
x = df[xname].values.reshape(len(df))

df_train = pd.concat([x_train,y_train],axis=1)                # features as train and test DataFrames
df_test = pd.concat([x_test,y_test],axis=1) 
```

## å¯è§†åŒ– DataFrame

å¯è§†åŒ– DataFrame æ˜¯æ•°æ®çš„ç¬¬ä¸€æ­¥æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½å‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨â€˜headâ€™ DataFrame æˆå‘˜å‡½æ•°ï¼ˆæ ¼å¼æ•´æ´ã€ç¾è§‚ï¼Œè§ä¸‹æ–‡ï¼‰æ¥é¢„è§ˆã€‚

+   æˆ‘ä»¬æœ‰ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°æ¥å¹¶æ’é¢„è§ˆè®­ç»ƒå’Œæµ‹è¯• DataFrameã€‚

```py
print('   Training DataFrame      Testing DataFrame')
display_sidebyside(df_train,df_test) 
```

```py
 Training DataFrame      Testing DataFrame 
```

|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| 24 | 1.778580 | 11.426485 |
| 101 | 2.410560 | 8.488544 |
| 88 | 2.216014 | 10.133693 |
| 79 | 1.631896 | 12.712326 |
| 58 | 1.528019 | 16.129542 |
|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| 59 | 1.742534 | 15.380154 |
| 1 | 1.404932 | 13.710628 |
| 35 | 1.552713 | 14.131878 |
| 92 | 1.762359 | 11.154896 |
| 22 | 1.885087 | 9.403056 |

## è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡

åœ¨ DataFrames ä¸­ä»è¡¨æ ¼æ•°æ®è®¡ç®—æ±‡æ€»ç»Ÿè®¡æœ‰å¾ˆå¤šé«˜æ•ˆçš„æ–¹æ³•ã€‚describe å‘½ä»¤ä»¥ä¸€ä¸ªç¾è§‚çš„æ•°æ®è¡¨å½¢å¼æä¾›è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€‚

```py
print('     Training DataFrame         Testing DataFrame')
display_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) 
```

```py
 Training DataFrame         Testing DataFrame 
```

|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| count | 78.000000 | 78.000000 |
| mean | 1.739027 | 12.501465 |
| std | 0.302510 | 3.428260 |
| min | 0.996736 | 3.276449 |
| max | 2.410560 | 21.660179 |
|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| count | 27.000000 | 27.000000 |
| mean | 1.734710 | 12.380796 |
| std | 0.247761 | 2.916045 |
| min | 1.067960 | 7.894595 |
| max | 2.119652 | 18.133771 |

## å¯è§†åŒ–æ•°æ®

è®©æˆ‘ä»¬é€šè¿‡ç›´æ–¹å›¾å’Œæ•£ç‚¹å›¾æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•çš„ä¸€è‡´æ€§å’Œè¦†ç›–èŒƒå›´ã€‚

+   æ£€æŸ¥ä»¥ç¡®ä¿è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®è¦†ç›–äº†å¯èƒ½çš„ç‰¹å¾ç»„åˆèŒƒå›´

+   ç¡®ä¿æµ‹è¯•æ¡ˆä¾‹ä¸ä¼šè¶…å‡ºè®­ç»ƒæ•°æ®çš„å¤–æ¨

```py
nbins = 20                                                    # number of histogram bins

plt.subplot(221)
freq1,_,_ = plt.hist(x=df_train[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=True,label='Train')
freq2,_,_ = plt.hist(x=df_test[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=True,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(xname + ' (' + xunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  
plt.xlim([xmin,xmax]); plt.legend(loc='upper right')   

plt.subplot(222)
freq1,_,_ = plt.hist(x=df_train[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=True,label='Train')
freq2,_,_ = plt.hist(x=df_test[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=True,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(yname + ' (' + yunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  
plt.xlim([ymin,ymax]); plt.legend(loc='upper right')   

plt.subplot(223)                                              # plot the model
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.title('Porosity vs Density')
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![å›¾ç‰‡](img/019f46cd3338e55a8b84bcd57a899945.png)

## çº¿æ€§å›å½’æ¨¡å‹

è®©æˆ‘ä»¬å…ˆè®¡ç®—çº¿æ€§å›å½’æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ scikit learnï¼Œç„¶åå°†ç›¸åŒçš„æµç¨‹æ‰©å±•åˆ°å²­å›å½’ã€‚

+   æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œ$\phi = f(\rho)$ï¼Œå…¶ä¸­$\phi$æ˜¯å­”éš™ç‡ï¼Œ$\rho$æ˜¯å¯†åº¦ã€‚

+   æˆ‘ä»¬ä¹Ÿå¯ä»¥è¯´ï¼Œæˆ‘ä»¬æœ‰â€œå­”éš™ç‡å¯¹å¯†åº¦å›å½’â€ã€‚

æˆ‘ä»¬æ¨¡å‹æœ‰è¿™ä¸ªç‰¹å®šçš„æ–¹ç¨‹ï¼Œ

$$ \phi = b_1 \times \rho + b_0 $$

```py
linear_reg = linear_model.LinearRegression()                  # instantiate the model

linear_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters
x_model = np.linspace(xmin,xmax,10)
y_model_linear = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data 

plt.subplot(111)                                              # plot the data, model with model parameters
plt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])
plt.title('Linear Regression Model, Porosity = f(Density)')
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/5bc6600437501d419bf56af2e1dc727d.png)

ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°åœ¨é¢„æµ‹å‡½æ•°ä¸­å¯¹é¢„æµ‹ç‰¹å¾åº”ç”¨äº†é¢å¤–çš„é‡å¡‘æ“ä½œã€‚

```py
y_linear_model = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data 
```

è¿™æ˜¯å› ä¸º scikit-learn å‡è®¾æœ‰å¤šä¸ªé¢„æµ‹ç‰¹å¾ï¼›å› æ­¤ï¼ŒæœŸæœ›ä¸€ä¸ªåŒ…å«æ ·æœ¬ï¼ˆè¡Œï¼‰å’Œç‰¹å¾ï¼ˆåˆ—ï¼‰çš„äºŒç»´æ•°ç»„ï¼Œä½†æˆ‘ä»¬åªæœ‰ä¸€ä¸ªä¸€ç»´å‘é‡ã€‚

+   é‡å¡‘æ“ä½œå°† 1D å‘é‡è½¬æ¢ä¸ºåªæœ‰ 1 åˆ—çš„ 2D å‘é‡

## çº¿æ€§å›å½’æ¨¡å‹æ£€æŸ¥

è®©æˆ‘ä»¬è¿è¡Œä¸€äº›å¿«é€Ÿæ¨¡å‹æ£€æŸ¥ã€‚å¯ä»¥åšå¾—æ›´å¤šï¼Œä½†ä¸ºäº†ç®€æ´ï¼Œæˆ‘åœ¨è¿™é‡Œé™åˆ¶äº†èŒƒå›´ã€‚

+   è¯·å‚é˜…çº¿æ€§å›å½’ç« èŠ‚ä»¥è·å–æ›´å¤šä¿¡æ¯å’Œç›¸å…³æ£€æŸ¥

```py
y_pred_linear = linear_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data
r_squared_linear = r2_score(df_test[yname].values, y_pred_linear)

plt.subplot(121)                                              # plot testing diagnostics 
plt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
# plt.scatter(df_test[xname], y_pred,color='grey',edgecolor='black',s = 40, alpha = 1.0, label = 'predictions',zorder=100)
plt.scatter(df_test[xname], y_pred_linear,color='white',s=120,marker='o',linewidths=1.0, edgecolors="black",zorder=300)
plt.scatter(df_test[xname], y_pred_linear,color='red',s=90,marker='*',linewidths=0.5, edgecolors="black",zorder=320,label='Predictions')

for idata in range(0,len(df_test)):
    if idata == 0:
        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],
                        y_pred_linear[idata]],color='grey',label=r'test $\Delta_{y_i}$',zorder=1)
    else:  
        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],
                        y_pred_linear[idata]],color='grey',zorder=1)

plt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])
plt.annotate(r'$rÂ²$ :' + str(np.round(r_squared_linear,2)),[1.97,15])
plt.title('Linear Regression Model, Porosity = f(Density)')
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

y_res_linear = y_pred_linear - df_test['Porosity'].values     # calculate the test residual

plt.subplot(122)
plt.hist(y_res_linear, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))
plt.title("Error Residual at Testing Data"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')
plt.vlines(0,0,5.5,color='black',ls='--',lw=2)
plt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics
plt.annotate(r'$\overline{\Delta{y}}$: ' + str(round(np.average(y_res_linear),2)),[-4,4.4])
plt.annotate(r'$\sigma_{\Delta{y}}$: ' + str(np.round(np.std(y_res_linear),2)),[-4,4.1])
add_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/208a933176542d3185b7f4f4c9a1bf61.png)

## å²­å›å½’æ¨¡å‹

è®©æˆ‘ä»¬å°† scikit-learn çš„çº¿æ€§å›å½’æ–¹æ³•æ›¿æ¢ä¸º scikit-learn çš„å²­å›å½’æ–¹æ³•ã€‚

+   æ³¨æ„ï¼Œæˆ‘ä»¬ç°åœ¨å¿…é¡»è®¾ç½®$\lambda$è¶…å‚æ•°ã€‚

+   åœ¨ scikit-learn ä¸­ï¼Œè¶…å‚æ•°æ˜¯é€šè¿‡æ¨¡å‹çš„å®ä¾‹åŒ–æ¥è®¾ç½®çš„

```py
lam = 13.0                                                     # lambda hyperparameter

ridge_reg = Ridge(alpha=lam)                                  # instantiate the model

ridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters
x_model = np.linspace(xmin,xmax,10)
y_model_ridge = ridge_reg.predict(x_model.reshape(10,1))      # predict with the fit model

plt.subplot(111)                                              # plot the data, model with model parameters
plt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Linear Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])
plt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(lam))
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/05ecf578788df580a4c1f1f25e9ffcb9.png)

è®©æˆ‘ä»¬é‡å¤æˆ‘ä»¬å¯¹çº¿æ€§å›å½’æ¨¡å‹åº”ç”¨çš„åŸºæœ¬æ¨¡å‹æ£€æŸ¥ã€‚

```py
y_pred_ridge = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data
r_squared = r2_score(df_test[yname].values, y_pred_ridge)

plt.subplot(121)                                              # plot testing diagnostics 
plt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Linear Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='test')
plt.scatter(df_test[xname], y_pred_ridge,color='white',s=120,marker='o',linewidths=1.0, edgecolors="black",zorder=300)
plt.scatter(df_test[xname], y_pred_ridge,color='red',s=90,marker='*',linewidths=0.5, edgecolors="black",zorder=320,label='predictions')

for idata in range(0,len(df_test)):
    if idata == 0:
        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],
                        y_pred_ridge[idata]],color='grey',label=r'test $\Delta_{y_i}$',zorder=1)
    else:  
        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],
                        y_pred_ridge[idata]],color='grey',zorder=1)

plt.annotate('Ridge Regression Model Parameters:',[1.81,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])
plt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(lam))
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

y_res_ridge = y_pred_ridge - df_test['Porosity'].values       # calculate the test residual

plt.subplot(122)
plt.hist(y_res_ridge, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))
plt.title("Error Residual at Testing Data"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')
plt.vlines(0,0,5.5,color='red',lw=2,zorder=1); plt.vlines(np.average(y_res_ridge),0,5.5,color='black',ls='--',zorder=10)
plt.annotate('Residual Average = ' + str(np.round(np.average(y_res_ridge),2)),[np.average(y_res_ridge)+0.2,2.5],rotation=90.0)
plt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics
plt.annotate(r'$\overline{\Delta{y}}$: ' + str(round(np.average(y_res_ridge),2)),[-4,4.4])
plt.annotate(r'$\sigma_{\Delta{y}}$: ' + str(np.round(np.std(y_res_ridge),2)),[-4,4.1])
add_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/2e6dac5a06cdc0e91c454e5758d2a7af.png)

æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬è§£é‡Šçš„æ–¹å·®æ›´å°‘ï¼Œæ®‹å·®æ ‡å‡†å·®ï¼ˆæ›´å¤šè¯¯å·®ï¼‰æ›´å¤§ã€‚

+   å¯¹äºæˆ‘ä»¬ä»»æ„é€‰æ‹©çš„è¶…å‚æ•°$\lambda$ï¼Œå²­å›å½’å®é™…ä¸Šå‡å°‘äº†æµ‹è¯•æ–¹å·®è§£é‡Šå’Œå‡†ç¡®æ€§

+   è¿™å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¹¶æ²¡æœ‰è°ƒæ•´è¶…å‚æ•°ä»¥è·å¾—æœ€ä½³æ¨¡å‹ï¼

## è°ƒæŸ¥$\lambda$è¶…å‚æ•°

è®©æˆ‘ä»¬éå†å¤šä¸ª$\lambda$å€¼ - ä» 0 åˆ° 100ï¼Œå¹¶è§‚å¯Ÿä»¥ä¸‹å˜åŒ–ï¼š

+   è®­ç»ƒå’Œæµ‹è¯•ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰å’Œè§£é‡Šçš„æ–¹å·®

```py
# Arrays to store the results
ncases = 6
lamd_mat = np.logspace(-3,4,ncases)
x_model = np.linspace(xmin,xmax,10)
var_explained_train = np.zeros(ncases); var_explained_test = np.zeros(ncases)
mse_train = np.zeros(ncases); mse_test = np.zeros(ncases)

for ilam in range(0,len(lamd_mat)):                           # Loop over all lambda values
    ridge_reg = Ridge(alpha=lamd_mat[ilam])
    ridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # fit model

    y_model = ridge_reg.predict(x_model.reshape(10,1))        # predict with the fit model 
    y_pred_train = ridge_reg.predict(df_train[xname].values.reshape(len(df_train),1)) # predict with the fit model 
    var_explained_train[ilam] = r2_score(df_train[yname].values, y_pred_train)
    mse_train[ilam] = mean_squared_error(df_train[yname].values, y_pred_train) 

    y_pred_test = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1))
    var_explained_test[ilam] = r2_score(df_test[yname].values, y_pred_test)
    mse_test[ilam] = mean_squared_error(df_test[yname].values, y_pred_test)    

    if ilam <= 7:
        plt.subplot(4,2,ilam+1)
        plt.plot(x_model,y_model, color='red', linewidth=2,label='Linear Regression',zorder=100)
        plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
        plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
        plt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot
        plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])
        plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])
        plt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(round(lamd_mat[ilam],4)))
        plt.xlabel(xname + ' (' + xunit + ')')
        plt.ylabel(yname + ' (' + yunit + ')')
        plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=4.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/5a585689234bfbaf29513933357bc5cb.png)

æˆ‘ä»¬å¯ä»¥ä»è®­ç»ƒçš„å²­å›å½’æ¨¡å‹çš„å‰ 8 ä¸ªæ¡ˆä¾‹ä¸­è§‚å¯Ÿåˆ°ï¼Œ$\lambda$è¶…å‚æ•°çš„å¢åŠ ä¼šé™ä½çº¿æ€§æ‹Ÿåˆçš„æ–œç‡ã€‚

è®©æˆ‘ä»¬ç»˜åˆ¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä¸Šçš„å¹³å‡å¹³æ–¹è¯¯å·®å’Œè§£é‡Šçš„æ–¹å·®ã€‚

å›æƒ³ä¸€ä¸‹ï¼Œè§£é‡Šçš„æ–¹å·®$RÂ²$ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼Œ

$$ RÂ² = 1 - \frac{\text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}} $$

å…¶ä¸­$SS_{\text{residual}}$æ˜¯æ®‹å·®ï¼ˆæˆ–è¯¯å·®ï¼‰çš„å¹³æ–¹å’Œï¼Œ$SS_{\text{total}}$æ˜¯æ€»å¹³æ–¹å’Œï¼ˆè§‚å¯Ÿæ•°æ®çš„æ–¹å·®ï¼‰ã€‚

å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼Œ

$$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)Â² $$

å…¶ä¸­$y_i$æ˜¯å®é™…å€¼ï¼Œ$\hat{y}_i$æ˜¯é¢„æµ‹å€¼ï¼Œ$n$æ˜¯æ•°æ®ç‚¹çš„æ•°é‡ã€‚

```py
ncases = 100
lamd_mat = np.logspace(-3,4,ncases)
x_model = np.linspace(xmin,xmax,10)
var_explained_train = np.zeros(ncases); var_explained_test = np.zeros(ncases)
mse_train = np.zeros(ncases); mse_test = np.zeros(ncases)

for ilam in range(0,len(lamd_mat)):                           # Loop over all lambda values
    ridge_reg = Ridge(alpha=lamd_mat[ilam])
    ridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # fit model

    y_model = ridge_reg.predict(x_model.reshape(10,1))        # predict with the fit model 
    y_pred_train = ridge_reg.predict(df_train[xname].values.reshape(len(df_train),1)) # predict with the fit model 
    var_explained_train[ilam] = r2_score(df_train[yname].values, y_pred_train)
    mse_train[ilam] = mean_squared_error(df_train[yname].values, y_pred_train) 

    y_pred_test = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1))
    var_explained_test[ilam] = r2_score(df_test[yname].values, y_pred_test)
    mse_test[ilam] = mean_squared_error(df_test[yname].values, y_pred_test) 

plt.subplot(121)
plt.plot(lamd_mat, var_explained_train,  color='darkorange', linewidth = 2, label = 'Train')
plt.plot(lamd_mat, var_explained_test,  color='red', linewidth = 2, label = 'Test')
plt.title('Variance Explained vs. Lambda'); plt.xlabel('Lambda'); plt.ylabel('Variance Explained')
plt.xlim(0.001,10000.); plt.xscale('log'); plt.ylim(0,1.0); 
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linestyle=':', linewidth=0.5); plt.legend()  

plt.subplot(122)
plt.plot(lamd_mat, mse_train,  color='darkorange', linewidth = 2, label = 'Train')
plt.plot(lamd_mat, mse_test,  color='red', linewidth = 2, label = 'Test')
plt.title('MSE vs. Lambda'); plt.xlabel('Lambda'); plt.ylabel('Mean Square Error')
plt.xlim(0.001,10000.); plt.xscale('log'); plt.ylim(0,15.0); plt.legend()
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linestyle=':', linewidth=0.5); plt.legend()  

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/cebb95f748df4c664c9a127975a7c87b.png)

æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œéšç€æˆ‘ä»¬å¢åŠ $\lambda$å‚æ•°ï¼Œè§£é‡Šçš„æ–¹å·®å‡å°‘ï¼Œå‡æ–¹è¯¯å·®å¢åŠ ã€‚

+   è¿™æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºæ•°æ®å…·æœ‰ä¸€è‡´çš„çº¿æ€§è¶‹åŠ¿ï¼Œå¹¶ä¸”éšç€æ–œç‡â€œç¼©å°â€åˆ°é›¶ï¼Œè¯¯å·®å¢åŠ ï¼Œè§£é‡Šçš„æ–¹å·®å‡å°‘

+   å¯èƒ½è¿˜æœ‰å…¶ä»–æƒ…å†µï¼Œå…¶ä¸­å‡å°‘çš„æ–œç‡åœ¨æµ‹è¯•ä¸­å®é™…ä¸Šè¡¨ç°æ›´å¥½ã€‚ä¾‹å¦‚ï¼Œå¯¹äºç¨€ç–å’Œæœ‰å™ªå£°çš„æ•°æ®ã€‚

## æ¨¡å‹æ–¹å·®

ç°åœ¨è®©æˆ‘ä»¬æ¢è®¨æ¨¡å‹æ–¹å·®çš„æ¦‚å¿µï¼Œè¿™æ˜¯æµ‹è¯•ä¸­æœºå™¨å­¦ä¹ å‡†ç¡®æ€§çš„ä¸€ä¸ªé‡è¦éƒ¨åˆ†ã€‚

+   æ¨¡å‹å¯¹ç‰¹å®šè®­ç»ƒæ•°æ®çš„æ•æ„Ÿæ€§

+   éšç€ $\lambda$ çš„å¢åŠ ï¼Œæ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„æ•æ„Ÿæ€§å¢åŠ ï¼Œæ¨¡å‹æ–¹å·®å‡å°

è®©æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹å·¥ä½œæµç¨‹æ¥æ¼”ç¤ºè¿™ä¸€ç‚¹ï¼š

+   éå†å¤šä¸ª $\lambda$ å€¼

    +   éå†å¤šä¸ªæ•°æ®è‡ªåŠ©æ ·æœ¬

        +   è®¡ç®—å²­å›å½’æ‹Ÿåˆï¼ˆæ–œç‡ï¼‰

    +   è®¡ç®—è¿™äº›è‡ªåŠ©ç»“æœçš„æ ‡å‡†å·®

è­¦å‘Šï¼šè¿™éœ€è¦å‡ åˆ†é’Ÿæ‰èƒ½è¿è¡Œ

```py
L = 200                                                       # the number of bootstrap realizations 

nsamples = 20                                                 # the number of samples in each bootstrap realization
nlambda = 100                                                 # number of lambda values to evaluate

coef_mat = np.zeros(L)                                        # declare arrays to store the results
variance_coef = np.zeros(nlambda)

#lamd_mat = np.linspace(0.0,100.0,nlambda) 
lambda_mat = np.logspace(-2,5,nlambda)
for ilam in range(0,len(lambda_mat)):                         # loop over all lambda values 
    for l in range(0, L):                                     # loop over all bootstrap realizations
        df_sample = df.sample(n = nsamples)                   # random sample (1 bootstrap)
        ridge_reg = Ridge(alpha=lambda_mat[ilam])             # instantiate model
        ridge_reg.fit(df_sample[xname].values.reshape(nsamples,1), df_sample[yname]) # fit model
        coef_mat[l] = ridge_reg.coef_[0]                      # get the slope parameter
    variance_coef[ilam] = np.var(coef_mat)                    # calculate the variance of the slopes over the L bootstraps

plt.subplot(111)
plt.plot(lambda_mat, variance_coef,  color='black', linewidth = 3, label = 'Slope Variance')
plt.title('Model Variance vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Model Variance - Variance of the Slope Parameter, $b_1$')
plt.xlim(0.01,100000.); plt.ylim(0.0,5.0); plt.xscale('log')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2)
plt.grid(which='both')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/42462583008a06affc490158f6f84d94.png)

ç»“æœæ­£å¦‚é¢„æœŸï¼Œéšç€ $\lambda$ è¶…å‚æ•°çš„å¢åŠ ï¼Œæ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„æ•æ„Ÿæ€§é™ä½ã€‚

## k æŠ˜äº¤å‰éªŒè¯

è¿›è¡Œå®Œæ•´çš„ k æŠ˜éªŒè¯ä»¥è¯„ä¼°æµ‹è¯•è¯¯å·®ä¸æ¨¡å‹è°ƒä¼˜çš„è¶…å‚æ•° $\lambda$ ä¹‹é—´çš„å…³ç³»å°†æ˜¯æœ‰ç”¨çš„ã€‚

+   ä»¥ä¸‹ä»£ç æä¾›ç”¨äºæ‰§è¡Œæ­¤æ“ä½œ

+   å†æ¬¡å¼ºè°ƒï¼Œå¯¹äºå•ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œæˆ‘ä»¬å¿…é¡»å°†å…¶é‡å¡‘ä¸ºäºŒç»´æ•°ç»„

æˆ‘ä»¬éå†ä» 0.01 åˆ° 100,000 çš„ 100 ä¸ª $\lambda$ å€¼ï¼Œ

+   è·å–æ¯ä¸ª 4 ä¸ª k æŠ˜çš„è´Ÿå‡æ–¹è¯¯å·®

+   ç„¶åæˆ‘ä»¬å–å¹³å‡å€¼å¹¶åº”ç”¨ç»å¯¹å€¼

ä¸ºä»€ä¹ˆä½¿ç”¨è´Ÿå‡æ–¹è¯¯å·®ï¼Ÿç®€å•æ¥è¯´ï¼Œä¸ºäº†ä½¿ç”¨ scikit-learn ä¸­çš„ä¼˜åŒ–åŠŸèƒ½ï¼Œè¯¥åŠŸèƒ½é€šè¿‡æœ€å¤§åŒ–è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ä¸”ä¸å…¶ä»–è¯„åˆ†ï¼ˆå¦‚ $rÂ²$ï¼‰ä¿æŒä¸€è‡´æ€§ï¼Œå…¶ä¸­è¾ƒå¤§çš„å€¼æ›´å¥½ã€‚

+   æˆ‘å‘ç°è´Ÿå‡æ–¹è¯¯å·®å¾ˆä»¤äººå›°æƒ‘ï¼Œå› æ­¤ä¸ºäº†ç»˜å›¾ï¼Œæˆ‘ä½¿ç”¨ç»å¯¹å€¼å°†å€¼è½¬æ¢ä¸ºä¸¥æ ¼æ­£å€¼ã€‚

```py
score = []                                                    # code modified from StackOverFlow by Dimosthenis

nlambda = 100
lambda_mat = np.logspace(-2,5,nlambda)
for ilam in range(0,nlambda):
    ridge_reg = Ridge(alpha=lambda_mat[ilam])
    scores = cross_val_score(estimator=ridge_reg, X= df['Density'].values.reshape(-1, 1), y=df['Porosity'].values, cv=10, n_jobs=4, scoring = "neg_mean_squared_error") # Perform 10-fold cross validation
    score.append(abs(scores.mean()))

plt.subplot(111)
plt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)
plt.title('Ridge Regression Test Mean Square Error vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')
plt.xlim(1.0e-2,1.0e5); plt.ylim(0.001,20.0); plt.xscale('log')
plt.vlines(0.5,0,20,color='red',lw=2); plt.vlines(1000,0,20,color='red',lw=2,zorder=10)
plt.annotate('Linear Regression',[0.4,12.5],rotation=90.0,color='red',zorder=10)
plt.annotate('Mean',[1100,14.5],rotation=90.0,color='red',zorder=10)
plt.fill_between([0.01,0.5],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.fill_between([1000,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.grid(which='both')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/8eb67ef97a23db1eb0ad78518b9e99c2.png)

ä»ä¸Šè¿°å†…å®¹ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ° 3 ä¸ªé˜¶æ®µï¼Œ

+   å·¦ - æµ‹è¯•å‡æ–¹è¯¯å·®è¾¾åˆ°æœ€å°å€¼ï¼Œæ¨¡å‹æ”¶æ•›åˆ°çº¿æ€§å›å½’ã€‚

+   å³ - æµ‹è¯•å‡æ–¹è¯¯å·®è¾¾åˆ°æœ€å¤§å€¼ï¼Œæ¨¡å‹æ”¶æ•›åˆ°é¢„æµ‹ç‰¹å¾å‡å€¼ï¼Œå³æ¨¡å‹å‚æ•°æ–œç‡ä¸º 0.0ã€‚

+   ä¸­å¿ƒ - ä¸¤ç§æƒ…å†µä¹‹é—´çš„è¿‡æ¸¡

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çœ‹åˆ°çº¿æ€§å›å½’æ¨¡å‹ï¼ˆ$\lambda = 0.0$ï¼‰æ˜¯æœ€ä½³æ¨¡å‹ï¼å¦‚æœå²­å›å½’æ˜¯æœ€ä½³æ¨¡å‹ï¼Œåˆ™æµ‹è¯•å‡æ–¹è¯¯å·®å°†åœ¨çº¿æ€§å›å½’å’Œå‡å€¼ä¹‹é—´æœ€å°åŒ–ã€‚

+   è¿™é€šå¸¸å‘ç”Ÿåœ¨å…·æœ‰å™ªå£°ã€æ•°æ®ç¨€å°‘å’Œé«˜ç»´æ•°æ®é—®é¢˜çš„æ•°æ®é›†ä¸Šã€‚

æˆ‘ä»¬çš„æ¨¡å¼ä¸çº¿æ€§å›å½’ç›¸åŒã€‚

+   æˆ‘ä»¬èƒ½å¦åˆ›é€ ä¸€ä¸ªæœ€ä½³æ¨¡å‹ä¸æ˜¯çº¿æ€§å›å½’çš„æƒ…å†µï¼Ÿå³ï¼Œæ­£åˆ™åŒ–æœ‰å¸®åŠ©çš„æƒ…å†µï¼Ÿ

+   æ˜¯çš„ï¼Œæˆ‘ä»¬å¯ä»¥ã€‚è®©æˆ‘ä»¬åˆ é™¤å¤§éƒ¨åˆ†æ ·æœ¬ä»¥åˆ›å»ºæ•°æ®ç¨€å°‘ï¼Œå¹¶æ·»åŠ å¤§é‡å™ªå£°ï¼

æ‰¿è®¤ï¼Œæˆ‘è¿­ä»£äº†æ ·æœ¬å’Œå™ªå£°çš„éšæœºç§å­ä»¥è·å¾—è¿™ä¸ªç»“æœã€‚

+   å°‘é‡æ•°æ®ï¼ˆä½ $n$ï¼‰å’Œé«˜ç»´ï¼ˆé«˜ $m$ï¼‰é€šå¸¸ä¼šå¯¼è‡´ LASSO æ¯”çº¿æ€§å›å½’è¡¨ç°æ›´å¥½

```py
df_sample = df.copy(deep=True).sample(n=10,random_state=11)
noise_stdev = 3.0
np.random.seed(seed=15)
df_sample['Porosity'] = df_sample['Porosity'] + np.random.normal(0.0, noise_stdev, size=len(df_sample))

score = []                                                    # code modified from StackOverFlow by Dimosthenis

nlambda = 100
lambda_mat = np.logspace(-4,3,nlambda)
for ilam in range(0,nlambda):
    ridge_reg = Ridge(alpha=lambda_mat[ilam])
    scores = cross_val_score(estimator=ridge_reg, X= df_sample['Density'].values.reshape(-1, 1), 
                             y=df_sample['Porosity'].values, cv=2, n_jobs=4, scoring = "neg_mean_squared_error") # Perform 10-fold cross validation
    score.append(abs(scores.mean()))

plt.subplot(111)
plt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)
plt.title('Ridge Regression Test Mean Square Error vs. Ridge Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')
plt.xlim(1.0e-4,1.0e3); plt.ylim(0.001,20.0); 
plt.xscale('log')
plt.vlines(0.001,0,20,color='red',lw=2); plt.vlines(10,0,20,color='red',lw=2,zorder=10); 
plt.vlines(0.07,0,20,color='red',lw=2,zorder=10);
plt.annotate('Linear Regression',[0.0007,12.5],rotation=90.0,color='red',zorder=10)
plt.annotate(r'Ridge Tuned $\lambda$',[0.055,12.5],rotation=90.0,color='red',zorder=10)
plt.annotate('Mean',[7.4,14.5],rotation=90.0,color='red',zorder=10)
plt.fill_between([0.0001,0.001],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.fill_between([10,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.grid(which='both')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/b011c5d0305db3ebe120ee6826faed3c.png)

## è¯„è®º

è¿™æ˜¯å¯¹å²­å›å½’çš„åŸºæœ¬å¤„ç†ã€‚è¿˜æœ‰æ›´å¤šå¯ä»¥åšçš„å’Œè®¨è®ºçš„ï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´ YouTube è®²åº§é“¾æ¥ä¸­çš„èµ„æºé“¾æ¥ï¼Œè§†é¢‘æè¿°ä¸­åŒ…å«èµ„æºé“¾æ¥ã€‚

å¸Œæœ›è¿™å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![å›¾ç‰‡](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ˜¯[ Cockrell å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[Jackson åœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œåœ¨[å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡](https://www.utexas.edu/)è¿›è¡Œåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ çš„ç ”ç©¶å’Œæ•™å­¦ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚

+   [ã€Šè®¡ç®—æœºä¸åœ°çƒç§‘å­¦ã€‹](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠ[ã€Šæ•°å­¦åœ°çƒç§‘å­¦ã€‹](https://link.springer.com/journal/11004/editorial-board)å›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼šçš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº†è¶…è¿‡ 70 ç¯‡[åŒè¡Œè¯„å®¡çš„å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦ã€Š[åœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ã€‹ï¼Œå¹¶æ˜¯ä¸¤æœ¬è¿‘æœŸå‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œåˆ†åˆ«æ˜¯[ã€ŠPython åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—ã€‹(https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œã€ŠPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šä»£ç å®è·µæŒ‡å—ã€‹(https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‘ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å« 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†å·¥ä½œæµç¨‹ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³è¦ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³è¦äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æä»¥åŠæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚ä¸ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼Œæ³¨å†Œå·¥ç¨‹å¸ˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢æ•™æˆ

æ›´å¤šèµ„æºè¯·è®¿é—®ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
