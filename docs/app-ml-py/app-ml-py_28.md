# æ¢¯åº¦æå‡æ ‘

> åŸæ–‡ï¼š[`geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_gradient_boosting.html`](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_gradient_boosting.html)

Michael J. Pyrczï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åº”ç”¨åœ°è´¨ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

ç”µå­ä¹¦â€œPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œâ€çš„ä¸€ç« ã€‚

å¼•ç”¨æ­¤ç”µå­ä¹¦å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *Python åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œ* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138 ![DOI](https://doi.org/10.5281/zenodo.15169138)

æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠæ›´å¤šå†…å®¹å¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š

å¼•ç”¨ MachineLearningDemos GitHub ä»“åº“å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *MachineLearningDemos: Python æœºå™¨å­¦ä¹ æ¼”ç¤ºå·¥ä½œæµç¨‹ä»“åº“* (0.0.3) [è½¯ä»¶]. Zenodo. DOI: 10.5281/zenodo.13835312\. GitHub ä»“åº“ï¼š[GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos) ![DOI](https://zenodo.org/doi/10.5281/zenodo.13835312)

ä½œè€…ï¼šMichael J. Pyrcz

Â© ç‰ˆæƒæ‰€æœ‰ 2024ã€‚

æœ¬ç« æ˜¯å…³äº **æ¢¯åº¦æå‡æ ‘** çš„æ•™ç¨‹å’Œæ¼”ç¤ºã€‚

**YouTube è®²åº§**ï¼šè¯·æŸ¥çœ‹æˆ‘åœ¨ä»¥ä¸‹ä¸»é¢˜ä¸Šçš„è®²åº§ï¼š

+   [æœºå™¨å­¦ä¹ ç®€ä»‹](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)

+   [å†³ç­–æ ‘](https://youtu.be/JUGo1Pu3QT4?si=ebQXv6Yglar0mYWp)

+   [éšæœºæ£®æ—](https://youtu.be/m5_wk310fho?si=up-mzVPHvniXsYE6)

+   [æ¢¯åº¦æå‡](https://youtu.be/___T8_ixIwc?si=ozHR_eIuMF3SPTxJ)

è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘ YouTube ä¸Šçš„ [æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI) çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«æœ‰è‰¯å¥½æ–‡æ¡£è®°å½•çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºç†è§£ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœæ‚¨æƒ³äº†è§£æˆ‘çš„åŠ¨æœºï¼Œè¯·æŸ¥çœ‹ [Michael çš„æ•…äº‹](https://michaelpyrcz.com/my-story)ã€‚

## æ¢¯åº¦æå‡æ ‘çš„åŠ¨åŠ›

åœ¨æˆ‘ä»¬èƒ½å¤Ÿç†è§£æ¢¯åº¦æå‡æ ‘ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦äº†è§£å†³ç­–æ ‘ã€‚ä»¥ä¸‹æ˜¯å†³ç­–æ ‘çš„å…³é”®æ¦‚å¿µã€‚

### å†³ç­–æ ‘

**é¢„æµ‹**

+   ä¼°è®¡ä¸€ä¸ªå‡½æ•° $\hat{f}$ï¼Œä»¥ä¾¿æˆ‘ä»¬ä»ä¸€ç»„é¢„æµ‹ç‰¹å¾ $X_1,\ldots,X_m$ ä¸­é¢„æµ‹å“åº”ç‰¹å¾ $Y$ã€‚

+   é¢„æµ‹å½¢å¼ä¸º $\hat{Y} = \hat{f}(X_1,\ldots,X_m)$

**ç›‘ç£å­¦ä¹ **

+   å“åº”ç‰¹å¾æ ‡ç­¾ï¼Œ$Y$ï¼Œåœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¸­éƒ½æ˜¯å¯ç”¨çš„

**åŸºäºå†³ç­–æ ‘é›†æˆ**

è¿™äº›æ˜¯ä¸å†³ç­–æ ‘ç›¸å…³çš„æ¦‚å¿µã€‚

**ç‰¹å¾ç©ºé—´çš„åˆ†å±‚ã€äºŒåˆ†åˆ†å‰²**

åŸºæœ¬æ€æƒ³æ˜¯å°†é¢„æµ‹ç©ºé—´ï¼Œ$ğ‘‹_1,\ldots,X_m$ï¼Œåˆ’åˆ†ä¸º $J$ ä¸ªäº’æ–¥ã€ç©·å°½çš„åŒºåŸŸ

+   **äº’æ–¥æ€§** â€“ ä»»ä½•é¢„æµ‹å› å­çš„ç»„åˆåªå±äºä¸€ä¸ªå•ä¸€çš„åŒºåŸŸï¼Œ$R_j$

+   **ç©·å°½æ€§** â€“ æ‰€æœ‰é¢„æµ‹å› å­çš„ç»„åˆéƒ½å±äºä¸€ä¸ªåŒºåŸŸï¼Œ$R_j$ï¼Œè¿™äº›åŒºåŸŸè¦†ç›–æ•´ä¸ªç‰¹å¾ç©ºé—´ï¼ˆè€ƒè™‘çš„å˜é‡çš„èŒƒå›´ï¼‰

å¯¹äºåŒºåŸŸ $R_j$ ä¸­çš„æ¯ä¸ªè§‚æµ‹å€¼ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„é¢„æµ‹ï¼Œ$\hat{Y}(R_j)$

ä¾‹å¦‚ï¼Œä»å­”éš™ç‡ï¼Œ${X_1}$ é¢„æµ‹ç”Ÿäº§ï¼Œ$\hat{Y}$

+   ç»™å®šä¸€ä¸ª mD ç‰¹å¾ç©ºé—´å†…çš„æ•°æ®ï¼Œ$X_1,\ldots,X_m$ï¼Œæ‰¾åˆ°è¾¹ç•Œæœ€å¤§åŒ–ä¸¤ä¸ªç±»åˆ«ä¹‹é—´çš„å·®è·

+   æ–°æ¡ˆä¾‹æ ¹æ®å®ƒä»¬ç›¸å¯¹äºè¿™ä¸ªè¾¹ç•Œçš„ç›¸å¯¹ä½ç½®è¿›è¡Œåˆ†ç±»

**æ ‘æ„å»ºçš„æ­¥éª¤**

æ ‘æ˜¯ä»ä¸Šå¾€ä¸‹æ„å»ºçš„ã€‚æˆ‘ä»¬ä»ä¸€ä¸ªè¦†ç›–æ•´ä¸ªç‰¹å¾ç©ºé—´çš„å•ä¸€åŒºåŸŸå¼€å§‹ï¼Œç„¶åè¿›è¡Œä¸€ç³»åˆ—çš„åˆ†å‰²ã€‚

+   **æ‰«ææ‰€æœ‰å¯èƒ½çš„åˆ†å‰²**è¦†ç›–æ‰€æœ‰åŒºåŸŸå’Œæ‰€æœ‰ç‰¹å¾ã€‚

+   **è´ªå©ªä¼˜åŒ–**è¯¥æ–¹æ³•é€šè¿‡æ‰¾åˆ°ä»»ä½•ç‰¹å¾ä¸­çš„ç¬¬ä¸€ä¸ªåˆ†å‰²ï¼ˆåˆ†å‰²ï¼‰æ¥æœ€å°åŒ–æ‰€æœ‰è®­ç»ƒæ•°æ® $y_i$ åœ¨æ‰€æœ‰åŒºåŸŸ $j = 1,\ldots,J$ ä¸Šçš„æ®‹å·®å¹³æ–¹å’Œã€‚

$$ RSS = \sum^{J}_{j=1} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})Â² $$

+   **åœæ­¢æ ‡å‡†**é€šå¸¸æ˜¯æ¯ä¸ªåŒºåŸŸä¸­è®­ç»ƒæ•°æ®çš„æœ€å°æ•°é‡ï¼Œä»¥è¿›è¡Œç¨³å¥ä¼°è®¡ï¼Œä»¥åŠ/æˆ–ä¸‹ä¸€ä¸ªåˆ†å‰²çš„æœ€å° RSS å‡å°‘é‡

ç°åœ¨æˆ‘ä»¬å¯ä»¥ä»‹ç»åŸºäºå†³ç­–æ ‘æ¦‚å¿µçš„æ¢¯åº¦æå‡æ ‘ã€‚

## å¢å¼ºæ¨¡å‹

å¢å¼ºé€šè¿‡æ·»åŠ å¤šä¸ªå¼±å­¦ä¹ å™¨æ¥æ„å»ºä¸€ä¸ªæ›´å¼ºçš„å­¦ä¹ å™¨ã€‚

+   å¼±å­¦ä¹ å™¨æ˜¯æŒ‡æä¾›é¢„æµ‹ç»“æœä»…ç•¥å¥½äºéšæœºé€‰æ‹©çš„æ¨¡å‹

æˆ‘ä¼šç”¨æ–‡å­—å’Œæ–¹ç¨‹å¼æ¥è§£é‡Šè¿™ä¸ªæ–¹æ³•ã€‚

+   æ„å»ºä¸€ä¸ªè¯¯å·®ç‡é«˜çš„ç®€å•æ¨¡å‹ï¼Œæ¨¡å‹å¯èƒ½éå¸¸ä¸å‡†ç¡®ï¼Œä½†æ–¹å‘æ˜¯æ­£ç¡®çš„

+   è®¡ç®—æ¨¡å‹çš„è¯¯å·®

+   å°†å¦ä¸€ä¸ªæ¨¡å‹æ‹Ÿåˆåˆ°è¯¯å·®ä¸Š

+   è®¡ç®—ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªæ¨¡å‹æ·»åŠ åçš„è¯¯å·®

+   é‡å¤ç›´åˆ°è¾¾åˆ°æ‰€éœ€çš„å‡†ç¡®åº¦æˆ–æ»¡è¶³å…¶ä»–åœæ­¢æ ‡å‡†

ä» $X_1,\ldots,X_m$ é¢„æµ‹ $Y$ çš„ä¸€èˆ¬å·¥ä½œæµç¨‹æ˜¯ï¼š

+   æ„å»ºä¸€ä¸ªå¼±å­¦ä¹ å™¨æ¥ä» $X_1,\ldots,X_m$ é¢„æµ‹ $Y$ï¼Œä»è®­ç»ƒæ•°æ® $x_{i,j}$ é¢„æµ‹ $\hat{F}_k(X)$ã€‚

+   éå†æ‰€éœ€ä¼°è®¡å™¨çš„æ•°é‡ï¼Œ$k = 1,\ldots,K$

    1.  è®¡ç®—è®­ç»ƒæ•°æ®ä¸­çš„æ®‹å·®ï¼Œ$h_k(x_{i}) = y_i - \hat{F}_k(x_{i})$

    1.  å°†å¦ä¸€ä¸ªå¼±å­¦ä¹ å™¨æ‹Ÿåˆåˆ°ä» $X_1,\ldots,X_m$ é¢„æµ‹ $h_k$ï¼Œä»è®­ç»ƒæ•°æ® $x_{i,j}$ é¢„æµ‹ $\hat{F}_k(X)$ã€‚

æˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„ $K$ æ¨¡å‹å±‚æ¬¡ç»“æ„ã€‚

+   æ¯ä¸ªæ¨¡å‹éƒ½æ˜¯åŸºäºå‰ä¸€ä¸ªæ¨¡å‹æ¥æé«˜å‡†ç¡®æ€§çš„

æˆ‘ä»¬çš„å›å½’ä¼°è®¡é‡æ˜¯ $K$ ä¸ªç®€å•æ¨¡å‹çš„å’Œã€‚

$$ \hat{Y} =\sum_{k=1}^{K} F_k(X_1,\ldots,X_m) $$

## æ¢¯åº¦æå‡æ–¹æ³•

å¦‚æœä½ çœ‹ä¸€ä¸‹å‰é¢çš„æ–¹æ³•ï¼Œå®ƒå°±æ¸…æ¥šå¯ä»¥æ˜ å°„åˆ°ä¸€ä¸ªæ¢¯åº¦ä¸‹é™é—®é¢˜

åœ¨æ¯ä¸ªæ­¥éª¤ $k$ï¼Œéƒ½åœ¨æ‹Ÿåˆä¸€ä¸ªæ¨¡å‹ï¼Œç„¶åè®¡ç®—è¯¯å·®ï¼Œ$h_k(X_1,\ldots,X_m)$ã€‚

æˆ‘ä»¬å¯ä»¥åˆ†é…ä¸€ä¸ªæŸå¤±å‡½æ•°

$$ L\left(y,F(X)\right) = \frac{\left(y - F(X)\right)Â²}{2} $$

å› æ­¤ï¼Œæˆ‘ä»¬æƒ³è¦æœ€å°åŒ– $\ell2$ æŸå¤±å‡½æ•°ï¼š

$$ J = \sum_{i=1}^{n} L\left(y_i, F_k(X) \right) $$

é€šè¿‡è°ƒæ•´æˆ‘ä»¬çš„æ¨¡å‹ç»“æœæ¥è°ƒæ•´æˆ‘ä»¬çš„è®­ç»ƒæ•°æ® $F(x_1), F(x_2),\ldots,F(x_n)$ã€‚

æˆ‘ä»¬å¯ä»¥è®¡ç®—è¯¯å·®ç›¸å¯¹äºæˆ‘ä»¬æ¨¡å‹çš„åå¯¼æ•°ã€‚

$$ \frac{\partial J}{\partial F(x_i)} = F(x_i) - y_i $$

æˆ‘ä»¬å¯ä»¥å°†æ®‹å·®è§£é‡Šä¸ºè´Ÿæ¢¯åº¦ã€‚

$$ y_i - F(x_i) = -1 \frac{\partial J}{\partial F(x_i)} $$

å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªæ¢¯åº¦ä¸‹é™é—®é¢˜ï¼š

$$ F_{k+1}(X_i) = F_k(X_i) + h(X_i) $$$$ F_{k+1}(X_i) = F_k(X_i) + y_i - F_k(X_i) $$$$ F_{k+1}(X_i) = F_k(X_i) - 1 \frac{\partial J}{\partial F_k(X_i)} $$

é€šç”¨å½¢å¼å¦‚ä¸‹ï¼š

$$ \phi_{k+1} = \phi_k - \rho \frac{\partial J}{\partial \phi_k} $$

å…¶ä¸­ $phi_k$ æ˜¯å½“å‰çŠ¶æ€ï¼Œ$\rho$ æ˜¯å­¦ä¹ ç‡ï¼Œ$J$ æ˜¯æŸå¤±å‡½æ•°ï¼Œè€Œ $\phi_{k+1}$ æ˜¯æˆ‘ä»¬ä¼°è®¡å™¨çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚

å¦‚æœæˆ‘ä»¬å°†è®­ç»ƒæ•°æ®ä¸­çš„æ®‹å·®è§†ä¸ºæ¢¯åº¦ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ­£åœ¨è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚

+   å¯¹è´Ÿæ¢¯åº¦æ‹Ÿåˆä¸€ç³»åˆ—æ¨¡å‹

é€šè¿‡å°†é—®é¢˜è§†ä¸ºæ¢¯åº¦ä¸‹é™é—®é¢˜ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåº”ç”¨å„ç§æŸå¤±å‡½æ•°

+   $\ell2$ æ˜¯æˆ‘ä»¬çš„ $\frac{\left(y - F(X)\right)Â²}{2}$ æ˜¯å®ç”¨çš„ï¼Œä½†å¯¹å¤–éƒ¨å¼‚å¸¸å€¼ä¸ç¨³å¥

$$ - 1 \frac{\partial J}{\partial F_k(X_i)} = y_i - F_k(X_i) $$

+   $\ell1$ æ˜¯æˆ‘ä»¬çš„ $|y - F(X)|$ å¯¹å¤–éƒ¨å¼‚å¸¸å€¼æ›´ç¨³å¥

$$ - 1 \frac{\partial J}{\partial F_k(X_i)} = sign(y_i - F_k(X_i)) $$

+   è¿˜æœ‰å…¶ä»–ä¸€äº›ï¼Œå¦‚ Huber æŸå¤±

**å¯è§£é‡Šæ€§**

ä¸å†³ç­–æ ‘ç›¸æ¯”ï¼Œé›†æˆæ–¹æ³•çš„å¯è§£é‡Šæ€§è¾ƒä½ã€‚æé«˜æ¨¡å‹å¯è§£é‡Šæ€§çš„ä¸€ä¸ªå·¥å…·æ˜¯ç‰¹å¾é‡è¦æ€§ã€‚

æˆ‘ä»¬é€šè¿‡è®¡ç®—ä»¥ä¸‹å¹³å‡æ¥è®¡ç®—å˜é‡é‡è¦æ€§ï¼š

+   å¯¹æ‰€æœ‰æ¶‰åŠæ¯ä¸ªé¢„æµ‹ç‰¹å¾çš„åˆ†éš”è¿›è¡Œæ®‹å·®å¹³æ–¹å’Œå‡å°‘ï¼Œä»¥è¿›è¡Œå›å½’

+   å¯¹äºæ‰€æœ‰æ¶‰åŠæ¯ä¸ªé¢„æµ‹ç‰¹å¾çš„åˆ†éš”ï¼Œé™ä½ Gini æŒ‡æ•°

è¿™ä¸¤ä¸ªåº“éƒ½æ ‡å‡†åŒ–ä¸ºåœ¨ç‰¹å¾ä¸Šçš„å’Œä¸º 1.0ã€‚

## åˆ†ç±»

å“åº”æ˜¯å¯èƒ½çš„æœ‰é™ç±»åˆ«é›†åˆã€‚

+   å¯¹äºæ¯ä¸ªè®­ç»ƒæ•°æ®ï¼ŒçœŸå®å€¼åœ¨è§‚å¯Ÿç±»åˆ«ä¸­çš„æ¦‚ç‡ä¸º 100%ï¼Œå¦åˆ™ä¸º 0%

+   ä½¿ç”¨å†³ç­–æ ‘ä¼°è®¡æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡

+   ä½¿ç”¨çœŸå®åˆ†å¸ƒå’Œä¼°è®¡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚åº¦ä½œä¸ºæŸå¤±å‡½æ•°æ¥æœ€å°åŒ–

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åº“ã€‚è¿™äº›åº“åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = True
import os                                                     # to set current working directory 
import math                                                   # square root operator
import numpy as np                                            # arrays and matrix math
import scipy.stats as st                                      # statistical methods
import pandas as pd                                           # DataFrames
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import seaborn as sns                                         # for matrix scatter plots
from sklearn.tree import DecisionTreeRegressor                # decision tree method
from sklearn.ensemble import GradientBoostingRegressor        # tree-based gradient boosting
from sklearn.tree import _tree                                # for accessing tree information
from sklearn import metrics                                   # measures to check our models
from sklearn.preprocessing import StandardScaler              # standardize the features
from sklearn.tree import export_graphviz                      # graphical visualization of trees
from sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning
from sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline
from sklearn import metrics                                   # measures to check our models
from sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation
from sklearn.model_selection import train_test_split          # train and test split
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # suppress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½é¦–å…ˆéœ€è¦å®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ›´å¤šå¸®åŠ©ï¼Œè¯·å‚é˜…å„è‡ªçš„åŒ…æ–‡æ¡£ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰å‡ ä¸ªå‡½æ•°æ¥ç®€åŒ–ç»˜åˆ¶ç›¸å…³çŸ©é˜µå’Œå†³ç­–æ ‘å›å½’æ¨¡å‹çš„å¯è§†åŒ–ã€‚

```py
def comma_format(x, pos):
    return f'{int(x):,}'

def feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot
    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1
    plt.plot(pred,metric,color='black',zorder=20)
    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)
    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  
    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)
    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();
    return

def plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix 
    my_colormap = plt.get_cmap('RdBu_r', 256)          
    newcolors = my_colormap(np.linspace(0, 1, 256))
    white = np.array([256/256, 256/256, 256/256, 1])
    white_low = int(128 - mask*128); white_high = int(128+mask*128)
    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)
    newcmp = ListedColormap(newcolors)
    m = corr_matrix.shape[0]
    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)
    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()
    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()
    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
    plt.colorbar(im, orientation = 'vertical')
    plt.title(title)
    for i in range(0,m):
        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')
        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')
    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def plot_CDF(data,color,alpha=1.0,lw=1,ls='solid',label='none'):
    cumprob = (np.linspace(1,len(data),len(data)))/(len(data)+1)
    plt.scatter(np.sort(data),cumprob,c=color,alpha=alpha,edgecolor='black',lw=lw,ls=ls,label=label,zorder=10)
    plt.plot(np.sort(data),cumprob,c=color,alpha=alpha,lw=lw,ls=ls,zorder=8)

def visualize_model(model,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,
                         ymax,title,Xname,yname,Xlabel,ylabel,annotate=True):# plots the data points and the decision tree prediction 
    cmap = plt.cm.inferno
    X1plot_step = (Xmax[0] - Xmin[0])/300.0; X2plot_step = -1*(Xmax[1] - Xmin[1])/300.0 # resolution of the model visualization
    XX1, XX2 = np.meshgrid(np.arange(Xmin[0], Xmax[0], X1plot_step), # set up the mesh
                     np.arange(Xmax[1], Xmin[1], X2plot_step))
    y_hat = model.predict(np.c_[XX1.ravel(), XX2.ravel()])    # predict with our trained model over the mesh
    y_hat = y_hat.reshape(XX1.shape)

    plt.imshow(y_hat,interpolation=None, aspect="auto", extent=[Xmin[0],Xmax[0],Xmin[1],Xmax[1]], 
        vmin=ymin,vmax=ymax,alpha = 1.0,cmap=cmap,zorder=1)
    sp = plt.scatter(X1_train,X2_train,s=None, c=y_train, marker='o', cmap=cmap, 
        norm=None, vmin=ymin, vmax=ymax, alpha=0.6, linewidths=0.3, edgecolors="black", label = 'Train',zorder=10)
    plt.scatter(X1_test,X2_test,s=None, c=y_test, marker='s', cmap=cmap, 
        norm=None, vmin=ymin, vmax=ymax, alpha=0.3, linewidths=0.3, edgecolors="black", label = 'Test',zorder=10)
    plt.title(title); plt.xlabel(Xlabel[0]); plt.ylabel(Xlabel[1])
    plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])
    cbar = plt.colorbar(sp, orientation = 'vertical')         # add the color bar
    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
    cbar.set_label(ylabel, rotation=270, labelpad=20)
    return y_hat

def check_model(model,X,y,ymin,ymax,ylabel,title): # get OOB MSE and cross plot a decision tree 
    y_hat = model.predict(X)
    MSE_test = metrics.mean_squared_error(y,y_hat)
    plt.scatter(y,y_hat,s=None, c='darkorange',marker=None,cmap=cmap,norm=None,vmin=None,vmax=None,alpha=0.8, 
                linewidths=1.0, edgecolors="black")
    plt.title(title); plt.xlabel('Truth: ' + str(ylabel)); plt.ylabel('Estimated: ' + str(ylabel))
    plt.xlim(ymin,ymax); plt.ylim(ymin,ymax)
    plt.plot([ymin,ymax],[ymin,ymax],color='black'); add_grid()
    plt.annotate('Testing MSE: ' + str(f'{(np.round(MSE_test,2)):,.0f}'),[4200,2500])
    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

def display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)
    html_str = ''
    for df in args:
        html_str += df.head().to_html()  # Using .head() for the first few rows
    display(HTML(f'<div style="display: flex;">{html_str}</div>')) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/PGE383")                                        # set the working directory 
```

æ‚¨å°†ä¸å¾—ä¸æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ï¼Œä»¥ä½¿ç”¨æ‚¨è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”åœ¨ Mac ä¸Šæ ¼å¼ä¸åŒï¼ˆä¾‹å¦‚ï¼šâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½æ•°æ®

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›† [unconv_MV.csv](https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv)ï¼Œå®ƒåœ¨æˆ‘çš„ GeoDataSet ä»“åº“ä¸­å¯ç”¨ã€‚å®ƒæ˜¯ä¸€ä¸ªé€—å·åˆ†éš”çš„æ–‡ä»¶ï¼ŒåŒ…å«ï¼š

+   äº•æŒ‡æ•°ï¼ˆæ•´æ•°ï¼‰

+   å­”éš™ç‡ (%)

+   æ¸—é€ç‡ ($mD$)

+   å£°æ³¢é˜»æŠ— ($\frac{kg}{mÂ³} \cdot \frac{m}{s} \cdot 10â¶$)ã€‚

+   å²©è„†æ€§ (%)

+   æ€»æœ‰æœºç¢³ (%)

+   ç»ç’ƒå…‰æ³½åå°„ç‡ (%)

+   åˆå§‹æ°”äº§é‡ï¼ˆ90 å¤©å¹³å‡ï¼‰ï¼ˆMCFPDï¼‰

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸ºâ€˜dfâ€™çš„æ•°æ®æ¡†ä¸­ï¼Œç„¶åé¢„è§ˆä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

**Python æŠ€å·§ï¼šä½¿ç”¨åŒ…ä¸­çš„å‡½æ•°**åªéœ€è¾“å…¥æˆ‘ä»¬åœ¨å¼€å¤´å£°æ˜çš„åŒ…çš„æ ‡ç­¾ï¼š

```py
import pandas as pd 
```

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‘½ä»¤è®¿é—® pandas å‡½æ•°â€˜read_csvâ€™ï¼š

```py
pd.read_csv() 
```

ä½†è¯»å– csv éœ€è¦è¾“å…¥å‚æ•°ã€‚æœ€é‡è¦çš„æ˜¯æ–‡ä»¶åã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–é»˜è®¤å‚æ•°éƒ½å¾ˆå¥½ã€‚å¦‚æœæ‚¨æƒ³æŸ¥çœ‹æ­¤å‡½æ•°çš„æ‰€æœ‰å¯èƒ½å‚æ•°ï¼Œè¯·è®¿é—®[æ­¤å¤„](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)çš„æ–‡æ¡£ã€‚

+   æ–‡æ¡£æ€»æ˜¯å¾ˆæœ‰å¸®åŠ©

+   å¯¹äº Python å‡½æ•°ï¼Œé€šå¸¸æœ‰å¾ˆå¤šçµæ´»æ€§ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨å„ç§è¾“å…¥å‚æ•°æ¥å®ç°ã€‚

æ­¤å¤–ï¼Œç¨‹åºæœ‰ä¸€ä¸ªè¾“å‡ºï¼Œå³ä»æ•°æ®åŠ è½½çš„ pandas DataFrameã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šä»£è¡¨è¯¥æ–°å¯¹è±¡çš„åç§°/å˜é‡ã€‚

```py
df = pd.read_csv("unconv_MV.csv") 
```

è®©æˆ‘ä»¬è¿è¡Œæ­¤å‘½ä»¤æ¥åŠ è½½æ•°æ®ï¼Œç„¶åè¿è¡Œæ­¤å‘½ä»¤æ¥æå–æ•°æ®çš„éšæœºå­é›†ã€‚

```py
df = df.sample(frac=.30, random_state = 73073); 
df = df.reset_index() 
```

## ç‰¹å¾å·¥ç¨‹

è®©æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œä¸€äº›ä¿®æ”¹ä»¥æ”¹è¿›å·¥ä½œæµç¨‹ï¼š

+   **é€‰æ‹©é¢„æµ‹ç‰¹å¾ï¼ˆx2ï¼‰å’Œå“åº”ç‰¹å¾ï¼ˆx1ï¼‰**ï¼Œç¡®ä¿å…ƒæ•°æ®ä¹Ÿæ˜¯ä¸€è‡´çš„ã€‚

+   **å…ƒæ•°æ®ç¼–ç **ï¼Œå¦‚æ¯ä¸ªç‰¹å¾çš„å•ä½ã€æ ‡ç­¾å’Œæ˜¾ç¤ºèŒƒå›´ã€‚

+   **ä¸ºäº†ä¾¿äºå¯è§†åŒ–å‡å°‘æ•°æ®æ•°é‡**ï¼ˆå¦‚æœå›¾è¡¨ä¸Šæœ‰å¤ªå¤šç‚¹å°±éš¾ä»¥çœ‹æ¸…ï¼‰ã€‚

+   **è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²**ä»¥æ¼”ç¤ºå’Œå¯è§†åŒ–ç®€å•çš„è¶…å‚æ•°è°ƒæ•´ã€‚

+   **å‘æ•°æ®æ·»åŠ éšæœºå™ªå£°**ä»¥æ¼”ç¤ºæ¨¡å‹è¿‡æ‹Ÿåˆã€‚åŸå§‹æ•°æ®æ²¡æœ‰é”™è¯¯ï¼Œå¹¶ä¸å®¹æ˜“å±•ç¤ºè¿‡æ‹Ÿåˆã€‚

å‡è®¾è¿™ä¸ªè®¾ç½®æ˜¯æ­£ç¡®çš„ï¼Œé‚£ä¹ˆåº”è¯¥èƒ½å¤Ÿä½¿ç”¨ä»»ä½•æ•°æ®é›†å’Œç‰¹å¾è¿›è¡Œè¿™ä¸ªæ¼”ç¤ºã€‚

+   ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬è¿™é‡Œä¸å±•ç¤ºä»»ä½•ç‰¹å¾é€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œå‰ä¸€ç« ä¸­çš„ k-æœ€è¿‘é‚»åŒ…æ‹¬ä¸€äº›ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œä½†è¯·å‚é˜…ç‰¹å¾é€‰æ‹©ç« èŠ‚ï¼Œä»¥äº†è§£è®¸å¤šå¯èƒ½çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•åŠå…¶ä»£ç ã€‚

## å¯é€‰ï¼šå‘å“åº”ç‰¹å¾æ·»åŠ éšæœºå™ªå£°

æˆ‘ä»¬å¯ä»¥è¿™æ ·åšæ¥è§‚å¯Ÿæ•°æ®å™ªå£°å¯¹è¿‡æ‹Ÿåˆå’Œè¶…å‚æ•°è°ƒæ•´çš„å½±å“ã€‚

+   è¿™æ˜¯ä¸ºäº†ç»éªŒå­¦ä¹ ï¼Œå½“ç„¶æˆ‘ä»¬ä¸ä¼šå‘æˆ‘ä»¬çš„æ•°æ®æ·»åŠ éšæœºå™ªå£°

+   æˆ‘ä»¬è®¾ç½®äº†éšæœºæ•°ç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§

```py
add_error = True                                              # add random error to the response feature
std_error = 1000                                               # standard deviation of random error, for demonstration only
idata = 2

if idata == 1:
    df_load = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv") # load the data from my github repo
    df_load = df_load.sample(frac=.30, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data

elif idata == 2:
    df_load = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv") # load the data 
    df_load = df_load.sample(frac=.70, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data
    df_load = df_load.rename(columns={"Prod": "Production"})

yname = 'Production'; Xname = ['Por','Brittle']               # specify the predictor features (x2) and response feature (x1)
Xmin = [5.0,0.0]; Xmax = [25.0,100.0]                         # set minimums and maximums for visualization 
ymin = 1500.0; ymax = 7000.0
Xlabel = ['Porosity','Brittleness']; ylabel = 'Production'    # specify the feature labels for plotting
Xunit = ['%','%']; yunit = 'MCFPD'
Xlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']
ylabelunit = ylabel + ' (' + yunit + ')'

if add_error == True:                                         # method to add error
    np.random.seed(seed=seed)                                 # set random number seed
    df_load[yname] = df_load[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df_load)) # add noise
    values = df_load._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray

y = pd.DataFrame(df_load[yname])                              # extract selected features as X and y DataFrames
X = df_load[Xname]
df = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) 
```

è®©æˆ‘ä»¬ç¡®ä¿æˆ‘ä»¬å·²ç»é€‰æ‹©äº†åˆç†çš„ç‰¹å¾æ¥æ„å»ºæ¨¡å‹

+   ä¸¤ä¸ªé¢„æµ‹ç‰¹å¾ä¸å…±çº¿æ€§ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´é¢„æµ‹æ¨¡å‹ä¸ç¨³å®š

+   æ¯ä¸ªç‰¹å¾éƒ½ä¸å“åº”ç‰¹å¾ç›¸å…³ï¼Œé¢„æµ‹ç‰¹å¾é€šçŸ¥å“åº”

## è®¡ç®—ç›¸å…³æ€§çŸ©é˜µå’Œä¸å“åº”æ’åçš„ç›¸å…³æ€§

è®©æˆ‘ä»¬ä»ç›¸å…³æ€§åˆ†æå¼€å§‹ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¹‹å‰å£°æ˜çš„å‡½æ•°æ¥è®¡ç®—å’ŒæŸ¥çœ‹ç›¸å…³æ€§çŸ©é˜µä»¥åŠä¸å“åº”ç‰¹å¾çš„å…³è”ã€‚

+   ç›¸å…³æ€§åˆ†æåŸºäºçº¿æ€§å…³ç³»çš„å‡è®¾ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªè‰¯å¥½çš„èµ·ç‚¹

```py
corr_matrix = df.corr()
correlation = corr_matrix.iloc[:,-1].values[:-1]

plt.subplot(121)
plot_corr(corr_matrix,'Correlation Matrix',1.0,0.1)           # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(Xname,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + yname,'Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![_images/f16d805e35f1c39ce293ec74df9132232e9f79c7ca0cb2ab8b63641a714d99c3.png](img/3e93a8978cec90fe97483ce65cda346f.png)

æ³¨æ„ç”±äºæ¯ä¸ªå˜é‡ä¸å…¶è‡ªèº«çš„ç›¸å…³æ€§è€Œäº§ç”Ÿçš„ 1.0 å¯¹è§’çº¿ã€‚

è¿™çœ‹èµ·æ¥ä¸é”™ã€‚å­˜åœ¨ä¸åŒå¤§å°çš„ç›¸å…³æ€§ã€‚å½“ç„¶ï¼Œç›¸å…³ç³»æ•°é™äºçº¿æ€§ç›¸å…³ç¨‹åº¦ã€‚

+   è®©æˆ‘ä»¬çœ‹çœ‹çŸ©é˜µæ•£ç‚¹å›¾ï¼Œä»¥äº†è§£ç‰¹å¾ä¹‹é—´çš„æˆå¯¹å…³ç³»ã€‚

```py
pairgrid = sns.PairGrid(df,vars=Xname+[yname])                # matrix scatter plots
pairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)
pairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle
pairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, 
                              alpha = 1.0, n_levels = 10)
pairgrid.add_legend()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/b641089892114f3044ca3ab9a43c2723da5e4105a26416ee749176d163822c57.png](img/8f6fc0dc9253e57bb674c41069015dde.png)

## è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²

ä¸ºäº†æ–¹ä¾¿å’Œç®€å•ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn çš„éšæœºè®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²ã€‚

```py
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split
df_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)
df_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) 
```

## å¯è§†åŒ– DataFrame

å¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯• DataFrame æ˜¯åœ¨æˆ‘ä»¬æ„å»ºæ¨¡å‹ä¹‹å‰çš„ä¸€ä¸ªæœ‰ç”¨çš„æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ DataFrame çš„â€˜headâ€™æˆå‘˜å‡½æ•°æ¥é¢„è§ˆï¼ˆæ ¼å¼æ•´æ´ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚

```py
print('       Training DataFrame          Testing DataFrame')
display_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display 
```

```py
 Training DataFrame          Testing DataFrame 
```

|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| 86 | 12.83 | 29.87 | 995.700671 |
| 35 | 17.39 | 56.43 | 6060.760806 |
| 75 | 12.23 | 40.67 | 3744.177137 |
| 36 | 13.72 | 40.24 | 4203.470533 |
| 126 | 12.83 | 17.20 | 2917.165695 |
|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| 5 | 15.55 | 58.25 | 5619.930037 |
| 46 | 20.21 | 23.78 | 3897.440411 |
| 96 | 15.07 | 39.39 | 4504.608029 |
| 45 | 12.10 | 63.24 | 3613.953926 |
| 105 | 19.54 | 37.40 | 5314.937997 |

## è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡

åœ¨ DataFrames ä¸­ï¼Œæœ‰è®¸å¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡ã€‚

+   describe å‘½ä»¤ä»¥ç¾è§‚çš„æ•°æ®è¡¨å½¢å¼æä¾›è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€‚

```py
print('            Training DataFrame                      Testing DataFrame')    # custom function for side-by-side summary statistics
display_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) 
```

```py
 Training DataFrame                      Testing DataFrame 
```

|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| count | 105.000000 | 105.000000 | 105.000000 |
| mean | 14.859238 | 48.861143 | 4192.479746 |
| std | 3.057228 | 14.432050 | 1347.391355 |
| min | 7.220000 | 10.940000 | 357.449794 |
| max | 23.550000 | 84.330000 | 7934.478879 |
|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| count | 35.000000 | 35.000000 | 35.000000 |
| mean | 15.011714 | 46.798286 | 4431.830496 |
| std | 3.574467 | 13.380910 | 1487.184992 |
| min | 6.550000 | 20.120000 | 1572.738774 |
| max | 20.860000 | 68.760000 | 7668.639376 |

å¾ˆå¥½ï¼Œæˆ‘ä»¬å·²ç»æ£€æŸ¥äº†æ‘˜è¦ç»Ÿè®¡ã€‚

+   æ²¡æœ‰æ˜æ˜¾çš„é”™è¯¯

+   æ£€æŸ¥æ¯ä¸ªç‰¹å¾çš„å€¼èŒƒå›´ï¼Œä»¥è®¾ç½®å’Œè°ƒæ•´ç»˜å›¾é™åˆ¶ã€‚è§ä¸Šå›¾ã€‚

## å¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²

è®©æˆ‘ä»¬æ£€æŸ¥ä½¿ç”¨ç›´æ–¹å›¾å’Œæ•£ç‚¹å›¾è®­ç»ƒå’Œæµ‹è¯•çš„ä¸€è‡´æ€§å’Œè¦†ç›–èŒƒå›´ã€‚

+   æ£€æŸ¥ä»¥ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•è¦†ç›–äº†å¯èƒ½çš„ç‰¹å¾ç»„åˆèŒƒå›´

+   ç¡®ä¿æµ‹è¯•æ¡ˆä¾‹ä¸ä¼šè¶…å‡ºè®­ç»ƒæ•°æ®çš„èŒƒå›´è¿›è¡Œå¤–æ¨

```py
nbins = 20                                                    # number of histogram bins

plt.subplot(131)                                              # predictor feature #1 histogram
freq1,_,_ = plt.hist(x=df_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=df_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[0]); add_grid()  
plt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   

plt.subplot(132)                                              # predictor feature #2 histogram
freq1,_,_ = plt.hist(x=df_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=df_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[1]); add_grid()  
plt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   

plt.subplot(133)                                              # predictor features #1 and #2 scatter plot
plt.scatter(df_train[Xname[0]],df_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[Xname[0]],df_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.title(Xlabel[1] + ' vs ' +  Xlabel[0])
plt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])
plt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=0.8, wspace=0.3, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![_images/39becd8bd5d8101d8e6567bb5e26eb2b8fbe6499ce965fd4f72fdc3d8b7a3013.png](img/6c7c51e5c24b5320a250e5d2881b5021.png)

æœ‰æ—¶æˆ‘å‘ç°é€šè¿‡æŸ¥çœ‹ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰è€Œä¸æ˜¯ç›´æ–¹å›¾æ¥æ¯”è¾ƒåˆ†å¸ƒæ›´æ–¹ä¾¿ã€‚

+   æˆ‘ä»¬é¿å…ä»»æ„é€‰æ‹©ç›´æ–¹å›¾æŸ±çŠ¶å¤§å°ï¼Œå› ä¸ºç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰ä¸æ•°æ®åˆ†è¾¨ç‡ä¸€è‡´ã€‚

```py
plt.subplot(131)                                              # predictor feature #1 CDF
plot_CDF(X_train[Xname[0]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(X_test[Xname[0]],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(Xlabelunit[0]); plt.xlim(Xmin[0],Xmax[0]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(Xlabel[0] + ' Train and Test CDFs')

plt.subplot(132)                                              # predictor feature #2 CDF
plot_CDF(X_train[Xname[1]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(X_test[Xname[1]],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(Xlabelunit[1]); plt.xlim(Xmin[1],Xmax[1]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(Xlabel[1] + ' Train and Test CDFs')

plt.subplot(133)                                              # response feature CDF
plot_CDF(y_train[yname],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(y_test[yname],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(ylabelunit); plt.xlim(ymin,ymax); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(ylabel + ' Train and Test CDFs')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=0.8, wspace=0.3, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![_images/26e6503a4d7debf39d1789ec8242383e943228b6a4f2f3dfc0a0f10cd99a5b46.png](img/e1017bcd0dfe07284cc05b94b7dd3861.png)

å†æ¬¡å¼ºè°ƒï¼Œåˆ†å¸ƒè¡¨ç°è‰¯å¥½ï¼Œ

+   æˆ‘ä»¬æ— æ³•è§‚å¯Ÿåˆ°æ˜æ˜¾çš„é—´éš™æˆ–æˆªæ–­ã€‚

+   æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„è¦†ç›–èŒƒå›´

è®©æˆ‘ä»¬çœ‹çœ‹å­”éš™ç‡ä¸è„†æ€§ä¹‹é—´çš„æ•£ç‚¹å›¾ï¼Œç‚¹æ ¹æ®ç”Ÿäº§é‡ç€è‰²ã€‚

```py
plt.subplot(111)                                              # visualize the train and test data in predictor feature space
im = plt.scatter(X_train[Xname[0]],X_train[Xname[1]],s=None, c=y_train[yname], marker='o', cmap=cmap, 
    norm=None, vmin=ymin, vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors="black", label = 'Train')
plt.scatter(X_test[Xname[0]],X_test[Xname[1]],s=None, c=y_test[yname], marker='s', cmap=cmap, 
    norm=None, vmin=ymin, vmax=ymax, alpha=0.5, linewidths=0.3, edgecolors="black", label = 'Test')
plt.title('Training ' + ylabel + ' vs. ' + Xlabel[1] + ' and ' + Xlabel[0]); 
plt.xlabel(Xlabel[0] + ' (' + Xunit[0] + ')'); plt.ylabel(Xlabel[1] + ' (' + Xunit[1] + ')')
plt.xlim(Xmin[0],Xmax[0]); plt.ylim(Xmin[1],Xmax[1]); plt.legend(loc = 'upper right'); add_grid()
cbar = plt.colorbar(im, orientation = 'vertical')
cbar.set_label(ylabel + ' (' + yunit + ')', rotation=270, labelpad=20)
cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/1a847dd38c7158f07f58197cf7bfb88e46738dacb7622cd79a3a267a25a94494.png](img/79e12ff1ac255ff9811cd9900912d141.png)

## åŸºäºæ ‘çš„æå‡

è¦æ‰§è¡ŒåŸºäºæ ‘çš„æå‡ï¼Œæ¢¯åº¦æå‡æ ‘å›å½’ï¼Œæˆ‘ä»¬ï¼š

1.  è®¾ç½®æˆ‘ä»¬æ¨¡å‹çš„è¶…å‚æ•°

```py
params = {
    'loss': 'squared_error'                                 # L2 Norm - least squares
    'n_estimators': 1,                                      # number of trees
    'max_depth': 1,                                         # maximum depth per tree
    'learning_rate': 1,                                     
    'criterion': 'squared_error'                            # tree construction criterion
} 
```

1.  å®ä¾‹åŒ–æ¨¡å‹

```py
boost_tree = GradientBoostingRegressor(**params) 
```

1.  è®­ç»ƒæ¨¡å‹

```py
boot_tree.fit(X = predictors, y = response) 
```

1.  å¯è§†åŒ–æ¨¡å‹ç»“æœåœ¨ç‰¹å¾ç©ºé—´ä¸Šçš„ç»“æœï¼ˆç”±äºæˆ‘ä»¬åªæœ‰ä¸¤ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œè¿™å¾ˆå®¹æ˜“åšåˆ°ï¼‰

## æå‡æ¼”ç¤º

ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å°†æ ‘çš„æœ€å¤§æ·±åº¦è®¾ç½®ä¸º 1 å’Œ 6 æ£µåŸºäºæ ‘çš„æå‡å›å½’æ ‘ã€‚

+   æ¯æ£µæ ‘åªæœ‰ä¸€ä¸ªåˆ†å‰²ï¼Œç§°ä¸ºå†³ç­–æ ‘æ¡©ã€‚è¿™å°†é˜²æ­¢é¢„æµ‹ç‰¹å¾ä¹‹é—´çš„äº¤äº’ï¼Œå¹¶ä¸”æ˜“äºè§£é‡Š

ä½ åº”è¯¥èƒ½å¤Ÿè§‚å¯Ÿåˆ°æ ‘çš„ç´¯åŠ æ€§è´¨ï¼Œå…ˆçœ‹ç¬¬ä¸€æ£µæ ‘ï¼Œç„¶åæ˜¯ç¬¬ä¸€æ£µåŠ ä¸Šç¬¬äºŒæ£µï¼Œä¾æ­¤ç±»æ¨ã€‚

+   å›æƒ³ä¸€ä¸‹ï¼Œä¼°è®¡æ˜¯å¤šä¸ªæ ‘çš„æ±‚å’Œ

+   ç”±äºæˆ‘ä»¬åœ¨ç¬¬ä¸€æ£µæ ‘ä¹‹åæ‹Ÿåˆæ¢¯åº¦ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰è´Ÿæ•°å’Œæ­£æ•°çš„ä¼°è®¡

+   åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›å®é™…ä¸Šæ˜¯è´Ÿæ•°çš„ç”Ÿäº§ä¼°è®¡

```py
params = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 1,                       
    'learning_rate': 1.0,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

num_trees = np.linspace(1,6,6)                              # build a list of numbers of trees 
boosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries

index = 1
for num_tree in num_trees:                                  # loop over number of trees
    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))
    boosting_models[index-1].fit(X = X_train, y = y_train)
    score.append(boosting_models[index-1].score(X = X_test, y = y_test))
    plt.subplot(2,3,index)
    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))
    index = index + 1

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3); plt.show() 
```

![_images/0689fdfd411d646efca1490a036e9354bc12c7bd5797bac40f5b0295fb0f11a3.png](img/b2a16fd01b6c4a7a3f642f8cf90844b3.png)

æ³¨æ„ï¼Œä¸æ•°æ®å­˜åœ¨æ˜¾è‘—çš„å¤±é…

+   æˆ‘ä»¬åªä½¿ç”¨äº†æœ€å¤š 6 ä¸ªå†³ç­–æ ‘æ¡©ï¼ˆ1 ä¸ªå†³ç­–æ ‘ï¼‰

è®©æˆ‘ä»¬æ£€æŸ¥ä¿ç•™çš„æµ‹è¯•æ•°æ®é›†çš„äº¤å‰éªŒè¯ç»“æœã€‚

```py
index = 1
for num_tree in num_trees:                                  # check over number of trees
    plt.subplot(2,3,index)
    check_model(boosting_models[index-1],X_test,y_test,ymin,ymax,ylabelunit,'Tree-Based Boosting with ' + str(int(num_tree)) + ' Trees')
    index = index + 1
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/c19d0d925995b408d0e14db0f66b52ee.png)

å½“ç„¶ï¼Œç”¨ä¸€æ£µæ ‘æˆ‘ä»¬è¡¨ç°ç›¸å½“ç³Ÿç³•ï¼Œä½†å½“æˆ‘ä»¬è¾¾åˆ° 6 ä¸ªæ ‘æ¡©æ ‘æ—¶ï¼Œæˆ‘ä»¬å‡ ä¹å°†å‡æ–¹è¯¯å·®å‡åŠã€‚

## æ˜¯æ—¶å€™ç§æ¤æ›´å¤šæ ‘æœ¨äº†

ç°åœ¨æˆ‘ä»¬å°†å±•ç¤ºåœ¨æˆ‘ä»¬çš„åŸºäºæ ‘çš„æå‡æ¨¡å‹ä¸­åˆ©ç”¨æ›´å¤šæ ‘æœ¨çš„ç»“æœã€‚

+   æˆ‘ä»¬ä»ç„¶å°†ä¸ç®€å•çš„å†³ç­–æ ‘æ¡©ä¸€èµ·å·¥ä½œï¼Œä¸ç”¨æ‹…å¿ƒï¼Œæˆ‘ä»¬ç¨åä¼šæ·»åŠ æ›´å¤š

```py
params = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 1,                                         # maximum depth per tree
    'learning_rate': 1,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

num_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees 
boosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries

index = 1
for num_tree in num_trees:                                  # loop over number of trees
    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))
    boosting_models[index-1].fit(X = X_train, y = y_train)
    score.append(boosting_models[index-1].score(X = X_test, y = y_test))
    plt.subplot(2,3,index)
    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))
    index = index + 1

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/3baea417052c8c32978d193e173228b1.png)

çœ‹åˆ°æ ¼å­å›¾æ¡ˆå—ï¼Ÿè¿™æ˜¯ç”±äºä½¿ç”¨äº†å†³ç­–æ ‘æ¡©ï¼Œå¹¶ä¸”ï¼š

+   ä¸€ä¸ªåŠ æ€§æ¨¡å‹

+   æ‰€æœ‰æ¨¡å‹éƒ½å¯¹æ‰€æœ‰é¢„æµ‹æœ‰è´¡çŒ®

çœ‹åˆ°æš—åŒºå’Œäº®åŒºäº†å—ï¼Ÿ

+   åŠ æ€§æ¨¡å‹å¯èƒ½ä¼šè¶…å‡ºæ•°æ®èŒƒå›´å¤–è¿›è¡Œå¤–æ¨

è®©æˆ‘ä»¬ç”¨æˆ‘ä»¬çš„æµ‹è¯•æ•°æ®æ¥äº¤å‰éªŒè¯ï¼Œçœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ›´å¤šæ ‘æœ¨çš„æƒ…å†µä¸‹å¦‚ä½•æ”¹è¿›ã€‚

```py
index = 1
for num_tree in num_trees:                                  # check over number of trees
    plt.subplot(2,3,index)
    check_model(boosting_models[index-1],X_test,y_test,ymin,ymax,ylabelunit,'Tree-Based Boosting with ' + str(int(num_tree)) + ' Trees')
    index = index + 1
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/3f37a775a250322ae1595efdbfe1669d.png)

å¤§çº¦ 20 æ£µæ ‘æ—¶ï¼Œæˆ‘ä»¬è·å¾—æœ€ä½³æ€§èƒ½ï¼Œç„¶åå¼€å§‹ä¸‹é™ï¼Œæˆ‘ä»¬å¯èƒ½å¼€å§‹è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚

## è¶…è¶Šå†³ç­–æ ‘æ¡©

å¦‚å‰æ‰€è¿°ï¼Œä½¿ç”¨å†³ç­–æ ‘æ¡©ï¼Œæˆ‘ä»¬é˜²æ­¢äº†ç‰¹å¾ä¹‹é—´çš„äº¤äº’ã€‚

è®©æˆ‘ä»¬æ‰©å±•åˆ°æ ‘æ·±åº¦ä¸º 2

+   ä¸¤ä¸ªåµŒå¥—å†³ç­–å¯¼è‡´ 4 ä¸ªç»ˆç«¯èŠ‚ç‚¹

```py
params = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 2,                                         # maximum depth per tree
    'learning_rate': 1,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

num_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees 
boosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries

index = 1
for num_tree in num_trees:                                  # loop over number of trees
    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))
    boosting_models[index-1].fit(X = X_train, y = y_train)
    score.append(boosting_models[index-1].score(X = X_test, y = y_test))
    plt.subplot(2,3,index)
    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))
    index = index + 1

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/65161de226c63aae698a6005291afae4.png)

æˆ‘ä»¬ç°åœ¨æœ‰æ›´å¤šçš„çµæ´»æ€§ã€‚

+   ç”¨ä¸€æ£µæ ‘æˆ‘ä»¬æœ‰ 4 ä¸ªç»ˆç«¯èŠ‚ç‚¹ï¼ˆåŒºåŸŸï¼‰

+   ä»…ç”¨ 6 æ£µæ ‘ï¼Œæˆ‘ä»¬å·²ç»æ•æ‰åˆ°ä¸€äº›å¤æ‚ç‰¹å¾

è®©æˆ‘ä»¬å†æ¬¡å¢åŠ æ ‘çš„æ·±åº¦ã€‚

```py
params = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 3,                                         # maximum depth per tree
    'learning_rate': 1,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

num_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees 
boosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries

index = 1
for num_tree in num_trees:                                  # loop over number of trees
    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))
    boosting_models[index-1].fit(X = X_train, y = y_train)
    score.append(boosting_models[index-1].score(X = X_test, y = y_test))
    plt.subplot(2,3,index)
    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))
    index = index + 1

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/b36175195239341c3b7ea8c842eea5b7.png)

å†æ¥ä¸€æ¬¡ï¼Œä½¿ç”¨æ·±åº¦ä¸º 4-8 çš„æ ‘å¾ˆå¸¸è§ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å°è¯• 5ã€‚

```py
params = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 5,                                         # maximum depth per tree
    'learning_rate': 1,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

num_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees 
boosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries

index = 1
for num_tree in num_trees:                                  # loop over number of trees
    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))
    boosting_models[index-1].fit(X = X_train, y = y_train)
    score.append(boosting_models[index-1].score(X = X_test, y = y_test))
    plt.subplot(2,3,index)
    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))
    index = index + 1

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/d2ae5319fcaa86e15b8c1c77df29902e.png)

è®©æˆ‘ä»¬ç”¨æµ‹è¯•æ•°æ®æ¥äº¤å‰éªŒè¯æ¨¡å‹ã€‚

```py
index = 1
for num_tree in num_trees:                                  # check over number of trees
    plt.subplot(2,3,index)
    check_model(boosting_models[index-1],X_test,y_test,ymin,ymax,ylabelunit,'Tree-Based Boosting with ' + str(int(num_tree)) + ' Trees')
    index = index + 1
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/cdbb3edec9e81f128298ed05c7cd6189.png)

å½“æ ‘çš„æœ€å¤§æ·±åº¦ä¸º 5 æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ€§èƒ½æ—©æœŸè¾¾åˆ°å³°å€¼ï¼Œå¢åŠ æ›´å¤šæ ‘æœ¨æ²¡æœ‰å½±å“ã€‚

+   å½“ç„¶ï¼Œè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªå½»åº•çš„åˆ†æ

è®©æˆ‘ä»¬å°è¯•æ›´å½»åº•çš„æ–¹æ³•

+   æˆ‘ä»¬å°†ä½¿ç”¨ $1,\ldots,100$ æ£µæ ‘è¿›è¡Œäº¤å‰éªŒè¯ï¼Œæœ€å¤§æ ‘æ·±åº¦ä¸º $1, 2, 3, 10$ã€‚

+   æˆ‘ä»¬è¿˜é™ä½äº†å­¦ä¹ ç‡ï¼Œæˆ‘ä¹‹å‰æé«˜äº†å®ƒä»¥æ”¾å¤§è¾“å‡ºçš„å·®å¼‚ä»¥è¿›è¡Œæ¼”ç¤ºï¼Œä½†ç°åœ¨æˆ‘ä»¬æƒ³çœ‹åˆ°æœ€å¥½çš„æ¨¡å‹ã€‚

```py
num_trees = np.linspace(1,200,200)
max_features = 1
MSE1_list = []; MSE2_list = []; MSE3_list = []; MSE4_list = [] 

params1 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 1,                                         # maximum depth per tree
    'learning_rate': 0.2,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

params2 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 2,                                         # maximum depth per tree
    'learning_rate': 0.2,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

params3 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 3,                                         # maximum depth per tree
    'learning_rate': 0.2,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

params4 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 10,                                        # maximum depth per tree
    'learning_rate': 0.2,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

index = 1
for num_tree in num_trees:                                  # loop over number of trees in our random forest
    boosting_model1 = GradientBoostingRegressor(n_estimators=int(num_tree),**params1).fit(X = X_train, y = y_train)
    y_test1_hat = boosting_model1.predict(X_test); MSE1_list.append(metrics.mean_squared_error(y_test,y_test1_hat))

    boosting_model2 = GradientBoostingRegressor(n_estimators=int(num_tree),**params2).fit(X = X_train, y = y_train)
    y_test2_hat = boosting_model2.predict(X_test); MSE2_list.append(metrics.mean_squared_error(y_test,y_test2_hat))

    boosting_model3 = GradientBoostingRegressor(n_estimators=int(num_tree),**params3).fit(X = X_train, y = y_train)
    y_test3_hat = boosting_model3.predict(X_test); MSE3_list.append(metrics.mean_squared_error(y_test,y_test3_hat))

    boosting_model4 = GradientBoostingRegressor(n_estimators=int(num_tree),**params4).fit(X = X_train, y = y_train)
    y_test4_hat = boosting_model4.predict(X_test); MSE4_list.append(metrics.mean_squared_error(y_test,y_test4_hat))

    index = index + 1

plt.subplot(111)                                            # plot jackknife results for all cases
plt.scatter(num_trees,MSE1_list,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 1")
plt.scatter(num_trees,MSE2_list,s=None,c='blue',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 2")
plt.scatter(num_trees,MSE3_list,s=None,c='black',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 3")
plt.scatter(num_trees,MSE4_list,s=None,c='green',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 10")

plt.title('Testing Mean Square Error vs. Number of Trees'); plt.xlabel('Number of Trees'); plt.ylabel('Test Mean Square Error')
plt.xlim(0,200); plt.legend(loc='lower right'); add_grid()
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/c2ee3e9f993c37a05e91acb8b4ce5baf.png)

éå¸¸æœ‰è¶£ï¼š

+   éšç€æ ‘æ·±åº¦çš„å¢åŠ ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯èƒ½ä¼šæ”¹è¿›

+   æ›´æ·±çš„æ ‘æ·±åº¦éœ€è¦æ›´å°‘çš„æ ‘æ¥æé«˜å‡†ç¡®æ€§

+   å½“æ ‘æ·±åº¦ä¸º 2 å’Œ 3 æ—¶ï¼Œæ¨¡å‹è¡¨ç°ç›¸åŒï¼Œåœ¨ 10-15 æ£µæ ‘ä¹‹åè¶‹äºå¹³ç¨³ï¼Œå®ƒä»¬å¯¹è¿‡åº¦æ‹Ÿåˆæœ‰æŠµæŠ—åŠ›

+   åœ¨æ ‘æ·±åº¦ä¸º 10 çš„æƒ…å†µä¸‹ï¼Œæ ‘çš„æ•°é‡å¯¹æ¨¡å‹æ€§èƒ½æ²¡æœ‰å½±å“

## æ¢¯åº¦ä¸‹é™è¶…å‚æ•°

å­¦ä¹ ç‡æŒ‰æ¯”ä¾‹æ”¾å¤§æ¯ä¸ªæ·»åŠ æ ‘å¯¹æ•´ä½“æ¨¡å‹é¢„æµ‹çš„é™„åŠ å½±å“ã€‚

+   è¾ƒä½çš„å­¦ä¹ ç‡å°†å‡æ…¢æ”¶æ•›åˆ°è§£çš„é€Ÿåº¦

+   è¾ƒä½çš„å­¦ä¹ ç‡å°†å¸®åŠ©æˆ‘ä»¬ä¸ä¼šè·³è¿‡æœ€ä¼˜è§£

æˆ‘ä»¬ä¸ä¼šåœ¨è¿™ä¸ªé—®é¢˜ä¸ŠèŠ±è´¹å¤ªå¤šæ—¶é—´ï¼Œä½†è®©æˆ‘ä»¬è¯•ç€æ”¹å˜å­¦ä¹ ç‡ã€‚

```py
learning_rates = np.arange(0.01,1.0,0.01)
MSE1_list = []; MSE2_list = []; MSE3_list = []; MSE4_list = [] 

params1 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 5,                                         # maximum depth per tree
    'n_estimators': 40,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

params2 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 5,                                         # maximum depth per tree
    'n_estimators': 40,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

params3 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 5,                                         # maximum depth per tree
    'n_estimators': 40,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

params4 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 5,                                        # maximum depth per tree
    'n_estimators': 40,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

index = 1
for learning_rate in learning_rates:                                  # loop over number of trees in our random forest
    boosting_model1 = GradientBoostingRegressor(learning_rate = learning_rate,**params1).fit(X = X_train, y = y_train)
    y_test1_hat = boosting_model1.predict(X_test); MSE1_list.append(metrics.mean_squared_error(y_test,y_test1_hat))

    boosting_model2 = GradientBoostingRegressor(learning_rate = learning_rate,**params2).fit(X = X_train, y = y_train)
    y_test2_hat = boosting_model2.predict(X_test); MSE2_list.append(metrics.mean_squared_error(y_test,y_test2_hat))

    boosting_model3 = GradientBoostingRegressor(learning_rate = learning_rate,**params3).fit(X = X_train, y = y_train)
    y_test3_hat = boosting_model3.predict(X_test); MSE3_list.append(metrics.mean_squared_error(y_test,y_test3_hat))

    boosting_model4 = GradientBoostingRegressor(learning_rate = learning_rate,**params4).fit(X = X_train, y = y_train)
    y_test4_hat = boosting_model4.predict(X_test); MSE4_list.append(metrics.mean_squared_error(y_test,y_test4_hat))

    index = index + 1

plt.subplot(111)                                            # plot jackknife results for all cases
plt.scatter(learning_rates,MSE1_list,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 1")
plt.scatter(learning_rates,MSE2_list,s=None,c='blue',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 2")
plt.scatter(learning_rates,MSE3_list,s=None,c='black',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 3")
plt.scatter(learning_rates,MSE4_list,s=None,c='green',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 10")

plt.title('Testing Mean Square Error vs. Learning Rate'); plt.xlabel('Learning Rate'); plt.ylabel('Test Mean Square Error')
plt.xlim(0.0,1.0); plt.legend(loc='upper left'); add_grid()
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/d130f276273e2b117adb7bf8d1768618.png)

å†æ¬¡å¼ºè°ƒï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„ç»“æœã€‚

+   æ— è®ºæ ‘å¤æ‚åº¦å¦‚ä½•ï¼Œç¼“æ…¢å­¦ä¹ æ€»æ˜¯æ›´å¥½çš„ï¼

## æ¸…æ´ã€ç´§å‡‘çš„æœºå™¨å­¦ä¹ ä»£ç çš„æœºå™¨å­¦ä¹ ç®¡é“

ç®¡é“æ˜¯ scikit-learn ä¸­çš„ä¸€ä¸ªç±»ï¼Œå…è®¸å°è£…ä¸€ç³»åˆ—æ•°æ®å‡†å¤‡å’Œå»ºæ¨¡æ­¥éª¤

+   ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†ç®¡é“è§†ä¸ºæˆ‘ä»¬é«˜åº¦ç²¾ç®€çš„å·¥ä½œæµç¨‹ä¸­çš„ä¸€ä¸ªå¯¹è±¡

ç®¡é“ç±»å…è®¸æˆ‘ä»¬ï¼š

+   æé«˜ä»£ç å¯è¯»æ€§å¹¶ä¿æŒä¸€åˆ‡äº•ç„¶æœ‰åº

+   ä½¿ç”¨éå¸¸å°‘çš„å¯è¯»ä»£ç è¡Œæ„å»ºå®Œæ•´çš„å·¥ä½œæµç¨‹

+   é¿å…å¸¸è§çš„æµç¨‹é—®é¢˜ï¼Œå¦‚æ•°æ®æ³„éœ²ã€æµ‹è¯•æ•°æ®å‘ŠçŸ¥æ¨¡å‹å‚æ•°è®­ç»ƒ

+   æŠ½è±¡å¸¸è§çš„æœºå™¨å­¦ä¹ å»ºæ¨¡ï¼Œä¸“æ³¨äºæ„å»ºå°½å¯èƒ½å¥½çš„æ¨¡å‹

åŸºæœ¬å“²å­¦æ˜¯å°†æœºå™¨å­¦ä¹ è§†ä¸ºä¸€ç§ç»„åˆæœç´¢ï¼Œä»¥æ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼ˆAutoMLï¼‰

æ›´å¤šä¿¡æ¯è¯·å‚é˜…æˆ‘å…³äº[æœºå™¨å­¦ä¹ ç®¡é“](https://www.youtube.com/watch?v=tYrPs8s1l9U&list=PLG19vXLQHvSAufDFgZEFAYQEwMJXklnQV&index=5)çš„å½•éŸ³è®²åº§å’Œä¸€ä»½è¯¦ç»†è®°å½•çš„æ¼”ç¤º[æœºå™¨å­¦ä¹ ç®¡é“å·¥ä½œæµç¨‹](http://localhost:8892/notebooks/OneDrive%20-%20The%20University%20of%20Texas%20at%20Austin/Courses/Workflows/PythonDataBasics_Pipelines.ipynb)ã€‚

```py
x1 = 0.25; x2 = 0.3                                           # predictor values for the prediction

pipe_boosting = Pipeline([                                      # the machine learning workflow as a pipeline object
    ('boosting', GradientBoostingRegressor())
])

params = {                                                    # the machine learning workflow method's parameters to search
    'boosting__learning_rate': np.arange(0.1,2.0,0.2),
    'boosting__n_estimators': np.arange(2,40,4,dtype = int),
}

tuned_boosting = GridSearchCV(pipe_boosting,params,scoring = 'neg_mean_squared_error', # hyperparameter tuning w. grid search k-fold cross validation 
                             refit = True)
tuned_boosting.fit(X,y)                                         # tune the model with k-fold and then train the model will the data 

print('Tuned hyperparameter: ' + str(tuned_boosting.best_params_))

estimate = tuned_boosting.predict([[x1,x2]])[0]                 # make a prediction (no tuning shown)
print('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + 
      str(round(estimate,1)) + ' ' + yunit)                     # print results 
```

```py
Tuned hyperparameter: {'boosting__learning_rate': 0.30000000000000004, 'boosting__n_estimators': 10}
Estimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1979.7 MCFPD 
```

## æ³¨é‡Š

å¸Œæœ›æ‚¨è§‰å¾—è¿™ä¸€ç« æœ‰å¸®åŠ©ã€‚è¿˜æœ‰æ›´å¤šå¯ä»¥åšçš„å’Œè®¨è®ºçš„ï¼Œæˆ‘æœ‰å¾ˆå¤šèµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ï¼Œ

*è¿ˆå…‹å°”*

## ä½œè€…ï¼š

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ *æ–°é¢–çš„æ•°æ®åˆ†æã€åœ°çƒç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ åœ°ä¸‹è§£å†³æ–¹æ¡ˆ*

è¿ˆå…‹å°”æ‹¥æœ‰è¶…è¿‡ 17 å¹´çš„åœ°ä¸‹å’¨è¯¢ã€ç ”ç©¶å’Œå¼€å‘ç»éªŒï¼Œä»–å› å¯¹æ•™å­¦çš„çƒ­æƒ…å’Œå¯¹å¢å¼ºå·¥ç¨‹å¸ˆå’Œåœ°çƒç§‘å­¦å®¶åœ¨åœ°ä¸‹èµ„æºå¼€å‘ä¸­å½±å“çš„çƒ­æƒ…è€Œé‡è¿”å­¦æœ¯ç•Œã€‚

æƒ³äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹é“¾æ¥ï¼š

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­çš„åº”ç”¨åœ°çƒç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°æ‚¨çš„å…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹æ¥å¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æˆ‘å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°æ‚¨ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢

## æ›´å¤šèµ„æºè¯·è®¿é—®ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## æ¢¯åº¦æå‡æ ‘çš„åŠ¨æœº

åœ¨æˆ‘ä»¬èƒ½å¤Ÿç†è§£æ¢¯åº¦æå‡æ ‘ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦äº†è§£å†³ç­–æ ‘ã€‚ä»¥ä¸‹æ˜¯å†³ç­–æ ‘çš„å…³é”®æ¦‚å¿µã€‚

### å†³ç­–æ ‘

**é¢„æµ‹**

+   ä¼°è®¡ä¸€ä¸ªå‡½æ•° $\hat{f}$ï¼Œä½¿å¾—æˆ‘ä»¬èƒ½ä»ä¸€ç»„é¢„æµ‹ç‰¹å¾ $X_1,\ldots,X_m$ ä¸­é¢„æµ‹å“åº”ç‰¹å¾ $Y$ã€‚

+   é¢„æµ‹çš„å½¢å¼ä¸º $\hat{Y} = \hat{f}(X_1,\ldots,X_m)$

**ç›‘ç£å­¦ä¹ **

+   å“åº”ç‰¹å¾æ ‡ç­¾ï¼Œ$Y$ï¼Œåœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¸­éƒ½æ˜¯å¯ç”¨çš„

**åŸºäºå†³ç­–æ ‘é›†æˆ**

è¿™äº›æ˜¯ä¸å†³ç­–æ ‘ç›¸å…³çš„æ¦‚å¿µã€‚

**ç‰¹å¾ç©ºé—´çš„åˆ†å±‚äºŒåˆ†åˆ†å‰²**

åŸºæœ¬æ€æƒ³æ˜¯å°†é¢„æµ‹ç©ºé—´ï¼Œ$ğ‘‹_1,\ldots,X_m$ï¼Œåˆ’åˆ†ä¸º $J$ ä¸ªäº’æ–¥ã€ç©·å°½çš„åŒºåŸŸ

+   **äº’æ–¥** â€“ ä»»ä½•é¢„æµ‹ç‰¹å¾çš„ç»„åˆåªå±äºå•ä¸ªåŒºåŸŸï¼Œ$R_j$

+   **ç©·å°½** â€“ æ‰€æœ‰é¢„æµ‹ç‰¹å¾çš„ç»„åˆéƒ½å±äºä¸€ä¸ªåŒºåŸŸï¼Œ$R_j$ï¼ŒåŒºåŸŸè¦†ç›–æ•´ä¸ªç‰¹å¾ç©ºé—´ï¼ˆè€ƒè™‘çš„å˜é‡çš„èŒƒå›´ï¼‰

å¯¹äºåŒºåŸŸ $R_j$ ä¸­çš„æ¯ä¸ªè§‚æµ‹å€¼ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„é¢„æµ‹ï¼Œ$\hat{Y}(R_j)$

ä¾‹å¦‚ï¼Œä»å­”éš™ç‡ï¼Œ${X_1}$ é¢„æµ‹äº§é‡ï¼Œ$\hat{Y}$

+   åœ¨ä¸€ä¸ª mD ç‰¹å¾ç©ºé—´å†…çš„æ•°æ®ä¸­ï¼Œ$X_1,\ldots,X_m$ï¼Œæ‰¾åˆ°æœ€å¤§åŒ–ä¸¤ä¸ªç±»åˆ«ä¹‹é—´å·®è·çš„è¾¹ç•Œ

+   æ–°æ¡ˆä¾‹æ ¹æ®å®ƒä»¬ç›¸å¯¹äºè¿™ä¸ªè¾¹ç•Œçš„ç›¸å¯¹ä½ç½®è¿›è¡Œåˆ†ç±»

**æ ‘æ„å»ºè¿‡ç¨‹**

æ ‘æ˜¯ä»ä¸Šå¾€ä¸‹æ„å»ºçš„ã€‚æˆ‘ä»¬ä»ä¸€ä¸ªè¦†ç›–æ•´ä¸ªç‰¹å¾ç©ºé—´çš„å•ä¸€åŒºåŸŸå¼€å§‹ï¼Œç„¶åè¿›è¡Œä¸€ç³»åˆ—çš„åˆ†å‰²ã€‚

+   **æ‰«ææ‰€æœ‰å¯èƒ½çš„åˆ†å‰²** åœ¨æ‰€æœ‰åŒºåŸŸå’Œæ‰€æœ‰ç‰¹å¾ä¸Šã€‚

+   **è´ªå©ªä¼˜åŒ–** è¯¥æ–¹æ³•é€šè¿‡åœ¨ä»»æ„ç‰¹å¾ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªåˆ†å‰²ï¼ˆåˆ†å‰²ï¼‰ï¼Œä»¥æœ€å°åŒ–æ‰€æœ‰è®­ç»ƒæ•°æ® $y_i$ åœ¨æ‰€æœ‰åŒºåŸŸ $j = 1,\ldots,J$ ä¸Šçš„æ®‹å·®å¹³æ–¹å’Œã€‚

$$ RSS = \sum^{J}_{j=1} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})Â² $$

+   **åœæ­¢æ ‡å‡†** é€šå¸¸åŸºäºæ¯ä¸ªåŒºåŸŸçš„æœ€å°è®­ç»ƒæ•°æ®é‡ä»¥è¿›è¡Œç¨³å¥ä¼°è®¡ï¼Œä»¥åŠ/æˆ–ä¸‹ä¸€ä¸ªåˆ†å‰²çš„æœ€å° RSS å‡å°‘ã€‚

ç°åœ¨æˆ‘ä»¬å¯ä»¥ä»‹ç»åŸºäºå†³ç­–æ ‘æ¦‚å¿µçš„æ¢¯åº¦æå‡æ ‘ã€‚

### å†³ç­–æ ‘

**é¢„æµ‹**

+   ä¼°è®¡ä¸€ä¸ªå‡½æ•° $\hat{f}$ï¼Œä»¥ä¾¿æˆ‘ä»¬ä»ä¸€ä¸ªé¢„æµ‹ç‰¹å¾é›† $X_1,\ldots,X_m$ ä¸­é¢„æµ‹å“åº”ç‰¹å¾ $Y$ã€‚

+   é¢„æµ‹å½¢å¼ä¸º $\hat{Y} = \hat{f}(X_1,\ldots,X_m)$ã€‚

**ç›‘ç£å­¦ä¹ **

+   å“åº”ç‰¹å¾æ ‡ç­¾ $Y$ åœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¸­å¯ç”¨ã€‚

**åŸºäºå†³ç­–æ ‘é›†æˆ**

è¿™äº›æ˜¯ä¸å†³ç­–æ ‘ç›¸å…³çš„æ¦‚å¿µã€‚

**ç‰¹å¾ç©ºé—´çš„åˆ†å±‚äºŒåˆ†åˆ†å‰²**

åŸºæœ¬æ€æƒ³æ˜¯å°†é¢„æµ‹ç©ºé—´ $ğ‘‹_1,\ldots,X_m$ åˆ’åˆ†ä¸º $J$ ä¸ªç›¸äº’æ’æ–¥ã€ç©·å°½çš„åŒºåŸŸã€‚

+   **ç›¸äº’æ’æ–¥** â€“ ä»»ä½•é¢„æµ‹å™¨çš„ç»„åˆåªå±äºå•ä¸ªåŒºåŸŸ $R_j$ã€‚

+   **ç©·å°½** â€“ æ‰€æœ‰é¢„æµ‹å™¨çš„ç»„åˆå±äºä¸€ä¸ªåŒºåŸŸ $R_j$ï¼ŒåŒºåŸŸè¦†ç›–æ•´ä¸ªç‰¹å¾ç©ºé—´ï¼ˆè€ƒè™‘çš„å˜é‡çš„èŒƒå›´ï¼‰ã€‚

å¯¹äºåŒºåŸŸ $R_j$ ä¸­çš„æ¯ä¸ªè§‚æµ‹å€¼ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„é¢„æµ‹ï¼Œ$\hat{Y}(R_j)$ã€‚

ä¾‹å¦‚ï¼Œä»å­”éš™ç‡ ${X_1}$ é¢„æµ‹ç”Ÿäº§ï¼Œ$\hat{Y}$ã€‚

+   ç»™å®šä¸€ä¸ª mD ç‰¹å¾ç©ºé—´å†…çš„æ•°æ®ï¼Œ$X_1,\ldots,X_m$ï¼Œæ‰¾åˆ°è¾¹ç•Œä»¥æœ€å¤§åŒ–ä¸¤ä¸ªç±»åˆ«ä¹‹é—´çš„å·®è·ã€‚

+   æ–°æ¡ˆä¾‹æ ¹æ®å®ƒä»¬ç›¸å¯¹äºè¿™ä¸ªè¾¹ç•Œçš„ä½ç½®è¿›è¡Œåˆ†ç±»ã€‚

**æ ‘æ„å»ºè¿‡ç¨‹**

æ ‘æ˜¯ä»ä¸Šå¾€ä¸‹æ„å»ºçš„ã€‚æˆ‘ä»¬ä»ä¸€ä¸ªè¦†ç›–æ•´ä¸ªç‰¹å¾ç©ºé—´çš„å•ä¸€åŒºåŸŸå¼€å§‹ï¼Œç„¶åè¿›è¡Œä¸€ç³»åˆ—çš„åˆ†å‰²ã€‚

+   **æ‰«ææ‰€æœ‰å¯èƒ½çš„åˆ†å‰²** åœ¨æ‰€æœ‰åŒºåŸŸå’Œæ‰€æœ‰ç‰¹å¾ä¸Šã€‚

+   **è´ªå©ªä¼˜åŒ–** è¯¥æ–¹æ³•é€šè¿‡åœ¨ä»»æ„ç‰¹å¾ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªåˆ†å‰²ï¼ˆåˆ†å‰²ï¼‰ï¼Œä»¥æœ€å°åŒ–æ‰€æœ‰è®­ç»ƒæ•°æ® $y_i$ åœ¨æ‰€æœ‰åŒºåŸŸ $j = 1,\ldots,J$ ä¸Šçš„æ®‹å·®å¹³æ–¹å’Œã€‚

$$ RSS = \sum^{J}_{j=1} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})Â² $$

+   **åœæ­¢æ ‡å‡†** é€šå¸¸åŸºäºæ¯ä¸ªåŒºåŸŸçš„æœ€å°è®­ç»ƒæ•°æ®é‡ä»¥è¿›è¡Œç¨³å¥ä¼°è®¡ï¼Œä»¥åŠ/æˆ–ä¸‹ä¸€ä¸ªåˆ†å‰²çš„æœ€å° RSS å‡å°‘ã€‚

ç°åœ¨æˆ‘ä»¬å¯ä»¥ä»‹ç»åŸºäºå†³ç­–æ ‘æ¦‚å¿µçš„æ¢¯åº¦æå‡æ ‘ã€‚

## å¢é‡æ¨¡å‹

å¢é‡åœ°å°†å¤šä¸ªå¼±å­¦ä¹ å™¨åº”ç”¨äºæ„å»ºæ›´å¼ºçš„å­¦ä¹ å™¨ã€‚

+   å¼±å­¦ä¹ å™¨æ˜¯æŒ‡ä»…æä¾›ç•¥ä¼˜äºéšæœºé€‰æ‹©çš„é¢„æµ‹çš„å­¦ä¹ å™¨ã€‚

æˆ‘ä¼šç”¨æ–‡å­—å’Œæ–¹ç¨‹æ¥è§£é‡Šè¿™ä¸ªæ–¹æ³•ã€‚

+   å»ºç«‹ä¸€ä¸ªè¯¯å·®ç‡é«˜çš„ç®€å•æ¨¡å‹ï¼Œæ¨¡å‹å¯èƒ½ç›¸å½“ä¸å‡†ç¡®ï¼Œä½†ä¼šæœç€æ­£ç¡®çš„æ–¹å‘ç§»åŠ¨

+   è®¡ç®—æ¨¡å‹çš„è¯¯å·®

+   æ‹Ÿåˆå¦ä¸€ä¸ªæ¨¡å‹æ¥æ‹Ÿåˆè¯¯å·®

+   è®¡ç®—ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªæ¨¡å‹ç»„åˆçš„è¯¯å·®

+   é‡å¤ç›´åˆ°è¾¾åˆ°æ‰€éœ€çš„ç²¾åº¦æˆ–æŸäº›å…¶ä»–åœæ­¢æ ‡å‡†

ä» $X_1,\ldots,X_m$ é¢„æµ‹ $Y$ çš„ä¸€èˆ¬å·¥ä½œæµç¨‹æ˜¯ï¼š

+   å»ºç«‹ä¸€ä¸ªå¼±å­¦ä¹ è€…ä» $X_1,\ldots,X_m$ é¢„æµ‹ $Y$ï¼Œä»è®­ç»ƒæ•°æ® $x_{i,j}$ é¢„æµ‹ $\hat{F}_k(X)$ã€‚

+   å¾ªç¯éå†æ‰€éœ€ä¼°è®¡å™¨çš„æ•°é‡ï¼Œ$k = 1,\ldots,K$

    1.  è®¡ç®—è®­ç»ƒæ•°æ®ä¸­çš„æ®‹å·®ï¼Œ$h_k(x_{i}) = y_i - \hat{F}_k(x_{i})$

    1.  ä½¿ç”¨å¦ä¸€å‘¨çš„å­¦ä¹ è€…ä» $X_1,\ldots,X_m$ é¢„æµ‹ $h_k$ï¼Œä»è®­ç»ƒæ•°æ® $x_{i,j}$ é¢„æµ‹ $\hat{F}_k(X)$ã€‚

æˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å• $K$ æ¨¡å‹çš„å±‚æ¬¡ç»“æ„ã€‚

+   æ¯ä¸ªæ¨¡å‹éƒ½åŸºäºå‰ä¸€ä¸ªæ¨¡å‹æ¥æé«˜ç²¾åº¦

æˆ‘ä»¬çš„å›å½’ä¼°è®¡å™¨æ˜¯ $K$ ä¸ªç®€å•æ¨¡å‹çš„æ€»å’Œã€‚

$$ \hat{Y} =\sum_{k=1}^{K} F_k(X_1,\ldots,X_m) $$

## æ¢¯åº¦æå‡æ–¹æ³•

å¦‚æœä½ çœ‹ä¸€ä¸‹ä¹‹å‰çš„æ–¹æ³•ï¼Œå®ƒå¾ˆæ¸…æ¥šåœ°å¯ä»¥æ˜ å°„åˆ°ä¸€ä¸ªæ¢¯åº¦ä¸‹é™é—®é¢˜

åœ¨æ¯ä¸ªæ­¥éª¤ $k$ï¼Œéƒ½åœ¨æ‹Ÿåˆä¸€ä¸ªæ¨¡å‹ï¼Œç„¶åè®¡ç®—è¯¯å·®ï¼Œ$h_k(X_1,\ldots,X_m)$ã€‚

æˆ‘ä»¬å¯ä»¥åˆ†é…ä¸€ä¸ªæŸå¤±å‡½æ•°

$$ L\left(y,F(X)\right) = \frac{\left(y - F(X)\right)Â²}{2} $$

å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›æœ€å°åŒ– $\ell2$ æŸå¤±å‡½æ•°ï¼š

$$ J = \sum_{i=1}^{n} L\left(y_i, F_k(X) \right) $$

é€šè¿‡è°ƒæ•´æˆ‘ä»¬çš„æ¨¡å‹ç»“æœæ¥è¦†ç›–æˆ‘ä»¬çš„è®­ç»ƒæ•°æ® $F(x_1), F(x_2),\ldots,F(x_n)$ã€‚

æˆ‘ä»¬å¯ä»¥æ±‚è¯¯å·®ç›¸å¯¹äºæˆ‘ä»¬æ¨¡å‹çš„åå¯¼æ•°ã€‚

$$ \frac{\partial J}{\partial F(x_i)} = F(x_i) - y_i $$

æˆ‘ä»¬å¯ä»¥å°†æ®‹å·®è§£é‡Šä¸ºè´Ÿæ¢¯åº¦ã€‚

$$ y_i - F(x_i) = -1 \frac{\partial J}{\partial F(x_i)} $$

å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªæ¢¯åº¦ä¸‹é™é—®é¢˜ï¼š

$$ F_{k+1}(X_i) = F_k(X_i) + h(X_i) $$$$ F_{k+1}(X_i) = F_k(X_i) + y_i - F_k(X_i) $$$$ F_{k+1}(X_i) = F_k(X_i) - 1 \frac{\partial J}{\partial F_k(X_i)} $$

çš„ä¸€èˆ¬å½¢å¼ï¼š

$$ \phi_{k+1} = \phi_k - \rho \frac{\partial J}{\partial \phi_k} $$

å…¶ä¸­ $\phi_k$ æ˜¯å½“å‰çŠ¶æ€ï¼Œ$\rho$ æ˜¯å­¦ä¹ ç‡ï¼Œ$J$ æ˜¯æŸå¤±å‡½æ•°ï¼Œè€Œ $\phi_{k+1}$ æ˜¯æˆ‘ä»¬ä¼°è®¡å™¨çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚

å¦‚æœæˆ‘ä»¬æŠŠè®­ç»ƒæ•°æ®ä¸­çš„æ®‹å·®çœ‹ä½œæ¢¯åº¦ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æ˜¯åœ¨æ‰§è¡Œæ¢¯åº¦ä¸‹é™ã€‚

+   æ‹Ÿåˆä¸€ç³»åˆ—æ¨¡å‹åˆ°è´Ÿæ¢¯åº¦

é€šè¿‡å°†é—®é¢˜è§†ä¸ºæ¢¯åº¦ä¸‹é™é—®é¢˜ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåº”ç”¨å„ç§æŸå¤±å‡½æ•°

+   $\ell2$ æ˜¯æˆ‘ä»¬çš„ $\frac{\left(y - F(X)\right)Â²}{2}$ å®é™…ä¸Šå¾ˆå®ç”¨ï¼Œä½†å¼‚å¸¸å€¼é²æ£’æ€§ä¸å¼º

$$ - 1 \frac{\partial J}{\partial F_k(X_i)} = y_i - F_k(X_i) $$

+   $\ell1$ æ˜¯æˆ‘ä»¬çš„ $|y - F(X)|$ å¯¹å¼‚å¸¸å€¼æ›´é²æ£’

$$ - 1 \frac{\partial J}{\partial F_k(X_i)} = sign(y_i - F_k(X_i)) $$

+   è¿˜æœ‰å…¶ä»–åƒ Huber Loss è¿™æ ·çš„æ–¹æ³•ã€‚

**å¯è§£é‡Šæ€§**

ä¸å†³ç­–æ ‘ç›¸æ¯”ï¼Œé›†æˆæ–¹æ³•é™ä½äº†å¯è§£é‡Šæ€§ã€‚æé«˜æ¨¡å‹å¯è§£é‡Šæ€§çš„ä¸€ä¸ªå·¥å…·æ˜¯ç‰¹å¾é‡è¦æ€§ã€‚

æˆ‘ä»¬é€šè¿‡è®¡ç®—å¹³å‡æ¥è®¡ç®—å˜é‡é‡è¦æ€§ï¼š

+   å¯¹äºæ‰€æœ‰æ¶‰åŠæ¯ä¸ªé¢„æµ‹ç‰¹å¾çš„åˆ†æï¼Œå›å½’çš„æ®‹å·®å¹³æ–¹å’Œå‡å°‘

+   å¯¹äºæ‰€æœ‰æ¶‰åŠæ¯ä¸ªé¢„æµ‹ç‰¹å¾çš„åˆ†æï¼ŒGini æŒ‡æ•°çš„é™ä½

è¿™ä¸¤ä¸ªéƒ½æ˜¯æ ‡å‡†åŒ–åˆ°ç‰¹å¾æ€»å’Œä¸º 1.0ã€‚

## åˆ†ç±»

å“åº”æ˜¯ä¸€ä¸ªå¯èƒ½çš„æœ‰é™ç±»åˆ«é›†åˆã€‚

+   å¯¹äºæ¯ä¸ªè®­ç»ƒæ•°æ®ï¼ŒçœŸå®æƒ…å†µæ˜¯è¯¥è§‚å¯Ÿç±»åˆ« 100%çš„æ¦‚ç‡ï¼Œå¦åˆ™ä¸º 0%

+   ä½¿ç”¨å†³ç­–æ ‘ä¼°è®¡æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡

+   ä½¿ç”¨çœŸå®å’Œä¼°è®¡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚åº¦é‡ä½œä¸ºæŸå¤±å‡½æ•°æ¥æœ€å°åŒ–

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»é€šè¿‡ Anaconda 3 å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = True
import os                                                     # to set current working directory 
import math                                                   # square root operator
import numpy as np                                            # arrays and matrix math
import scipy.stats as st                                      # statistical methods
import pandas as pd                                           # DataFrames
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import seaborn as sns                                         # for matrix scatter plots
from sklearn.tree import DecisionTreeRegressor                # decision tree method
from sklearn.ensemble import GradientBoostingRegressor        # tree-based gradient boosting
from sklearn.tree import _tree                                # for accessing tree information
from sklearn import metrics                                   # measures to check our models
from sklearn.preprocessing import StandardScaler              # standardize the features
from sklearn.tree import export_graphviz                      # graphical visualization of trees
from sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning
from sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline
from sklearn import metrics                                   # measures to check our models
from sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation
from sklearn.model_selection import train_test_split          # train and test split
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # suppress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½å¿…é¡»é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ›´å¤šå¸®åŠ©ï¼Œè¯·å‚é˜…å„è‡ªçš„åŒ…æ–‡æ¡£ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰å‡ ä¸ªå‡½æ•°æ¥ç®€åŒ–ç»˜åˆ¶ç›¸å…³çŸ©é˜µå’Œå†³ç­–æ ‘å›å½’æ¨¡å‹çš„å¯è§†åŒ–ã€‚

```py
def comma_format(x, pos):
    return f'{int(x):,}'

def feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot
    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1
    plt.plot(pred,metric,color='black',zorder=20)
    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)
    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  
    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)
    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();
    return

def plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix 
    my_colormap = plt.get_cmap('RdBu_r', 256)          
    newcolors = my_colormap(np.linspace(0, 1, 256))
    white = np.array([256/256, 256/256, 256/256, 1])
    white_low = int(128 - mask*128); white_high = int(128+mask*128)
    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)
    newcmp = ListedColormap(newcolors)
    m = corr_matrix.shape[0]
    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)
    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()
    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()
    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
    plt.colorbar(im, orientation = 'vertical')
    plt.title(title)
    for i in range(0,m):
        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')
        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')
    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def plot_CDF(data,color,alpha=1.0,lw=1,ls='solid',label='none'):
    cumprob = (np.linspace(1,len(data),len(data)))/(len(data)+1)
    plt.scatter(np.sort(data),cumprob,c=color,alpha=alpha,edgecolor='black',lw=lw,ls=ls,label=label,zorder=10)
    plt.plot(np.sort(data),cumprob,c=color,alpha=alpha,lw=lw,ls=ls,zorder=8)

def visualize_model(model,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,
                         ymax,title,Xname,yname,Xlabel,ylabel,annotate=True):# plots the data points and the decision tree prediction 
    cmap = plt.cm.inferno
    X1plot_step = (Xmax[0] - Xmin[0])/300.0; X2plot_step = -1*(Xmax[1] - Xmin[1])/300.0 # resolution of the model visualization
    XX1, XX2 = np.meshgrid(np.arange(Xmin[0], Xmax[0], X1plot_step), # set up the mesh
                     np.arange(Xmax[1], Xmin[1], X2plot_step))
    y_hat = model.predict(np.c_[XX1.ravel(), XX2.ravel()])    # predict with our trained model over the mesh
    y_hat = y_hat.reshape(XX1.shape)

    plt.imshow(y_hat,interpolation=None, aspect="auto", extent=[Xmin[0],Xmax[0],Xmin[1],Xmax[1]], 
        vmin=ymin,vmax=ymax,alpha = 1.0,cmap=cmap,zorder=1)
    sp = plt.scatter(X1_train,X2_train,s=None, c=y_train, marker='o', cmap=cmap, 
        norm=None, vmin=ymin, vmax=ymax, alpha=0.6, linewidths=0.3, edgecolors="black", label = 'Train',zorder=10)
    plt.scatter(X1_test,X2_test,s=None, c=y_test, marker='s', cmap=cmap, 
        norm=None, vmin=ymin, vmax=ymax, alpha=0.3, linewidths=0.3, edgecolors="black", label = 'Test',zorder=10)
    plt.title(title); plt.xlabel(Xlabel[0]); plt.ylabel(Xlabel[1])
    plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])
    cbar = plt.colorbar(sp, orientation = 'vertical')         # add the color bar
    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
    cbar.set_label(ylabel, rotation=270, labelpad=20)
    return y_hat

def check_model(model,X,y,ymin,ymax,ylabel,title): # get OOB MSE and cross plot a decision tree 
    y_hat = model.predict(X)
    MSE_test = metrics.mean_squared_error(y,y_hat)
    plt.scatter(y,y_hat,s=None, c='darkorange',marker=None,cmap=cmap,norm=None,vmin=None,vmax=None,alpha=0.8, 
                linewidths=1.0, edgecolors="black")
    plt.title(title); plt.xlabel('Truth: ' + str(ylabel)); plt.ylabel('Estimated: ' + str(ylabel))
    plt.xlim(ymin,ymax); plt.ylim(ymin,ymax)
    plt.plot([ymin,ymax],[ymin,ymax],color='black'); add_grid()
    plt.annotate('Testing MSE: ' + str(f'{(np.round(MSE_test,2)):,.0f}'),[4200,2500])
    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

def display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)
    html_str = ''
    for df in args:
        html_str += df.head().to_html()  # Using .head() for the first few rows
    display(HTML(f'<div style="display: flex;">{html_str}</div>')) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/PGE383")                                        # set the working directory 
```

æ‚¨å°†éœ€è¦æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ä¸ºæ‚¨çš„è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”æ ¼å¼åœ¨ Mac ä¸Šä¸åŒï¼ˆä¾‹å¦‚ï¼šâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½æ•°æ®

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›† [unconv_MV.csv](https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv)ï¼Œå®ƒåœ¨æˆ‘çš„ GeoDataSet ä»“åº“ä¸­å¯ç”¨ã€‚å®ƒæ˜¯ä¸€ä¸ªé€—å·åˆ†éš”çš„æ–‡ä»¶ï¼ŒåŒ…å«ï¼š

+   äº•æŒ‡æ•°ï¼ˆæ•´æ•°ï¼‰

+   å­”éš™ç‡ (%) 

+   æ¸—é€ç‡ ($mD$)

+   å£°é˜»æŠ— ($\frac{kg}{mÂ³} \cdot \frac{m}{s} \cdot 10â¶$)

+   å‰ªåˆ‡ç‡ (%) 

+   æ€»æœ‰æœºç¢³ (%) 

+   ç»ç’ƒè´¨åå°„ç‡ (%) 

+   åˆå§‹æ°”äº§é‡ï¼ˆ90 å¤©å¹³å‡ï¼‰(MCFPD)

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸ºâ€˜dfâ€™çš„æ•°æ®æ¡†ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

**Python æŠ€å·§ï¼šä½¿ç”¨åŒ…ä¸­çš„å‡½æ•°**åªéœ€è¾“å…¥æˆ‘ä»¬åœ¨å¼€å¤´å£°æ˜çš„åŒ…çš„æ ‡ç­¾ï¼š

```py
import pandas as pd 
```

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‘½ä»¤è®¿é—® pandas å‡½æ•°â€˜read_csvâ€™ï¼š

```py
pd.read_csv() 
```

ä½†æ˜¯ read csv éœ€è¦è¾“å…¥å‚æ•°ã€‚æœ€é‡è¦çš„ä¸€ä¸ªæ˜¯æ–‡ä»¶åã€‚å¯¹äºæˆ‘ä»¬çš„æƒ…å†µï¼Œæ‰€æœ‰å…¶ä»–é»˜è®¤å‚æ•°éƒ½å¾ˆå¥½ã€‚å¦‚æœæ‚¨æƒ³æŸ¥çœ‹æ­¤å‡½æ•°çš„æ‰€æœ‰å¯èƒ½å‚æ•°ï¼Œè¯·è®¿é—®[è¿™é‡Œ](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)çš„æ–‡æ¡£ã€‚

+   æ–‡æ¡£æ€»æ˜¯å¾ˆæœ‰å¸®åŠ©

+   Python å‡½æ•°é€šå¸¸æœ‰å¾ˆå¤šçµæ´»æ€§ï¼Œè¿™å¯ä»¥é€šè¿‡ä½¿ç”¨å„ç§è¾“å…¥å‚æ•°æ¥å®ç°

æ­¤å¤–ï¼Œç¨‹åºæœ‰ä¸€ä¸ªè¾“å‡ºï¼Œä¸€ä¸ªä»æ•°æ®åŠ è½½çš„ pandas DataFrameã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šä»£è¡¨è¯¥æ–°å¯¹è±¡çš„åå­—/å˜é‡ã€‚

```py
df = pd.read_csv("unconv_MV.csv") 
```

è®©æˆ‘ä»¬è¿è¡Œæ­¤å‘½ä»¤æ¥åŠ è½½æ•°æ®ï¼Œç„¶åè¿è¡Œæ­¤å‘½ä»¤æ¥æå–æ•°æ®çš„éšæœºå­é›†ã€‚

```py
df = df.sample(frac=.30, random_state = 73073); 
df = df.reset_index() 
```

## ç‰¹å¾å·¥ç¨‹

è®©æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œä¸€äº›ä¿®æ”¹ä»¥æ”¹è¿›å·¥ä½œæµç¨‹ï¼š

+   **é€‰æ‹©é¢„æµ‹ç‰¹å¾ï¼ˆx2ï¼‰å’Œå“åº”ç‰¹å¾ï¼ˆx1ï¼‰**ï¼Œç¡®ä¿å…ƒæ•°æ®ä¹Ÿä¸€è‡´ã€‚

+   **å…ƒæ•°æ®**ç¼–ç ï¼Œå¦‚æ¯ä¸ªç‰¹å¾çš„å•ä½ã€æ ‡ç­¾å’Œæ˜¾ç¤ºèŒƒå›´ã€‚

+   **å‡å°‘æ•°æ®æ•°é‡**ä»¥æ–¹ä¾¿å¯è§†åŒ–ï¼ˆå¦‚æœå›¾è¡¨ä¸Šæœ‰å¤ªå¤šç‚¹å°±éš¾ä»¥çœ‹åˆ°ï¼‰ã€‚

+   **è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²**ä»¥æ¼”ç¤ºå’Œå¯è§†åŒ–ç®€å•çš„è¶…å‚æ•°è°ƒæ•´ã€‚

+   **å‘æ•°æ®æ·»åŠ éšæœºå™ªå£°**ä»¥æ¼”ç¤ºæ¨¡å‹è¿‡æ‹Ÿåˆã€‚åŸå§‹æ•°æ®æ˜¯æ— é”™è¯¯çš„ï¼Œå¹¶ä¸å®¹æ˜“å±•ç¤ºè¿‡æ‹Ÿåˆã€‚

å‡è®¾è¿™è¢«æ­£ç¡®è®¾ç½®ï¼Œäººä»¬åº”è¯¥èƒ½å¤Ÿä½¿ç”¨ä»»ä½•æ•°æ®é›†å’Œç‰¹å¾è¿›è¡Œæ­¤æ¼”ç¤ºã€‚

+   ä¸ºäº†ç®€æ´ï¼Œæˆ‘ä»¬åœ¨æ­¤ä¸å±•ç¤ºä»»ä½•ç‰¹å¾é€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œå‰ä¸€ç« ä¸­çš„ k è¿‘é‚»åŒ…æ‹¬ä¸€äº›ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œä½†è¯·å‚é˜…ç‰¹å¾é€‰æ‹©ç« èŠ‚ï¼Œä»¥äº†è§£è®¸å¤šå¯èƒ½çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•åŠå…¶ä»£ç ã€‚

## å¯é€‰ï¼šå‘å“åº”ç‰¹å¾æ·»åŠ éšæœºå™ªå£°

æˆ‘ä»¬å¯ä»¥è¿™æ ·åšæ¥è§‚å¯Ÿæ•°æ®å™ªå£°å¯¹è¿‡æ‹Ÿåˆå’Œè¶…å‚æ•°è°ƒæ•´çš„å½±å“ã€‚

+   è¿™æ˜¯ä¸ºäº†ç»éªŒå­¦ä¹ ï¼Œå½“ç„¶æˆ‘ä»¬ä¸ä¼šå‘æˆ‘ä»¬çš„æ•°æ®æ·»åŠ éšæœºå™ªå£°

+   æˆ‘ä»¬è®¾ç½®éšæœºæ•°ç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§

```py
add_error = True                                              # add random error to the response feature
std_error = 1000                                               # standard deviation of random error, for demonstration only
idata = 2

if idata == 1:
    df_load = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv") # load the data from my github repo
    df_load = df_load.sample(frac=.30, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data

elif idata == 2:
    df_load = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv") # load the data 
    df_load = df_load.sample(frac=.70, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data
    df_load = df_load.rename(columns={"Prod": "Production"})

yname = 'Production'; Xname = ['Por','Brittle']               # specify the predictor features (x2) and response feature (x1)
Xmin = [5.0,0.0]; Xmax = [25.0,100.0]                         # set minimums and maximums for visualization 
ymin = 1500.0; ymax = 7000.0
Xlabel = ['Porosity','Brittleness']; ylabel = 'Production'    # specify the feature labels for plotting
Xunit = ['%','%']; yunit = 'MCFPD'
Xlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']
ylabelunit = ylabel + ' (' + yunit + ')'

if add_error == True:                                         # method to add error
    np.random.seed(seed=seed)                                 # set random number seed
    df_load[yname] = df_load[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df_load)) # add noise
    values = df_load._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray

y = pd.DataFrame(df_load[yname])                              # extract selected features as X and y DataFrames
X = df_load[Xname]
df = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) 
```

è®©æˆ‘ä»¬ç¡®ä¿æˆ‘ä»¬å·²é€‰æ‹©äº†åˆç†çš„ç‰¹å¾æ¥æ„å»ºæ¨¡å‹

+   ä¸¤ä¸ªé¢„æµ‹ç‰¹å¾ä¸å…±çº¿ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´é¢„æµ‹æ¨¡å‹ä¸ç¨³å®š

+   æ¯ä¸ªç‰¹å¾éƒ½ä¸å“åº”ç‰¹å¾ç›¸å…³ï¼Œé¢„æµ‹ç‰¹å¾å‘å“åº”ç‰¹å¾æä¾›ä¿¡æ¯

## è®¡ç®—ç›¸å…³æ€§çŸ©é˜µå’Œä¸å“åº”ç‰¹å¾çš„ç›¸å…³æ€§æ’å

è®©æˆ‘ä»¬ä»ç›¸å…³æ€§åˆ†æå¼€å§‹ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¹‹å‰å£°æ˜çš„å‡½æ•°è®¡ç®—å¹¶æŸ¥çœ‹ç›¸å…³æ€§çŸ©é˜µå’Œä¸å“åº”ç‰¹å¾çš„ç›¸å…³æ€§ã€‚

+   ç›¸å…³æ€§åˆ†æåŸºäºçº¿æ€§å…³ç³»çš„å‡è®¾ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªè‰¯å¥½çš„å¼€ç«¯

```py
corr_matrix = df.corr()
correlation = corr_matrix.iloc[:,-1].values[:-1]

plt.subplot(121)
plot_corr(corr_matrix,'Correlation Matrix',1.0,0.1)           # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(Xname,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + yname,'Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/3e93a8978cec90fe97483ce65cda346f.png)

æ³¨æ„ç”±äºæ¯ä¸ªå˜é‡ä¸å…¶è‡ªèº«çš„ç›¸å…³æ€§è€Œäº§ç”Ÿçš„ 1.0 å¯¹è§’çº¿ã€‚

è¿™çœ‹èµ·æ¥ä¸é”™ã€‚å­˜åœ¨ä¸åŒç¨‹åº¦çš„ç›¸å…³æ€§ã€‚å½“ç„¶ï¼Œç›¸å…³ç³»æ•°ä»…é™äºçº¿æ€§ç›¸å…³ç¨‹åº¦ã€‚

+   è®©æˆ‘ä»¬çœ‹çœ‹çŸ©é˜µæ•£ç‚¹å›¾ï¼Œä»¥äº†è§£ç‰¹å¾ä¹‹é—´çš„æˆå¯¹å…³ç³»ã€‚

```py
pairgrid = sns.PairGrid(df,vars=Xname+[yname])                # matrix scatter plots
pairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)
pairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle
pairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, 
                              alpha = 1.0, n_levels = 10)
pairgrid.add_legend()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/8f6fc0dc9253e57bb674c41069015dde.png)

## è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²

ä¸ºäº†æ–¹ä¾¿å’Œç®€å•ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn çš„éšæœºè®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²ã€‚

```py
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split
df_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)
df_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) 
```

## å¯è§†åŒ– DataFrame

åœ¨æˆ‘ä»¬æ„å»ºæ¨¡å‹ä¹‹å‰ï¼Œå¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯• DataFrame æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨â€˜headâ€™ DataFrame æˆå‘˜å‡½æ•°æ¥é¢„è§ˆï¼ˆæ ¼å¼æ•´æ´ç¾è§‚ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚

```py
print('       Training DataFrame          Testing DataFrame')
display_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display 
```

```py
 Training DataFrame          Testing DataFrame 
```

|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| 86 | 12.83 | 29.87 | 995.700671 |
| 35 | 17.39 | 56.43 | 6060.760806 |
| 75 | 12.23 | 40.67 | 3744.177137 |
| 36 | 13.72 | 40.24 | 4203.470533 |
| 126 | 12.83 | 17.20 | 2917.165695 |
|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| 5 | 15.55 | 58.25 | 5619.930037 |
| 46 | 20.21 | 23.78 | 3897.440411 |
| 96 | 15.07 | 39.39 | 4504.608029 |
| 45 | 12.10 | 63.24 | 3613.953926 |
| 105 | 19.54 | 37.40 | 5314.937997 |

## è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯

åœ¨ DataFrame ä¸­ï¼Œæœ‰è®¸å¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ã€‚

+   `describe` å‘½ä»¤ä»¥ä¸€ä¸ªç¾è§‚çš„æ•°æ®è¡¨å½¢å¼æä¾›è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€‚

```py
print('            Training DataFrame                      Testing DataFrame')    # custom function for side-by-side summary statistics
display_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) 
```

```py
 Training DataFrame                      Testing DataFrame 
```

|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| è®¡æ•° | 105.000000 | 105.000000 | 105.000000 |
| å¹³å‡å€¼ | 14.859238 | 48.861143 | 4192.479746 |
| æ ‡å‡†å·® | 3.057228 | 14.432050 | 1347.391355 |
| æœ€å°å€¼ | 7.220000 | 10.940000 | 357.449794 |
| æœ€å¤§å€¼ | 23.550000 | 84.330000 | 7934.478879 |
|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| è®¡æ•° | 35.000000 | 35.000000 | 35.000000 |
| å¹³å‡å€¼ | 15.011714 | 46.798286 | 4431.830496 |
| æ ‡å‡†å·® | 3.574467 | 13.380910 | 1487.184992 |
| æœ€å°å€¼ | 6.550000 | 20.120000 | 1572.738774 |
| æœ€å¤§å€¼ | 20.860000 | 68.760000 | 7668.639376 |

å¾ˆå¥½ï¼Œæˆ‘ä»¬å·²ç»æ£€æŸ¥äº†æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ã€‚

+   æ²¡æœ‰æ˜æ˜¾çš„æ˜æ˜¾é—®é¢˜

+   æ£€æŸ¥æ¯ä¸ªç‰¹å¾çš„å€¼èŒƒå›´ä»¥è®¾ç½®å’Œè°ƒæ•´ç»˜å›¾é™åˆ¶ã€‚è§ä¸Šæ–‡ã€‚

## å¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²

è®©æˆ‘ä»¬æ£€æŸ¥ä½¿ç”¨ç›´æ–¹å›¾å’Œæ•£ç‚¹å›¾æ¥æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•çš„ä¸€è‡´æ€§å’Œè¦†ç›–ç‡ã€‚

+   æ£€æŸ¥ä»¥ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•è¦†ç›–äº†å¯èƒ½çš„ç‰¹å¾ç»„åˆèŒƒå›´

+   ç¡®ä¿æˆ‘ä»¬ä¸æ˜¯é€šè¿‡æµ‹è¯•æ¡ˆä¾‹å¤–æ¨è®­ç»ƒæ•°æ®

```py
nbins = 20                                                    # number of histogram bins

plt.subplot(131)                                              # predictor feature #1 histogram
freq1,_,_ = plt.hist(x=df_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=df_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[0]); add_grid()  
plt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   

plt.subplot(132)                                              # predictor feature #2 histogram
freq1,_,_ = plt.hist(x=df_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=df_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[1]); add_grid()  
plt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   

plt.subplot(133)                                              # predictor features #1 and #2 scatter plot
plt.scatter(df_train[Xname[0]],df_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[Xname[0]],df_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.title(Xlabel[1] + ' vs ' +  Xlabel[0])
plt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])
plt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=0.8, wspace=0.3, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![_images/39becd8bd5d8101d8e6567bb5e26eb2b8fbe6499ce965fd4f72fdc3d8b7a3013.png](img/6c7c51e5c24b5320a250e5d2881b5021.png)

æœ‰æ—¶æˆ‘å‘ç°é€šè¿‡æŸ¥çœ‹ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰è€Œä¸æ˜¯ç›´æ–¹å›¾æ¥æ¯”è¾ƒåˆ†å¸ƒæ›´æ–¹ä¾¿ã€‚

+   æˆ‘ä»¬é¿å…äº†é€‰æ‹©ç›´æ–¹å›¾ bin å¤§å°çš„ä»»æ„æ€§ï¼Œå› ä¸ºç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰ä¸æ•°æ®åˆ†è¾¨ç‡ä¸€è‡´ã€‚

```py
plt.subplot(131)                                              # predictor feature #1 CDF
plot_CDF(X_train[Xname[0]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(X_test[Xname[0]],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(Xlabelunit[0]); plt.xlim(Xmin[0],Xmax[0]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(Xlabel[0] + ' Train and Test CDFs')

plt.subplot(132)                                              # predictor feature #2 CDF
plot_CDF(X_train[Xname[1]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(X_test[Xname[1]],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(Xlabelunit[1]); plt.xlim(Xmin[1],Xmax[1]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(Xlabel[1] + ' Train and Test CDFs')

plt.subplot(133)                                              # response feature CDF
plot_CDF(y_train[yname],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(y_test[yname],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(ylabelunit); plt.xlim(ymin,ymax); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(ylabel + ' Train and Test CDFs')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=0.8, wspace=0.3, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![_images/26e6503a4d7debf39d1789ec8242383e943228b6a4f2f3dfc0a0f10cd99a5b46.png](img/e1017bcd0dfe07284cc05b94b7dd3861.png)

ä¸€æ¬¡åˆä¸€æ¬¡ï¼Œåˆ†å¸ƒè¡¨ç°å¾—å¾ˆå¥½ï¼Œ

+   æˆ‘ä»¬æ— æ³•è§‚å¯Ÿåˆ°æ˜æ˜¾çš„ç¼ºå£æˆ–æˆªæ–­ã€‚

+   æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„è¦†ç›–ç‡

è®©æˆ‘ä»¬çœ‹çœ‹å­”éš™ç‡ä¸è„†æ€§ä¹‹é—´çš„æ•£ç‚¹å›¾ï¼Œç‚¹æŒ‰ç”Ÿäº§é‡ç€è‰²ã€‚

```py
plt.subplot(111)                                              # visualize the train and test data in predictor feature space
im = plt.scatter(X_train[Xname[0]],X_train[Xname[1]],s=None, c=y_train[yname], marker='o', cmap=cmap, 
    norm=None, vmin=ymin, vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors="black", label = 'Train')
plt.scatter(X_test[Xname[0]],X_test[Xname[1]],s=None, c=y_test[yname], marker='s', cmap=cmap, 
    norm=None, vmin=ymin, vmax=ymax, alpha=0.5, linewidths=0.3, edgecolors="black", label = 'Test')
plt.title('Training ' + ylabel + ' vs. ' + Xlabel[1] + ' and ' + Xlabel[0]); 
plt.xlabel(Xlabel[0] + ' (' + Xunit[0] + ')'); plt.ylabel(Xlabel[1] + ' (' + Xunit[1] + ')')
plt.xlim(Xmin[0],Xmax[0]); plt.ylim(Xmin[1],Xmax[1]); plt.legend(loc = 'upper right'); add_grid()
cbar = plt.colorbar(im, orientation = 'vertical')
cbar.set_label(ylabel + ' (' + yunit + ')', rotation=270, labelpad=20)
cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/1a847dd38c7158f07f58197cf7bfb88e46738dacb7622cd79a3a267a25a94494.png](img/79e12ff1ac255ff9811cd9900912d141.png)

## åŸºäºæ ‘çš„å¢å¼ºå­¦ä¹ 

è¦æ‰§è¡ŒåŸºäºæ ‘çš„å¢å¼ºå­¦ä¹ ï¼Œæ¢¯åº¦æå‡æ ‘å›å½’ï¼Œæˆ‘ä»¬ï¼š

1.  ä¸ºæˆ‘ä»¬çš„æ¨¡å‹è®¾ç½®è¶…å‚æ•°

```py
params = {
    'loss': 'squared_error'                                 # L2 Norm - least squares
    'n_estimators': 1,                                      # number of trees
    'max_depth': 1,                                         # maximum depth per tree
    'learning_rate': 1,                                     
    'criterion': 'squared_error'                            # tree construction criterion
} 
```

1.  å®ä¾‹åŒ–æ¨¡å‹

```py
boost_tree = GradientBoostingRegressor(**params) 
```

1.  è®­ç»ƒæ¨¡å‹

```py
boot_tree.fit(X = predictors, y = response) 
```

1.  å¯è§†åŒ–æ¨¡å‹ç»“æœåœ¨ç‰¹å¾ç©ºé—´ä¸Šçš„ç»“æœï¼ˆç”±äºæˆ‘ä»¬åªæœ‰ä¸¤ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œè¿™å¾ˆå®¹æ˜“åšåˆ°ï¼‰

## å¢å¼ºå­¦ä¹ æ¼”ç¤º

ä¸ºäº†æ¼”ç¤ºï¼Œè®©æˆ‘ä»¬å°†æ ‘çš„æœ€å¤§æ·±åº¦è®¾ç½®ä¸º 1 å’Œ 6 ä¸ªåŸºäºæ ‘çš„æå‡å›å½’æ ‘ã€‚

+   æ¯æ£µæ ‘åªæœ‰ä¸€ä¸ªåˆ†å‰²ï¼Œç§°ä¸ºå†³ç­–æ ‘æ¡©ã€‚è¿™å°†é˜²æ­¢é¢„æµ‹ç‰¹å¾ä¹‹é—´çš„äº¤äº’ï¼Œå¹¶ä¸”å…·æœ‰é«˜åº¦çš„å¯è§£é‡Šæ€§

ä½ åº”è¯¥èƒ½å¤Ÿè§‚å¯Ÿåˆ°æ ‘çš„åŠ æ€§æ€§è´¨ï¼Œçœ‹åˆ°ç¬¬ä¸€æ£µæ ‘ï¼Œç„¶åæ˜¯ç¬¬ä¸€æ£µåŠ ä¸Šç¬¬äºŒæ£µï¼Œä»¥æ­¤ç±»æ¨ã€‚

+   å›æƒ³ä¸€ä¸‹ï¼Œä¼°è®¡æ˜¯å¤šä¸ªæ ‘çš„åŠ å’Œ

+   ç”±äºæˆ‘ä»¬åœ¨ç¬¬ä¸€æ£µæ ‘ä¹‹åæ‹Ÿåˆæ¢¯åº¦ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰è´Ÿçš„å’Œæ­£çš„ä¼°è®¡

+   åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›å®é™…ä¸Šæ˜¯è´Ÿæ•°çš„ç”Ÿäº§ä¼°è®¡

```py
params = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 1,                       
    'learning_rate': 1.0,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

num_trees = np.linspace(1,6,6)                              # build a list of numbers of trees 
boosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries

index = 1
for num_tree in num_trees:                                  # loop over number of trees
    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))
    boosting_models[index-1].fit(X = X_train, y = y_train)
    score.append(boosting_models[index-1].score(X = X_test, y = y_test))
    plt.subplot(2,3,index)
    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))
    index = index + 1

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/b2a16fd01b6c4a7a3f642f8cf90844b3.png)

æ³¨æ„ï¼Œä¸æ•°æ®å­˜åœ¨æ˜¾è‘—çš„åå·®

+   æˆ‘ä»¬åªä½¿ç”¨äº†æœ€å¤š 6 ä¸ªå†³ç­–æ ‘æ¡©ï¼ˆ1 ä¸ªå†³ç­–æ ‘ï¼‰

è®©æˆ‘ä»¬æ£€æŸ¥ä¸ä¿ç•™çš„æµ‹è¯•æ•°æ®ä¸€èµ·çš„äº¤å‰éªŒè¯ç»“æœã€‚

```py
index = 1
for num_tree in num_trees:                                  # check over number of trees
    plt.subplot(2,3,index)
    check_model(boosting_models[index-1],X_test,y_test,ymin,ymax,ylabelunit,'Tree-Based Boosting with ' + str(int(num_tree)) + ' Trees')
    index = index + 1
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/c19d0d925995b408d0e14db0f66b52ee.png)

å½“ç„¶ï¼Œç”¨ä¸€æ£µæ ‘æˆ‘ä»¬åšå¾—ç›¸å½“å·®ï¼Œä½†å½“æˆ‘ä»¬è¾¾åˆ° 6 ä¸ªæ ‘æ¡©æ ‘æ—¶ï¼Œæˆ‘ä»¬å°†å‡æ–¹è¯¯å·®å‡ ä¹å‡åŠã€‚

## å»ºç«‹æ›´å¤šæ ‘çš„æ—¶å€™åˆ°äº†

ç°åœ¨è®©æˆ‘ä»¬æ¼”ç¤ºåœ¨åŸºäºæ ‘çš„æå‡æ¨¡å‹ä¸­ä½¿ç”¨æ›´å¤šæ ‘çš„ç»“æœã€‚

+   æˆ‘ä»¬ä»ç„¶ä¼šä½¿ç”¨ç®€å•çš„å†³ç­–æ ‘æ¡©ï¼Œä¸ç”¨æ‹…å¿ƒï¼Œæˆ‘ä»¬ç¨åä¼šæ·»åŠ æ›´å¤š

```py
params = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 1,                                         # maximum depth per tree
    'learning_rate': 1,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

num_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees 
boosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries

index = 1
for num_tree in num_trees:                                  # loop over number of trees
    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))
    boosting_models[index-1].fit(X = X_train, y = y_train)
    score.append(boosting_models[index-1].score(X = X_test, y = y_test))
    plt.subplot(2,3,index)
    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))
    index = index + 1

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/3baea417052c8c32978d193e173228b1.png)

çœ‹åˆ°æ ¼å­å›¾æ¡ˆäº†å—ï¼Ÿè¿™æ˜¯ç”±äºä½¿ç”¨äº†å†³ç­–æ ‘æ¡©ï¼Œå¹¶ä¸”ï¼š

+   ä¸€ä¸ªåŠ æ€§æ¨¡å‹

+   æ‰€æœ‰æ¨¡å‹éƒ½å¯¹æ‰€æœ‰é¢„æµ‹æœ‰è´¡çŒ®

çœ‹åˆ°æš—å’Œäº®åŒºåŸŸäº†å—ï¼Ÿ

+   åŠ æ€§æ¨¡å‹å¯èƒ½ä¼šè¶…å‡ºæ•°æ®èŒƒå›´

è®©æˆ‘ä»¬ç”¨æµ‹è¯•æ•°æ®äº¤å‰éªŒè¯ï¼Œçœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹éšç€æ›´å¤šæ ‘çš„ä½¿ç”¨æ˜¯å¦‚ä½•æ”¹è¿›çš„ã€‚

```py
index = 1
for num_tree in num_trees:                                  # check over number of trees
    plt.subplot(2,3,index)
    check_model(boosting_models[index-1],X_test,y_test,ymin,ymax,ylabelunit,'Tree-Based Boosting with ' + str(int(num_tree)) + ' Trees')
    index = index + 1
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/3f37a775a250322ae1595efdbfe1669d.png)

å¤§çº¦ 20 æ£µæ ‘æ—¶ï¼Œæˆ‘ä»¬è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œç„¶åå¼€å§‹ä¸‹é™ï¼Œæˆ‘ä»¬å¯èƒ½å¼€å§‹è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚

## è¶…è¶Šå†³ç­–æ ‘æ¡©

å¦‚å‰æ‰€è¿°ï¼Œä½¿ç”¨å†³ç­–æ ‘æ¡©æˆ‘ä»¬å¯ä»¥é˜²æ­¢ç‰¹å¾ä¹‹é—´çš„äº¤äº’ã€‚

è®©æˆ‘ä»¬æ‰©å±•åˆ°æ ‘çš„æ·±åº¦ä¸º 2

+   ä¸¤ä¸ªåµŒå¥—å†³ç­–å¯¼è‡´ 4 ä¸ªç»ˆç«¯èŠ‚ç‚¹

```py
params = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 2,                                         # maximum depth per tree
    'learning_rate': 1,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

num_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees 
boosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries

index = 1
for num_tree in num_trees:                                  # loop over number of trees
    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))
    boosting_models[index-1].fit(X = X_train, y = y_train)
    score.append(boosting_models[index-1].score(X = X_test, y = y_test))
    plt.subplot(2,3,index)
    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))
    index = index + 1

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/65161de226c63aae698a6005291afae4.png)

æˆ‘ä»¬ç°åœ¨æœ‰æ›´å¤šçš„çµæ´»æ€§ã€‚

+   ç”¨ä¸€æ£µæ ‘æˆ‘ä»¬æœ‰ 4 ä¸ªç»ˆç«¯èŠ‚ç‚¹ï¼ˆåŒºåŸŸï¼‰

+   åªç”¨ 6 æ£µæ ‘ï¼Œæˆ‘ä»¬å°±èƒ½æ•æ‰åˆ°ä¸€äº›å¤æ‚ç‰¹å¾

è®©æˆ‘ä»¬å†æ¬¡å¢åŠ æ ‘çš„æ·±åº¦ã€‚

```py
params = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 3,                                         # maximum depth per tree
    'learning_rate': 1,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

num_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees 
boosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries

index = 1
for num_tree in num_trees:                                  # loop over number of trees
    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))
    boosting_models[index-1].fit(X = X_train, y = y_train)
    score.append(boosting_models[index-1].score(X = X_test, y = y_test))
    plt.subplot(2,3,index)
    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))
    index = index + 1

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/b36175195239341c3b7ea8c842eea5b7.png)

å†ä¸€æ¬¡ï¼Œä½¿ç”¨æ·±åº¦ä¸º 4-8 çš„æ ‘æ˜¯å¸¸è§çš„ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å°è¯• 5ã€‚

```py
params = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 5,                                         # maximum depth per tree
    'learning_rate': 1,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

num_trees = [5,10,20,100,500,5000]                          # build a list of numbers of trees 
boosting_models = []; score = []; pred = []                 # arrays for storage of models and model summaries

index = 1
for num_tree in num_trees:                                  # loop over number of trees
    boosting_models.append(GradientBoostingRegressor(n_estimators=int(num_tree),**params))
    boosting_models[index-1].fit(X = X_train, y = y_train)
    score.append(boosting_models[index-1].score(X = X_test, y = y_test))
    plt.subplot(2,3,index)
    pred.append(visualize_model(boosting_models[index-1],X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],ymin,ymax,'Gradient Boosting with ' + str(int(num_tree)) + ' Trees',Xname,yname,Xlabelunit,ylabelunit))
    index = index + 1

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/d2ae5319fcaa86e15b8c1c77df29902e.png)

è®©æˆ‘ä»¬ç”¨æµ‹è¯•æ•°æ®äº¤å‰éªŒè¯æ¨¡å‹ã€‚

```py
index = 1
for num_tree in num_trees:                                  # check over number of trees
    plt.subplot(2,3,index)
    check_model(boosting_models[index-1],X_test,y_test,ymin,ymax,ylabelunit,'Tree-Based Boosting with ' + str(int(num_tree)) + ' Trees')
    index = index + 1
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) 
```

![å›¾ç‰‡](img/cdbb3edec9e81f128298ed05c7cd6189.png)

å½“æœ€å¤§æ ‘æ·±åº¦ä¸º 5 æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ€§èƒ½åœ¨æ—©æœŸè¾¾åˆ°å³°å€¼ï¼Œå¢åŠ æ›´å¤šæ ‘æ²¡æœ‰å½±å“ã€‚

+   å½“ç„¶ï¼Œè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªå½»åº•çš„åˆ†æã€‚

è®©æˆ‘ä»¬å°è¯•æ›´æ·±å…¥çš„ä¸œè¥¿ã€‚

+   æˆ‘ä»¬å°†ä½¿ç”¨$1,\ldots,100$æ£µæ ‘è¿›è¡Œäº¤å‰éªŒè¯ï¼Œæœ€å¤§æ ‘æ·±åº¦ä¸º$1, 2, 3, 10$ã€‚

+   æˆ‘ä»¬è¿˜å‡æ…¢äº†å­¦ä¹ ç‡ï¼Œæˆ‘ä¹‹å‰æé«˜äº†å®ƒä»¥æ”¾å¤§è¾“å‡ºå·®å¼‚çš„æ¼”ç¤ºï¼Œä½†ç°åœ¨æˆ‘ä»¬æƒ³çœ‹åˆ°æœ€ä½³å¯èƒ½çš„æ¨¡å‹ã€‚

```py
num_trees = np.linspace(1,200,200)
max_features = 1
MSE1_list = []; MSE2_list = []; MSE3_list = []; MSE4_list = [] 

params1 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 1,                                         # maximum depth per tree
    'learning_rate': 0.2,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

params2 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 2,                                         # maximum depth per tree
    'learning_rate': 0.2,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

params3 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 3,                                         # maximum depth per tree
    'learning_rate': 0.2,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

params4 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 10,                                        # maximum depth per tree
    'learning_rate': 0.2,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

index = 1
for num_tree in num_trees:                                  # loop over number of trees in our random forest
    boosting_model1 = GradientBoostingRegressor(n_estimators=int(num_tree),**params1).fit(X = X_train, y = y_train)
    y_test1_hat = boosting_model1.predict(X_test); MSE1_list.append(metrics.mean_squared_error(y_test,y_test1_hat))

    boosting_model2 = GradientBoostingRegressor(n_estimators=int(num_tree),**params2).fit(X = X_train, y = y_train)
    y_test2_hat = boosting_model2.predict(X_test); MSE2_list.append(metrics.mean_squared_error(y_test,y_test2_hat))

    boosting_model3 = GradientBoostingRegressor(n_estimators=int(num_tree),**params3).fit(X = X_train, y = y_train)
    y_test3_hat = boosting_model3.predict(X_test); MSE3_list.append(metrics.mean_squared_error(y_test,y_test3_hat))

    boosting_model4 = GradientBoostingRegressor(n_estimators=int(num_tree),**params4).fit(X = X_train, y = y_train)
    y_test4_hat = boosting_model4.predict(X_test); MSE4_list.append(metrics.mean_squared_error(y_test,y_test4_hat))

    index = index + 1

plt.subplot(111)                                            # plot jackknife results for all cases
plt.scatter(num_trees,MSE1_list,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 1")
plt.scatter(num_trees,MSE2_list,s=None,c='blue',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 2")
plt.scatter(num_trees,MSE3_list,s=None,c='black',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 3")
plt.scatter(num_trees,MSE4_list,s=None,c='green',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 10")

plt.title('Testing Mean Square Error vs. Number of Trees'); plt.xlabel('Number of Trees'); plt.ylabel('Test Mean Square Error')
plt.xlim(0,200); plt.legend(loc='lower right'); add_grid()
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/c2ee3e9f993c37a05e91acb8b4ce5baf.png)

éå¸¸æœ‰è¶£ï¼š

+   éšç€æ ‘æ·±åº¦çš„å¢åŠ ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯èƒ½ä¼šæ”¹è¿›ã€‚

+   æ›´æ·±çš„æ ‘æ·±åº¦éœ€è¦æ›´å°‘çš„æ ‘æ¥æé«˜å‡†ç¡®æ€§ã€‚

+   å½“æ ‘æ·±åº¦ä¸º 2 å’Œ 3 æ—¶ï¼Œæ¨¡å‹çš„è¡Œä¸ºç›¸åŒï¼Œå¹¶ä¸”åœ¨ 10-15 æ£µæ ‘ä¹‹åè¶‹äºå¹³ç¨³ï¼Œå®ƒä»¬å¯¹è¿‡æ‹Ÿåˆå…·æœ‰æŠµæŠ—åŠ›ã€‚

+   å½“æ ‘æ·±åº¦ä¸º 10 æ—¶ï¼Œæ ‘çš„æ•°é‡å¯¹æ¨¡å‹æ€§èƒ½æ²¡æœ‰å½±å“ã€‚

## æ¢¯åº¦ä¸‹é™è¶…å‚æ•°

å­¦ä¹ ç‡è°ƒæ•´æ¯ä¸ªæ·»åŠ æ ‘å¯¹æ•´ä½“æ¨¡å‹é¢„æµ‹çš„é™„åŠ å½±å“ã€‚

+   æ›´ä½çš„ learning rate å°†å‡æ…¢æ”¶æ•›åˆ°è§£å†³æ–¹æ¡ˆçš„é€Ÿåº¦ã€‚

+   æ›´ä½çš„ learning rate å°†å¸®åŠ©æˆ‘ä»¬ä¸ä¼šè·³è¿‡æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚

æˆ‘ä»¬ä¸ä¼šåœ¨è¿™ä¸ªé—®é¢˜ä¸ŠèŠ±è´¹å¤ªå¤šæ—¶é—´ï¼Œä½†è®©æˆ‘ä»¬è¯•ç€æ”¹å˜å­¦ä¹ ç‡ã€‚

```py
learning_rates = np.arange(0.01,1.0,0.01)
MSE1_list = []; MSE2_list = []; MSE3_list = []; MSE4_list = [] 

params1 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 5,                                         # maximum depth per tree
    'n_estimators': 40,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

params2 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 5,                                         # maximum depth per tree
    'n_estimators': 40,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

params3 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 5,                                         # maximum depth per tree
    'n_estimators': 40,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

params4 = {
    'loss': 'squared_error',                                           # L2 Norm - least squares
    'max_depth': 5,                                        # maximum depth per tree
    'n_estimators': 40,
    'criterion': 'squared_error'                                      # tree construction criteria is mean square error over training
}

index = 1
for learning_rate in learning_rates:                                  # loop over number of trees in our random forest
    boosting_model1 = GradientBoostingRegressor(learning_rate = learning_rate,**params1).fit(X = X_train, y = y_train)
    y_test1_hat = boosting_model1.predict(X_test); MSE1_list.append(metrics.mean_squared_error(y_test,y_test1_hat))

    boosting_model2 = GradientBoostingRegressor(learning_rate = learning_rate,**params2).fit(X = X_train, y = y_train)
    y_test2_hat = boosting_model2.predict(X_test); MSE2_list.append(metrics.mean_squared_error(y_test,y_test2_hat))

    boosting_model3 = GradientBoostingRegressor(learning_rate = learning_rate,**params3).fit(X = X_train, y = y_train)
    y_test3_hat = boosting_model3.predict(X_test); MSE3_list.append(metrics.mean_squared_error(y_test,y_test3_hat))

    boosting_model4 = GradientBoostingRegressor(learning_rate = learning_rate,**params4).fit(X = X_train, y = y_train)
    y_test4_hat = boosting_model4.predict(X_test); MSE4_list.append(metrics.mean_squared_error(y_test,y_test4_hat))

    index = index + 1

plt.subplot(111)                                            # plot jackknife results for all cases
plt.scatter(learning_rates,MSE1_list,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 1")
plt.scatter(learning_rates,MSE2_list,s=None,c='blue',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 2")
plt.scatter(learning_rates,MSE3_list,s=None,c='black',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 3")
plt.scatter(learning_rates,MSE4_list,s=None,c='green',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.6,
            linewidths=0.3,edgecolors="black",label = "Tree Depth = 10")

plt.title('Testing Mean Square Error vs. Learning Rate'); plt.xlabel('Learning Rate'); plt.ylabel('Test Mean Square Error')
plt.xlim(0.0,1.0); plt.legend(loc='upper left'); add_grid()
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/d130f276273e2b117adb7bf8d1768618.png)

å†æ¬¡å¼ºè°ƒï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„ç»“æœã€‚

+   æ— è®ºæ ‘å¤æ‚åº¦å¦‚ä½•ï¼Œç¼“æ…¢å­¦ä¹ æ€»æ˜¯æ›´å¥½çš„ï¼

## æ¸…æ´ã€ç´§å‡‘çš„æœºå™¨å­¦ä¹ ä»£ç çš„æœºå™¨å­¦ä¹ ç®¡é“

ç®¡é“æ˜¯ scikit-learn ä¸­çš„ä¸€ä¸ªç±»ï¼Œå®ƒå…è®¸å°è£…ä¸€ç³»åˆ—æ•°æ®å‡†å¤‡å’Œå»ºæ¨¡æ­¥éª¤ã€‚

+   ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†ç®¡é“è§†ä¸ºæˆ‘ä»¬é«˜åº¦æµ“ç¼©çš„å·¥ä½œæµç¨‹ä¸­çš„ä¸€ä¸ªå¯¹è±¡ã€‚

ç®¡é“ç±»å…è®¸æˆ‘ä»¬ï¼š

+   æé«˜ä»£ç å¯è¯»æ€§ï¼Œå¹¶ä¿æŒä¸€åˆ‡äº•ç„¶æœ‰åºã€‚

+   ç”¨éå¸¸å°‘çš„å¯è¯»ä»£ç æ„å»ºå®Œæ•´çš„æµç¨‹ã€‚

+   é¿å…å¸¸è§çš„æµç¨‹é—®é¢˜ï¼Œå¦‚æ•°æ®æ³„éœ²ï¼Œæµ‹è¯•æ•°æ®å½±å“æ¨¡å‹å‚æ•°è®­ç»ƒã€‚

+   æŠ½è±¡å¸¸è§çš„æœºå™¨å­¦ä¹ å»ºæ¨¡ï¼Œä¸“æ³¨äºæ„å»ºå°½å¯èƒ½å¥½çš„æ¨¡å‹ã€‚

åŸºæœ¬å“²å­¦æ˜¯å°†æœºå™¨å­¦ä¹ è§†ä¸ºç»„åˆæœç´¢ä»¥æ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼ˆAutoMLï¼‰ã€‚

æ›´å¤šä¿¡æ¯è¯·å‚é˜…æˆ‘å…³äº[æœºå™¨å­¦ä¹ ç®¡é“](https://www.youtube.com/watch?v=tYrPs8s1l9U&list=PLG19vXLQHvSAufDFgZEFAYQEwMJXklnQV&index=5)çš„å½•éŸ³è®²åº§å’Œä¸€ä»½è¯¦ç»†è®°å½•çš„æ¼”ç¤º[æœºå™¨å­¦ä¹ ç®¡é“å·¥ä½œæµç¨‹](http://localhost:8892/notebooks/OneDrive%20-%20The%20University%20of%20Texas%20at%20Austin/Courses/Workflows/PythonDataBasics_Pipelines.ipynb)ã€‚

```py
x1 = 0.25; x2 = 0.3                                           # predictor values for the prediction

pipe_boosting = Pipeline([                                      # the machine learning workflow as a pipeline object
    ('boosting', GradientBoostingRegressor())
])

params = {                                                    # the machine learning workflow method's parameters to search
    'boosting__learning_rate': np.arange(0.1,2.0,0.2),
    'boosting__n_estimators': np.arange(2,40,4,dtype = int),
}

tuned_boosting = GridSearchCV(pipe_boosting,params,scoring = 'neg_mean_squared_error', # hyperparameter tuning w. grid search k-fold cross validation 
                             refit = True)
tuned_boosting.fit(X,y)                                         # tune the model with k-fold and then train the model will the data 

print('Tuned hyperparameter: ' + str(tuned_boosting.best_params_))

estimate = tuned_boosting.predict([[x1,x2]])[0]                 # make a prediction (no tuning shown)
print('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + 
      str(round(estimate,1)) + ' ' + yunit)                     # print results 
```

```py
Tuned hyperparameter: {'boosting__learning_rate': 0.30000000000000004, 'boosting__n_estimators': 10}
Estimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1979.7 MCFPD 
```

## æ³¨é‡Š

å¸Œæœ›ä½ å‘ç°è¿™ä¸€ç« å¾ˆæœ‰å¸®åŠ©ã€‚è¿˜æœ‰å¾ˆå¤šå¯ä»¥åšçš„å’Œè®¨è®ºçš„ï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ï¼Œ

*è¿ˆå…‹å°”*

## ä½œè€…ï¼š

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ *æ–°é¢–çš„æ•°æ®åˆ†æã€åœ°çƒç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ åœ°ä¸‹è§£å†³æ–¹æ¡ˆ*

åœ¨åœ°ä¸‹å’¨è¯¢ã€ç ”ç©¶å’Œå¼€å‘æ–¹é¢æ‹¥æœ‰è¶…è¿‡ 17 å¹´çš„ç»éªŒï¼Œè¿ˆå…‹å°”å› ä»–å¯¹æ•™å­¦çš„çƒ­æƒ…å’Œå¯¹å¢å¼ºå·¥ç¨‹å¸ˆå’Œåœ°çƒç§‘å­¦å®¶åœ¨åœ°ä¸‹èµ„æºå¼€å‘ä¸­å½±å“çš„çƒ­æƒ…è€Œé‡è¿”å­¦æœ¯ç•Œã€‚

æ›´å¤šå…³äºè¿ˆå…‹å°”çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹é“¾æ¥ï¼š

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­çš„åº”ç”¨åœ°çƒç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚ä¸ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œï¼Œæ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶ç»“åˆæ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µï¼Œå¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ä»¥å¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡çš„ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢å’Œæ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢

## æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥è·å–ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­çš„åº”ç”¨åœ°çƒç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
