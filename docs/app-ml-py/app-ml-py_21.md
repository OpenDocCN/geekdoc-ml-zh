# LASSO å›å½’

> åŸæ–‡ï¼š[`geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_LASSO_regression.html`](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_LASSO_regression.html)

Michael J. Pyrczï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

ç”µå­ä¹¦â€œPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œâ€çš„ç« èŠ‚ã€‚

è¯·å°†æ­¤ç”µå­ä¹¦å¼•ç”¨å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *Python åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œ* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138 ![DOI](https://doi.org/10.5281/zenodo.15169138)

æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠæ›´å¤šå†…å®¹åœ¨æ­¤å¤„å¯ç”¨ï¼š

è¯·å°† MachineLearningDemos GitHub ä»“åº“å¼•ç”¨å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *MachineLearningDemos: Python æœºå™¨å­¦ä¹ æ¼”ç¤ºå·¥ä½œæµç¨‹ä»“åº“* (0.0.3) [è½¯ä»¶]. Zenodo. DOI: 10.5281/zenodo.13835312\. GitHub ä»“åº“ï¼š[GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos) ![DOI](https://zenodo.org/doi/10.5281/zenodo.13835312)

ç”± Michael J. Pyrcz æ’°å†™

Â© ç‰ˆæƒæ‰€æœ‰ 2024ã€‚

æœ¬ç« æ˜¯å…³äº**LASSO å›å½’**çš„æ•™ç¨‹å’Œæ¼”ç¤ºã€‚

**YouTube è®²åº§**ï¼šæŸ¥çœ‹æˆ‘åœ¨ä»¥ä¸‹ä¸»é¢˜ä¸Šçš„è®²åº§ï¼š

+   [æœºå™¨å­¦ä¹ ç®€ä»‹](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)

+   [çº¿æ€§å›å½’](https://youtu.be/0fzbyhWiP84)

+   [å²­å›å½’](https://youtu.be/pMGO40yXZ5Y?si=ygJAheyX-v2BmSiR)

+   [LASSO å›å½’](https://youtu.be/cVFYhlCCI_8?si=NbwIDaZj30vxezn2)

+   [è§„èŒƒ](https://youtu.be/JmxGlrurQp0?si=vuF1TXDbZkyRC1j-)

è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘ YouTube ä¸Šçš„[æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«æœ‰è‰¯å¥½æ–‡æ¡£è®°å½•çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºè·å–ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœä½ æƒ³çŸ¥é“æˆ‘çš„åŠ¨æœºï¼Œè¯·æŸ¥çœ‹[Michael çš„æ•…äº‹](https://michaelpyrcz.com/my-story)ã€‚

## LASSO å›å½’çš„åŠ¨æœº

è¿™é‡Œæœ‰ä¸€ä¸ªç®€å•çš„æµç¨‹ï¼Œå±•ç¤ºäº†å²­å›å½’çš„æ¼”ç¤ºï¼Œå¹¶ä¸åŸºäºæœºå™¨å­¦ä¹ çš„é¢„æµ‹ä¸­çš„çº¿æ€§å›å½’å’Œå²­å›å½’è¿›è¡Œäº†æ¯”è¾ƒã€‚ä¸ºä»€ä¹ˆä»çº¿æ€§å›å½’å¼€å§‹ï¼Ÿ

+   çº¿æ€§å›å½’æ˜¯æœ€ç®€å•çš„å‚æ•°åŒ–é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹

+   æˆ‘ä»¬é€šè¿‡è¿­ä»£æ–¹æ³•å­¦ä¹ è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨ LASSO æˆ‘ä»¬å¤±å»äº†çº¿æ€§å›å½’å’Œå²­å›å½’çš„è§£æè§£

+   è®©æˆ‘ä»¬ä»æŸå¤±å‡½æ•°å’ŒèŒƒæ•°çš„æ¦‚å¿µå¼€å§‹

+   æˆ‘ä»¬å¯ä»¥è®¿é—®æ¨¡å‹ä¸ç¡®å®šæ€§çš„ç½®ä¿¡åŒºé—´å’Œå‚æ•°æ˜¾è‘—æ€§çš„å‡è®¾æ£€éªŒçš„ç»Ÿè®¡åˆ†æè¡¨è¾¾å¼

ä¸ºä»€ä¹ˆåœ¨ LASSO å›å½’ä¹‹å‰è¿˜è¦ä»‹ç»å²­å›å½’ï¼Ÿ

+   æœ‰æ—¶çº¿æ€§å›å½’å¹¶ä¸è¶³å¤Ÿç®€å•ï¼Œæˆ‘ä»¬å®é™…ä¸Šéœ€è¦ä¸€ä¸ªæ›´ç®€å•çš„æ¨¡å‹ï¼

+   ä»‹ç»æ¨¡å‹æ­£åˆ™åŒ–å’Œè¶…å‚æ•°è°ƒæ•´çš„æ¦‚å¿µ

ç„¶åæˆ‘ä»¬ä»‹ç» LASSO å›å½’ï¼Œä»¥äº†è§£æŸå¤±å‡½æ•°èŒƒæ•°é€‰æ‹©å¯¹è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„å½±å“ã€‚

+   åœ¨å²­å›å½’æŸå¤±å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬ç”¨ L1 æ­£åˆ™åŒ–æ›¿æ¢ L2 æ­£åˆ™åŒ–é¡¹

+   å› æ­¤ï¼ŒLASSO é€ä¸ªå°†æ¨¡å‹å‚æ•°ç¼©å°åˆ° 0.0ï¼Œä»è€Œå†…ç½®äº†ç‰¹å¾é€‰æ‹©ï¼

è¿™é‡Œæ˜¯ä¸€äº›å…³äºé¢„æµ‹æœºå™¨å­¦ä¹  LASSO å›å½’æ¨¡å‹çš„åŸºæœ¬ç»†èŠ‚ï¼Œè®©æˆ‘ä»¬ä»çº¿æ€§å›å½’å’Œå²­å›å½’å¼€å§‹ï¼Œé€æ­¥è¿‡æ¸¡åˆ°å²­å›å½’ï¼š

## çº¿æ€§å›å½’

ç”¨äºé¢„æµ‹çš„çº¿æ€§å›å½’ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹ä¸€ç»„æ•°æ®æ‹Ÿåˆçš„çº¿æ€§æ¨¡å‹ã€‚

![](img/ed71b506ab0f5b47754cb1c92fc8935a.png)

ç¤ºä¾‹çº¿æ€§å›å½’æ¨¡å‹ã€‚

è®©æˆ‘ä»¬å…ˆå®šä¹‰ä¸€äº›æœ¯è¯­ï¼Œ

+   **é¢„æµ‹ç‰¹å¾** - é¢„æµ‹æ¨¡å‹çš„è¾“å…¥ç‰¹å¾ï¼Œé‰´äºæˆ‘ä»¬åªè®¨è®ºçº¿æ€§å›å½’è€Œä¸è®¨è®ºå¤šå…ƒçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œ$x$ã€‚åœ¨æˆ‘ä»¬çš„å›¾è¡¨ä¸­ï¼ˆåŒ…æ‹¬ä¸Šé¢çš„ï¼‰ï¼Œé¢„æµ‹ç‰¹å¾ä½äº x è½´ä¸Šã€‚

+   **å“åº”ç‰¹å¾** - é¢„æµ‹æ¨¡å‹çš„è¾“å‡ºç‰¹å¾ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ$y$ã€‚åœ¨æˆ‘ä»¬çš„å›¾è¡¨ä¸­ï¼ˆåŒ…æ‹¬ä¸Šé¢çš„ï¼‰ï¼Œå“åº”ç‰¹å¾ä½äº y è½´ä¸Šã€‚

ç°åœ¨ï¼Œä»¥ä¸‹æ˜¯çº¿æ€§å›å½’çš„ä¸€äº›å…³é”®æ–¹é¢ï¼š

**å‚æ•°æ¨¡å‹**

è¿™æ˜¯ä¸€ä¸ªå‚æ•°é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬æ¥å—ä¸€ä¸ªå…ˆéªŒå‡è®¾çš„çº¿æ€§ï¼Œç„¶åè·å¾—ä¸€ä¸ªéå¸¸ä½çš„å‚æ•°è¡¨ç¤ºï¼Œæ˜“äºè®­ç»ƒè€Œæ— éœ€å¤§é‡æ•°æ®ã€‚

+   é€‚é…æ¨¡å‹æ˜¯ä¸€ä¸ªåŸºäºæ‰€æœ‰å¯ç”¨ç‰¹å¾ $x_1,\ldots,x_m$ çš„ç®€å•åŠ æƒçº¿æ€§åŠ æ€§æ¨¡å‹ã€‚

+   å‚æ•°æ¨¡å‹çš„å½¢å¼å¦‚ä¸‹ï¼š

$$ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 $$

è¿™é‡Œæ˜¯çº¿æ€§æ¨¡å‹å‚æ•°çš„å¯è§†åŒ–ï¼Œ

![](img/e798c74dc4ed5ec8fcbd2c8ffe0ef5fd.png)

çº¿æ€§æ¨¡å‹å‚æ•°ã€‚

**æœ€å°äºŒä¹˜æ³•**

å¯¹äº L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼Œæ¨¡å‹å‚æ•° $b_1,\ldots,b_m,b_0$ çš„è§£æè§£æ˜¯å¯ç”¨çš„ï¼Œè¯¯å·®æ˜¯æ±‚å’Œå¹¶å¹³æ–¹çš„ï¼Œå³æœ€å°äºŒä¹˜æ³•ã€‚

+   æˆ‘ä»¬åœ¨è®­ç»ƒæ•°æ®ä¸Šæœ€å°åŒ–è¯¯å·®ï¼Œå³æ®‹å·®å¹³æ–¹å’Œï¼ˆRSSï¼‰ï¼š

$$ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)Â² $$

å…¶ä¸­ $y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾å€¼ï¼Œ$\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ æ˜¯æ¨¡å‹é¢„æµ‹ï¼Œåœ¨ $\alpha = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šã€‚

è¿™é‡Œæ˜¯ L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰çš„å¯è§†åŒ–ï¼Œ

![](img/e91d94eb7bac509a6ec741d8af33082f.png)

çº¿æ€§æ¨¡å‹çš„æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®ã€‚

+   è¿™å¯ä»¥ç®€åŒ–ä¸ºè®­ç»ƒæ•°æ®ä¸Šçš„å¹³æ–¹è¯¯å·®ä¹‹å’Œï¼Œ

\begin{equation} \sum_{i=1}^n (\Delta y_i)Â² \end{equation}

å…¶ä¸­ $\Delta y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾è§‚æµ‹ $y_i$ å‡å»æ¨¡å‹é¢„æµ‹ $\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ï¼Œåœ¨ $i = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šã€‚

**å‡è®¾**

æˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹æœ‰ä¸€äº›é‡è¦çš„å‡è®¾ï¼Œ

+   **æ— è¯¯å·®** - é¢„æµ‹å˜é‡æ— è¯¯å·®ï¼Œä¸æ˜¯éšæœºå˜é‡

+   **çº¿æ€§** - å“åº”æ˜¯ç‰¹å¾ï¼ˆsï¼‰çš„çº¿æ€§ç»„åˆ

+   **æ’å®šæ–¹å·®** - å“åº”è¯¯å·®åœ¨é¢„æµ‹å˜é‡å€¼ä¸Šæ’å®š

+   **è¯¯å·®ç‹¬ç«‹æ€§** - å“åº”è¯¯å·®ä¹‹é—´ç›¸äº’ä¸ç›¸å…³

+   **æ— å¤šé‡å…±çº¿æ€§** - æ²¡æœ‰ç‰¹å¾ä¸å…¶ä»–ç‰¹å¾å†—ä½™

## å²­å›å½’

åœ¨å²­å›å½’ä¸­ï¼Œæˆ‘ä»¬å‘æœ€å°åŒ–è¿‡ç¨‹ä¸­æ·»åŠ ä¸€ä¸ªè¶…å‚æ•° $\lambda$ï¼Œå¹¶å¸¦æœ‰æ”¶ç¼©æƒ©ç½šé¡¹ $\sum_{j=1}^m b_{\alpha}Â²$ã€‚

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)Â² + \lambda \sum_{j=1}^m b_{\alpha}Â² $$

å› æ­¤ï¼Œå²­å›å½’è®­ç»ƒæ•´åˆäº†ä¸¤ä¸ªç»å¸¸æ˜¯ç›¸äº’ç«äº‰çš„ç›®æ ‡æ¥å¯»æ‰¾æ¨¡å‹å‚æ•°ï¼Œ

+   æ‰¾åˆ°ä½¿è®­ç»ƒæ•°æ®è¯¯å·®æœ€å°çš„æ¨¡å‹å‚æ•°

+   å°†æ–œç‡å‚æ•°æœ€å°åŒ–åˆ°é›¶

æ³¨æ„ï¼šlambda ä¸åŒ…æ‹¬æˆªè·ï¼Œ$b_0$ã€‚

$\lambda$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œå®ƒæ§åˆ¶ç€æ¨¡å‹çš„æ‹Ÿåˆç¨‹åº¦ï¼Œå¯èƒ½ä¸æ¨¡å‹çš„åå·®-æ–¹å·®æƒè¡¡æœ‰å…³ã€‚

+   å½“ $\lambda \rightarrow 0$ æ—¶ï¼Œè§£è¶‹è¿‘äºçº¿æ€§å›å½’ï¼Œæ²¡æœ‰åå·®ï¼ˆç›¸å¯¹äºçº¿æ€§æ¨¡å‹æ‹Ÿåˆï¼‰ï¼Œä½†æ¨¡å‹æ–¹å·®å¯èƒ½æ›´é«˜

+   éšç€ $\lambda$ çš„å¢åŠ ï¼Œæ¨¡å‹æ–¹å·®å‡å°ï¼Œæ¨¡å‹åå·®å¢åŠ ï¼Œæ¨¡å‹å˜å¾—ç®€å•

+   å½“ $\lambda \rightarrow \infty$ æ—¶ï¼Œæ¨¡å‹å‚æ•° $b_1,\ldots,b_m$ æ”¶ç¼©åˆ° 0.0ï¼Œæ¨¡å‹é¢„æµ‹è¶‹è¿‘äºè®­ç»ƒæ•°æ®å“åº”ç‰¹å¾çš„å‡å€¼

## LASSO å›å½’

å¯¹äº LASSOï¼Œç±»ä¼¼äºå²­å›å½’ï¼Œæˆ‘ä»¬åœ¨æœ€å°åŒ–è¿‡ç¨‹ä¸­æ·»åŠ ä¸€ä¸ªè¶…å‚æ•° $\lambda$ï¼Œå¹¶å¸¦æœ‰æ”¶ç¼©æƒ©ç½šé¡¹ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨ L1 èŒƒæ•°è€Œä¸æ˜¯ L2ï¼ˆç»å¯¹å€¼ä¹‹å’Œè€Œä¸æ˜¯å¹³æ–¹ä¹‹å’Œï¼‰ã€‚

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)Â² + \lambda \sum_{j=1}^m |b_{\alpha}| $$

å› æ­¤ï¼ŒLASSO å›å½’è®­ç»ƒæ•´åˆäº†ä¸¤ä¸ªç»å¸¸æ˜¯ç›¸äº’ç«äº‰çš„ç›®æ ‡æ¥å¯»æ‰¾æ¨¡å‹å‚æ•°ï¼Œ

+   æ‰¾åˆ°ä½¿è®­ç»ƒæ•°æ®è¯¯å·®æœ€å°çš„æ¨¡å‹å‚æ•°

+   å°†æ–œç‡å‚æ•°æœ€å°åŒ–åˆ°é›¶

å†æ¬¡å¼ºè°ƒï¼ŒLASSO å’Œå²­å›å½’ä¹‹é—´çš„å”¯ä¸€åŒºåˆ«æ˜¯ï¼š

+   å¯¹äº LASSOï¼Œæ”¶ç¼©é¡¹è¢«è¡¨ç¤ºä¸ºä¸€ä¸ª $\ell_1$ æƒ©ç½šï¼Œ

$$ \lambda \sum_{\alpha=1}^m |b_{\alpha}| $$

+   å¯¹äºå²­å›å½’ï¼Œæ”¶ç¼©é¡¹è¢«è¡¨ç¤ºä¸ºä¸€ä¸ª $\ell_2$ æƒ©ç½šï¼Œ

$$ \lambda \sum_{\alpha=1}^m \left(b_{\alpha}\right)Â² $$

å½“å²­å›å½’å’Œ LASSO éƒ½å°†æ¨¡å‹å‚æ•° ($b_{\alpha}, \alpha = 1,\ldots,m$) æ”¶ç¼©åˆ°é›¶æ—¶ï¼š

+   éšç€ $\lambda$ï¼Œå³è¶…å‚æ•°çš„å¢åŠ ï¼ŒLASSO å‚æ•°ä»¥ä¸åŒçš„é€Ÿç‡è¾¾åˆ°æ¯ä¸ªé¢„æµ‹ç‰¹å¾ä¸ºé›¶ã€‚

+   å› æ­¤ï¼ŒLASSO æä¾›äº†ä¸€ç§ç‰¹å¾æ’åºå’Œé€‰æ‹©çš„æ–¹æ³•ï¼

$\lambda$ï¼Œå³è¶…å‚æ•°ï¼Œæ§åˆ¶ç€æ¨¡å‹çš„æ‹Ÿåˆç¨‹åº¦ï¼Œå¯èƒ½ä¸æ¨¡å‹åå·®-æ–¹å·®æƒè¡¡æœ‰å…³ã€‚

+   å½“ $\lambda \rightarrow 0$ æ—¶ï¼Œé¢„æµ‹æ¨¡å‹è¶‹è¿‘äºçº¿æ€§å›å½’ï¼Œæ¨¡å‹åå·®è¾ƒä½ï¼Œä½†æ¨¡å‹æ–¹å·®è¾ƒé«˜

+   éšç€ $\lambda$ çš„å¢åŠ ï¼Œæ¨¡å‹æ–¹å·®é™ä½ï¼Œæ¨¡å‹åå·®å¢åŠ 

+   å½“ $\lambda \rightarrow \infty$ æ—¶ï¼Œæ‰€æœ‰ç³»æ•°éƒ½å˜ä¸º 0.0ï¼Œæ¨¡å‹æ˜¯è®­ç»ƒæ•°æ®å“åº”ç‰¹å¾çš„å¹³å‡å€¼

## **$LÂ¹$ ä¸ $LÂ²$ èŒƒæ•°**

è¿™å°†æ˜¯è®¨è®º $LÂ¹$ å’Œ $LÂ²$ èŒƒæ•°é€‰æ‹©çš„å¥½æ—¶æœºã€‚ä¸ºäº†è§£é‡Šè¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬æ¯”è¾ƒåœ¨è®­ç»ƒæ¨¡å‹å‚æ•°æ—¶ $LÂ¹$ å’Œ $LÂ²$ èŒƒæ•°åœ¨æŸå¤±å‡½æ•°ä¸­çš„æ€§èƒ½ã€‚

| å±æ€§ | æœ€å°ç»å¯¹åå·®ï¼ˆL1ï¼‰ | æœ€å°äºŒä¹˜æ³•ï¼ˆL2ï¼‰ |
| --- | --- | --- |
| **é²æ£’æ€§** | é²æ£’ | ä¸å¤ªé²æ£’ |
| **è§£çš„ç¨³å®šæ€§** | ä¸ç¨³å®šè§£ | ç¨³å®šè§£ |
| **è§£çš„æ•°é‡** | å¯èƒ½å­˜åœ¨å¤šä¸ªè§£ | æ€»æ˜¯æœ‰ä¸€ä¸ªè§£ |
| **ç‰¹å¾é€‰æ‹©** | å†…ç½®ç‰¹å¾é€‰æ‹© | æ— ç‰¹å¾é€‰æ‹© |
| **è¾“å‡ºç¨€ç–æ€§** | ç¨€ç–è¾“å‡º | éç¨€ç–è¾“å‡º |
| **è§£æè§£** | æ²¡æœ‰è§£æè§£ | è§£æè§£ |

è¿™é‡Œæœ‰ä¸€äº›ä¸“é—¨é’ˆå¯¹ LASSO å›å½’çš„é‡è¦è§‚ç‚¹ï¼Œ

## ç‰¹å¾é€‰æ‹©

è®©æˆ‘ä»¬æ¯”è¾ƒå…·æœ‰ $ğ‘³ğŸ$ æ­£åˆ™åŒ–çš„å²­å›å½’å’Œå…·æœ‰ $ğ‘³ğŸ$ æ­£åˆ™åŒ–çš„ LASSO çš„è§£ã€‚

+   å¯¹äºç›¸åŒçš„æ­£åˆ™åŒ–æˆæœ¬ï¼Œæˆ‘ä»¬åœ¨æ¨¡å‹å‚æ•°ç©ºé—´ä¸­æœ‰ä¸åŒçš„å½¢çŠ¶

![](img/1f84730c294a4da3a38601107b6392e8.png)

LASSOï¼ˆå·¦ï¼‰å’Œå²­å›å½’ï¼ˆå³ï¼‰çš„æ­£åˆ™åŒ–æŸå¤±ç­‰é«˜çº¿ã€‚

+   å¦‚æœ $ğ‘ $ è¶³å¤Ÿå¤§ï¼ˆ$\lambda \rightarrow 0$ï¼‰ï¼Œåˆ™é€‰æ‹©æœ€å°äºŒä¹˜æ³•æ‹Ÿåˆå‚æ•°ï¼Œå®ƒå­˜åœ¨äºç©ºé—´ $ğ‘ $ ä¸­ï¼

ç°åœ¨è€ƒè™‘æŸå¤±å‡½æ•°ä¸­çš„æœ€å°äºŒä¹˜ä¼°è®¡é¡¹å’Œæ­£åˆ™åŒ–é¡¹ï¼Œ

![](img/83fdbd948d363151e2f912612c25b44e.png)

LASSOï¼ˆå·¦ï¼‰å’Œå²­å›å½’ï¼ˆå³ï¼‰çš„æ­£åˆ™åŒ–æŸå¤±å’Œå¹³æ–¹è¯¯å·®æŸå¤±ç­‰é«˜çº¿ã€‚

+   æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå½“æˆ‘ä»¬å¹³è¡¡æ­£åˆ™åŒ–å’Œå¹³æ–¹è¯¯å·®æŸå¤±é¡¹æ—¶ï¼Œéšç€ $\lambda$ çš„å¢åŠ ï¼Œæ¨¡å‹å‚æ•°ä»æœ€å°äºŒä¹˜æ³•éå†åˆ° 0ï¼Œç”±äº LASSO æ­£åˆ™åŒ–é¡¹çš„å½¢çŠ¶ï¼Œæ¨¡å‹å‚æ•°æ›´æœ‰å¯èƒ½æ”¶ç¼©åˆ° 0.0

ä¸ºäº†å¸®åŠ©å¯è§†åŒ–éšç€ $\lambda$ çš„å˜åŒ–ï¼Œå²­å›å½’ä¸ LASSO å›å½’è®­ç»ƒæ¨¡å‹å‚æ•°çš„å˜åŒ–ï¼Œæˆ‘æ„å»ºäº†ä¸€ä¸ªäº¤äº’å¼ Python [çº¿æ€§è§£ä»ªè¡¨æ¿](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Linear_Solutions.ipynb)ã€‚

![](img/07f28f16502ca9a33065e5ae4077a163.png)

äº¤äº’å¼ä»ªè¡¨æ¿ä»¥å¯è§†åŒ–å¹³æ–¹è¯¯å·®å’Œæ”¶ç¼©æŸå¤±ã€‚

æˆ‘ä»¬å¯ä»¥çœ‹åˆ° LASSO åœ¨é¢„æµ‹çš„åŒæ—¶è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚

## æ•°å€¼è§£

$ğ¿Â¹$ èŒƒæ•°æ²¡æœ‰è§£æè§£ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªéå¯å¾®åˆ†çš„åˆ†æ®µå‡½æ•°ï¼ˆåŒ…å«ç»å¯¹å€¼ï¼‰ã€‚

+   åœ¨ LASSO ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨æ•°å€¼è§£ï¼Œä¾‹å¦‚ï¼Œè¿­ä»£æ¢¯åº¦ä¸‹é™è§£è€Œä¸æ˜¯è§£æè§£ï¼Œä¾‹å¦‚ï¼Œçº¿æ€§å›å½’å’Œå²­å›å½’

+   Tibshirani (2012) è¯æ˜äº†åœ¨æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯è¿ç»­çš„æƒ…å†µä¸‹ï¼Œå¯¹äºä»»æ„æ•°é‡çš„ç‰¹å¾ï¼Œğ‘šï¼ŒLASSO è§£æ˜¯å”¯ä¸€çš„ã€‚å› æ­¤ï¼ŒæŸå¤±å‡½æ•°å…·æœ‰å…¨å±€æœ€å°å€¼ã€‚

å›å¿†ä¸€ä¸‹ LASSO æŸå¤±å‡½æ•°ï¼Œ

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)Â² + \lambda \sum_{j=1}^m |b_{\alpha}| $$

æˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªä¾‹å­æ¥è¯´æ˜æ¨¡å‹å‚æ•° $b_1$ çš„æ•°å€¼è§£ï¼Œ

![](img/cb6737831e2959fec37f3c649753e935.png)

ä¸­ç­‰æ–œç‡ï¼ˆå·¦ä¾§ï¼‰å’Œä½æ–œç‡ï¼ˆå³ä¾§ï¼‰çš„è®­ç»ƒè¯¯å·®ã€‚

ç°åœ¨æˆ‘ä»¬è®¡ç®—è®¸å¤š $b_1$ çš„æƒ…å†µï¼Œå¹¶å¯è§†åŒ–æŸå¤±ä¸æ¨¡å‹å‚æ•°çš„å¯¹æ¯”å›¾ï¼Œ

![](img/bb152078a3aa5de4ab91fc1bd73d4892.png)

æŸå¤±ä¸ $b_1$ æ¨¡å‹å‚æ•°çš„å¯¹æ¯”å›¾ï¼Œä½å’Œé«˜æƒ…å†µçªå‡ºæ˜¾ç¤ºã€‚

æ‰¾åˆ°ä½¿æŸå¤±å‡½æ•°æœ€å°åŒ–çš„æ¨¡å‹å‚æ•°æ˜¯æ•°å€¼ä¼˜åŒ–ã€‚

+   å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨å¸¸è§çš„æ•°å€¼ä¼˜åŒ–æ–¹æ³•æ¥è®­ç»ƒæˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹

## ç½‘æ ¼æœç´¢ï¼Œæš´åŠ›ä¼˜åŒ–

æˆ‘ä»¬å¯ä»¥å°è¯•æ‰€æœ‰æ¨¡å‹å‚æ•°çš„ç»„åˆï¼Œé€šè¿‡è¶³å¤Ÿçš„ç¦»æ•£åŒ–ï¼Œå¹¶ä¿ç•™ä½¿æŸå¤±å‡½æ•°æœ€å°åŒ–çš„æ¨¡å‹å‚æ•°ç»„åˆï¼Œ

+   å¯¹äºå•ä¸ªæ¨¡å‹å‚æ•°æ˜¯å¯èƒ½çš„

+   ç”±äºå¯èƒ½çš„æ¨¡å‹å‚æ•°å€¼ç»„åˆå¾ˆå¤§ï¼Œå¯¹äºå¤§å¤šæ•°æœºå™¨æ¥è¯´ä¸åˆ‡å®é™…

![](img/a0d4db417b916675d6246cc896f46f62.png)

æ¨¡å‹å‚æ•°ç½‘æ ¼æœç´¢ï¼Œæš´åŠ›ä¼˜åŒ–ï¼Œå¯¹ 1 ä¸ªæ¨¡å‹å‚æ•°ï¼ˆä¸Šæ–¹ï¼‰å’Œ 2 ä¸ªæ¨¡å‹å‚æ•°ï¼ˆä¸‹æ–¹ï¼‰çš„æŸå¤±å‡½æ•°è¿›è¡Œè§„åˆ™é‡‡æ ·ã€‚ .

æ¨¡å‹å‚æ•°çš„ç»„åˆæ˜¯ï¼Œ

$$ ğ‘›_ğ‘=ğ‘›_{ğ‘ğ‘–ğ‘›ğ‘ }^{ğ‘›_ğ‘} $$

å…¶ä¸­ $ğ‘›_ğ‘$ æ˜¯æ¨¡å‹å‚æ•°çš„æ•°é‡ï¼Œ$ğ‘›_{ğ‘ğ‘–ğ‘›ğ‘ }$ æ˜¯æ¯ä¸ªæ¨¡å‹å‚æ•°çš„ç¦»æ•£åŒ–æ•°é‡ã€‚

+   åœ¨è´å¶æ–¯æ–¹æ³•ä¸­ï¼Œæ¨¡å‹å‚æ•°ç”±åˆ†å¸ƒè¡¨ç¤ºï¼Œç©ºé—´çš„å¤§å°ç”šè‡³æ›´å¤§ã€‚

## æ¢¯åº¦ä¸‹é™ä¼˜åŒ–

æ•°å€¼è§£çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•å¦‚ä¸‹ï¼Œ

1.  ä»ä¸€ä¸ªéšæœºçš„æ¨¡å‹å‚æ•°å¼€å§‹

1.  è®¡ç®—æŸå¤±å‡½æ•°

1.  è®¡ç®—æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œé€šå¸¸æ²¡æœ‰æŸå¤±å‡½æ•°çš„æ–¹ç¨‹ï¼Œé€šè¿‡æ•°å€¼è®¡ç®—å±€éƒ¨æŸå¤±å‡½æ•°çš„å¯¼æ•°è¿›è¡Œé‡‡æ ·ã€‚

$$ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) = \frac{L(y_{\alpha}, F(X_{\alpha}, b_1 - \epsilon)) - L(y_{\alpha}, F(X_{\alpha}, b_1 + \epsilon))}{2\epsilon} $$

1.  é€šè¿‡æ²¿ç€æ–œå¡/æ¢¯åº¦ä¸‹é™æ¥æ›´æ–°å‚æ•°ä¼°è®¡

$$ \hat{b}_{1,t+1} = \hat{b}_{1,t} - r \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) $$

å…¶ä¸­ $ğ‘Ÿ$ æ˜¯å­¦ä¹ ç‡/æ­¥é•¿ï¼Œ$\hat{b}_{1,ğ‘¡}$ æ˜¯å½“å‰çš„æ¨¡å‹å‚æ•°ä¼°è®¡ï¼Œ$\hat{ğ‘}_{1,ğ‘¡+1}$ æ˜¯æ›´æ–°çš„å‚æ•°ä¼°è®¡ã€‚

æ¢¯åº¦æœç´¢æ”¶æ•›ï¼Œ

+   æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å°†æ‰¾åˆ°ä¸€ä¸ªå±€éƒ¨æˆ–å…¨å±€æœ€å°å€¼

æ¢¯åº¦æœç´¢æ­¥é•¿ï¼Œ

+   $ğ‘Ÿ$ å¤ªå°ï¼Œéœ€è¦å¤ªé•¿æ—¶é—´æ‰èƒ½æ”¶æ•›åˆ°è§£

+   $ğ‘Ÿ$ å¤ªå¤§ï¼Œè§£å¯èƒ½è·³è¿‡/é”™è¿‡å…¨å±€æœ€å°å€¼æˆ–å‘æ•£

![å›¾ç‰‡](img/ea7617f79b05f8efb8ee8b35f2c254b6.png)

è§£çš„æ”¶æ•›ï¼ˆå·¦ï¼‰å’Œå‘æ•£ï¼ˆå³ï¼‰ã€‚

å¤šå˜é‡ä¼˜åŒ–ï¼Œå¦‚æœæ¨¡å‹å°†å…·æœ‰è¶…è¿‡ 1 ä¸ªæ¨¡å‹å‚æ•°ï¼Œ

+   è®¡ç®—å¹¶åˆ†è§£å¤šä¸ªæ¨¡å‹å‚æ•°ä¸Šçš„æ¢¯åº¦ï¼Œç°åœ¨ä½¿ç”¨æ‰€æœ‰æ¨¡å‹å‚æ•°çš„æ¢¯åº¦å‘é‡è¡¨ç¤º

+   ä¾‹å¦‚ï¼Œå¯¹äº 2 ä¸ªæ¨¡å‹å‚æ•°ï¼Œ

$$\begin{split} \nabla L(y_{\alpha}, F(X_{\alpha}, b_1, b_2)) = \left[ \begin{array}{c} \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) \\ \nabla L(y_{\alpha}, F(X_{\alpha}, b_2)) \end{array} \right] \end{split}$$

æˆ‘ä»¬å¯ä»¥å›¾å½¢åŒ–åœ°è¡¨ç¤ºè¿™ä¸€ç‚¹ï¼Œ

![å›¾ç‰‡](img/ff8218bd8083f37ce36b750a43c46485.png)

é€šè¿‡æŸå¤±è¡¨ç¤ºçš„å‘é‡å¯¹ 2 ä¸ªæ¨¡å‹å‚æ•°è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚

+   è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„ä¼˜åŒ–æ˜¯æ¢ç´¢é«˜ç»´æ¨¡å‹å‚æ•°ç©ºé—´

ç¼“è§£å±€éƒ¨æœ€å°å€¼

1.  ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯å¤šæ¬¡å¼€å§‹å¹¶å–æœ€ä½³ç»“æœã€‚

![å›¾ç‰‡](img/cde0ae9c36b1219d5667c512d04756fd.png)

å¤šæ¬¡éšæœºå¼€å§‹ä»¥æé«˜å…¨å±€æœ€å°å€¼çš„è¯†åˆ«ã€‚

1.  ä»è¾ƒå¤§çš„å­¦ä¹ ç‡ã€æ­¥é•¿å¼€å§‹ï¼Œéšç€ $ğ‘¡=1,\dots,ğ‘‡$ çš„å‡å°‘ï¼Œè¿›è¡Œæœç´¢ç„¶åæ”¶æ•›ã€‚

+   ä½¿ç”¨æ­¥é•¿è®¡åˆ’/è‡ªé€‚åº”æ­¥é•¿è¿›è¡Œè¿­ä»£ï¼Œä¾‹å¦‚ï¼ŒAdam ä¼˜åŒ–å™¨å¸¸ç”¨äºäººå·¥ç¥ç»ç½‘ç»œã€‚

+   æ¨¡æ‹Ÿé€€ç«æœ‰ä¸€ä¸ªæ¥å—åæ­¥éª¤çš„æ¦‚ç‡è®¡åˆ’ï¼æ—©æœŸæ¥å—æ›´å¤šåæ­¥éª¤ä»¥è¿›è¡Œæ¢ç´¢ï¼ŒåæœŸæ¥å—è¾ƒå°‘åæ­¥éª¤ä»¥æ”¶æ•›ã€‚

1.  åŠ¨é‡æœ‰åŠ©äºæé«˜è§£çš„ç¨³å®šæ€§

ä½¿ç”¨æ–°çš„æ­¥é•¿ã€åŠ¨é‡ã€$\lambda$ æ›´æ–°å‰ä¸€æ­¥ï¼Œå…¶ä¸­ $\lambda$ æ˜¯å‰ä¸€æ­¥çš„æƒé‡

$$ (r \cdot \nabla L)_{t-1}^m = \lambda \cdot (r \cdot \nabla L)_{t-2} + (1 - \lambda) \cdot (r \cdot \nabla L)_{t-1} $$

æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œå¯è§†åŒ–è¿™ä¸€ç‚¹ï¼Œ

![å›¾ç‰‡](img/3bb90a5ffaaa17e9358dbf00fcc3ef14.png)

åŠ¨é‡ç”¨äºåŠ æƒå‰ä¸€æ­¥å¹¶é€šè¿‡æŸå¤±å‡½æ•°å¹³æ»‘è·¯å¾„ã€‚

+   ä»æ¯ä¸ªæ¨¡å‹å‚æ•°çš„æŸå¤±å‡½æ•°åå¯¼æ•°è®¡ç®—å‡ºçš„æ¢¯åº¦å­˜åœ¨å™ªå£°ã€‚

+   åŠ¨é‡å¹³æ»‘ï¼Œå‡å°‘è¿™ç§å™ªå£°ã€‚

åŠ¨é‡å¸®åŠ©è§£æ²¿ç€æŸå¤±å‡½æ•°çš„ä¸€èˆ¬æ–œå¡å‰è¿›ï¼Œè€Œä¸æ˜¯åœ¨å±€éƒ¨å±±è°·æˆ–å‡¹æ§½ä¸­æŒ¯è¡ã€‚

## éšæœºæ¢¯åº¦ä¸‹é™

æˆ‘ä»¬å¯èƒ½æœ‰å¤§é‡æ•°æ® $\rightarrow \nabla ğ¿_ğ‘¡$ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚

+   æˆ‘ä»¬å¯ä»¥ç”¨éšæœºé€¼è¿‘æ›¿æ¢æ¢¯åº¦ï¼Œ$\nabla L_{ğ‘¡^{\ell}}$ é€šè¿‡ä¿ç•™è®­ç»ƒæ•°æ®çš„éšæœºå­é›†ï¼Œåœ¨çº¿ï¼ˆ1 ä¸ªæ•°æ®ï¼‰æˆ–å°æ‰¹é‡ï¼ˆ>1 ä¸ªæ•°æ®ï¼Œ$ğ‘›_{ğ‘ğ‘ğ‘¡ğ‘â„}$ï¼‰ï¼Œå…¶ä¸­ $\ell$ è¡¨ç¤ºæ¢¯åº¦çš„ä¸€ä¸ªå®ç°ã€‚

+   æˆ‘ä»¬åœ¨æ¢¯åº¦ä¸‹é™ä¸­é™ä½ç²¾åº¦ï¼Œä½†åŠ å¿«è®¡ç®—é€Ÿåº¦ï¼Œå¯ä»¥æ‰§è¡Œæ›´å¤šæ­¥éª¤ï¼Œé€šå¸¸æ¯”æ¢¯åº¦ä¸‹é™å¿«

+   å¢åŠ  $ğ‘›_{ğ‘ğ‘ğ‘¡ğ‘â„}$ ä»¥æé«˜æ¢¯åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œå¹¶å‡å°‘ $ğ‘›_{ğ‘ğ‘ğ‘¡ğ‘â„}$ ä»¥åŠ å¿«æ­¥éª¤

æ ¹æ® Robbins-Siegmundï¼ˆ1971ï¼‰å®šç† - å¯¹äºå‡¸æŸå¤±å‡½æ•°æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼ï¼Œå¯¹äºéå‡¸æŸå¤±å‡½æ•°æ”¶æ•›åˆ°å…¨å±€æˆ–å±€éƒ¨æœ€å°å€¼ã€‚

**ç¨€ç–æ€§** - $ğ¿Â¹$ ç§»é™¤ç‰¹å¾ï¼Œå†…ç½®ç‰¹å¾é€‰æ‹©ï¼Œå°†æ¨¡å‹å‚æ•°ç¼©å°åˆ°æ­£å¥½ä¸º 0ï¼Œæé«˜æ¨¡å‹å‚æ•°çš„ç¨€ç–æ€§ã€‚

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = False
import os                                                     # to set current working directory 
import numpy as np                                            # arrays and matrix math
import scipy.stats as st                                      # statistical methods
import pandas as pd                                           # DataFrames
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks
from sklearn import metrics                                   # measures to check our models
from sklearn.preprocessing import StandardScaler              # standardize the features
from sklearn import linear_model                              # linear regression
from sklearn.linear_model import Ridge                        # ridge regression implemented in scikit-learn
from sklearn.linear_model import Lasso                        # LASSO regression implemented in scikit-learn
from sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation
from sklearn.model_selection import train_test_split          # train and test split
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # suppress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½é¦–å…ˆéœ€è¦å®‰è£…è¿™äº›åŒ…ä¹‹ä¸€ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£ï¼Œè¿˜æœ‰æ›´å¤šå¸®åŠ©ä¿¡æ¯ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä»¥ä¾¿å°†æŒ‡å®šçš„ç™¾åˆ†ä½æ•°å’Œä¸»æ¬¡ç½‘æ ¼çº¿æ·»åŠ åˆ°æˆ‘ä»¬çš„å›¾è¡¨ä¸­ã€‚

```py
def weighted_percentile(data, weights, perc):                 # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) 
    ix = np.argsort(data)
    data = data[ix] 
    weights = weights[ix] 
    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) 
    return np.interp(perc, cdf, data)

def histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram 
    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)
    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')
    plt.plot([avg,avg],[0.0,45],color = color)
    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)
    html_str = ''
    for df in args:
        html_str += df.head().to_html()  # Using .head() for the first few rows
    display(HTML(f'<div style="display: flex;">{html_str}</div>')) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯·ç¡®ä¿å°†æ‰€éœ€çš„æ•°æ®æ–‡ä»¶ï¼ˆè§ä¸‹æ–‡ï¼‰æ”¾ç½®åœ¨æ­¤å·¥ä½œç›®å½•ä¸­ã€‚

```py
#os.chdir("C:\PGE337")                                        # set the working directory 
```

æ‚¨å¿…é¡»æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ä»¥åŒ…å«æ‚¨è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”åœ¨ Mac ä¸Šæ ¼å¼ä¸åŒï¼ˆä¾‹å¦‚ï¼šâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½è¡¨æ ¼æ•°æ®

è¿™æ˜¯å°†æˆ‘ä»¬çš„é€—å·åˆ†éš”æ•°æ®æ–‡ä»¶åŠ è½½åˆ° Pandas DataFrame å¯¹è±¡çš„å‘½ä»¤ã€‚

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›†â€˜unconv_MV.csvâ€™ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…å«æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   å¯†åº¦ ($g/cm^{3}$)

+   å­”éš™ç‡ï¼ˆä½“ç§¯ç™¾åˆ†æ¯”ï¼‰

æ³¨æ„ï¼Œæ•°æ®é›†æ˜¯åˆæˆçš„ã€‚

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸ºâ€˜my_dataâ€™çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

```py
add_error = True                                              # add random error to the response feature
std_error = 1.0; seed = 71071

yname = 'Porosity'; xname = 'Density'                         # specify the predictor features (x2) and response feature (x1)
xmin = 1.0; xmax = 2.5                                        # set minimums and maximums for visualization 
ymin = 0.0; ymax = 25.0    
xlabel = 'Porosity'; ylabel = 'Density'                       # specify the feature labels for plotting
yunit = '%'; xunit = '$g/cm^{3}$'    
Xlabelunit = xlabel + ' (' + xunit + ')'
ylabelunit = ylabel + ' (' + yunit + ')'

#df = pd.read_csv("Density_Por_data.csv")                     # load the data from local current directory
df = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv") # load the data from my github repo
df = df.sample(frac=1.0, random_state = 73073); df = df.reset_index() # extract 30% random to reduce the number of data

if add_error == True:                                         # method to add error
    np.random.seed(seed=seed)                                 # set random number seed
    df[yname] = df[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df)) # add noise
    values = df._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray 
```

## è®­ç»ƒ-æµ‹è¯•åˆ†å‰²

ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn åŒ…ä¸­çš„ model_selection æ¨¡å—çš„ train_test_split å‡½æ•°åº”ç”¨éšæœºè®­ç»ƒ-æµ‹è¯•åˆ†å‰²ã€‚

```py
x_train, x_test, y_train, y_test = train_test_split(df[xname],df[yname],test_size=0.25,random_state=73073) # train and test split
# y_train = pd.DataFrame({yname:y_train.values}); y_test = pd.DataFrame({yname:y_test.values}) # optional to ensure response is a DataFrame

y = df[yname].values.reshape(len(df))                         # features as 1D vectors
x = df[xname].values.reshape(len(df))

df_train = pd.concat([x_train,y_train],axis=1)                # features as train and test DataFrames
df_test = pd.concat([x_test,y_test],axis=1) 
```

## å¯è§†åŒ– DataFrame

å¯è§†åŒ– DataFrame æ˜¯æ£€æŸ¥æ•°æ®çš„æœ‰ç”¨ç¬¬ä¸€æ­¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ï¼Œç­‰ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨â€˜headâ€™DataFrame æˆå‘˜å‡½æ•°æ¥é¢„è§ˆï¼ˆæ ¼å¼æ•´æ´ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚

+   æˆ‘ä»¬æœ‰ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°æ¥å¹¶æ’é¢„è§ˆè®­ç»ƒå’Œæµ‹è¯• DataFrameã€‚

```py
print('   Training DataFrame      Testing DataFrame')
display_sidebyside(df_train,df_test) 
```

```py
 Training DataFrame      Testing DataFrame 
```

|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| 24 | 1.778580 | 11.426485 |
| 101 | 2.410560 | 8.488544 |
| 88 | 2.216014 | 10.133693 |
| 79 | 1.631896 | 12.712326 |
| 58 | 1.528019 | 16.129542 |
|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| 59 | 1.742534 | 15.380154 |
| 1 | 1.404932 | 13.710628 |
| 35 | 1.552713 | 14.131878 |
| 92 | 1.762359 | 11.154896 |
| 22 | 1.885087 | 9.403056 |

## è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯

åœ¨ DataFrames ä¸­ï¼Œæœ‰è®¸å¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ã€‚describe å‘½ä»¤ä»¥ä¸€ä¸ªæ•´æ´çš„æ•°æ®è¡¨å½¢å¼æä¾›è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼å’Œæœ€å¤§å€¼ã€‚

```py
print('     Training DataFrame         Testing DataFrame')
display_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) 
```

```py
 Training DataFrame         Testing DataFrame 
```

|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| count | 78.000000 | 78.000000 |
| mean | 1.739027 | 12.501465 |
| std | 0.302510 | 3.428260 |
| min | 0.996736 | 3.276449 |
| max | 2.410560 | 21.660179 |
|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| count | 27.000000 | 27.000000 |
| mean | 1.734710 | 12.380796 |
| std | 0.247761 | 2.916045 |
| min | 1.067960 | 7.894595 |
| max | 2.119652 | 18.133771 |

## å¯è§†åŒ–æ•°æ®

è®©æˆ‘ä»¬ä½¿ç”¨ç›´æ–¹å›¾å’Œæ•£ç‚¹å›¾æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„ä¸€è‡´æ€§å’Œè¦†ç›–èŒƒå›´ã€‚

+   æ£€æŸ¥ä»¥ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•æ•°æ®è¦†ç›–äº†å¯èƒ½çš„é¢„æµ‹ç‰¹å¾ç»„åˆçš„èŒƒå›´

+   ç¡®ä¿æµ‹è¯•æ¡ˆä¾‹ä¸ä¼šè¶…å‡ºè®­ç»ƒæ•°æ®èŒƒå›´

```py
nbins = 20                                                    # number of histogram bins

plt.subplot(221)
freq1,_,_ = plt.hist(x=df_train[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=True,label='Train')
freq2,_,_ = plt.hist(x=df_test[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=True,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(xname + ' (' + xunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  
plt.xlim([xmin,xmax]); plt.legend(loc='upper right')   

plt.subplot(222)
freq1,_,_ = plt.hist(x=df_train[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=True,label='Train')
freq2,_,_ = plt.hist(x=df_test[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=True,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(yname + ' (' + yunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  
plt.xlim([ymin,ymax]); plt.legend(loc='upper right')   

plt.subplot(223)                                              # plot the model
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.title('Porosity vs Density')
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.3, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![å›¾åƒ](img/3766b457d4d8b8c75cfa925ac2f27fad.png)

## Linear Regression Model

è®©æˆ‘ä»¬å…ˆè®¡ç®—çº¿æ€§å›å½’æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ scikit learnï¼Œç„¶åå°†ç›¸åŒçš„æµç¨‹æ‰©å±•åˆ°å²­å›å½’ã€‚

+   æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œ$\phi = f(\rho)$ï¼Œå…¶ä¸­ $\phi$ æ˜¯å­”éš™ç‡ï¼Œ$\rho$ æ˜¯å¯†åº¦ã€‚

+   æˆ‘ä»¬ä¹Ÿå¯ä»¥è¯´ï¼Œæˆ‘ä»¬æœ‰â€œå¯†åº¦å›å½’åˆ°å­”éš™ç‡â€ã€‚

æˆ‘ä»¬æ¨¡å‹å…·æœ‰è¿™ä¸ªç‰¹å®šçš„æ–¹ç¨‹ï¼Œ

$$ \phi = b_1 \times \rho + b_0 $$

```py
linear_reg = linear_model.LinearRegression()                  # instantiate the model

linear_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters
x_model = np.linspace(xmin,xmax,10)
y_model_linear = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data 

plt.subplot(111)                                              # plot the data, model with model parameters
plt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])
plt.title('Linear Regression Model, Porosity = f(Density)')
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾åƒ](img/5330f8e13578eabf8f67b4e8379f1dd5.png)

ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°åœ¨é¢„æµ‹å‡½æ•°ä¸­å¯¹é¢„æµ‹ç‰¹å¾åº”ç”¨äº†é¢å¤–çš„é‡å¡‘æ“ä½œã€‚

```py
y_linear_model = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data 
```

è¿™æ˜¯å› ä¸º scikit-learn å‡è®¾æœ‰å¤šä¸ªé¢„æµ‹ç‰¹å¾ï¼›å› æ­¤ï¼ŒæœŸæœ›ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼ŒåŒ…å«æ ·æœ¬ï¼ˆè¡Œï¼‰å’Œç‰¹å¾ï¼ˆåˆ—ï¼‰ï¼Œä½†æˆ‘ä»¬åªæœ‰ä¸€ç»´å‘é‡ã€‚

+   é‡å¡‘æ“ä½œå°†ä¸€ç»´å‘é‡è½¬æ¢ä¸ºä¸€ä¸ªåªæœ‰ä¸€åˆ—çš„äºŒç»´å‘é‡

## Linear Regression Model Checks

è®©æˆ‘ä»¬è¿›è¡Œä¸€äº›å¿«é€Ÿæ¨¡å‹æ£€æŸ¥ã€‚å¯ä»¥åšå¾—æ›´å¤šï¼Œä½†ä¸ºäº†ç®€æ´ï¼Œè¿™é‡Œæˆ‘é™åˆ¶äº†è¿™ä¸ªèŒƒå›´ã€‚

+   æœ‰å…³æ›´å¤šä¿¡æ¯åŠæ£€æŸ¥ï¼Œè¯·å‚é˜…çº¿æ€§å›å½’ç« èŠ‚

```py
y_pred_linear = linear_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data
r_squared_linear = metrics.r2_score(df_test[yname].values, y_pred_linear)

plt.subplot(121)                                              # plot testing diagnostics 
plt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
# plt.scatter(df_test[xname], y_pred,color='grey',edgecolor='black',s = 40, alpha = 1.0, label = 'predictions',zorder=100)
plt.scatter(df_test[xname], y_pred_linear,color='white',s=120,marker='o',linewidths=1.0, edgecolors="black",zorder=300)
plt.scatter(df_test[xname], y_pred_linear,color='red',s=90,marker='*',linewidths=0.5, edgecolors="black",zorder=320,label='Predictions')

plt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])
plt.annotate(r'$rÂ²$ :' + str(np.round(r_squared_linear,2)),[1.97,15])
plt.title('Linear Regression Model, Porosity = f(Density)')
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

y_res_linear = y_pred_linear - df_test['Porosity'].values     # calculate the test residual

plt.subplot(122)
plt.hist(y_res_linear, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))
plt.title("Error Residual at Testing Data"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')
plt.vlines(0,0,5.5,color='black',ls='--',lw=2)
plt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics
plt.annotate(r'$\overline{\Delta{y}}$: ' + str(round(np.average(y_res_linear),2)),[-4,4.4])
plt.annotate(r'$\sigma_{\Delta{y}}$: ' + str(np.round(np.std(y_res_linear),2)),[-4,4.1])
add_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾åƒ](img/cf89ec094ccb520da30fa1ee810410b6.png)

## Ridge Regression Model

è®©æˆ‘ä»¬ç”¨ scikit-learn çš„å²­å›å½’æ–¹æ³•æ›¿æ¢ scikit-learn çš„çº¿æ€§å›å½’æ–¹æ³•ã€‚

+   æ³¨æ„ï¼Œæˆ‘ä»¬ç°åœ¨å¿…é¡»è®¾ç½® $\lambda$ è¶…å‚æ•°ã€‚

+   åœ¨ scikit-learn ä¸­ï¼Œè¶…å‚æ•°é€šè¿‡æ¨¡å‹çš„å®ä¾‹åŒ–æ¥è®¾ç½®

```py
lam = 1.0                                                     # lambda hyperparameter

ridge_reg = Ridge(alpha=lam)                                  # instantiate the model

ridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters
x_model = np.linspace(xmin,xmax,10)
y_model_ridge = ridge_reg.predict(x_model.reshape(10,1)) # predict with the fit model

plt.subplot(111)                                              # plot the data, model with model parameters
plt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Ridge Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])
plt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(lam))
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾åƒ](img/a6448c37319c913a38a278536faa63c1.png)

è®©æˆ‘ä»¬é‡å¤ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹æ‰€åº”ç”¨çš„ç®€å•æ¨¡å‹æ£€æŸ¥ã€‚

```py
y_pred_ridge = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data
r_squared = metrics.r2_score(df_test[yname].values, y_pred_ridge)

plt.subplot(121)                                              # plot testing diagnostics 
plt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Ridge Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.scatter(df_test[xname], y_pred_ridge,color='white',s=120,marker='o',linewidths=1.0, edgecolors="black",zorder=300)
plt.scatter(df_test[xname], y_pred_ridge,color='red',s=90,marker='*',linewidths=0.5, edgecolors="black",zorder=320,label='Predictions')

plt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])
plt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(lam))
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

y_res_ridge = y_pred_ridge - df_test['Porosity'].values       # calculate the test residual

plt.subplot(122)
plt.hist(y_res_ridge, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))
plt.title("Error Residual at Testing Data"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')
plt.vlines(0,0,5.5,color='black',ls='--',lw=2)
plt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics
plt.annotate(r'$\overline{\Delta{y}}$: ' + str(round(np.average(y_res_ridge),2)),[-4,4.4])
plt.annotate(r'$\sigma_{\Delta{y}}$: ' + str(np.round(np.std(y_res_ridge),2)),[-4,4.1])
add_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾åƒ](img/5ac80d876690b7e916dd4280243aed8a.png)

å¾ˆæœ‰è¶£ï¼Œæˆ‘ä»¬è§£é‡Šçš„æ–¹å·®æ›´å°‘ï¼Œå¹¶ä¸”æ®‹å·®æ ‡å‡†å·®æ›´å¤§ï¼ˆæ›´å¤šè¯¯å·®ï¼‰ã€‚

+   å¯¹äºæˆ‘ä»¬ä»»æ„é€‰æ‹©çš„è¶…å‚æ•° $\lambda$ï¼Œå²­å›å½’å®é™…ä¸Šå‡å°‘äº†æµ‹è¯•æ–¹å·®è§£é‡Šå’Œå‡†ç¡®åº¦

+   è¿™å¹¶ä¸å¥‡æ€ªï¼Œæˆ‘ä»¬å®é™…ä¸Šå¹¶æ²¡æœ‰è°ƒæ•´è¶…å‚æ•°ä»¥è·å¾—æœ€ä½³æ¨¡å‹ï¼

## LASSO Regression Model

è®©æˆ‘ä»¬ç”¨ scikit learn çš„ LASSO å›å½’æ–¹æ³•æ›¿æ¢ scikit learn çš„çº¿æ€§å›å½’å’Œå²­å›å½’æ–¹æ³•ã€‚æ³¨æ„ï¼Œå†æ¬¡å¿…é¡»è®¾ç½® lambda è¶…å‚æ•°ã€‚

+   è®°ä½ï¼Œlambda è¶…å‚æ•° $\lambda$ æ˜¯åœ¨æ¨¡å‹å®ä¾‹åŒ–æ—¶è®¾ç½®çš„

```py
lam = 0.1                                                     # lambda hyperparameter

lasso_reg = Lasso(alpha=lam)                                  # instantiate the model

lasso_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters
x_model = np.linspace(xmin,xmax,10)
y_model_lasso = lasso_reg.predict(x_model.reshape(10,1))      # predict with the fit model

plt.subplot(111)                                              # plot the data, model with model parameters
plt.plot(x_model,y_model_lasso, color='red', linewidth=2,label='LASSO Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.annotate('LASSO Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(lasso_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(lasso_reg.intercept_,2)),[1.97,16])
plt.title('LASSO Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(lam))
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡é“¾æ¥](img/c9cb274630b2d8b01b6f4bd114f7ef0e.png)

è®©æˆ‘ä»¬é‡å¤ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹æ‰€åº”ç”¨çš„ç®€å•æ¨¡å‹æ£€æŸ¥ã€‚

## LASSO è¶…å‚æ•°è°ƒæ•´

åœ¨ä¸Šé¢ï¼Œæˆ‘ä»¬åªæ˜¯ä»»æ„é€‰æ‹©äº†ä¸€ä¸ª $\lambda$ è¶…å‚æ•°ï¼Œç°åœ¨è®©æˆ‘ä»¬è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ã€‚

+   åœ¨äº¤å‰éªŒè¯ä¸­æ€»ç»“ MSE å¹¶åœ¨å¹¿æ³›çš„ $\lambda$ å€¼ä¸Šå¾ªç¯

è®°ä½ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼Œ

$$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)Â² $$

å…¶ä¸­ $y_i$ æ˜¯å®é™…å€¼ï¼Œ$\hat{y}_i$ æ˜¯é¢„æµ‹å€¼ï¼Œ$n$ æ˜¯æ•°æ®ç‚¹çš„æ•°é‡ã€‚

```py
score = []                                                    # code modified from StackOverFlow by Dimosthenis

nlambda = 100
lambda_mat = np.logspace(-2,5,nlambda)
for ilam in range(0,nlambda):
    lasso_reg = Lasso(alpha=lambda_mat[ilam])
    scores = cross_val_score(estimator=lasso_reg, X= df['Density'].values.reshape(-1, 1), 
                             y=df['Porosity'].values, cv=10, n_jobs=4, scoring = "neg_mean_squared_error") # Perform 10-fold cross validation
    score.append(abs(scores.mean()))

plt.subplot(111)
plt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)
plt.title('LASSO Regression Test Mean Square Error vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')
plt.xlim(1.0e-2,1.0e5); plt.ylim(0.001,20.0); plt.xscale('log')
plt.vlines(0.1,0,20,color='red',lw=2); plt.vlines(0.9,0,20,color='red',lw=2,zorder=10)
plt.annotate('Linear Regression',[0.075,12.5],rotation=90.0,color='red',zorder=10)
plt.annotate('Mean of Response Feature',[1.06,11.4],rotation=90.0,color='red',zorder=10)
plt.fill_between([0.01,0.1],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.fill_between([0.9,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.grid(which='both')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡é“¾æ¥](img/aa9ecd6f50ebe5d0f0c5ca2842d39012.png)

ä»ä¸Šé¢æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ä»»ä½• $\lambda > 0.1$ éƒ½ä¼šå¯¼è‡´æœ€å°çš„æµ‹è¯•å‡æ–¹è¯¯å·®ã€‚

+   é˜ˆå€¼è¡Œä¸ºæ˜¯ç”±äºä»¥ä¸‹äº‹å®ï¼šåœ¨è¿™ä¸ªæ­£åˆ™åŒ–æ°´å¹³ä»¥ä¸‹ï¼Œæ¨¡å‹çš„è¡Œä¸ºç±»ä¼¼äºçº¿æ€§å›å½’ã€‚

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªè¶…å‚æ•°åœ¨æ‰€æœ‰æ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚

```py
lam = 0.01                                                      # tuned hyperparameter
lasso_tuned = Lasso(alpha=lam)                                  # instantiate the model
lasso_tuned.fit(df[xname].values.reshape(len(df),1), df[yname]) # train the model parameters on all data

y_pred_lasso_tuned = lasso_tuned.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data
r_squared = metrics.r2_score(df_test[yname].values, y_pred_lasso_tuned)

plt.subplot(121)                                              # plot testing diagnostics 
plt.plot(x_model,y_model_lasso, color='red', linewidth=2,label='LASSO Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.scatter(df_test[xname], y_pred_ridge,color='white',s=120,marker='o',linewidths=1.0, edgecolors="black",zorder=300)
plt.scatter(df_test[xname], y_pred_ridge,color='red',s=90,marker='*',linewidths=0.5, edgecolors="black",zorder=320,label='Predictions')

plt.annotate('LASSO Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])
plt.title('Tuned LASSO Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(lam))
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

y_res_ridge = y_pred_ridge - df_test['Porosity'].values       # calculate the test residual

plt.subplot(122)
plt.hist(y_res_ridge, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))
plt.title("Error Residual at Testing Data"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')
plt.vlines(0,0,5.5,color='black',ls='--',lw=2)
plt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics
plt.annotate(r'$\overline{\Delta{y}}$: ' + str(round(np.average(y_res_ridge),2)),[-4,4.4])
plt.annotate(r'$\sigma_{\Delta{y}}$: ' + str(np.round(np.std(y_res_ridge),2)),[-4,4.1])
add_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡é“¾æ¥](img/64e13b841e7ab592022426ef7bc2bddf.png)

ä½¿ç”¨æˆ‘ä»¬è°ƒæ•´è¿‡çš„ $\lambda$ è¶…å‚æ•°ï¼Œ

```py
lam = 0.01 
```

æˆ‘ä»¬çš„æ¨¡å¼ä¸çº¿æ€§å›å½’ç›¸åŒã€‚

+   æˆ‘ä»¬èƒ½å¦åˆ›é€ ä¸€ä¸ªæœ€ä½³æ¨¡å‹ä¸æ˜¯çº¿æ€§å›å½’çš„æƒ…å†µï¼Ÿå³ï¼Œæ­£åˆ™åŒ–æœ‰å¸®åŠ©çš„æƒ…å†µï¼Ÿ

+   æ˜¯çš„ï¼Œæˆ‘ä»¬å¯ä»¥ã€‚è®©æˆ‘ä»¬ç§»é™¤å¤§éƒ¨åˆ†æ ·æœ¬ä»¥åˆ›å»ºæ•°æ®ç¨€ç–æ€§ï¼Œå¹¶æ·»åŠ å¤§é‡å™ªå£°ï¼

æ‰¿è®¤ï¼Œæˆ‘è¿­ä»£äº†æ ·æœ¬å’Œå™ªå£°çš„éšæœºç§å­ä»¥å¾—åˆ°è¿™ä¸ªç»“æœã€‚

+   å°‘é‡æ•°æ®ï¼ˆä½ $n$ï¼‰å’Œé«˜ç»´æ€§ï¼ˆé«˜ $m$ï¼‰é€šå¸¸ä¼šå¯¼è‡´ LASSO æ¯”çº¿æ€§å›å½’è¡¨ç°æ›´å¥½

```py
df_sample = df.copy(deep=True).sample(n=10,random_state=11)
noise_stdev = 3.0
np.random.seed(seed=15)
df_sample['Porosity'] = df_sample['Porosity'] + np.random.normal(0.0, noise_stdev, size=len(df_sample))

score = []                                                    # code modified from StackOverFlow by Dimosthenis

nlambda = 100
lambda_mat = np.logspace(-3,5,nlambda)
for ilam in range(0,nlambda):
    lasso_reg = Lasso(alpha=lambda_mat[ilam])
    scores = cross_val_score(estimator=lasso_reg, X= df_sample['Density'].values.reshape(-1, 1), 
                             y=df_sample['Porosity'].values, cv=2, n_jobs=4, scoring = "neg_mean_squared_error") # Perform 10-fold cross validation
    score.append(abs(scores.mean()))

plt.subplot(111)
plt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)
plt.title('LASSO Regression Test Mean Square Error vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')
plt.xlim(1.0e-3,1.0e5); plt.ylim(0.001,20.0); 
plt.xscale('log')
plt.vlines(0.003,0,20,color='red',lw=2); plt.vlines(0.4,0,20,color='red',lw=2,zorder=10); plt.vlines(0.1,0,20,color='red',lw=2,zorder=10);
plt.annotate('Linear Regression',[0.0022,12.5],rotation=90.0,color='red',zorder=10)
plt.annotate(r'LASSO Tuned $\lambda$',[0.075,12.5],rotation=90.0,color='red',zorder=10)
plt.annotate('Mean of Response Feature',[0.46,11.7],rotation=90.0,color='red',zorder=10)
plt.fill_between([0.001,0.003],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.fill_between([0.4,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.grid(which='both')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡é“¾æ¥](img/03c4aefb2d59d3a6e5c4b3652011ed5c.png)

## è°ƒæŸ¥ Lambda è¶…å‚æ•°å¯¹æ¨¡å‹å‚æ•°çš„å½±å“

è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å·²åŠ è½½çš„å¤šå…ƒæ•°æ®é›†ã€‚è¿™æ ·æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿæ¨¡å‹åœ¨ä¸€ç³»åˆ—ç‰¹å¾å’Œ lambda è¶…å‚æ•°å€¼èŒƒå›´å†…çš„è¡Œä¸ºã€‚æˆ‘ä»¬å°†æ‰§è¡Œä¸€ç³»åˆ—æ­¥éª¤æ¥è¾¾åˆ°å…³é”®ç‚¹ï¼

+   åŠ è½½ä¸€ä¸ªå¤šå…ƒæ•°æ®é›†

+   è®¡ç®—æ±‡æ€»ç»Ÿè®¡é‡

+   æ ‡å‡†åŒ–ç‰¹å¾

ç„¶åï¼Œæˆ‘ä»¬å°†æ”¹å˜è¶…å‚æ•°å¹¶è§‚å¯Ÿæ¨¡å‹å‚æ•°ã€‚

### åŠ è½½ä¸€ä¸ªå¤šå…ƒæ•°æ®é›†

è®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªåŒ…å«æ›´å¤šå˜é‡çš„æ•°æ®é›†æ¥å±•ç¤º LASSO å›å½’çš„ç‰¹å¾æ’åºï¼Œå¹¶æ¯”è¾ƒè¶…å‚æ•°å€¼ä¸‹æ¨¡å‹å‚æ•°çš„è¡Œä¸ºã€‚æ•°æ®é›†â€˜unconv_MV_v5.csvâ€™æ˜¯ä¸€ä¸ªåŸºäº 1,000 ä¸ªéå¸¸è§„äº•çš„é€—å·åˆ†éš”æ–‡ä»¶ï¼ŒåŒ…æ‹¬ç‰¹å¾ï¼Œ

+   å¥½çš„å¹³å‡å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   å‰ªåˆ‡æ¯”ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   ç»ç’ƒè´¨åå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹ç”Ÿäº§ 90 å¤©å¹³å‡ï¼ˆMCFPDï¼‰ã€‚

æˆ‘ä»¬å‡è®¾åˆå§‹ç”Ÿäº§æ˜¯å“åº”ç‰¹å¾ï¼Œæ‰€æœ‰å…¶ä»–ç‰¹å¾æ˜¯é¢„æµ‹ç‰¹å¾ã€‚

æ­¤å¤–ï¼Œæ‚¨è¿˜å¯ä»¥é€šè¿‡åˆ‡æ¢ mv_data æ•´æ•°åˆ° 1 æ¥å°è¯•å¦ä¸€ä¸ªç±»ä¼¼çš„æ•°æ®é›†ã€‚

```py
mv_data = 2

if mv_data == 1:
    df_mv = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv")
    df_mv = df_mv.drop('WellIndex',axis = 1)                  # remove the well index feature
elif mv_data == 2:
    df_mv = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv")
    df_mv = df_mv.rename({'Prod':'Production'},axis=1)
    df_mv = df_mv.drop('Well',axis = 1)                       # remove the well index feature
df_mv.head()                                                  # load the comma delimited data file 
```

|  | Por | Perm | AI | Brittle | TOC | VR | Production |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 12.08 | 2.92 | 2.80 | 81.40 | 1.16 | 2.31 | 4165.196191 |
| 1 | 12.38 | 3.53 | 3.22 | 46.17 | 0.89 | 1.88 | 3561.146205 |
| 2 | 14.02 | 2.59 | 4.01 | 72.80 | 0.89 | 2.72 | 4284.348574 |
| 3 | 17.67 | 6.75 | 2.63 | 39.81 | 1.08 | 1.88 | 5098.680869 |
| 4 | 17.52 | 4.57 | 3.18 | 10.94 | 1.51 | 1.90 | 3406.132832 |

### è®¡ç®—æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯

è®©æˆ‘ä»¬è®¡ç®—æˆ‘ä»¬çš„å¤šå…ƒæ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ã€‚

```py
df_mv.describe().transpose() 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 200.0 | 14.991150 | 2.971176 | 6.550000 | 12.912500 | 15.070000 | 17.402500 | 23.550000 |
| Perm | 200.0 | 4.330750 | 1.731014 | 1.130000 | 3.122500 | 4.035000 | 5.287500 | 9.870000 |
| AI | 200.0 | 2.968850 | 0.566885 | 1.280000 | 2.547500 | 2.955000 | 3.345000 | 4.630000 |
| Brittle | 200.0 | 48.161950 | 14.129455 | 10.940000 | 37.755000 | 49.510000 | 58.262500 | 84.330000 |
| TOC | 200.0 | 0.990450 | 0.481588 | -0.190000 | 0.617500 | 1.030000 | 1.350000 | 2.180000 |
| VR | 200.0 | 1.964300 | 0.300827 | 0.930000 | 1.770000 | 1.960000 | 2.142500 | 2.870000 |
| Production | 200.0 | 4311.219852 | 992.038414 | 2107.139414 | 3618.064513 | 4284.687348 | 5086.089761 | 6662.622385 |

### æ ‡å‡†åŒ–ç‰¹å¾

è®©æˆ‘ä»¬å°†ç‰¹å¾æ ‡å‡†åŒ–ï¼Œä»¥ä¾¿å…·æœ‰ï¼š

+   å¹³å‡å€¼ = 0.0

+   æ–¹å·® = æ ‡å‡†å·® = 1.0

æˆ‘ä»¬è¿™æ ·åšæ˜¯ä¸ºäº†ä½¿æ¨¡å‹å‚æ•°å…·æœ‰ç›¸ä¼¼çš„èŒƒå›´ï¼Œå¹¶ä¸”å¯ä»¥è¿›è¡Œæ¯”è¾ƒï¼Œå³åƒç‰¹å¾æ’åä¸­çš„ $\beta$ ä¸ $B$ ç³»æ•°ä¸€æ ·ã€‚

è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ï¼š

1.  ä» scikit learn å®ä¾‹åŒ– StandardScalerã€‚æˆ‘ä»¬å°†å…¶å‘½åä¸ºâ€˜scalerâ€™ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æ–¹ä¾¿åœ°åè½¬è½¬æ¢ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¿™æ ·åšçš„è¯ã€‚æˆ‘ä»¬éœ€è¦è¿™æ ·åšæ‰èƒ½å°†æˆ‘ä»¬çš„é¢„æµ‹å€¼è½¬æ¢å›å¸¸è§„çš„ç”Ÿäº§å•ä½ã€‚

```py
scaler = StandardScaler() 
```

1.  ç„¶åï¼Œæˆ‘ä»¬ä» DataFrame ä¸­æå–æ‰€æœ‰å€¼å¹¶åº”ç”¨æŒ‰åˆ—æ ‡å‡†åŒ–ã€‚ç»“æœæ˜¯ 2D ndarray

```py
sfeatures = scaler.fit_transform(df_mv.values) 
```

1.  æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ç©º DataFrame

```py
df_nmv = pd.DataFrame() 
```

1.  ç„¶åæˆ‘ä»¬å°†è½¬æ¢åçš„å€¼æ·»åŠ åˆ°æ–°çš„ DataFrame ä¸­ï¼ŒåŒæ—¶ä¿ç•™æ—§ DataFrame ä¸­çš„æ ·æœ¬ç´¢å¼•å’Œç‰¹å¾åç§°

```py
df_nmv = pd.DataFrame(sfeatures, index=df_mv.index, columns=df_mv.columns) 
```

```py
scaler = StandardScaler()                                     # instantiate the scaler 

sfeatures = scaler.fit_transform(df_mv.values)                # standardize all the values extracted from the DataFrame 
df_nmv = pd.DataFrame()                                       # instantiate a new DataFrame
df_nmv = pd.DataFrame(sfeatures, index=df_mv.index, columns=df_mv.columns) # copy the standardized values into the new DataFrame
df_nmv.head()                                                 # preview the new DataFrame 
```

|  | Por | Perm | AI | Brittle | TOC | VR | Production |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | -0.982256 | -0.817030 | -0.298603 | 2.358297 | 0.352948 | 1.152048 | -0.147565 |
| 1 | -0.881032 | -0.463751 | 0.444147 | -0.141332 | -0.209104 | -0.280931 | -0.757991 |
| 2 | -0.327677 | -1.008148 | 1.841224 | 1.748113 | -0.209104 | 2.518377 | -0.027155 |
| 3 | 0.903875 | 1.401098 | -0.599240 | -0.592585 | 0.186414 | -0.280931 | 0.795773 |
| 4 | 0.853263 | 0.138561 | 0.373409 | -2.640962 | 1.081534 | -0.214280 | -0.914640 |

### æ£€æŸ¥æ‘˜è¦ç»Ÿè®¡

è®©æˆ‘ä»¬æ£€æŸ¥æ‘˜è¦ç»Ÿè®¡ã€‚

```py
df_nmv.describe().transpose()                                 # summary statistics from the new DataFrame 
```

|  | æ•°é‡ | å¹³å‡å€¼ | æ ‡å‡†å·® | æœ€å°å€¼ | 25% | 50% | 75% | æœ€å¤§å€¼ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| å­”éš™ç‡ | 200.0 | 2.486900e-16 | 1.002509 | -2.848142 | -0.701361 | 0.026605 | 0.813617 | 2.887855 |
| æ°¸ä¹… | 200.0 | -6.217249e-17 | 1.002509 | -1.853701 | -0.699753 | -0.171282 | 0.554098 | 3.208033 |
| äººå·¥æ™ºèƒ½ | 200.0 | 4.130030e-16 | 1.002509 | -2.986650 | -0.745137 | -0.024493 | 0.665203 | 2.937664 |
| å²©è„† | 200.0 | 2.042810e-16 | 1.002509 | -2.640962 | -0.738391 | 0.095646 | 0.716652 | 2.566186 |
| æœ‰æœºç¢³ | 200.0 | 3.375078e-16 | 1.002509 | -2.457313 | -0.776361 | 0.082330 | 0.748466 | 2.476256 |
| åå°„ç‡ | 200.0 | 9.081624e-16 | 1.002509 | -3.446814 | -0.647507 | -0.014330 | 0.593853 | 3.018254 |
| ç”Ÿäº§ | 200.0 | 1.598721e-16 | 1.002509 | -2.227345 | -0.700472 | -0.026813 | 0.783049 | 2.376222 |

æˆåŠŸï¼Œæˆ‘ä»¬å·²ç»æ ‡å‡†åŒ–äº†æ‰€æœ‰ç‰¹å¾ã€‚æˆ‘ä»¬å‡†å¤‡å¥½æ„å»ºæˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚è®©æˆ‘ä»¬æå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

```py
X_train, X_test, y_train, y_test = train_test_split(df_nmv.iloc[:,:6], pd.DataFrame({'Production':df_nmv['Production']}), 
                                                    test_size=0.33, random_state=73073)
print('Number of training data = ' + str(len(X_train)) + ' and number of testing data = ' + str(len(X_test))) 
```

```py
Number of training data = 134 and number of testing data = 66 
```

### æ”¹å˜è¶…å‚æ•°å¹¶è§‚å¯Ÿæ¨¡å‹å‚æ•°

ç°åœ¨è®©æˆ‘ä»¬è§‚å¯Ÿä¸€ç³»åˆ—$\lambda$è¶…å‚æ•°å€¼ä¸‹çš„æ¨¡å‹ç³»æ•°($b_{\alpha}, \alpha = 1,\ldots,m$)ã€‚

```py
nbins = 1000                                                # number of bins to explore the hyperparameter 
df_nmv.describe().transpose()                               # summary statistics from the new DataFrame 
lams = np.logspace(-3,1,nbins)                              # make a list of lambda values
coefs = np.ndarray((nbins,6))

index = 0
for lam in lams:
    lasso_reg = Lasso(alpha=lam)                            # instantiate the model
    lasso_reg.fit(X_train, y_train)                         # fit model
    coefs[index,:] = lasso_reg.coef_                        # retrieve the coefficients
    index = index + 1

color = ['black','blue','green','red','orange','grey']
plt.subplot(111)                                            # plot the results
for ifeature in range(0,6):
    plt.semilogx(lams,coefs[:,ifeature], label = df_mv.columns[ifeature], c = color[ifeature], linewidth = 3.0)

plt.title('Standardized Model Coefficients vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); plt.ylabel('Standardized Model Coefficients')
plt.xlim(1.0e-3,1.0e1); plt.ylim(-1.0,1.0); plt.grid(); plt.legend(loc = 'lower right')
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1., wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/b105175b634806ea901b8442f3b13e47.png)

æˆ‘ä»¬çœ‹åˆ°äº†ä»€ä¹ˆï¼Ÿ

+   å¯¹äºéå¸¸ä½çš„ lambda å€¼ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½è¢«åŒ…å«

+   éšç€ lambda è¶…å‚æ•°çš„å¢åŠ ï¼Œæ€»æœ‰æœºç¢³æ˜¯ç¬¬ä¸€ä¸ªè¢«ç§»é™¤çš„é¢„æµ‹ç‰¹å¾

+   ç„¶åæ˜¯å£°é˜»æŠ—ã€é•œè´¨ä½“åå°„ç‡ã€è„†æ€§ã€å¯¹æ•°æ¸—é€ç‡å’Œæœ€ç»ˆå­”éš™ç‡ã€‚

+   åœ¨$\lambda \ge 0.8$æ—¶ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½è¢«ç§»é™¤ã€‚

è®©æˆ‘ä»¬é‡å¤è¿™ä¸ªå·¥ä½œæµç¨‹ï¼Œç”¨å²­å›å½’è¿›è¡Œæ¯”è¾ƒã€‚

```py
nbins = 1000                                                # number of bins to explore the hyperparameter 
lams = np.logspace(-2,5,nbins)       
ridge_coefs = np.ndarray((nbins,6))

index = 0
for lam in lams:
    ridge_reg = Ridge(alpha=lam)
    ridge_reg.fit(X_train, y_train) # fit model
    ridge_coefs[index,:] = ridge_reg.coef_
    index = index + 1

color = ['black','blue','green','red','orange','grey']
plt.subplot(111)
for ifeature in range(0,6):
    plt.semilogx(lams,ridge_coefs[:,ifeature], label = df_mv.columns[ifeature], c = color[ifeature], linewidth = 3.0)

plt.title('Standardized Model Coefficients vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); plt.ylabel('Standardized Model Coefficients')
plt.xlim(1.0e-2,1.0e5); plt.ylim(-1.0,1.0); plt.legend(loc = 'lower right')
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1., wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/ff11eb98e492b6fb3503b042054e1b80.png)

å²­å›å½’åœ¨é¢„æµ‹ç‰¹å¾å¯¹$\lambda$è¶…å‚æ•°å˜åŒ–å“åº”æ–¹é¢ç›¸å½“ä¸åŒã€‚

+   éšç€ lambda è¶…å‚æ•°çš„å¢åŠ ï¼Œæ²¡æœ‰é€‰æ‹©æ€§åœ°ç§»é™¤é¢„æµ‹ç‰¹å¾

+   å¯¹äº$\lambda \in [10Â¹, 10âµ]$ï¼Œä¸€ä¸ªä¸»è¦æˆåˆ†æ˜¯æ‰€æœ‰ç³»æ•°å‘é›¶çš„å‡åŒ€æ”¶ç¼©

## å±•ç¤ºè§£çš„ä¸ç¨³å®šæ€§

è®©æˆ‘ä»¬é‡å¤ä¸Šè¿°å®éªŒï¼Œå¹¶è·Ÿè¸ªæ¨¡å‹åœ¨è¶…å‚æ•°$\lambda$ä¸Šçš„æŸäº›ä¼°è®¡ã€‚

```py
nbins = 1000                                                # number of bins to explore the hyperparameter 
df_nmv.describe().transpose()                               # summary statistics from the new DataFrame 
lams = np.logspace(-4,6,nbins)                              # make a list of lambda values
coefs = np.ndarray((nbins,6))
estimates_ridge = np.zeros((nbins,len(X_test)))
estimates_lasso = np.zeros((nbins,len(X_test)))

index = 0
for lam in lams:
    lasso_reg = Lasso(alpha=lam)                            # instantiate the model
    lasso_reg.fit(X_train, y_train)                         # fit model
    ridge_reg = Ridge(alpha=lam)
    ridge_reg.fit(X_train, y_train) # fit model
    estimates_ridge[index] = ridge_reg.predict(X_test).flatten() # predict at test data
    estimates_lasso[index] = lasso_reg.predict(X_test).flatten() # predict at test data
    index = index + 1

color = ['black','blue','green','red','orange','grey']
plt.subplot(211)                                            # plot the results
for iest in range(0,6):
    plt.semilogx(lams,estimates_ridge[:,iest], label = 'Estimate #' + str(iest+1), c = color[iest], linewidth = 3.0)
plt.title('Ridge Regression - 6 Example Predictions vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); 
plt.ylabel(r'Predictions, $\hat{y}_i$')
plt.xlim(1.0e-4,1.0e6); plt.ylim(-1.0,1.5); plt.grid(); plt.legend(loc = 'lower right')
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)
plt.vlines(0.001,-1,1.5,color='red',lw=2); plt.vlines(1.0e5,-0.1,1.5,color='red',lw=2,zorder=10)
plt.annotate('Linear Regression',[0.0007,0.6],rotation=90.0,color='red',zorder=10)
plt.annotate('Mean of Response Feature',[110000,0.4],rotation=90.0,color='red',zorder=10)
plt.fill_between([0.0001,0.001],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)
plt.fill_between([1.0e5,1.0e7],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)

plt.subplot(212)                                            # plot the results
for iest in range(0,6):
    plt.semilogx(lams,estimates_lasso[:,iest], label = 'Estimate #' + str(iest+1), c = color[iest], linewidth = 3.0)
plt.title('LASSO Regression - 6 Example Predictions vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); 
plt.ylabel(r'Predictions, $\hat{y}_i$')
plt.xlim(1.0e-4,1.0e6); plt.ylim(-1.0,1.5); plt.grid(); plt.legend(loc = 'lower right')
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)
plt.vlines(0.001,-1,1.5,color='red',lw=2); plt.vlines(0.90,-1.0,1.5,color='red',lw=2,zorder=10)
plt.annotate('Linear Regression',[0.0007,0.6],rotation=90.0,color='red',zorder=10)
plt.annotate('Mean of Response Feature',[1.05,0.4],rotation=90.0,color='red',zorder=10)
plt.fill_between([0.0001,0.001],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)
plt.fill_between([0.9,100000],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=2.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/d0fb23953092ed15bd21494f123ca3f0.png)

+   å²­å›å½’ä¼°è®¡ä»çº¿æ€§å›å½’å¹³æ»‘å˜åŒ–åˆ°å“åº”ç‰¹å¾çš„å…¨çƒå‡å€¼ï¼ˆç¨³å®šæ€§ï¼‰

+   LASSO å›å½’ä¼°è®¡æ˜¾ç¤ºè·³è·ƒï¼ˆä¸ç¨³å®šæ€§ï¼‰

## æ³¨é‡Š

è¿™æ˜¯å¯¹ LASSO å›å½’çš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´ YouTube è®²åº§ä¸­çš„èµ„æºé“¾æ¥ï¼Œè§†é¢‘æè¿°ä¸­åŒ…å«èµ„æºé“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![å›¾ç‰‡](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼ˆMichael Pyrczï¼‰æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[Cockrell å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œä»–åœ¨é‚£é‡Œç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚

+   [ã€Šè®¡ç®—æœºä¸åœ°çƒç§‘å­¦ã€‹](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[ã€Šæ•°å­¦åœ°çƒç§‘å­¦ã€‹](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº†è¶…è¿‡ 70 ç¯‡[åŒè¡Œè¯„å®¡çš„å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[ã€Šåœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡ã€‹](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œåˆ†åˆ«æ˜¯[ã€ŠPython åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—ã€‹](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[ã€ŠPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„å®è·µæŒ‡å—ã€‹](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å« 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ª GitHub ä»“åº“ä¸­çš„è¯¦ç»†è®°å½•çš„å·¥ä½œæµç¨‹é“¾æ¥ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›æŒç»­æ›´æ–°çš„å†…å®¹ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³è¦ä¸€èµ·å·¥ä½œå—ï¼Ÿ

å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   å¦‚æœæ‚¨æœ‰å…´è¶£åˆä½œï¼Œæ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰ï¼Œæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢

æ›´å¤šèµ„æºè¯·è®¿é—®ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## LASSO å›å½’çš„åŠ¨æœº

è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æµç¨‹ï¼Œæ¼”ç¤ºäº†å²­å›å½’ï¼Œå¹¶å°†å…¶ä¸çº¿æ€§å›å½’å’Œå²­å›å½’åœ¨æœºå™¨å­¦ä¹ é¢„æµ‹ä¸­çš„æ¯”è¾ƒã€‚ä¸ºä»€ä¹ˆä»çº¿æ€§å›å½’å¼€å§‹ï¼Ÿ

+   çº¿æ€§å›å½’æ˜¯æœ€ç®€å•çš„å‚æ•°é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹

+   æˆ‘ä»¬é€šè¿‡è¿­ä»£æ–¹æ³•å­¦ä¹ è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨ LASSO æˆ‘ä»¬å¤±å»äº†çº¿æ€§å›å½’å’Œå²­å›å½’çš„è§£æè§£

+   è®©æˆ‘ä»¬ä»æŸå¤±å‡½æ•°å’ŒèŒƒæ•°æ¦‚å¿µå¼€å§‹

+   æˆ‘ä»¬å¯ä»¥è®¿é—®æ¨¡å‹ä¸ç¡®å®šæ€§çš„ç½®ä¿¡åŒºé—´å’Œå‚æ•°æ˜¾è‘—æ€§çš„å‡è®¾æ£€éªŒçš„è§£æè¡¨è¾¾å¼

ä¸ºä»€ä¹ˆåœ¨ LASSO å›å½’ä¹‹å‰è¿˜è¦ä»‹ç»å²­å›å½’ï¼Ÿ

+   æœ‰æ—¶çº¿æ€§å›å½’å¹¶ä¸è¶³å¤Ÿç®€å•ï¼Œæˆ‘ä»¬å®é™…ä¸Šéœ€è¦ä¸€ä¸ªæ›´ç®€å•çš„æ¨¡å‹ï¼

+   ä»‹ç»æ¨¡å‹æ­£åˆ™åŒ–å’Œè¶…å‚æ•°è°ƒæ•´çš„æ¦‚å¿µ

ç„¶åæˆ‘ä»¬å°†ä»‹ç» LASSO å›å½’ï¼Œä»¥äº†è§£æŸå¤±å‡½æ•°èŒƒæ•°é€‰æ‹©å¯¹è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„å½±å“ã€‚

+   åœ¨ LASSO å›å½’ä¸­ï¼Œæˆ‘ä»¬ç”¨ L1 æ­£åˆ™åŒ–æ›¿æ¢å²­å›å½’æŸå¤±å‡½æ•°ä¸­çš„ L2 æ­£åˆ™åŒ–é¡¹

+   å› æ­¤ï¼ŒLASSO é€ä¸ªå°†æ¨¡å‹å‚æ•°ç¼©å°åˆ° 0.0ï¼Œä»è€Œå†…ç½®äº†ç‰¹å¾é€‰æ‹©ï¼

è¿™é‡Œæœ‰ä¸€äº›å…³äºé¢„æµ‹æœºå™¨å­¦ä¹  LASSO å›å½’æ¨¡å‹çš„åŸºæœ¬ç»†èŠ‚ï¼Œè®©æˆ‘ä»¬å…ˆä»çº¿æ€§å›å½’å’Œå²­å›å½’å¼€å§‹ï¼Œç„¶åè¿‡æ¸¡åˆ°å²­å›å½’ï¼š

## çº¿æ€§å›å½’

ç”¨äºé¢„æµ‹çš„çº¿æ€§å›å½’ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹ä¸€ä¸ªæ‹Ÿåˆåˆ°æ•°æ®é›†çš„çº¿æ€§æ¨¡å‹ã€‚

![](img/ed71b506ab0f5b47754cb1c92fc8935a.png)

ä¸¾ä¾‹è¯´æ˜çº¿æ€§å›å½’æ¨¡å‹ã€‚

è®©æˆ‘ä»¬å…ˆå®šä¹‰ä¸€äº›æœ¯è¯­ï¼Œ

+   **é¢„æµ‹ç‰¹å¾** - é¢„æµ‹æ¨¡å‹çš„è¾“å…¥ç‰¹å¾ï¼Œé‰´äºæˆ‘ä»¬åªè®¨è®ºçº¿æ€§å›å½’è€Œä¸æ˜¯å¤šå…ƒçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œ$x$ã€‚åœ¨æˆ‘ä»¬çš„å›¾è¡¨ï¼ˆåŒ…æ‹¬ä¸Šé¢çš„ï¼‰ä¸­ï¼Œé¢„æµ‹ç‰¹å¾åœ¨ x è½´ä¸Šã€‚

+   **å“åº”ç‰¹å¾** - é¢„æµ‹æ¨¡å‹çš„è¾“å‡ºç‰¹å¾ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ$y$ã€‚åœ¨æˆ‘ä»¬çš„å›¾è¡¨ï¼ˆåŒ…æ‹¬ä¸Šé¢çš„ï¼‰ä¸­ï¼Œå“åº”ç‰¹å¾åœ¨ y è½´ä¸Šã€‚

ç°åœ¨ï¼Œè¿™é‡Œæ˜¯çº¿æ€§å›å½’çš„ä¸€äº›å…³é”®æ–¹é¢ï¼š

**å‚æ•°æ¨¡å‹**

è¿™æ˜¯ä¸€ä¸ªå‚æ•°é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬æ¥å—ä¸€ä¸ªå…ˆéªŒå‡è®¾çš„çº¿æ€§ï¼Œç„¶åè·å¾—ä¸€ä¸ªéå¸¸ä½çš„å‚æ•°è¡¨ç¤ºï¼Œæ˜“äºè®­ç»ƒè€Œæ— éœ€å¤§é‡æ•°æ®ã€‚

+   é€‚åˆçš„æ¨¡å‹æ˜¯ä¸€ä¸ªåŸºäºæ‰€æœ‰å¯ç”¨ç‰¹å¾ï¼ˆ$x_1,\ldots,x_m$ï¼‰çš„ç®€å•åŠ æƒçº¿æ€§åŠ æ€§æ¨¡å‹ã€‚

+   å‚æ•°æ¨¡å‹çš„å½¢å¼ä¸ºï¼š

$$ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 $$

è¿™é‡Œæ˜¯çº¿æ€§æ¨¡å‹å‚æ•°çš„å¯è§†åŒ–ï¼Œ

![å›¾ç‰‡](img/e798c74dc4ed5ec8fcbd2c8ffe0ef5fd.png)

çº¿æ€§æ¨¡å‹å‚æ•°ã€‚

**æœ€å°äºŒä¹˜æ³•**

å¯¹äº L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼Œæ¨¡å‹å‚æ•° $b_1,\ldots,b_m,b_0$ çš„è§£æè§£æ˜¯å¯ç”¨çš„ï¼Œè¯¯å·®æ˜¯æ€»å’Œå¹¶å¹³æ–¹çš„ï¼Œå·²çŸ¥ä¸ºæœ€å°äºŒä¹˜æ³•ã€‚

+   æˆ‘ä»¬æœ€å°åŒ–è®­ç»ƒæ•°æ®ä¸Šçš„è¯¯å·®ï¼Œå³æ®‹å·®å¹³æ–¹å’Œ (RSS)ï¼š

$$ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)Â² $$

å…¶ä¸­ $y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾å€¼ï¼Œ$\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ æ˜¯æ¨¡å‹é¢„æµ‹ï¼Œåœ¨ $\alpha = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šã€‚

è¿™é‡Œæ˜¯ L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼ˆMSEï¼‰çš„å¯è§†åŒ–ï¼Œ

![å›¾ç‰‡](img/e91d94eb7bac509a6ec741d8af33082f.png)

çº¿æ€§æ¨¡å‹æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®ã€‚

+   è¿™å¯ä»¥ç®€åŒ–ä¸ºè®­ç»ƒæ•°æ®ä¸Šçš„å¹³æ–¹è¯¯å·®æ€»å’Œï¼Œ

$$ \sum_{i=1}^n (\Delta y_i)Â² $$

å…¶ä¸­ $\Delta y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾è§‚å¯Ÿå€¼ $y_i$ å‡å»æ¨¡å‹é¢„æµ‹ $\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ï¼Œåœ¨ $i = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šã€‚

**å‡è®¾**

æˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹æœ‰ä¸€äº›é‡è¦çš„å‡è®¾ï¼Œ

+   **æ— è¯¯å·®** - é¢„æµ‹å˜é‡æ— è¯¯å·®ï¼Œä¸æ˜¯éšæœºå˜é‡

+   **çº¿æ€§** - å“åº”æ˜¯ç‰¹å¾ï¼ˆsï¼‰çš„çº¿æ€§ç»„åˆ

+   **å¸¸æ•°æ–¹å·®** - å“åº”è¯¯å·®åœ¨é¢„æµ‹å€¼ä¸Šæ˜¯æ’å®šçš„

+   **è¯¯å·®ç‹¬ç«‹æ€§** - å“åº”è¯¯å·®å½¼æ­¤ä¸ç›¸å…³

+   **æ— å¤šé‡å…±çº¿æ€§** - æ²¡æœ‰ç‰¹å¾ä¸å…¶ä»–ç‰¹å¾å†—ä½™

## å²­å›å½’

ä½¿ç”¨å²­å›å½’ï¼Œæˆ‘ä»¬åœ¨æœ€å°åŒ–ä¸­æ·»åŠ ä¸€ä¸ªè¶…å‚æ•° $\lambda$ï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ”¶ç¼©æƒ©ç½šé¡¹ï¼Œ$\sum_{j=1}^m b_{\alpha}Â²$ã€‚

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)Â² + \lambda \sum_{j=1}^m b_{\alpha}Â² $$

å› æ­¤ï¼Œå²­å›å½’è®­ç»ƒæ•´åˆäº†ä¸¤ä¸ªç»å¸¸ç«äº‰çš„ç›®æ ‡æ¥æ‰¾åˆ°æ¨¡å‹å‚æ•°ï¼Œ

+   æ‰¾åˆ°ä½¿è®­ç»ƒæ•°æ®è¯¯å·®æœ€å°çš„æ¨¡å‹å‚æ•°

+   å°†æ–œç‡å‚æ•°æœ€å°åŒ–åˆ°é›¶

æ³¨æ„ï¼š$\lambda$ ä¸åŒ…æ‹¬æˆªè·ï¼Œ$b_0$ã€‚

$\lambda$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œå®ƒæ§åˆ¶ç€æ¨¡å‹çš„æ‹Ÿåˆç¨‹åº¦ï¼Œå¯èƒ½ä¸æ¨¡å‹çš„åå·®-æ–¹å·®æƒè¡¡æœ‰å…³ã€‚

+   å½“ $\lambda \rightarrow 0$ æ—¶ï¼Œè§£è¶‹è¿‘äºçº¿æ€§å›å½’ï¼Œæ²¡æœ‰åå·®ï¼ˆç›¸å¯¹äºçº¿æ€§æ¨¡å‹æ‹Ÿåˆï¼‰ï¼Œä½†æ¨¡å‹æ–¹å·®å¯èƒ½æ›´é«˜

+   éšç€ $\lambda$ çš„å¢åŠ ï¼Œæ¨¡å‹æ–¹å·®å‡å°ï¼Œæ¨¡å‹åå·®å¢åŠ ï¼Œæ¨¡å‹å˜å¾—ç®€å•

+   å½“ $\lambda \rightarrow \infty$ æ—¶ï¼Œæ¨¡å‹å‚æ•° $b_1,\ldots,b_m$ æ”¶ç¼©åˆ° 0.0ï¼Œæ¨¡å‹é¢„æµ‹è¶‹è¿‘äºè®­ç»ƒæ•°æ®å“åº”ç‰¹å¾çš„å‡å€¼

## LASSO å›å½’

å¯¹äº LASSOï¼Œä¸å²­å›å½’ç±»ä¼¼ï¼Œæˆ‘ä»¬åœ¨æœ€å°åŒ–ä¸­æ·»åŠ äº†ä¸€ä¸ªè¶…å‚æ•° $\lambda$ï¼Œå¹¶ä½¿ç”¨æ”¶ç¼©æƒ©ç½šé¡¹ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨ L1 èŒƒæ•°è€Œä¸æ˜¯ L2ï¼ˆç»å¯¹å€¼ä¹‹å’Œè€Œä¸æ˜¯å¹³æ–¹ä¹‹å’Œï¼‰ã€‚

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)Â² + \lambda \sum_{j=1}^m |b_{\alpha}| $$

å› æ­¤ï¼ŒLASSO å›å½’è®­ç»ƒé›†æˆäº†ä¸¤ä¸ªç»å¸¸æ˜¯ç›¸äº’ç«äº‰çš„ç›®æ ‡ï¼Œä»¥æ‰¾åˆ°æ¨¡å‹å‚æ•°ï¼Œ

+   æ‰¾åˆ°ä½¿è®­ç»ƒæ•°æ®è¯¯å·®æœ€å°çš„æ¨¡å‹å‚æ•°

+   å°†æ–œç‡å‚æ•°æœ€å°åŒ–åˆ°é›¶

å†æ¬¡å¼ºè°ƒï¼ŒLASSO å’Œå²­å›å½’ä¹‹é—´çš„å”¯ä¸€åŒºåˆ«æ˜¯ï¼š

+   å¯¹äº LASSOï¼Œæ”¶ç¼©é¡¹è¢«è¡¨ç¤ºä¸º $\ell_1$ æƒ©ç½šï¼Œ

$$ \lambda \sum_{\alpha=1}^m |b_{\alpha}| $$

+   å¯¹äºå²­å›å½’ï¼Œæ”¶ç¼©é¡¹è¢«è¡¨ç¤ºä¸º $\ell_2$ æƒ©ç½šï¼Œ

$$ \lambda \sum_{\alpha=1}^m \left(b_{\alpha}\right)Â² $$

å½“å²­å›å½’å’Œ LASSO éƒ½å°†æ¨¡å‹å‚æ•° ($b_{\alpha}, \alpha = 1,\ldots,m$) æ”¶ç¼©åˆ°é›¶æ—¶ï¼š

+   éšç€è¶…å‚æ•° $\lambda$ çš„å¢åŠ ï¼ŒLASSO å‚æ•°ä»¥ä¸åŒçš„é€Ÿç‡è¾¾åˆ°é›¶ï¼Œå¯¹äºæ¯ä¸ªé¢„æµ‹ç‰¹å¾

+   å› æ­¤ï¼ŒLASSO æä¾›äº†ä¸€ç§ç‰¹å¾æ’åºå’Œé€‰æ‹©çš„æ–¹æ³•ï¼

$\lambda$ï¼Œå³ $\lambda$ è¶…å‚æ•°æ§åˆ¶ç€æ¨¡å‹çš„æ‹Ÿåˆç¨‹åº¦ï¼Œå¯èƒ½ä¸æ¨¡å‹çš„åå·®-æ–¹å·®æƒè¡¡æœ‰å…³ã€‚

+   å½“ $\lambda \rightarrow 0$ æ—¶ï¼Œé¢„æµ‹æ¨¡å‹è¶‹è¿‘äºçº¿æ€§å›å½’ï¼Œæ¨¡å‹åå·®è¾ƒä½ï¼Œä½†æ¨¡å‹æ–¹å·®è¾ƒé«˜

+   éšç€ $\lambda$ çš„å¢åŠ ï¼Œæ¨¡å‹æ–¹å·®å‡å°ï¼Œæ¨¡å‹åå·®å¢åŠ 

+   å½“ $\lambda \rightarrow \infty$ æ—¶ï¼Œæ‰€æœ‰ç³»æ•°éƒ½å˜ä¸º 0.0ï¼Œæ¨¡å‹æ˜¯è®­ç»ƒæ•°æ®å“åº”ç‰¹å¾çš„å‡å€¼

## **$LÂ¹$ ä¸ $LÂ²$ èŒƒæ•°**

è¿™å°†æ˜¯è®¨è®º $LÂ¹$ å’Œ $LÂ²$ èŒƒæ•°é€‰æ‹©çš„å¥½æ—¶æœºã€‚ä¸ºäº†è§£é‡Šè¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬æ¯”è¾ƒåœ¨è®­ç»ƒæ¨¡å‹å‚æ•°æ—¶ $LÂ¹$ å’Œ $LÂ²$ èŒƒæ•°åœ¨æŸå¤±å‡½æ•°ä¸­çš„æ€§èƒ½ã€‚

| å±æ€§ | æœ€å°ç»å¯¹åå·®ï¼ˆL1ï¼‰ | æœ€å°äºŒä¹˜ï¼ˆL2ï¼‰ |
| --- | --- | --- |
| **é²æ£’æ€§** | é²æ£’ | ä¸å¤ªé²æ£’ |
| **è§£çš„ç¨³å®šæ€§** | ä¸ç¨³å®šè§£ | ç¨³å®šè§£ |
| **è§£çš„æ•°é‡** | å¯èƒ½å­˜åœ¨å¤šä¸ªè§£ | æ€»æ˜¯åªæœ‰ä¸€ä¸ªè§£ |
| **ç‰¹å¾é€‰æ‹©** | å†…ç½®ç‰¹å¾é€‰æ‹© | æ— ç‰¹å¾é€‰æ‹© |
| **è¾“å‡ºç¨€ç–æ€§** | ç¨€ç–è¾“å‡º | éç¨€ç–è¾“å‡º |
| **è§£æè§£** | æ— è§£æè§£ | è§£æè§£ |

è¿™é‡Œæœ‰ä¸€äº›ä¸“é—¨é’ˆå¯¹ LASSO å›å½’çš„é‡è¦è§‚ç‚¹ï¼Œ

## ç‰¹å¾é€‰æ‹©

è®©æˆ‘ä»¬æ¯”è¾ƒå²­å›å½’çš„ $ğ‘³ğŸ$ å’Œ LASSO çš„ $ğ‘³ğŸ$ æ­£åˆ™åŒ–ä¸‹çš„è§£ã€‚

+   å¯¹äºç›¸åŒçš„æ­£åˆ™åŒ–æˆæœ¬ï¼Œæˆ‘ä»¬åœ¨æ¨¡å‹å‚æ•°ç©ºé—´ä¸­æœ‰ä¸åŒçš„å½¢çŠ¶

![](img/1f84730c294a4da3a38601107b6392e8.png)

LASSOï¼ˆå·¦ä¾§ï¼‰å’Œå²­å›å½’ï¼ˆå³ä¾§ï¼‰çš„ç­‰æ­£åˆ™åŒ–æŸå¤±ç­‰é«˜çº¿ã€‚

+   å¦‚æœ $ğ‘ $ è¶³å¤Ÿå¤§ï¼ˆ$\lambda \rightarrow 0$ï¼‰ï¼Œåˆ™é€‰æ‹©å‚æ•°çš„æœ€å°äºŒä¹˜æ‹Ÿåˆï¼Œå®ƒå­˜åœ¨äº $ğ‘ $ ç©ºé—´ä¸­ï¼

ç°åœ¨è€ƒè™‘æŸå¤±å‡½æ•°ä¸­çš„æœ€å°äºŒä¹˜ä¼°è®¡é¡¹å’Œæ­£åˆ™åŒ–é¡¹ï¼Œ

![](img/83fdbd948d363151e2f912612c25b44e.png)

LASSOï¼ˆå·¦ä¾§ï¼‰å’Œå²­å›å½’ï¼ˆå³ä¾§ï¼‰çš„ç­‰å¹³æ–¹è¯¯å·®å’Œæ­£åˆ™åŒ–æŸå¤±ç­‰é«˜çº¿ã€‚

+   æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå½“æˆ‘ä»¬å¹³è¡¡æ­£åˆ™åŒ–å’Œå¹³æ–¹è¯¯å·®æŸå¤±é¡¹æ—¶ï¼Œéšç€ $\lambda$ çš„å¢åŠ ï¼Œæ¨¡å‹å‚æ•°ä»æœ€å°äºŒä¹˜æ³•éå†åˆ° 0ï¼Œç”±äº LASSO æ­£åˆ™é¡¹çš„å½¢çŠ¶ï¼Œæ¨¡å‹å‚æ•°æ›´æœ‰å¯èƒ½æ”¶ç¼©åˆ° 0.0ã€‚

ä¸ºäº†å¸®åŠ©å¯è§†åŒ–éšç€ $\lambda$ å˜åŒ–ï¼Œå²­å›å½’ä¸ LASSO å›å½’è®­ç»ƒæ¨¡å‹å‚æ•°çš„å˜åŒ–ï¼Œæˆ‘æ„å»ºäº†ä¸€ä¸ªäº¤äº’å¼çš„ Python [çº¿æ€§è§£ä»ªè¡¨æ¿](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Linear_Solutions.ipynb)ã€‚

![](img/07f28f16502ca9a33065e5ae4077a163.png)

äº¤äº’å¼ä»ªè¡¨æ¿ä»¥å¯è§†åŒ–å¹³æ–¹è¯¯å·®å’Œæ”¶ç¼©æŸå¤±ã€‚

æˆ‘ä»¬å¯ä»¥çœ‹åˆ° LASSO åœ¨é¢„æµ‹çš„åŒæ—¶è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚

## æ•°å€¼è§£

$ğ¿Â¹$ èŒƒæ•°æ²¡æœ‰è§£æè§£ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªéå¯å¾®åˆ†çš„åˆ†æ®µå‡½æ•°ï¼ˆåŒ…å«ç»å¯¹å€¼ï¼‰ã€‚

+   ä½¿ç”¨ LASSO æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨æ•°å€¼è§£ï¼Œä¾‹å¦‚ï¼Œè¿­ä»£æ¢¯åº¦ä¸‹é™è§£è€Œä¸æ˜¯è§£æè§£ï¼Œä¾‹å¦‚ï¼Œçº¿æ€§å›å½’å’Œå²­å›å½’

+   Tibshirani (2012) è¯æ˜äº†å¯¹äºä»»ä½•æ•°é‡çš„ç‰¹å¾ $ğ‘š$ï¼Œç»™å®šæ‰€æœ‰ç‰¹å¾éƒ½æ˜¯è¿ç»­çš„ï¼ŒLASSO è§£æ˜¯å”¯ä¸€çš„ã€‚å› æ­¤ï¼ŒæŸå¤±å‡½æ•°æœ‰ä¸€ä¸ªå…¨å±€æœ€å°å€¼ã€‚

å›å¿†ä¸€ä¸‹ LASSO æŸå¤±å‡½æ•°ï¼Œ

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)Â² + \lambda \sum_{j=1}^m |b_{\alpha}| $$

æˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªä¾‹å­æ¥è¯´æ˜æ¨¡å‹å‚æ•° $b_1$ çš„æ•°å€¼è§£ï¼Œ

![](img/cb6737831e2959fec37f3c649753e935.png)

å¯¹äºä¸­ç­‰æ–œç‡ï¼ˆå·¦ä¾§ï¼‰å’Œä½æ–œç‡ï¼ˆå³ä¾§ï¼‰çš„è®­ç»ƒè¯¯å·®ã€‚

ç°åœ¨æˆ‘ä»¬è®¡ç®—è®¸å¤š $b_1$ çš„æƒ…å†µï¼Œå¹¶å¯è§†åŒ–æŸå¤±ä¸æ¨¡å‹å‚æ•°çš„å›¾ï¼Œ

![](img/bb152078a3aa5de4ab91fc1bd73d4892.png)

ä½å’Œä¸­ç­‰æƒ…å†µçš„æŸå¤±ä¸ $b_1$ æ¨¡å‹å‚æ•°çš„å›¾ã€‚

æ‰¾åˆ°ä½¿æŸå¤±å‡½æ•°æœ€å°åŒ–çš„æ¨¡å‹å‚æ•°æ˜¯æ•°å€¼ä¼˜åŒ–ã€‚

+   å› æ­¤æˆ‘ä»¬ä½¿ç”¨å¸¸è§çš„æ•°å€¼ä¼˜åŒ–æ–¹æ³•æ¥è®­ç»ƒæˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹

## ç½‘æ ¼æœç´¢ï¼Œæš´åŠ›ä¼˜åŒ–

æˆ‘ä»¬å¯ä»¥å°è¯•æ‰€æœ‰æ¨¡å‹å‚æ•°çš„ç»„åˆï¼Œåœ¨è¶³å¤Ÿçš„ç¦»æ•£åŒ–ä¸‹ï¼Œå¹¶ä¿ç•™æœ€å°åŒ–æŸå¤±å‡½æ•°çš„æ¨¡å‹å‚æ•°ç»„åˆï¼Œ

+   å•ä¸ªæ¨¡å‹å‚æ•°çš„æƒ…å†µ

+   ç”±äºå¯èƒ½çš„æ¨¡å‹å‚æ•°å€¼çš„ç»„åˆå¾ˆå¤§ï¼Œå¯¹äºå¤§å¤šæ•°æœºå™¨æ¥è¯´ä¸åˆ‡å®é™…ã€‚

![](img/a0d4db417b916675d6246cc896f46f62.png)

æ¨¡å‹å‚æ•°ç½‘æ ¼æœç´¢ï¼Œæš´åŠ›ä¼˜åŒ–ï¼Œå¯¹ 1 ä¸ªæ¨¡å‹å‚æ•°ï¼ˆä¸Šæ–¹ï¼‰å’Œ 2 ä¸ªæ¨¡å‹å‚æ•°ï¼ˆä¸‹æ–¹ï¼‰çš„æŸå¤±å‡½æ•°è¿›è¡Œå¸¸è§„é‡‡æ ·ã€‚

æ¨¡å‹å‚æ•°çš„ç»„åˆæ•°æ˜¯ï¼Œ

$$ ğ‘›_ğ‘=ğ‘›_{ğ‘ğ‘–ğ‘›ğ‘ }^{ğ‘›_ğ‘} $$

å…¶ä¸­ $ğ‘›_ğ‘$ æ˜¯æ¨¡å‹å‚æ•°çš„æ•°é‡ï¼Œ$ğ‘›_{ğ‘ğ‘–ğ‘›ğ‘ }$ æ˜¯æ¯ä¸ªæ¨¡å‹å‚æ•°çš„ç¦»æ•£åŒ–æ•°é‡ã€‚

+   åœ¨è´å¶æ–¯æ–¹æ³•ä¸­ï¼Œæ¨¡å‹å‚æ•°ç”±åˆ†å¸ƒè¡¨ç¤ºï¼Œç©ºé—´çš„å¤§å°ç”šè‡³æ›´å¤§ã€‚

## æ¢¯åº¦ä¸‹é™ä¼˜åŒ–

æ•°å€¼è§£çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•å¦‚ä¸‹è¿›è¡Œï¼Œ

1.  ä»ä¸€ä¸ªéšæœºçš„æ¨¡å‹å‚æ•°å¼€å§‹

1.  è®¡ç®—æŸå¤±å‡½æ•°

1.  è®¡ç®—æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œé€šå¸¸æ²¡æœ‰æŸå¤±å‡½æ•°çš„æ–¹ç¨‹ï¼Œé€šè¿‡æ•°å€¼è®¡ç®—å±€éƒ¨æŸå¤±å‡½æ•°çš„å¯¼æ•°è¿›è¡Œé‡‡æ ·ã€‚

$$ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) = \frac{L(y_{\alpha}, F(X_{\alpha}, b_1 - \epsilon)) - L(y_{\alpha}, F(X_{\alpha}, b_1 + \epsilon))}{2\epsilon} $$

1.  é€šè¿‡å‘ä¸‹æ­¥è¿›/æ¢¯åº¦æ¥æ›´æ–°å‚æ•°ä¼°è®¡

$$ \hat{b}_{1,t+1} = \hat{b}_{1,t} - r \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) $$

å…¶ä¸­ $ğ‘Ÿ$ æ˜¯å­¦ä¹ ç‡/æ­¥é•¿ï¼Œ$\hat{b}_{1,ğ‘¡}$ æ˜¯å½“å‰æ¨¡å‹å‚æ•°ä¼°è®¡ï¼Œ$\hat{ğ‘}_{1,ğ‘¡+1}$ æ˜¯æ›´æ–°åçš„å‚æ•°ä¼°è®¡ã€‚

æ¢¯åº¦æœç´¢æ”¶æ•›ï¼Œ

+   æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å°†æ‰¾åˆ°å±€éƒ¨æˆ–å…¨å±€æœ€å°å€¼

æ¢¯åº¦æœç´¢æ­¥é•¿ï¼Œ

+   $ğ‘Ÿ$ å¤ªå°ï¼Œéœ€è¦å¤ªé•¿æ—¶é—´æ‰èƒ½æ”¶æ•›åˆ°è§£

+   $ğ‘Ÿ$ å¤ªå¤§ï¼Œè§£å¯èƒ½è·³è¿‡/é”™è¿‡å…¨å±€æœ€å°å€¼æˆ–å‘æ•£

![](img/ea7617f79b05f8efb8ee8b35f2c254b6.png)

è§£çš„æ”¶æ•›ï¼ˆå·¦ä¾§ï¼‰å’Œå‘æ•£ï¼ˆå³ä¾§ï¼‰ã€‚

å¤šå˜é‡ä¼˜åŒ–ï¼Œå¦‚æœæ¨¡å‹å°†å…·æœ‰è¶…è¿‡ 1 ä¸ªæ¨¡å‹å‚æ•°ï¼Œ

+   è®¡ç®—å¹¶åˆ†è§£å¤šä¸ªæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ï¼Œç°åœ¨ä½¿ç”¨æ‰€æœ‰æ¨¡å‹å‚æ•°çš„æ¢¯åº¦å‘é‡è¡¨ç¤º

+   ä¾‹å¦‚ï¼Œå¯¹äº 2 ä¸ªæ¨¡å‹å‚æ•°ï¼Œ

$$\begin{split} \nabla L(y_{\alpha}, F(X_{\alpha}, b_1, b_2)) = \left[ \begin{array}{c} \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) \\ \nabla L(y_{\alpha}, F(X_{\alpha}, b_2)) \end{array} \right] \end{split}$$

æˆ‘ä»¬å¯ä»¥å°†å…¶å›¾å½¢åŒ–è¡¨ç¤ºä¸ºï¼Œ

![](img/ff8218bd8083f37ce36b750a43c46485.png)

é€šè¿‡æŸå¤±è¡¨ç¤ºçš„å‘é‡å¯¹ 2 ä¸ªæ¨¡å‹å‚æ•°è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚

+   è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„ä¼˜åŒ–æ˜¯æ¢ç´¢é«˜ç»´æ¨¡å‹å‚æ•°ç©ºé—´

å±€éƒ¨æœ€å°å€¼çš„ç¼“è§£

1.  ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯å¤šæ¬¡å¼€å§‹å¹¶å–æœ€ä½³ç»“æœã€‚

![](img/cde0ae9c36b1219d5667c512d04756fd.png)

å¤šæ¬¡éšæœºèµ·å§‹ä»¥æ”¹å–„å…¨å±€æœ€å°å€¼çš„è¯†åˆ«ã€‚

1.  ä»è¾ƒå¤§çš„å­¦ä¹ ç‡ã€æ­¥é•¿å¼€å§‹ï¼Œåœ¨ $ğ‘¡=1,\dots,ğ‘‡$ ä¸Šå‡å°‘æ­¥æ•°ï¼Œä»¥è¿›è¡Œæœç´¢ç„¶åæ”¶æ•›ã€‚

+   ä½¿ç”¨æ­¥é•¿è°ƒåº¦/è‡ªé€‚åº”æ­¥é•¿è¿›è¡Œè¿­ä»£ï¼Œä¾‹å¦‚ï¼ŒAdam ä¼˜åŒ–å™¨å¸¸ç”¨äºäººå·¥ç¥ç»ç½‘ç»œã€‚

+   æ¨¡æ‹Ÿé€€ç«æœ‰ä¸€ä¸ªæ¥å—åæ­¥éª¤çš„æ¦‚ç‡è°ƒåº¦ï¼æ¥å—æ›´å¤šåæ­¥éª¤ä»¥æ—©æœŸæ¢ç´¢ï¼Œå¹¶åœ¨åæœŸæ¥å—è¾ƒå°‘çš„åæ­¥éª¤ä»¥æ”¶æ•›ã€‚

1.  åŠ¨é‡æé«˜è§£çš„ç¨³å®šæ€§

ä½¿ç”¨æ–°çš„æ­¥é•¿ã€åŠ¨é‡ã€$\lambda$ æ›´æ–°å‰ä¸€æ­¥ï¼Œ$\lambda$ æ˜¯å‰ä¸€æ­¥çš„æƒé‡

$$ (r \cdot \nabla L)_{t-1}^m = \lambda \cdot (r \cdot \nabla L)_{t-2} + (1 - \lambda) \cdot (r \cdot \nabla L)_{t-1} $$

æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œå¯è§†åŒ–è¿™ä¸€ç‚¹ï¼Œ

![](img/3bb90a5ffaaa17e9358dbf00fcc3ef14.png)

åŠ¨é‡å¯¹å‰ä¸€æ­¥è¿›è¡ŒåŠ æƒå¹¶å¹³æ»‘é€šè¿‡æŸå¤±å‡½æ•°çš„è·¯å¾„ã€‚

+   ä»æ¯ä¸ªæ¨¡å‹å‚æ•°çš„æŸå¤±å‡½æ•°çš„åå¯¼æ•°è®¡ç®—å‡ºçš„æ¢¯åº¦æœ‰å™ªå£°ã€‚

+   åŠ¨é‡å¹³æ»‘ï¼Œå‡å°‘è¿™ç§å™ªå£°ã€‚

åŠ¨é‡å¸®åŠ©è§£æ²¿ç€æŸå¤±å‡½æ•°çš„ä¸€èˆ¬æ–œç‡å‰è¿›ï¼Œè€Œä¸æ˜¯åœ¨å±€éƒ¨å³¡è°·æˆ–å‡¹æ§½ä¸­æŒ¯è¡ã€‚

## éšæœºæ¢¯åº¦ä¸‹é™

æˆ‘ä»¬å¯èƒ½æœ‰å¤§é‡æ•°æ® $\rightarrow \nabla ğ¿_ğ‘¡$ï¼Œè®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚

+   æˆ‘ä»¬å¯ä»¥ç”¨éšæœºé€¼è¿‘æ›¿æ¢æ¢¯åº¦ï¼Œä¿ç•™è®­ç»ƒæ•°æ®çš„ä¸€ä¸ªéšæœºå­é›†ï¼Œåœ¨çº¿ï¼ˆ1 ä¸ªæ•°æ®ï¼‰æˆ–å°æ‰¹é‡ï¼ˆ>1 ä¸ªæ•°æ®ï¼Œ$ğ‘›_{ğ‘ğ‘ğ‘¡ğ‘â„}$ï¼‰ï¼Œå…¶ä¸­ $\ell$ è¡¨ç¤ºæ¢¯åº¦çš„ä¸€ä¸ªå®ç°ã€‚

+   æˆ‘ä»¬åœ¨æ¢¯åº¦ä¸‹é™ä¸­é™ä½ç²¾åº¦ï¼Œä½†åŠ å¿«è®¡ç®—é€Ÿåº¦ï¼Œå¯ä»¥æ‰§è¡Œæ›´å¤šæ­¥éª¤ï¼Œé€šå¸¸æ¯”æ¢¯åº¦ä¸‹é™æ›´å¿«ã€‚

+   å¢åŠ  $ğ‘›_{ğ‘ğ‘ğ‘¡ğ‘â„}$ ä»¥æé«˜æ¢¯åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œå¹¶å‡å°‘ $ğ‘›_{ğ‘ğ‘ğ‘¡ğ‘â„}$ ä»¥åŠ å¿«æ­¥éª¤

é€šè¿‡ Robbins-Siegmundï¼ˆ1971ï¼‰å®šç†â€”â€”å¯¹äºå‡¸æŸå¤±å‡½æ•°æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼ï¼Œå¯¹äºéå‡¸æŸå¤±å‡½æ•°æ”¶æ•›åˆ°å…¨å±€æˆ–å±€éƒ¨æœ€å°å€¼ã€‚

**ç¨€ç–æ€§** - $ğ¿Â¹$ ç§»é™¤ç‰¹å¾ï¼Œå†…ç½®ç‰¹å¾é€‰æ‹©ï¼Œå°†æ¨¡å‹å‚æ•°ç¼©å°åˆ°æ­£å¥½ä¸º 0ï¼Œæé«˜æ¨¡å‹å‚æ•°çš„ç¨€ç–æ€§ã€‚

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = False
import os                                                     # to set current working directory 
import numpy as np                                            # arrays and matrix math
import scipy.stats as st                                      # statistical methods
import pandas as pd                                           # DataFrames
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks
from sklearn import metrics                                   # measures to check our models
from sklearn.preprocessing import StandardScaler              # standardize the features
from sklearn import linear_model                              # linear regression
from sklearn.linear_model import Ridge                        # ridge regression implemented in scikit-learn
from sklearn.linear_model import Lasso                        # LASSO regression implemented in scikit-learn
from sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation
from sklearn.model_selection import train_test_split          # train and test split
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # suppress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœé‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½éœ€è¦é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£ï¼Œå¯ä»¥è·å¾—æ›´å¤šå¸®åŠ©ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä»¥ç®€åŒ–å‘æˆ‘ä»¬çš„å›¾è¡¨æ·»åŠ æŒ‡å®šçš„ç™¾åˆ†ä½æ•°å’Œä¸»æ¬¡ç½‘æ ¼çº¿ã€‚

```py
def weighted_percentile(data, weights, perc):                 # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) 
    ix = np.argsort(data)
    data = data[ix] 
    weights = weights[ix] 
    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) 
    return np.interp(perc, cdf, data)

def histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram 
    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)
    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')
    plt.plot([avg,avg],[0.0,45],color = color)
    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)
    html_str = ''
    for df in args:
        html_str += df.head().to_html()  # Using .head() for the first few rows
    display(HTML(f'<div style="display: flex;">{html_str}</div>')) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œä»¥å…ä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆæ¯æ¬¡é¿å…åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯·ç¡®ä¿å°†æ‰€éœ€çš„æ•°æ®æ–‡ä»¶ï¼ˆè§ä¸‹æ–‡ï¼‰æ”¾ç½®åœ¨æ­¤å·¥ä½œç›®å½•ä¸­ã€‚

```py
#os.chdir("C:\PGE337")                                        # set the working directory 
```

æ‚¨å°†éœ€è¦æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ä¸ºæ‚¨çš„è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”æ ¼å¼åœ¨ Mac ä¸Šä¸åŒï¼ˆä¾‹å¦‚ï¼šâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½è¡¨æ ¼æ•°æ®

è¿™æ˜¯å°†æˆ‘ä»¬çš„é€—å·åˆ†éš”æ•°æ®æ–‡ä»¶åŠ è½½åˆ° Pandas DataFrame å¯¹è±¡ä¸­çš„å‘½ä»¤ã€‚

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›† â€˜unconv_MV.csvâ€™ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   å¯†åº¦ ($g/cm^{3}$)

+   å­”éš™ç‡ (ä½“ç§¯ %)

æ³¨æ„ï¼Œæ•°æ®é›†æ˜¯åˆæˆçš„ã€‚

æˆ‘ä»¬ä½¿ç”¨ pandas çš„ â€˜read_csvâ€™ å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸º â€˜my_dataâ€™ çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

```py
add_error = True                                              # add random error to the response feature
std_error = 1.0; seed = 71071

yname = 'Porosity'; xname = 'Density'                         # specify the predictor features (x2) and response feature (x1)
xmin = 1.0; xmax = 2.5                                        # set minimums and maximums for visualization 
ymin = 0.0; ymax = 25.0    
xlabel = 'Porosity'; ylabel = 'Density'                       # specify the feature labels for plotting
yunit = '%'; xunit = '$g/cm^{3}$'    
Xlabelunit = xlabel + ' (' + xunit + ')'
ylabelunit = ylabel + ' (' + yunit + ')'

#df = pd.read_csv("Density_Por_data.csv")                     # load the data from local current directory
df = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv") # load the data from my github repo
df = df.sample(frac=1.0, random_state = 73073); df = df.reset_index() # extract 30% random to reduce the number of data

if add_error == True:                                         # method to add error
    np.random.seed(seed=seed)                                 # set random number seed
    df[yname] = df[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df)) # add noise
    values = df._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray 
```

## è®­ç»ƒ-æµ‹è¯•åˆ†å‰²

ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn åŒ…ä¸­çš„ model_selection æ¨¡å—çš„ train_test_split å‡½æ•°åº”ç”¨éšæœºè®­ç»ƒ-æµ‹è¯•åˆ†å‰²ã€‚

```py
x_train, x_test, y_train, y_test = train_test_split(df[xname],df[yname],test_size=0.25,random_state=73073) # train and test split
# y_train = pd.DataFrame({yname:y_train.values}); y_test = pd.DataFrame({yname:y_test.values}) # optional to ensure response is a DataFrame

y = df[yname].values.reshape(len(df))                         # features as 1D vectors
x = df[xname].values.reshape(len(df))

df_train = pd.concat([x_train,y_train],axis=1)                # features as train and test DataFrames
df_test = pd.concat([x_test,y_test],axis=1) 
```

## å¯è§†åŒ– DataFrame

å¯è§†åŒ– DataFrame æ˜¯æ•°æ®çš„ç¬¬ä¸€æ­¥æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ â€˜headâ€™ DataFrame æˆå‘˜å‡½æ•°æ¥é¢„è§ˆï¼ˆæ ¼å¼æ•´æ´ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚

+   æˆ‘ä»¬æœ‰ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°æ¥å¹¶æ’é¢„è§ˆè®­ç»ƒå’Œæµ‹è¯• DataFrameã€‚

```py
print('   Training DataFrame      Testing DataFrame')
display_sidebyside(df_train,df_test) 
```

```py
 Training DataFrame      Testing DataFrame 
```

|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| 24 | 1.778580 | 11.426485 |
| 101 | 2.410560 | 8.488544 |
| 88 | 2.216014 | 10.133693 |
| 79 | 1.631896 | 12.712326 |
| 58 | 1.528019 | 16.129542 |
|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| 59 | 1.742534 | 15.380154 |
| 1 | 1.404932 | 13.710628 |
| 35 | 1.552713 | 14.131878 |
| 92 | 1.762359 | 11.154896 |
| 22 | 1.885087 | 9.403056 |

## è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡

åœ¨ DataFrame ä¸­ä»è¡¨æ ¼æ•°æ®è®¡ç®—æ±‡æ€»ç»Ÿè®¡æœ‰å¾ˆå¤šé«˜æ•ˆçš„æ–¹æ³•ã€‚describe å‘½ä»¤ä»¥ä¸€ä¸ªæ¼‚äº®çš„æ•°æ®è¡¨å½¢å¼æä¾›è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€‚

```py
print('     Training DataFrame         Testing DataFrame')
display_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) 
```

```py
 Training DataFrame         Testing DataFrame 
```

|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| count | 78.000000 | 78.000000 |
| mean | 1.739027 | 12.501465 |
| std | 0.302510 | 3.428260 |
| min | 0.996736 | 3.276449 |
| max | 2.410560 | 21.660179 |
|  | å¯†åº¦ | å­”éš™ç‡ |
| --- | --- | --- |
| count | 27.000000 | 27.000000 |
| mean | 1.734710 | 12.380796 |
| std | 0.247761 | 2.916045 |
| min | 1.067960 | 7.894595 |
| max | 2.119652 | 18.133771 |

## å¯è§†åŒ–æ•°æ®

è®©æˆ‘ä»¬ä½¿ç”¨ç›´æ–¹å›¾å’Œæ•£ç‚¹å›¾æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•çš„ä¸€è‡´æ€§å’Œè¦†ç›–ç‡ã€‚

+   æ£€æŸ¥ä»¥ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•æ•°æ®è¦†ç›–äº†å¯èƒ½çš„é¢„æµ‹ç‰¹å¾ç»„åˆçš„èŒƒå›´ã€‚

+   ç¡®ä¿æµ‹è¯•ç”¨ä¾‹ä¸ä¼šè¶…å‡ºè®­ç»ƒæ•°æ®çš„èŒƒå›´è¿›è¡Œå¤–æ¨ã€‚

```py
nbins = 20                                                    # number of histogram bins

plt.subplot(221)
freq1,_,_ = plt.hist(x=df_train[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=True,label='Train')
freq2,_,_ = plt.hist(x=df_test[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=True,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(xname + ' (' + xunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  
plt.xlim([xmin,xmax]); plt.legend(loc='upper right')   

plt.subplot(222)
freq1,_,_ = plt.hist(x=df_train[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=True,label='Train')
freq2,_,_ = plt.hist(x=df_test[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=True,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(yname + ' (' + yunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  
plt.xlim([ymin,ymax]); plt.legend(loc='upper right')   

plt.subplot(223)                                              # plot the model
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.title('Porosity vs Density')
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.3, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![_images/4f4677e517ed9b0f7ed7a658d2f332e4319eee7f3a590ba6a296c8e12695d6ac.png](img/3766b457d4d8b8c75cfa925ac2f27fad.png)

## çº¿æ€§å›å½’æ¨¡å‹

è®©æˆ‘ä»¬å…ˆè®¡ç®—çº¿æ€§å›å½’æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ scikit learnï¼Œç„¶åå°†ç›¸åŒçš„æµç¨‹æ‰©å±•åˆ°å²­å›å½’ã€‚

+   æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œ$\phi = f(\rho)$ï¼Œå…¶ä¸­ $\phi$ æ˜¯å­”éš™ç‡ï¼Œ$\rho$ æ˜¯å¯†åº¦ã€‚

+   æˆ‘ä»¬ä¹Ÿå¯ä»¥è¯´ï¼Œæˆ‘ä»¬æœ‰â€œå­”éš™ç‡å¯¹å¯†åº¦å›å½’â€ã€‚

æˆ‘ä»¬æ¨¡å‹å…·æœ‰è¿™ä¸ªç‰¹å®šçš„æ–¹ç¨‹ï¼Œ

$$ \phi = b_1 \times \rho + b_0 $$

```py
linear_reg = linear_model.LinearRegression()                  # instantiate the model

linear_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters
x_model = np.linspace(xmin,xmax,10)
y_model_linear = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data 

plt.subplot(111)                                              # plot the data, model with model parameters
plt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])
plt.title('Linear Regression Model, Porosity = f(Density)')
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/5330f8e13578eabf8f67b4e8379f1dd5.png)

ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°åœ¨é¢„æµ‹å‡½æ•°ä¸­å¯¹é¢„æµ‹ç‰¹å¾åº”ç”¨äº†é¢å¤–çš„é‡å¡‘æ“ä½œã€‚

```py
y_linear_model = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data 
```

è¿™æ˜¯å› ä¸º scikit-learn å‡è®¾æœ‰å¤šä¸ªé¢„æµ‹ç‰¹å¾ï¼›å› æ­¤ï¼ŒæœŸæœ›ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼ŒåŒ…å«æ ·æœ¬ï¼ˆè¡Œï¼‰å’Œç‰¹å¾ï¼ˆåˆ—ï¼‰ï¼Œä½†æˆ‘ä»¬åªæœ‰ä¸€ä¸ªä¸€ç»´å‘é‡ã€‚

+   é‡å¡‘æ“ä½œå°†ä¸€ç»´å‘é‡è½¬æ¢ä¸ºä¸€ä¸ªåªæœ‰ä¸€åˆ—çš„äºŒç»´å‘é‡

## çº¿æ€§å›å½’æ¨¡å‹æ£€æŸ¥

è®©æˆ‘ä»¬è¿è¡Œä¸€äº›å¿«é€Ÿæ¨¡å‹æ£€æŸ¥ã€‚å¯ä»¥åšå¾—æ›´å¤šï¼Œä½†ä¸ºäº†ç®€æ´ï¼Œæˆ‘åœ¨è¿™é‡Œé™åˆ¶äº†èŒƒå›´ã€‚

+   æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…çº¿æ€§å›å½’ç« èŠ‚ã€‚

```py
y_pred_linear = linear_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data
r_squared_linear = metrics.r2_score(df_test[yname].values, y_pred_linear)

plt.subplot(121)                                              # plot testing diagnostics 
plt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
# plt.scatter(df_test[xname], y_pred,color='grey',edgecolor='black',s = 40, alpha = 1.0, label = 'predictions',zorder=100)
plt.scatter(df_test[xname], y_pred_linear,color='white',s=120,marker='o',linewidths=1.0, edgecolors="black",zorder=300)
plt.scatter(df_test[xname], y_pred_linear,color='red',s=90,marker='*',linewidths=0.5, edgecolors="black",zorder=320,label='Predictions')

plt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])
plt.annotate(r'$rÂ²$ :' + str(np.round(r_squared_linear,2)),[1.97,15])
plt.title('Linear Regression Model, Porosity = f(Density)')
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

y_res_linear = y_pred_linear - df_test['Porosity'].values     # calculate the test residual

plt.subplot(122)
plt.hist(y_res_linear, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))
plt.title("Error Residual at Testing Data"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')
plt.vlines(0,0,5.5,color='black',ls='--',lw=2)
plt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics
plt.annotate(r'$\overline{\Delta{y}}$: ' + str(round(np.average(y_res_linear),2)),[-4,4.4])
plt.annotate(r'$\sigma_{\Delta{y}}$: ' + str(np.round(np.std(y_res_linear),2)),[-4,4.1])
add_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/cf89ec094ccb520da30fa1ee810410b6.png)

## å²­å›å½’æ¨¡å‹

è®©æˆ‘ä»¬ç”¨ scikit-learn çš„å²­å›å½’æ–¹æ³•æ›¿æ¢ scikit-learn çš„çº¿æ€§å›å½’æ–¹æ³•ã€‚

+   æ³¨æ„ï¼Œæˆ‘ä»¬ç°åœ¨å¿…é¡»è®¾ç½® $\lambda$ è¶…å‚æ•°ã€‚

+   åœ¨ scikit-learn ä¸­ï¼Œè¶…å‚æ•°æ˜¯åœ¨æ¨¡å‹å®ä¾‹åŒ–æ—¶è®¾ç½®çš„

```py
lam = 1.0                                                     # lambda hyperparameter

ridge_reg = Ridge(alpha=lam)                                  # instantiate the model

ridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters
x_model = np.linspace(xmin,xmax,10)
y_model_ridge = ridge_reg.predict(x_model.reshape(10,1)) # predict with the fit model

plt.subplot(111)                                              # plot the data, model with model parameters
plt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Ridge Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])
plt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(lam))
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/a6448c37319c913a38a278536faa63c1.png)

è®©æˆ‘ä»¬é‡å¤ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹è¿›è¡Œçš„ç®€å•æ¨¡å‹æ£€æŸ¥ã€‚

```py
y_pred_ridge = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data
r_squared = metrics.r2_score(df_test[yname].values, y_pred_ridge)

plt.subplot(121)                                              # plot testing diagnostics 
plt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Ridge Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.scatter(df_test[xname], y_pred_ridge,color='white',s=120,marker='o',linewidths=1.0, edgecolors="black",zorder=300)
plt.scatter(df_test[xname], y_pred_ridge,color='red',s=90,marker='*',linewidths=0.5, edgecolors="black",zorder=320,label='Predictions')

plt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])
plt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(lam))
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

y_res_ridge = y_pred_ridge - df_test['Porosity'].values       # calculate the test residual

plt.subplot(122)
plt.hist(y_res_ridge, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))
plt.title("Error Residual at Testing Data"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')
plt.vlines(0,0,5.5,color='black',ls='--',lw=2)
plt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics
plt.annotate(r'$\overline{\Delta{y}}$: ' + str(round(np.average(y_res_ridge),2)),[-4,4.4])
plt.annotate(r'$\sigma_{\Delta{y}}$: ' + str(np.round(np.std(y_res_ridge),2)),[-4,4.1])
add_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/5ac80d876690b7e916dd4280243aed8a.png)

æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬è§£é‡Šçš„æ–¹å·®æ›´å°‘ï¼Œå¹¶ä¸”æ®‹å·®æ ‡å‡†å·®æ›´å¤§ï¼ˆé”™è¯¯æ›´å¤šï¼‰ã€‚

+   å¯¹äºæˆ‘ä»¬éšæ„é€‰æ‹©çš„è¶…å‚æ•° $\lambda$ï¼Œå²­å›å½’å®é™…ä¸Šå‡å°‘äº†æµ‹è¯•æ–¹å·®è§£é‡Šå’Œå‡†ç¡®åº¦

+   è¿™å¹¶ä¸å¥‡æ€ªï¼Œæˆ‘ä»¬å®é™…ä¸Šå¹¶æ²¡æœ‰è°ƒæ•´è¶…å‚æ•°ä»¥è·å¾—æœ€ä½³æ¨¡å‹ï¼

## LASSO å›å½’æ¨¡å‹

è®©æˆ‘ä»¬ç”¨ scikit-learn çš„ LASSO å›å½’æ–¹æ³•æ›¿æ¢ scikit-learn çº¿æ€§å›å½’å’Œå²­å›å½’æ–¹æ³•ã€‚æ³¨æ„ï¼Œå†æ¬¡å¿…é¡»ç°åœ¨è®¾ç½® lambda è¶…å‚æ•°ã€‚

+   å›æƒ³ä¸€ä¸‹ï¼Œlambda è¶…å‚æ•° $\lambda$ æ˜¯åœ¨æ¨¡å‹å®ä¾‹åŒ–æ—¶è®¾ç½®çš„

```py
lam = 0.1                                                     # lambda hyperparameter

lasso_reg = Lasso(alpha=lam)                                  # instantiate the model

lasso_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters
x_model = np.linspace(xmin,xmax,10)
y_model_lasso = lasso_reg.predict(x_model.reshape(10,1))      # predict with the fit model

plt.subplot(111)                                              # plot the data, model with model parameters
plt.plot(x_model,y_model_lasso, color='red', linewidth=2,label='LASSO Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.annotate('LASSO Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(lasso_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(lasso_reg.intercept_,2)),[1.97,16])
plt.title('LASSO Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(lam))
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/c9cb274630b2d8b01b6f4bd114f7ef0e.png)

è®©æˆ‘ä»¬é‡å¤ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹è¿›è¡Œçš„ç®€å•æ¨¡å‹æ£€æŸ¥ã€‚

## LASSO è¶…å‚æ•°è°ƒæ•´

åœ¨ä¸Šé¢ï¼Œæˆ‘ä»¬åªæ˜¯éšæ„é€‰æ‹©äº†ä¸€ä¸ªè¶…å‚æ•° $\lambda$ï¼Œç°åœ¨è®©æˆ‘ä»¬è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ã€‚

+   åœ¨äº¤å‰éªŒè¯ä¸­å¾ªç¯éå†å¹¿æ³›çš„ $\lambda$ å€¼çš„åŒæ—¶æ€»ç»“ MSE

å›æƒ³ä¸€ä¸‹ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼Œ

$$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)Â² $$

å…¶ä¸­ $y_i$ æ˜¯å®é™…å€¼ï¼Œ$\hat{y}_i$ æ˜¯é¢„æµ‹å€¼ï¼Œ$n$ æ˜¯æ•°æ®ç‚¹çš„æ•°é‡ã€‚

```py
score = []                                                    # code modified from StackOverFlow by Dimosthenis

nlambda = 100
lambda_mat = np.logspace(-2,5,nlambda)
for ilam in range(0,nlambda):
    lasso_reg = Lasso(alpha=lambda_mat[ilam])
    scores = cross_val_score(estimator=lasso_reg, X= df['Density'].values.reshape(-1, 1), 
                             y=df['Porosity'].values, cv=10, n_jobs=4, scoring = "neg_mean_squared_error") # Perform 10-fold cross validation
    score.append(abs(scores.mean()))

plt.subplot(111)
plt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)
plt.title('LASSO Regression Test Mean Square Error vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')
plt.xlim(1.0e-2,1.0e5); plt.ylim(0.001,20.0); plt.xscale('log')
plt.vlines(0.1,0,20,color='red',lw=2); plt.vlines(0.9,0,20,color='red',lw=2,zorder=10)
plt.annotate('Linear Regression',[0.075,12.5],rotation=90.0,color='red',zorder=10)
plt.annotate('Mean of Response Feature',[1.06,11.4],rotation=90.0,color='red',zorder=10)
plt.fill_between([0.01,0.1],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.fill_between([0.9,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.grid(which='both')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/aa9ecd6f50ebe5d0f0c5ca2842d39012.png)

ä»ä¸Šé¢çš„ç»“æœæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œä»»ä½• $\lambda > 0.1$ éƒ½ä¼šå¯¼è‡´æœ€å°çš„æµ‹è¯•å‡æ–¹è¯¯å·®ã€‚

+   é˜ˆå€¼è¡Œä¸ºæ˜¯ç”±äºä»¥ä¸‹äº‹å®ï¼šåœ¨è¿™ä¸ªæ­£åˆ™åŒ–æ°´å¹³ä»¥ä¸‹ï¼Œæ¨¡å‹çš„è¡Œä¸ºç±»ä¼¼äºçº¿æ€§å›å½’ã€‚

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªè¶…å‚æ•°åœ¨æ‰€æœ‰æ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚

```py
lam = 0.01                                                      # tuned hyperparameter
lasso_tuned = Lasso(alpha=lam)                                  # instantiate the model
lasso_tuned.fit(df[xname].values.reshape(len(df),1), df[yname]) # train the model parameters on all data

y_pred_lasso_tuned = lasso_tuned.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data
r_squared = metrics.r2_score(df_test[yname].values, y_pred_lasso_tuned)

plt.subplot(121)                                              # plot testing diagnostics 
plt.plot(x_model,y_model_lasso, color='red', linewidth=2,label='LASSO Regression',zorder=100)
plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.scatter(df_test[xname], y_pred_ridge,color='white',s=120,marker='o',linewidths=1.0, edgecolors="black",zorder=300)
plt.scatter(df_test[xname], y_pred_ridge,color='red',s=90,marker='*',linewidths=0.5, edgecolors="black",zorder=320,label='Predictions')

plt.annotate('LASSO Regression Model Parameters:',[1.86,18]) # add the model to the plot
plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])
plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])
plt.title('Tuned LASSO Model, Regression of ' + yname + ' on ' + xname + r' with a $\lambda = $' + str(lam))
plt.xlabel(xname + ' (' + xunit + ')')
plt.ylabel(yname + ' (' + yunit + ')')
plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])

y_res_ridge = y_pred_ridge - df_test['Porosity'].values       # calculate the test residual

plt.subplot(122)
plt.hist(y_res_ridge, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))
plt.title("Error Residual at Testing Data"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')
plt.vlines(0,0,5.5,color='black',ls='--',lw=2)
plt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics
plt.annotate(r'$\overline{\Delta{y}}$: ' + str(round(np.average(y_res_ridge),2)),[-4,4.4])
plt.annotate(r'$\sigma_{\Delta{y}}$: ' + str(np.round(np.std(y_res_ridge),2)),[-4,4.1])
add_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/64e13b841e7ab592022426ef7bc2bddf.png)

ä½¿ç”¨æˆ‘ä»¬è°ƒæ•´çš„$\lambda$è¶…å‚æ•°ï¼Œ

```py
lam = 0.01 
```

æˆ‘ä»¬çš„æ¨¡å¼ä¸çº¿æ€§å›å½’ç›¸åŒã€‚

+   æˆ‘ä»¬èƒ½å¦åˆ›é€ ä¸€ä¸ªæœ€ä½³æ¨¡å‹ä¸æ˜¯çº¿æ€§å›å½’çš„æƒ…å†µï¼Ÿå³ï¼Œæ­£åˆ™åŒ–æ˜¯æœ‰å¸®åŠ©çš„ï¼Ÿ

+   æ˜¯çš„ï¼Œæˆ‘ä»¬å¯ä»¥ã€‚è®©æˆ‘ä»¬ç§»é™¤å¤§éƒ¨åˆ†æ ·æœ¬ä»¥åˆ›å»ºæ•°æ®ç¨€ç¼ºæ€§å¹¶æ·»åŠ å¤§é‡å™ªå£°ï¼

å®é™…ä¸Šï¼Œæˆ‘è¿­ä»£äº†æ ·æœ¬å’Œå™ªå£°çš„éšæœºç§å­ä»¥å¾—åˆ°è¿™ä¸ªç»“æœã€‚

+   å°‘é‡æ•°æ®ï¼ˆä½$n$ï¼‰å’Œé«˜ç»´æ€§ï¼ˆé«˜$m$ï¼‰é€šå¸¸ä¼šå¯¼è‡´ LASSO ä¼˜äºçº¿æ€§å›å½’

```py
df_sample = df.copy(deep=True).sample(n=10,random_state=11)
noise_stdev = 3.0
np.random.seed(seed=15)
df_sample['Porosity'] = df_sample['Porosity'] + np.random.normal(0.0, noise_stdev, size=len(df_sample))

score = []                                                    # code modified from StackOverFlow by Dimosthenis

nlambda = 100
lambda_mat = np.logspace(-3,5,nlambda)
for ilam in range(0,nlambda):
    lasso_reg = Lasso(alpha=lambda_mat[ilam])
    scores = cross_val_score(estimator=lasso_reg, X= df_sample['Density'].values.reshape(-1, 1), 
                             y=df_sample['Porosity'].values, cv=2, n_jobs=4, scoring = "neg_mean_squared_error") # Perform 10-fold cross validation
    score.append(abs(scores.mean()))

plt.subplot(111)
plt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)
plt.title('LASSO Regression Test Mean Square Error vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')
plt.xlim(1.0e-3,1.0e5); plt.ylim(0.001,20.0); 
plt.xscale('log')
plt.vlines(0.003,0,20,color='red',lw=2); plt.vlines(0.4,0,20,color='red',lw=2,zorder=10); plt.vlines(0.1,0,20,color='red',lw=2,zorder=10);
plt.annotate('Linear Regression',[0.0022,12.5],rotation=90.0,color='red',zorder=10)
plt.annotate(r'LASSO Tuned $\lambda$',[0.075,12.5],rotation=90.0,color='red',zorder=10)
plt.annotate('Mean of Response Feature',[0.46,11.7],rotation=90.0,color='red',zorder=10)
plt.fill_between([0.001,0.003],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.fill_between([0.4,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)
plt.grid(which='both')
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/03c4aefb2d59d3a6e5c4b3652011ed5c.png)

## è°ƒæŸ¥ Lambda è¶…å‚æ•°å¯¹æ¨¡å‹å‚æ•°çš„å½±å“

è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å·²ç»åŠ è½½çš„å¤šå…ƒæ•°æ®é›†ã€‚è¿™æ ·æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåœ¨ä¸€ç³»åˆ—ç‰¹å¾å’Œä¸€ç³»åˆ— lambda è¶…å‚æ•°å€¼ä¸Šçš„æ¨¡å‹è¡Œä¸ºã€‚æˆ‘ä»¬å°†æ‰§è¡Œå¸¸è§„æ­¥éª¤ä»¥åˆ°è¾¾è¦ç‚¹ï¼

+   åŠ è½½ä¸€ä¸ªå¤šå…ƒæ•°æ®é›†

+   è®¡ç®—æ‘˜è¦ç»Ÿè®¡

+   æ ‡å‡†åŒ–ç‰¹å¾

ç„¶åï¼Œæˆ‘ä»¬å°†æ”¹å˜è¶…å‚æ•°å¹¶è§‚å¯Ÿæ¨¡å‹å‚æ•°ã€‚

### åŠ è½½ä¸€ä¸ªå¤šå…ƒæ•°æ®é›†

è®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªåŒ…å«æ›´å¤šå˜é‡çš„æ•°æ®é›†æ¥å±•ç¤ºä½¿ç”¨ LASSO å›å½’è¿›è¡Œç‰¹å¾æ’åºï¼Œå¹¶æ¯”è¾ƒåœ¨è¶…å‚æ•°å€¼ä¸Šçš„æ¨¡å‹å‚æ•°çš„è¡Œä¸ºã€‚æ•°æ®é›†â€˜unconv_MV_v5.csvâ€™æ˜¯ä¸€ä¸ªåŸºäº 1,000 ä¸ªéå¸¸è§„äº•çš„é€—å·åˆ†éš”æ–‡ä»¶ï¼ŒåŒ…æ‹¬ç‰¹å¾ï¼Œ

+   äº•å¹³å‡å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   å²©è„†æ€§æ¯”ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   ç»ç’ƒè´¨åå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹äº§é‡ 90 å¤©å¹³å‡ï¼ˆMCFPDï¼‰ã€‚

æˆ‘ä»¬å‡è®¾åˆå§‹äº§é‡æ˜¯å“åº”ç‰¹å¾ï¼Œæ‰€æœ‰å…¶ä»–ç‰¹å¾éƒ½æ˜¯é¢„æµ‹ç‰¹å¾ã€‚

æ­¤å¤–ï¼Œæ‚¨å¯ä»¥é€šè¿‡åˆ‡æ¢ mv_data æ•´æ•°ä¸º 1 æ¥å°è¯•å¦ä¸€ä¸ªç±»ä¼¼çš„æ•°æ®åº“ã€‚

```py
mv_data = 2

if mv_data == 1:
    df_mv = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv")
    df_mv = df_mv.drop('WellIndex',axis = 1)                  # remove the well index feature
elif mv_data == 2:
    df_mv = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv")
    df_mv = df_mv.rename({'Prod':'Production'},axis=1)
    df_mv = df_mv.drop('Well',axis = 1)                       # remove the well index feature
df_mv.head()                                                  # load the comma delimited data file 
```

|  | Por | Perm | AI | Brittle | TOC | VR | Production |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 12.08 | 2.92 | 2.80 | 81.40 | 1.16 | 2.31 | 4165.196191 |
| 1 | 12.38 | 3.53 | 3.22 | 46.17 | 0.89 | 1.88 | 3561.146205 |
| 2 | 14.02 | 2.59 | 4.01 | 72.80 | 0.89 | 2.72 | 4284.348574 |
| 3 | 17.67 | 6.75 | 2.63 | 39.81 | 1.08 | 1.88 | 5098.680869 |
| 4 | 17.52 | 4.57 | 3.18 | 10.94 | 1.51 | 1.90 | 3406.132832 |

### è®¡ç®—æ‘˜è¦ç»Ÿè®¡

è®©æˆ‘ä»¬è®¡ç®—æˆ‘ä»¬çš„å¤šå…ƒæ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ã€‚

```py
df_mv.describe().transpose() 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 200.0 | 14.991150 | 2.971176 | 6.550000 | 12.912500 | 15.070000 | 17.402500 | 23.550000 |
| Perm | 200.0 | 4.330750 | 1.731014 | 1.130000 | 3.122500 | 4.035000 | 5.287500 | 9.870000 |
| AI | 200.0 | 2.968850 | 0.566885 | 1.280000 | 2.547500 | 2.955000 | 3.345000 | 4.630000 |
| Brittle | 200.0 | 48.161950 | 14.129455 | 10.940000 | 37.755000 | 49.510000 | 58.262500 | 84.330000 |
| TOC | 200.0 | 0.990450 | 0.481588 | -0.190000 | 0.617500 | 1.030000 | 1.350000 | 2.180000 |
| VR | 200.0 | 1.964300 | 0.300827 | 0.930000 | 1.770000 | 1.960000 | 2.142500 | 2.870000 |
| Production | 200.0 | 4311.219852 | 992.038414 | 2107.139414 | 3618.064513 | 4284.687348 | 5086.089761 | 6662.622385 |

### æ ‡å‡†åŒ–ç‰¹å¾

è®©æˆ‘ä»¬å°†ç‰¹å¾æ ‡å‡†åŒ–ä¸ºï¼š

+   å‡å€¼ = 0.0

+   æ–¹å·® = æ ‡å‡†å·® = 1.0

æˆ‘ä»¬è¿™æ ·åšæ˜¯ä¸ºäº†ä½¿æ¨¡å‹å‚æ•°å¤„äºç›¸ä¼¼çš„èŒƒå›´å†…ï¼Œä»¥ä¾¿è¿›è¡Œæ¯”è¾ƒï¼Œå³åƒç‰¹å¾æ’åä¸­çš„$\beta$ä¸$B$ç³»æ•°ä¸€æ ·ã€‚

ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ï¼š

1.  ä» scikit learn ä¸­å®ä¾‹åŒ– StandardScalerã€‚æˆ‘ä»¬å°†å…¶å‘½åä¸ºâ€˜scalerâ€™ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æ–¹ä¾¿åœ°åè½¬è½¬æ¢ï¼Œå¦‚æœæˆ‘ä»¬æ„¿æ„çš„è¯ã€‚æˆ‘ä»¬éœ€è¦è¿™æ ·åšæ‰èƒ½å°†é¢„æµ‹å€¼è½¬æ¢å›å¸¸è§„çš„ç”Ÿäº§å•ä½ã€‚

```py
scaler = StandardScaler() 
```

1.  ç„¶åï¼Œæˆ‘ä»¬ä» DataFrame ä¸­æå–æ‰€æœ‰å€¼å¹¶åº”ç”¨æŒ‰åˆ—æ ‡å‡†åŒ–ã€‚ç»“æœæ˜¯äºŒç»´ ndarray

```py
sfeatures = scaler.fit_transform(df_mv.values) 
```

1.  æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ç©º DataFrame

```py
df_nmv = pd.DataFrame() 
```

1.  ç„¶åæˆ‘ä»¬å°†è½¬æ¢åçš„å€¼æ·»åŠ åˆ°æ–°çš„ DataFrame ä¸­ï¼ŒåŒæ—¶ä¿ç•™æ—§ DataFrame ä¸­çš„æ ·æœ¬ç´¢å¼•å’Œç‰¹å¾åç§°

```py
df_nmv = pd.DataFrame(sfeatures, index=df_mv.index, columns=df_mv.columns) 
```

```py
scaler = StandardScaler()                                     # instantiate the scaler 

sfeatures = scaler.fit_transform(df_mv.values)                # standardize all the values extracted from the DataFrame 
df_nmv = pd.DataFrame()                                       # instantiate a new DataFrame
df_nmv = pd.DataFrame(sfeatures, index=df_mv.index, columns=df_mv.columns) # copy the standardized values into the new DataFrame
df_nmv.head()                                                 # preview the new DataFrame 
```

|  | Por | Perm | AI | Brittle | TOC | VR | Production |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | -0.982256 | -0.817030 | -0.298603 | 2.358297 | 0.352948 | 1.152048 | -0.147565 |
| 1 | -0.881032 | -0.463751 | 0.444147 | -0.141332 | -0.209104 | -0.280931 | -0.757991 |
| 2 | -0.327677 | -1.008148 | 1.841224 | 1.748113 | -0.209104 | 2.518377 | -0.027155 |
| 3 | 0.903875 | 1.401098 | -0.599240 | -0.592585 | 0.186414 | -0.280931 | 0.795773 |
| 4 | 0.853263 | 0.138561 | 0.373409 | -2.640962 | 1.081534 | -0.214280 | -0.914640 |

### æ£€æŸ¥æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯

è®©æˆ‘ä»¬æ£€æŸ¥æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ã€‚

```py
df_nmv.describe().transpose()                                 # summary statistics from the new DataFrame 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 200.0 | 2.486900e-16 | 1.002509 | -2.848142 | -0.701361 | 0.026605 | 0.813617 | 2.887855 |
| Perm | 200.0 | -6.217249e-17 | 1.002509 | -1.853701 | -0.699753 | -0.171282 | 0.554098 | 3.208033 |
| AI | 200.0 | 4.130030e-16 | 1.002509 | -2.986650 | -0.745137 | -0.024493 | 0.665203 | 2.937664 |
| Brittle | 200.0 | 2.042810e-16 | 1.002509 | -2.640962 | -0.738391 | 0.095646 | 0.716652 | 2.566186 |
| TOC | 200.0 | 3.375078e-16 | 1.002509 | -2.457313 | -0.776361 | 0.082330 | 0.748466 | 2.476256 |
| VR | 200.0 | 9.081624e-16 | 1.002509 | -3.446814 | -0.647507 | -0.014330 | 0.593853 | 3.018254 |
| Production | 200.0 | 1.598721e-16 | 1.002509 | -2.227345 | -0.700472 | -0.026813 | 0.783049 | 2.376222 |

æˆåŠŸï¼Œæˆ‘ä»¬å·²ç»å°†æ‰€æœ‰ç‰¹å¾æ ‡å‡†åŒ–ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥æ„å»ºæˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚è®©æˆ‘ä»¬æå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

```py
X_train, X_test, y_train, y_test = train_test_split(df_nmv.iloc[:,:6], pd.DataFrame({'Production':df_nmv['Production']}), 
                                                    test_size=0.33, random_state=73073)
print('Number of training data = ' + str(len(X_train)) + ' and number of testing data = ' + str(len(X_test))) 
```

```py
Number of training data = 134 and number of testing data = 66 
```

### è°ƒæ•´è¶…å‚æ•°å¹¶è§‚å¯Ÿæ¨¡å‹å‚æ•°

ç°åœ¨ï¼Œè®©æˆ‘ä»¬è§‚å¯Ÿä¸€ç³»åˆ— $\lambda$ è¶…å‚æ•°å€¼ä¸‹çš„æ¨¡å‹ç³»æ•° ($b_{\alpha}, \alpha = 1,\ldots,m$)ã€‚

```py
nbins = 1000                                                # number of bins to explore the hyperparameter 
df_nmv.describe().transpose()                               # summary statistics from the new DataFrame 
lams = np.logspace(-3,1,nbins)                              # make a list of lambda values
coefs = np.ndarray((nbins,6))

index = 0
for lam in lams:
    lasso_reg = Lasso(alpha=lam)                            # instantiate the model
    lasso_reg.fit(X_train, y_train)                         # fit model
    coefs[index,:] = lasso_reg.coef_                        # retrieve the coefficients
    index = index + 1

color = ['black','blue','green','red','orange','grey']
plt.subplot(111)                                            # plot the results
for ifeature in range(0,6):
    plt.semilogx(lams,coefs[:,ifeature], label = df_mv.columns[ifeature], c = color[ifeature], linewidth = 3.0)

plt.title('Standardized Model Coefficients vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); plt.ylabel('Standardized Model Coefficients')
plt.xlim(1.0e-3,1.0e1); plt.ylim(-1.0,1.0); plt.grid(); plt.legend(loc = 'lower right')
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1., wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/b105175b634806ea901b8442f3b13e47.png)

æˆ‘ä»¬çœ‹åˆ°äº†ä»€ä¹ˆï¼Ÿ

+   å¯¹äºéå¸¸ä½çš„ lambda å€¼ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½åŒ…å«åœ¨å†…

+   éšç€ lambda è¶…å‚æ•°çš„å¢åŠ ï¼Œæ€»æœ‰æœºç¢³æ˜¯ç¬¬ä¸€ä¸ªè¢«ç§»é™¤çš„é¢„æµ‹ç‰¹å¾

+   ç„¶åæ˜¯å£°é˜»æŠ—ã€é•œè´¨ä½“åå°„ç‡ã€è„†æ€§ã€å¯¹æ•°æ¸—é€ç‡å’Œæœ€ç»ˆå­”éš™ç‡ã€‚

+   å½“ $\lambda \ge 0.8$ æ—¶ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½è¢«ç§»é™¤ã€‚

è®©æˆ‘ä»¬é‡å¤è¿™ä¸ªå·¥ä½œæµç¨‹ï¼Œä½¿ç”¨å²­å›å½’è¿›è¡Œæ¯”è¾ƒã€‚

```py
nbins = 1000                                                # number of bins to explore the hyperparameter 
lams = np.logspace(-2,5,nbins)       
ridge_coefs = np.ndarray((nbins,6))

index = 0
for lam in lams:
    ridge_reg = Ridge(alpha=lam)
    ridge_reg.fit(X_train, y_train) # fit model
    ridge_coefs[index,:] = ridge_reg.coef_
    index = index + 1

color = ['black','blue','green','red','orange','grey']
plt.subplot(111)
for ifeature in range(0,6):
    plt.semilogx(lams,ridge_coefs[:,ifeature], label = df_mv.columns[ifeature], c = color[ifeature], linewidth = 3.0)

plt.title('Standardized Model Coefficients vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); plt.ylabel('Standardized Model Coefficients')
plt.xlim(1.0e-2,1.0e5); plt.ylim(-1.0,1.0); plt.legend(loc = 'lower right')
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1., wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/ff11eb98e492b6fb3503b042054e1b80.png)

å²­å›å½’åœ¨é¢„æµ‹ç‰¹å¾å¯¹ $\lambda$ è¶…å‚æ•°å˜åŒ–çš„å“åº”ä¸Šç›¸å½“ä¸åŒã€‚

+   éšç€ lambda è¶…å‚æ•°çš„å¢åŠ ï¼Œæ²¡æœ‰é€‰æ‹©æ€§åœ°ç§»é™¤é¢„æµ‹ç‰¹å¾

+   ä¸€ä¸ªä¸»è¦æˆåˆ†æ˜¯æ‰€æœ‰ç³»æ•°åœ¨ $\lambda \in [10Â¹, 10âµ]$ èŒƒå›´å†…å‘é›¶å‡åŒ€æ”¶ç¼©

### åŠ è½½ä¸€ä¸ªå¤šå…ƒæ•°æ®é›†

è®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªåŒ…å«æ›´å¤šå˜é‡çš„æ•°æ®é›†ï¼Œä»¥å±•ç¤ºä½¿ç”¨ LASSO å›å½’çš„ç‰¹å¾æ’åºï¼Œå¹¶æ¯”è¾ƒè¶…å‚æ•°å€¼ä¸‹æ¨¡å‹å‚æ•°çš„è¡Œä¸ºã€‚æ•°æ®é›†â€˜unconv_MV_v5.csvâ€™æ˜¯ä¸€ä¸ªåŸºäº 1,000 å£éå¸¸è§„äº•çš„é€—å·åˆ†éš”æ–‡ä»¶ï¼ŒåŒ…æ‹¬ç‰¹å¾ï¼Œ

+   äº•å¹³å‡å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   è„†æ€§æ¯”ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   é•œè´¨ä½“åå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹äº§é‡ 90 å¤©å¹³å‡ï¼ˆMCFPDï¼‰ã€‚

æˆ‘ä»¬å‡è®¾åˆå§‹äº§é‡æ˜¯å“åº”ç‰¹å¾ï¼Œæ‰€æœ‰å…¶ä»–ç‰¹å¾æ˜¯é¢„æµ‹ç‰¹å¾ã€‚

æ­¤å¤–ï¼Œæ‚¨è¿˜å¯ä»¥é€šè¿‡åˆ‡æ¢ mv_data æ•´æ•°åˆ° 1 æ¥å°è¯•å¦ä¸€ä¸ªç±»ä¼¼çš„æ•°æ®åº“ã€‚

```py
mv_data = 2

if mv_data == 1:
    df_mv = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv")
    df_mv = df_mv.drop('WellIndex',axis = 1)                  # remove the well index feature
elif mv_data == 2:
    df_mv = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv")
    df_mv = df_mv.rename({'Prod':'Production'},axis=1)
    df_mv = df_mv.drop('Well',axis = 1)                       # remove the well index feature
df_mv.head()                                                  # load the comma delimited data file 
```

|  | å­” | æ¸—é€ç‡ | AI | è„†æ€§ | TOC | VR | ç”Ÿäº§ |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 12.08 | 2.92 | 2.80 | 81.40 | 1.16 | 2.31 | 4165.196191 |
| 1 | 12.38 | 3.53 | 3.22 | 46.17 | 0.89 | 1.88 | 3561.146205 |
| 2 | 14.02 | 2.59 | 4.01 | 72.80 | 0.89 | 2.72 | 4284.348574 |
| 3 | 17.67 | 6.75 | 2.63 | 39.81 | 1.08 | 1.88 | 5098.680869 |
| 4 | 17.52 | 4.57 | 3.18 | 10.94 | 1.51 | 1.90 | 3406.132832 |

### è®¡ç®—æ‘˜è¦ç»Ÿè®¡é‡

è®©æˆ‘ä»¬è®¡ç®—æˆ‘ä»¬çš„å¤šå…ƒæ•°æ®çš„æ‘˜è¦ç»Ÿè®¡é‡ã€‚

```py
df_mv.describe().transpose() 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- |
| å­” | 200.0 | 14.991150 | 2.971176 | 6.550000 | 12.912500 | 15.070000 | 17.402500 | 23.550000 |
| æ¸—é€ç‡ | 200.0 | 4.330750 | 1.731014 | 1.130000 | 3.122500 | 4.035000 | 5.287500 | 9.870000 |
| AI | 200.0 | 2.968850 | 0.566885 | 1.280000 | 2.547500 | 2.955000 | 3.345000 | 4.630000 |
| è„†æ€§ | 200.0 | 48.161950 | 14.129455 | 10.940000 | 37.755000 | 49.510000 | 58.262500 | 84.330000 |
| TOC | 200.0 | 0.990450 | 0.481588 | -0.190000 | 0.617500 | 1.030000 | 1.350000 | 2.180000 |
| VR | 200.0 | 1.964300 | 0.300827 | 0.930000 | 1.770000 | 1.960000 | 2.142500 | 2.870000 |
| ç”Ÿäº§ | 200.0 | 4311.219852 | 992.038414 | 2107.139414 | 3618.064513 | 4284.687348 | 5086.089761 | 6662.622385 |

### æ ‡å‡†åŒ–ç‰¹å¾

è®©æˆ‘ä»¬å°†ç‰¹å¾æ ‡å‡†åŒ–ä¸ºï¼š

+   å‡å€¼ = 0.0

+   æ–¹å·® = æ ‡å‡†å·® = 1.0

æˆ‘ä»¬è¿™æ ·åšæ˜¯ä¸ºäº†ä½¿æ¨¡å‹å‚æ•°å…·æœ‰ç›¸ä¼¼çš„æ•°å€¼èŒƒå›´ï¼Œä»¥ä¾¿è¿›è¡Œæ¯”è¾ƒï¼Œå³åƒç‰¹å¾æ’åä¸­çš„ $\beta$ ä¸ $B$ ç³»æ•°ä¸€æ ·ã€‚

è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ï¼š

1.  ä» scikit learn ä¸­å®ä¾‹åŒ– StandardScalerã€‚æˆ‘ä»¬å°†å…¶å‘½åä¸ºâ€˜scalerâ€™ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æ–¹ä¾¿åœ°åè½¬è½¬æ¢ï¼Œå¦‚æœæˆ‘ä»¬æ„¿æ„çš„è¯ã€‚æˆ‘ä»¬éœ€è¦è¿™æ ·åšæ‰èƒ½å°†æˆ‘ä»¬çš„é¢„æµ‹å€¼è½¬æ¢å›å¸¸è§„çš„ç”Ÿäº§å•ä½ã€‚

```py
scaler = StandardScaler() 
```

1.  æˆ‘ä»¬é¦–å…ˆä»æˆ‘ä»¬çš„ DataFrame ä¸­æå–æ‰€æœ‰å€¼å¹¶åº”ç”¨æŒ‰åˆ—æ ‡å‡†åŒ–ã€‚ç»“æœæ˜¯äºŒç»´ ndarray

```py
sfeatures = scaler.fit_transform(df_mv.values) 
```

1.  æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ç©º DataFrame

```py
df_nmv = pd.DataFrame() 
```

1.  ç„¶åæˆ‘ä»¬å°†è½¬æ¢åçš„å€¼æ·»åŠ åˆ°æ–°çš„ DataFrame ä¸­ï¼ŒåŒæ—¶ä¿ç•™æ¥è‡ªæ—§ DataFrame çš„æ ·æœ¬ç´¢å¼•å’Œç‰¹å¾åç§°

```py
df_nmv = pd.DataFrame(sfeatures, index=df_mv.index, columns=df_mv.columns) 
```

```py
scaler = StandardScaler()                                     # instantiate the scaler 

sfeatures = scaler.fit_transform(df_mv.values)                # standardize all the values extracted from the DataFrame 
df_nmv = pd.DataFrame()                                       # instantiate a new DataFrame
df_nmv = pd.DataFrame(sfeatures, index=df_mv.index, columns=df_mv.columns) # copy the standardized values into the new DataFrame
df_nmv.head()                                                 # preview the new DataFrame 
```

|  | Por | Perm | AI | Brittle | TOC | VR | ç”Ÿäº§ |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | -0.982256 | -0.817030 | -0.298603 | 2.358297 | 0.352948 | 1.152048 | -0.147565 |
| 1 | -0.881032 | -0.463751 | 0.444147 | -0.141332 | -0.209104 | -0.280931 | -0.757991 |
| 2 | -0.327677 | -1.008148 | 1.841224 | 1.748113 | -0.209104 | 2.518377 | -0.027155 |
| 3 | 0.903875 | 1.401098 | -0.599240 | -0.592585 | 0.186414 | -0.280931 | 0.795773 |
| 4 | 0.853263 | 0.138561 | 0.373409 | -2.640962 | 1.081534 | -0.214280 | -0.914640 |

### æ£€æŸ¥æ‘˜è¦ç»Ÿè®¡

è®©æˆ‘ä»¬æ£€æŸ¥æ‘˜è¦ç»Ÿè®¡ã€‚

```py
df_nmv.describe().transpose()                                 # summary statistics from the new DataFrame 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 200.0 | 2.486900e-16 | 1.002509 | -2.848142 | -0.701361 | 0.026605 | 0.813617 | 2.887855 |
| Perm | 200.0 | -6.217249e-17 | 1.002509 | -1.853701 | -0.699753 | -0.171282 | 0.554098 | 3.208033 |
| AI | 200.0 | 4.130030e-16 | 1.002509 | -2.986650 | -0.745137 | -0.024493 | 0.665203 | 2.937664 |
| Brittle | 200.0 | 2.042810e-16 | 1.002509 | -2.640962 | -0.738391 | 0.095646 | 0.716652 | 2.566186 |
| TOC | 200.0 | 3.375078e-16 | 1.002509 | -2.457313 | -0.776361 | 0.082330 | 0.748466 | 2.476256 |
| VR | 200.0 | 9.081624e-16 | 1.002509 | -3.446814 | -0.647507 | -0.014330 | 0.593853 | 3.018254 |
| ç”Ÿäº§ | 200.0 | 1.598721e-16 | 1.002509 | -2.227345 | -0.700472 | -0.026813 | 0.783049 | 2.376222 |

æˆåŠŸï¼Œæˆ‘ä»¬å·²ç»å¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œäº†æ ‡å‡†åŒ–ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥æ„å»ºæˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚è®©æˆ‘ä»¬æå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

```py
X_train, X_test, y_train, y_test = train_test_split(df_nmv.iloc[:,:6], pd.DataFrame({'Production':df_nmv['Production']}), 
                                                    test_size=0.33, random_state=73073)
print('Number of training data = ' + str(len(X_train)) + ' and number of testing data = ' + str(len(X_test))) 
```

```py
Number of training data = 134 and number of testing data = 66 
```

### è°ƒæ•´è¶…å‚æ•°å¹¶è§‚å¯Ÿæ¨¡å‹å‚æ•°

ç°åœ¨ï¼Œè®©æˆ‘ä»¬è§‚å¯Ÿä¸€ç³»åˆ— $\lambda$ è¶…å‚æ•°å€¼å¯¹åº”çš„æ¨¡å‹ç³»æ•° ($b_{\alpha}, \alpha = 1,\ldots,m$)ã€‚

```py
nbins = 1000                                                # number of bins to explore the hyperparameter 
df_nmv.describe().transpose()                               # summary statistics from the new DataFrame 
lams = np.logspace(-3,1,nbins)                              # make a list of lambda values
coefs = np.ndarray((nbins,6))

index = 0
for lam in lams:
    lasso_reg = Lasso(alpha=lam)                            # instantiate the model
    lasso_reg.fit(X_train, y_train)                         # fit model
    coefs[index,:] = lasso_reg.coef_                        # retrieve the coefficients
    index = index + 1

color = ['black','blue','green','red','orange','grey']
plt.subplot(111)                                            # plot the results
for ifeature in range(0,6):
    plt.semilogx(lams,coefs[:,ifeature], label = df_mv.columns[ifeature], c = color[ifeature], linewidth = 3.0)

plt.title('Standardized Model Coefficients vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); plt.ylabel('Standardized Model Coefficients')
plt.xlim(1.0e-3,1.0e1); plt.ylim(-1.0,1.0); plt.grid(); plt.legend(loc = 'lower right')
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1., wspace=0.2, hspace=0.2); plt.show() 
```

![_images/09312a2f4e51e5cd4d8ecb5211db9a6800b6305d448d91ebf9c9ee6164703c6b.png](img/b105175b634806ea901b8442f3b13e47.png)

æˆ‘ä»¬çœ‹åˆ°äº†ä»€ä¹ˆï¼Ÿ

+   å¯¹äºéå¸¸ä½çš„ lambda å€¼ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½è¢«åŒ…å«åœ¨å†…

+   éšç€ lambda è¶…å‚æ•°çš„å¢åŠ ï¼Œæ€»æœ‰æœºç¢³æ˜¯ç¬¬ä¸€ä¸ªè¢«ç§»é™¤çš„é¢„æµ‹ç‰¹å¾ã€‚

+   ç„¶åæ˜¯å£°é˜»æŠ—ã€é•œè´¨ä½“åå°„ç‡ã€è„†æ€§ã€å¯¹æ•°æ¸—é€ç‡å’Œæœ€ç»ˆå­”éš™ç‡ã€‚

+   å½“ $\lambda \ge 0.8$ æ—¶ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½è¢«ç§»é™¤ã€‚

è®©æˆ‘ä»¬ç”¨å²­å›å½’æ¥é‡å¤è¿™ä¸ªå·¥ä½œæµç¨‹è¿›è¡Œæ¯”è¾ƒã€‚

```py
nbins = 1000                                                # number of bins to explore the hyperparameter 
lams = np.logspace(-2,5,nbins)       
ridge_coefs = np.ndarray((nbins,6))

index = 0
for lam in lams:
    ridge_reg = Ridge(alpha=lam)
    ridge_reg.fit(X_train, y_train) # fit model
    ridge_coefs[index,:] = ridge_reg.coef_
    index = index + 1

color = ['black','blue','green','red','orange','grey']
plt.subplot(111)
for ifeature in range(0,6):
    plt.semilogx(lams,ridge_coefs[:,ifeature], label = df_mv.columns[ifeature], c = color[ifeature], linewidth = 3.0)

plt.title('Standardized Model Coefficients vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); plt.ylabel('Standardized Model Coefficients')
plt.xlim(1.0e-2,1.0e5); plt.ylim(-1.0,1.0); plt.legend(loc = 'lower right')
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1., wspace=0.2, hspace=0.2); plt.show() 
```

![_images/09a0ac5014577cbd314e22f6233b84ec6bc3af2a939d398f99e71ae5dc3c24fe.png](img/ff11eb98e492b6fb3503b042054e1b80.png)

å²­å›å½’åœ¨é¢„æµ‹ç‰¹å¾å¯¹$\lambda$è¶…å‚æ•°å˜åŒ–çš„å“åº”ä¸Šä¸çº¿æ€§å›å½’æœ‰å¾ˆå¤§ä¸åŒã€‚

+   éšç€ lambda è¶…å‚æ•°çš„å¢åŠ ï¼Œæ²¡æœ‰é€‰æ‹©æ€§ç§»é™¤é¢„æµ‹ç‰¹å¾ã€‚

+   ä¸»è¦æˆåˆ†æ˜¯æ‰€æœ‰ç³»æ•°åœ¨ $\lambda \in [10Â¹, 10âµ]$ èŒƒå›´å†…å‘é›¶çš„å‡åŒ€æ”¶ç¼©ã€‚

## å±•ç¤ºè§£å†³æ–¹æ¡ˆçš„ä¸ç¨³å®šæ€§

è®©æˆ‘ä»¬é‡å¤ä¸Šè¿°å®éªŒï¼Œå¹¶è·Ÿè¸ªæ¨¡å‹åœ¨è¶…å‚æ•°$\lambda$ä¸Šçš„æŸäº›ä¼°è®¡ã€‚

```py
nbins = 1000                                                # number of bins to explore the hyperparameter 
df_nmv.describe().transpose()                               # summary statistics from the new DataFrame 
lams = np.logspace(-4,6,nbins)                              # make a list of lambda values
coefs = np.ndarray((nbins,6))
estimates_ridge = np.zeros((nbins,len(X_test)))
estimates_lasso = np.zeros((nbins,len(X_test)))

index = 0
for lam in lams:
    lasso_reg = Lasso(alpha=lam)                            # instantiate the model
    lasso_reg.fit(X_train, y_train)                         # fit model
    ridge_reg = Ridge(alpha=lam)
    ridge_reg.fit(X_train, y_train) # fit model
    estimates_ridge[index] = ridge_reg.predict(X_test).flatten() # predict at test data
    estimates_lasso[index] = lasso_reg.predict(X_test).flatten() # predict at test data
    index = index + 1

color = ['black','blue','green','red','orange','grey']
plt.subplot(211)                                            # plot the results
for iest in range(0,6):
    plt.semilogx(lams,estimates_ridge[:,iest], label = 'Estimate #' + str(iest+1), c = color[iest], linewidth = 3.0)
plt.title('Ridge Regression - 6 Example Predictions vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); 
plt.ylabel(r'Predictions, $\hat{y}_i$')
plt.xlim(1.0e-4,1.0e6); plt.ylim(-1.0,1.5); plt.grid(); plt.legend(loc = 'lower right')
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)
plt.vlines(0.001,-1,1.5,color='red',lw=2); plt.vlines(1.0e5,-0.1,1.5,color='red',lw=2,zorder=10)
plt.annotate('Linear Regression',[0.0007,0.6],rotation=90.0,color='red',zorder=10)
plt.annotate('Mean of Response Feature',[110000,0.4],rotation=90.0,color='red',zorder=10)
plt.fill_between([0.0001,0.001],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)
plt.fill_between([1.0e5,1.0e7],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)

plt.subplot(212)                                            # plot the results
for iest in range(0,6):
    plt.semilogx(lams,estimates_lasso[:,iest], label = 'Estimate #' + str(iest+1), c = color[iest], linewidth = 3.0)
plt.title('LASSO Regression - 6 Example Predictions vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); 
plt.ylabel(r'Predictions, $\hat{y}_i$')
plt.xlim(1.0e-4,1.0e6); plt.ylim(-1.0,1.5); plt.grid(); plt.legend(loc = 'lower right')
plt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)
plt.vlines(0.001,-1,1.5,color='red',lw=2); plt.vlines(0.90,-1.0,1.5,color='red',lw=2,zorder=10)
plt.annotate('Linear Regression',[0.0007,0.6],rotation=90.0,color='red',zorder=10)
plt.annotate('Mean of Response Feature',[1.05,0.4],rotation=90.0,color='red',zorder=10)
plt.fill_between([0.0001,0.001],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)
plt.fill_between([0.9,100000],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=2.0, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/2b66c5aac73becda389b3090d50d3c69d950f870ace31d26bed7e234ad1d97da.png](img/d0fb23953092ed15bd21494f123ca3f0.png)

+   å²­å›å½’ä¼°è®¡åœ¨ä»çº¿æ€§å›å½’åˆ°å“åº”ç‰¹å¾çš„å…¨çƒå‡å€¼ä¹‹é—´å¹³ç¨³å˜åŒ–ï¼ˆç¨³å®šæ€§ï¼‰ã€‚

+   LASSO å›å½’ä¼°è®¡æ˜¾ç¤ºå‡ºè·³è·ƒï¼ˆä¸ç¨³å®šæ€§ï¼‰

## è¯„è®º

è¿™æ˜¯å¯¹ LASSO å›å½’çš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„ [å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources) å’Œæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„ YouTube è®²åº§é“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼ˆMichael Pyrczï¼‰æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ [Cockrell å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p) å’Œ [Jackson åœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/) çš„æ•™æˆï¼Œä»–åœ¨ [å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡](https://www.utexas.edu/) ç ”ç©¶å’Œæ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics) æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚

+   [è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board) çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š [æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board) çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº†è¶…è¿‡ 70 ç¯‡[åŒè¡Œè¯„å®¡çš„å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦ã€Š[åœ°è´¨ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ã€‹ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œåˆ†åˆ«æ˜¯ã€Š[Python ä¸­çš„åº”ç”¨åœ°è´¨ç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)ã€‹å’Œã€Š[Python ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„å®è·µæŒ‡å—](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‹ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œé™„æœ‰ 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†è®°å½•å·¥ä½œæµç¨‹çš„é“¾æ¥ï¼Œè¿™äº›å­˜å‚¨åº“ä½äºä»–çš„[GitHub è´¦æˆ·](https://github.com/GeostatsGuy)ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ã€‚äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

å¸Œæœ›è¿™ä¸ªå†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œå­¦ä¹ æœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢

æ›´å¤šèµ„æºè¯·è®¿é—®ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°è´¨ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­çš„åº”ç”¨åœ°è´¨ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
