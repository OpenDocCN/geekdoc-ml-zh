# å†³ç­–æ ‘

> åŸæ–‡ï¼š[`geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_decision_tree.html`](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_decision_tree.html)

è¿ˆå…‹å°”Â·JÂ·çš®å°”èŒ¨ï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åœ°ç»Ÿè®¡å­¦åº”ç”¨ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python æœºå™¨å­¦ä¹ åº”ç”¨ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

ç”µå­ä¹¦â€œPython æœºå™¨å­¦ä¹ åº”ç”¨ï¼šå¸¦ä»£ç çš„æ‰‹å†Œâ€çš„ä¸€ç« ã€‚

å°†æ­¤ç”µå­ä¹¦å¼•ç”¨ä¸ºï¼š

Pyrcz, M.J., 2024, *Python æœºå™¨å­¦ä¹ åº”ç”¨ï¼šå¸¦ä»£ç çš„æ‰‹å†Œ* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138 ![DOI](https://doi.org/10.5281/zenodo.15169138)

æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠæ›´å¤šå†…å®¹éƒ½å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š

å°† MachineLearningDemos GitHub ä»“åº“å¼•ç”¨ä¸ºï¼š

Pyrcz, M.J., 2024, *MachineLearningDemos: Python æœºå™¨å­¦ä¹ æ¼”ç¤ºå·¥ä½œæµç¨‹å­˜å‚¨åº“* (0.0.3) [è½¯ä»¶]. Zenodo. DOI: 10.5281/zenodo.13835312\. GitHub ä»“åº“: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos) ![DOI](https://zenodo.org/doi/10.5281/zenodo.13835312)

ä½œè€…ï¼šè¿ˆå…‹å°”Â·JÂ·çš®å°”èŒ¨

Â© ç‰ˆæƒæ‰€æœ‰ 2024.

æœ¬ç« æ˜¯å…³äº**å†³ç­–æ ‘**çš„æ•™ç¨‹å’Œæ¼”ç¤ºã€‚

**YouTube è®²åº§**ï¼šæŸ¥çœ‹æˆ‘åœ¨ä»¥ä¸‹ä¸»é¢˜ä¸Šçš„è®²åº§ï¼š

+   [æœºå™¨å­¦ä¹ ç®€ä»‹](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)

+   [å†³ç­–æ ‘](https://youtu.be/JUGo1Pu3QT4?si=ebQXv6Yglar0mYWp)

+   [éšæœºæ£®æ—](https://youtu.be/m5_wk310fho?si=up-mzVPHvniXsYE6)

+   [æ¢¯åº¦æå‡](https://youtu.be/___T8_ixIwc?si=ozHR_eIuMF3SPTxJ)

è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘ [Machine Learning Course](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI) çš„ä¸€éƒ¨åˆ†ï¼ŒYouTube ä¸Šçš„è¯¾ç¨‹é…æœ‰è¯¦ç»†è®°å½•çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºç†è§£ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœä½ æƒ³çŸ¥é“æˆ‘çš„åŠ¨æœºï¼Œè¯·æŸ¥çœ‹ [Michaelâ€™s Story](https://michaelpyrcz.com/my-story)ã€‚

## åŠ¨æœº

å†³ç­–æ ‘å¹¶ä¸æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€ä¸ºå¼ºå¤§å’Œå‰æ²¿çš„æ–¹æ³•ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆè¿˜è¦ä»‹ç»å†³ç­–æ ‘ï¼Ÿ

+   æœ€æ˜“äºç†è§£ã€å¯è§£é‡Šçš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹ä¸€

+   å†³ç­–æ ‘é€šè¿‡éšæœºæ£®æ—ã€è¢‹è£…å’Œæå‡æˆä¸ºè®¸å¤šæƒ…å†µä¸‹æœ€ä½³çš„æœºå™¨å­¦ä¹ é¢„æµ‹æ¨¡å‹ä¹‹ä¸€

![](img/90b8611f2db53bd1e441fa8eb6dce0d1.png)

é˜¿æ‹‰æ–¯åŠ åŒ—ææ£®æ—ä¸­çš„ä¸€æ£µå•ç‹¬çš„å‚²ç„¶é»‘äº‘æ‰æ ‘ï¼Œç±»ä¼¼äºæˆ‘å®¶ä¹¡çœä»½é˜¿å°”ä¼¯å¡”çš„åŒ—éƒ¨åœ°åŒºã€‚ç…§ç‰‡æ¥æº https://www.britannica.com/plant/spruce#/media/1/561445/8933ï¼Œè®¿é—®æ—¥æœŸ 2025 å¹´ 5 æœˆ 1 æ—¥

è®©æˆ‘ä»¬æ¢è®¨å†³ç­–æ ‘çš„ä¸€äº›å…³é”®æ–¹é¢ã€‚

## æ¨¡å‹å…¬å¼

é¢„æµ‹ç‰¹å¾ç©ºé—´è¢«åˆ’åˆ†ä¸º $J$ ä¸ªäº’æ–¥çš„ã€ç©·å°½çš„åŒºåŸŸ $R_1, R_2, \ldots, R_J$ã€‚å¯¹äºç»™å®šçš„é¢„æµ‹æ¡ˆä¾‹ $x_1,\ldots,x_m \in R_j$ï¼Œé¢„æµ‹å¦‚ä¸‹ï¼š

**å›å½’** - è¯¥åŒºåŸŸ $R_j$ ä¸­è®­ç»ƒæ•°æ®çš„å¹³å‡å€¼

$$ \hat{y} = \frac{1}{|R_j|} \sum_{\mathbf{x}_i \in R_j} y_i $$

å…¶ä¸­ $\hat{y}$ æ˜¯è¾“å…¥ $\mathbf{x}$ çš„é¢„æµ‹å€¼ï¼Œ$R_j$ æ˜¯ $\mathbf{x}$ è½å…¥çš„åŒºåŸŸï¼ˆå¶èŠ‚ç‚¹ï¼‰ï¼Œ$|R_j|$ æ˜¯åŒºåŸŸ $R_j$ ä¸­è®­ç»ƒæ ·æœ¬çš„æ•°é‡ï¼Œ$y_i$ æ˜¯ $R_j$ ä¸­é‚£äº›è®­ç»ƒæ ·æœ¬çš„å®é™…ç›®æ ‡å€¼ã€‚

**åˆ†ç±»** - åŒºåŸŸ $R_j$ ä¸­è®­ç»ƒæ¡ˆä¾‹æ•°é‡æœ€å¤šçš„ç±»åˆ«ï¼ˆæœ€å¸¸è§çš„æƒ…å†µï¼‰ï¼š

$$ \hat{y} = \arg\max_{c \in C} \left( \frac{1}{|R_j|} \sum_{\mathbf{x}_i \in R_j} \mathbb{1}(y_i = c) \right) $$

å…¶ä¸­ $C$ æ˜¯æ‰€æœ‰å¯èƒ½ç±»åˆ«çš„é›†åˆï¼Œ$\mathbb{1}(y_i = c)$ æ˜¯æŒ‡ç¤ºå˜æ¢ï¼Œå¦‚æœ $y_i = c$ åˆ™ä¸º 1ï¼Œå¦åˆ™ä¸º 0ï¼Œ$|R_j|$ æ˜¯åŒºåŸŸ $R_j$ ä¸­è®­ç»ƒæ ·æœ¬çš„æ•°é‡ï¼Œ$\hat{y}$ æ˜¯é¢„æµ‹çš„ç±»åˆ«æ ‡ç­¾ã€‚

é¢„æµ‹ç©ºé—´ $ğ‘‹_1,\ldots,ğ‘‹_ğ‘š$ è¢«åˆ†å‰²æˆ $J$ ä¸ªäº’æ–¥çš„ã€ç©·å°½çš„åŒºåŸŸ $R_j, j = 1,\ldots,J$ï¼Œå…¶ä¸­åŒºåŸŸä¸ºï¼Œ

+   **äº’æ–¥** â€“ ä»»ä½•é¢„æµ‹ç‰¹å¾ $x_1,\ldots,x_ğ‘š$ çš„ç»„åˆåªå±äºå•ä¸ªåŒºåŸŸ $R_j$

+   **ç©·å°½** â€“ æ‰€æœ‰é¢„æµ‹ç‰¹å¾å€¼çš„ç»„åˆå±äºä¸€ä¸ªåŒºåŸŸ $R_j$ï¼Œå³æ‰€æœ‰åŒºåŸŸ $R_j, j = 1,\ldots,J$ è¦†ç›–æ•´ä¸ªé¢„æµ‹ç‰¹å¾ç©ºé—´

æ‰€æœ‰è½åœ¨åŒä¸€åŒºåŸŸ $R_j$ ä¸­çš„é¢„æµ‹æ¡ˆä¾‹ $x_1,\ldots,x_m$ éƒ½ç”¨ç›¸åŒçš„å€¼è¿›è¡Œä¼°è®¡ã€‚

+   é¢„æµ‹æ¨¡å‹åœ¨åŒºåŸŸè¾¹ç•Œå¤„æœ¬è´¨ä¸Šæ˜¯ä¸è¿ç»­çš„

ä¾‹å¦‚ï¼Œè€ƒè™‘è¿™ä¸ªç”¨äºç”Ÿäº§å“åº”ç‰¹å¾ $\hat{Y}$ çš„å†³ç­–æ ‘é¢„æµ‹æ¨¡å‹ï¼Œä»å­”éš™ç‡ $X_1$ é¢„æµ‹ç‰¹å¾ï¼Œ

![](img/98d8fb73fe41299a6a9b443163b47c96.png)

å››åŒºåŸŸå†³ç­–æ ‘ï¼ŒåŒ…å«æ•°æ®å’Œé¢„æµ‹ï¼Œ$\hat{Y}(R_j) = \overline{Y}(R_j)$ æŒ‰åŒºåŸŸ $R_j, j=1,â€¦,4$ è®¡ç®—ã€‚ä¾‹å¦‚ï¼Œç»™å®šä¸€ä¸ª 13% å­”éš™ç‡çš„é¢„æµ‹ç‰¹å¾å€¼ï¼Œæ¨¡å‹é¢„æµ‹ç”Ÿäº§å¤§çº¦ 2,000 MCFPDã€‚

æˆ‘ä»¬å¦‚ä½•å¯¹é¢„æµ‹ç‰¹å¾ç©ºé—´è¿›è¡Œåˆ†å‰²ï¼Ÿ

çœ‹è¿™ä¸ªä¾‹å­ï¼Œä½¿ç”¨å­”éš™ç‡å’Œè„†æ€§ä½œä¸ºé¢„æµ‹ç‰¹å¾æ¥é¢„æµ‹ç”Ÿäº§å“åº”ç‰¹å¾ã€‚

![](img/31b00c9edfa4f39d2ae71626df2cd687.png)

ä¸€ä¸ªéå¸¸å¤æ‚çš„é¢„æµ‹ç‰¹å¾ç©ºé—´åˆ†å‰²ï¼Œæœ‰ 3 ä¸ªåŒºåŸŸã€‚

+   è¿™äº›æ˜¯éå¸¸æœ‰æ•ˆçš„è¾¹ç•Œï¼Œå¯ä»¥æ•æ‰åˆ°ä½ã€ä¸­ã€é«˜äº§é‡

ä½†æ˜¯ï¼Œè¿™ä¸ªæ¨¡å‹å°†ä¼šç›¸å½“å¤æ‚ï¼Œ

+   éœ€è¦å¤§é‡çš„æ¨¡å‹å‚æ•°

+   å¯¹äºå¤§é‡é¢„æµ‹ç‰¹å¾æ¥è¯´ï¼Œè®­ç»ƒéš¾åº¦è¾ƒå¤§ï¼Œå³é«˜ç»´æ€§

å¦‚æœæˆ‘èƒ½è¯´æœä½ æ¥å—è¿™äº›åŒºåŸŸï¼Œé‚£ä¹ˆä½ å°†æ‹¥æœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œ

+   éå¸¸å®¹æ˜“è®­ç»ƒ

+   å…·æœ‰éå¸¸å°‘çš„å‚æ•°

+   éå¸¸ç´§å‡‘ä¸”å¯è§£é‡Š

![](img/13cf7080472834d100f0e7812be6d2d3.png)

ä¸€ä¸ªæ›´ç®€å•çš„é¢„æµ‹ç‰¹å¾ç©ºé—´åˆ†å‰²ï¼Œæœ‰ 9 ä¸ªåŒºåŸŸï¼Œä½†å‚æ•°æ›´å°‘ï¼Œä¸”æ˜“äºè®­ç»ƒä»»ä½•ç»´åº¦ã€‚

è¿™æ˜¯ä¸€ä¸ªåŸºäºåˆ†å±‚ã€äºŒåˆ†åˆ†å‰²çš„åŒºåŸŸé›†åˆã€‚è®©æˆ‘ä»¬æ˜ç¡®é¢„æµ‹ç‰¹å¾ç©ºé—´çš„æ¦‚å¿µï¼Œç„¶åè§£é‡Šè¿™ç§é¢„æµ‹ç‰¹å¾åˆ†å‰²çš„å½¢å¼ã€‚

## é¢„æµ‹ç‰¹å¾ç©ºé—´

è®©æˆ‘ä»¬é€€ä¸€æ­¥ï¼Œå»ºç«‹é¢„æµ‹ç‰¹å¾ç©ºé—´çš„è§‚å¿µã€‚æˆ‘ä»¬å°†å…¶å®šä¹‰ä¸ºï¼Œ

+   åŒ…å«æ‰€æœ‰å¯èƒ½çš„ä¼°è®¡é—®é¢˜çš„ç©ºé—´ï¼Œå³æ‰€æœ‰å¯èƒ½çš„é¢„æµ‹ç‰¹å¾å€¼çš„ç»„åˆï¼Œ$x_1, x_2,\ldots,x_m$ã€‚

![](img/2328290a847e59dd59f76c37a18c6df3.png)

3 ä¸ªé¢„æµ‹ç‰¹å¾ä¸”æ¯ä¸ªç‰¹å¾æŒ‡å®šæœ€å°å’Œæœ€å¤§å€¼çš„æƒ…å†µä¸‹çš„é¢„æµ‹ç‰¹å¾ç©ºé—´ç¤ºæ„å›¾ï¼Œç»“æœæ˜¯ä¸€ä¸ªå¯èƒ½çš„é¢„æµ‹çŸ©å½¢ç«‹æ–¹ä½“ï¼Œ$x_1, x_2, x_3$ã€‚

é€šå¸¸è¿™ç”±å¯èƒ½å€¼çš„èŒƒå›´å®šä¹‰ï¼Œ$x_{\alpha} \in \left[X_{\alpha,\text{ğ‘šğ‘–ğ‘›}},ğ‘‹_{\alpha,\text{max}} \right]$ï¼Œä»è€Œå¾—åˆ°ï¼Œ

+   1 ä¸ªé¢„æµ‹ç‰¹å¾ $\rightarrow$ çº¿æ®µ

+   2 ä¸ªé¢„æµ‹ç‰¹å¾ $\rightarrow$ çŸ©å½¢

+   3 ä¸ªé¢„æµ‹ç‰¹å¾ $\rightarrow$ çŸ©å½¢ç«‹æ–¹ä½“

+   $>$3 ä¸ªé¢„æµ‹ç‰¹å¾ $\rightarrow$ è¶…çŸ©å½¢

å½“æˆ‘ä»¬ç”¨é¢„æµ‹ç‰¹å¾çš„å–å€¼èŒƒå›´æ¥å®šä¹‰é¢„æµ‹ç‰¹å¾ç©ºé—´æ—¶ï¼Œæˆ‘åº”è¯¥æä¾›ä¸€æ¡è­¦å‘Šæ€§è¯´æ˜ã€‚

å†³ç­–æ ‘å…·æœ‰éšå¼å¤–æ¨æ¨¡å‹

æ­£å¦‚ä½ ä¸‹é¢å°†çœ‹åˆ°çš„ï¼Œæ²¿ç€å¤–éƒ¨çš„åŒºåŸŸå»¶ä¼¸åˆ°æ— ç©·å¤§ï¼Œå®é™…ä¸Šå‡è®¾äº†ä¸€ä¸ªå¸¸æ•°å¤–æ¨æ¨¡å‹ã€‚

## æ ‘æŸå¤±å‡½æ•°

å¯¹äºå›å½’æ ‘ï¼Œæˆ‘ä»¬æœ€å°åŒ–æ®‹å·®å¹³æ–¹å’Œï¼Œå¯¹äºåˆ†ç±»æ ‘ï¼Œæˆ‘ä»¬æœ€å°åŒ–åŠ æƒå¹³å‡ Gini ä¸çº¯åº¦ã€‚

æ®‹å·®å¹³æ–¹å’Œï¼ˆRSSï¼‰è¡¡é‡å›å½’æ ‘ä¸­å®é™…å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´æ€»å¹³æ–¹å·®çš„åº¦é‡ï¼Œ

$$ \text{RSS} = \sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})Â² $$

å…¶ä¸­ $J$ æ˜¯æ ‘ä¸­çš„åŒºåŸŸæ€»æ•°ï¼Œ$R_j$ æ˜¯ç¬¬ $j$ ä¸ªåŒºåŸŸï¼Œ$y_i$ æ˜¯ç¬¬ $i$ ä¸ªè®­ç»ƒæ•°æ®å“åº”ç‰¹å¾çš„çœŸå€¼ï¼Œ$\hat{y}_{R_j}$ æ˜¯åŒºåŸŸ $R_j$ çš„é¢„æµ‹å€¼ï¼Œå³ $y_i \; \forall \; i \in R_j$ çš„å¹³å‡å€¼ã€‚

å½“çˆ¶èŠ‚ç‚¹åˆ†è£‚æˆä¸¤ä¸ªå­èŠ‚ç‚¹ï¼ˆt_Lï¼‰å’Œï¼ˆt_Rï¼‰æ—¶ï¼ŒåŠ æƒ Gini ä¸çº¯åº¦ä¸ºï¼š

$$ \text{Gini}_{\text{total}} = \sum_{j=1}^{J} \frac{N_j}{N} \cdot \text{Gini}(j) $$

å…¶ä¸­ $J$ æ˜¯æ ‘ä¸­çš„åŒºåŸŸæ€»æ•°ï¼Œ$N$ æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ€»æ•°ï¼Œ$N_j$ æ˜¯å¶èŠ‚ç‚¹ $j$ ä¸­çš„æ ·æœ¬æ•°ï¼Œ$\text{Gini}(j)$ æ˜¯å¶èŠ‚ç‚¹ $j$ çš„ Gini ä¸çº¯åº¦ã€‚

å•ä¸ªå†³ç­–æ ‘èŠ‚ç‚¹çš„ Gini ä¸çº¯åº¦è®¡ç®—å¦‚ä¸‹ï¼Œ

$$ \text{Gini}(j) = 1 - \sum_{c=1}^{C} p_{j,c}Â² $$

å…¶ä¸­ $p_{j,c}$ æ˜¯èŠ‚ç‚¹ $j$ ä¸­ç±»åˆ« $c$ æ ·æœ¬çš„æ¯”ä¾‹ã€‚

å¯¹äºåˆ†ç±»ï¼Œæˆ‘ä»¬çš„æŸå¤±å‡½æ•°ä¸æ¯”è¾ƒé¢„æµ‹å€¼ä¸çœŸå®å€¼ï¼Œå°±åƒæˆ‘ä»¬çš„å›å½’æŸå¤±ä¸€æ ·ï¼

+   Gini ä¸çº¯åº¦æƒ©ç½šè®­ç»ƒæ•°æ®ç±»åˆ«çš„æ··åˆï¼æ‰€æœ‰è®­ç»ƒæ•°æ®éƒ½æ˜¯åŒä¸€ç±»åˆ«çš„åŒºåŸŸå°†å…·æœ‰ 0 çš„ Gini ä¸çº¯åº¦ï¼Œä»è€Œæœ‰åŠ©äºæ•´ä½“æŸå¤±ã€‚

æ³¨æ„ï¼ŒæŒ‰åŒºåŸŸè®¡ç®—çš„ Gini ä¸çº¯åº¦æ˜¯ï¼Œ

+   **åŠ æƒ** - ç”±æ¯ä¸ªåŒºåŸŸä¸­çš„è®­ç»ƒæ•°æ®æ•°é‡å†³å®šï¼Œå…·æœ‰æ›´å¤šè®­ç»ƒæ•°æ®çš„åŒºåŸŸå¯¹æ•´ä½“æŸå¤±çš„å½±å“æ›´å¤§

+   **å¹³å‡** - åœ¨æ‰€æœ‰åŒºåŸŸä¸Šè®¡ç®—å†³ç­–æ ‘çš„æ€» Gini ä¸çº¯åº¦

è¿™äº›æŸå¤±æ˜¯åœ¨è®¡ç®—æœŸé—´è®¡ç®—çš„ï¼Œ

+   **æ ‘æ¨¡å‹è®­ç»ƒ** - æ ¹æ®è®­ç»ƒæ•°æ®æ¥ç”Ÿé•¿æ ‘

+   **æ ‘æ¨¡å‹è°ƒä¼˜** - æ ¹æ®ä¿ç•™çš„æµ‹è¯•æ•°æ®æ¥é€‰æ‹©æœ€ä½³æ ‘å¤æ‚åº¦ã€‚

é¦–å…ˆè®©æˆ‘ä»¬è°ˆè°ˆæ ‘æ¨¡å‹è®­ç»ƒï¼Œç„¶åå†è®¨è®ºæ ‘æ¨¡å‹è°ƒä¼˜ã€‚

## è®­ç»ƒæ ‘æ¨¡å‹

æˆ‘ä»¬å¦‚ä½•è®¡ç®—è¿™äº›äº’æ–¥çš„ã€ç©·å°½çš„åŒºåŸŸï¼Ÿè¿™æ˜¯é€šè¿‡é¢„æµ‹ç‰¹å¾ç©ºé—´çš„åˆ†å±‚äºŒè¿›åˆ¶åˆ†å‰²æ¥å®ç°çš„ã€‚

è®­ç»ƒå†³ç­–æ ‘æ¨¡å‹æ—¢æ˜¯ï¼Œ

1.  åˆ†é…äº’æ–¥çš„ã€ç©·å°½çš„åŒºåŸŸ

1.  æ„å»ºå†³ç­–æ ‘æ—¶ï¼Œæ¯ä¸ªåŒºåŸŸéƒ½æ˜¯ä¸€ä¸ªç»ˆç«¯èŠ‚ç‚¹ï¼Œä¹Ÿç§°ä¸ºå¶èŠ‚ç‚¹

è¿™äº›æ˜¯åŒä¸€ä»¶äº‹ï¼è®©æˆ‘ä»¬åˆ—å‡ºæ­¥éª¤ï¼Œç„¶åé€šè¿‡è®­ç»ƒä¸€æ£µæ ‘æ¥æ¼”ç¤ºè¿™ä¸€ç‚¹ã€‚

1.  **å°†æ‰€æœ‰æ•°æ®åˆ†é…åˆ°å•ä¸ªåŒºåŸŸ** - è¿™ä¸ªåŒºåŸŸè¦†ç›–äº†æ•´ä¸ªé¢„æµ‹ç‰¹å¾ç©ºé—´

1.  **æ‰«ææ‰€æœ‰å¯èƒ½çš„åˆ†å‰²** - åœ¨æ‰€æœ‰åŒºåŸŸå’Œæ‰€æœ‰ç‰¹å¾ä¸Š

1.  **é€‰æ‹©æœ€ä½³åˆ†å‰²** - è¿™æ˜¯ä¸€ç§è´ªå©ªä¼˜åŒ–ï¼Œå³æœ€ä½³åˆ†å‰²æœ€å°åŒ–æ‰€æœ‰è®­ç»ƒæ•°æ® $y_i$ åœ¨æ‰€æœ‰åŒºåŸŸ $j = 1,\ldots,J$ ä¸Šçš„æ®‹å·®å¹³æ–¹å’Œã€‚

1.  **è¿­ä»£ç›´åˆ°è¿‡åº¦æ‹Ÿåˆ** - è¿”å›æ­¥éª¤ 1 è¿›è¡Œä¸‹ä¸€æ¬¡åˆ†å‰²ï¼Œç›´åˆ°æ ‘éå¸¸è¿‡åº¦æ‹Ÿåˆã€‚

æ³¨æ„ï¼Œè¿™ç§è®­ç»ƒå†³ç­–æ ‘çš„æ–¹æ³•æ˜¯ä¸€ç§å¯å‘å¼è§£å†³æ–¹æ¡ˆï¼Œ

+   æ²¡æœ‰åŠªåŠ›åŒæ—¶ä¼˜åŒ–æ‰€æœ‰åˆ†å‰²ï¼Œä¾‹å¦‚ï¼Œé€‰æ‹©ä¸€ä¸ªæ¬¡ä¼˜åˆ†å‰²ä»¥æœ€å¤§åŒ–åç»­åˆ†å‰²çš„è®­ç»ƒè¯¯å·®å‡å°‘

æ­¤å¤–ï¼Œå†³ç­–æ ‘æ˜¯ä»ä¸Šåˆ°ä¸‹æ„å»ºçš„ã€‚

+   æˆ‘ä»¬ä»ä¸€ä¸ªè¦†ç›–æ•´ä¸ªé¢„æµ‹ç‰¹å¾ç©ºé—´çš„å•ä¸ªåŒºåŸŸå¼€å§‹ï¼Œç„¶åè¿›è¡Œä¸€ç³»åˆ—çš„åŒºåŸŸåˆ†å‰²/æ ‘åˆ†æ”¯ã€‚

ç°åœ¨è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜è¿™ä¸€ç‚¹ï¼Œä½¿ç”¨ 2 ä¸ªé¢„æµ‹ç‰¹å¾æ¥é¢„æµ‹å¤©ç„¶æ°”ç”Ÿäº§å“åº”ç‰¹å¾ï¼Œ

+   å­”éš™ç‡ - å½±å“å­”éš™ä½“ç§¯å’ŒæµåŠ¨

+   æ˜“ç¢æ€§ - å½±å“è¯±å¯¼å’Œä¿æŒå¼€æ”¾è£‚ç¼çš„èƒ½åŠ›

æˆ‘ä»¬ä»ä¸€ä¸ªåŒ…å«æ‰€æœ‰é¢„æµ‹ç‰¹å¾ç©ºé—´çš„å•ä¸ªåŒºåŸŸå¼€å§‹ï¼Œå°†æ‰€æœ‰è®­ç»ƒæ•°æ®çš„å¹³å‡å€¼ä½œä¸ºå¯èƒ½çš„å”¯ä¸€é¢„æµ‹ã€‚

![](img/4a6be7714eba68f580cfee8421d15f61.png)

åˆå§‹æ•°æ®å…¨éƒ¨ä½äºä¸€ä¸ªåŒºåŸŸä¸­ï¼Œå³ 1 ä¸ªå¶èŠ‚ç‚¹çš„è¶…å‚æ•°æ•°é‡ï¼Œä½¿ç”¨å“åº”ç‰¹å¾çš„å…¨çƒå‡å€¼è¿›è¡Œé¢„æµ‹ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ‰«ææ‰€æœ‰ç‰¹å¾ä»¥æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ€ä½³åˆ†å‰²ï¼Œå­”éš™ç‡ä¸º 16.7%ã€‚è¿™ä¸ªéå¸¸ç®€å•çš„å†³ç­–æ ‘åªæœ‰ä¸€ä¸ªå†³ç­–èŠ‚ç‚¹å’Œ 2 ä¸ªåŒºåŸŸæˆ–å¶å­èŠ‚ç‚¹ï¼Œè¢«ç§°ä¸ºæ ‘æ¡©æ ‘ï¼Œå³æœ€ç®€å•çš„å†³ç­–æ ‘æ¨¡å‹ã€‚

![å›¾ç‰‡](img/c418139a6f37dbda3462cd0ab2d519d6.png)

å¶å­èŠ‚ç‚¹çš„è¶…å‚æ•°æ•°é‡ä¸º 2ï¼Œç¬¬ä¸€ä¸ªæœ€ä½³åˆ†å‰²å¯¼è‡´æ ‘æ¡©æ ‘ã€‚

ç°åœ¨æˆ‘ä»¬æ‰«æä¸¤ä¸ªåŒºåŸŸä»¥åŠæ‰€æœ‰é¢„æµ‹ç‰¹å¾ï¼Œä»¥æ‰¾åˆ°æœ€ä½³ä¸‹ä¸€ä¸ªåˆ†å‰²ï¼Œåœ¨å­”éš™ç‡å¤§äºæˆ–ç­‰äº 16.7%çš„åŒºåŸŸä¸­ï¼Œè„†æ€§ä¸º 36.1ã€‚

![å›¾ç‰‡](img/5786413cc0e329c72793e1d2adf2a54b.png)

å¶å­èŠ‚ç‚¹çš„è¶…å‚æ•°æ•°é‡ä¸º 3ã€‚

ç»§ç»­è¿›è¡Œï¼Œæˆ‘ä»¬åœ¨å³ä¸ŠåŒºåŸŸæ‰¾åˆ°ä¸‹ä¸€ä¸ªåˆ†å‰²ï¼Œå­”éš™ç‡ä¸º 18.5%ã€‚ç°åœ¨æˆ‘ä»¬æœ‰ 4 ä¸ªåŒºåŸŸã€‚æˆ‘ä»¬çš„å†³ç­–æ ‘å¼€å§‹æ•æ‰éšç€å­”éš™ç‡çš„å¢åŠ è€Œå¢åŠ çš„äº§é‡ï¼Œä»¥åŠä½è„†æ€§çš„ä½äº§é‡ã€‚

![å›¾ç‰‡](img/9931df51305b719f93fc51eecf44641d.png)

å¶å­èŠ‚ç‚¹çš„è¶…å‚æ•°æ•°é‡ä¸º 4ã€‚

ç°åœ¨ä¸‹ä¸€ä¸ªæœ€ä½³åˆ†å‰²æ˜¯åœ¨åŸå§‹ä½å­”éš™ç‡åŒºåŸŸï¼Œæ¥è‡ªå­”éš™ç‡ä¸º 13.2%çš„æ ‘æ¡©æ ‘ã€‚

![å›¾ç‰‡](img/597e4adea071f8f67d71e82f4c1015f2.png)

å¶å­èŠ‚ç‚¹çš„è¶…å‚æ•°æ•°é‡ä¸º 5ã€‚

ä¸‹ä¸€ä¸ªæœ€ä½³åˆ†å‰²å°†å­”éš™ç‡åŒºåŸŸä¸€åˆ†ä¸ºäºŒï¼Œæ•æ‰äº†ä½è„†æ€§ä½äº§å‡ºçš„è¶‹åŠ¿ï¼Œå³ä½¿åœ¨é«˜å­”éš™ç‡ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚

![å›¾ç‰‡](img/b1024ff1d9be25d7b007a5ee890b6d8f.png)

å¶å­èŠ‚ç‚¹çš„è¶…å‚æ•°æ•°é‡ä¸º 6ã€‚

ä¸‹ä¸€ä¸ªåˆ†å‰²æ•æ‰äº†é«˜è„†æ€§ä¸‹çš„äº§é‡å‡å°‘ï¼Œå³ä½¿åœ¨é«˜å­”éš™ç‡ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œ

![å›¾ç‰‡](img/782938df9c411d347bdc8e6e031a6d66.png)

å¶å­èŠ‚ç‚¹çš„è¶…å‚æ•°æ•°é‡ä¸º 7ã€‚

è¿™ä¸ªåˆ†å‰²ç»§ç»­æ•æ‰æ•°æ®ä¸­çš„ç›¸åŒæ¨¡å¼ã€‚

![å›¾ç‰‡](img/c3401dcf9ca381bf65d39fada64ecde2.png)

å¶å­èŠ‚ç‚¹çš„è¶…å‚æ•°æ•°é‡ä¸º 8ã€‚

ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œåœæ­¢ï¼Œå¹¶åšå‡ºä»¥ä¸‹è§‚å¯Ÿï¼Œ

+   å±‚æ¬¡äºŒåˆ†æ³•ä¸é¡ºåºæ„å»ºå†³ç­–æ ‘ç›¸åŒï¼Œæ¯æ¬¡åˆ†å‰²å¢åŠ ä¸€ä¸ªæ–°çš„å†³ç­–èŠ‚ç‚¹ï¼Œå¹¶å°†å¶å­èŠ‚ç‚¹æ•°é‡å¢åŠ ä¸€ä¸ªã€‚

+   ç®€å•å†³ç­–æ ‘æ˜¯å¤æ‚å†³ç­–æ ‘çš„ä¸€éƒ¨åˆ†ï¼Œå³å¦‚æœæˆ‘ä»¬æ„å»ºä¸€ä¸ª 8 ä¸ªå¶å­èŠ‚ç‚¹çš„æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡é¡ºåºç§»é™¤å†³ç­–èŠ‚ç‚¹ï¼Œä»¥æœ€åä¸€ä¸ªç§»é™¤çš„é¡ºåºï¼Œå¾—åˆ° 8ã€7ã€...ã€2 ä¸ªå¶å­èŠ‚ç‚¹çš„æ¨¡å‹ã€‚

+   æœ€ç»ˆè¿‡æ‹Ÿåˆæ¨¡å‹æ˜¯å¶å­èŠ‚ç‚¹æ•°é‡ç­‰äºè®­ç»ƒæ•°æ®æ•°é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®­ç»ƒè¯¯å·®ä¸º 0.0ï¼Œå› ä¸ºæ¯ä¸ªè®­ç»ƒæ•°æ®éƒ½æœ‰ä¸€ä¸ªåŒºåŸŸï¼Œæˆ‘ä»¬ä½¿ç”¨è®­ç»ƒæ•°æ®çš„å“åº”ç‰¹å¾å€¼æ¥ä¼°è®¡æ‰€æœ‰è®­ç»ƒæ•°æ®æ¡ˆä¾‹ã€‚

## ä½¿ç”¨æ–°çš„åˆ†å‰²æ›´æ–°æŸå¤±å‡½æ•°

ä¸ºäº†æ‰¾åˆ°ä¸‹ä¸€ä¸ªæœ€ä½³åˆ†å‰²ï¼Œæˆ‘ä»¬å¿…é¡»æ‰«ææ‰€æœ‰åŒºåŸŸä»¥åŠæ‰€æœ‰ä¸åŒºåŸŸç›¸å…³çš„ç‰¹å¾ã€‚è¿™å¬èµ·æ¥å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—ï¼Œä½†å®é™…ä¸Šéå¸¸é«˜æ•ˆã€‚

+   æˆ‘ä»¬åªéœ€è¦æ£€æŸ¥æ¯ä¸ªåŒºåŸŸä¸­æ¯ä¸ªç‰¹å¾çš„æ’åºè®­ç»ƒæ•°æ®çš„ä¸­ç‚¹ï¼Œå› ä¸ºä»»ä½•ä¸ä¼šæ”¹å˜è®­ç»ƒæ•°æ®åŒºåŸŸåˆ†é…çš„åˆ†å‰²éƒ½ä¸ä¼šæ”¹å˜è®­ç»ƒæŸå¤±ã€‚

å¯¹äºä¸€ä¸ªè¢«åˆ†å‰²æˆå€™é€‰åŒºåŸŸ $R_L$ å’Œ $R_R$ çš„åŒºåŸŸ $R$ï¼Œåˆ†å‰²åçš„å‡æ–¹æ®‹å·®ï¼ˆRSSï¼‰ä¸ºï¼š

$$ \text{RSS}_{\text{split}} = \sum_{i \in R_L} (y_i - \hat{y}_{R_L})Â² + \sum_{i \in R_R} (y_i - \hat{y}_{R_R})Â² $$

å…¶ä¸­ï¼Œ$y_i$ æ˜¯è®­ç»ƒæ•°æ®è§‚å¯Ÿ $i$ çš„å®é™…å“åº”ç‰¹å¾ï¼Œè€Œ $\hat{y}_{R_L}$ï¼Œ$\hat{y}_{R_R}$ æ˜¯å€™é€‰åŒºåŸŸ $R_L$ å’Œ $R_R$ ä¸­è®­ç»ƒæ•°æ®å“åº”ç‰¹å¾çš„å‡å€¼ã€‚

æ³¨æ„ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å…¶ä»–åŒºåŸŸçš„ RSS åˆ†é‡æ·»åŠ è¿›æ¥ï¼Œä»¥è·å¾—æ€»æ¨¡å‹ RSSï¼Œä»è€Œåœ¨æ•´ä¸ªåŒºåŸŸä¸­æ‰¾åˆ°æœ€ä½³åˆ†å‰²ï¼Œ

+   é€‰æ‹©å…·æœ‰æœ€ä½ $\text{RSS}_{\text{split}}$ çš„åˆ†å‰²ä½œä¸ºåŒºåŸŸï¼Œå¹¶å°†å…¶ä¸å…¶ä»–åŒºåŸŸçš„æ‰€æœ‰æœ€ä½³åˆ†å‰²è¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ‰¾åˆ°ä¸‹ä¸€ä¸ªæœ€ä½³åˆ†å‰²ï¼Œè¿™æ˜¯ä¸€ç§è´ªå©ªè§£æ³•ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½è°ƒæ•´å†³ç­–æ ‘æ¨¡å‹äº†ã€‚

## è°ƒæ•´æ ‘æ¨¡å‹

ä¸ºäº†è°ƒæ•´å†³ç­–æ ‘ï¼Œæˆ‘ä»¬é‡‡ç”¨éå¸¸è¿‡æ‹Ÿåˆçš„å·²è®­ç»ƒæ ‘æ¨¡å‹ï¼Œ

+   ä¾æ¬¡åˆ‡å‰²æœ€åä¸€ä¸ªå†³ç­–èŠ‚ç‚¹

+   å³å‰ªæ‰å†³ç­–æ ‘çš„æœ€åä¸€ä¸ªåˆ†æ”¯

å› ä¸ºç®€å•çš„æ ‘åœ¨å¤æ‚çš„æ ‘å†…éƒ¨ï¼

æˆ‘ä»¬å¯ä»¥åœ¨å‰ªæçš„åŒæ—¶è®¡ç®—æµ‹è¯•é”™è¯¯ï¼Œå¹¶é€‰æ‹©å…·æœ‰æœ€å°æµ‹è¯•é”™è¯¯çš„æ ‘ã€‚

æˆ‘ä»¬å¯¹å†³ç­–æ ‘æ¨¡å‹è¿›è¡Œäº†è¿‡æ‹Ÿåˆï¼Œæ‹¥æœ‰å¤§é‡çš„å¶å­èŠ‚ç‚¹ï¼Œç„¶åæˆ‘ä»¬åœ¨è·Ÿè¸ªæµ‹è¯•é”™è¯¯çš„åŒæ—¶å‡å°‘äº†å¶å­èŠ‚ç‚¹çš„æ•°é‡ã€‚

+   æˆ‘ä»¬é€‰æ‹©ä½¿æµ‹è¯•é”™è¯¯æœ€å°åŒ–çš„å¶å­èŠ‚ç‚¹æ•°é‡ã€‚

+   ç”±äºæˆ‘ä»¬æ˜¯ä¾æ¬¡ç§»é™¤æœ€åä¸€ä¸ªåˆ†æ”¯ä»¥ç®€åŒ–æ ‘ï¼Œæ‰€ä»¥æˆ‘ä»¬ç§°æ¨¡å‹è°ƒæ•´ä¸ºå†³ç­–æ ‘çš„**å‰ªæ**ã€‚

è¿™é‡Œæ˜¯ä¸€ä¸ªå…·æœ‰è®¸å¤šå¶å­èŠ‚ç‚¹ï¼ˆ100 ä¸ªï¼‰çš„è¿‡æ‹Ÿåˆå†³ç­–æ ‘ã€‚

![](img/3383a81c81cdb9ed5c96432a485e7194.png)

è¿™æ˜¯ä¸€ä¸ªéå¸¸è¿‡æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œå¶å­èŠ‚ç‚¹æ•°é‡ä¸º 100ï¼ˆå·¦ï¼‰ï¼Œè®­ç»ƒå’Œæµ‹è¯•äº¤å‰éªŒè¯å›¾ï¼ˆä¸­å¿ƒï¼‰ä»¥åŠè®­ç»ƒå’Œæµ‹è¯•é”™è¯¯ä¸å¶å­èŠ‚ç‚¹æ•°é‡çš„å…³ç³»ï¼ˆå³ï¼‰ã€‚

ç”±äºè¿™æ£µæ ‘æ˜¯ç”¨æˆ‘çš„äº¤äº’å¼ Python ä»ªè¡¨æ¿è®¡ç®—çš„ï¼Œæ‰€ä»¥æˆ‘èƒ½å¤Ÿè½»æ¾åœ°å°†åŒºåŸŸæ•°é‡ä» $100, 99, 98, 96, 95, \ldots$ å‡å°‘å¹¶å¯è§†åŒ–æ ‘ï¼Œä»¥æ¢ç´¢ä»å¤æ‚åˆ°ç®€å•çš„æ ‘ã€‚

+   é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å¯ä»¥è¯æ˜ç®€å•çš„æ ‘åœ¨å¤æ‚çš„æ ‘ä¸­ã€‚

ä¾‹å¦‚ï¼Œè¿™é‡Œæ˜¯åœ¨è¿‡æ‹Ÿåˆçš„ 100 ä¸ªåŒºåŸŸå†³ç­–æ ‘ä¸­çš„ 5 ä¸ªåŒºåŸŸå†³ç­–æ ‘ï¼Œ

![](img/566f12da178143a07d17de9adc100684.png)

åœ¨éå¸¸è¿‡æ‹Ÿåˆçš„ 100 ä¸ªå¶å­èŠ‚ç‚¹æ ‘æ¨¡å‹ä¸­çš„ 5 ä¸ªå¶å­èŠ‚ç‚¹æ ‘ã€‚

è¿™é‡Œæ˜¯åœ¨è¿‡æ‹Ÿåˆçš„ 100 ä¸ªåŒºåŸŸå†³ç­–æ ‘ä¸­çš„ 10 ä¸ªåŒºåŸŸå†³ç­–æ ‘ï¼Œ

![](img/96e18c15cb01e9558e4b2512a6b1ef20.png)

åœ¨éå¸¸è¿‡æ‹Ÿåˆçš„ 100 ä¸ªå¶å­èŠ‚ç‚¹æ ‘æ¨¡å‹ä¸­çš„ 10 ä¸ªå¶å­èŠ‚ç‚¹æ ‘ã€‚

æœ€åï¼Œè¿™é‡Œæ˜¯åœ¨è¿‡æ‹Ÿåˆçš„ 100 ä¸ªåŒºåŸŸå†³ç­–æ ‘ä¸­çš„ 20 ä¸ªåŒºåŸŸå†³ç­–æ ‘ï¼Œ

![](img/19d39b48f3c129dc12099bd52c3bb546.png)

åœ¨éå¸¸è¿‡æ‹Ÿåˆçš„ 100 ä¸ªå¶å­èŠ‚ç‚¹æ ‘æ¨¡å‹ä¸­çš„ 20 ä¸ªå¶å­èŠ‚ç‚¹æ ‘ã€‚

ä½ å¯èƒ½ä¼šæƒ³çŸ¥é“ï¼Œä¸ºä»€ä¹ˆæˆ‘æ²¡æœ‰ç›´æ¥æ›´æ–°å†³ç­–æ ‘å›¾ï¼Ÿscikit-learn çš„å†³ç­–æ ‘ç»˜å›¾å‡½æ•°ä¼šé‡æ–°ç¼©æ”¾å›¾è¡¨ï¼Œå¹¶ä¸”å‡ ä½•å½¢çŠ¶å˜åŒ–å¾ˆå¤§ï¼Œè¿™ä½¿å¾—åœ¨å¤æ‚çš„æ ‘ä¸­å¯è§†åŒ–ç®€å•çš„æ ‘å˜å¾—å›°éš¾ã€‚

+   æˆ‘è®¤ä¸ºè¿™ç§é€šè¿‡å¯è§†åŒ–ç®€å•æ ‘å’Œç»˜åˆ¶å¤šè¾¹å½¢çš„å¯è§†åŒ–æ–¹æ³•å¯¹äºæ•™è‚²ç›®çš„æ¥è¯´æ•ˆæœå¾ˆå¥½ï¼

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„è¿‡åº¦æ‹Ÿåˆæ ‘ï¼Œå¹¶é€šè¿‡æ ‘å‰ªææ–¹æ³•æ¼”ç¤ºè¶…å‚æ•°è°ƒæ•´ï¼Œ

![å›¾ç‰‡](img/24f9e4745fc35d0cf9c039aec32684f0.png)

è¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰çš„å…³ç³»ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºæœ€åæ·»åŠ çš„åˆ†æ”¯å¹¶å°†å…¶ç§»é™¤ï¼Œä»¥è®¡ç®— 99 åŒºåŸŸå†³ç­–æ ‘ï¼Œè¿™åˆæ˜¯ä¸€ä¸ªç¨å¾®ç®€å•çš„å†³ç­–æ ‘ï¼Œç„¶åæˆ‘ä»¬è®¡ç®—æµ‹è¯•è¯¯å·®ã€‚

![å›¾ç‰‡](img/2c6ffbcc9634d3d3f7350cb0564380d1.png)

è¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰çš„å…³ç³»ã€‚

æˆ‘ä»¬å†æ¬¡è¯†åˆ«å‡ºæœ€åæ·»åŠ çš„åˆ†æ”¯å¹¶å°†å…¶ç§»é™¤ï¼Œä»¥è®¡ç®— 98 åŒºåŸŸå†³ç­–æ ‘ï¼Œè¿™åˆæ˜¯ä¸€ä¸ªç¨å¾®ç®€å•çš„å†³ç­–æ ‘ï¼Œç„¶åæˆ‘ä»¬è®¡ç®—æµ‹è¯•è¯¯å·®ã€‚

![å›¾ç‰‡](img/efae49220a4ea118479dcc21d333e37a.png)

è¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰çš„å…³ç³»ã€‚

æˆ‘ä»¬å†æ¬¡è¯†åˆ«å‡ºæœ€åæ·»åŠ çš„åˆ†æ”¯å¹¶å°†å…¶ç§»é™¤ï¼Œä»¥è®¡ç®— 97 åŒºåŸŸå†³ç­–æ ‘ï¼Œè¿™åˆæ˜¯ä¸€ä¸ªç¨å¾®ç®€å•çš„å†³ç­–æ ‘ï¼Œç„¶åæˆ‘ä»¬è®¡ç®—æµ‹è¯•è¯¯å·®ã€‚

![å›¾ç‰‡](img/f4af3bd7dce90621152ff68c398bd578.png)

è¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰çš„å…³ç³»ã€‚

æˆ‘ä»¬å†æ¬¡è¯†åˆ«å‡ºæœ€åæ·»åŠ çš„åˆ†æ”¯å¹¶å°†å…¶ç§»é™¤ï¼Œä»¥è®¡ç®— 96 åŒºåŸŸå†³ç­–æ ‘ï¼Œè¿™åˆæ˜¯ä¸€ä¸ªç¨å¾®ç®€å•çš„å†³ç­–æ ‘ï¼Œç„¶åæˆ‘ä»¬è®¡ç®—æµ‹è¯•è¯¯å·®ã€‚

![å›¾ç‰‡](img/eeea92714bf8b2f878a05ce7f50e9eae.png)

è¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰çš„å…³ç³»ã€‚

æˆ‘ä»¬å†æ¬¡è¯†åˆ«å‡ºæœ€åæ·»åŠ çš„åˆ†æ”¯å¹¶å°†å…¶ç§»é™¤ï¼Œä»¥è®¡ç®— 95 åŒºåŸŸå†³ç­–æ ‘ï¼Œè¿™åˆæ˜¯ä¸€ä¸ªç¨å¾®ç®€å•çš„å†³ç­–æ ‘ï¼Œç„¶åæˆ‘ä»¬è®¡ç®—æµ‹è¯•è¯¯å·®ã€‚

![å›¾ç‰‡](img/90180132c5183f3342f2051c501aee4a.png)

è¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰çš„å…³ç³»ã€‚

æˆ‘ä»¬å†æ¬¡è¯†åˆ«å‡ºæœ€åæ·»åŠ çš„åˆ†æ”¯å¹¶å°†å…¶ç§»é™¤ï¼Œä»¥è®¡ç®— 94 åŒºåŸŸå†³ç­–æ ‘ï¼Œè¿™åˆæ˜¯ä¸€ä¸ªç¨å¾®ç®€å•çš„å†³ç­–æ ‘ï¼Œç„¶åæˆ‘ä»¬è®¡ç®—æµ‹è¯•è¯¯å·®ã€‚

![å›¾ç‰‡](img/53f087807a31ef6a8e1be0e1ec170292.png)

è¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰çš„å…³ç³»ã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å›åˆ°è¿™ä¸ªè¿‡åº¦æ‹Ÿåˆçš„æ¨¡å‹ï¼Œå¹¶åœ¨ä¸åŒå¤æ‚åº¦çº§åˆ«ä¸Šæ·»åŠ æ›´å¤šä¿¡æ¯ã€‚

![å›¾ç‰‡](img/ab38e613fbf8ad17fdab5bd4f89adaa6.png)

è¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œå¶èŠ‚ç‚¹æ•°é‡ä¸º 100ï¼ˆå·¦ï¼‰ï¼Œè®­ç»ƒå’Œæµ‹è¯•äº¤å‰éªŒè¯å›¾ï¼ˆä¸­å¿ƒï¼‰ä»¥åŠè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸å¶èŠ‚ç‚¹æ•°é‡çš„å…³ç³»ï¼ˆå³ï¼‰ã€‚

æˆ‘åŒ…æ‹¬çš„ï¼Œ

+   è®­ç»ƒå’Œæµ‹è¯•äº¤å‰éªŒè¯å›¾ï¼Œå¯¹äº 100 ä¸ªå¶èŠ‚ç‚¹çš„è¿‡åº¦æ‹Ÿåˆå†³ç­–æ ‘ï¼Œå‡ ä¹å®Œç¾çš„è®­ç»ƒé¢„æµ‹å’Œéå¸¸å·®çš„æµ‹è¯•é¢„æµ‹ã€‚

+   è®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸å¶èŠ‚ç‚¹æ•°é‡çš„å…³ç³»ã€‚

è¿™è¡¨æ˜å†³ç­–æ ‘æ¨¡å‹ç¡®å®éå¸¸è¿‡åº¦æ‹Ÿåˆï¼Œä¾‹å¦‚ï¼Œçœ‹åˆ°ä¸‹é™çš„è®­ç»ƒè¯¯å·®å’Œä¸Šå‡çš„æµ‹è¯•è¯¯å·®ã€‚

ç°åœ¨æˆ‘ä»¬ä¿®å‰ªå†³ç­–èŠ‚ç‚¹ï¼Œç›´åˆ°æˆ‘ä»¬è·å¾—åœ¨çº¦ 19 ä¸ªå¶èŠ‚ç‚¹å¤„å…·æœ‰æœ€å°æµ‹è¯•è¯¯å·®çš„æ¨¡å‹ã€‚

![å›¾ç‰‡](img/064eac7331174101230aee9c413623f3.png)

è°ƒæ•´åçš„å†³ç­–æ ‘ï¼Œå¶èŠ‚ç‚¹æ•°é‡ä¸º 20ï¼ˆå·¦ï¼‰ï¼Œè®­ç»ƒå’Œæµ‹è¯•äº¤å‰éªŒè¯å›¾ï¼ˆä¸­å¿ƒï¼‰ä»¥åŠè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®éšå¶èŠ‚ç‚¹æ•°é‡çš„å˜åŒ–ï¼Œè¡¨æ˜æµ‹è¯•è¯¯å·®æœ€å°åŒ–ï¼ˆå³ï¼‰ã€‚

ä¸ºäº†å®Œæ•´æ€§ï¼Œæˆ‘åŒ…æ‹¬äº†ä¸€ä¸ªæ¬ æ‹Ÿåˆæ¨¡å‹ï¼Œå³å¦‚æœæˆ‘ä»¬è¿‡åº¦ä¿®å‰ªæˆ‘ä»¬çš„å†³ç­–æ ‘ï¼Œåªæœ‰ 8 ä¸ªå¶èŠ‚ç‚¹ã€‚

![å›¾ç‰‡](img/9d7b773b0de829f858763876c59a1c04.png)

æ¬ æ‹Ÿåˆå†³ç­–æ ‘ï¼Œå¶èŠ‚ç‚¹æ•°é‡ä¸º 8ï¼ˆå·¦ï¼‰ï¼Œè®­ç»ƒå’Œæµ‹è¯•äº¤å‰éªŒè¯å›¾ï¼ˆä¸­å¿ƒï¼‰ä»¥åŠè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®éšå¶èŠ‚ç‚¹æ•°é‡çš„å˜åŒ–ï¼Œè¡¨æ˜æµ‹è¯•è¯¯å·®æœ€å°åŒ–ï¼ˆå³ï¼‰ã€‚

æ³¨æ„ï¼Œè®­ç»ƒè¯¯å·®å’Œæµ‹è¯•è¯¯å·®éƒ½éå¸¸é«˜ï¼Œè¿™æ˜¯æ¬ æ‹Ÿåˆå†³ç­–æ ‘çš„è¡¨ç°ã€‚

æˆ‘æ›´å–œæ¬¢å°†å¶èŠ‚ç‚¹æ•°é‡ä½œä¸ºæˆ‘çš„å†³ç­–æ ‘è¶…å‚æ•°ï¼Œå› ä¸ºå®ƒæä¾›äº†ï¼Œ

+   **è¿ç»­ã€å‡åŒ€çš„å¤æ‚æ€§å¢åŠ ** - å¤æ‚æ€§å¢åŠ çš„æ­¥éª¤ç›¸ç­‰ï¼Œæ²¡æœ‰è·³è·ƒ

+   **ç›´è§‚çš„å¤æ‚æ€§æ§åˆ¶** - æˆ‘ä»¬å¯ä»¥ç†è§£å’Œå…³è”$2, 3, \ldots, 100$ä¸ªå¶èŠ‚ç‚¹çš„å†³ç­–æ ‘

+   **çµæ´»çš„å¤æ‚æ€§** - æ ‘å¯ä»¥è‡ªç”±åœ°ä»¥ä»»ä½•æ–¹å¼å¢é•¿ä»¥å‡å°‘è®­ç»ƒè¯¯å·®ï¼ŒåŒ…æ‹¬é«˜åº¦ä¸å¯¹ç§°çš„å†³ç­–æ ‘

å…¶ä»–å¸¸è§çš„å†³ç­–æ ‘è¶…å‚æ•°åŒ…æ‹¬ï¼Œ

+   **æœ€å°å‡å°‘çš„ RSS** â€“ ä¸å¢é‡å¢åŠ å¤æ‚æ€§å¿…é¡»ç”±è¶³å¤Ÿçš„è®­ç»ƒè¯¯å·®å‡å°‘æ¥æŠµæ¶ˆçš„æƒ³æ³•ç›¸å…³ã€‚è¿™å¯èƒ½å¯¼è‡´æ¨¡å‹æå‰åœæ­¢ï¼Œä¾‹å¦‚ï¼Œå…·æœ‰ä½è®­ç»ƒè¯¯å·®å‡å°‘çš„åˆ†å‰²å¯èƒ½å¯¼è‡´éšåçš„åˆ†å‰²å…·æœ‰æ›´å¤§çš„è®­ç»ƒè¯¯å·®å‡å°‘

+   **æ¯ä¸ªåŒºåŸŸçš„æœ€å°è®­ç»ƒæ•°æ®é‡** â€“ ä¸åŒºåŸŸä¼°è®¡çš„å‡†ç¡®æ€§æ¦‚å¿µç›¸å…³ï¼Œå³æˆ‘ä»¬éœ€è¦è‡³å°‘$n$ä¸ªæ•°æ®æ¥è·å¾—å¯é çš„å‡å€¼å’Œæœ€å¸¸è§ç±»åˆ«

+   **æœ€å¤§å±‚æ•°** â€“ å¼ºåˆ¶å¯¹ç§°æ ‘ï¼Œåˆ°è¾¾æ¯ä¸ªå¶èŠ‚ç‚¹çš„åˆ†å‰²æ•°ç›¸ä¼¼ã€‚æ¨¡å‹å¤æ‚åº¦éšç€è¶…å‚æ•°çš„å˜åŒ–è€Œæœ‰å¾ˆå¤§å˜åŒ–ã€‚

## é¢„æµ‹æ¨¡å‹

å†³ç­–æ ‘é¢„æµ‹æ¨¡å‹è¡¨ç¤ºä¸º**åµŒå¥—çš„ if è¯­å¥**ï¼Œä¾‹å¦‚ï¼š

```py
if porosity > 0.15:
    if brittleness < 20:
        initial_production = 1000
    else:
        initial_production = 7000
else:
    if brittleness < 40:
        initial_production = 500
    else:
        initial_production = 3000 
```

ä»¥åŠä¸Šè¿°é¢„æµ‹æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼Œ

+   å›å½’ - åŒºåŸŸå†…è®­ç»ƒæ•°æ®çš„å¹³å‡å€¼

+   åˆ†ç±» - åŒºåŸŸå†…è®­ç»ƒæ•°æ®çš„å¤šæ•°ç±»åˆ«

## å†³ç­–æ ‘ä¸­çš„ Shapley å€¼

å›æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ä¸€ä¸ªå•ä¸€æ¨¡å‹è¿›è¡Œä¼°è®¡ï¼Œä¾‹å¦‚ï¼Œ$f(x_1,x_2,x_3,x_4)$ï¼Œå¹¶å¯¹æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾å­é›†ç»„åˆè¿›è¡Œä¼°è®¡ï¼Œä¾‹å¦‚ï¼Œ

$$ f(x_1) \quad f(x_2,x_4) \quad f(x_1,x_2,x_3) $$

+   æ³¨æ„ï¼Œè®¡ç®— Shapley å€¼çš„æœ´ç´ æ–¹æ³•æ˜¯å¯¹å…·æœ‰ä¸åŒé¢„æµ‹ç‰¹å¾çš„æ¨¡å‹çš„å…¨ç»„åˆè¿›è¡Œè®­ç»ƒï¼Œä½†å¦‚æœæˆ‘ä»¬ç›®æ ‡æ˜¯ç‰¹å¾é‡è¦æ€§ä»¥è¯Šæ–­æˆ‘ä»¬çš„ç‰¹å®šæ¨¡å‹$f$ï¼Œæ”¯æŒæ¨¡å‹å¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬ä¸æƒ³åˆ›å»ºæ–°çš„æ¨¡å‹ã€‚

ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯åº”ç”¨å¤šç§æ–¹æ³•ï¼Œç±»ä¼¼äºæ’è¡¥æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼Œ

+   ç”¨æœŸæœ›å€¼ï¼Œå³å…¨å±€å¹³å‡å€¼æ›¿æ¢è¢«æ’é™¤çš„ç‰¹å¾ï¼Œ

$$ f(x_1,x_2,x_3) = f(x_1,x_2,x_3,x_4=E[x_4]) $$

+   ç”¨ä¸­ä½æ•°ï¼Œå³ç¬¬ 50 ç™¾åˆ†ä½æ•°æ›¿æ¢è¢«æ’é™¤çš„ç‰¹å¾ï¼Œ

$$ f(x_1,x_2,x_3) = f(x_1,x_2,x_3,x_4=P50_{x_4}) $$

å¯¹äºåŸºäºæ ‘çš„æ¨¡å‹ï¼Œæœ‰ä¸€ä¸ªæ›´å‡†ç¡®ã€ç‹¬ç‰¹çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥åœ¨æ¨¡å‹è®­ç»ƒåç§»é™¤ä»»ä½•ç‰¹å¾å¯¹å†³ç­–æ ‘çš„å½±å“ï¼Œä¾‹å¦‚ï¼Œ

+   ç§»é™¤æ‰€æœ‰$x_4$åˆ†æ”¯ï¼Œç„¶åæ¨¡å‹ä¸ä¼šä½¿ç”¨$x_4$è¿›è¡Œé¢„æµ‹

å½“ç„¶ï¼Œæˆ‘ä»¬ä¸å¯èƒ½åªæ˜¯ç§»é™¤åˆ†æ”¯ï¼Œç„¶åç”¨â€œèƒ¶æ°´â€å°†æ ‘é‡æ–°ç²˜åˆåœ¨ä¸€èµ·ï¼

+   æˆ‘ä»¬å¿…é¡»åšå‡ºæ–°çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹ä¸ä¼šå¼•å…¥åå·®ã€‚

è®©æˆ‘ä»¬é€šè¿‡å‡ ä¸ªé¢„æµ‹æ¡ˆä¾‹æ¥æ¼”ç¤ºä»å†³ç­–æ ‘ä¸­ç§»é™¤ç‰¹å¾çš„è¿‡ç¨‹ï¼Œ

1.  è¿™é‡Œæ˜¯ä¸€ä¸ªæ²¡æœ‰é‡åˆ°è¢«ç§»é™¤ç‰¹å¾çš„é¢„æµ‹æ¡ˆä¾‹ï¼Œç§»é™¤$x_2$åï¼Œ

$$ x_1=25 $$

+   é¢„æµ‹é€šå¸¸è¿›è¡Œã€‚

![å›¾ç‰‡](img/a9783a200ab476bb3a92cb2cfec12d63.png)

å¯¹äºæ²¡æœ‰é‡åˆ°è¢«ç§»é™¤ç‰¹å¾çš„é¢„æµ‹æ¡ˆä¾‹ï¼Œé€šå¸¸è¿›è¡Œé¢„æµ‹ã€‚

$$ f(x_1=25) = 20 $$

1.  ä¸€ä¸ªé‡åˆ°è¢«ç§»é™¤ç‰¹å¾$x_1$çš„é¢„æµ‹æ¡ˆä¾‹ï¼Œç§»é™¤$x_1$åï¼Œ

$$ x_2 = 60 $$

+   æˆ‘ä»¬å®é™…ä¸Šé€šè¿‡åŠ æƒï¼ŒæŒ‰è®­ç»ƒæ•°æ®æ•°é‡åŠ æƒï¼Œæ²¿ç€ä¸¤æ¡è·¯å¾„éƒ½æ‰¾åˆ°äº†è§£å†³æ–¹æ¡ˆï¼

![å›¾ç‰‡](img/8152951b8a3c920f03825528d98c5ae7.png)

å¯¹äºé‡åˆ°è¢«ç§»é™¤ç‰¹å¾çš„é¢„æµ‹æ¡ˆä¾‹ï¼Œé€šè¿‡æŒ‰è®­ç»ƒæ•°æ®æ•°é‡åŠ æƒä¸¤æ¡è·¯å¾„æ¥è¿›è¡Œé¢„æµ‹ã€‚

$$ f(x_2=60) = \frac{60}{100} \left[ \frac{15}{60} \times 20 + \frac{45}{60} \times 70 \right] + \frac{40}{100} \left[130\right] = 86.5 $$$$ f(x_2=60) = 86.5 $$

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = True                                      # toggle to supress warnings
import os                                                     # to set current working directory 
import math                                                   # square root operator
import numpy as np                                            # arrays and matrix math
import scipy.stats as st                                      # statistical methods
import pandas as pd                                           # DataFrames
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import seaborn as sns                                         # for matrix scatter plots
from sklearn import tree                                      # tree program from scikit learn (package for machine learning)
from sklearn.tree import _tree                                # for accessing tree information
from sklearn import metrics                                   # measures to check our models
from sklearn.preprocessing import StandardScaler              # standardize the features
from sklearn.tree import export_graphviz                      # graphical visualization of trees
from sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning
from sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline
from sklearn import metrics                                   # measures to check our models
from sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation
from sklearn.model_selection import train_test_split          # train and test split
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # supress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœä½ é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½éœ€è¦é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„å‡ ä¸ªã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æ›´å¤šå¸®åŠ©å¯ä»¥åœ¨ç›¸åº”åŒ…çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰å‡ ä¸ªå‡½æ•°æ¥ç®€åŒ–ç›¸å…³çŸ©é˜µçš„ç»˜åˆ¶å’Œå†³ç­–æ ‘å›å½’æ¨¡å‹çš„å¯è§†åŒ–ã€‚

```py
def comma_format(x, pos):
    return f'{int(x):,}'

def feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot
    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1
    plt.plot(pred,metric,color='black',zorder=20)
    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)
    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  
    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)
    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();
    return

def plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix 
    my_colormap = plt.get_cmap('RdBu_r', 256)          
    newcolors = my_colormap(np.linspace(0, 1, 256))
    white = np.array([256/256, 256/256, 256/256, 1])
    white_low = int(128 - mask*128); white_high = int(128+mask*128)
    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)
    newcmp = ListedColormap(newcolors)
    m = corr_matrix.shape[0]
    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)
    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()
    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()
    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
    plt.colorbar(im, orientation = 'vertical')
    plt.title(title)
    for i in range(0,m):
        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')
        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')
    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def plot_CDF(data,color,alpha=1.0,lw=1,ls='solid',label='none'):
    cumprob = (np.linspace(1,len(data),len(data)))/(len(data)+1)
    plt.scatter(np.sort(data),cumprob,c=color,alpha=alpha,edgecolor='black',lw=lw,ls=ls,label=label,zorder=10)
    plt.plot(np.sort(data),cumprob,c=color,alpha=alpha,lw=lw,ls=ls,zorder=8)

def extract_rules(tree_model, feature_names):                 # recursive method to extract rules, from paulkernfeld Stack Overflow (?)
    rules = []
    def traverse(node, depth, prev_rule):
        if tree_model.tree_.children_left[node] == -1:        # Leaf node
            class_label = np.argmax(tree_model.tree_.value[node])
            rule = f"{prev_rule} => Class {class_label}"
            rules.append(rule)
        else:  # Split node
            feature = feature_names[tree_model.tree_.feature[node]]
            threshold = tree_model.tree_.threshold[node]
            left_child = tree_model.tree_.children_left[node]
            right_child = tree_model.tree_.children_right[node]
            traverse(left_child, depth + 1, f"{prev_rule} & {feature} <= {threshold}") # Recursively traverse left and right subtrees
            traverse(right_child, depth + 1, f"{prev_rule} & {feature} > {threshold}")
    traverse(0, 0, "Root")
    return rules

def plot_decision_tree_regions(tree_model, feature_names,X_min,X_max,annotate=True):
    rules = extract_rules(tree_model, feature_names)
    for irule, ____ in enumerate(rules):
        rule = rules[irule].split()[2:]
        X_min = Xmin[0]; X_max = Xmax[0]; Y_min = Xmin[1]; Y_max = Xmax[1];
        index = [i for i,val in enumerate(rule) if val==feature_names[0]]
        for i in index:
            if rule[i+1] == '<=':
                X_max = min(float(rule[i+2]),X_max)
            else:
                X_min = max(float(rule[i+2]),X_min)
        index = [i for i,val in enumerate(rule) if val==feature_names[1]]
        for i in index:
            if rule[i+1] == '<=':
                Y_max = min(float(rule[i+2]),Y_max)
            else:
                Y_min = max(float(rule[i+2]),Y_min) 
        plt.gca().add_patch(plt.Rectangle((X_min,Y_min),X_max-X_min,Y_max-Y_min, lw=2,ec='black',fc="none"))
        cx = (X_min + X_max)*0.5; cy = (Y_min + Y_max)*0.5; loc = np.array((cx,cy)).reshape(1, -1)
        if annotate == True:
            plt.annotate(text = str(f'{np.round(tree_model.predict(loc)[0],2):,.0f}'),xy=(cx,cy),ha='center',
                         weight='bold',c='white',zorder=100)

def visualize_tree_model(model,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,
                         ymax,title,Xname,yname,Xlabel,ylabel,annotate=True):# plots the data points and the decision tree prediction 
    cmap = plt.cm.inferno
    X1plot_step = (Xmax[0] - Xmin[0])/300.0; X2plot_step = -1*(Xmax[1] - Xmin[1])/300.0 # resolution of the model visualization
    XX1, XX2 = np.meshgrid(np.arange(Xmin[0], Xmax[0], X1plot_step), # set up the mesh
                     np.arange(Xmax[1], Xmin[1], X2plot_step))
    y_hat = model.predict(np.c_[XX1.ravel(), XX2.ravel()])    # predict with our trained model over the mesh
    y_hat = y_hat.reshape(XX1.shape)

    plt.imshow(y_hat,interpolation=None, aspect="auto", extent=[Xmin[0],Xmax[0],Xmin[1],Xmax[1]], 
        vmin=ymin,vmax=ymax,alpha = 0.2,cmap=cmap,zorder=1)
    sp = plt.scatter(X1_train,X2_train,s=None, c=y_train, marker='o', cmap=cmap, 
        norm=None, vmin=ymin, vmax=ymax, alpha=0.6, linewidths=0.3, edgecolors="black", label = 'Train',zorder=10)
    plt.scatter(X1_test,X2_test,s=None, c=y_test, marker='s', cmap=cmap, 
        norm=None, vmin=ymin, vmax=ymax, alpha=0.3, linewidths=0.3, edgecolors="black", label = 'Test',zorder=10)

    plot_decision_tree_regions(model,Xname,Xmin,Xmax,annotate)
    plt.title(title); plt.xlabel(Xlabel[0]); plt.ylabel(Xlabel[1])
    plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])
    cbar = plt.colorbar(sp, orientation = 'vertical')         # add the color bar
    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
    cbar.set_label(ylabel, rotation=270, labelpad=20)
    return y_hat

def check_tree_model(model,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,ymax,title): # plots the estimated vs. the actual 
    y_hat_train = model.predict(np.c_[X1_train,X2_train]); y_hat_test = model.predict(np.c_[X1_test,X2_test])

    df_cross = pd.DataFrame(np.c_[y_test,y_hat_test],columns=['y_test','y_hat_test'])
    df_cross_train = pd.DataFrame(np.c_[y_train,y_hat_train],columns=['y_train','y_hat_train'])

    plt.scatter(y_train,y_hat_train,s=15, c='blue',marker='o', cmap=None, norm=None, vmin=None, vmax=None, alpha=0.7, 
                linewidths=0.3, edgecolors="black",label='Train',zorder=10)
    plt.scatter(y_test,y_hat_test,s=15, c='red',marker='s', cmap=None, norm=None, vmin=None, vmax=None, alpha=0.7, 
                linewidths=0.3, edgecolors="black",label='Test',zorder=10)

    unique_y_hat_all = set(np.concatenate([y_hat_test,y_hat_train]))
    for y_hat in unique_y_hat_all:
        plt.plot([ymin,ymax],[y_hat,y_hat],c='black',alpha=0.2,ls='--',zorder=1)

    unique_y_hat_test = set(y_hat_test)
    for y_hat in unique_y_hat_test:
        #plt.plot([ymin,ymax],[y_hat,y_hat],c='black',alpha=0.3,ls='--',zorder=1)
        cond_mean_y_hat = df_cross.loc[df_cross['y_hat_test'] == y_hat, 'y_test'].mean()
        cond_P75_y_hat = df_cross.loc[df_cross['y_hat_test'] == y_hat, 'y_test'].quantile(0.75)
        cond_P25_y_hat = df_cross.loc[df_cross['y_hat_test'] == y_hat, 'y_test'].quantile(0.25)
        plt.scatter(cond_mean_y_hat,y_hat-0.02*(ymax-ymin),color='red',edgecolor='black',s=60,marker='^',zorder=100)
        plt.plot([cond_P25_y_hat,cond_P75_y_hat],[y_hat-0.025*(ymax-ymin),y_hat-0.025*(ymax-ymin)],c='black',lw=0.7)
        plt.plot([cond_P25_y_hat,cond_P25_y_hat],[y_hat-0.032*(ymax-ymin),y_hat-0.018*(ymax-ymin)],c='black',lw=0.7)
        plt.plot([cond_P75_y_hat,cond_P75_y_hat],[y_hat-0.032*(ymax-ymin),y_hat-0.018*(ymax-ymin)],c='black',lw=0.7)

    unique_y_hat_train = set(y_hat_train)
    for y_hat in unique_y_hat_train:
        #plt.plot([ymin,ymax],[y_hat,y_hat],c='black',alpha=0.3,ls='--',zorder=1)
        cond_mean_y_hat = df_cross_train.loc[df_cross_train['y_hat_train'] == y_hat, 'y_train'].mean()
        cond_P75_y_hat = df_cross_train.loc[df_cross_train['y_hat_train'] == y_hat, 'y_train'].quantile(0.75)
        cond_P25_y_hat = df_cross_train.loc[df_cross_train['y_hat_train'] == y_hat, 'y_train'].quantile(0.25)
        plt.scatter(cond_mean_y_hat,y_hat+0.02*(ymax-ymin),color='blue',edgecolor='black',s=60,marker='v',zorder=100)
        plt.plot([cond_P25_y_hat,cond_P75_y_hat],[y_hat+0.025*(ymax-ymin),y_hat+0.025*(ymax-ymin)],c='black',lw=0.7)
        plt.plot([cond_P25_y_hat,cond_P25_y_hat],[y_hat+0.032*(ymax-ymin),y_hat+0.018*(ymax-ymin)],c='black',lw=0.7)
        plt.plot([cond_P75_y_hat,cond_P75_y_hat],[y_hat+0.032*(ymax-ymin),y_hat+0.018*(ymax-ymin)],c='black',lw=0.7)

    plt.title(title); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')
    plt.xlim([ymin,ymax]); plt.ylim([ymin,ymax]); plt.legend(loc='upper left')
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks

    plt.arrow(ymin,ymin,ymax,ymax,width=0.02,color='black',head_length=0.0,head_width=0.0)
    MSE_train = metrics.mean_squared_error(y_train,y_hat_train); MSE_test = metrics.mean_squared_error(y_test,y_hat_test)
    plt.gca().add_patch(plt.Rectangle((ymin+0.6*(ymax-ymin),ymin+0.1*(ymax-ymin)),0.40*(ymax-ymin),0.12*(ymax-ymin),
        lw=0.5,ec='black',fc="white",zorder=100))
    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.annotate('MSE Testing:  ' + str(f'{np.round(MSE_test,2):,.0f}'),(ymin+0.62*(ymax-ymin),ymin+0.18*(ymax-ymin)),zorder=1000)
    plt.annotate('MSE Training: ' + str(f'{np.round(MSE_train,2):,.0f}'),(ymin+0.62*(ymax-ymin),ymin+0.12*(ymax-ymin)),zorder=1000)

def tree_tuning(node_max,cnode,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,ymax,title,seed):
    MSE_test_mat = np.zeros(node_max-1); MSE_train_mat = np.zeros(node_max-1);

    for imax_leaf_node, max_leaf_node in enumerate(range(2,node_max+1)):
        np.random.seed(seed = seed)
        tree_temp = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_node)
        tree_temp = tree_temp.fit(X_train.values, y_train.values)
        y_hat_train = tree_temp.predict(np.c_[X1_train,X2_train]); y_hat_test = tree_temp.predict(np.c_[X1_test,X2_test])  
        MSE_train_mat[imax_leaf_node] = metrics.mean_squared_error(y_train,y_hat_train)
        MSE_test_mat[imax_leaf_node] = metrics.mean_squared_error(y_test,y_hat_test)
        if max_leaf_node == cnode:
            plt.scatter(cnode,MSE_train_mat[imax_leaf_node],color='blue',edgecolor='black',s=20,marker='o',zorder=1000)
            plt.scatter(cnode,MSE_test_mat[imax_leaf_node],color='red',edgecolor='black',s=20,marker='o',zorder=1000)
    maxcheck = max(np.max(MSE_train_mat),np.max(MSE_test_mat))

    plt.vlines(cnode,0,maxcheck,color='black',ls='--',lw=1,zorder=1) 
    plt.plot(range(2,node_max+1),MSE_train_mat,color='blue',zorder=100,label='Train')
    plt.plot(range(2,node_max+1),MSE_test_mat,color='red',zorder=100,label='Test')

    plt.title(title); plt.xlabel('Maximum Number of Leaf Nodes'); plt.ylabel('Means Square Error')
    plt.xlim([0,node_max]); plt.ylim([0,maxcheck]); plt.legend(loc='upper right')
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def tree_to_code(tree, feature_names):                        # code from StackOverFlow by paulkernfeld
    tree_ = tree.tree_                                        # convert tree object to portable code to use anywhere
    feature_name = [
        feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
        for i in tree_.feature
    ]
    print("def tree({}):".format(", ".join(feature_names)))

    def recurse(node, depth):
        indent = "  " * depth
        if tree_.feature[node] != _tree.TREE_UNDEFINED:
            name = feature_name[node]
            threshold = tree_.threshold[node]
            print("{}if {} <= {}:".format(indent, name, threshold))
            recurse(tree_.children_left[node], depth + 1)
            print("{}elif {} > {}".format(indent, name, threshold))
            recurse(tree_.children_right[node], depth + 1)
        else:
            print("{}return {}".format(indent, tree_.value[node]))
    recurse(0, 1) 

def get_lineage(tree, feature_names):                         # code from StackOverFlow by Zelanzny7
    left      = tree.tree_.children_left                      # track the decision path for any set of inputs
    right     = tree.tree_.children_right
    threshold = tree.tree_.threshold
    features  = [feature_names[i] for i in tree.tree_.feature]
    # get ids of child nodes
    idx = np.argwhere(left == -1)[:,0]     
    def recurse(left, right, child, lineage=None):          
        if lineage is None:
            lineage = [child]
        if child in left:
            parent = np.where(left == child)[0].item()
            split = 'l'
        else:
            parent = np.where(right == child)[0].item()
            split = 'r'
        lineage.append((parent, split, threshold[parent], features[parent]))
        if parent == 0:
            lineage.reverse()
            return lineage
        else:
            return recurse(left, right, parent, lineage)
    for child in idx:
        for node in recurse(left, right, child):
            print(node) 

def display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)
    html_str = ''
    for df in args:
        html_str += df.head().to_html()  # Using .head() for the first few rows
    display(HTML(f'<div style="display: flex;">{html_str}</div>')) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆæ¯æ¬¡éƒ½é¿å…åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/PGE383")                                        # set the working directory 
```

ä½ å°†ä¸å¾—ä¸æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ä»¥åŒ…å«ä½ è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”åœ¨ Mac ä¸Šæ ¼å¼ä¸åŒï¼ˆä¾‹å¦‚ï¼Œâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½æ•°æ®

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›†[unconv_MV.csv](https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv)ï¼Œå®ƒåœ¨æˆ‘çš„ GeoDataSet ä»“åº“ä¸­å¯ç”¨ã€‚å®ƒæ˜¯ä¸€ä¸ªé€—å·åˆ†éš”çš„æ–‡ä»¶ï¼ŒåŒ…å«ï¼š

+   äº•æŒ‡æ•°ï¼ˆæ•´æ•°ï¼‰

+   å­”éš™ç‡ (%)

+   æ¸—é€ç‡ ($mD$)

+   å£°æ³¢é˜»æŠ— ($\frac{kg}{mÂ³} \cdot \frac{m}{s} \cdot 10â¶$)ã€‚

+   å‰ªåˆ‡ç‡ (%) 

+   æ€»æœ‰æœºç¢³å«é‡ (%) 

+   ç»ç’ƒå…‰æ³½ç‡ (%) 

+   åˆå§‹æ°”ä½“äº§é‡ï¼ˆ90 å¤©å¹³å‡ï¼‰(MCFPD)

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸ºâ€˜dfâ€™çš„æ•°æ®æ¡†ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

**Python æŠ€å·§ï¼šä½¿ç”¨åŒ…ä¸­çš„å‡½æ•°**åªéœ€è¾“å…¥æˆ‘ä»¬åœ¨å¼€å¤´å£°æ˜çš„åŒ…çš„æ ‡ç­¾ï¼š

```py
import pandas as pd 
```

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®¿é—® pandas å‡½æ•°â€˜read_csvâ€™ï¼š

```py
pd.read_csv() 
```

ä½†è¯»å– csv æ–‡ä»¶éœ€è¦è¾“å…¥å‚æ•°ã€‚å…¶ä¸­æœ€é‡è¦çš„ä¸€ä¸ªæ˜¯æ–‡ä»¶åã€‚å¯¹äºæˆ‘ä»¬çš„æƒ…å†µï¼Œæ‰€æœ‰å…¶ä»–é»˜è®¤å‚æ•°éƒ½å¾ˆå¥½ã€‚å¦‚æœæ‚¨æƒ³æŸ¥çœ‹æ­¤å‡½æ•°çš„æ‰€æœ‰å¯èƒ½å‚æ•°ï¼Œè¯·è®¿é—®[è¿™é‡Œ](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)çš„æ–‡æ¡£ã€‚

+   æ–‡æ¡£æ€»æ˜¯å¾ˆæœ‰å¸®åŠ©ã€‚

+   Python å‡½æ•°é€šå¸¸æœ‰å¾ˆå¤šçµæ´»æ€§ï¼Œè¿™å¯ä»¥é€šè¿‡ä½¿ç”¨å„ç§è¾“å…¥å‚æ•°æ¥å®ç°ã€‚

æ­¤å¤–ï¼Œç¨‹åºæœ‰ä¸€ä¸ªè¾“å‡ºï¼Œä¸€ä¸ªä»æ•°æ®åŠ è½½çš„ pandas DataFrameã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šä»£è¡¨è¿™ä¸ªæ–°å¯¹è±¡çš„åå­—/å˜é‡ã€‚

```py
df = pd.read_csv("unconv_MV.csv") 
```

è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªå‘½ä»¤æ¥åŠ è½½æ•°æ®ï¼Œç„¶åè¿è¡Œè¿™ä¸ªå‘½ä»¤æ¥æå–æ•°æ®çš„ä¸€ä¸ªéšæœºå­é›†ã€‚

```py
df = df.sample(frac=.30, random_state = 73073); 
df = df.reset_index() 
```

## ç‰¹å¾å·¥ç¨‹

è®©æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œä¸€äº›ä¿®æ”¹ä»¥æ”¹è¿›å·¥ä½œæµç¨‹ï¼š

+   **é€‰æ‹©é¢„æµ‹ç‰¹å¾ï¼ˆx2ï¼‰å’Œå“åº”ç‰¹å¾ï¼ˆx1ï¼‰**ï¼Œç¡®ä¿å…ƒæ•°æ®ä¹Ÿä¿æŒä¸€è‡´ã€‚

+   **å…ƒæ•°æ®**ç¼–ç ï¼Œå¦‚æ¯ä¸ªç‰¹å¾çš„å•ä½ã€æ ‡ç­¾å’Œæ˜¾ç¤ºèŒƒå›´ã€‚

+   **å‡å°‘æ•°æ®æ•°é‡**ä»¥æ–¹ä¾¿å¯è§†åŒ–ï¼ˆå¦‚æœå›¾è¡¨ä¸Šçš„ç‚¹å¤ªå¤šï¼Œåˆ™éš¾ä»¥çœ‹æ¸…ï¼‰ã€‚

+   **è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²**ä»¥æ¼”ç¤ºå’Œå¯è§†åŒ–ç®€å•çš„è¶…å‚æ•°è°ƒæ•´ã€‚

+   **å‘æ•°æ®æ·»åŠ éšæœºå™ªå£°**ä»¥æ¼”ç¤ºæ¨¡å‹è¿‡æ‹Ÿåˆã€‚åŸå§‹æ•°æ®æ— è¯¯å·®ï¼Œä¸æ˜“å±•ç¤ºè¿‡æ‹Ÿåˆã€‚

å¦‚æœè®¾ç½®æ­£ç¡®ï¼Œåº”è¯¥èƒ½å¤Ÿä½¿ç”¨ä»»ä½•æ•°æ®é›†å’Œç‰¹å¾è¿›è¡Œæ­¤æ¼”ç¤ºã€‚

+   ä¸ºäº†ç®€æ´ï¼Œæˆ‘ä»¬è¿™é‡Œæ²¡æœ‰å±•ç¤ºä»»ä½•ç‰¹å¾é€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œå‰ä¸€ç« ä¸­æåˆ°çš„ k-æœ€è¿‘é‚»ç®—æ³•åŒ…æ‹¬ä¸€äº›ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œä½†æ›´å¤šå¯èƒ½çš„æ–¹æ³•å’Œç‰¹å¾é€‰æ‹©ä»£ç è¯·å‚é˜…ç‰¹å¾é€‰æ‹©ç« èŠ‚ã€‚

## å¯é€‰ï¼šå‘å“åº”ç‰¹å¾æ·»åŠ éšæœºå™ªå£°

æˆ‘ä»¬å¯ä»¥é€šè¿‡è§‚å¯Ÿæ•°æ®å™ªå£°å¯¹è¿‡æ‹Ÿåˆå’Œè¶…å‚æ•°è°ƒæ•´çš„å½±å“æ¥åšè¿™ä»¶äº‹ã€‚

+   è¿™æ˜¯ä¸ºäº†ç»éªŒå­¦ä¹ ï¼Œå½“ç„¶æˆ‘ä»¬ä¸ä¼šå‘æ•°æ®ä¸­æ·»åŠ éšæœºå™ªå£°ã€‚

+   æˆ‘ä»¬è®¾ç½®äº†éšæœºæ•°ç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§

```py
add_error = True                                              # add random error to the response feature
std_error = 500                                               # standard deviation of random error, for demonstration only
idata = 2

if idata == 1:
    df_load = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv") # load the data from my github repo
    df_load = df_load.sample(frac=.30, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data

elif idata == 2:
    df_load = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv") # load the data 
    df_load = df_load.sample(frac=.70, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data
    df_load = df_load.rename(columns={"Prod": "Production"})

yname = 'Production'; Xname = ['Por','Brittle']               # specify the predictor features (x2) and response feature (x1)
Xmin = [5.0,0.0]; Xmax = [25.0,100.0]                         # set minimums and maximums for visualization 
ymin = 1000.0; ymax = 9000.0
Xlabel = ['Porosity','Brittleness']; ylabel = 'Production'    # specify the feature labels for plotting
Xunit = ['%','%']; yunit = 'MCFPD'
Xlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']
ylabelunit = ylabel + ' (' + yunit + ')'

if add_error == True:                                         # method to add error
    np.random.seed(seed=seed)                                 # set random number seed
    df_load[yname] = df_load[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df_load)) # add noise
    values = df_load._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray

y = pd.DataFrame(df_load[yname])                              # extract selected features as X and y DataFrames
X = df_load[Xname]
df = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) 
```

è®©æˆ‘ä»¬ç¡®ä¿æˆ‘ä»¬å·²ç»é€‰æ‹©äº†åˆç†çš„ç‰¹å¾æ¥æ„å»ºæ¨¡å‹ã€‚

+   ä¸¤ä¸ªé¢„æµ‹ç‰¹å¾ä¸å…±çº¿ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´é¢„æµ‹æ¨¡å‹ä¸ç¨³å®šã€‚

+   æ¯ä¸ªç‰¹å¾éƒ½ä¸å“åº”ç‰¹å¾ç›¸å…³ï¼Œé¢„æµ‹ç‰¹å¾å‘ŠçŸ¥å“åº”ã€‚

## è®¡ç®—ç›¸å…³çŸ©é˜µå’Œä¸å“åº”æ’åçš„ç›¸å…³æ€§

è®©æˆ‘ä»¬ä»ç›¸å…³æ€§åˆ†æå¼€å§‹ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¹‹å‰å£°æ˜çš„å‡½æ•°è®¡ç®—å¹¶æŸ¥çœ‹ç›¸å…³çŸ©é˜µä»¥åŠä¸å“åº”ç‰¹å¾çš„å…³è”ã€‚

+   ç›¸å…³æ€§åˆ†æåŸºäºçº¿æ€§å…³ç³»çš„å‡è®¾ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªè‰¯å¥½çš„å¼€å§‹

```py
corr_matrix = df.corr()
correlation = corr_matrix.iloc[:,-1].values[:-1]

plt.subplot(121)
plot_corr(corr_matrix,'Correlation Matrix',1.0,0.1)           # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(Xname,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + yname,'Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![_images/152d837d72f43ba9a48527a444d81b3bbc73a2ba553d2760d27f5c20206ea0b2.png](img/fe078f42023f81da1972474b1d3bbf26.png)

æ³¨æ„ç”±äºæ¯ä¸ªå˜é‡ä¸å…¶è‡ªèº«çš„ç›¸å…³æ€§è€Œäº§ç”Ÿçš„ 1.0 å¯¹è§’çº¿ã€‚

è¿™çœ‹èµ·æ¥ä¸é”™ã€‚å­˜åœ¨å¤šç§ç›¸å…³æ€§çš„å¤§å°ã€‚å½“ç„¶ï¼Œç›¸å…³ç³»æ•°ä»…é™äºçº¿æ€§ç›¸å…³æ€§çš„ç¨‹åº¦ã€‚

+   è®©æˆ‘ä»¬æŸ¥çœ‹çŸ©é˜µæ•£ç‚¹å›¾ï¼Œä»¥æŸ¥çœ‹ç‰¹å¾ä¹‹é—´çš„æˆå¯¹å…³ç³»ã€‚

```py
pairgrid = sns.PairGrid(df,vars=Xname+[yname])                # matrix scatter plots
pairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)
pairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle
pairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, 
                              alpha = 1.0, n_levels = 10)
pairgrid.add_legend()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/8d03fa748bcc76aea92c8e57b5fb8071473e84b910d1402f5fd62dd21b102145.png](img/515a70a53d49c49c9ecf98249cd67b5d.png)

## Train and Test Split

ä¸ºäº†æ–¹ä¾¿å’Œç®€å•ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn çš„éšæœºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²ã€‚

```py
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split
df_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)
df_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) 
```

## å¯è§†åŒ– DataFrame

åœ¨æˆ‘ä»¬æ„å»ºæ¨¡å‹ä¹‹å‰ï¼Œå¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯• DataFrame æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ©ç”¨ 'head' DataFrame æˆå‘˜å‡½æ•°æ¥é¢„è§ˆï¼ˆæ ¼å¼æ•´æ´ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚

```py
print('       Training DataFrame          Testing DataFrame')
display_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display 
```

```py
 Training DataFrame          Testing DataFrame 
```

|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| 86 | 12.83 | 29.87 | 2089.258307 |
| 35 | 17.39 | 56.43 | 5803.596379 |
| 75 | 12.23 | 40.67 | 3511.348151 |
| 36 | 13.72 | 40.24 | 4004.849870 |
| 126 | 12.83 | 17.20 | 2712.836372 |
|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| 5 | 15.55 | 58.25 | 5353.761093 |
| 46 | 20.21 | 23.78 | 4387.577571 |
| 96 | 15.07 | 39.39 | 4412.135054 |
| 45 | 12.10 | 63.24 | 3654.779704 |
| 105 | 19.54 | 37.40 | 5251.551624 |

## è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡

åœ¨ DataFrame ä¸­ï¼Œæœ‰è®¸å¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ã€‚

+   The describe command provides count, mean, minimum, maximum in a nice data table.

```py
print('            Training DataFrame                      Testing DataFrame')    # custom function for side-by-side summary statistics
display_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) 
```

```py
 Training DataFrame                      Testing DataFrame 
```

|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| count | 105.000000 | 105.000000 | 105.000000 |
| mean | 14.859238 | 48.861143 | 4238.554591 |
| std | 3.057228 | 14.432050 | 1087.707113 |
| min | 7.220000 | 10.940000 | 1517.373571 |
| max | 23.550000 | 84.330000 | 6907.632261 |
|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| count | 35.000000 | 35.000000 | 35.000000 |
| mean | 15.011714 | 46.798286 | 4378.913131 |
| std | 3.574467 | 13.380910 | 1290.216113 |
| min | 6.550000 | 20.120000 | 1846.027145 |
| max | 20.860000 | 68.760000 | 6593.447893 |

æˆ‘ä»¬æ£€æŸ¥äº†æ‘˜è¦ç»Ÿè®¡æ˜¯ä»¶å¥½äº‹ã€‚

+   æ²¡æœ‰æ˜æ˜¾çš„é”™è¯¯

+   æ£€æŸ¥æ¯ä¸ªç‰¹å¾çš„å€¼èŒƒå›´ï¼Œä»¥è®¾ç½®å’Œè°ƒæ•´ç»˜å›¾é™åˆ¶ã€‚è§ä¸Šæ–‡ã€‚

## å¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²

è®©æˆ‘ä»¬ä½¿ç”¨ç›´æ–¹å›¾å’Œæ•£ç‚¹å›¾æ¥æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„ä¸€è‡´æ€§å’Œè¦†ç›–ç‡ã€‚

+   æ£€æŸ¥ä»¥ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•è¦†ç›–äº†å¯èƒ½çš„ç‰¹å¾ç»„åˆèŒƒå›´

+   ç¡®ä¿æµ‹è¯•ç”¨ä¾‹ä¸ä¼šè¶…å‡ºè®­ç»ƒæ•°æ®èŒƒå›´è¿›è¡Œå¤–æ¨

```py
nbins = 20                                                    # number of histogram bins

plt.subplot(221)                                              # predictor feature #1 histogram
freq1,_,_ = plt.hist(x=df_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=df_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  
plt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   

plt.subplot(222)                                              # predictor feature #2 histogram
freq1,_,_ = plt.hist(x=df_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=df_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  
plt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   

plt.subplot(223)                                              # predictor features #1 and #2 scatter plot
plt.scatter(df_train[Xname[0]],df_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[Xname[0]],df_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.title(Xlabel[0] + ' vs ' +  Xlabel[1])
plt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])
plt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![å›¾ç‰‡](img/3e84676c9f7f37dcabed4194a238047f.png)

æœ‰æ—¶æˆ‘å‘ç°é€šè¿‡æŸ¥çœ‹ CDF è€Œä¸æ˜¯ç›´æ–¹å›¾æ¥æ¯”è¾ƒåˆ†å¸ƒæ›´æ–¹ä¾¿ã€‚

+   æˆ‘ä»¬é¿å…é€‰æ‹©ä»»æ„ç›´æ–¹å›¾åˆ†ç®±å¤§å°ï¼Œå› ä¸ºç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰ä¸æ•°æ®åˆ†è¾¨ç‡ä¸€è‡´ã€‚

```py
plt.subplot(221)                                              # predictor feature #1 CDF
plot_CDF(X_train[Xname[0]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(X_test[Xname[0]],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(Xlabelunit[0]); plt.xlim(Xmin[0],Xmax[0]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(Xlabel[0] + ' Train and Test CDFs')

plt.subplot(222)                                              # predictor feature #2 CDF
plot_CDF(X_train[Xname[1]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(X_test[Xname[1]],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(Xlabelunit[1]); plt.xlim(Xmin[1],Xmax[1]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(Xlabel[1] + ' Train and Test CDFs')

plt.subplot(223)                                              # response feature CDF
plot_CDF(y_train[yname],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(y_test[yname],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(ylabelunit); plt.xlim(ymin,ymax); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(ylabel + ' Train and Test CDFs')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![å›¾ç‰‡](img/89d70d18d70c546bb0ac8c55112e894e.png)

å†æ¬¡å¼ºè°ƒï¼Œåˆ†å¸ƒè¡¨ç°è‰¯å¥½ï¼Œ

+   æˆ‘ä»¬æ— æ³•è§‚å¯Ÿåˆ°æ˜æ˜¾çš„é—´éš™æˆ–æˆªæ–­ã€‚

+   æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„è¦†ç›–ç‡

è®©æˆ‘ä»¬çœ‹çœ‹å­”éš™ç‡ä¸è„†æ€§ä¹‹é—´çš„æ•£ç‚¹å›¾ï¼Œç‚¹æ ¹æ®äº§é‡ç€è‰²ã€‚

```py
plt.subplot(111)                                              # visualize the train and test data in predictor feature space
im = plt.scatter(X_train[Xname[0]],X_train[Xname[1]],s=None, c=y_train[yname], marker='o', cmap=cmap, 
    norm=None, vmin=ymin, vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors="black", label = 'Train')
plt.scatter(X_test[Xname[0]],X_test[Xname[1]],s=None, c=y_test[yname], marker='s', cmap=cmap, 
    norm=None, vmin=ymin, vmax=ymax, alpha=0.5, linewidths=0.3, edgecolors="black", label = 'Test')
plt.title('Training ' + ylabel + ' vs. ' + Xlabel[1] + ' and ' + Xlabel[0]); 
plt.xlabel(Xlabel[0] + ' (' + Xunit[0] + ')'); plt.ylabel(Xlabel[1] + ' (' + Xunit[1] + ')')
plt.xlim(Xmin[0],Xmax[0]); plt.ylim(Xmin[1],Xmax[1]); plt.legend(loc = 'upper right'); add_grid()
cbar = plt.colorbar(im, orientation = 'vertical')
cbar.set_label(ylabel + ' (' + yunit + ')', rotation=270, labelpad=20)
cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/3ec3d66453c69d41e8b4c7a2508530a0.png)

è¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥å¾ˆå¤æ‚ï¼Œæ— æ³•ç”¨ç®€å•çš„çº¿æ€§å›å½’å»ºæ¨¡ã€‚ä¼¼ä¹å­˜åœ¨éçº¿æ€§ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€å•çš„éå‚æ•°æ¨¡å‹ï¼Œå³å†³ç­–æ ‘ã€‚

## Instantiate, Fit and Predict with scikit-learn

è®©æˆ‘ä»¬é€šè¿‡å®ä¾‹åŒ–ã€æ‹Ÿåˆå’Œé¢„æµ‹æ¥æ„å»ºæˆ‘ä»¬çš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨ scikit-learnã€‚

+   **instantiate** æ¨¡å‹å¯¹è±¡ï¼Œä½¿ç”¨è¶…å‚æ•°ï¼Œk-æœ€è¿‘é‚»

+   **fit** é€šè¿‡ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ fit æˆå‘˜å‡½æ•°

+   **predict** ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚åœ¨ fit è¿è¡Œåï¼Œpredict å¯ç”¨äºè¿›è¡Œé¢„æµ‹

## è®­ç»ƒå†³ç­–æ ‘ï¼ˆå›å½’æ ‘ï¼‰

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½è¿è¡Œ DecisionTreeRegressor å‘½ä»¤æ¥æ„å»ºæˆ‘ä»¬çš„å›å½’æ ‘ï¼Œä»¥é¢„æµ‹æˆ‘ä»¬çš„å“åº”ç‰¹å¾ï¼Œç»™å®šæˆ‘ä»¬çš„ä¸¤ä¸ªé¢„æµ‹ç‰¹å¾ï¼ˆè®°ä½ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œé™åˆ¶è‡ªå·±ä½¿ç”¨ä¸¤ä¸ªé¢„æµ‹ç‰¹å¾ä»¥ç®€åŒ–å¯è§†åŒ–ï¼‰ã€‚

+   æˆ‘ä»¬å°†ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„ä¸¤ä¸ªå‡½æ•°æ¥å¯è§†åŒ–å†³ç­–æ ‘åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„é¢„æµ‹ï¼Œä»¥åŠè®­ç»ƒæ•°æ®çš„å®é™…äº§é‡å’Œä¼°è®¡äº§é‡çš„äº¤å‰å›¾ï¼Œä»¥åŠæ¥è‡ª sklearn.metrics æ¨¡å—çš„ä¸‰ä¸ªæ¨¡å‹åº¦é‡ã€‚

**è¶…å‚æ•°** - æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼çº¦æŸæ ‘å¤æ‚åº¦ï¼š

+   *max_leaf_nodes* - æœ€å¤§åŒºåŸŸæ•°ï¼Œä¹Ÿç§°ä¸ºå†³ç­–æ ‘ä¸­çš„ç»ˆç«¯æˆ–å¼•å¯¼èŠ‚ç‚¹

+   *max_depth* - æœ€å¤§å±‚æ•°ï¼Œä¾‹å¦‚ï¼Œmax_depth = 1 æ˜¯ä¸€ä¸ªåªæœ‰ 1 ä¸ªå†³ç­–å’Œä¸¤ä¸ªåŒºåŸŸçš„æ ‘æ¡©æ ‘

+   *min_samples_leaf* - æ–°åŒºåŸŸä¸­çš„æœ€å°æ•°æ®é‡ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„çº¦æŸæ¡ä»¶ï¼Œä»¥ç¡®ä¿æ¯ä¸ªåŒºåŸŸéƒ½æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥åšå‡ºåˆç†çš„ä¼°è®¡

ç›®å‰æˆ‘ä»¬åªå°è¯•ä¸€äº›è¶…å‚æ•°ã€‚

### æ¬ æ‹Ÿåˆå†³ç­–æ ‘æ¨¡å‹

è®©æˆ‘ä»¬ä½¿ç”¨å¤ªå°‘çš„åŒºåŸŸï¼Œè®¾ç½® max_leaf_nodes å¤ªå°ï¼Œçœ‹çœ‹ç»“æœå†³ç­–æ ‘æ¨¡å‹ã€‚

```py
max_leaf_nodes = 5; max_depth =99; min_samples_leaf = 1      # hyperparameters

tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes,max_depth = max_depth,min_samples_leaf = min_samples_leaf)
tree_model = tree_model.fit(X_train.values, y_train.values)

plt.subplot(121)                                              # visualize, data, and decision tree regions and predictions
visualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model',Xname,yname,Xlabelunit,ylabelunit) 

plt.subplot(122)                                              # cross validation with conditional statistics plot
check_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot',)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/be259c4cb62524490823c4f4d2d09c0c.png)

è¿™ä¸ªæ¨¡å‹éå¸¸æ¬ æ‹Ÿåˆï¼Œå®ƒå¤ªç®€å•äº†ï¼Œæ— æ³•æ‹Ÿåˆé¢„æµ‹é—®é¢˜çš„å½¢çŠ¶ã€‚ä»¥ä¸‹æ˜¯å…³äºå›¾è¡¨çš„ä¸€äº›æ›´å¤šä¿¡æ¯ã€‚

çœ‹çœ‹ä¼°è®¡ç”Ÿäº§ä¸å®é™…ç”Ÿäº§ï¼ˆåº•éƒ¨å›¾è¡¨ï¼‰çš„å›¾ä¸­æ°´å¹³çº¿ï¼Ÿ

+   è¿™æ˜¯å¯ä»¥é¢„æ–™çš„ï¼Œå› ä¸ºå›å½’æ ‘ä½¿ç”¨ç‰¹å¾ç©ºé—´æ¯ä¸ªåŒºåŸŸï¼ˆç»ˆç«¯èŠ‚ç‚¹ï¼‰ä¸­çš„æ•°æ®å¹³å‡å€¼è¿›è¡Œä¼°è®¡ã€‚

+   ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘åŒ…æ‹¬äº†æ¯ä¸ªç»ˆç«¯èŠ‚ç‚¹ã€åŒºåŸŸåœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­çš„å®é™…å“åº” P10ã€å¹³å‡å€¼å’Œ P90ã€‚

+   ä½æ‹Ÿåˆé¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­éƒ½æœ‰è¾ƒå·®çš„å‡†ç¡®åº¦ã€‚

å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæ›´å¤æ‚çš„æ ‘ï¼Œæœ‰æ›´å¤šçš„ç»ˆç«¯èŠ‚ç‚¹ï¼Œé‚£ä¹ˆå°±ä¼šæœ‰æ›´å¤šçš„çº¿æ¡ã€‚

### è¿‡æ‹Ÿåˆå†³ç­–æ ‘æ¨¡å‹ã€‚

è®©æˆ‘ä»¬ä½¿ç”¨å¤ªå¤šçš„åŒºåŸŸï¼Œè®¾ç½® max_leaf_nodes å¤ªå¤§ï¼Œçœ‹çœ‹ç»“æœå†³ç­–æ ‘æ¨¡å‹ã€‚

```py
max_leaf_nodes = 50; max_depth = 9; min_samples_leaf = 1     # hyperparameters

tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes,max_depth = max_depth,min_samples_leaf = min_samples_leaf)
tree_model = tree_model.fit(X_train.values, y_train.values)

plt.subplot(121)                                              # visualize, data, and decision tree regions and predictions
visualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model',Xname,yname,Xlabelunit,ylabelunit) 

plt.subplot(122)                                              # cross validation with conditional statistics plot
check_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot',)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/c28519e97623d45a6f72540a6b408c6d.png)

ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªè¿‡æ‹Ÿåˆçš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

+   è¿‡å¤šçš„å¤æ‚æ€§å’Œçµæ´»æ€§ã€‚

+   æˆ‘ä»¬æ­£åœ¨æ‹Ÿåˆæ•°æ®ä¸­çš„å™ªå£°ã€‚

+   è®­ç»ƒæ—¶å‡†ç¡®åº¦å¥½ï¼Œä½†æµ‹è¯•æ—¶å‡†ç¡®åº¦å·®ã€‚

éšç€æˆ‘ä»¬é€æ­¥æ·»åŠ ç»ˆç«¯èŠ‚ç‚¹ï¼Œè§‚å¯Ÿå†³ç­–æ ‘æ¨¡å‹åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„è¡¨ç°æ˜¯æœ‰æ•™è‚²æ„ä¹‰çš„ã€‚æˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°å›¾å½¢åŒ–åœ°è§‚å¯Ÿåˆ°åˆ†å±‚äºŒåˆ†åˆ†è£‚ã€‚

+   è®©æˆ‘ä»¬ä»ç®€å•çš„å¤æ‚æ¨¡å‹å¼€å§‹å¯è§†åŒ–ã€‚

```py
leaf_nodes_list = [2,3,4,10,20,100]

for inode,leaf_nodes in enumerate(leaf_nodes_list):

    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes)
    tree_model = tree_model.fit(X_train.values, y_train.values)

    plt.subplot(3,2,inode+1)                                         # visualize, data, and decision tree regions and predictions
    visualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],1000,9000,'Decision Tree Model, Number of Leaf Nodes: ' + str(leaf_nodes),Xname,yname,Xlabelunit,ylabelunit,annotate=False)   

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=3.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/43008585fa66e722cde17f42284274b4.png)

å¯èƒ½ä¼šæœ‰ç”¨çš„æ˜¯å¹¶æ’æŸ¥çœ‹å†³ç­–æ ‘æ¨¡å‹å’Œç›¸å…³çš„å†³ç­–æ ‘ã€‚

```py
leaf_nodes_viz = 2

tree_model_viz = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes_viz).fit(X_train.values, y_train.values)

fig = plt.figure(figsize=(10, 6))
gs = fig.add_gridspec(1, 2, width_ratios=[1, 2])  # 1 row, 3 columns with 1:2 width ratio

ax1 = fig.add_subplot(gs[0])                         # visualize, data, and decision tree regions and predictions 
visualize_tree_model(tree_model_viz,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],1000,9000,'Decision Tree Model, Number of Leaf Nodes: ' + str(leaf_nodes),Xname,yname,
        Xlabelunit,ylabelunit,annotate=False)   

ax2 = fig.add_subplot(gs[1:])                                  # visualize, data, and decision tree regions and predictions
_ = tree.plot_tree(tree_model_viz,ax = ax2,feature_names=list(Xname),class_names=list(yname),filled=False,label='none',rounded=True,precision=0,
                  proportion=True,max_depth=4,fontsize=15)

plt.tight_layout()
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/de575bff5cbcf3e18e0bc1c385400e3e.png)

æˆ‘ä»¬å¦‚ä½•æ‰¾åˆ°æœ€ä½³è¶…å‚æ•°ï¼Œä»¥å®ç°æœ€ä½³å¤æ‚æ€§å’Œæµ‹è¯•é¢„æµ‹ç²¾åº¦çš„æœ€ä½³åŒ–ï¼Ÿè¿™å°±æ˜¯è¶…å‚æ•°è°ƒæ•´ã€‚

## è°ƒæ•´å†³ç­–æ ‘ï¼ˆå›å½’æ ‘ï¼‰ã€‚

è®©æˆ‘ä»¬è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ï¼Œ

1.  çœ‹çœ‹å¯èƒ½çš„è¶…å‚æ•°å€¼èŒƒå›´ã€‚

1.  éå†å¯èƒ½è¶…å‚æ•°å€¼çš„èŒƒå›´ã€‚

    +   ä½¿ç”¨å½“å‰è¶…å‚æ•°å€¼åœ¨è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒã€‚

    +   åœ¨æµ‹è¯•æ•°æ®ä¸Šé¢„æµ‹ã€‚

    +   æ€»ç»“æ‰€æœ‰æµ‹è¯•æ•°æ®çš„é”™è¯¯ã€‚

1.  é€‰æ‹©æµ‹è¯•æ•°æ®é›†ä¸Šæœ€å°åŒ–è¯¯å·®çš„è¶…å‚æ•°ã€‚

å½“æˆ‘æŠŠè¿™ä¸ªæ•™ç»™æˆ‘çš„å­¦ç”Ÿæ—¶ï¼Œæˆ‘å»ºè®®è¿™æ˜¯ä¸€ä¸ªæ¨¡å‹å½©æ’ã€‚æˆ‘ä»¬é€šè¿‡ä¸ºæœªç”¨äºè®­ç»ƒæ¨¡å‹çš„æ¡ˆä¾‹åšå‡ºé¢„æµ‹æ¥å¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬å¸Œæœ›æ¨¡å‹åœ¨æœªè®­ç»ƒçš„æ¡ˆä¾‹ä¸Šè¡¨ç°æœ€å¥½ï¼Œå› æ­¤æˆ‘ä»¬æ­£åœ¨æ¨¡æ‹Ÿæ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­çš„ä½¿ç”¨ï¼

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ‰‹åŠ¨è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ï¼Œé€šè¿‡æ”¹å˜å†³ç­–æ ‘å¤æ‚æ€§ï¼Œæ‰¾åˆ°æœ€å°åŒ–æµ‹è¯•ä¸­å‡æ–¹è¯¯å·®çš„å¤æ‚æ€§ã€‚

+   ä¸ºäº†ç®€å•èµ·è§ï¼Œä¸‹é¢çš„ä»£ç åªéå†æœ€å¤§å¶èŠ‚ç‚¹è¶…å‚æ•°ã€‚

+   æˆ‘ä»¬å°†æœ€å°æ ·æœ¬æ•°è®¾ç½®ä¸º 1ï¼Œæœ€å¤§æ·±åº¦è®¾ç½®ä¸º 9ï¼Œä»¥ç¡®ä¿è¿™äº›è¶…å‚æ•°ä¸ä¼šäº§ç”Ÿä»»ä½•å½±å“ï¼ˆæˆ‘ä»¬å°†å®ƒä»¬è®¾ç½®å¾—éå¸¸å¤æ‚ï¼Œè¿™æ ·å°±ä¸ä¼šé™åˆ¶æ¨¡å‹å¤æ‚æ€§ï¼‰ã€‚

```py
trees = []; MSE_CV = []; node_CV = []

inode = 2
while inode < len(X_train):                                   # loop over the hyperparameter, train with training and test with testing
    tree_model = tree.DecisionTreeRegressor(min_samples_leaf=1,max_leaf_nodes=inode).fit(X_train.values, y_train.values)
    trees.append(tree_model)
    predict_train = tree_model.predict(np.c_[X_test[Xname[0]],X_test[Xname[1]]]) 
    MSE_CV.append(metrics.mean_squared_error(y_test[yname],predict_train))   
    all_nodes = tree_model.tree_.node_count             
    decision_nodes = len([x for x in tree_model.tree_.feature if x != _tree.TREE_UNDEFINED]); terminal_nodes = all_nodes - decision_nodes
    node_CV.append(terminal_nodes); inode+=1

plt.subplot(111)
plt.scatter(node_CV,MSE_CV,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,linewidths=0.3,
            edgecolors="black",zorder=20)
tuned_node = node_CV[np.argmin(MSE_CV)]; max_MSE_CV = np.max(MSE_CV)
plt.vlines(tuned_node,0,1.05*max_MSE_CV,lw=1.0,ls='--',color='red',zorder=10)
plt.annotate('Tuned Max Nodes = ' + str(tuned_node),(tuned_node-2,3.5e5),rotation=90,zorder=30)
plt.title('Decision Tree Cross Validation Testing Error vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); plt.ylabel('Mean Square Error')
plt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_CV); add_grid()
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.6, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/f43c9ad2b759692c2b199e397c735352.png)

é€šè¿‡è§‚å¯Ÿå‡†ç¡®ç‡ä¸å¤æ‚åº¦ä¹‹é—´çš„å…³ç³»ï¼Œè¯„ä¼°æˆ‘ä»¬æ ‘çš„è¡¨ç°æ˜¯æœ‰ç”¨çš„ï¼Œæœ€å°å€¼æ˜¯ç”±äºæ¨¡å‹æ–¹å·®å’Œæ¨¡å‹åå·®ä¹‹é—´çš„æƒè¡¡ã€‚

ä¸ºäº†å¾—åˆ°æ›´ç¨³å¥çš„ç»“æœï¼Œè®©æˆ‘ä»¬å°è¯• k æŠ˜äº¤å‰éªŒè¯ã€‚sklearn æœ‰ä¸€ä¸ªå†…ç½®çš„äº¤å‰éªŒè¯æ–¹æ³•ï¼Œåä¸º cross_val_scoreï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥ï¼š

1.  åº”ç”¨ k æŠ˜æ³•ï¼Œé€šè¿‡è¿­ä»£åˆ†ç¦»è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ã€‚

1.  å½“ k=5 æ—¶ï¼Œæ¯ä¸ªæŠ˜ä¿ç•™ 20% çš„æ•°æ®ç”¨äºæµ‹è¯•ã€‚

1.  è‡ªåŠ¨åŒ–æ¨¡å‹æ„å»ºï¼Œå¾ªç¯éå†æŠ˜ï¼Œå¹¶å¹³å‡æ„Ÿå…´è¶£çš„æŒ‡æ ‡ã€‚

è®©æˆ‘ä»¬åœ¨å…·æœ‰ä¸åŒç»ˆç«¯èŠ‚ç‚¹æ•°é‡çš„æ ‘ä¸Šå°è¯•ä¸€ä¸‹ã€‚æ³¨æ„ï¼Œäº¤å‰éªŒè¯è®¾ç½®ä¸ºä½¿ç”¨ 4 ä¸ªå¤„ç†å™¨ï¼Œä½†ä»å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ‰èƒ½è¿è¡Œã€‚

```py
MSE_kF = []; node_kF = []                                     # k-fold iteration code modified from StackOverFlow by Dimosthenis

inode = 2
while inode < len(X_train):
    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=inode).fit(X_train.values, y_train.values)
    scores = cross_val_score(estimator=tree_model, X= np.c_[df[Xname[0]],df[Xname[1]]],y=df[yname], cv=5, n_jobs=4,
        scoring = "neg_mean_squared_error")                   # perform 4-fold cross validation
    MSE_kF.append(abs(scores.mean()))
    all_nodes = tree_model.tree_.node_count   
    decision_nodes = len([x for x in tree_model.tree_.feature if x != _tree.TREE_UNDEFINED]); terminal_nodes = all_nodes - decision_nodes
    node_kF.append(terminal_nodes); inode+=1

tuned_node_kF = node_kF[np.argmin(MSE_kF)]; max_MSE_kF = np.max(MSE_kF)  
plt.subplot(111)
plt.vlines(tuned_node_kF,0,1.05*max_MSE_kF,lw=1.0,ls='--',color='red',zorder=10)
plt.annotate('Tuned Max Nodes = ' + str(tuned_node_kF),(tuned_node_kF-2,3.5e5),rotation=90,zorder=30)
plt.scatter(node_kF,MSE_kF,s=None,c="red",marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,
            linewidths=0.5, edgecolors="black",zorder=40,label='k-Fold')
plt.scatter(node_CV,MSE_CV,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.4,linewidths=0.3,
            edgecolors="black",zorder=20,label='Cross Validation')
plt.title('Decision Tree k-Fold Cross Validation Error (MSE) vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); 
plt.ylabel('Mean Square Error'); plt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_kF); add_grid()
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
plt.legend(loc='upper right')
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.6, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/2df3cfa16f6fb36c7bd2a52fe7d20a6d.png)

k æŠ˜äº¤å‰éªŒè¯æä¾›äº† MSE ä¸è¶…å‚æ•°ä¹‹é—´çš„æ›´å¹³æ»‘çš„å›¾è¡¨ã€‚

+   é€šè¿‡å¯¹æ‰€æœ‰æŠ˜çš„å¹³å‡ MSE æ¥å‡å°‘è¯¥æŒ‡æ ‡å¯¹ç‰¹å®šè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†é…çš„æ•æ„Ÿæ€§ã€‚

+   æˆ‘ä»¬è¿›è¡Œçš„æ‰€æœ‰è®­ç»ƒå’Œæµ‹è¯•äº¤å‰éªŒè¯æˆ– k æŠ˜äº¤å‰éªŒè¯éƒ½æ˜¯ä¸ºäº†å¾—åˆ°è¿™ä¸ªå•ä¸€å€¼ï¼Œå³æ¨¡å‹çš„**è¶…å‚æ•°**ã€‚

## æ„å»ºæœ€ç»ˆæ¨¡å‹ã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç”¨è¿™ä¸ªè¶…å‚æ•°åœ¨æ‰€æœ‰æ•°æ®ä¸Šè®­ç»ƒï¼Œè¿™æ˜¯æˆ‘ä»¬**æœ€ç»ˆæ¨¡å‹**ã€‚

```py
pruned_tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=tuned_node_kF)
pruned_tree_model = pruned_tree_model.fit(X, y)               # re-train

plt.subplot(121)                                              # visualize, data, and decision tree regions and predictions
visualize_tree_model(pruned_tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model, Tuned Leaf Nodes: ' + str(tuned_node_kF),Xname,yname,
                    Xlabelunit,ylabelunit) # plots the data points and the decision tree prediction 

plt.subplot(122)                                              # cross validation with conditional statistics plot
check_tree_model(pruned_tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot, Tuned Leaf Nodes: ' + 
                    str(tuned_node_kF),)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/608907e2a0d015a0d611204bfa6c41e5.png)

æˆ‘ä»¬å·²ç»å®Œæˆäº†æˆ‘ä»¬çš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ç°åœ¨è®©æˆ‘ä»¬å†è®¨è®ºä¸€äº›å†³ç­–æ ‘çš„è¯Šæ–­ã€‚

## æŸ¥è¯¢å†³ç­–æ ‘ã€‚

è¯„ä¼°ä»»ä½•å¯èƒ½çš„ç‰¹å¾ç»„åˆï¼Œä»¥åŠå¯¼è‡´ç‰¹å®šé¢„æµ‹çš„å†³ç­–èŠ‚ç‚¹é¡ºåºå¯èƒ½ä¹Ÿå¾ˆæœ‰ç”¨ã€‚ä»¥ä¸‹å‡½æ•°æä¾›äº†é¢„æµ‹æ¡ˆä¾‹é€šè¿‡çš„èŠ‚ç‚¹åˆ—è¡¨ã€‚

```py
x1 = 7.0; x2 = 10.0                                          # the predictor feature values for the decision path

decision_path = pruned_tree_model.decision_path(np.c_[x1,x2])
print(decision_path) 
```

```py
 (0, 0)	1
  (0, 1)	1
  (0, 3)	1
  (0, 13)	1 
```

## æå–å†³ç­–æ ‘é¢„æµ‹æ¨¡å‹ä½œä¸ºå‡½æ•°ã€‚

æ­¤å¤–ï¼Œå°†å†³ç­–æ ‘è½¬æ¢ä¸ºä»£ç ï¼Œå³åµŒå¥—çš„â€œifâ€è¯­å¥é›†ï¼Œå¯èƒ½ä¹Ÿå¾ˆæœ‰ç”¨ã€‚

+   è¿™åˆ›å»ºäº†ä¸€ä¸ªå¯ç§»æ¤çš„æ¨¡å‹ï¼Œå¯ä»¥å¤åˆ¶å¹¶ä½œä¸ºç‹¬ç«‹å‡½æ•°åº”ç”¨ã€‚

æ­¤å¤–ï¼Œè¿˜å¯ä»¥æ–¹ä¾¿åœ°æŸ¥è¯¢æ ‘çš„ä»£ç ç‰ˆæœ¬ã€‚

+   æˆ‘ä»¬ä½¿ç”¨å…ˆå‰å®šä¹‰çš„å‡½æ•°æ¥å¤„ç†æˆ‘ä»¬çš„å‰ªææ ‘ã€‚

```py
tree_to_code(pruned_tree_model, list(Xname))                  # convert a decision tree to Python code, nested if statements 
```

```py
def tree(Por, Brittle):
  if Por <= 14.789999961853027:
    if Por <= 12.425000190734863:
      if Por <= 8.335000038146973:
        return [[1879.19091537]]
      elif Por > 8.335000038146973
        if Brittle <= 39.125:
          return [[2551.00021508]]
        elif Brittle > 39.125
          return [[3369.12903299]]
    elif Por > 12.425000190734863
      if Brittle <= 39.26500129699707:
        return [[3160.11022857]]
      elif Brittle > 39.26500129699707
        return [[4154.18334527]]
  elif Por > 14.789999961853027
    if Por <= 18.015000343322754:
      if Brittle <= 33.25:
        return [[3883.19381758]]
      elif Brittle > 33.25
        if Por <= 16.434999465942383:
          return [[4544.69777089]]
        elif Por > 16.434999465942383
          return [[5240.84146117]]
    elif Por > 18.015000343322754
      if Brittle <= 31.5600004196167:
        return [[4353.11874206]]
      elif Brittle > 31.5600004196167
        return [[5868.56369869]] 
```

## åŸºäºå†³ç­–æ ‘çš„ç‰¹æ€§é‡è¦æ€§ã€‚

ç‰¹æ€§é‡è¦æ€§æ˜¯é€šè¿‡å†³ç­–æ ‘è®¡ç®—å¾—å‡ºçš„ï¼Œé€šè¿‡æ€»ç»“åŒ…å«æ¯ä¸ªç‰¹å¾æ—¶çš„å¹³å‡å¹³æ–¹è¯¯å·®æ¥è®¡ç®—ï¼Œå¹¶æ€»ç»“å¦‚ä¸‹ï¼š

$$ FI(x) = \sum_{t \in T_f} \frac{N_t}{N} \Delta_{MSE_t} $$

$T_f$ ä»£è¡¨æ‰€æœ‰ä»¥ç‰¹å¾ $x$ ä½œä¸ºåˆ†å‰²ç‚¹çš„èŠ‚ç‚¹ï¼Œ$N_t$ æ˜¯è¾¾åˆ°èŠ‚ç‚¹ $t$ çš„è®­ç»ƒæ ·æœ¬æ•°é‡ï¼Œ$N$ æ˜¯æ•°æ®é›†ä¸­æ ·æœ¬çš„æ€»æ•°ï¼Œ$\Delta_{MSE_t}$ æ˜¯ $t$ åˆ†å‰²ç‚¹å¤„ MSE çš„å‡å°‘é‡ã€‚

æ³¨æ„ï¼Œç‰¹å¾é‡è¦æ€§å¯ä»¥åƒä¸Šé¢çš„ MSE ä¸€æ ·è®¡ç®—ï¼Œé€‚ç”¨äºå…·æœ‰**åŸºå°¼ä¸çº¯åº¦**çš„åˆ†ç±»æ ‘ã€‚

```py
plt.subplot(111)                                              # plot the feature importance 
plt.title("Decision Tree Feature Importance")
plt.bar(Xlabel, pruned_tree_model.feature_importances_,edgecolor = 'black',
       color="darkorange",alpha = 0.6, align="center")
plt.xlim([-0.5,len(Xname)-0.5]); plt.ylim([0.,1.0])
plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')
plt.subplots_adjust(left=0.0, bottom=0.0, right=1., top=0.8, wspace=0.2, hspace=0.5); plt.show() 
```

![å›¾ç‰‡](img/e7672512b746dfc82482f39bc8c78ddc.png)

## å¯è§†åŒ–æ¨¡å‹ã€‚

è®©æˆ‘ä»¬æœ€åçœ‹çœ‹æˆ‘ä»¬ä¿®å‰ªåçš„æ ‘çš„å›¾å½¢è¡¨ç¤ºã€‚

```py
fig = plt.figure(figsize=(15,10))

_ = tree.plot_tree(pruned_tree_model,                         # plot the decision tree for model visualization
                   feature_names=list(Xname),  
                   class_names=list(yname),
                   filled=True) 
```

![å›¾ç‰‡](img/dceac0eb1f800b2c21d5e490c1248210.png)

## ç®€å•ä»£ç åˆ¶ä½œå†³ç­–æ ‘æœºå™¨å¹¶è®¡ç®—é¢„æµ‹

ä¸ºäº†æ”¯æŒé‚£äº›åˆšå¼€å§‹çš„äººï¼Œä»¥ä¸‹æ˜¯ä¸€å°æ®µä»£ç æ¥ï¼š

+   åŠ è½½ç”¨äºå†³ç­–æ ‘çš„ scikit-learn åŒ…

+   åŠ è½½æ•°æ®

+   å®ä¾‹åŒ–ä¸€ä¸ªå…·æœ‰è¶…å‚æ•°çš„å†³ç­–æ ‘ï¼ˆæœªæ˜¾ç¤ºè°ƒæ•´ï¼‰

+   ä½¿ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒå†³ç­–æ ‘

+   ä½¿ç”¨å†³ç­–æ ‘è¿›è¡Œé¢„æµ‹

```py
from sklearn import tree                                      # import decision tree from scikit-learn
Xname = ['Por','Brittle']; yname='Production'                 # predictor features and response feature
x1 = 0.25; x2 = 0.3                                           # predictor values for the prediction
my_data = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv") # load subsurface data table
my_tree = tree.DecisionTreeRegressor(max_leaf_nodes=26)       # instantiate tree with hyperparameters
my_tree = my_tree.fit(X.values,y.values)                      # train tree with training data
estimate = my_tree.predict([[x1,x2]])[0]                      # make a prediction (no tuning shown)
print('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + str(round(estimate,1)) + ' ' + yunit) # print results 
```

```py
Estimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1879.2 MCFPD 
```

## æ¸…æ´ã€ç´§å‡‘çš„æœºå™¨å­¦ä¹ ä»£ç çš„æœºå™¨å­¦ä¹ ç®¡é“

ç®¡é“æ˜¯ scikit-learn ç±»ï¼Œå…è®¸å°è£…ä¸€ç³»åˆ—æ•°æ®å‡†å¤‡å’Œå»ºæ¨¡æ­¥éª¤

+   ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†ç®¡é“è§†ä¸ºæˆ‘ä»¬é«˜åº¦æµ“ç¼©çš„å·¥ä½œæµç¨‹ä¸­çš„ä¸€ä¸ªå¯¹è±¡

ç®¡é“ç±»å…è®¸æˆ‘ä»¬ï¼š

+   æé«˜ä»£ç å¯è¯»æ€§å¹¶ä¿æŒä¸€åˆ‡äº•ç„¶æœ‰åº

+   ç”¨å¾ˆå°‘çš„ä»£ç è¡Œæ„å»ºå®Œæ•´çš„æµç¨‹

+   é¿å…å¸¸è§çš„æµç¨‹é—®é¢˜ï¼Œå¦‚æ•°æ®æ³„éœ²ã€æµ‹è¯•æ•°æ®å‘ŠçŸ¥æ¨¡å‹å‚æ•°è®­ç»ƒ

+   æŠ½è±¡å¸¸è§çš„æœºå™¨å­¦ä¹ å»ºæ¨¡ï¼Œä¸“æ³¨äºæ„å»ºå°½å¯èƒ½å¥½çš„æ¨¡å‹

åŸºæœ¬å“²å­¦æ˜¯å°†æœºå™¨å­¦ä¹ è§†ä¸ºç»„åˆæœç´¢ä»¥æ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼ˆAutoMLï¼‰

æ›´å¤šä¿¡æ¯è¯·å‚é˜…æˆ‘å…³äº [æœºå™¨å­¦ä¹ ç®¡é“](https://www.youtube.com/watch?v=tYrPs8s1l9U&list=PLG19vXLQHvSAufDFgZEFAYQEwMJXklnQV&index=5) çš„å½•éŸ³è®²åº§å’Œè¯¦ç»†è®°å½•çš„æ¼”ç¤º [æœºå™¨å­¦ä¹ ç®¡é“å·¥ä½œæµç¨‹](http://localhost:8892/notebooks/OneDrive%20-%20The%20University%20of%20Texas%20at%20Austin/Courses/Workflows/PythonDataBasics_Pipelines.ipynb)ã€‚

```py
pipe_tree = Pipeline([                                        # the machine learning workflow as a pipeline object

    ('tree', tree.DecisionTreeRegressor())
])

params = {                                                    # the machine learning workflow method's parameters to search
    'tree__max_leaf_nodes': np.arange(2,len(X),1,dtype = int),
}

KF_tuned_tree = GridSearchCV(pipe_tree,params,scoring = 'neg_mean_squared_error', # hyperparameter tuning w. grid search k-fold cross validation 
                             cv=KFold(n_splits=5,shuffle=False),refit = True)
KF_tuned_tree.fit(X,y)                                        # tune and train the model

print('Tuned hyperparameter: max_leaf_nodes = ' + str(KF_tuned_tree.best_params_))

estimate = KF_tuned_tree.predict([[x1,x2]])[0]                # make a prediction (no tuning shown)
print('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + str(round(estimate,1)) + ' ' + yunit) # print results 
```

```py
Tuned hyperparameter: max_leaf_nodes = {'tree__max_leaf_nodes': 10}
Estimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1879.2 MCFPD 
```

## åœ¨æ–°æ•°æ®é›†ä¸Šè¿›è¡Œå®è·µ

å¥½çš„ï¼Œæ˜¯æ—¶å€™å¼€å§‹å·¥ä½œäº†ã€‚è®©æˆ‘ä»¬åŠ è½½æ•°æ®é›†å¹¶ä½¿ç”¨ä»¥ä¸‹å†…å®¹æ„å»ºå†³ç­–æ ‘é¢„æµ‹æ¨¡å‹ï¼Œ

+   ç´§å‡‘çš„ä»£ç 

+   åŸºæœ¬å¯è§†åŒ–

+   ä¿å­˜è¾“å‡º

æ‚¨å¯ä»¥é€‰æ‹©è¿™äº›æ•°æ®é›†ä¹‹ä¸€æˆ–ä¿®æ”¹ä»£ç å¹¶æ·»åŠ æ‚¨è‡ªå·±çš„æ•°æ®é›†æ¥æ‰§è¡Œæ­¤æ“ä½œã€‚

### æ•°æ®é›† 0ï¼Œéå¸¸è§„å¤šå…ƒå˜é‡ v4

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒæ•°æ®é›† [unconv_MV.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v4.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   äº•å¹³å‡å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   å²©è„†æ€§æ¯” (%)

+   æ€»æœ‰æœºç¢³ (%)

+   ç»ç’ƒå…‰æ³½åå°„ç‡ (%)

+   åˆå§‹ç”Ÿäº§ 90 å¤©å¹³å‡ (MCFPD)ã€‚

### æ•°æ®é›† 2ï¼Œå‚¨å±‚ 21

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒï¼Œ3D ç©ºé—´æ•°æ®é›† [res21_wells.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/res21_wells.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 73 ä¸ªå‚ç›´äº•åœ¨ 10,000m x 10,000m x 50 m å‚¨å±‚å•å…ƒçš„å˜é‡ï¼š

+   äº•ï¼ˆIDï¼‰

+   X (m), Y (m), æ·±åº¦ (m) ä½ç½®åæ ‡

+   å•ä½è½¬æ¢åçš„å­”éš™ç‡ (%)

+   æ¸—é€ç‡ (mD)

+   å£°é˜»æŠ—ï¼ˆkg/m2s*10â¶ï¼‰å•ä½è½¬æ¢å

+   å²©æ€§ï¼ˆåˆ†ç±»ï¼‰ - æœ‰åºçš„ï¼Œä»é¡µå²©ã€ç ‚è´¨é¡µå²©ã€é¡µå²©ç ‚åˆ°ç ‚å²©ã€‚

+   å¯†åº¦ (g/cmÂ³)

+   å¯å‹ç¼©é€Ÿåº¦ï¼ˆm/sï¼‰

+   Youngs æ¨¡é‡ï¼ˆGPaï¼‰

+   å‰ªåˆ‡é€Ÿåº¦ï¼ˆm/sï¼‰

+   å‰ªåˆ‡æ¨¡é‡ï¼ˆGPaï¼‰

+   3 å¹´ç´¯è®¡æ²¹äº§é‡ï¼ˆç™¾ä¸‡æ¡¶ï¼‰

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†è¡¨æ ¼æ•°æ®åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸ºâ€˜my_dataâ€™çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

+   æˆ‘ä»¬è¿˜ç”¨æ•°æ®èŒƒå›´å’Œæ ‡ç­¾å¡«å……åˆ—è¡¨ï¼Œä»¥ä¾¿äºç»˜å›¾

åŠ è½½æ•°æ®å¹¶æ ¼å¼åŒ–ï¼Œ

+   åˆ é™¤å“åº”ç‰¹å¾

+   æ ¹æ®éœ€è¦é‡æ–°æ ¼å¼åŒ–ç‰¹å¾

+   æ­¤å¤–ï¼Œæˆ‘è¿˜å–œæ¬¢å°†å…ƒæ•°æ®å­˜å‚¨åœ¨åˆ—è¡¨ä¸­

```py
idata = 2                                                    # select the dataset

if idata == 0:
    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['Well'],axis=1,inplace=True)                 # remove well index and response feature

    features = df_new.columns.values.tolist()                 # store the names of the features

    xname = features[:-1]
    yname = [features[-1]]

    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting
    ymin_new = 0.0; ymax_new = 10000.0
    xlabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Brittleness Ratio (%)', # set the names for plotting
             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']

    ylabel_new = 'Production (MCFPD)'

    xtitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting
             'Total Organic Carbon','Vitrinite Reflectance']

    ytitle_new = 'Production'

    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames 
    X = df_new[xname]

elif idata == 1:
    names = {'Porosity':'Por'}

    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['X','Y','Unnamed: 0','Facies'],axis=1,inplace=True)   # remove response feature
    df_new = df_new.rename(columns=names)
    df_new['Por'] = df_new['Por'] * 100.0; df_new['AI'] = df_new['AI'] / 1000.0
    features = df_new.columns.values.tolist()                 # store the names of the features

    xname = features[:-1]
    yname = [features[-1]]

    xmin_new = [4.0,0.0]; xmax_new = [19.0,500.0] # set the minimum and maximum values for plotting

    ymin_new = 1.60; ymax_new = 6.20

    xlabel_new = ['Porosity (fraction)','Permeability (mD)'] # set the names for plotting

    ylabel_new = 'Acoustic Impedance (kg/m2s*10â¶)'

    xtitle_new = ['Porosity','Permeability']

    ytitle_new = 'Acoustic Impedance (kg/m2s*10â¶)'

    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames 
    X = df_new[xname]

elif idata == 2:  
    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['Well_ID','X','Y'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature
    df_new = df_new.dropna(how='any',inplace=False)

    features = df_new.columns.values.tolist()                 # store the names of the features

    xname = features[:-1]
    yname = [features[-1]]

    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting

    ymin_new = 0.0; ymax_new = 1600.0

    xlabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Facies (categorical)',
              'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting

    ylabel_new = 'Production (Mbbl)'

    xtitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',
              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']

    ytitle_new = 'Production'

    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames 
    X = df_new[xname]

df_new.head(n=13) 
```

|  | Por | Perm | AI | å¯†åº¦ | PVel | Youngs | SVel | å‰ªåˆ‡ | ç´¯è®¡æ²¹é‡ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | 12.907730 | 133.910637 | 7.308846 | 2.146360 | 3563.549461 | 25.688560 | 1673.770439 | 6.429229 | 1201.20 |
| 7 | 12.647965 | 114.359667 | 7.343836 | 2.188597 | 3570.094553 | 25.444064 | 1670.043495 | 6.100984 | 683.92 |
| 10 | 12.998469 | 129.332122 | 7.282051 | 2.131121 | 3524.448615 | 25.985734 | 1681.960101 | 6.203527 | 978.14 |
| 15 | 12.426141 | 123.227677 | 7.351795 | 2.203026 | 3417.596818 | 25.976462 | 1675.355860 | 6.288040 | 608.09 |
| 16 | 13.507371 | 147.562087 | 7.300360 | 2.210916 | 3476.167397 | 24.817767 | 1656.890690 | 6.222528 | 1062.10 |
| 36 | 13.309477 | 122.818961 | 7.345220 | 2.178749 | 3346.347661 | 25.436579 | 1651.679529 | 6.334308 | 539.98 |
| 49 | 11.822910 | 98.168307 | 7.386212 | 2.301552 | 3250.020705 | 24.340656 | 1662.438742 | 6.617267 | 1095.30 |
| 51 | 13.986616 | 132.575456 | 7.194749 | 2.108986 | 3415.255945 | 26.253236 | 1712.017629 | 5.583251 | 805.49 |
| 61 | 14.735895 | 128.201000 | 7.172693 | 1.841786 | 3886.950307 | 28.289950 | 1672.370150 | 5.044439 | 1146.00 |

### æ„å»ºå’Œæ£€æŸ¥æ¨¡å‹

æˆ‘ä»¬æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼Œ

1.  æŒ‡å®š K æŠ˜æ–¹æ³•

1.  éå†å¶èŠ‚ç‚¹æ•°é‡ï¼Œå®ä¾‹åŒ–ã€æ‹Ÿåˆå¹¶è®°å½•è¯¯å·®

1.  ç»˜åˆ¶æµ‹è¯•è¯¯å·®ä¸å¶èŠ‚ç‚¹æ•°é‡çš„å…³ç³»å›¾ï¼Œé€‰æ‹©æœ€å°åŒ–æµ‹è¯•è¯¯å·®çš„è¶…å‚æ•°

1.  ä½¿ç”¨è°ƒæ•´å¥½çš„è¶…å‚æ•°å’Œæ‰€æœ‰æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹

```py
MSE_kF = []; node_kF = []                                     
kf = KFold(n_splits=5, shuffle=True, random_state=seed)       # k-fold specification 

inode = 2
while inode < len(X_train):
    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=inode,random_state=seed)
    scores = cross_val_score(estimator=tree_model,X=X,y=y,cv=kf,n_jobs=4,scoring = "neg_mean_squared_error") # perform 5-fold cross validation
    MSE_kF.append(abs(scores.mean()))
    node_kF.append(inode); inode+=1

tuned_node_kF = node_kF[np.argmin(MSE_kF)]
tuned_tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=tuned_node_kF).fit(X.values, y.values) # retrain on all the data

plt.subplot(121)
plt.vlines(tuned_node_kF,0,1.05*max_MSE_kF,lw=1.0,ls='--',color='red',zorder=10)
plt.annotate('Tuned Max Nodes = ' + str(tuned_node_kF),(tuned_node_kF-2,3.5e5),rotation=90,zorder=30)
plt.scatter(node_kF,MSE_kF,s=None,c="red",marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,
            linewidths=0.5, edgecolors="black",zorder=40,label='k-Fold')
plt.title('Decision Tree k-Fold Cross Validation Error (MSE) vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); 
plt.ylabel('Mean Square Error'); plt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_kF); add_grid()
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
plt.legend(loc='upper right')

y_hat = tuned_tree_model.predict(X)

plt.subplot(122)
plt.scatter(y,y_hat,color='green',edgecolor='black') # cross validation plot
plt.plot([ymin_new,ymax_new],[ymin_new,ymax_new],color='black',zorder=-1)
plt.xlim(ymin_new,ymax_new); plt.ylim(ymin_new,ymax_new); add_grid() 
plt.xlabel('Truth: ' + ylabel_new); plt.ylabel('Estimate: ' + ylabel_new)
plt.title('Tuned Decision Tree, Cross Validation')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/800e93156a21d23fb27ec3b349cbd0c234a2789e27a8582567a80abbbf0e08b0.png](img/a3d66d6b8f851c5edb5b770a5393116c.png)

## è¯„è®º

è¿™æ˜¯å¯¹å†³ç­–æ ‘çš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„è§†é¢‘è®²åº§é“¾æ¥ã€‚

æˆ‘å¸Œæœ›è¿™ä¼šæœ‰æ‰€å¸®åŠ©ï¼Œ

*Michael*

## å…³äºä½œè€…

![](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡æ ¡å›­å†… 40 è‹±äº©åœŸåœ°ä¸Šï¼Œè¿ˆå…‹å°”Â·çš®å°”èŒ¨æ•™æˆåœ¨ä»–çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡å…¹æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œä»–åœ¨[å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡](https://www.utexas.edu/)ä»äº‹å’Œæ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ

+   è¯¥[èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„è´Ÿè´£äººï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜

+   [è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº†è¶…è¿‡ 70 ç¯‡[åŒè¡Œè¯„å®¡çš„å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦ã€Š[åœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ã€‹ï¼Œå¹¶æ˜¯ä¸¤æœ¬è¿‘æœŸå‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œåˆ†åˆ«æ˜¯ã€Š[Python åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)ã€‹å’Œã€Š[Python åº”ç”¨æœºå™¨å­¦ä¹ ï¼šä»£ç å®è·µæŒ‡å—](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‹ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œé™„æœ‰ 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†å·¥ä½œæµç¨‹é“¾æ¥ï¼Œè¿™äº›å­˜å‚¨åº“ä½äºä»–çš„[GitHub è´¦æˆ·](https://github.com/GeostatsGuy)ï¼Œä»¥æ”¯æŒä»»ä½•æœ‰å…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ã€‚æƒ³äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„¿æ„åˆä½œï¼Œæ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°è´¨å­¦é™¢

æ›´å¤šèµ„æºè¯·è®¿é—®ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## åŠ¨æœº

å†³ç­–æ ‘ä¸æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€ä¸ºå¼ºå¤§ã€æœ€å‰æ²¿çš„æ–¹æ³•ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆè¿˜è¦ä»‹ç»å†³ç­–æ ‘ï¼Ÿ

+   æœ€æ˜“äºç†è§£ã€å¯è§£é‡Šçš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹ä¸€

+   å†³ç­–æ ‘é€šè¿‡éšæœºæ£®æ—ã€è¢‹è£…å’Œæå‡è¢«å¢å¼ºï¼Œæˆä¸ºè®¸å¤šæƒ…å†µä¸‹æœ€ä½³çš„æœºå™¨å­¦ä¹ é¢„æµ‹æ¨¡å‹ä¹‹ä¸€

![](img/90b8611f2db53bd1e441fa8eb6dce0d1.png)

é˜¿æ‹‰æ–¯åŠ åŒ—éƒ¨çš„é’ˆå¶æ—ä¸­çš„ä¸€æ£µé«˜å‚²çš„é»‘è‰²äº‘æ‰æ ‘ï¼Œä¸æˆ‘å®¶ä¹¡é˜¿å°”ä¼¯å¡”çœåŒ—éƒ¨åœ°åŒºç›¸ä¼¼ã€‚ç…§ç‰‡æ¥è‡ª https://www.britannica.com/plant/spruce#/media/1/561445/8933ï¼Œè®¿é—®æ—¥æœŸ 2025 å¹´ 5 æœˆ 1 æ—¥ã€‚

è®©æˆ‘ä»¬æ¢è®¨å†³ç­–æ ‘çš„ä¸€äº›å…³é”®æ–¹é¢ã€‚

## æ¨¡å‹å…¬å¼

é¢„æµ‹ç‰¹å¾ç©ºé—´è¢«åˆ†å‰²æˆ $J$ ä¸ªç©·å°½ã€äº’æ–¥çš„åŒºåŸŸ $R_1, R_2, \ldots, R_J$ã€‚å¯¹äºç»™å®šçš„é¢„æµ‹æ¡ˆä¾‹ $x_1,\ldots,x_m \in R_j$ï¼Œé¢„æµ‹æ˜¯ï¼š

**å›å½’** - åŒºåŸŸ $R_j$ ä¸­è®­ç»ƒæ•°æ®çš„å¹³å‡å€¼ï¼Œ$R_j$

$$ \hat{y} = \frac{1}{|R_j|} \sum_{\mathbf{x}_i \in R_j} y_i $$

å…¶ä¸­ $\hat{y}$ æ˜¯è¾“å…¥ $\mathbf{x}$ çš„é¢„æµ‹å€¼ï¼Œ$R_j$ æ˜¯ $\mathbf{x}$ è½å…¥çš„åŒºåŸŸï¼ˆå¶èŠ‚ç‚¹ï¼‰ï¼Œ$|R_j|$ æ˜¯åŒºåŸŸ $R_j$ ä¸­è®­ç»ƒæ ·æœ¬çš„æ•°é‡ï¼Œ$y_i$ æ˜¯è¿™äº›è®­ç»ƒæ ·æœ¬åœ¨ $R_j$ ä¸­çš„å®é™…ç›®æ ‡å€¼ã€‚

**åˆ†ç±»** - åŒºåŸŸ $R_j$ ä¸­è®­ç»ƒæ¡ˆä¾‹æ•°é‡æœ€å¤šçš„ç±»åˆ«ï¼ˆæœ€å¸¸è§çš„æƒ…å†µï¼‰ï¼š

$$ \hat{y} = \arg\max_{c \in C} \left( \frac{1}{|R_j|} \sum_{\mathbf{x}_i \in R_j} \mathbb{1}(y_i = c) \right) $$

å…¶ä¸­ $C$ æ˜¯æ‰€æœ‰å¯èƒ½ç±»åˆ«çš„é›†åˆï¼Œ$\mathbb{1}(y_i = c)$ æ˜¯æŒ‡ç¤ºå˜æ¢ï¼Œå¦‚æœ $y_i = c$ åˆ™ä¸º 1ï¼Œå¦åˆ™ä¸º 0ï¼Œ$|R_j|$ æ˜¯åŒºåŸŸ $R_j$ ä¸­è®­ç»ƒæ ·æœ¬çš„æ•°é‡ï¼Œ$\hat{y}$ æ˜¯é¢„æµ‹çš„ç±»åˆ«æ ‡ç­¾ã€‚

é¢„æµ‹ç©ºé—´ $ğ‘‹_1,\ldots,ğ‘‹_ğ‘š$ è¢«åˆ†å‰²æˆ $J$ ä¸ªäº’æ–¥ã€ç©·å°½çš„åŒºåŸŸï¼Œ$R_j, j = 1,\ldots,J$ï¼Œå…¶ä¸­åŒºåŸŸæ˜¯ï¼Œ

+   **äº’æ–¥** â€“ ä»»ä½•é¢„æµ‹ç‰¹å¾ç»„åˆ $x_1,\ldots,x_ğ‘š$ åªå±äºå•ä¸ªåŒºåŸŸ $R_j$

+   **è¯¦å°½æ— é—** â€“ æ‰€æœ‰é¢„æµ‹ç‰¹å¾å€¼çš„ç»„åˆéƒ½å±äºä¸€ä¸ªåŒºåŸŸï¼Œ$R_j$ï¼Œå³æ‰€æœ‰åŒºåŸŸï¼Œ$R_j, j = 1,\ldots,J$ï¼Œè¦†ç›–æ•´ä¸ªé¢„æµ‹ç‰¹å¾ç©ºé—´

æ‰€æœ‰è½åœ¨åŒä¸€åŒºåŸŸï¼Œ$R_j$ï¼Œçš„é¢„æµ‹æ¡ˆä¾‹ï¼Œ$x_1,\ldots,x_m$ï¼Œéƒ½ä½¿ç”¨ç›¸åŒçš„å€¼è¿›è¡Œä¼°è®¡ã€‚

+   é¢„æµ‹æ¨¡å‹åœ¨åŒºåŸŸè¾¹ç•Œå†…åœ¨æœ¬è´¨ä¸Šæ˜¯ä¸è¿ç»­çš„

ä¾‹å¦‚ï¼Œè€ƒè™‘è¿™ä¸ªç”¨äºç”Ÿäº§å“åº”ç‰¹å¾ï¼Œ$\hat{Y}$çš„å†³ç­–æ ‘é¢„æµ‹æ¨¡å‹ï¼Œä»å­”éš™ç‡ï¼Œ$X_1$é¢„æµ‹ç‰¹å¾ï¼Œ

![å›¾ç‰‡](img/98d8fb73fe41299a6a9b443163b47c96.png)

æ•°æ®å’Œé¢„æµ‹çš„å››åŒºåŸŸå†³ç­–æ ‘ï¼Œ$\hat{Y}(R_j) = \overline{Y}(R_j)$æŒ‰åŒºåŸŸï¼Œ$R_j, j=1,â€¦,4$ã€‚ä¾‹å¦‚ï¼Œç»™å®š 13%çš„å­”éš™ç‡é¢„æµ‹ç‰¹å¾å€¼ï¼Œæ¨¡å‹é¢„æµ‹ç”Ÿäº§å¤§çº¦ 2,000 MCFPDã€‚

æˆ‘ä»¬å¦‚ä½•åˆ†å‰²é¢„æµ‹ç‰¹å¾ç©ºé—´ï¼Ÿ

çœ‹è¿™ä¸ªä¾‹å­ï¼Œä½¿ç”¨é¢„æµ‹ç‰¹å¾å­”éš™ç‡å’Œè„†æ€§æ¥é¢„æµ‹ç”Ÿäº§å“åº”ç‰¹å¾ã€‚

![å›¾ç‰‡](img/31b00c9edfa4f39d2ae71626df2cd687.png)

é¢„æµ‹ç‰¹å¾ç©ºé—´çš„ä¸€ä¸ªéå¸¸å¤æ‚çš„ 3 åŒºåŸŸåˆ†å‰²ã€‚

+   è¿™äº›æ˜¯éå¸¸é«˜æ•ˆçš„è¾¹ç•Œï¼Œå¯ä»¥æ•æ‰ä½ã€ä¸­ã€é«˜äº§é‡

ä½†æ˜¯ï¼Œè¿™ä¸ªæ¨¡å‹å°†ä¼šç›¸å½“å¤æ‚ï¼Œ

+   éœ€è¦å¤§é‡çš„æ¨¡å‹å‚æ•°

+   å¯¹äºå¤§é‡é¢„æµ‹ç‰¹å¾ï¼Œå³é«˜ç»´åº¦ï¼Œéš¾ä»¥è®­ç»ƒã€‚

å¦‚æœæˆ‘èƒ½è¯´æœä½ æ¥å—è¿™äº›åŒºåŸŸï¼Œé‚£ä¹ˆä½ å°†æœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œ

+   éå¸¸å®¹æ˜“è®­ç»ƒ

+   å‚æ•°éå¸¸å°‘

+   éå¸¸ç´§å‡‘ä¸”å¯è§£é‡Š

![å›¾ç‰‡](img/13cf7080472834d100f0e7812be6d2d3.png)

ä½¿ç”¨ 9 ä¸ªåŒºåŸŸçš„æ›´ç®€å•é¢„æµ‹ç‰¹å¾ç©ºé—´åˆ†å‰²ï¼Œä½†å‚æ•°è¾ƒå°‘ï¼Œä¸”æ˜“äºè®­ç»ƒä»»ä½•ç»´åº¦ã€‚

è¿™æ˜¯ä¸€ä¸ªåŸºäºåˆ†å±‚äºŒåˆ†åˆ†å‰²çš„åŒºåŸŸé›†ã€‚è®©æˆ‘ä»¬å…ˆæ˜ç¡®é¢„æµ‹ç‰¹å¾ç©ºé—´çš„æ¦‚å¿µï¼Œç„¶åè§£é‡Šè¿™ç§é¢„æµ‹ç‰¹å¾åˆ†å‰²å½¢å¼ã€‚

## é¢„æµ‹ç‰¹å¾ç©ºé—´

è®©æˆ‘ä»¬å›é¡¾å¹¶å»ºç«‹é¢„æµ‹ç‰¹å¾ç©ºé—´çš„æ¦‚å¿µã€‚æˆ‘ä»¬å°†å…¶å®šä¹‰ä¸ºï¼Œ

+   åŒ…å«æ‰€æœ‰å¯èƒ½çš„ä¼°è®¡é—®é¢˜ï¼Œå³æ‰€æœ‰å¯èƒ½çš„é¢„æµ‹ç‰¹å¾å€¼çš„ç»„åˆï¼Œ$x_1, x_2,\ldots,x_m$ã€‚

![å›¾ç‰‡](img/2328290a847e59dd59f76c37a18c6df3.png)

3 ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾æœ‰æŒ‡å®šæœ€å°å’Œæœ€å¤§å€¼çš„æƒ…å†µä¸‹çš„é¢„æµ‹ç‰¹å¾ç©ºé—´ç¤ºæ„å›¾ï¼Œç»“æœæ˜¯ä¸€ä¸ªå¯èƒ½çš„é¢„æµ‹çŸ©å½¢ç«‹æ–¹ä½“ï¼Œ$x_1, x_2, x_3$ã€‚

é€šå¸¸è¿™ç”±å¯èƒ½å€¼çš„èŒƒå›´å®šä¹‰ï¼Œ$x_{\alpha} \in \left[X_{\alpha,\text{ğ‘šğ‘–ğ‘›}},ğ‘‹_{\alpha,\text{max}} \right]$ï¼Œç»“æœä¸ºï¼Œ

+   1 ä¸ªé¢„æµ‹ç‰¹å¾ $\rightarrow$ çº¿æ®µ

+   2 ä¸ªé¢„æµ‹ç‰¹å¾ $\rightarrow$ çŸ©å½¢

+   3 ä¸ªé¢„æµ‹ç‰¹å¾ $\rightarrow$ çŸ©å½¢ç«‹æ–¹ä½“

+   $>$3 ä¸ªé¢„æµ‹ç‰¹å¾ $\rightarrow$ è¶…çŸ©å½¢

å½“æˆ‘ä»¬ä½¿ç”¨é¢„æµ‹ç‰¹å¾çš„å–å€¼èŒƒå›´å®šä¹‰é¢„æµ‹ç‰¹å¾ç©ºé—´æ—¶ï¼Œæˆ‘åº”è¯¥æä¾›ä¸€äº›æ³¨æ„äº‹é¡¹ã€‚

å†³ç­–æ ‘å…·æœ‰éšå¼å¤–æ¨æ¨¡å‹

æ­£å¦‚ä½ ä¸‹é¢å°†çœ‹åˆ°çš„ï¼Œæ²¿å¤–éƒ¨çš„åŒºåŸŸå»¶ä¼¸åˆ°æ— ç©·å¤§ï¼Œå®é™…ä¸Šå‡è®¾äº†ä¸€ä¸ªæ’å®šçš„å¤–æ¨æ¨¡å‹ã€‚

## æ ‘æŸå¤±å‡½æ•°

å¯¹äºå›å½’æ ‘ï¼Œæˆ‘ä»¬æœ€å°åŒ–æ®‹å·®å¹³æ–¹å’Œï¼Œå¯¹äºåˆ†ç±»æ ‘ï¼Œæˆ‘ä»¬æœ€å°åŒ–åŠ æƒå¹³å‡åŸºå°¼ä¸çº¯åº¦ã€‚

æ®‹å·®å¹³æ–¹å’Œï¼ˆRSSï¼‰è¡¡é‡å›å½’æ ‘ä¸­å®é™…å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„æ€»å¹³æ–¹å·®ï¼Œ

$$ \text{RSS} = \sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})Â² $$

å…¶ä¸­ $J$ æ˜¯æ ‘ä¸­çš„åŒºåŸŸæ€»æ•°ï¼Œ$R_j$ æ˜¯ $j$ åŒºåŸŸï¼Œ$y_i$ æ˜¯ç¬¬ $i$ ä¸ªè®­ç»ƒæ•°æ®å“åº”ç‰¹å¾çš„çœŸå€¼ï¼Œ$\hat{y}_{R_j}$ æ˜¯åŒºåŸŸ $R_j$ çš„é¢„æµ‹å€¼ï¼Œå³ $y_i \; \forall \; i \in R_j$ çš„å¹³å‡å€¼ã€‚

å½“ä¸€ä¸ªçˆ¶èŠ‚ç‚¹åˆ†è£‚æˆä¸¤ä¸ªå­èŠ‚ç‚¹ï¼ˆt_Lï¼‰å’Œï¼ˆt_Rï¼‰æ—¶ï¼ŒåŠ æƒåŸºå°¼ä¸çº¯åº¦ä¸ºï¼š

$$ \text{Gini}_{\text{total}} = \sum_{j=1}^{J} \frac{N_j}{N} \cdot \text{Gini}(j) $$

å…¶ä¸­ $J$ æ˜¯æ ‘ä¸­çš„åŒºåŸŸæ€»æ•°ï¼Œ$N$ æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ€»æ•°ï¼Œ$N_j$ æ˜¯å¶èŠ‚ç‚¹ $j$ ä¸­çš„æ ·æœ¬æ•°é‡ï¼Œ$\text{Gini}(j)$ æ˜¯å¶èŠ‚ç‚¹ $j$ çš„åŸºå°¼ä¸çº¯åº¦ã€‚

å•ä¸ªå†³ç­–æ ‘èŠ‚ç‚¹çš„åŸºå°¼ä¸çº¯åº¦è®¡ç®—å¦‚ä¸‹ï¼Œ

$$ \text{Gini}(j) = 1 - \sum_{c=1}^{C} p_{j,c}Â² $$

å…¶ä¸­ $p_{j,c}$ æ˜¯èŠ‚ç‚¹ $j$ ä¸­ç±»åˆ« $c$ æ ·æœ¬çš„æ¯”ä¾‹ã€‚

å¯¹äºåˆ†ç±»ï¼Œæˆ‘ä»¬çš„æŸå¤±å‡½æ•°ä¸æ¯”è¾ƒé¢„æµ‹å€¼ä¸çœŸå€¼ï¼Œå°±åƒæˆ‘ä»¬çš„å›å½’æŸå¤±ä¸€æ ·ï¼

+   åŸºå°¼ä¸çº¯åº¦æƒ©ç½šè®­ç»ƒæ•°æ®ç±»åˆ«æ··åˆï¼æ‰€æœ‰è®­ç»ƒæ•°æ®ä¸ºåŒä¸€ç±»åˆ«çš„åŒºåŸŸå°†å…·æœ‰åŸºå°¼ä¸çº¯åº¦ä¸º 0ï¼Œä»è€Œå¯¹æ•´ä½“æŸå¤±åšå‡ºè´¡çŒ®ã€‚

æ³¨æ„ï¼ŒæŒ‰åŒºåŸŸè®¡ç®—çš„åŸºå°¼ä¸çº¯åº¦æ˜¯ï¼Œ

+   **åŠ æƒ** - ç”±æ¯ä¸ªåŒºåŸŸçš„è®­ç»ƒæ•°æ®æ•°é‡å†³å®šï¼Œè®­ç»ƒæ•°æ®è¾ƒå¤šçš„åŒºåŸŸå¯¹æ•´ä½“æŸå¤±çš„å½±å“æ›´å¤§

+   **å¹³å‡** - åœ¨æ‰€æœ‰åŒºåŸŸä¸Šè®¡ç®—å†³ç­–æ ‘çš„æ€»åŸºå°¼ä¸çº¯åº¦

è¿™äº›æŸå¤±åœ¨ä»¥ä¸‹è¿‡ç¨‹ä¸­è®¡ç®—ï¼Œ

+   **æ ‘æ¨¡å‹è®­ç»ƒ** - æ ¹æ®è®­ç»ƒæ•°æ®æ¥ç”Ÿé•¿æ ‘

+   **æ ‘æ¨¡å‹è°ƒæ•´** - æ ¹æ®ä¿ç•™çš„æµ‹è¯•æ•°æ®é€‰æ‹©æœ€ä½³æ ‘å¤æ‚åº¦ã€‚

è®©æˆ‘ä»¬å…ˆè°ˆè°ˆæ ‘æ¨¡å‹è®­ç»ƒï¼Œç„¶åå†è°ˆæ ‘æ¨¡å‹è°ƒæ•´ã€‚

## è®­ç»ƒæ ‘æ¨¡å‹

æˆ‘ä»¬å¦‚ä½•è®¡ç®—è¿™äº›äº’æ–¥ä¸”ç©·å°½çš„åŒºåŸŸï¼Ÿè¿™æ˜¯é€šè¿‡é¢„æµ‹ç‰¹å¾ç©ºé—´çš„åˆ†å±‚äºŒåˆ†åˆ†å‰²æ¥å®ç°çš„ã€‚

è®­ç»ƒå†³ç­–æ ‘æ¨¡å‹æ˜¯æ—¢ï¼Œ

1.  åˆ†é…äº’æ–¥ä¸”ç©·å°½çš„åŒºåŸŸ

1.  æ„å»ºå†³ç­–æ ‘æ—¶ï¼Œæ¯ä¸ªåŒºåŸŸéƒ½æ˜¯ä¸€ä¸ªç»ˆç«¯èŠ‚ç‚¹ï¼Œä¹Ÿç§°ä¸ºå¶èŠ‚ç‚¹

è¿™äº›æ˜¯åŒä¸€ä»¶äº‹ï¼è®©æˆ‘ä»¬åˆ—å‡ºæ­¥éª¤ï¼Œç„¶åé€šè¿‡è®­ç»ƒä¸€ä¸ªæ ‘æ¥æ¼”ç¤ºè¿™ä¸€ç‚¹ã€‚

1.  **å°†æ‰€æœ‰æ•°æ®åˆ†é…åˆ°å•ä¸ªåŒºåŸŸ** - è¿™ä¸ªåŒºåŸŸè¦†ç›–äº†æ•´ä¸ªé¢„æµ‹ç‰¹å¾ç©ºé—´

1.  **æ‰«ææ‰€æœ‰å¯èƒ½çš„åˆ†å‰²** - åœ¨æ‰€æœ‰åŒºåŸŸå’Œæ‰€æœ‰ç‰¹å¾ä¸Š

1.  **é€‰æ‹©æœ€ä½³åˆ†å‰²** - è¿™æ˜¯è´ªå©ªä¼˜åŒ–ï¼Œå³æœ€ä½³åˆ†å‰²æœ€å°åŒ–äº†æ‰€æœ‰è®­ç»ƒæ•°æ® $y_i$ åœ¨æ‰€æœ‰åŒºåŸŸ $j = 1,\ldots,J$ ä¸Šçš„æ®‹å·®å¹³æ–¹å’Œã€‚

1.  **è¿­ä»£è‡³è¿‡åº¦æ‹Ÿåˆ** - è¿”å›æ­¥éª¤ 1 è¿›è¡Œä¸‹ä¸€ä¸ªåˆ†å‰²ï¼Œç›´åˆ°æ ‘è¿‡åº¦æ‹Ÿåˆã€‚

æ³¨æ„ï¼Œè¿™ç§è®­ç»ƒå†³ç­–æ ‘çš„æ–¹æ³•æ˜¯ä¸€ç§å¯å‘å¼è§£å†³æ–¹æ¡ˆï¼Œ

+   æ²¡æœ‰åŠªåŠ›åŒæ—¶ä¼˜åŒ–æ‰€æœ‰åˆ†å‰²ï¼Œä¾‹å¦‚ï¼Œä¸ºäº†é€‰æ‹©æ¬¡ä¼˜åˆ†å‰²ä»¥æœ€å¤§åŒ–è®­ç»ƒè¯¯å·®çš„å‡å°‘ï¼Œéšåè¿›è¡Œåˆ†å‰²

æ­¤å¤–ï¼Œå†³ç­–æ ‘æ˜¯ä»ä¸Šåˆ°ä¸‹æ„å»ºçš„ã€‚

+   æˆ‘ä»¬ä»ä¸€ä¸ªè¦†ç›–æ•´ä¸ªé¢„æµ‹ç‰¹å¾ç©ºé—´çš„å•ä¸€åŒºåŸŸå¼€å§‹ï¼Œç„¶åè¿›è¡Œä¸€ç³»åˆ—çš„åŒºåŸŸåˆ†å‰²/æ ‘åˆ†æ”¯ã€‚

ç°åœ¨è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜è¿™ä¸€ç‚¹ï¼Œä½¿ç”¨ 2 ä¸ªé¢„æµ‹ç‰¹å¾é¢„æµ‹å¤©ç„¶æ°”ç”Ÿäº§å“åº”ç‰¹å¾ã€‚

+   å­”éš™ç‡ - å½±å“å­”éš™ä½“ç§¯å’ŒæµåŠ¨

+   è„†æ€§ - å½±å“è¯±å¯¼å’Œä¿æŒå¼€æ”¾è£‚ç¼çš„èƒ½åŠ›

æˆ‘ä»¬ä»æ‰€æœ‰é¢„æµ‹ç‰¹å¾ç©ºé—´çš„ä¸€ä¸ªå•ä¸€åŒºåŸŸå¼€å§‹ï¼Œå”¯ä¸€çš„é¢„æµ‹æ˜¯æ‰€æœ‰è®­ç»ƒæ•°æ®çš„å¹³å‡å€¼ã€‚

![å›¾ç‰‡](img/4a6be7714eba68f580cfee8421d15f61.png)

åˆå§‹æ•°æ®å…¨éƒ¨åœ¨ 1 ä¸ªåŒºåŸŸï¼Œå³å¶èŠ‚ç‚¹æ•°é‡çš„è¶…å‚æ•°ä¸º 1ï¼Œä½¿ç”¨å“åº”ç‰¹å¾çš„å…¨çƒå‡å€¼è¿›è¡Œé¢„æµ‹ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ‰«ææ‰€æœ‰ç‰¹å¾ä»¥æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ€ä½³åˆ†å‰²ï¼Œå­”éš™ç‡ä¸º 16.7%ã€‚è¿™ä¸ªåªæœ‰ä¸€ä¸ªå†³ç­–èŠ‚ç‚¹å’Œ 2 ä¸ªåŒºåŸŸæˆ–å¶èŠ‚ç‚¹çš„éå¸¸ç®€å•çš„å†³ç­–æ ‘è¢«ç§°ä¸ºå•èŠ‚ç‚¹æ ‘ï¼Œå³æœ€ç®€å•çš„å†³ç­–æ ‘æ¨¡å‹ã€‚

![å›¾ç‰‡](img/c418139a6f37dbda3462cd0ab2d519d6.png)

å¶èŠ‚ç‚¹æ•°é‡çš„è¶…å‚æ•°ä¸º 2ï¼Œç¬¬ä¸€ä¸ªæœ€ä½³åˆ†å‰²å¯¼è‡´ä¸€ä¸ªå•èŠ‚ç‚¹æ ‘ã€‚

ç°åœ¨æˆ‘ä»¬æ‰«æä¸¤ä¸ªåŒºåŸŸä»¥åŠæ‰€æœ‰é¢„æµ‹ç‰¹å¾ï¼Œä»¥æ‰¾åˆ°æœ€ä½³ä¸‹ä¸€ä¸ªåˆ†å‰²ï¼Œåœ¨å­”éš™ç‡å¤§äºæˆ–ç­‰äº 16.7%çš„åŒºåŸŸï¼Œè„†æ€§ä¸º 36.1ã€‚

![å›¾ç‰‡](img/5786413cc0e329c72793e1d2adf2a54b.png)

å¶èŠ‚ç‚¹æ•°é‡çš„è¶…å‚æ•°ä¸º 3ã€‚

ç»§ç»­è¿›è¡Œï¼Œæˆ‘ä»¬åœ¨å³ä¸ŠåŒºåŸŸæ‰¾åˆ°ä¸‹ä¸€ä¸ªå­”éš™ç‡ä¸º 18.5%çš„åˆ†å‰²ã€‚æˆ‘ä»¬ç°åœ¨æœ‰ 4 ä¸ªåŒºåŸŸã€‚æˆ‘ä»¬çš„å†³ç­–æ ‘å¼€å§‹æ•æ‰éšç€å­”éš™ç‡çš„å¢åŠ è€Œå¢åŠ çš„ç”Ÿäº§é‡ï¼Œä»¥åŠä½è„†æ€§æ—¶çš„ä½ç”Ÿäº§é‡ã€‚

![å›¾ç‰‡](img/9931df51305b719f93fc51eecf44641d.png)

å¶èŠ‚ç‚¹æ•°é‡çš„è¶…å‚æ•°ä¸º 4ã€‚

ç°åœ¨ä¸‹ä¸€ä¸ªæœ€ä½³åˆ†å‰²æ˜¯åœ¨åŸå§‹çš„ä½å­”éš™ç‡åŒºåŸŸï¼Œæ¥è‡ªå­”éš™ç‡ä¸º 13.2%çš„å•èŠ‚ç‚¹æ ‘ã€‚

![å›¾ç‰‡](img/597e4adea071f8f67d71e82f4c1015f2.png)

å¶èŠ‚ç‚¹æ•°é‡çš„è¶…å‚æ•°ä¸º 5ã€‚

ä¸‹ä¸€ä¸ªæœ€ä½³åˆ†å‰²å°†å­”éš™ç‡ä¸­é—´çš„åŒºåŸŸåˆ†å‰²å¼€ï¼Œæ•æ‰äº†ä½è„†æ€§ä½ç”Ÿäº§é‡çš„è¶‹åŠ¿ï¼Œå³ä½¿åœ¨é«˜å­”éš™ç‡çš„æƒ…å†µä¸‹ã€‚

![å›¾ç‰‡](img/b1024ff1d9be25d7b007a5ee890b6d8f.png)

å¶èŠ‚ç‚¹æ•°é‡çš„è¶…å‚æ•°ä¸º 6ã€‚

ä¸‹ä¸€ä¸ªåˆ†å‰²æ•æ‰äº†ç”Ÿäº§é‡å› é«˜è„†æ€§è€Œå‡å°‘çš„æƒ…å†µï¼Œå³ä½¿åœ¨é«˜å­”éš™ç‡çš„æƒ…å†µä¸‹ï¼Œ

![å›¾ç‰‡](img/782938df9c411d347bdc8e6e031a6d66.png)

å¶èŠ‚ç‚¹æ•°é‡çš„è¶…å‚æ•°ä¸º 7ã€‚

å¹¶ä¸”è¿™ä¸ªåˆ†å‰²ç»§ç»­æ•æ‰æ•°æ®ä¸­çš„ç›¸åŒæ¨¡å¼ã€‚

![](img/c3401dcf9ca381bf65d39fada64ecde2.png)

å¶å­èŠ‚ç‚¹æ•°é‡çš„è¶…å‚æ•°ä¸º 8ã€‚

ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œåœæ­¢ï¼Œå¹¶åšå‡ºä»¥ä¸‹è§‚å¯Ÿï¼Œ

+   åˆ†å±‚ï¼ŒäºŒè¿›åˆ¶åˆ†å‰²ä¸é¡ºåºæ„å»ºå†³ç­–æ ‘ç›¸åŒï¼Œæ¯æ¬¡åˆ†å‰²æ·»åŠ ä¸€ä¸ªæ–°çš„å†³ç­–èŠ‚ç‚¹ï¼Œå¹¶ä½¿å¶å­èŠ‚ç‚¹æ•°é‡å¢åŠ ä¸€ä¸ªã€‚

+   ç®€å•çš„å†³ç­–æ ‘åŒ…å«åœ¨å¤æ‚çš„å†³ç­–æ ‘ä¸­ï¼Œå³ï¼Œå¦‚æœæˆ‘ä»¬æ„å»ºä¸€ä¸ª$8$å¶å­èŠ‚ç‚¹æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡ä¾æ¬¡ç§»é™¤å†³ç­–èŠ‚ç‚¹ï¼Œä»¥æœ€åä¸€ä¸ªç§»é™¤çš„èŠ‚ç‚¹ä¸ºé¡ºåºï¼Œå¯ä»¥å¾—åˆ°$8, 7, \ldots, 2$å¶å­èŠ‚ç‚¹æ¨¡å‹ã€‚

+   æœ€ç»ˆè¿‡åº¦æ‹Ÿåˆçš„æ¨¡å‹æ˜¯å¶å­èŠ‚ç‚¹æ•°é‡ç­‰äºè®­ç»ƒæ•°æ®æ•°é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®­ç»ƒè¯¯å·®ä¸º 0.0ï¼Œå› ä¸ºæ¯ä¸ªè®­ç»ƒæ•°æ®éƒ½æœ‰ä¸€ä¸ªåŒºåŸŸï¼Œæˆ‘ä»¬ä½¿ç”¨è®­ç»ƒæ•°æ®å“åº”ç‰¹å¾å€¼æ¥ä¼°è®¡æ‰€æœ‰è®­ç»ƒæ•°æ®æ¡ˆä¾‹ã€‚

## ä½¿ç”¨æ–°çš„åˆ†å‰²æ›´æ–°æŸå¤±å‡½æ•°

ä¸ºäº†æ‰¾åˆ°ä¸‹ä¸€ä¸ªæœ€ä½³åˆ†å‰²ï¼Œæˆ‘ä»¬å¿…é¡»æ‰«ææ‰€æœ‰åŒºåŸŸï¼Œä»¥åŠæ‰€æœ‰å…·æœ‰åŒºåŸŸçš„ç‰¹å¾ã€‚è¿™å¬èµ·æ¥å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—ï¼Œä½†å®é™…ä¸Šéå¸¸é«˜æ•ˆã€‚

+   å¯¹äºæ¯ä¸ªåŒºåŸŸä¸­çš„æ¯ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬åªéœ€è¦æ£€æŸ¥æ’åºåçš„è®­ç»ƒæ•°æ®çš„ä¸­ç‚¹ï¼Œå› ä¸ºä»»ä½•ä¸ä¼šæ”¹å˜è®­ç»ƒæ•°æ®åŒºåŸŸåˆ†é…çš„åˆ†å‰²éƒ½ä¸ä¼šæ”¹å˜è®­ç»ƒæŸå¤±ã€‚

å¯¹äºåˆ†å‰²æˆå€™é€‰åŒºåŸŸ$R_L$å’Œ$R_R$çš„åŒºåŸŸ$R$ï¼Œåˆ†å‰²åçš„ RSS ä¸ºï¼š

$$ \text{RSS}_{\text{split}} = \sum_{i \in R_L} (y_i - \hat{y}_{R_L})Â² + \sum_{i \in R_R} (y_i - \hat{y}_{R_R})Â² $$

å…¶ä¸­ï¼Œ$y_i$æ˜¯è®­ç»ƒæ•°æ®è§‚å¯Ÿ$i$çš„å®é™…å“åº”ç‰¹å¾ï¼Œè€Œ$\hat{y}_{R_L}$ï¼Œ$\hat{y}_{R_R}$æ˜¯å€™é€‰åŒºåŸŸ$R_L$å’Œ$R_R$ä¸­è®­ç»ƒæ•°æ®å“åº”ç‰¹å¾çš„å‡å€¼ã€‚

æ³¨æ„ï¼Œæˆ‘ä»¬æ·»åŠ äº†æ‰€æœ‰å…¶ä»–åŒºåŸŸçš„ RSS æˆåˆ†ï¼Œä»¥è·å¾—æ‰€æœ‰åŒºåŸŸçš„æ€»ä½“æ¨¡å‹ RSSï¼Œä»¥åœ¨æ‰€æœ‰åŒºåŸŸä¸­æ‰¾åˆ°æœ€ä½³åˆ†å‰²ï¼Œ

+   å…·æœ‰æœ€ä½$\text{RSS}_{\text{split}}$çš„åˆ†å‰²è¢«é€‰ä¸ºåŒºåŸŸï¼Œå¹¶ä¸å…¶ä»–åŒºåŸŸçš„æ‰€æœ‰æœ€ä½³åˆ†å‰²è¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ‰¾åˆ°ä¸‹ä¸€ä¸ªæœ€ä½³åˆ†å‰²ï¼Œè´ªå©ªè§£æ³•ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½è°ƒæ•´å†³ç­–æ ‘æ¨¡å‹ã€‚

## è°ƒæ•´æ ‘æ¨¡å‹

ä¸ºäº†è°ƒæ•´å†³ç­–æ ‘ï¼Œæˆ‘ä»¬é‡‡ç”¨éå¸¸è¿‡åº¦æ‹Ÿåˆçš„å·²è®­ç»ƒæ ‘æ¨¡å‹ï¼Œ

+   ä¾æ¬¡åˆ‡å‰²æœ€åä¸€ä¸ªå†³ç­–èŠ‚ç‚¹

+   å³ï¼Œä¿®å‰ªå†³ç­–æ ‘çš„æœ€æœ«åˆ†æ”¯

ç”±äºç®€å•çš„æ ‘åœ¨å¤æ‚çš„æ ‘å†…éƒ¨ï¼

æˆ‘ä»¬å¯ä»¥åœ¨ä¿®å‰ªå’Œé€‰æ‹©å…·æœ‰æœ€å°æµ‹è¯•è¯¯å·®çš„æ ‘æ—¶è®¡ç®—æµ‹è¯•è¯¯å·®

æˆ‘ä»¬è¿‡åº¦æ‹Ÿåˆäº†å†³ç­–æ ‘æ¨¡å‹ï¼Œå…·æœ‰å¤§é‡å¶å­èŠ‚ç‚¹ï¼Œç„¶åæˆ‘ä»¬åœ¨è·Ÿè¸ªæµ‹è¯•è¯¯å·®çš„åŒæ—¶å‡å°‘å¶å­èŠ‚ç‚¹æ•°é‡ã€‚

+   æˆ‘ä»¬é€‰æ‹©æœ€å°åŒ–æµ‹è¯•è¯¯å·®çš„å¶å­èŠ‚ç‚¹æ•°é‡ã€‚

+   ç”±äºæˆ‘ä»¬æ˜¯ä¾æ¬¡ç§»é™¤æœ€åä¸€ä¸ªåˆ†æ”¯ä»¥ç®€åŒ–æ ‘ï¼Œæ‰€ä»¥æˆ‘ä»¬ç§°æ¨¡å‹è°ƒæ•´**ä¿®å‰ª**ä¸ºå†³ç­–æ ‘

è¿™é‡Œæ˜¯ä¸€ä¸ªè¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œæ‹¥æœ‰è®¸å¤šï¼Œ$100$ï¼Œå¶å­èŠ‚ç‚¹ã€‚

![](img/3383a81c81cdb9ed5c96432a485e7194.png)

æåº¦è¿‡æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œå¶èŠ‚ç‚¹æ•°ä¸º 100 çš„è¶…å‚æ•°ï¼ˆå·¦ï¼‰ï¼Œè®­ç»ƒå’Œæµ‹è¯•äº¤å‰éªŒè¯å›¾ï¼ˆä¸­å¿ƒï¼‰å’Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸å¶èŠ‚ç‚¹æ•°çš„å…³ç³»ï¼ˆå³ï¼‰ã€‚

ç”±äºè¿™æ£µæ ‘æ˜¯ç”¨æˆ‘çš„äº¤äº’å¼ Python ä»ªè¡¨æ¿è®¡ç®—çš„ï¼Œæˆ‘èƒ½å¤Ÿè½»æ¾åœ°å°†åŒºåŸŸæ•°ä»$100, 99, 98, 96, 95, \ldots$å‡å°‘ï¼Œå¹¶å¯è§†åŒ–æ ‘ä»¥æ¢ç´¢ä»å¤æ‚åˆ°ç®€å•çš„æ ‘ã€‚

+   é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å¯ä»¥è¯æ˜ç®€å•çš„æ ‘åŒ…å«åœ¨å¤æ‚çš„æ ‘ä¸­ã€‚

ä¾‹å¦‚ï¼Œè¿™é‡Œæ˜¯è¿‡æ‹Ÿåˆçš„ 100 åŒºåŸŸå†³ç­–æ ‘ä¸­çš„ 5 åŒºåŸŸå†³ç­–æ ‘ï¼Œ

![å›¾ç‰‡](img/566f12da178143a07d17de9adc100684.png)

åœ¨æåº¦è¿‡æ‹Ÿåˆçš„ 100 å¶èŠ‚ç‚¹æ ‘æ¨¡å‹ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ª 5 å¶èŠ‚ç‚¹çš„æ ‘ã€‚

è¿™é‡Œæ˜¯è¿‡æ‹Ÿåˆçš„ 100 åŒºåŸŸå†³ç­–æ ‘ä¸­çš„ 10 åŒºåŸŸå†³ç­–æ ‘ï¼Œ

![å›¾ç‰‡](img/96e18c15cb01e9558e4b2512a6b1ef20.png)

åœ¨æåº¦è¿‡æ‹Ÿåˆçš„ 100 å¶èŠ‚ç‚¹æ ‘æ¨¡å‹ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ª 10 å¶èŠ‚ç‚¹çš„æ ‘ã€‚

æœ€åï¼Œè¿™é‡Œæ˜¯è¿‡æ‹Ÿåˆçš„ 100 åŒºåŸŸå†³ç­–æ ‘ä¸­çš„ 20 åŒºåŸŸå†³ç­–æ ‘ï¼Œ

![å›¾ç‰‡](img/19d39b48f3c129dc12099bd52c3bb546.png)

åœ¨æåº¦è¿‡æ‹Ÿåˆçš„ 100 å¶èŠ‚ç‚¹æ ‘æ¨¡å‹ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ª 20 å¶èŠ‚ç‚¹çš„æ ‘ã€‚

ä½ å¯èƒ½ä¼šæƒ³çŸ¥é“ï¼Œä¸ºä»€ä¹ˆæˆ‘æ²¡æœ‰åªæ˜¯æ›´æ–°å†³ç­–æ ‘å›¾ï¼Ÿscikit-learn çš„å†³ç­–æ ‘ç»˜å›¾å‡½æ•°é‡æ–°ç¼©æ”¾å›¾è¡¨ï¼Œå‡ ä½•å½¢çŠ¶å˜åŒ–å¾ˆå¤§ï¼Œè¿™ä½¿å¾—åœ¨å¤æ‚æ ‘ä¸­å¯è§†åŒ–ç®€å•æ ‘å˜å¾—å›°éš¾ã€‚

+   æˆ‘è®¤ä¸ºè¿™ç§å°†ç®€å•æ ‘å¯è§†åŒ–å¹¶ç»˜åˆ¶å¤šè¾¹å½¢çš„åšæ³•åœ¨æ•™è‚²ç›®çš„ä¸Šæ•ˆæœå¾ˆå¥½ï¼

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„æåº¦è¿‡æ‹Ÿåˆçš„æ ‘ï¼Œå¹¶é€šè¿‡æ ‘å‰ªææ–¹æ³•æ¼”ç¤ºè¶…å‚æ•°è°ƒæ•´ï¼Œ

![å›¾ç‰‡](img/24f9e4745fc35d0cf9c039aec32684f0.png)

æåº¦è¿‡æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å†æ¬¡è¯†åˆ«æœ€åæ·»åŠ çš„åˆ†æ”¯å¹¶å°†å…¶ç§»é™¤ï¼Œä»¥è®¡ç®— 99 åŒºåŸŸçš„å†³ç­–æ ‘ï¼Œä¸€ä¸ªç¨å¾®ç®€å•çš„å†³ç­–æ ‘ï¼Œå¹¶è®¡ç®—æµ‹è¯•è¯¯å·®ã€‚

![å›¾ç‰‡](img/2c6ffbcc9634d3d3f7350cb0564380d1.png)

æåº¦è¿‡æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰ã€‚

æˆ‘ä»¬å†æ¬¡è¯†åˆ«æœ€åæ·»åŠ çš„åˆ†æ”¯å¹¶å°†å…¶ç§»é™¤ï¼Œä»¥è®¡ç®— 98 åŒºåŸŸçš„å†³ç­–æ ‘ï¼Œå†æ¬¡æ˜¯ä¸€ä¸ªç¨å¾®ç®€å•çš„å†³ç­–æ ‘ï¼Œå¹¶è®¡ç®—æµ‹è¯•è¯¯å·®ã€‚

![å›¾ç‰‡](img/efae49220a4ea118479dcc21d333e37a.png)

æåº¦è¿‡æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰ã€‚

æˆ‘ä»¬å†æ¬¡è¯†åˆ«æœ€åæ·»åŠ çš„åˆ†æ”¯å¹¶å°†å…¶ç§»é™¤ï¼Œä»¥è®¡ç®— 97 åŒºåŸŸçš„å†³ç­–æ ‘ï¼Œå†æ¬¡æ˜¯ä¸€ä¸ªç¨å¾®ç®€å•çš„å†³ç­–æ ‘ï¼Œå¹¶è®¡ç®—æµ‹è¯•è¯¯å·®ã€‚

![å›¾ç‰‡](img/f4af3bd7dce90621152ff68c398bd578.png)

æåº¦è¿‡æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰ã€‚

æˆ‘ä»¬å†æ¬¡è¯†åˆ«æœ€åæ·»åŠ çš„åˆ†æ”¯å¹¶å°†å…¶ç§»é™¤ï¼Œä»¥è®¡ç®— 96 åŒºåŸŸçš„å†³ç­–æ ‘ï¼Œå†æ¬¡æ˜¯ä¸€ä¸ªç¨å¾®ç®€å•çš„å†³ç­–æ ‘ï¼Œå¹¶è®¡ç®—æµ‹è¯•è¯¯å·®ã€‚

![å›¾ç‰‡](img/eeea92714bf8b2f878a05ce7f50e9eae.png)

è¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰çš„å…³ç³»ã€‚

æˆ‘ä»¬å†æ¬¡è¯†åˆ«å‡ºæœ€åæ·»åŠ çš„åˆ†æ”¯å¹¶å°†å…¶ç§»é™¤ï¼Œä»¥è®¡ç®— 95 ä¸ªåŒºåŸŸçš„å†³ç­–æ ‘ï¼Œè¿™åˆæ˜¯ä¸€ä¸ªç¨å¾®ç®€å•çš„å†³ç­–æ ‘ï¼Œç„¶åæˆ‘ä»¬è®¡ç®—æµ‹è¯•è¯¯å·®ã€‚

![å›¾ç‰‡](img/90180132c5183f3342f2051c501aee4a.png)

è¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰çš„å…³ç³»ã€‚

æˆ‘ä»¬å†æ¬¡è¯†åˆ«å‡ºæœ€åæ·»åŠ çš„åˆ†æ”¯å¹¶å°†å…¶ç§»é™¤ï¼Œä»¥è®¡ç®— 94 ä¸ªåŒºåŸŸçš„å†³ç­–æ ‘ï¼Œè¿™åˆæ˜¯ä¸€ä¸ªç¨å¾®ç®€å•çš„å†³ç­–æ ‘ï¼Œç„¶åæˆ‘ä»¬è®¡ç®—æµ‹è¯•è¯¯å·®ã€‚

![å›¾ç‰‡](img/53f087807a31ef6a8e1be0e1ec170292.png)

è¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸æ¨¡å‹å¤æ‚åº¦ï¼ˆå·¦ï¼‰å’Œå†³ç­–æ ‘ï¼ˆå³ï¼‰çš„å…³ç³»ã€‚

ç°åœ¨è®©æˆ‘ä»¬è¿”å›å¹¶æŸ¥çœ‹è¿‡åº¦æ‹Ÿåˆçš„æ¨¡å‹ï¼Œå¹¶åœ¨ä¸åŒå¤æ‚åº¦çº§åˆ«ä¸Šæ·»åŠ ä¸€äº›æ›´å¤šä¿¡æ¯ã€‚

![å›¾ç‰‡](img/ab38e613fbf8ad17fdab5bd4f89adaa6.png)

è¿‡åº¦æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œå¶èŠ‚ç‚¹æ•°é‡çš„è¶…å‚æ•°ä¸º 100ï¼ˆå·¦ï¼‰ï¼Œè®­ç»ƒå’Œæµ‹è¯•äº¤å‰éªŒè¯å›¾ï¼ˆä¸­å¿ƒï¼‰ä»¥åŠè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸å¶èŠ‚ç‚¹æ•°é‡çš„å…³ç³»ï¼ˆå³ï¼‰ã€‚

æˆ‘åŒ…æ‹¬çš„ï¼Œ

+   è®­ç»ƒå’Œæµ‹è¯•äº¤å‰éªŒè¯å›¾ï¼Œå¯¹äº 100 ä¸ªå¶èŠ‚ç‚¹çš„è¿‡åº¦æ‹Ÿåˆå†³ç­–æ ‘ï¼Œå‡ ä¹å®Œç¾çš„è®­ç»ƒé¢„æµ‹å’Œéå¸¸å·®çš„æµ‹è¯•é¢„æµ‹

+   è®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸å¶èŠ‚ç‚¹æ•°é‡çš„å…³ç³»ã€‚

è¿™è¡¨æ˜å†³ç­–æ ‘æ¨¡å‹ç¡®å®éå¸¸è¿‡åº¦æ‹Ÿåˆï¼Œä¾‹å¦‚ï¼Œå¯ä»¥çœ‹åˆ°è®­ç»ƒè¯¯å·®ä¸‹é™å’Œæµ‹è¯•è¯¯å·®ä¸Šå‡ã€‚

ç°åœ¨æˆ‘ä»¬ä¿®å‰ªå†³ç­–èŠ‚ç‚¹ï¼Œç›´åˆ°æˆ‘ä»¬è·å¾—å¤§çº¦ 19 ä¸ªå¶èŠ‚ç‚¹çš„æœ€å°æµ‹è¯•è¯¯å·®çš„æ¨¡å‹ã€‚

![å›¾ç‰‡](img/064eac7331174101230aee9c413623f3.png)

è°ƒæ•´åçš„å†³ç­–æ ‘ï¼Œå¶èŠ‚ç‚¹æ•°é‡çš„è¶…å‚æ•°ä¸º 20ï¼ˆå·¦ï¼‰ï¼Œè®­ç»ƒå’Œæµ‹è¯•äº¤å‰éªŒè¯å›¾ï¼ˆä¸­å¿ƒï¼‰ä»¥åŠè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸å¶èŠ‚ç‚¹æ•°é‡çš„å…³ç³»ï¼Œè¡¨æ˜æµ‹è¯•è¯¯å·®æœ€å°åŒ–ï¼ˆå³ï¼‰ã€‚

ä¸ºäº†å®Œæ•´æ€§ï¼Œæˆ‘åŒ…æ‹¬äº†ä¸€ä¸ªæ¬ æ‹Ÿåˆæ¨¡å‹ï¼Œå³å¦‚æœæˆ‘ä»¬è¿‡åº¦ä¿®å‰ªæˆ‘ä»¬çš„å†³ç­–æ ‘ï¼Œåªæœ‰ 8 ä¸ªå¶èŠ‚ç‚¹ã€‚

![å›¾ç‰‡](img/9d7b773b0de829f858763876c59a1c04.png)

æ¬ æ‹Ÿåˆçš„å†³ç­–æ ‘ï¼Œå¶èŠ‚ç‚¹æ•°é‡çš„è¶…å‚æ•°ä¸º 8ï¼ˆå·¦ï¼‰ï¼Œè®­ç»ƒå’Œæµ‹è¯•äº¤å‰éªŒè¯å›¾ï¼ˆä¸­å¿ƒï¼‰ä»¥åŠè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ä¸å¶èŠ‚ç‚¹æ•°é‡çš„å…³ç³»ï¼Œè¡¨æ˜æµ‹è¯•è¯¯å·®æœ€å°åŒ–ï¼ˆå³ï¼‰ã€‚

æ³¨æ„ï¼Œåœ¨æ¬ æ‹Ÿåˆçš„å†³ç­–æ ‘ä¸­ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®éƒ½éå¸¸é«˜ã€‚

æˆ‘æ›´å–œæ¬¢å°†å¶èŠ‚ç‚¹æ•°é‡ä½œä¸ºæˆ‘çš„å†³ç­–æ ‘è¶…å‚æ•°ï¼Œå› ä¸ºå®ƒæä¾›äº†ï¼Œ

+   **è¿ç»­ã€å‡åŒ€çš„å¤æ‚åº¦å¢åŠ ** - å¤æ‚åº¦å¢åŠ çš„æ­¥éª¤ç›¸ç­‰ï¼Œæ²¡æœ‰è·³è·ƒ

+   **ç›´è§‚çš„å¤æ‚åº¦æ§åˆ¶** - æˆ‘ä»¬å¯ä»¥ç†è§£å’Œå…³è”$2, 3, \ldots, 100$ä¸ªå¶èŠ‚ç‚¹çš„å†³ç­–æ ‘

+   **çµæ´»çš„å¤æ‚åº¦** - æ ‘å¯ä»¥è‡ªç”±åœ°ä»¥ä»»ä½•æ–¹å¼ç”Ÿé•¿ä»¥å‡å°‘è®­ç»ƒè¯¯å·®ï¼ŒåŒ…æ‹¬é«˜åº¦ä¸å¯¹ç§°çš„å†³ç­–æ ‘

å…¶ä»–å¸¸è§çš„å†³ç­–æ ‘è¶…å‚æ•°åŒ…æ‹¬ï¼Œ

+   **æœ€å°åŒ– RSS å‡å°‘** â€“ ä¸å¢é‡å¢åŠ å¤æ‚æ€§å¿…é¡»ç”±è¶³å¤Ÿçš„è®­ç»ƒé”™è¯¯å‡å°‘æ¥æŠµæ¶ˆçš„æƒ³æ³•ç›¸å…³ã€‚è¿™å¯èƒ½å¯¼è‡´æ¨¡å‹æå‰åœæ­¢ï¼Œä¾‹å¦‚ï¼Œè®­ç»ƒé”™è¯¯å‡å°‘è¾ƒå°çš„åˆ†å‰²å¯èƒ½å¯¼è‡´éšåçš„åˆ†å‰²æœ‰æ›´å¤§çš„è®­ç»ƒé”™è¯¯å‡å°‘

+   **æ¯ä¸ªåŒºåŸŸçš„æœ€å°è®­ç»ƒæ•°æ®æ•°é‡** â€“ ä¸åŒºåŸŸä¼°è®¡çš„å‡†ç¡®æ€§æ¦‚å¿µç›¸å…³ï¼Œå³æˆ‘ä»¬éœ€è¦è‡³å°‘ $n$ ä¸ªæ•°æ®æ¥è·å¾—å¯é çš„å‡å€¼å’Œæœ€å¸¸è§çš„ç±»åˆ«

+   **æœ€å¤§å±‚æ•°** â€“ å¼ºåˆ¶å¯¹ç§°æ ‘ï¼Œåˆ°è¾¾æ¯ä¸ªå¶èŠ‚ç‚¹çš„åˆ†å‰²æ•°é‡ç›¸ä¼¼ã€‚æ¨¡å‹å¤æ‚åº¦éšç€è¶…å‚æ•°çš„å˜åŒ–è€Œå¤§å¹…å˜åŒ–ã€‚

## é¢„æµ‹æ¨¡å‹

å†³ç­–æ ‘é¢„æµ‹æ¨¡å‹è¡¨ç¤ºä¸º**åµŒå¥—çš„ if è¯­å¥é›†åˆ**ï¼Œä¾‹å¦‚ï¼š

```py
if porosity > 0.15:
    if brittleness < 20:
        initial_production = 1000
    else:
        initial_production = 7000
else:
    if brittleness < 40:
        initial_production = 500
    else:
        initial_production = 3000 
```

ä»¥åŠä¸Šè¿°é¢„æµ‹è¦ä¹ˆæ˜¯ï¼Œ

+   å›å½’ - è¯¥åŒºåŸŸå†…è®­ç»ƒæ•°æ®çš„å¹³å‡å€¼

+   åˆ†ç±» - è¯¥åŒºåŸŸå†…è®­ç»ƒæ•°æ®çš„å¤šæ•°ï¼Œæœ€å¸¸è§çš„ç±»åˆ«

## å†³ç­–æ ‘ä¸­çš„ Shapley å€¼

å›æƒ³ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å•ä¸ªæ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œ$f(x_1,x_2,x_3,x_4)$ï¼Œå¯¹æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾å­é›†ç»„åˆè¿›è¡Œä¼°è®¡ï¼Œä¾‹å¦‚ï¼Œ

$$ f(x_1) \quad f(x_2,x_4) \quad f(x_1,x_2,x_3) $$

+   æ³¨æ„ï¼Œè®¡ç®— Shapley å€¼çš„æœ´ç´ æ–¹æ³•æ˜¯è®­ç»ƒå…·æœ‰ä¸åŒé¢„æµ‹ç‰¹å¾çš„å®Œæ•´ç»„åˆæ¨¡å‹ï¼Œä½†å¦‚æœæˆ‘ä»¬ç›®æ ‡æ˜¯ç‰¹å¾é‡è¦æ€§ä»¥è¯Šæ–­æˆ‘ä»¬çš„ç‰¹å®šæ¨¡å‹ $f$ï¼Œä»¥æ”¯æŒæ¨¡å‹å¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬ä¸æƒ³åˆ›å»ºæ–°çš„æ¨¡å‹ã€‚

ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯åº”ç”¨å¤šç§æ–¹æ³•ï¼Œç±»ä¼¼äºæ’è¡¥æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼Œ

+   å°†æ’é™¤çš„ç‰¹å¾æ›¿æ¢ä¸ºæœŸæœ›å€¼ï¼Œå…¨å±€å‡å€¼ï¼Œ

$$ f(x_1,x_2,x_3) = f(x_1,x_2,x_3,x_4=E[x_4]) $$

+   å°†æ’é™¤çš„ç‰¹å¾æ›¿æ¢ä¸ºä¸­ä½æ•°ï¼Œå³ç¬¬ 50 ç™¾åˆ†ä½æ•°ï¼Œ

$$ f(x_1,x_2,x_3) = f(x_1,x_2,x_3,x_4=P50_{x_4}) $$

åŸºäºæ ‘æ¨¡å‹çš„æ¨¡å‹æœ‰æ›´å‡†ç¡®ã€ç‹¬ç‰¹çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥åœ¨æ¨¡å‹è®­ç»ƒåç§»é™¤ä»»ä½•ç‰¹å¾çš„å½±å“ï¼Œä¾‹å¦‚ï¼Œ

+   ç§»é™¤æ‰€æœ‰ $x_4$ åˆ†æ”¯ï¼Œç„¶åæ¨¡å‹ä¸ä¼šä½¿ç”¨ $x_4$ è¿›è¡Œé¢„æµ‹

å½“ç„¶ï¼Œæˆ‘ä»¬ä¸èƒ½åªæ˜¯ç§»é™¤åˆ†æ”¯ï¼Œç„¶åç”¨â€œæœ¨èƒ¶â€å°†æ ‘é‡æ–°ç²˜åˆåœ¨ä¸€èµ·ï¼

+   æˆ‘ä»¬å¿…é¡»åšå‡ºæ–°çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹ä¸ä¼šå¼•å…¥åå·®ã€‚

è®©æˆ‘ä»¬é€šè¿‡å‡ ä¸ªé¢„æµ‹æ¡ˆä¾‹æ¥æ¼”ç¤ºä»å†³ç­–æ ‘ä¸­ç§»é™¤ç‰¹å¾çš„è¿‡ç¨‹ï¼Œ

1.  è¿™é‡Œæ˜¯ä¸€ä¸ªä¸é‡åˆ°ç§»é™¤ç‰¹å¾çš„é¢„æµ‹æƒ…å†µï¼Œ$x_2$ è¢«ç§»é™¤ï¼Œ

$$ x_1=25 $$

+   é¢„æµ‹é€šå¸¸è¿›è¡Œã€‚

![](img/a9783a200ab476bb3a92cb2cfec12d63.png)

ä¸é‡åˆ°ç§»é™¤ç‰¹å¾çš„é¢„æµ‹æƒ…å†µï¼Œé€šå¸¸è¿›è¡Œé¢„æµ‹ã€‚

$$ f(x_1=25) = 20 $$

1.  é‡åˆ°ç§»é™¤ç‰¹å¾çš„é¢„æµ‹æƒ…å†µï¼Œ$x_1$ è¢«ç§»é™¤ï¼Œ

$$ x_2 = 60 $$

+   æˆ‘ä»¬å®é™…ä¸Šé€šè¿‡åŠ æƒï¼Œé€šè¿‡è®­ç»ƒæ•°æ®æ•°é‡ï¼Œåœ¨ä¸¤æ¡è·¯å¾„ä¸Šéƒ½æ‰¾åˆ°äº†è§£å†³æ–¹æ¡ˆï¼

![](img/8152951b8a3c920f03825528d98c5ae7.png)

é¢„æµ‹æ¡ˆä¾‹é‡åˆ°å·²åˆ é™¤ç‰¹å¾æ—¶ï¼Œé€šè¿‡åŠ æƒè®­ç»ƒæ•°æ®æ•°é‡æ¥åˆ¶ä½œä¸¤ä¸ªè·¯å¾„ã€‚

$$ f(x_2=60) = \frac{60}{100} \left[ \frac{15}{60} \times 20 + \frac{45}{60} \times 70 \right] + \frac{40}{100} \left[130\right] = 86.5 $$$$ f(x_2=60) = 86.5 $$

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = True                                      # toggle to supress warnings
import os                                                     # to set current working directory 
import math                                                   # square root operator
import numpy as np                                            # arrays and matrix math
import scipy.stats as st                                      # statistical methods
import pandas as pd                                           # DataFrames
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import seaborn as sns                                         # for matrix scatter plots
from sklearn import tree                                      # tree program from scikit learn (package for machine learning)
from sklearn.tree import _tree                                # for accessing tree information
from sklearn import metrics                                   # measures to check our models
from sklearn.preprocessing import StandardScaler              # standardize the features
from sklearn.tree import export_graphviz                      # graphical visualization of trees
from sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning
from sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline
from sklearn import metrics                                   # measures to check our models
from sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation
from sklearn.model_selection import train_test_split          # train and test split
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # supress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½å¿…é¡»é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥ â€˜python -m pip install [package-name]â€™ æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ›´å¤šå¸®åŠ©ï¼Œè¯·å‚é˜…å„è‡ªçš„åŒ…æ–‡æ¡£ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰å‡ ä¸ªå‡½æ•°æ¥ç®€åŒ–ç»˜åˆ¶ç›¸å…³çŸ©é˜µå’Œå†³ç­–æ ‘å›å½’æ¨¡å‹çš„å¯è§†åŒ–ã€‚

```py
def comma_format(x, pos):
    return f'{int(x):,}'

def feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot
    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1
    plt.plot(pred,metric,color='black',zorder=20)
    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)
    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  
    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)
    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();
    return

def plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix 
    my_colormap = plt.get_cmap('RdBu_r', 256)          
    newcolors = my_colormap(np.linspace(0, 1, 256))
    white = np.array([256/256, 256/256, 256/256, 1])
    white_low = int(128 - mask*128); white_high = int(128+mask*128)
    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)
    newcmp = ListedColormap(newcolors)
    m = corr_matrix.shape[0]
    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)
    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()
    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()
    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
    plt.colorbar(im, orientation = 'vertical')
    plt.title(title)
    for i in range(0,m):
        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')
        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')
    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def plot_CDF(data,color,alpha=1.0,lw=1,ls='solid',label='none'):
    cumprob = (np.linspace(1,len(data),len(data)))/(len(data)+1)
    plt.scatter(np.sort(data),cumprob,c=color,alpha=alpha,edgecolor='black',lw=lw,ls=ls,label=label,zorder=10)
    plt.plot(np.sort(data),cumprob,c=color,alpha=alpha,lw=lw,ls=ls,zorder=8)

def extract_rules(tree_model, feature_names):                 # recursive method to extract rules, from paulkernfeld Stack Overflow (?)
    rules = []
    def traverse(node, depth, prev_rule):
        if tree_model.tree_.children_left[node] == -1:        # Leaf node
            class_label = np.argmax(tree_model.tree_.value[node])
            rule = f"{prev_rule} => Class {class_label}"
            rules.append(rule)
        else:  # Split node
            feature = feature_names[tree_model.tree_.feature[node]]
            threshold = tree_model.tree_.threshold[node]
            left_child = tree_model.tree_.children_left[node]
            right_child = tree_model.tree_.children_right[node]
            traverse(left_child, depth + 1, f"{prev_rule} & {feature} <= {threshold}") # Recursively traverse left and right subtrees
            traverse(right_child, depth + 1, f"{prev_rule} & {feature} > {threshold}")
    traverse(0, 0, "Root")
    return rules

def plot_decision_tree_regions(tree_model, feature_names,X_min,X_max,annotate=True):
    rules = extract_rules(tree_model, feature_names)
    for irule, ____ in enumerate(rules):
        rule = rules[irule].split()[2:]
        X_min = Xmin[0]; X_max = Xmax[0]; Y_min = Xmin[1]; Y_max = Xmax[1];
        index = [i for i,val in enumerate(rule) if val==feature_names[0]]
        for i in index:
            if rule[i+1] == '<=':
                X_max = min(float(rule[i+2]),X_max)
            else:
                X_min = max(float(rule[i+2]),X_min)
        index = [i for i,val in enumerate(rule) if val==feature_names[1]]
        for i in index:
            if rule[i+1] == '<=':
                Y_max = min(float(rule[i+2]),Y_max)
            else:
                Y_min = max(float(rule[i+2]),Y_min) 
        plt.gca().add_patch(plt.Rectangle((X_min,Y_min),X_max-X_min,Y_max-Y_min, lw=2,ec='black',fc="none"))
        cx = (X_min + X_max)*0.5; cy = (Y_min + Y_max)*0.5; loc = np.array((cx,cy)).reshape(1, -1)
        if annotate == True:
            plt.annotate(text = str(f'{np.round(tree_model.predict(loc)[0],2):,.0f}'),xy=(cx,cy),ha='center',
                         weight='bold',c='white',zorder=100)

def visualize_tree_model(model,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,
                         ymax,title,Xname,yname,Xlabel,ylabel,annotate=True):# plots the data points and the decision tree prediction 
    cmap = plt.cm.inferno
    X1plot_step = (Xmax[0] - Xmin[0])/300.0; X2plot_step = -1*(Xmax[1] - Xmin[1])/300.0 # resolution of the model visualization
    XX1, XX2 = np.meshgrid(np.arange(Xmin[0], Xmax[0], X1plot_step), # set up the mesh
                     np.arange(Xmax[1], Xmin[1], X2plot_step))
    y_hat = model.predict(np.c_[XX1.ravel(), XX2.ravel()])    # predict with our trained model over the mesh
    y_hat = y_hat.reshape(XX1.shape)

    plt.imshow(y_hat,interpolation=None, aspect="auto", extent=[Xmin[0],Xmax[0],Xmin[1],Xmax[1]], 
        vmin=ymin,vmax=ymax,alpha = 0.2,cmap=cmap,zorder=1)
    sp = plt.scatter(X1_train,X2_train,s=None, c=y_train, marker='o', cmap=cmap, 
        norm=None, vmin=ymin, vmax=ymax, alpha=0.6, linewidths=0.3, edgecolors="black", label = 'Train',zorder=10)
    plt.scatter(X1_test,X2_test,s=None, c=y_test, marker='s', cmap=cmap, 
        norm=None, vmin=ymin, vmax=ymax, alpha=0.3, linewidths=0.3, edgecolors="black", label = 'Test',zorder=10)

    plot_decision_tree_regions(model,Xname,Xmin,Xmax,annotate)
    plt.title(title); plt.xlabel(Xlabel[0]); plt.ylabel(Xlabel[1])
    plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])
    cbar = plt.colorbar(sp, orientation = 'vertical')         # add the color bar
    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
    cbar.set_label(ylabel, rotation=270, labelpad=20)
    return y_hat

def check_tree_model(model,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,ymax,title): # plots the estimated vs. the actual 
    y_hat_train = model.predict(np.c_[X1_train,X2_train]); y_hat_test = model.predict(np.c_[X1_test,X2_test])

    df_cross = pd.DataFrame(np.c_[y_test,y_hat_test],columns=['y_test','y_hat_test'])
    df_cross_train = pd.DataFrame(np.c_[y_train,y_hat_train],columns=['y_train','y_hat_train'])

    plt.scatter(y_train,y_hat_train,s=15, c='blue',marker='o', cmap=None, norm=None, vmin=None, vmax=None, alpha=0.7, 
                linewidths=0.3, edgecolors="black",label='Train',zorder=10)
    plt.scatter(y_test,y_hat_test,s=15, c='red',marker='s', cmap=None, norm=None, vmin=None, vmax=None, alpha=0.7, 
                linewidths=0.3, edgecolors="black",label='Test',zorder=10)

    unique_y_hat_all = set(np.concatenate([y_hat_test,y_hat_train]))
    for y_hat in unique_y_hat_all:
        plt.plot([ymin,ymax],[y_hat,y_hat],c='black',alpha=0.2,ls='--',zorder=1)

    unique_y_hat_test = set(y_hat_test)
    for y_hat in unique_y_hat_test:
        #plt.plot([ymin,ymax],[y_hat,y_hat],c='black',alpha=0.3,ls='--',zorder=1)
        cond_mean_y_hat = df_cross.loc[df_cross['y_hat_test'] == y_hat, 'y_test'].mean()
        cond_P75_y_hat = df_cross.loc[df_cross['y_hat_test'] == y_hat, 'y_test'].quantile(0.75)
        cond_P25_y_hat = df_cross.loc[df_cross['y_hat_test'] == y_hat, 'y_test'].quantile(0.25)
        plt.scatter(cond_mean_y_hat,y_hat-0.02*(ymax-ymin),color='red',edgecolor='black',s=60,marker='^',zorder=100)
        plt.plot([cond_P25_y_hat,cond_P75_y_hat],[y_hat-0.025*(ymax-ymin),y_hat-0.025*(ymax-ymin)],c='black',lw=0.7)
        plt.plot([cond_P25_y_hat,cond_P25_y_hat],[y_hat-0.032*(ymax-ymin),y_hat-0.018*(ymax-ymin)],c='black',lw=0.7)
        plt.plot([cond_P75_y_hat,cond_P75_y_hat],[y_hat-0.032*(ymax-ymin),y_hat-0.018*(ymax-ymin)],c='black',lw=0.7)

    unique_y_hat_train = set(y_hat_train)
    for y_hat in unique_y_hat_train:
        #plt.plot([ymin,ymax],[y_hat,y_hat],c='black',alpha=0.3,ls='--',zorder=1)
        cond_mean_y_hat = df_cross_train.loc[df_cross_train['y_hat_train'] == y_hat, 'y_train'].mean()
        cond_P75_y_hat = df_cross_train.loc[df_cross_train['y_hat_train'] == y_hat, 'y_train'].quantile(0.75)
        cond_P25_y_hat = df_cross_train.loc[df_cross_train['y_hat_train'] == y_hat, 'y_train'].quantile(0.25)
        plt.scatter(cond_mean_y_hat,y_hat+0.02*(ymax-ymin),color='blue',edgecolor='black',s=60,marker='v',zorder=100)
        plt.plot([cond_P25_y_hat,cond_P75_y_hat],[y_hat+0.025*(ymax-ymin),y_hat+0.025*(ymax-ymin)],c='black',lw=0.7)
        plt.plot([cond_P25_y_hat,cond_P25_y_hat],[y_hat+0.032*(ymax-ymin),y_hat+0.018*(ymax-ymin)],c='black',lw=0.7)
        plt.plot([cond_P75_y_hat,cond_P75_y_hat],[y_hat+0.032*(ymax-ymin),y_hat+0.018*(ymax-ymin)],c='black',lw=0.7)

    plt.title(title); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')
    plt.xlim([ymin,ymax]); plt.ylim([ymin,ymax]); plt.legend(loc='upper left')
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks

    plt.arrow(ymin,ymin,ymax,ymax,width=0.02,color='black',head_length=0.0,head_width=0.0)
    MSE_train = metrics.mean_squared_error(y_train,y_hat_train); MSE_test = metrics.mean_squared_error(y_test,y_hat_test)
    plt.gca().add_patch(plt.Rectangle((ymin+0.6*(ymax-ymin),ymin+0.1*(ymax-ymin)),0.40*(ymax-ymin),0.12*(ymax-ymin),
        lw=0.5,ec='black',fc="white",zorder=100))
    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.annotate('MSE Testing:  ' + str(f'{np.round(MSE_test,2):,.0f}'),(ymin+0.62*(ymax-ymin),ymin+0.18*(ymax-ymin)),zorder=1000)
    plt.annotate('MSE Training: ' + str(f'{np.round(MSE_train,2):,.0f}'),(ymin+0.62*(ymax-ymin),ymin+0.12*(ymax-ymin)),zorder=1000)

def tree_tuning(node_max,cnode,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,ymax,title,seed):
    MSE_test_mat = np.zeros(node_max-1); MSE_train_mat = np.zeros(node_max-1);

    for imax_leaf_node, max_leaf_node in enumerate(range(2,node_max+1)):
        np.random.seed(seed = seed)
        tree_temp = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_node)
        tree_temp = tree_temp.fit(X_train.values, y_train.values)
        y_hat_train = tree_temp.predict(np.c_[X1_train,X2_train]); y_hat_test = tree_temp.predict(np.c_[X1_test,X2_test])  
        MSE_train_mat[imax_leaf_node] = metrics.mean_squared_error(y_train,y_hat_train)
        MSE_test_mat[imax_leaf_node] = metrics.mean_squared_error(y_test,y_hat_test)
        if max_leaf_node == cnode:
            plt.scatter(cnode,MSE_train_mat[imax_leaf_node],color='blue',edgecolor='black',s=20,marker='o',zorder=1000)
            plt.scatter(cnode,MSE_test_mat[imax_leaf_node],color='red',edgecolor='black',s=20,marker='o',zorder=1000)
    maxcheck = max(np.max(MSE_train_mat),np.max(MSE_test_mat))

    plt.vlines(cnode,0,maxcheck,color='black',ls='--',lw=1,zorder=1) 
    plt.plot(range(2,node_max+1),MSE_train_mat,color='blue',zorder=100,label='Train')
    plt.plot(range(2,node_max+1),MSE_test_mat,color='red',zorder=100,label='Test')

    plt.title(title); plt.xlabel('Maximum Number of Leaf Nodes'); plt.ylabel('Means Square Error')
    plt.xlim([0,node_max]); plt.ylim([0,maxcheck]); plt.legend(loc='upper right')
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def tree_to_code(tree, feature_names):                        # code from StackOverFlow by paulkernfeld
    tree_ = tree.tree_                                        # convert tree object to portable code to use anywhere
    feature_name = [
        feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
        for i in tree_.feature
    ]
    print("def tree({}):".format(", ".join(feature_names)))

    def recurse(node, depth):
        indent = "  " * depth
        if tree_.feature[node] != _tree.TREE_UNDEFINED:
            name = feature_name[node]
            threshold = tree_.threshold[node]
            print("{}if {} <= {}:".format(indent, name, threshold))
            recurse(tree_.children_left[node], depth + 1)
            print("{}elif {} > {}".format(indent, name, threshold))
            recurse(tree_.children_right[node], depth + 1)
        else:
            print("{}return {}".format(indent, tree_.value[node]))
    recurse(0, 1) 

def get_lineage(tree, feature_names):                         # code from StackOverFlow by Zelanzny7
    left      = tree.tree_.children_left                      # track the decision path for any set of inputs
    right     = tree.tree_.children_right
    threshold = tree.tree_.threshold
    features  = [feature_names[i] for i in tree.tree_.feature]
    # get ids of child nodes
    idx = np.argwhere(left == -1)[:,0]     
    def recurse(left, right, child, lineage=None):          
        if lineage is None:
            lineage = [child]
        if child in left:
            parent = np.where(left == child)[0].item()
            split = 'l'
        else:
            parent = np.where(right == child)[0].item()
            split = 'r'
        lineage.append((parent, split, threshold[parent], features[parent]))
        if parent == 0:
            lineage.reverse()
            return lineage
        else:
            return recurse(left, right, parent, lineage)
    for child in idx:
        for node in recurse(left, right, child):
            print(node) 

def display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)
    html_str = ''
    for df in args:
        html_str += df.head().to_html()  # Using .head() for the first few rows
    display(HTML(f'<div style="display: flex;">{html_str}</div>')) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆæ¯æ¬¡éƒ½é¿å…åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/PGE383")                                        # set the working directory 
```

æ‚¨å°†ä¸å¾—ä¸æ›´æ–°å¼•å·ä¸­çš„éƒ¨åˆ†ä»¥åŒ…å«æ‚¨è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”æ ¼å¼åœ¨ Mac ä¸Šä¸åŒï¼ˆä¾‹å¦‚ï¼Œâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½æ•°æ®

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›† [unconv_MV.csv](https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv)ï¼Œå®ƒåœ¨æˆ‘çš„ GeoDataSet ä»“åº“ä¸­å¯ç”¨ã€‚å®ƒæ˜¯ä¸€ä¸ªé€—å·åˆ†éš”çš„æ–‡ä»¶ï¼ŒåŒ…å«ï¼š

+   äº•æŒ‡æ•°ï¼ˆæ•´æ•°ï¼‰

+   å­”éš™ç‡ï¼ˆ%ï¼‰

+   æ¸—é€ç‡ ($mD$)

+   å£°æ³¢é˜»æŠ— ($\frac{kg}{mÂ³} \cdot \frac{m}{s} \cdot 10â¶$)

+   å²©è„†æ€§ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   ç»ç’ƒè´¨åå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹æ°”ä½“äº§é‡ï¼ˆ90 å¤©å¹³å‡ï¼‰(MCFPD)

æˆ‘ä»¬ä½¿ç”¨ pandas çš„ â€˜read_csvâ€™ å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸º â€˜dfâ€™ çš„æ•°æ®æ¡†ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

**Python å°è´´å£«ï¼šä½¿ç”¨åŒ…ä¸­çš„å‡½æ•°**åªéœ€è¾“å…¥æˆ‘ä»¬åœ¨å¼€å¤´å£°æ˜çš„åŒ…çš„æ ‡ç­¾ï¼š

```py
import pandas as pd 
```

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‘½ä»¤è®¿é—® pandas å‡½æ•° â€˜read_csvâ€™ï¼š

```py
pd.read_csv() 
```

ä½†æ˜¯ï¼Œread csv éœ€è¦è¾“å…¥å‚æ•°ã€‚æœ€é‡è¦çš„æ˜¯æ–‡ä»¶çš„åç§°ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–é»˜è®¤å‚æ•°éƒ½å¾ˆå¥½ã€‚å¦‚æœæ‚¨æƒ³æŸ¥çœ‹æ­¤å‡½æ•°çš„æ‰€æœ‰å¯èƒ½å‚æ•°ï¼Œè¯·è®¿é—®[æ­¤å¤„](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)çš„æ–‡æ¡£ã€‚

+   æ–‡æ¡£æ€»æ˜¯å¾ˆæœ‰å¸®åŠ©

+   Python å‡½æ•°é€šå¸¸æœ‰å¾ˆå¤šçµæ´»æ€§ï¼Œè¿™å¾—ç›Šäºä½¿ç”¨å„ç§è¾“å…¥å‚æ•°

æ­¤å¤–ï¼Œç¨‹åºè¿˜æœ‰ä¸€ä¸ªè¾“å‡ºï¼Œä¸€ä¸ªä»æ•°æ®åŠ è½½çš„ pandas DataFrameã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šä»£è¡¨è¯¥æ–°å¯¹è±¡çš„åç§°/å˜é‡ã€‚

```py
df = pd.read_csv("unconv_MV.csv") 
```

è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªå‘½ä»¤æ¥åŠ è½½æ•°æ®ï¼Œç„¶åè¿è¡Œè¿™ä¸ªå‘½ä»¤æ¥æå–æ•°æ®çš„ä¸€ä¸ªéšæœºå­é›†ã€‚

```py
df = df.sample(frac=.30, random_state = 73073); 
df = df.reset_index() 
```

## ç‰¹å¾å·¥ç¨‹

è®©æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œä¸€äº›ä¿®æ”¹ä»¥æ”¹è¿›å·¥ä½œæµç¨‹ï¼š

+   **é€‰æ‹©é¢„æµ‹ç‰¹å¾ï¼ˆx2ï¼‰å’Œå“åº”ç‰¹å¾ï¼ˆx1ï¼‰**ï¼Œç¡®ä¿å…ƒæ•°æ®ä¹Ÿä¸€è‡´ã€‚

+   **å…ƒæ•°æ®**ç¼–ç ï¼Œä¾‹å¦‚æ¯ä¸ªç‰¹å¾çš„å•ä½ã€æ ‡ç­¾å’Œæ˜¾ç¤ºèŒƒå›´ã€‚

+   **å‡å°‘æ•°æ®æ•°é‡**ä»¥æ–¹ä¾¿å¯è§†åŒ–ï¼ˆå¦‚æœå›¾è¡¨ä¸Šç‚¹å¤ªå¤šï¼Œåˆ™éš¾ä»¥çœ‹æ¸…ï¼‰ã€‚

+   **è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²**ä»¥æ¼”ç¤ºå’Œå¯è§†åŒ–ç®€å•çš„è¶…å‚æ•°è°ƒæ•´ã€‚

+   **å‘æ•°æ®æ·»åŠ éšæœºå™ªå£°**ä»¥æ¼”ç¤ºæ¨¡å‹è¿‡æ‹Ÿåˆã€‚åŸå§‹æ•°æ®æ˜¯æ— é”™è¯¯çš„ï¼Œå¹¶ä¸”ä¸èƒ½å¾ˆå¥½åœ°å±•ç¤ºè¿‡æ‹Ÿåˆã€‚

ç»™å®šè¿™æ˜¯æ­£ç¡®è®¾ç½®çš„ï¼Œåº”è¯¥èƒ½å¤Ÿä½¿ç”¨ä»»ä½•æ•°æ®é›†å’Œç‰¹å¾è¿›è¡Œæ­¤æ¼”ç¤ºã€‚

+   ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬è¿™é‡Œä¸å±•ç¤ºä»»ä½•ç‰¹å¾é€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œå‰ä¸€ç« ä¸­çš„ k-æœ€è¿‘é‚»åŒ…æ‹¬ä¸€äº›ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œä½†è¯·å‚é˜…ç‰¹å¾é€‰æ‹©ç« èŠ‚ï¼Œä»¥äº†è§£è®¸å¤šå¯èƒ½çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•åŠå…¶ä»£ç ã€‚

## å¯é€‰ï¼šå‘å“åº”ç‰¹å¾æ·»åŠ éšæœºå™ªå£°

æˆ‘ä»¬å¯ä»¥è¿™æ ·åšæ¥è§‚å¯Ÿæ•°æ®å™ªå£°å¯¹è¿‡æ‹Ÿåˆå’Œè¶…å‚æ•°è°ƒæ•´çš„å½±å“ã€‚

+   è¿™æ˜¯ä¸ºäº†ç»éªŒå­¦ä¹ ï¼Œå½“ç„¶æˆ‘ä»¬ä¸ä¼šå‘æˆ‘ä»¬çš„æ•°æ®æ·»åŠ éšæœºå™ªå£°

+   æˆ‘ä»¬è®¾ç½®äº†éšæœºæ•°ç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§

```py
add_error = True                                              # add random error to the response feature
std_error = 500                                               # standard deviation of random error, for demonstration only
idata = 2

if idata == 1:
    df_load = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv") # load the data from my github repo
    df_load = df_load.sample(frac=.30, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data

elif idata == 2:
    df_load = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv") # load the data 
    df_load = df_load.sample(frac=.70, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data
    df_load = df_load.rename(columns={"Prod": "Production"})

yname = 'Production'; Xname = ['Por','Brittle']               # specify the predictor features (x2) and response feature (x1)
Xmin = [5.0,0.0]; Xmax = [25.0,100.0]                         # set minimums and maximums for visualization 
ymin = 1000.0; ymax = 9000.0
Xlabel = ['Porosity','Brittleness']; ylabel = 'Production'    # specify the feature labels for plotting
Xunit = ['%','%']; yunit = 'MCFPD'
Xlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']
ylabelunit = ylabel + ' (' + yunit + ')'

if add_error == True:                                         # method to add error
    np.random.seed(seed=seed)                                 # set random number seed
    df_load[yname] = df_load[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df_load)) # add noise
    values = df_load._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray

y = pd.DataFrame(df_load[yname])                              # extract selected features as X and y DataFrames
X = df_load[Xname]
df = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) 
```

è®©æˆ‘ä»¬ç¡®ä¿æˆ‘ä»¬å·²é€‰æ‹©äº†åˆç†çš„ç‰¹å¾æ¥æ„å»ºæ¨¡å‹

+   ä¸¤ä¸ªé¢„æµ‹ç‰¹å¾ä¸å…±çº¿æ€§ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´é¢„æµ‹æ¨¡å‹ä¸ç¨³å®š

+   æ¯ä¸ªç‰¹å¾éƒ½ä¸å“åº”ç‰¹å¾ç›¸å…³ï¼Œé¢„æµ‹ç‰¹å¾é€šçŸ¥å“åº”

## è®¡ç®—ç›¸å…³çŸ©é˜µå’Œç›¸å…³å“åº”æ’å

è®©æˆ‘ä»¬ä»ç›¸å…³æ€§åˆ†æå¼€å§‹ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¹‹å‰å£°æ˜çš„å‡½æ•°è®¡ç®—å¹¶æŸ¥çœ‹ç›¸å…³çŸ©é˜µå’Œå“åº”ç‰¹å¾çš„ç›¸å…³æ€§ã€‚

+   ç›¸å…³æ€§åˆ†æåŸºäºçº¿æ€§å…³ç³»çš„å‡è®¾ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªè‰¯å¥½çš„èµ·ç‚¹

```py
corr_matrix = df.corr()
correlation = corr_matrix.iloc[:,-1].values[:-1]

plt.subplot(121)
plot_corr(corr_matrix,'Correlation Matrix',1.0,0.1)           # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(Xname,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + yname,'Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![_images/152d837d72f43ba9a48527a444d81b3bbc73a2ba553d2760d27f5c20206ea0b2.png](img/fe078f42023f81da1972474b1d3bbf26.png)

æ³¨æ„ç”±äºæ¯ä¸ªå˜é‡ä¸å…¶è‡ªèº«ç›¸å…³è€Œäº§ç”Ÿçš„ 1.0 å¯¹è§’çº¿ã€‚

è¿™çœ‹èµ·æ¥ä¸é”™ã€‚å­˜åœ¨ä¸åŒç¨‹åº¦çš„ç›¸å…³æ€§ã€‚å½“ç„¶ï¼Œç›¸å…³ç³»æ•°ä»…é™äºçº¿æ€§ç›¸å…³ç¨‹åº¦ã€‚

+   è®©æˆ‘ä»¬çœ‹çœ‹çŸ©é˜µæ•£ç‚¹å›¾ï¼Œä»¥äº†è§£ç‰¹å¾ä¹‹é—´çš„æˆå¯¹å…³ç³»ã€‚

```py
pairgrid = sns.PairGrid(df,vars=Xname+[yname])                # matrix scatter plots
pairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)
pairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle
pairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, 
                              alpha = 1.0, n_levels = 10)
pairgrid.add_legend()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/8d03fa748bcc76aea92c8e57b5fb8071473e84b910d1402f5fd62dd21b102145.png](img/515a70a53d49c49c9ecf98249cd67b5d.png)

## è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²

ä¸ºäº†æ–¹ä¾¿å’Œç®€å•ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn çš„éšæœºè®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²ã€‚

```py
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split
df_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)
df_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) 
```

## å¯è§†åŒ– DataFrame

åœ¨æˆ‘ä»¬æ„å»ºæ¨¡å‹ä¹‹å‰ï¼Œå¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯• DataFrame æ˜¯æœ‰ç”¨çš„æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ©ç”¨â€˜headâ€™ DataFrame æˆå‘˜å‡½æ•°ï¼ˆæ ¼å¼æ•´æ´ã€ç¾è§‚ï¼Œè§ä¸‹æ–‡ï¼‰æ¥é¢„è§ˆã€‚

```py
print('       Training DataFrame          Testing DataFrame')
display_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display 
```

```py
 Training DataFrame          Testing DataFrame 
```

|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| 86 | 12.83 | 29.87 | 2089.258307 |
| 35 | 17.39 | 56.43 | 5803.596379 |
| 75 | 12.23 | 40.67 | 3511.348151 |
| 36 | 13.72 | 40.24 | 4004.849870 |
| 126 | 12.83 | 17.20 | 2712.836372 |
|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| 5 | 15.55 | 58.25 | 5353.761093 |
| 46 | 20.21 | 23.78 | 4387.577571 |
| 96 | 15.07 | 39.39 | 4412.135054 |
| 45 | 12.10 | 63.24 | 3654.779704 |
| 105 | 19.54 | 37.40 | 5251.551624 |

## è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ã€‚

æœ‰å¾ˆå¤šæœ‰æ•ˆçš„æ–¹æ³•å¯ä»¥ä» DataFrame ä¸­çš„è¡¨æ ¼æ•°æ®è®¡ç®—æ‘˜è¦ç»Ÿè®¡ã€‚

+   describe å‘½ä»¤æä¾›äº†ä¸€ä¸ªç¾è§‚çš„æ•°æ®è¡¨ï¼Œæä¾›äº†è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼å’Œæœ€å¤§å€¼ã€‚

```py
print('            Training DataFrame                      Testing DataFrame')    # custom function for side-by-side summary statistics
display_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) 
```

```py
 Training DataFrame                      Testing DataFrame 
```

|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| è®¡æ•° | 105.000000 | 105.000000 | 105.000000 |
| å¹³å‡å€¼ | 14.859238 | 48.861143 | 4238.554591 |
| æ ‡å‡†å·® | 3.057228 | 14.432050 | 1087.707113 |
| æœ€å°å€¼ | 7.220000 | 10.940000 | 1517.373571 |
| æœ€å¤§å€¼ | 23.550000 | 84.330000 | 6907.632261 |
|  | Por | Brittle | Production |
| --- | --- | --- | --- |
| è®¡æ•° | 35.000000 | 35.000000 | 35.000000 |
| å¹³å‡å€¼ | 15.011714 | 46.798286 | 4378.913131 |
| æ ‡å‡†å·® | 3.574467 | 13.380910 | 1290.216113 |
| æœ€å°å€¼ | 6.550000 | 20.120000 | 1846.027145 |
| æœ€å¤§å€¼ | 20.860000 | 68.760000 | 6593.447893 |

æ£€æŸ¥æ‘˜è¦ç»Ÿè®¡æ˜¯ä»¶å¥½äº‹ã€‚

+   æ²¡æœ‰æ˜æ˜¾çš„å¼‚å¸¸ã€‚

+   æ£€æŸ¥æ¯ä¸ªç‰¹å¾å€¼çš„èŒƒå›´ï¼Œä»¥è®¾ç½®å’Œè°ƒæ•´ç»˜å›¾é™åˆ¶ã€‚è§ä¸Šæ–‡ã€‚

## å¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²ã€‚

è®©æˆ‘ä»¬é€šè¿‡ç›´æ–¹å›¾å’Œæ•£ç‚¹å›¾æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•çš„ä¸€è‡´æ€§å’Œè¦†ç›–ç‡ã€‚

+   æ£€æŸ¥ä»¥ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•æ¶µç›–äº†å¯èƒ½çš„ç‰¹å¾ç»„åˆèŒƒå›´ã€‚

+   ç¡®ä¿æµ‹è¯•æ¡ˆä¾‹ä¸ä¼šè¶…å‡ºè®­ç»ƒæ•°æ®çš„èŒƒå›´è¿›è¡Œå¤–æ¨ã€‚

```py
nbins = 20                                                    # number of histogram bins

plt.subplot(221)                                              # predictor feature #1 histogram
freq1,_,_ = plt.hist(x=df_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=df_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  
plt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   

plt.subplot(222)                                              # predictor feature #2 histogram
freq1,_,_ = plt.hist(x=df_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=df_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  
plt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   

plt.subplot(223)                                              # predictor features #1 and #2 scatter plot
plt.scatter(df_train[Xname[0]],df_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(df_test[Xname[0]],df_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.title(Xlabel[0] + ' vs ' +  Xlabel[1])
plt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])
plt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![å›¾ç‰‡](img/3e84676c9f7f37dcabed4194a238047f.png)

æœ‰æ—¶æˆ‘å‘ç°é€šè¿‡æŸ¥çœ‹ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰è€Œä¸æ˜¯ç›´æ–¹å›¾æ¥æ¯”è¾ƒåˆ†å¸ƒæ›´æ–¹ä¾¿ã€‚

+   æˆ‘ä»¬é¿å…é€‰æ‹©ç›´æ–¹å›¾æŸ±çŠ¶å¤§å°çš„ä»»æ„æ€§ï¼Œå› ä¸ºç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰ä¸æ•°æ®åˆ†è¾¨ç‡ä¸€è‡´ã€‚

```py
plt.subplot(221)                                              # predictor feature #1 CDF
plot_CDF(X_train[Xname[0]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(X_test[Xname[0]],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(Xlabelunit[0]); plt.xlim(Xmin[0],Xmax[0]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(Xlabel[0] + ' Train and Test CDFs')

plt.subplot(222)                                              # predictor feature #2 CDF
plot_CDF(X_train[Xname[1]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(X_test[Xname[1]],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(Xlabelunit[1]); plt.xlim(Xmin[1],Xmax[1]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(Xlabel[1] + ' Train and Test CDFs')

plt.subplot(223)                                              # response feature CDF
plot_CDF(y_train[yname],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(y_test[yname],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(ylabelunit); plt.xlim(ymin,ymax); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(ylabel + ' Train and Test CDFs')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![å›¾ç‰‡](img/89d70d18d70c546bb0ac8c55112e894e.png)

å†æ¬¡å¼ºè°ƒï¼Œåˆ†å¸ƒæƒ…å†µè‰¯å¥½ï¼Œ

+   æˆ‘ä»¬æ— æ³•è§‚å¯Ÿåˆ°æ˜æ˜¾çš„é—´éš™æˆ–æˆªæ–­ã€‚

+   æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„è¦†ç›–ç‡ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹å­”éš™ç‡ä¸è„†æ€§ä¹‹é—´çš„æ•£ç‚¹å›¾ï¼Œç‚¹æ ¹æ®ç”Ÿäº§é‡ç€è‰²ã€‚

```py
plt.subplot(111)                                              # visualize the train and test data in predictor feature space
im = plt.scatter(X_train[Xname[0]],X_train[Xname[1]],s=None, c=y_train[yname], marker='o', cmap=cmap, 
    norm=None, vmin=ymin, vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors="black", label = 'Train')
plt.scatter(X_test[Xname[0]],X_test[Xname[1]],s=None, c=y_test[yname], marker='s', cmap=cmap, 
    norm=None, vmin=ymin, vmax=ymax, alpha=0.5, linewidths=0.3, edgecolors="black", label = 'Test')
plt.title('Training ' + ylabel + ' vs. ' + Xlabel[1] + ' and ' + Xlabel[0]); 
plt.xlabel(Xlabel[0] + ' (' + Xunit[0] + ')'); plt.ylabel(Xlabel[1] + ' (' + Xunit[1] + ')')
plt.xlim(Xmin[0],Xmax[0]); plt.ylim(Xmin[1],Xmax[1]); plt.legend(loc = 'upper right'); add_grid()
cbar = plt.colorbar(im, orientation = 'vertical')
cbar.set_label(ylabel + ' (' + yunit + ')', rotation=270, labelpad=20)
cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/3ec3d66453c69d41e8b4c7a2508530a0.png)

è¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥å¾ˆå¤æ‚ï¼Œæ— æ³•ç”¨ç®€å•çš„çº¿æ€§å›å½’å»ºæ¨¡ã€‚ä¼¼ä¹å­˜åœ¨éçº¿æ€§ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€å•çš„éå‚æ•°æ¨¡å‹ï¼Œå†³ç­–æ ‘ã€‚

## ä½¿ç”¨ scikit-learn å®ä¾‹åŒ–ã€æ‹Ÿåˆå’Œé¢„æµ‹ã€‚

è®©æˆ‘ä»¬é€šè¿‡å®ä¾‹åŒ–ã€æ‹Ÿåˆå’Œé¢„æµ‹æ¥æ„å»ºæˆ‘ä»¬çš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨ scikit-learnã€‚

+   **å®ä¾‹åŒ–**æ¨¡å‹å¯¹è±¡ï¼Œä½¿ç”¨è¶…å‚æ•°ï¼Œk-æœ€è¿‘é‚»ã€‚

+   **æ‹Ÿåˆ**é€šè¿‡ä½¿ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆå‘˜å‡½æ•° fitã€‚

+   **é¢„æµ‹**ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚è¿è¡Œ fit åï¼Œpredict å¯ç”¨äºè¿›è¡Œé¢„æµ‹ã€‚

## è®­ç»ƒå†³ç­–æ ‘ï¼ˆå›å½’æ ‘ï¼‰ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½è¿è¡Œ DecisionTreeRegressor å‘½ä»¤æ¥æ„å»ºæˆ‘ä»¬çš„å›å½’æ ‘ï¼Œä»¥é¢„æµ‹æˆ‘ä»¬çš„å“åº”ç‰¹å¾ï¼Œç»™å®šæˆ‘ä»¬çš„ 2 ä¸ªé¢„æµ‹ç‰¹å¾ï¼ˆè®°ä½ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œé™åˆ¶è‡ªå·±ä½¿ç”¨ 2 ä¸ªé¢„æµ‹ç‰¹å¾ä»¥ç®€åŒ–å¯è§†åŒ–ï¼‰ã€‚

+   æˆ‘ä»¬å°†ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„ä¸¤ä¸ªå‡½æ•°æ¥å¯è§†åŒ–å†³ç­–æ ‘åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„é¢„æµ‹ä»¥åŠè®­ç»ƒæ•°æ®çš„å®é™…å’Œä¼°è®¡ç”Ÿäº§çš„äº¤å‰å›¾ï¼Œä»¥åŠæ¥è‡ª sklearn.metric æ¨¡å—çš„ä¸‰ç§æ¨¡å‹åº¦é‡ã€‚

**è¶…å‚æ•°** - æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼é™åˆ¶æ ‘çš„å¤æ‚æ€§ï¼š

+   *max_leaf_nodes* - æœ€å¤§åŒºåŸŸæ•°ï¼Œä¹Ÿç§°ä¸ºå†³ç­–æ ‘ä¸­çš„ç»ˆç«¯æˆ–é¢†å…ˆèŠ‚ç‚¹

+   *max_depth* - æœ€å¤§å±‚æ•°ï¼Œä¾‹å¦‚ï¼Œmax_depth = 1 æ˜¯ä¸€ä¸ªåªæœ‰ 1 ä¸ªå†³ç­–å’Œä¸¤ä¸ªåŒºåŸŸçš„æ ‘æ¡©æ ‘

+   *min_samples_leaf* - æ–°åŒºåŸŸä¸­çš„æœ€å°æ•°æ®é‡ï¼Œæ˜¯ç¡®ä¿æ¯ä¸ªåŒºåŸŸæœ‰è¶³å¤Ÿæ•°æ®åšå‡ºåˆç†ä¼°è®¡çš„è‰¯å¥½çº¦æŸ

ç›®å‰ï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€äº›è¶…å‚æ•°ã€‚

### æ¬ æ‹Ÿåˆå†³ç­–æ ‘æ¨¡å‹

è®©æˆ‘ä»¬ä½¿ç”¨å¤ªå°‘çš„åŒºåŸŸï¼Œè®¾ç½® max_leaf_nodes å¤ªå°ï¼Œçœ‹çœ‹ç»“æœå†³ç­–æ ‘æ¨¡å‹ã€‚

```py
max_leaf_nodes = 5; max_depth =99; min_samples_leaf = 1      # hyperparameters

tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes,max_depth = max_depth,min_samples_leaf = min_samples_leaf)
tree_model = tree_model.fit(X_train.values, y_train.values)

plt.subplot(121)                                              # visualize, data, and decision tree regions and predictions
visualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model',Xname,yname,Xlabelunit,ylabelunit) 

plt.subplot(122)                                              # cross validation with conditional statistics plot
check_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot',)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/be259c4cb62524490823c4f4d2d09c0c.png)

è¿™ä¸ªæ¨¡å‹éå¸¸æ¬ æ‹Ÿåˆï¼Œå®ƒå¤ªç®€å•äº†ï¼Œæ— æ³•æ‹Ÿåˆé¢„æµ‹é—®é¢˜çš„å½¢çŠ¶ã€‚ä»¥ä¸‹æ˜¯å…³äºå›¾è¡¨çš„ä¸€äº›æ›´å¤šä¿¡æ¯ã€‚

è¯·çœ‹ä¼°è®¡ç”Ÿäº§ä¸å®é™…ç”Ÿäº§å¯¹æ¯”å›¾ï¼ˆåº•éƒ¨å›¾è¡¨ï¼‰ä¸Šçš„æ°´å¹³çº¿ï¼Ÿ

+   è¿™æ˜¯å¯ä»¥é¢„æ–™çš„ï¼Œå› ä¸ºå›å½’æ ‘ä½¿ç”¨ç‰¹å¾ç©ºé—´æ¯ä¸ªåŒºåŸŸï¼ˆç»ˆç«¯èŠ‚ç‚¹ï¼‰çš„æ•°æ®çš„å¹³å‡å€¼è¿›è¡Œä¼°è®¡ã€‚

+   ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘åŒ…æ‹¬äº†æ¯ä¸ªå¶èŠ‚ç‚¹ã€åŒºåŸŸçš„å®é™…å“åº” P10ã€å¹³å‡å€¼å’Œ P90ï¼Œå¯¹äºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚

+   æ¬ æ‹Ÿåˆçš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­å‡†ç¡®åº¦éƒ½å·®ã€‚

å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæ›´å¤æ‚çš„æ ‘ï¼Œæœ‰æ›´å¤šçš„ç»ˆç«¯èŠ‚ç‚¹ï¼Œé‚£ä¹ˆå°±ä¼šæœ‰æ›´å¤šçš„çº¿ã€‚

### è¿‡åº¦æ‹Ÿåˆå†³ç­–æ ‘æ¨¡å‹

è®©æˆ‘ä»¬ä½¿ç”¨å¤ªå¤šçš„åŒºåŸŸï¼Œè®¾ç½® max_leaf_nodes å¤ªå¤§ï¼Œçœ‹çœ‹ç»“æœå†³ç­–æ ‘æ¨¡å‹ã€‚

```py
max_leaf_nodes = 50; max_depth = 9; min_samples_leaf = 1     # hyperparameters

tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes,max_depth = max_depth,min_samples_leaf = min_samples_leaf)
tree_model = tree_model.fit(X_train.values, y_train.values)

plt.subplot(121)                                              # visualize, data, and decision tree regions and predictions
visualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model',Xname,yname,Xlabelunit,ylabelunit) 

plt.subplot(122)                                              # cross validation with conditional statistics plot
check_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot',)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/c28519e97623d45a6f72540a6b408c6d.png)

ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªæ¬ æ‹Ÿåˆçš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

+   è¿‡å¤šçš„å¤æ‚æ€§å’Œçµæ´»æ€§

+   æˆ‘ä»¬æ­£åœ¨æ‹Ÿåˆæ•°æ®ä¸­çš„å™ªå£°

+   è®­ç»ƒæ—¶å‡†ç¡®åº¦è‰¯å¥½ï¼Œä½†æµ‹è¯•æ—¶å‡†ç¡®åº¦å·®

éšç€æˆ‘ä»¬é€æ­¥æ·»åŠ ç»ˆç«¯èŠ‚ç‚¹ï¼Œè§‚å¯Ÿå†³ç­–æ ‘æ¨¡å‹åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„è¡¨ç°æ˜¯æœ‰æ•™è‚²æ„ä¹‰çš„ã€‚æˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°å›¾å½¢åŒ–åœ°è§‚å¯Ÿåˆ°åˆ†å±‚äºŒåˆ†åˆ†å‰²ã€‚

+   è®©æˆ‘ä»¬ä»ç®€å•çš„å¤æ‚æ¨¡å‹å¼€å§‹å¯è§†åŒ–ã€‚

```py
leaf_nodes_list = [2,3,4,10,20,100]

for inode,leaf_nodes in enumerate(leaf_nodes_list):

    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes)
    tree_model = tree_model.fit(X_train.values, y_train.values)

    plt.subplot(3,2,inode+1)                                         # visualize, data, and decision tree regions and predictions
    visualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],1000,9000,'Decision Tree Model, Number of Leaf Nodes: ' + str(leaf_nodes),Xname,yname,Xlabelunit,ylabelunit,annotate=False)   

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=3.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/43008585fa66e722cde17f42284274b4.png)

å¯èƒ½æœ‰å¿…è¦å¹¶æ’æŸ¥çœ‹å†³ç­–æ ‘æ¨¡å‹å’Œç›¸å…³å†³ç­–æ ‘ã€‚

```py
leaf_nodes_viz = 2

tree_model_viz = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes_viz).fit(X_train.values, y_train.values)

fig = plt.figure(figsize=(10, 6))
gs = fig.add_gridspec(1, 2, width_ratios=[1, 2])  # 1 row, 3 columns with 1:2 width ratio

ax1 = fig.add_subplot(gs[0])                         # visualize, data, and decision tree regions and predictions 
visualize_tree_model(tree_model_viz,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],1000,9000,'Decision Tree Model, Number of Leaf Nodes: ' + str(leaf_nodes),Xname,yname,
        Xlabelunit,ylabelunit,annotate=False)   

ax2 = fig.add_subplot(gs[1:])                                  # visualize, data, and decision tree regions and predictions
_ = tree.plot_tree(tree_model_viz,ax = ax2,feature_names=list(Xname),class_names=list(yname),filled=False,label='none',rounded=True,precision=0,
                  proportion=True,max_depth=4,fontsize=15)

plt.tight_layout()
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/de575bff5cbcf3e18e0bc1c385400e3e.png)

æˆ‘ä»¬å¦‚ä½•æ‰¾åˆ°æœ€ä½³çš„è¶…å‚æ•°ï¼Œä»¥è·å¾—æœ€ä½³å¤æ‚æ€§å’Œæµ‹è¯•é¢„æµ‹å‡†ç¡®æ€§çš„æœ€ä½³å€¼ï¼Ÿè¿™å°±æ˜¯è¶…å‚æ•°è°ƒæ•´ã€‚

### æ¬ æ‹Ÿåˆçš„å†³ç­–æ ‘æ¨¡å‹

è®©æˆ‘ä»¬ä½¿ç”¨å¤ªå°‘çš„åŒºåŸŸï¼Œè®¾ç½® max_leaf_nodes å¤ªå°ï¼Œçœ‹çœ‹ç»“æœå†³ç­–æ ‘æ¨¡å‹ã€‚

```py
max_leaf_nodes = 5; max_depth =99; min_samples_leaf = 1      # hyperparameters

tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes,max_depth = max_depth,min_samples_leaf = min_samples_leaf)
tree_model = tree_model.fit(X_train.values, y_train.values)

plt.subplot(121)                                              # visualize, data, and decision tree regions and predictions
visualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model',Xname,yname,Xlabelunit,ylabelunit) 

plt.subplot(122)                                              # cross validation with conditional statistics plot
check_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot',)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/be259c4cb62524490823c4f4d2d09c0c.png)

è¿™ä¸ªæ¨¡å‹éå¸¸æ¬ æ‹Ÿåˆï¼Œå®ƒå¤ªç®€å•äº†ï¼Œæ— æ³•æ‹Ÿåˆé¢„æµ‹é—®é¢˜çš„å½¢çŠ¶ã€‚ä»¥ä¸‹æ˜¯å›¾è¡¨ä¸Šçš„æ›´å¤šä¿¡æ¯ã€‚

çœ‹çœ‹ä¼°è®¡å€¼ä¸å®é™…ç”Ÿäº§ï¼ˆåº•éƒ¨å›¾ä¸Šçš„å›¾ï¼‰çš„å¯¹æ¯”å›¾ä¸Šçš„æ°´å¹³çº¿ï¼Ÿ

+   è¿™æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºå›å½’æ ‘ä½¿ç”¨ç‰¹å¾ç©ºé—´ä¸­æ¯ä¸ªåŒºåŸŸçš„å¹³å‡æ•°æ®ï¼ˆç»ˆç«¯èŠ‚ç‚¹ï¼‰è¿›è¡Œä¼°è®¡ã€‚

+   ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘åŒ…æ‹¬äº†æ¯ä¸ªå¶èŠ‚ç‚¹ã€åŒºåŸŸåœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­çš„å®é™…å“åº” P10ã€å¹³å‡å€¼å’Œ P90ã€‚

+   æ¬ æ‹Ÿåˆçš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­å‡†ç¡®æ€§å·®ã€‚

å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæ›´å¤æ‚çš„æ ‘ï¼Œæœ‰æ›´å¤šçš„ç»ˆç«¯èŠ‚ç‚¹ï¼Œé‚£ä¹ˆå°±ä¼šæœ‰æ›´å¤šçš„çº¿ã€‚

### è¿‡æ‹Ÿåˆçš„å†³ç­–æ ‘æ¨¡å‹

è®©æˆ‘ä»¬ä½¿ç”¨å¤ªå¤šçš„åŒºåŸŸï¼Œè®¾ç½® max_leaf_nodes å¤ªå¤§ï¼Œçœ‹çœ‹ç»“æœå†³ç­–æ ‘æ¨¡å‹ã€‚

```py
max_leaf_nodes = 50; max_depth = 9; min_samples_leaf = 1     # hyperparameters

tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes,max_depth = max_depth,min_samples_leaf = min_samples_leaf)
tree_model = tree_model.fit(X_train.values, y_train.values)

plt.subplot(121)                                              # visualize, data, and decision tree regions and predictions
visualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model',Xname,yname,Xlabelunit,ylabelunit) 

plt.subplot(122)                                              # cross validation with conditional statistics plot
check_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot',)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/c28519e97623d45a6f72540a6b408c6d.png)

ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªè¿‡åº¦æ‹Ÿåˆçš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

+   è¿‡å¤šçš„å¤æ‚æ€§å’Œçµæ´»æ€§

+   æˆ‘ä»¬æ­£åœ¨æ‹Ÿåˆæ•°æ®ä¸­çš„å™ªå£°

+   è®­ç»ƒæ—¶çš„å‡†ç¡®æ€§å¥½ï¼Œä½†æµ‹è¯•æ—¶çš„å‡†ç¡®æ€§å·®

å½“æˆ‘ä»¬é€æ­¥æ·»åŠ ç»ˆç«¯èŠ‚ç‚¹æ—¶ï¼Œè§‚å¯Ÿå†³ç­–æ ‘æ¨¡å‹åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„å˜åŒ–æ˜¯æœ‰æ•™è‚²æ„ä¹‰çš„ã€‚æˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°å›¾å½¢åŒ–åœ°è§‚å¯Ÿåˆ°åˆ†å±‚äºŒåˆ†åˆ†å‰²ã€‚

+   è®©æˆ‘ä»¬ä»ç®€å•çš„å¤æ‚æ¨¡å‹å¼€å§‹å¯è§†åŒ–ã€‚

```py
leaf_nodes_list = [2,3,4,10,20,100]

for inode,leaf_nodes in enumerate(leaf_nodes_list):

    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes)
    tree_model = tree_model.fit(X_train.values, y_train.values)

    plt.subplot(3,2,inode+1)                                         # visualize, data, and decision tree regions and predictions
    visualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],1000,9000,'Decision Tree Model, Number of Leaf Nodes: ' + str(leaf_nodes),Xname,yname,Xlabelunit,ylabelunit,annotate=False)   

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=3.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/43008585fa66e722cde17f42284274b4.png)

å¯èƒ½ä¼šæœ‰ç”¨ï¼Œçœ‹çœ‹å†³ç­–æ ‘æ¨¡å‹å’Œç›¸å…³çš„å†³ç­–æ ‘å¹¶æ’ã€‚

```py
leaf_nodes_viz = 2

tree_model_viz = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes_viz).fit(X_train.values, y_train.values)

fig = plt.figure(figsize=(10, 6))
gs = fig.add_gridspec(1, 2, width_ratios=[1, 2])  # 1 row, 3 columns with 1:2 width ratio

ax1 = fig.add_subplot(gs[0])                         # visualize, data, and decision tree regions and predictions 
visualize_tree_model(tree_model_viz,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
        y_train[yname],y_test[yname],1000,9000,'Decision Tree Model, Number of Leaf Nodes: ' + str(leaf_nodes),Xname,yname,
        Xlabelunit,ylabelunit,annotate=False)   

ax2 = fig.add_subplot(gs[1:])                                  # visualize, data, and decision tree regions and predictions
_ = tree.plot_tree(tree_model_viz,ax = ax2,feature_names=list(Xname),class_names=list(yname),filled=False,label='none',rounded=True,precision=0,
                  proportion=True,max_depth=4,fontsize=15)

plt.tight_layout()
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/de575bff5cbcf3e18e0bc1c385400e3e.png)

æˆ‘ä»¬å¦‚ä½•æ‰¾åˆ°æœ€ä½³çš„è¶…å‚æ•°ï¼Œä»¥è·å¾—æœ€ä½³å¤æ‚æ€§å’Œæµ‹è¯•é¢„æµ‹å‡†ç¡®æ€§çš„æœ€ä½³å€¼ï¼Ÿè¿™å°±æ˜¯è¶…å‚æ•°è°ƒæ•´ã€‚

## è°ƒæ•´å†³ç­–æ ‘ï¼ˆå›å½’æ ‘ï¼‰

è®©æˆ‘ä»¬è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ã€‚ä¸ºæ­¤æˆ‘ä»¬ï¼Œ

1.  çœ‹çœ‹å¯èƒ½çš„è¶…å‚æ•°å€¼èŒƒå›´ã€‚

1.  åœ¨å¯èƒ½çš„è¶…å‚æ•°å€¼èŒƒå›´å†…å¾ªç¯ã€‚

    +   ä½¿ç”¨å½“å‰è¶…å‚æ•°å€¼åœ¨è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒã€‚

    +   åœ¨æµ‹è¯•æ•°æ®ä¸Šé¢„æµ‹

    +   æ€»ç»“æ‰€æœ‰æµ‹è¯•æ•°æ®çš„é”™è¯¯

1.  é€‰æ‹©æœ€å°åŒ–æµ‹è¯•æ•°æ®é”™è¯¯çš„è¶…å‚æ•°

å½“æˆ‘å‘æˆ‘çš„å­¦ç”Ÿæ•™æˆè¿™ä¸ªæ—¶ï¼Œæˆ‘å»ºè®®è¿™æ˜¯ä¸€ä¸ªæ¨¡å‹å½©æ’ã€‚æˆ‘ä»¬é€šè¿‡ä¸ºæœªç”¨äºè®­ç»ƒæ¨¡å‹çš„æ¡ˆä¾‹åšå‡ºé¢„æµ‹æ¥å¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬å¸Œæœ›æ¨¡å‹åœ¨æœªåœ¨è®­ç»ƒä¸­ä½¿ç”¨çš„æ¡ˆä¾‹ä¸­è¡¨ç°æœ€ä½³ï¼Œå› æ­¤æˆ‘ä»¬æ­£åœ¨æ¨¡æ‹Ÿæ¨¡å‹çš„å®é™…ä¸–ç•Œä½¿ç”¨ï¼

ç°åœ¨ï¼Œè®©æˆ‘ä»¬é€šè¿‡æ‰‹åŠ¨è°ƒæ•´å†³ç­–æ ‘å¤æ‚æ€§æ¥æ‰§è¡Œè¶…å‚æ•°è°ƒæ•´ï¼Œæ‰¾åˆ°ä½¿æµ‹è¯•ä¸­çš„ MSE æœ€å°åŒ–çš„å¤æ‚æ€§ã€‚

+   ä¸ºäº†ç®€å•èµ·è§ï¼Œä¸‹é¢çš„ä»£ç åªéå†æœ€å¤§å¶èŠ‚ç‚¹è¶…å‚æ•°

+   æˆ‘ä»¬å°†æ ·æœ¬çš„æœ€å°æ•°é‡è®¾ç½®ä¸º 1ï¼Œæœ€å¤§æ·±åº¦è®¾ç½®ä¸º 9ï¼Œä»¥ç¡®ä¿è¿™äº›è¶…å‚æ•°ä¸ä¼šäº§ç”Ÿä»»ä½•å½±å“ï¼ˆæˆ‘ä»¬å°†å®ƒä»¬è®¾ç½®å¾—éå¸¸å¤æ‚ï¼Œè¿™æ ·å®ƒä»¬å°±ä¸ä¼šé™åˆ¶æ¨¡å‹å¤æ‚æ€§ï¼‰

```py
trees = []; MSE_CV = []; node_CV = []

inode = 2
while inode < len(X_train):                                   # loop over the hyperparameter, train with training and test with testing
    tree_model = tree.DecisionTreeRegressor(min_samples_leaf=1,max_leaf_nodes=inode).fit(X_train.values, y_train.values)
    trees.append(tree_model)
    predict_train = tree_model.predict(np.c_[X_test[Xname[0]],X_test[Xname[1]]]) 
    MSE_CV.append(metrics.mean_squared_error(y_test[yname],predict_train))   
    all_nodes = tree_model.tree_.node_count             
    decision_nodes = len([x for x in tree_model.tree_.feature if x != _tree.TREE_UNDEFINED]); terminal_nodes = all_nodes - decision_nodes
    node_CV.append(terminal_nodes); inode+=1

plt.subplot(111)
plt.scatter(node_CV,MSE_CV,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,linewidths=0.3,
            edgecolors="black",zorder=20)
tuned_node = node_CV[np.argmin(MSE_CV)]; max_MSE_CV = np.max(MSE_CV)
plt.vlines(tuned_node,0,1.05*max_MSE_CV,lw=1.0,ls='--',color='red',zorder=10)
plt.annotate('Tuned Max Nodes = ' + str(tuned_node),(tuned_node-2,3.5e5),rotation=90,zorder=30)
plt.title('Decision Tree Cross Validation Testing Error vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); plt.ylabel('Mean Square Error')
plt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_CV); add_grid()
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.6, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/f43c9ad2b759692c2b199e397c735352.png)

é€šè¿‡è§‚å¯Ÿå‡†ç¡®æ€§ä¸å¤æ‚æ€§çš„å…³ç³»ï¼Œè¯„ä¼°æˆ‘ä»¬æ ‘çš„è¡¨ç°æ˜¯æœ‰ç”¨çš„ï¼Œæœ€å°å€¼æ˜¯ç”±äºæ¨¡å‹æ–¹å·®å’Œæ¨¡å‹åå·®æƒè¡¡ã€‚

ä¸ºäº†å¾—åˆ°æ›´ç¨³å¥çš„ç»“æœï¼Œè®©æˆ‘ä»¬å°è¯• k æŠ˜äº¤å‰éªŒè¯ã€‚sklearn æœ‰ä¸€ä¸ªå†…ç½®çš„äº¤å‰éªŒè¯æ–¹æ³•ï¼Œç§°ä¸º cross_val_scoreï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥ï¼š

1.  åº”ç”¨ k æŠ˜æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£åˆ†ç¦»è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®

1.  å½“ k=5 æ—¶ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªæŠ˜å ä¿ç•™äº† 20%çš„æ•°æ®ç”¨äºæµ‹è¯•ã€‚

1.  è‡ªåŠ¨åŒ–æ¨¡å‹æ„å»ºï¼Œéå†æŠ˜å å¹¶å¹³å‡æ„Ÿå…´è¶£çš„æŒ‡æ ‡

è®©æˆ‘ä»¬åœ¨å…·æœ‰å¯å˜ç»ˆç«¯èŠ‚ç‚¹æ•°é‡çš„æ ‘ä¸Šå°è¯•å®ƒã€‚æ³¨æ„äº¤å‰éªŒè¯è®¾ç½®ä¸ºä½¿ç”¨ 4 ä¸ªå¤„ç†å™¨ï¼Œä½†ä»å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ‰èƒ½è¿è¡Œã€‚

```py
MSE_kF = []; node_kF = []                                     # k-fold iteration code modified from StackOverFlow by Dimosthenis

inode = 2
while inode < len(X_train):
    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=inode).fit(X_train.values, y_train.values)
    scores = cross_val_score(estimator=tree_model, X= np.c_[df[Xname[0]],df[Xname[1]]],y=df[yname], cv=5, n_jobs=4,
        scoring = "neg_mean_squared_error")                   # perform 4-fold cross validation
    MSE_kF.append(abs(scores.mean()))
    all_nodes = tree_model.tree_.node_count   
    decision_nodes = len([x for x in tree_model.tree_.feature if x != _tree.TREE_UNDEFINED]); terminal_nodes = all_nodes - decision_nodes
    node_kF.append(terminal_nodes); inode+=1

tuned_node_kF = node_kF[np.argmin(MSE_kF)]; max_MSE_kF = np.max(MSE_kF)  
plt.subplot(111)
plt.vlines(tuned_node_kF,0,1.05*max_MSE_kF,lw=1.0,ls='--',color='red',zorder=10)
plt.annotate('Tuned Max Nodes = ' + str(tuned_node_kF),(tuned_node_kF-2,3.5e5),rotation=90,zorder=30)
plt.scatter(node_kF,MSE_kF,s=None,c="red",marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,
            linewidths=0.5, edgecolors="black",zorder=40,label='k-Fold')
plt.scatter(node_CV,MSE_CV,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.4,linewidths=0.3,
            edgecolors="black",zorder=20,label='Cross Validation')
plt.title('Decision Tree k-Fold Cross Validation Error (MSE) vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); 
plt.ylabel('Mean Square Error'); plt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_kF); add_grid()
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
plt.legend(loc='upper right')
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.6, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/2df3cfa16f6fb36c7bd2a52fe7d20a6d.png)

k æŠ˜äº¤å‰éªŒè¯æä¾›äº† MSE ä¸è¶…å‚æ•°çš„æ›´å¹³æ»‘çš„å›¾è¡¨ã€‚

+   è¿™æ˜¯é€šè¿‡åœ¨æ‰€æœ‰æŠ˜å ä¸Šå¹³å‡ MSE æ¥å®Œæˆçš„ï¼Œä»¥å‡å°‘æŒ‡æ ‡å¯¹ç‰¹å®šè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†é…çš„æ•æ„Ÿæ€§ã€‚

+   æˆ‘ä»¬æ‰€æœ‰çš„è®­ç»ƒå’Œæµ‹è¯•äº¤å‰éªŒè¯æˆ– k æŠ˜äº¤å‰éªŒè¯éƒ½æ˜¯ä¸ºäº†å¾—åˆ°è¿™ä¸ªå€¼ï¼Œå³æ¨¡å‹çš„**è¶…å‚æ•°**

## æ„å»ºæœ€ç»ˆæ¨¡å‹

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç”¨è¿™ä¸ªè¶…å‚æ•°åœ¨æ‰€æœ‰æ•°æ®ä¸Šè®­ç»ƒï¼Œè¿™æ˜¯æˆ‘ä»¬**æœ€ç»ˆæ¨¡å‹**

```py
pruned_tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=tuned_node_kF)
pruned_tree_model = pruned_tree_model.fit(X, y)               # re-train

plt.subplot(121)                                              # visualize, data, and decision tree regions and predictions
visualize_tree_model(pruned_tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model, Tuned Leaf Nodes: ' + str(tuned_node_kF),Xname,yname,
                    Xlabelunit,ylabelunit) # plots the data points and the decision tree prediction 

plt.subplot(122)                                              # cross validation with conditional statistics plot
check_tree_model(pruned_tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,
                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot, Tuned Leaf Nodes: ' + 
                    str(tuned_node_kF),)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/608907e2a0d015a0d611204bfa6c41e5.png)

æˆ‘ä»¬å·²ç»å®Œæˆäº†æˆ‘ä»¬çš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ç°åœ¨è®©æˆ‘ä»¬å†ä»‹ç»ä¸€äº›å†³ç­–æ ‘è¯Šæ–­ã€‚

## è¯¢é—®å†³ç­–æ ‘

æœ‰å¿…è¦è¯„ä¼°ä»»ä½•å¯èƒ½çš„ç‰¹å¾ç»„åˆï¼Œä»¥åŠå¯¼è‡´ç‰¹å®šé¢„æµ‹çš„å†³ç­–èŠ‚ç‚¹é¡ºåºã€‚ä»¥ä¸‹å‡½æ•°æä¾›äº†é¢„æµ‹æ¡ˆä¾‹é€šè¿‡çš„èŠ‚ç‚¹åˆ—è¡¨ã€‚

```py
x1 = 7.0; x2 = 10.0                                          # the predictor feature values for the decision path

decision_path = pruned_tree_model.decision_path(np.c_[x1,x2])
print(decision_path) 
```

```py
 (0, 0)	1
  (0, 1)	1
  (0, 3)	1
  (0, 13)	1 
```

## å°†å†³ç­–æ ‘é¢„æµ‹æ¨¡å‹æå–ä¸ºå‡½æ•°

æ­¤å¤–ï¼Œå°†å†³ç­–æ ‘è½¬æ¢ä¸ºä»£ç ï¼Œä¸€ä¸ªåµŒå¥—çš„â€œifâ€è¯­å¥é›†åˆå¯èƒ½ä¹Ÿå¾ˆæœ‰ç”¨ã€‚

+   è¿™åˆ›å»ºäº†ä¸€ä¸ªå¯ç§»æ¤çš„æ¨¡å‹ï¼Œå¯ä»¥å¤åˆ¶å¹¶ä½œä¸ºç‹¬ç«‹å‡½æ•°åº”ç”¨ã€‚

æ­¤å¤–ï¼Œè¿˜å¯ä»¥æ–¹ä¾¿åœ°è¯¢é—®æ ‘çš„ä»£ç ç‰ˆæœ¬ã€‚

+   æˆ‘ä»¬ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„å‡½æ•°æ¥å¯¹æˆ‘ä»¬çš„å‰ªææ ‘æ‰§è¡Œæ­¤æ“ä½œã€‚

```py
tree_to_code(pruned_tree_model, list(Xname))                  # convert a decision tree to Python code, nested if statements 
```

```py
def tree(Por, Brittle):
  if Por <= 14.789999961853027:
    if Por <= 12.425000190734863:
      if Por <= 8.335000038146973:
        return [[1879.19091537]]
      elif Por > 8.335000038146973
        if Brittle <= 39.125:
          return [[2551.00021508]]
        elif Brittle > 39.125
          return [[3369.12903299]]
    elif Por > 12.425000190734863
      if Brittle <= 39.26500129699707:
        return [[3160.11022857]]
      elif Brittle > 39.26500129699707
        return [[4154.18334527]]
  elif Por > 14.789999961853027
    if Por <= 18.015000343322754:
      if Brittle <= 33.25:
        return [[3883.19381758]]
      elif Brittle > 33.25
        if Por <= 16.434999465942383:
          return [[4544.69777089]]
        elif Por > 16.434999465942383
          return [[5240.84146117]]
    elif Por > 18.015000343322754
      if Brittle <= 31.5600004196167:
        return [[4353.11874206]]
      elif Brittle > 31.5600004196167
        return [[5868.56369869]] 
```

## åŸºäºå†³ç­–æ ‘çš„ç‰¹å¾é‡è¦æ€§

ç‰¹å¾é‡è¦æ€§æ˜¯é€šè¿‡å†³ç­–æ ‘è®¡ç®—çš„ï¼Œé€šè¿‡åŒ…å«æ¯ä¸ªç‰¹å¾æ¥æ€»ç»“å‡æ–¹è¯¯å·®çš„å‡å°‘ï¼Œå¹¶æ€»ç»“å¦‚ä¸‹ï¼š

$$ FI(x) = \sum_{t \in T_f} \frac{N_t}{N} \Delta_{MSE_t} $$

å…¶ä¸­ $T_f$ æ˜¯æ‰€æœ‰ä»¥ç‰¹å¾ $x$ ä½œä¸ºåˆ†å‰²çš„èŠ‚ç‚¹ï¼Œ$N_t$ æ˜¯è¾¾åˆ°èŠ‚ç‚¹ $t$ çš„è®­ç»ƒæ ·æœ¬æ•°é‡ï¼Œ$N$ æ˜¯æ•°æ®é›†ä¸­æ ·æœ¬çš„æ€»æ•°ï¼Œ$\Delta_{MSE_t}$ æ˜¯ $t$ åˆ†å‰²å¸¦æ¥çš„ MSE å‡å°‘é‡ã€‚

æ³¨æ„ï¼Œå¯¹äºå…·æœ‰ **Gini ä¸çº¯åº¦**çš„åˆ†ç±»æ ‘ï¼Œç‰¹å¾é‡è¦æ€§å¯ä»¥ä»¥ç±»ä¼¼äºä¸Šè¿° MSE çš„æ–¹å¼è®¡ç®—ã€‚

```py
plt.subplot(111)                                              # plot the feature importance 
plt.title("Decision Tree Feature Importance")
plt.bar(Xlabel, pruned_tree_model.feature_importances_,edgecolor = 'black',
       color="darkorange",alpha = 0.6, align="center")
plt.xlim([-0.5,len(Xname)-0.5]); plt.ylim([0.,1.0])
plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids
plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')
plt.subplots_adjust(left=0.0, bottom=0.0, right=1., top=0.8, wspace=0.2, hspace=0.5); plt.show() 
```

![å›¾ç‰‡](img/e7672512b746dfc82482f39bc8c78ddc.png)

## å¯è§†åŒ–æ¨¡å‹

è®©æˆ‘ä»¬æœ€åçœ‹çœ‹æˆ‘ä»¬ä¿®å‰ªåçš„æ ‘çš„å›¾å½¢è¡¨ç¤ºã€‚

```py
fig = plt.figure(figsize=(15,10))

_ = tree.plot_tree(pruned_tree_model,                         # plot the decision tree for model visualization
                   feature_names=list(Xname),  
                   class_names=list(yname),
                   filled=True) 
```

![å›¾ç‰‡](img/dceac0eb1f800b2c21d5e490c1248210.png)

## ç®€å•ä»£ç åˆ›å»ºå†³ç­–æ ‘æœºå™¨å¹¶è®¡ç®—é¢„æµ‹

ä¸ºäº†æ”¯æŒåˆå­¦è€…ï¼Œä»¥ä¸‹æ˜¯ä¸€æ®µæœ€å°åŒ–ä»£ç ï¼Œä»¥ï¼š

+   åŠ è½½ç”¨äºå†³ç­–æ ‘çš„ scikit-learn åŒ…

+   åŠ è½½æ•°æ®

+   ä½¿ç”¨è¶…å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªå†³ç­–æ ‘ï¼ˆæœªæ˜¾ç¤ºè°ƒæ•´ï¼‰

+   ä½¿ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒå†³ç­–æ ‘

+   ä½¿ç”¨å†³ç­–æ ‘è¿›è¡Œé¢„æµ‹

```py
from sklearn import tree                                      # import decision tree from scikit-learn
Xname = ['Por','Brittle']; yname='Production'                 # predictor features and response feature
x1 = 0.25; x2 = 0.3                                           # predictor values for the prediction
my_data = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv") # load subsurface data table
my_tree = tree.DecisionTreeRegressor(max_leaf_nodes=26)       # instantiate tree with hyperparameters
my_tree = my_tree.fit(X.values,y.values)                      # train tree with training data
estimate = my_tree.predict([[x1,x2]])[0]                      # make a prediction (no tuning shown)
print('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + str(round(estimate,1)) + ' ' + yunit) # print results 
```

```py
Estimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1879.2 MCFPD 
```

## æ¸…æ´ã€ç´§å‡‘çš„æœºå™¨å­¦ä¹ ä»£ç çš„æœºå™¨å­¦ä¹ ç®¡é“

ç®¡é“æ˜¯ scikit-learn ä¸­çš„ä¸€ä¸ªç±»ï¼Œå®ƒå…è®¸å°è£…ä¸€ç³»åˆ—æ•°æ®å‡†å¤‡å’Œå»ºæ¨¡æ­¥éª¤

+   ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†ç®¡é“ä½œä¸ºæˆ‘ä»¬é«˜åº¦ç²¾ç®€çš„å·¥ä½œæµç¨‹ä¸­çš„ä¸€ä¸ªå¯¹è±¡æ¥å¤„ç†

ç®¡é“ç±»å…è®¸æˆ‘ä»¬ï¼š

+   æé«˜ä»£ç å¯è¯»æ€§å¹¶ä¿æŒä¸€åˆ‡äº•ç„¶æœ‰åº

+   ä½¿ç”¨éå¸¸å°‘çš„å¯è¯»ä»£ç è¡Œæ„å»ºå®Œæ•´çš„æµç¨‹

+   é¿å…å¸¸è§çš„æµç¨‹é—®é¢˜ï¼Œå¦‚æ•°æ®æ³„éœ²ã€æµ‹è¯•æ•°æ®å½±å“æ¨¡å‹å‚æ•°è®­ç»ƒ

+   æŠ½è±¡å‡ºå¸¸è§çš„æœºå™¨å­¦ä¹ å»ºæ¨¡ï¼Œä¸“æ³¨äºæ„å»ºå°½å¯èƒ½å¥½çš„æ¨¡å‹

åŸºæœ¬å“²å­¦æ˜¯å°†æœºå™¨å­¦ä¹ è§†ä¸ºä¸€ç§ç»„åˆæœç´¢ï¼Œä»¥æ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼ˆAutoMLï¼‰

æ›´å¤šä¿¡æ¯è¯·å‚é˜…æˆ‘å…³äº [æœºå™¨å­¦ä¹ ç®¡é“](https://www.youtube.com/watch?v=tYrPs8s1l9U&list=PLG19vXLQHvSAufDFgZEFAYQEwMJXklnQV&index=5) çš„å½•éŸ³è®²åº§å’Œè¯¦ç»†è®°å½•çš„æ¼”ç¤º [æœºå™¨å­¦ä¹ ç®¡é“å·¥ä½œæµç¨‹](http://localhost:8892/notebooks/OneDrive%20-%20The%20University%20of%20Texas%20at%20Austin/Courses/Workflows/PythonDataBasics_Pipelines.ipynb)ã€‚

```py
pipe_tree = Pipeline([                                        # the machine learning workflow as a pipeline object

    ('tree', tree.DecisionTreeRegressor())
])

params = {                                                    # the machine learning workflow method's parameters to search
    'tree__max_leaf_nodes': np.arange(2,len(X),1,dtype = int),
}

KF_tuned_tree = GridSearchCV(pipe_tree,params,scoring = 'neg_mean_squared_error', # hyperparameter tuning w. grid search k-fold cross validation 
                             cv=KFold(n_splits=5,shuffle=False),refit = True)
KF_tuned_tree.fit(X,y)                                        # tune and train the model

print('Tuned hyperparameter: max_leaf_nodes = ' + str(KF_tuned_tree.best_params_))

estimate = KF_tuned_tree.predict([[x1,x2]])[0]                # make a prediction (no tuning shown)
print('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + str(round(estimate,1)) + ' ' + yunit) # print results 
```

```py
Tuned hyperparameter: max_leaf_nodes = {'tree__max_leaf_nodes': 10}
Estimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1879.2 MCFPD 
```

## åœ¨æ–°æ•°æ®é›†ä¸Šç»ƒä¹ 

å¥½çš„ï¼Œæ˜¯æ—¶å€™å¼€å§‹å·¥ä½œäº†ã€‚è®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªæ•°æ®é›†å¹¶ä½¿ç”¨ä»¥ä¸‹å†…å®¹æ„å»ºä¸€ä¸ªå†³ç­–æ ‘é¢„æµ‹æ¨¡å‹ï¼Œ

+   ç´§å‡‘çš„ä»£ç 

+   åŸºæœ¬å¯è§†åŒ–

+   ä¿å­˜è¾“å‡º

æ‚¨å¯ä»¥é€‰æ‹©è¿™äº›æ•°æ®é›†ä¹‹ä¸€ï¼Œæˆ–ä¿®æ”¹ä»£ç å¹¶æ·»åŠ æ‚¨è‡ªå·±çš„å†…å®¹æ¥å®Œæˆæ­¤æ“ä½œã€‚

### æ•°æ®é›† 0ï¼Œéå¸¸è§„å¤šå…ƒ v4

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒæ•°æ®é›† [unconv_MV.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v4.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   äº•å¹³å‡å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   å‰ªåˆ‡æ¯”ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   çƒƒåå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹ç”Ÿäº§ 90 å¤©å¹³å‡ï¼ˆMCFPDï¼‰ã€‚

### æ•°æ®é›† 2ï¼Œå‚¨å±‚ 21

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒï¼Œ3D ç©ºé—´æ•°æ®é›†[res21_wells.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/res21_wells.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 73 å£å‚ç›´äº•åœ¨ä¸€ä¸ª 10,000m x 10,000m x 50 m å‚¨å±‚å•å…ƒçš„å˜é‡ï¼š

+   äº•ï¼ˆIDï¼‰

+   Xï¼ˆmï¼‰ï¼ŒYï¼ˆmï¼‰ï¼Œæ·±åº¦ï¼ˆmï¼‰ä½ç½®åæ ‡

+   å•ä½è½¬æ¢åçš„å­”éš™ç‡ï¼ˆ%ï¼‰

+   æ¸—é€ç‡ï¼ˆmDï¼‰

+   å•ä½è½¬æ¢åçš„å£°é˜»æŠ—ï¼ˆkg/m2s*10â¶ï¼‰

+   ç›¸ï¼ˆåˆ†ç±»ï¼‰ - æœ‰åºï¼Œä»é¡µå²©ã€æ²™è´¨é¡µå²©ã€é¡µå²©ç ‚åˆ°ç ‚å²©ã€‚

+   å¯†åº¦ï¼ˆg/cmÂ³ï¼‰

+   å‹ç¼©é€Ÿåº¦ï¼ˆm/sï¼‰

+   æ¨æ°æ¨¡é‡ï¼ˆGPaï¼‰

+   å‰ªåˆ‡é€Ÿåº¦ï¼ˆm/sï¼‰

+   å‰ªåˆ‡æ¨¡é‡ï¼ˆGPaï¼‰

+   3 å¹´ç´¯è®¡æ²¹äº§é‡ï¼ˆMbblï¼‰

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†è¡¨æ ¼æ•°æ®åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸ºâ€˜my_dataâ€™çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

+   æˆ‘ä»¬è¿˜ç”¨æ•°æ®èŒƒå›´å’Œæ ‡ç­¾å¡«å……åˆ—è¡¨ï¼Œä»¥ä¾¿äºç»˜å›¾

åŠ è½½æ•°æ®å¹¶æ ¼å¼åŒ–ï¼Œ

+   åˆ é™¤å“åº”ç‰¹å¾

+   æ ¹æ®éœ€è¦é‡æ–°æ ¼å¼åŒ–ç‰¹å¾

+   æ­¤å¤–ï¼Œæˆ‘è¿˜å–œæ¬¢å°†å…ƒæ•°æ®å­˜å‚¨åœ¨åˆ—è¡¨ä¸­

```py
idata = 2                                                    # select the dataset

if idata == 0:
    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['Well'],axis=1,inplace=True)                 # remove well index and response feature

    features = df_new.columns.values.tolist()                 # store the names of the features

    xname = features[:-1]
    yname = [features[-1]]

    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting
    ymin_new = 0.0; ymax_new = 10000.0
    xlabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Brittleness Ratio (%)', # set the names for plotting
             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']

    ylabel_new = 'Production (MCFPD)'

    xtitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting
             'Total Organic Carbon','Vitrinite Reflectance']

    ytitle_new = 'Production'

    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames 
    X = df_new[xname]

elif idata == 1:
    names = {'Porosity':'Por'}

    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['X','Y','Unnamed: 0','Facies'],axis=1,inplace=True)   # remove response feature
    df_new = df_new.rename(columns=names)
    df_new['Por'] = df_new['Por'] * 100.0; df_new['AI'] = df_new['AI'] / 1000.0
    features = df_new.columns.values.tolist()                 # store the names of the features

    xname = features[:-1]
    yname = [features[-1]]

    xmin_new = [4.0,0.0]; xmax_new = [19.0,500.0] # set the minimum and maximum values for plotting

    ymin_new = 1.60; ymax_new = 6.20

    xlabel_new = ['Porosity (fraction)','Permeability (mD)'] # set the names for plotting

    ylabel_new = 'Acoustic Impedance (kg/m2s*10â¶)'

    xtitle_new = ['Porosity','Permeability']

    ytitle_new = 'Acoustic Impedance (kg/m2s*10â¶)'

    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames 
    X = df_new[xname]

elif idata == 2:  
    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['Well_ID','X','Y'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature
    df_new = df_new.dropna(how='any',inplace=False)

    features = df_new.columns.values.tolist()                 # store the names of the features

    xname = features[:-1]
    yname = [features[-1]]

    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting

    ymin_new = 0.0; ymax_new = 1600.0

    xlabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Facies (categorical)',
              'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting

    ylabel_new = 'Production (Mbbl)'

    xtitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',
              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']

    ytitle_new = 'Production'

    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames 
    X = df_new[xname]

df_new.head(n=13) 
```

|  | å­”éš™ç‡ | æ¸—é€ç‡ | AI | å¯†åº¦ | PVel | Youngs | SVel | å‰ªåˆ‡ | ç´¯è®¡æ²¹ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | 12.907730 | 133.910637 | 7.308846 | 2.146360 | 3563.549461 | 25.688560 | 1673.770439 | 6.429229 | 1201.20 |
| 7 | 12.647965 | 114.359667 | 7.343836 | 2.188597 | 3570.094553 | 25.444064 | 1670.043495 | 6.100984 | 683.92 |
| 10 | 12.998469 | 129.332122 | 7.282051 | 2.131121 | 3524.448615 | 25.985734 | 1681.960101 | 6.203527 | 978.14 |
| 15 | 12.426141 | 123.227677 | 7.351795 | 2.203026 | 3417.596818 | 25.976462 | 1675.355860 | 6.288040 | 608.09 |
| 16 | 13.507371 | 147.562087 | 7.300360 | 2.210916 | 3476.167397 | 24.817767 | 1656.890690 | 6.222528 | 1062.10 |
| 36 | 13.309477 | 122.818961 | 7.345220 | 2.178749 | 3346.347661 | 25.436579 | 1651.679529 | 6.334308 | 539.98 |
| 49 | 11.822910 | 98.168307 | 7.386212 | 2.301552 | 3250.020705 | 24.340656 | 1662.438742 | 6.617267 | 1095.30 |
| 51 | 13.986616 | 132.575456 | 7.194749 | 2.108986 | 3415.255945 | 26.253236 | 1712.017629 | 5.583251 | 805.49 |
| 61 | 14.735895 | 128.201000 | 7.172693 | 1.841786 | 3886.950307 | 28.289950 | 1672.370150 | 5.044439 | 1146.00 |

### æ„å»ºå’Œæ£€æŸ¥æ¨¡å‹

æˆ‘ä»¬åº”ç”¨ä»¥ä¸‹æ­¥éª¤ï¼Œ

1.  æŒ‡å®š K æŠ˜æ–¹æ³•

1.  éå†å¶èŠ‚ç‚¹æ•°ï¼Œå®ä¾‹åŒ–ã€æ‹Ÿåˆå¹¶è®°å½•é”™è¯¯

1.  ç»˜åˆ¶æµ‹è¯•è¯¯å·®ä¸å¶èŠ‚ç‚¹æ•°çš„å…³ç³»å›¾ï¼Œé€‰æ‹©æœ€å°åŒ–æµ‹è¯•è¯¯å·®çš„è¶…å‚æ•°

1.  ä½¿ç”¨è°ƒæ•´åçš„è¶…å‚æ•°å’Œæ‰€æœ‰æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹

```py
MSE_kF = []; node_kF = []                                     
kf = KFold(n_splits=5, shuffle=True, random_state=seed)       # k-fold specification 

inode = 2
while inode < len(X_train):
    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=inode,random_state=seed)
    scores = cross_val_score(estimator=tree_model,X=X,y=y,cv=kf,n_jobs=4,scoring = "neg_mean_squared_error") # perform 5-fold cross validation
    MSE_kF.append(abs(scores.mean()))
    node_kF.append(inode); inode+=1

tuned_node_kF = node_kF[np.argmin(MSE_kF)]
tuned_tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=tuned_node_kF).fit(X.values, y.values) # retrain on all the data

plt.subplot(121)
plt.vlines(tuned_node_kF,0,1.05*max_MSE_kF,lw=1.0,ls='--',color='red',zorder=10)
plt.annotate('Tuned Max Nodes = ' + str(tuned_node_kF),(tuned_node_kF-2,3.5e5),rotation=90,zorder=30)
plt.scatter(node_kF,MSE_kF,s=None,c="red",marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,
            linewidths=0.5, edgecolors="black",zorder=40,label='k-Fold')
plt.title('Decision Tree k-Fold Cross Validation Error (MSE) vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); 
plt.ylabel('Mean Square Error'); plt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_kF); add_grid()
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
plt.legend(loc='upper right')

y_hat = tuned_tree_model.predict(X)

plt.subplot(122)
plt.scatter(y,y_hat,color='green',edgecolor='black') # cross validation plot
plt.plot([ymin_new,ymax_new],[ymin_new,ymax_new],color='black',zorder=-1)
plt.xlim(ymin_new,ymax_new); plt.ylim(ymin_new,ymax_new); add_grid() 
plt.xlabel('Truth: ' + ylabel_new); plt.ylabel('Estimate: ' + ylabel_new)
plt.title('Tuned Decision Tree, Cross Validation')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/800e93156a21d23fb27ec3b349cbd0c234a2789e27a8582567a80abbbf0e08b0.png](img/a3d66d6b8f851c5edb5b770a5393116c.png)

### æ•°æ®é›† 0ï¼Œéå¸¸è§„å¤šå…ƒ v4

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒï¼Œ3D ç©ºé—´æ•°æ®é›†[unconv_MV.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v4.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼š

+   äº•å¹³å‡å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°é˜»æŠ— (kg/mÂ³ x m/s x 10â¶)

+   å‰ªåˆ‡æ¯” (%)

+   æ€»æœ‰æœºç¢³ (%) 

+   ç»ç’ƒå…‰æ³½ç‡ (%)

+   åˆå§‹ç”Ÿäº§ 90 å¤©å¹³å‡ (MCFPD)ã€‚

### æ•°æ®é›† 2ï¼Œå‚¨å±‚ 21

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€3D ç©ºé—´æ•°æ®é›† [res21_wells.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/res21_wells.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 73 ä¸ªå‚ç›´äº•åœ¨ 10,000m x 10,000m x 50 m å‚¨å±‚å•å…ƒçš„å˜é‡ï¼š

+   äº• (ID)

+   X (m), Y (m), æ·±åº¦ (m) ä½ç½®åæ ‡

+   å•ä½è½¬æ¢åçš„å­”éš™ç‡ (%)

+   æ¸—é€ç‡ (mD)

+   å•ä½è½¬æ¢åçš„å£°é˜»æŠ— (kg/m2s*10â¶)

+   ç›¸ (åˆ†ç±») - ä»é¡µå²©ã€ç ‚è´¨é¡µå²©ã€é¡µå²©ç ‚åˆ°ç ‚å²©çš„é¡ºåºã€‚

+   å¯†åº¦ (g/cmÂ³)

+   å¯å‹ç¼©é€Ÿåº¦ (m/s)

+   æ¨æ°æ¨¡é‡ (GPa)

+   å‰ªåˆ‡é€Ÿåº¦ (m/s)

+   å‰ªåˆ‡æ¨¡é‡ (GPa)

+   3 å¹´ç´¯è®¡çŸ³æ²¹äº§é‡ (Mbbl)

æˆ‘ä»¬ä½¿ç”¨ pandas çš„ 'read_csv' å‡½æ•°å°†è¡¨æ ¼æ•°æ®åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸º 'my_data' çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

+   æˆ‘ä»¬è¿˜ç”¨æ•°æ®èŒƒå›´å’Œæ ‡ç­¾å¡«å……åˆ—è¡¨ï¼Œä»¥ä¾¿äºç»˜å›¾

åŠ è½½æ•°æ®å¹¶æ ¼å¼åŒ–ï¼Œ

+   åˆ é™¤å“åº”ç‰¹å¾

+   æ ¹æ®éœ€è¦é‡æ–°æ ¼å¼åŒ–ç‰¹å¾

+   æ­¤å¤–ï¼Œæˆ‘è¿˜å–œæ¬¢å°†å…ƒæ•°æ®å­˜å‚¨åœ¨åˆ—è¡¨ä¸­

```py
idata = 2                                                    # select the dataset

if idata == 0:
    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['Well'],axis=1,inplace=True)                 # remove well index and response feature

    features = df_new.columns.values.tolist()                 # store the names of the features

    xname = features[:-1]
    yname = [features[-1]]

    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting
    ymin_new = 0.0; ymax_new = 10000.0
    xlabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Brittleness Ratio (%)', # set the names for plotting
             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']

    ylabel_new = 'Production (MCFPD)'

    xtitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting
             'Total Organic Carbon','Vitrinite Reflectance']

    ytitle_new = 'Production'

    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames 
    X = df_new[xname]

elif idata == 1:
    names = {'Porosity':'Por'}

    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['X','Y','Unnamed: 0','Facies'],axis=1,inplace=True)   # remove response feature
    df_new = df_new.rename(columns=names)
    df_new['Por'] = df_new['Por'] * 100.0; df_new['AI'] = df_new['AI'] / 1000.0
    features = df_new.columns.values.tolist()                 # store the names of the features

    xname = features[:-1]
    yname = [features[-1]]

    xmin_new = [4.0,0.0]; xmax_new = [19.0,500.0] # set the minimum and maximum values for plotting

    ymin_new = 1.60; ymax_new = 6.20

    xlabel_new = ['Porosity (fraction)','Permeability (mD)'] # set the names for plotting

    ylabel_new = 'Acoustic Impedance (kg/m2s*10â¶)'

    xtitle_new = ['Porosity','Permeability']

    ytitle_new = 'Acoustic Impedance (kg/m2s*10â¶)'

    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames 
    X = df_new[xname]

elif idata == 2:  
    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['Well_ID','X','Y'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature
    df_new = df_new.dropna(how='any',inplace=False)

    features = df_new.columns.values.tolist()                 # store the names of the features

    xname = features[:-1]
    yname = [features[-1]]

    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting

    ymin_new = 0.0; ymax_new = 1600.0

    xlabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Facies (categorical)',
              'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting

    ylabel_new = 'Production (Mbbl)'

    xtitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',
              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']

    ytitle_new = 'Production'

    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames 
    X = df_new[xname]

df_new.head(n=13) 
```

|  | Por | Perm | AI | Density | PVel | Youngs | SVel | Shear | CumulativeOil |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | 12.907730 | 133.910637 | 7.308846 | 2.146360 | 3563.549461 | 25.688560 | 1673.770439 | 6.429229 | 1201.20 |
| 7 | 12.647965 | 114.359667 | 7.343836 | 2.188597 | 3570.094553 | 25.444064 | 1670.043495 | 6.100984 | 683.92 |
| 10 | 12.998469 | 129.332122 | 7.282051 | 2.131121 | 3524.448615 | 25.985734 | 1681.960101 | 6.203527 | 978.14 |
| 15 | 12.426141 | 123.227677 | 7.351795 | 2.203026 | 3417.596818 | 25.976462 | 1675.355860 | 6.288040 | 608.09 |
| 16 | 13.507371 | 147.562087 | 7.300360 | 2.210916 | 3476.167397 | 24.817767 | 1656.890690 | 6.222528 | 1062.10 |
| 36 | 13.309477 | 122.818961 | 7.345220 | 2.178749 | 3346.347661 | 25.436579 | 1651.679529 | 6.334308 | 539.98 |
| 49 | 11.822910 | 98.168307 | 7.386212 | 2.301552 | 3250.020705 | 24.340656 | 1662.438742 | 6.617267 | 1095.30 |
| 51 | 13.986616 | 132.575456 | 7.194749 | 2.108986 | 3415.255945 | 26.253236 | 1712.017629 | 5.583251 | 805.49 |
| 61 | 14.735895 | 128.201000 | 7.172693 | 1.841786 | 3886.950307 | 28.289950 | 1672.370150 | 5.044439 | 1146.00 |

### æ„å»ºå’Œæ£€æŸ¥æ¨¡å‹

æˆ‘ä»¬åº”ç”¨ä»¥ä¸‹æ­¥éª¤ï¼Œ

1.  æŒ‡å®š K æŠ˜å æ–¹æ³•

1.  å¾ªç¯éå†å¶èŠ‚ç‚¹æ•°é‡ï¼Œå®ä¾‹åŒ–ã€æ‹Ÿåˆå¹¶è®°å½•è¯¯å·®

1.  ç»˜åˆ¶æµ‹è¯•è¯¯å·®ä¸å¶èŠ‚ç‚¹æ•°é‡çš„å…³ç³»å›¾ï¼Œé€‰æ‹©æœ€å°åŒ–æµ‹è¯•è¯¯å·®çš„è¶…å‚æ•°

1.  ä½¿ç”¨è°ƒæ•´å¥½çš„è¶…å‚æ•°å’Œæ‰€æœ‰æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹

```py
MSE_kF = []; node_kF = []                                     
kf = KFold(n_splits=5, shuffle=True, random_state=seed)       # k-fold specification 

inode = 2
while inode < len(X_train):
    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=inode,random_state=seed)
    scores = cross_val_score(estimator=tree_model,X=X,y=y,cv=kf,n_jobs=4,scoring = "neg_mean_squared_error") # perform 5-fold cross validation
    MSE_kF.append(abs(scores.mean()))
    node_kF.append(inode); inode+=1

tuned_node_kF = node_kF[np.argmin(MSE_kF)]
tuned_tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=tuned_node_kF).fit(X.values, y.values) # retrain on all the data

plt.subplot(121)
plt.vlines(tuned_node_kF,0,1.05*max_MSE_kF,lw=1.0,ls='--',color='red',zorder=10)
plt.annotate('Tuned Max Nodes = ' + str(tuned_node_kF),(tuned_node_kF-2,3.5e5),rotation=90,zorder=30)
plt.scatter(node_kF,MSE_kF,s=None,c="red",marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,
            linewidths=0.5, edgecolors="black",zorder=40,label='k-Fold')
plt.title('Decision Tree k-Fold Cross Validation Error (MSE) vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); 
plt.ylabel('Mean Square Error'); plt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_kF); add_grid()
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
plt.legend(loc='upper right')

y_hat = tuned_tree_model.predict(X)

plt.subplot(122)
plt.scatter(y,y_hat,color='green',edgecolor='black') # cross validation plot
plt.plot([ymin_new,ymax_new],[ymin_new,ymax_new],color='black',zorder=-1)
plt.xlim(ymin_new,ymax_new); plt.ylim(ymin_new,ymax_new); add_grid() 
plt.xlabel('Truth: ' + ylabel_new); plt.ylabel('Estimate: ' + ylabel_new)
plt.title('Tuned Decision Tree, Cross Validation')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/a3d66d6b8f851c5edb5b770a5393116c.png)

## æ³¨é‡Š

è¿™æ˜¯å¯¹å†³ç­–æ ‘çš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´ YouTube è®²åº§çš„é“¾æ¥ï¼Œè§†é¢‘æè¿°ä¸­åŒ…å«èµ„æºé“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![å›¾ç‰‡](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­å†…ï¼Œè¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆçš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œåœ¨é‚£é‡Œä»–ç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„ä¸»è¦ç ”ç©¶å‘˜ï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚

+   [è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº† 70 å¤šç¯‡[åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[åœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[Python åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[Python åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå®è·µæŒ‡å—ä¸ä»£ç ](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å« 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ª GitHub è´¦æˆ·ä¸Šçš„è¯¦ç»†æ–‡æ¡£å·¥ä½œæµç¨‹ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›å¸¸é’å†…å®¹ã€‚æƒ³äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œå­¦ä¹ çš„äººæ¥è¯´æœ‰å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œï¼Œæ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹æ¥å¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢

æ›´å¤šèµ„æºè¯·è®¿é—®ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
