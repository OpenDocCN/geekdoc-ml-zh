# k-æœ€è¿‘é‚»

> åŸæ–‡ï¼š[`geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_knearest_neighbours.html`](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_knearest_neighbours.html)

Michael J. Pyrczï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

ç”µå­ä¹¦â€œPython ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œâ€çš„ä¸€ç« ã€‚

è¯·å°†æ­¤ç”µå­ä¹¦å¼•ç”¨å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *Python ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œ* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138 ![DOI](https://doi.org/10.5281/zenodo.15169138)

æœ¬ä¹¦åŠæ›´å¤šå·¥ä½œæµç¨‹åœ¨æ­¤å¤„å¯ç”¨ï¼š

è¯·å°† MachineLearningDemos GitHub ä»“åº“å¼•ç”¨å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration Workflows Repository* (0.0.3) [è½¯ä»¶]. Zenodo. DOI: 10.5281/zenodo.13835312\. GitHub ä»“åº“ï¼š[GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos) ![DOI](https://zenodo.org/doi/10.5281/zenodo.13835312)

ä½œè€…ï¼šMichael J. Pyrcz

Â© ç‰ˆæƒæ‰€æœ‰ 2024ã€‚

è¿™ç« æ˜¯å¯¹**k-æœ€è¿‘é‚»**çš„æ•™ç¨‹/æ¼”ç¤ºã€‚

**YouTube è®²åº§**ï¼šæŸ¥çœ‹æˆ‘å…³äºä»¥ä¸‹å†…å®¹çš„è®²åº§ï¼š

+   [æœºå™¨å­¦ä¹ ç®€ä»‹](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)

+   [k-æœ€è¿‘é‚»å›å½’](https://youtu.be/lzmeChSYvv8?si=nfcvGtkIAQ7rFkjo)

è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘ YouTube ä¸Šçš„[æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«é“¾æ¥è‰¯å¥½çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºç†è§£ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœä½ æƒ³çŸ¥é“æˆ‘çš„åŠ¨æœºï¼Œè¯·æŸ¥çœ‹[Michael çš„æ•…äº‹](https://michaelpyrcz.com/my-story)ã€‚

## k-æœ€è¿‘é‚»å›å½’çš„åŠ¨æœº

è¦†ç›– k-æœ€è¿‘é‚»å›å½’æœ‰è®¸å¤šå¾ˆå¥½çš„ç†ç”±ã€‚é™¤äº†å®ƒæ˜¯ä¸€ä¸ªç®€å•ã€å¯è§£é‡Šä¸”çµæ´»çš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹å¤–ï¼Œå®ƒè¿˜å±•ç¤ºäº†é‡è¦çš„æ¦‚å¿µï¼Œ

+   **éå‚æ•°é¢„æµ‹æ¨¡å‹** - ä»æ•°æ®ä¸­å­¦ä¹ å…³ç³»çš„å½¢çŠ¶ï¼Œå³ä¸å¯¹å…³ç³»çš„å½¢çŠ¶åšå‡ºå…ˆéªŒå‡è®¾

+   **åŸºäºå®ä¾‹çš„ã€æ‡’æƒ°å­¦ä¹ ** - æ¨¡å‹è®­ç»ƒæ¨è¿Ÿåˆ°éœ€è¦é¢„æµ‹æ—¶ï¼Œä¸éœ€è¦é¢„å…ˆè®¡ç®—æ¨¡å‹ã€‚å³é¢„æµ‹éœ€è¦è®¿é—®æ•°æ®

+   **è¶…å‚æ•°è°ƒæ•´** - é€šè¿‡å¯ç†è§£çš„è¶…å‚æ•°æ¥æ§åˆ¶æ¨¡å‹æ‹Ÿåˆ

+   **éå¸¸çµæ´»ã€å¤šåŠŸèƒ½çš„é¢„æµ‹æ¨¡å‹** - åœ¨è®¸å¤šæƒ…å†µä¸‹è¡¨ç°è‰¯å¥½

## å·ç§¯

å®é™…ä¸Šï¼Œk è¿‘é‚»ç±»ä¼¼äºåœ¨å±€éƒ¨é‚»åŸŸå†…é€šè¿‡åŠ æƒå¹³å‡è¿›è¡Œç©ºé—´ä¼°è®¡ã€‚

![](img/0cfb5694654b0332a568b81d0ae25755.png)

åœ¨é¢„æµ‹ç‰¹å¾ç©ºé—´ä¸­å¯¹ç©ºé—´æ’å€¼è¿›è¡Œå»ºæ¨¡ã€‚

k è¿‘é‚»æ–¹æ³•ç±»ä¼¼äºç©ºé—´æ’å€¼çš„å·ç§¯æ–¹æ³•ã€‚å·ç§¯æ˜¯ä¸¤ä¸ªå‡½æ•°çš„ç§¯åˆ†ä¹˜ç§¯ï¼Œå…¶ä¸­ä¸€ä¸ªå‡½æ•°ç»è¿‡åè½¬å’Œå¹³ç§»$\tau$ã€‚

+   ä¸€ç§è§£é‡Šæ˜¯ä½¿ç”¨åŠ æƒå‡½æ•°ï¼Œ$ğ‘“(\Delta)$ï¼Œå¯¹å‡½æ•°ï¼Œ$ğ‘”(x)$ï¼Œè¿›è¡ŒåŠ æƒå¹³å‡è®¡ç®—ï¼Œ

$$ (f * g)(x) = \int_{-\infty}^{\infty} f(\tau) g(x - \tau) \, d\tau $$

è¿™å¾ˆå®¹æ˜“æ‰©å±•åˆ°å¤šç»´

$$ (f * g)(x, y, z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(\tau_x, \tau_y, \tau_z) g(x - \tau_x, y - \tau_y, z - \tau_z) \, d\tau_x \, d\tau_y \, d\tau_z $$

åœ¨ç§¯åˆ†ä¹‹å‰é€‰æ‹©å“ªä¸ªå‡½æ•°è¿›è¡Œå¹³ç§»ä¸ä¼šæ”¹å˜ç»“æœï¼Œå·ç§¯ç®—å­å…·æœ‰äº¤æ¢æ€§ï¼Œ

$$ (f * g)(x) = \int_{-\infty}^{\infty} f(\tau) g(x - \tau) \, d\tau $$$$ (f * g)(x) = \int_{-\infty}^{\infty} f(x - \tau) g(\tau) \, d\tau $$

+   å¦‚æœä»»ä¸€å‡½æ•°è¢«åå°„ï¼Œåˆ™å·ç§¯ç­‰åŒäºäº’ç›¸å…³ï¼Œå®ƒæ˜¯ä¸¤ä¸ªä¿¡å·ä½œä¸ºä½ç§»å‡½æ•°ç›¸ä¼¼åº¦çš„åº¦é‡ã€‚

ä¸ºäº†æ¼”ç¤ºä½¿ç”¨è¯¦å°½çš„$g(x)$å’Œç¨€ç–é‡‡æ ·çš„$g(x)$è¿›è¡Œå·ç§¯ï¼Œæˆ‘æ„å»ºäº†ä¸€ä¸ª[äº¤äº’å¼ Python å·ç§¯ä»ªè¡¨æ¿](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Convolution_kNearest.ipynb),

![](img/f55c37e0f7da98a233affd5fbd5ba38c.png)

äº¤äº’å¼ Python ä»ªè¡¨æ¿ç”¨äºæ¼”ç¤ºå·ç§¯ã€‚

```py
import numpy as np                                            # arrays and matrix math
import pandas as pd                                           # DataFrames
import matplotlib.pyplot as plt                               # for plotting
from scipy.ndimage import convolve1d
seed = 73073                                                  # random number seed

kr = 5; kloc = 45                                             # kernel radius, kernel location for example point

df_denpor = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/1D_por_perm_smooth.csv') # load data from Dr. Pyrcz's GitHub 

plt.subplot(121)                    
plt.plot(df_denpor['Por'],df_denpor['Depth'],color='black',label='f(x)'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)')
plt.ylim([100,0]); plt.xlim([2,22]); plt.title('Exhaustive Case: Original and Convolved Function'); plt.yticks(np.arange(100, -1, -5));

size = 2 * kr + 1                                             # make uniform kernel
kernel = np.ones(size) / size                                 # normalize kernel to sum to one for unbiasedness

convolved = np.convolve(df_denpor['Por'].values, kernel, mode='same') # convolution

k = len(kernel)
trim = k // 2  # how many values to trim from each edge
convolved_valid = convolved[trim:-trim] if trim > 0 else convolved
depth_valid = df_denpor['Depth'].values[trim:-trim] if trim > 0 else df_denpor['Depth'].values
convolved_pt = convolved_valid[np.abs(depth_valid - kloc).argmin()]; depth_pt = depth_valid[np.abs(depth_valid - kloc).argmin()]
plt.plot(convolved_valid,depth_valid,color='red',label=r'$(f * g)(x)$'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)'); 
plt.ylim([100,0]); plt.xlim([2,22]); plt.legend(loc='lower left'); plt.plot([2,22],[depth_pt,depth_pt],color='red',ls='--')
plt.scatter(convolved_pt,depth_pt,color='red',marker='o',edgecolor='black',zorder=10)

plt.subplot(122)
plt.plot(kernel,np.linspace(-1*kr+kloc,kr+kloc,2*kr+1),color='black',label=r'$g(\tau)$'); plt.xlim(0.35,0.0); plt.ylim([100,0]); 
plt.yticks(np.arange(100, -1, -5)); plt.plot([kernel[0],0.0],[-1*kr+kloc,-1*kr+kloc],color='black'); 
plt.plot([kernel[0],0.0],[kr+kloc,kr+kloc],color='black'); plt.legend(loc='lower left'); plt.title('Kernel')
plt.plot([0.35,0.0],[depth_pt,depth_pt],color='red',ls='--'); plt.ylabel('Depth (m)'); plt.xlabel('Weight (unitless)')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/58f0d6ec82ae506386975d87b4fbaa3fa88f321e3ef516e9f956402a156d2751.png](img/83364c0e2e061f041cd64b882996eb7e.png)

```py
import astropy.convolution.convolve as convolve               # sparse data convolution
frac = 0.2

df_denpor['Por_Sparse'] = df_denpor['Por'].copy()

np.random.seed(seed = seed)
nan_indices = np.random.choice(len(df_denpor), size=int(len(df_denpor)*(1.0-frac)), replace=False)
df_denpor.loc[nan_indices, 'Por_Sparse'] = np.nan

plt.subplot(121)                    
plt.scatter(df_denpor['Por_Sparse'],df_denpor['Depth'],color='black',label='f(x) Sparse'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)')
plt.plot(df_denpor['Por'],df_denpor['Depth'],color='black',alpha=0.3,label='f(x) Exhaustive');
plt.ylim([26,0]); plt.xlim([2,22]); plt.title('Sparse Case: Original and Convolved Function'); plt.yticks(np.arange(100, -1, -5));

size = 2 * kr + 1                                             # make uniform kernel
kernel = np.ones(size) / size                                 # normalize kernel to sum to one for unbiasedness

convolved = convolve(df_denpor['Por_Sparse'].values,kernel,boundary='extend',nan_treatment='interpolate',normalize_kernel=True) # convolve

k = len(kernel)
trim = k // 2  # how many values to trim from each edge
convolved_valid = convolved[trim:-trim] if trim > 0 else convolved
depth_valid = df_denpor['Depth'].values[trim:-trim] if trim > 0 else df_denpor['Depth'].values
convolved_pt = convolved_valid[np.abs(depth_valid - kloc).argmin()]; depth_pt = depth_valid[np.abs(depth_valid - kloc).argmin()]
plt.plot(convolved_valid,depth_valid,color='red',label=r'$(f * g)(x)$'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)'); 
plt.ylim([100,0]); plt.xlim([2,22]); plt.legend(loc='lower left'); plt.plot([2,22],[depth_pt,depth_pt],color='red',ls='--')
plt.scatter(convolved_pt,depth_pt,color='red',marker='o',edgecolor='black',zorder=10)

plt.subplot(122)
plt.plot(kernel,np.linspace(-1*kr+kloc,kr+kloc,2*kr+1),color='black',label=r'$g(\tau)$'); plt.xlim(0.35,0.0); plt.ylim([100,0]); 
plt.yticks(np.arange(100, -1, -5)); plt.plot([kernel[0],0.0],[-1*kr+kloc,-1*kr+kloc],color='black'); 
plt.plot([kernel[0],0.0],[kr+kloc,kr+kloc],color='black'); plt.legend(loc='lower left'); plt.title('Kernel')
plt.plot([0.35,0.0],[depth_pt,depth_pt],color='red',ls='--'); plt.ylabel('Depth (m)'); plt.xlabel('Weight (unitless)')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

```py
WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve] 
```

![_images/ae797e2e907bd48f0c7b524dd697136f6a863602289a91dc8e372c930f4a72c1.png](img/2eab3ef179c50a4d663c4b21d6e38124.png)

è™½ç„¶å›é¡¾å’Œè®¨è®ºå·ç§¯å¾ˆæœ‰ç”¨ï¼Œä½† k è¿‘é‚»é€šè¿‡æŒ‡å®š$k$ä¸ªæœ€è¿‘é‚»æ¥åŒ…å«åœ¨åŠ æƒå¹³å‡ä¸­ï¼Œä¸å·ç§¯æœ‰æ‰€ä¸åŒï¼Œ

+   æŒ‡å®š$k$ä¼šå¯¼è‡´å±€éƒ¨è‡ªé€‚åº”çª—å£å¤§å°ï¼Œå±€éƒ¨é‚»åŸŸæ‰©å±•è¶³å¤Ÿè¿œä»¥æ‰¾åˆ°$k$ä¸ªè®­ç»ƒæ•°æ®

![](img/2ffab60ca866a945ae26ac4aab566a02.png)

å¯¹äºç»™å®šçš„$k$ä¸ªæœ€è¿‘é‚»ï¼Œæ•°æ®ä»é¢„æµ‹ç‰¹å¾ç©ºé—´çš„ç¨€ç–æ•°æ®åŒºåŸŸæ”¶é›†ã€‚

## k è¿‘é‚»è¶…å‚æ•°

ç°åœ¨è®©æˆ‘ä»¬è®¨è®º k è¿‘é‚»è¶…å‚æ•°ã€‚

1.  **k ä¸ªæœ€è¿‘æ•°æ®** - ç”¨äºé¢„æµ‹

1.  **æ•°æ®åŠ æƒ** - ä¾‹å¦‚ï¼Œå‡åŒ€åŠ æƒï¼ˆä½¿ç”¨æœ¬åœ°è®­ç»ƒæ•°æ®å¹³å‡å€¼ï¼‰ï¼Œé€†è·ç¦»åŠ æƒ

æ³¨æ„ï¼Œå¯¹äºé€†è·ç¦»åŠ æƒçš„æƒ…å†µï¼Œè¯¥æ–¹æ³•ç±»ä¼¼äºé€†è·ç¦»åŠ æƒæ’å€¼ï¼Œé€šå¸¸åº”ç”¨äºç©ºé—´æ’å€¼ï¼Œå¹¶æœ‰ä¸€ä¸ªå¸¸ç”¨çš„å±€éƒ¨æ•°æ®æœ€å¤§æ•°é‡çº¦æŸã€‚é€†è·ç¦»åœ¨ GeostatsPy ä¸­å¯ç”¨äºç©ºé—´æ˜ å°„ã€‚

1.  **è·ç¦»åº¦é‡** - é¢„æµ‹ç‰¹å¾ç©ºé—´å†…çš„è®­ç»ƒæ•°æ®æŒ‰è·ç¦»æ’åºï¼Œä»è¿‘åˆ°è¿œï¼Œå¯ä»¥åº”ç”¨å¤šç§è·ç¦»åº¦é‡ï¼ŒåŒ…æ‹¬ï¼š

+   æ¬§å‡ é‡Œå¾—è·ç¦»

\begin{equation}

d_i = \sqrt{\sum_{\alpha = 1}^{m} \left(x_{\alpha,i} - x_{\alpha,0}\right)Â²} \end{equation}

+   Minkowski è·ç¦» - ä¸€ç§è·ç¦»çš„æ¨å¹¿å½¢å¼ï¼Œå…¶ä¸­å·²çŸ¥çš„æ›¼å“ˆé¡¿å’Œæ¬§å‡ é‡Œå¾—è·ç¦»æ˜¯ç‰¹æ®Šæƒ…å†µï¼Œ

$$ d_{(i,i')} = \left( \sum_{j=1}^{m} \left( x_{(j,i)} - x_{(j,i')} \right)^p \right)^{\frac{1}{p}} $$

+   å½“ $p=2$ æ—¶ï¼Œè¿™å˜ä¸ºæ¬§å‡ é‡Œå¾—è·ç¦»

+   å½“ $p=1$ æ—¶ï¼Œå®ƒå˜ä¸ºæ›¼å“ˆé¡¿è·ç¦»

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = True
import os                                                     # to set current working directory 
import math                                                   # square root operator
import numpy as np                                            # arrays and matrix math
import scipy.stats as st                                      # statistical methods
import pandas as pd                                           # DataFrames
import pandas.plotting as pd_plot
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import seaborn as sns                                         # for matrix scatter plots
from sklearn import metrics                                   # measures to check our models
from sklearn.preprocessing import StandardScaler              # standardize the features
from sklearn.neighbors import KNeighborsRegressor             # for nearest k neighbours
from sklearn import metrics                                   # measures to check our models
from sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning
from sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # suppress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½å¿…é¡»é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£å¹¶è¾“å…¥ 'python -m pip install [package-name]' æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£ï¼Œè¿˜æœ‰æ›´å¤šå¸®åŠ©å¯ç”¨ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰å‡ ä¸ªå‡½æ•°æ¥ç®€åŒ–ç»˜åˆ¶ç›¸å…³çŸ©é˜µã€å†³ç­–æ ‘å›å½’æ¨¡å‹çš„å¯è§†åŒ–ï¼Œä»¥åŠå°†æŒ‡å®šçš„ç™¾åˆ†ä½æ•°å’Œä¸»æ¬¡ç½‘æ ¼çº¿æ·»åŠ åˆ°æˆ‘ä»¬çš„å›¾è¡¨ä¸­ã€‚

```py
def comma_format(x, pos):
    return f'{int(x):,}'

def feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot
    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1
    plt.plot(pred,metric,color='black',zorder=20)
    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)
    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  
    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)
    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();
    return

def plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix 
    my_colormap = plt.get_cmap('RdBu_r', 256)          
    newcolors = my_colormap(np.linspace(0, 1, 256))
    white = np.array([256/256, 256/256, 256/256, 1])
    white_low = int(128 - mask*128); white_high = int(128+mask*128)
    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)
    newcmp = ListedColormap(newcolors)
    m = corr_matrix.shape[0]
    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)
    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()
    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()
    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
    cbar = plt.colorbar(im, orientation = 'vertical')
    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.title(title)
    for i in range(0,m):
        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')
        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')
    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])

def visualize_model(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,title,axes_commas = True): # plots the data points and the decision tree prediction 
    n_classes = 10
    cmap_temp = plt.cm.inferno
    xplot_step = (x_max-x_min)/100; yplot_step = (y_max-y_min)/100
    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step),
                     np.arange(y_min, y_max, yplot_step))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    cs = plt.contourf(xx, yy, Z, cmap=cmap_temp,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))

    im = plt.scatter(xfeature,yfeature,s=30, c=response, marker=None, cmap=cmap_temp, norm=None, vmin=z_min, vmax=z_max, 
                     alpha=1.0, linewidths=0.8, edgecolors="black",zorder=10)
    plt.scatter(xfeature,yfeature,s=60, c='white', marker=None, cmap=cmap_temp, norm=None, vmin=z_min, vmax=z_max, 
                     alpha=1.0, linewidths=0.8, edgecolors=None,zorder=8)
    plt.title(title); plt.xlabel(xfeature.name); plt.ylabel(yfeature.name)
    cbar = plt.colorbar(im, orientation = 'vertical'); cbar.set_label(response.name, rotation=270, labelpad=20)
    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))
    if axes_commas == True:
        plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
        plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
    return Z

def visualize_tuned_model(k_tuned,k_mat,score_mat):
    plt.scatter(k_mat,score_mat,s=10.0, c="red", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, 
                linewidths=0.5, edgecolors="black")
    plt.plot([k_tuned,k_tuned],[0,10000000],color='black',linestyle=(6, (2,3)),label='tuned',zorder=1)
    plt.title('k-fold Cross Validation Error (MSE) vs. k Nearest Neighbours'); plt.xlabel('Number of Nearest Neighbours')
    plt.ylabel('Mean Square Error')
    plt.xlim(k_min,k_max); plt.ylim(0,np.max(score_mat))

def check_model(model,xtrain,ytrain,xtest,ytest,ymin,ymax,rtrain,rtest,title): # plots the estimated vs. the actual 
    predict_train = model.predict(np.c_[xtrain,ytrain])
    predict_test = model.predict(np.c_[xtest,ytest])
    plt.scatter(rtrain,predict_train,s=None, c='darkorange',marker=None, cmap=None, norm=None, vmin=None, vmax=None, 
                alpha=0.8, linewidths=0.8, edgecolors="black",label='Train')
    plt.scatter(rtest,predict_test,s=None, c='red',marker=None, cmap=None, norm=None, vmin=None, vmax=None, 
                alpha=0.8, linewidths=0.8, edgecolors="black",label='Test')
    plt.title(title); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')
    plt.xlim(ymin,ymax); plt.ylim(ymin,ymax)
    plt.arrow(ymin,ymin,ymax,ymax,width=0.02,color='black',head_length=0.0,head_width=0.0)
    MSE_train = metrics.mean_squared_error(rtrain,predict_train)
    Var_Explained_train = metrics.explained_variance_score(rtrain,predict_train)
    cor_train = math.sqrt(metrics.r2_score(rtrain,predict_train))
    MSE_test = metrics.mean_squared_error(rtest,predict_test)
    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.annotate('Train MSE: ' + str(f'{(np.round(MSE_train,2)):,}'),[0.05*(ymax-ymin)+ymin,0.95*(ymax-ymin)+ymin]) 
    plt.annotate('Test MSE:  ' + str(f'{(np.round(MSE_test,2)):,}'),[0.05*(ymax-ymin)+ymin,0.90*(ymax-ymin)+ymin])
    add_grid(); plt.legend(loc='lower right')
    # print('Mean Squared Error on Training = ', round(MSE_test,2),', Variance Explained =', round(Var_Explained,2),'Cor =', round(cor,2))

def weighted_percentile(data, weights, perc):                 # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) 
    ix = np.argsort(data)
    data = data[ix] 
    weights = weights[ix] 
    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) 
    return np.interp(perc, cdf, data)

def histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram 
    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)
    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')
    plt.plot([avg,avg],[0.0,45],color = color)
    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)
    html_str = ''
    for df in args:
        html_str += df.head().to_html()  # Using .head() for the first few rows
    display(HTML(f'<div style="display: flex;">{html_str}</div>')) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆæ¯æ¬¡é¿å…åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/PGE383")                                        # set the working directory 
```

æ‚¨å¿…é¡»æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ä»¥åŒ…å«æ‚¨è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”æ ¼å¼åœ¨ Mac ä¸Šä¸åŒï¼ˆä¾‹å¦‚ï¼šâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½è¡¨æ ¼æ•°æ®

è¿™æ˜¯å°†æˆ‘ä»¬çš„é€—å·åˆ†éš”æ•°æ®æ–‡ä»¶åŠ è½½åˆ° Pandas DataFrame å¯¹è±¡ä¸­çš„å‘½ä»¤ã€‚

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›† 'unconv_MV.csv'ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…æ‹¬æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼š

+   äº•å¹³å‡å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   å²©è„†æ€§æ¯”ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   ç»ç’ƒè´¨åå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹ç”Ÿäº§ 90 å¤©å¹³å‡ï¼ˆMCFPDï¼‰ã€‚

æ³¨æ„ï¼Œè¿™ä¸ªæ•°æ®é›†æ˜¯åˆæˆçš„ã€‚

æˆ‘ä»¬ä½¿ç”¨ pandas çš„ 'read_csv' å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸º 'my_data' çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

## å¯é€‰ï¼šå‘å“åº”ç‰¹å¾æ·»åŠ éšæœºå™ªå£°

æˆ‘ä»¬å¯ä»¥è¿™æ ·åšæ¥è§‚å¯Ÿæ•°æ®å™ªå£°å¯¹è¿‡æ‹Ÿåˆå’Œè¶…å‚æ•°è°ƒæ•´çš„å½±å“ã€‚

+   è¿™æ˜¯ä¸ºäº†ç»éªŒå­¦ä¹ ï¼Œå½“ç„¶æˆ‘ä»¬ä¸ä¼šå‘æˆ‘ä»¬çš„æ•°æ®æ·»åŠ éšæœºå™ªå£°

+   æˆ‘ä»¬è®¾ç½®äº†éšæœºæ•°ç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§

```py
df_load = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub 
df_load = df_load.iloc[:,1:8]                                 # copy all rows and columns 1 through 8, note 0 column is removed

response = 'Prod'                                             # specify the response feature
add_noise = True                                              # set True to add noise to response feature to demonstrate overfit
noise_stdev = 500                                             # amount of noise to add to response feature to demonstrate overfit

np.random.seed(seed = seed)                                   # set the random number seed
if add_noise == True:
    df_load[response] = df_load[response] + np.random.normal(loc=0.0,scale=noise_stdev,size = len(df_load))

X = df_load.copy(deep = False)
X = X.drop([response],axis='columns')                         # make predictor and response DataFrames
y = df_load.loc[:,response]

features = X.columns.values.tolist() + [y.name]               # store the names of the features
pred = X.columns.values.tolist()
resp = [y.name]

Xmin = [6.0,0.0,1.0,10.0,0.0,0.9]; Xmax = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minumum and maximum values for plotting
ymin = 1000.0; ymax = 9000.0

predlabel = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Brittleness Ratio (%)', # set the names for plotting
             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']
resplabel = 'Normalized Initial Production (MCFPD)'

predtitle = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting
             'Total Organic Carbon','Vitrinite Reflectance']
resptitle = 'Initial Production'

featurelabel = predlabel + [resplabel]                        # make feature labels and titles for concise code
featuretitle = predtitle + [resptitle]

m = len(pred) + 1
mpred = len(pred)

df = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) 
```

## å¯è§†åŒ– DataFrame

å¯è§†åŒ– DataFrame æ˜¯æ•°æ®çš„ç¬¬ä¸€æ­¥æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ï¼Œç­‰ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ DataFrame çš„â€˜headâ€™æˆå‘˜å‡½æ•°ï¼ˆä»¥æ•´æ´çš„æ ¼å¼é¢„è§ˆï¼Œè§ä¸‹æ–‡ï¼‰æ¥é¢„è§ˆã€‚

+   æ·»åŠ å‚æ•°â€˜n=13â€™ä»¥æŸ¥çœ‹æ•°æ®é›†çš„å‰ 13 è¡Œã€‚

```py
df.head(n=13) 
```

|  | Por | Perm | AI | Brittle | TOC | VR | Prod |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 12.08 | 2.92 | 2.80 | 81.40 | 1.16 | 2.31 | 1339.165488 |
| 1 | 12.38 | 3.53 | 3.22 | 46.17 | 0.89 | 1.88 | 3383.979252 |
| 2 | 14.02 | 2.59 | 4.01 | 72.80 | 0.89 | 2.72 | 2509.686720 |
| 3 | 17.67 | 6.75 | 2.63 | 39.81 | 1.08 | 1.88 | 5514.421023 |
| 4 | 17.52 | 4.57 | 3.18 | 10.94 | 1.51 | 1.90 | 3532.020478 |
| 5 | 14.53 | 4.81 | 2.69 | 53.60 | 0.94 | 1.67 | 4283.543382 |
| 6 | 13.49 | 3.60 | 2.93 | 63.71 | 0.80 | 1.85 | 3627.906723 |
| 7 | 11.58 | 3.03 | 3.25 | 53.00 | 0.69 | 1.93 | 3101.539533 |
| 8 | 12.52 | 2.72 | 2.43 | 65.77 | 0.95 | 1.98 | 3213.391047 |
| 9 | 13.25 | 3.94 | 3.71 | 66.20 | 1.14 | 2.65 | 2200.204701 |
| 10 | 15.04 | 4.39 | 2.22 | 61.11 | 1.08 | 1.77 | 3433.752662 |
| 11 | 16.19 | 6.30 | 2.29 | 49.10 | 1.53 | 1.86 | 4465.007131 |
| 12 | 16.82 | 5.42 | 2.80 | 66.65 | 1.17 | 1.98 | 4373.060709 |

## è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡

åœ¨ DataFrames ä¸­ä»è¡¨æ ¼æ•°æ®è®¡ç®—æ‘˜è¦ç»Ÿè®¡æœ‰å¾ˆå¤šé«˜æ•ˆçš„æ–¹æ³•ã€‚describe å‘½ä»¤ä»¥æ•´æ´çš„æ•°æ®è¡¨å½¢å¼æä¾›è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€‚

+   æˆ‘ä»¬æœ‰ä¸€äº›è´Ÿçš„ TOC å€¼ï¼è®©æˆ‘ä»¬æ£€æŸ¥åˆ†å¸ƒæƒ…å†µã€‚

```py
df.describe().transpose()                                     # calculate summary statistics for the data 
```

|  | è®¡æ•° | å¹³å‡å€¼ | æ ‡å‡†å·® | æœ€å°å€¼ | 25% | 50% | 75% | æœ€å¤§å€¼ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 200.0 | 14.991150 | 2.971176 | 6.550000 | 12.912500 | 15.070000 | 17.40250 | 23.550000 |
| Perm | 200.0 | 4.330750 | 1.731014 | 1.130000 | 3.122500 | 4.035000 | 5.28750 | 9.870000 |
| AI | 200.0 | 2.968850 | 0.566885 | 1.280000 | 2.547500 | 2.955000 | 3.34500 | 4.630000 |
| Brittle | 200.0 | 48.161950 | 14.129455 | 10.940000 | 37.755000 | 49.510000 | 58.26250 | 84.330000 |
| TOC | 200.0 | 0.990450 | 0.481588 | -0.190000 | 0.617500 | 1.030000 | 1.35000 | 2.180000 |
| VR | 200.0 | 1.964300 | 0.300827 | 0.930000 | 1.770000 | 1.960000 | 2.14250 | 2.870000 |
| Prod | 200.0 | 3842.630027 | 1594.301295 | 803.640483 | 2551.414599 | 3626.229052 | 4739.73408 | 9021.792491 |

+   åªæœ‰ä¸€ä¸¤ä¸ªç¨å¾®è´Ÿçš„å€¼ï¼Œæˆ‘ä»¬åªéœ€å°†å®ƒä»¬æˆªæ–­åˆ°é›¶ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å°† DataFrame ä¸­æ‰€æœ‰å°äº 0.0 çš„ TOC å€¼è®¾ç½®ä¸º 0.0ï¼Œå¦åˆ™ä¿æŒåŸå§‹çš„ TOC å€¼ã€‚

```py
num = df._get_numeric_data()                                  # get the numerical values
num[num < 0] = 0                                              # truncate negative values to 0.0
df.describe().transpose()                                     # calculate summary statistics for the data 
```

|  | è®¡æ•° | å¹³å‡å€¼ | æ ‡å‡†å·® | æœ€å°å€¼ | 25% | 50% | 75% | æœ€å¤§å€¼ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 200.0 | 14.991150 | 2.971176 | 6.550000 | 12.912500 | 15.070000 | 17.40250 | 23.550000 |
| Perm | 200.0 | 4.330750 | 1.731014 | 1.130000 | 3.122500 | 4.035000 | 5.28750 | 9.870000 |
| AI | 200.0 | 2.968850 | 0.566885 | 1.280000 | 2.547500 | 2.955000 | 3.34500 | 4.630000 |
| Brittle | 200.0 | 48.161950 | 14.129455 | 10.940000 | 37.755000 | 49.510000 | 58.26250 | 84.330000 |
| TOC | 200.0 | 0.991950 | 0.478264 | 0.000000 | 0.617500 | 1.030000 | 1.35000 | 2.180000 |
| VR | 200.0 | 1.964300 | 0.300827 | 0.930000 | 1.770000 | 1.960000 | 2.14250 | 2.870000 |
| Prod | 200.0 | 3842.630027 | 1594.301295 | 803.640483 | 2551.414599 | 3626.229052 | 4739.73408 | 9021.792491 |

æ£€æŸ¥æ€»ç»“ç»Ÿè®¡é‡æ˜¯å¥½çš„ã€‚

+   æ²¡æœ‰æ˜æ˜¾çš„é”™è¯¯

+   æ£€æŸ¥æ¯ä¸ªç‰¹å¾å€¼çš„èŒƒå›´ï¼Œä»¥è®¾ç½®å’Œè°ƒæ•´ç»˜å›¾é™åˆ¶ã€‚è§ä¸Šå›¾ã€‚

## è®¡ç®—ç›¸å…³çŸ©é˜µå’Œä¸å“åº”æ’åçš„ç›¸å…³æ€§

è®©æˆ‘ä»¬è¿›è¡Œç›¸å…³æ€§åˆ†æã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¹‹å‰å£°æ˜çš„å‡½æ•°è®¡ç®—å¹¶æŸ¥çœ‹ç›¸å…³çŸ©é˜µä»¥åŠä¸å“åº”ç‰¹å¾çš„å…³è”ã€‚

+   ç›¸å…³æ€§åˆ†æåŸºäºçº¿æ€§å…³ç³»çš„å‡è®¾ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªå¥½çš„å¼€å§‹

```py
corr_matrix = df.corr()
correlation = corr_matrix.iloc[:,-1].values[:-1]

plt.subplot(121)
plot_corr(corr_matrix,'Correlation Matrix',1.0,0.5)           # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(pred,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp[0],'Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![_images/9ccb60421ffa42bec24f02270e0cb19e4ab0827d13308e827a76a7a86b2c0dc6.png](img/6194a66796a23a77575acb7c89abb176.png)

æ³¨æ„ç”±äºæ¯ä¸ªå˜é‡ä¸å…¶è‡ªèº«çš„ç›¸å…³æ€§è€Œäº§ç”Ÿçš„ 1.0 å¯¹è§’çº¿ã€‚

è¿™çœ‹èµ·æ¥ä¸é”™ã€‚å­˜åœ¨ä¸åŒå¤§å°çš„ç›¸å…³æ€§ã€‚å½“ç„¶ï¼Œç›¸å…³ç³»æ•°é™äºçº¿æ€§ç›¸å…³ç¨‹åº¦çš„åº¦é‡ã€‚

+   è®©æˆ‘ä»¬æŸ¥çœ‹çŸ©é˜µæ•£ç‚¹å›¾ï¼Œä»¥äº†è§£ç‰¹å¾ä¹‹é—´çš„æˆå¯¹å…³ç³»ã€‚

```py
pairgrid = sns.PairGrid(df,vars=['Por','Perm','AI','Brittle','TOC','Prod']) # matrix scatter plots
pairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)
pairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle
pairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, 
                              alpha = 1.0, n_levels = 10)
pairgrid.add_legend()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/4c23ae18eda1e533de50c2330dba444a3862043169ebd7569532055f40e72e5f.png](img/18798d69cb2a69a3c10006380bba6873.png)

## ä»…ä½¿ç”¨ä¸¤ä¸ªé¢„æµ‹ç‰¹å¾

è®©æˆ‘ä»¬å°†é—®é¢˜ç®€åŒ–ä¸º 2 ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œå³å­”éš™ç‡å’Œè„†æ€§ï¼Œä»¥é¢„æµ‹ç”Ÿäº§ç‡ã€‚é€šè¿‡ä»…ä½¿ç”¨ 2 ä¸ªç‰¹å¾ï¼Œå¾ˆå®¹æ˜“å¯è§†åŒ–ç‰¹å¾ç©ºé—´çš„åˆ†å‰²ï¼ˆå®ƒåªæœ‰ 2 ç»´ï¼Œå¯ä»¥åœ¨å•ä¸ªå›¾è¡¨ä¸Šå®Œå…¨æ˜¾ç¤ºï¼‰ã€‚

## æ ‡å‡†åŒ–é¢„æµ‹ç‰¹å¾

k-è¿‘é‚»æ–¹æ³•åœ¨ç‰¹å¾ç©ºé—´ä¸­æ‰§è¡Œæœ€è¿‘è®­ç»ƒæ ·æœ¬æœç´¢ï¼ˆç±»ä¼¼äº k-means èšç±»ï¼‰ã€‚ä¸ºäº†æ¶ˆé™¤æ–¹æ³•ä¸­ç‰¹å¾èŒƒå›´çš„å½±å“ï¼Œæˆ‘ä»¬æ ‡å‡†åŒ–äº†ç‰¹å¾ã€‚

+   æˆ‘ä»¬å°†æ ‡å‡†åŒ–æˆ‘ä»¬çš„é¢„æµ‹ç‰¹å¾ï¼Œä½¿å…¶å‡å€¼ä¸ºé›¶ï¼Œæ–¹å·®ä¸º 1ã€‚

+   æˆ‘ä»¬ä½¿ç”¨ scikit-learn é¢„å¤„ç†æ¨¡å—æ¥ç®€åŒ–è¿™ä¸€æ­¥éª¤ï¼Œå¹¶æä¾›ä¸€ä¸ªæ–¹ä¾¿ä¸”å®‰å…¨çš„åå‘è½¬æ¢ã€‚

```py
if1 = 0; if2 = 3                                              # selected predictor features

transform = StandardScaler();                                 # instantiate feature standardization method

sel_pred = [pred[if1],pred[if2]]
sel_features = pred + [resp]

spredlabel = ['Standardized ' + element for element in predlabel] # standardized predictors list

sel_spredlabel = [spredlabel[if1]] + [spredlabel[if2]] 

sel_spred = ['s' + element for element in sel_pred]           # standardized predictors list

df[sel_spred[0]] = transform.fit_transform(df.loc[:,sel_pred].values)[:,0] # standardize the data features to mean = 0, var = 1.0
df[sel_spred[1]] = transform.fit_transform(df.loc[:,sel_pred].values)[:,1] # standardize the data features to mean = 0, var = 1.0

print('Selected Predictor Features: ' + str(sel_pred))
print('Standardized Selected Predictor Features: ' + str(sel_spred))
print('Response Feature: ' + str([resp]))
df.head() 
```

```py
Selected Predictor Features: ['Por', 'Brittle']
Standardized Selected Predictor Features: ['sPor', 'sBrittle']
Response Feature: [['Prod']] 
```

|  | Por | Perm | AI | Brittle | TOC | VR | Prod | sPor | sBrittle |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 12.08 | 2.92 | 2.80 | 81.40 | 1.16 | 2.31 | 1339.165488 | -0.982256 | 2.358297 |
| 1 | 12.38 | 3.53 | 3.22 | 46.17 | 0.89 | 1.88 | 3383.979252 | -0.881032 | -0.141332 |
| 2 | 14.02 | 2.59 | 4.01 | 72.80 | 0.89 | 2.72 | 2509.686720 | -0.327677 | 1.748113 |
| 3 | 17.67 | 6.75 | 2.63 | 39.81 | 1.08 | 1.88 | 5514.421023 | 0.903875 | -0.592585 |
| 4 | 17.52 | 4.57 | 3.18 | 10.94 | 1.51 | 1.90 | 3532.020478 | 0.853263 | -2.640962 |

è®©æˆ‘ä»¬æ¼”ç¤ºä»æ ‡å‡†åŒ–ç‰¹å¾åˆ°åŸå§‹ç‰¹å¾çš„åå‘è½¬æ¢ã€‚

+   åœ¨æˆ‘ä»¬çš„å·¥ä½œæµç¨‹ä¸­æˆ‘ä»¬ä¸éœ€è¦è¿™ä¸ªï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦å°†é¢„æµ‹ç‰¹å¾å‰å‘è½¬æ¢ä»¥è®­ç»ƒæ¨¡å‹å’Œåšå‡ºé¢„æµ‹

```py
print('Backtransformed: \n Por    Brittle')
transform.inverse_transform(df.loc[:,sel_spred])[:5,:]        # check the reverse standardization 
```

```py
Backtransformed: 
        Por    Brittle 
```

```py
array([[12.08, 81.4 ],
       [12.38, 46.17],
       [14.02, 72.8 ],
       [17.67, 39.81],
       [17.52, 10.94]]) 
```

æˆ‘ä»¬å¯ä»¥å°†ä¸Šé¢çš„è¾“å‡ºä¸åŸå§‹çš„å­”éš™ç‡å’Œè„†æ€§è¿›è¡Œæ¯”è¾ƒã€‚åå‘è½¬æ¢æ˜¯æœ‰æ•ˆçš„ï¼

+   å½“éœ€è¦æ—¶ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ­¤æ–¹æ³•è¿”å›åŸå§‹ç‰¹å¾å•ä½ã€‚

+   é€šå¸¸ï¼Œå¯¹äºé¢„æµ‹ç‰¹å¾ï¼Œä¸éœ€è¦è¿›è¡Œåå‘è½¬æ¢ï¼Œæˆ‘ä»¬åªå¯¹é¢„æµ‹ç‰¹å¾è¿›è¡Œæ­£å‘è½¬æ¢ä»¥é¢„æµ‹å“åº”ç‰¹å¾ã€‚

+   åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œåœ¨æ„å»ºæˆ‘ä»¬çš„æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ä¸éœ€è¦è½¬æ¢å“åº”ç‰¹å¾ã€‚å“åº”ç‰¹å¾åˆ†å¸ƒè¡¨ç°è‰¯å¥½ï¼Œk æœ€è¿‘é‚»ç†è®ºä¸­æ²¡æœ‰æœŸæœ›å“åº”ç‰¹å¾å…·æœ‰ç‰¹å®šèŒƒå›´æˆ–åˆ†å¸ƒä»½é¢ã€‚

## ç‰¹å¾èŒƒå›´

è®©æˆ‘ä»¬è®¾ç½®ä¸€äº›ç»˜å›¾èŒƒå›´ã€‚æ³¨æ„ï¼Œå¯¹äºæ ‡å‡†åŒ–çš„é¢„æµ‹ç‰¹å¾ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨-3.5 åˆ° 3.5 ä½œä¸ºé™åˆ¶ã€‚

```py
Xmin = [5.0, 0.0]; Xmax = [25.0,100.0]                        # selected predictor features min and max 
```

## è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²

ä¸ºäº†æ–¹ä¾¿å’Œç®€å•ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn çš„éšæœºè®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²ã€‚

```py
X_train, X_test, y_train, y_test = train_test_split(df.loc[:,sel_spred],df.loc[:,resp],test_size=0.25,random_state=73073)
df_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)
df_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) 
```

è®©æˆ‘ä»¬å…ˆæ£€æŸ¥å­”éš™ç‡ã€è„†æ€§å’Œç”Ÿäº§çš„å•å˜é‡ç»Ÿè®¡ã€‚

```py
nbins = 20                                                    # number of histogram bins

plt.subplot(221)
plt.hist(X_train[sel_spred[0]],alpha = 0.8,color = 'darkorange',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Train')
plt.hist(X_test[sel_spred[0]],alpha = 0.8,color = 'red',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Test')
plt.title(sel_spred[0]); plt.xlim(-3,3); plt.xlabel(sel_spredlabel[0]); plt.ylabel('Frequency'); add_grid(); plt.legend(loc='upper right')

plt.subplot(222)
plt.hist(X_train[sel_spred[1]],alpha = 0.8,color = 'darkorange',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Train')
plt.hist(X_test[sel_spred[1]],alpha = 0.8,color = 'red',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Test')
plt.title(sel_spred[1]); plt.xlim(-3,3); plt.xlabel(sel_spredlabel[1]); plt.ylabel('Frequency'); add_grid(); plt.legend(loc='upper right')

plt.subplot(223)
plt.hist(y_train[resp],alpha = 0.8,color = 'darkorange',edgecolor = 'black',bins=np.linspace(ymin,ymax,nbins),label='Train')
plt.hist(y_test[resp],alpha = 0.8,color = 'red',edgecolor = 'black',bins=np.linspace(ymin,ymax,nbins),label='Test')
plt.legend(loc='upper right'); plt.title(resp[0]); plt.xlim(ymin,ymax) 
plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
plt.xlabel(resplabel); plt.ylabel('Frequency'); add_grid()

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/2447bd1e6857b017d16d4197616a3dc1.png)

åˆ†å¸ƒè¡¨ç°è‰¯å¥½ï¼Œ

+   æˆ‘ä»¬æ— æ³•è§‚å¯Ÿåˆ°æ˜æ˜¾çš„ç¼ºå£æˆ–æˆªæ–­ã€‚

+   æ£€æŸ¥è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„è¦†ç›–ç‡

è®©æˆ‘ä»¬çœ‹çœ‹å­”éš™ç‡ä¸è„†æ€§çš„æ•£ç‚¹å›¾ï¼Œç‚¹æ ¹æ®ç”Ÿäº§é‡ç€è‰²ã€‚

```py
plt.subplot(121)                                              # train data plot
im = plt.scatter(X_train[sel_spred[0]],X_train[sel_spred[1]],s=None, c=y_train[resp[0]], marker=None, cmap=cmap, norm=None, vmin=ymin, 
                 vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors="black")
plt.title('Train ' + resp[0] + ' vs. ' + sel_spred[1] + ' and ' + sel_spred[1]); plt.xlabel(sel_spredlabel[0]); plt.ylabel(sel_spredlabel[1])
plt.xlim(-3,3); plt.ylim(-3,3)
cbar = plt.colorbar(im, orientation = 'vertical')
cbar.set_label(resplabel, rotation=270, labelpad=20)
cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))
add_grid()

plt.subplot(122)                                               # test data plot
im = plt.scatter(X_test[sel_spred[0]],X_test[sel_spred[1]],s=None, c=y_test[resp[0]], marker=None, cmap=cmap, norm=None, vmin=ymin, 
                 vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors="black")
plt.title('Test ' + resp[0] + ' vs. ' + sel_spred[1] + ' and ' + sel_spred[1]); plt.xlabel(sel_spredlabel[0]); plt.ylabel(sel_spredlabel[1])
plt.xlim(-3,3); plt.ylim(-3,3)
cbar = plt.colorbar(im, orientation = 'vertical')
cbar.set_label(resplabel, rotation=270, labelpad=20)
cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))
add_grid()

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/0b1f8f8266faddc853479c3aab599c2e.png)

è¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥æ˜¯éçº¿æ€§çš„ï¼Œä¸èƒ½ç”¨ç®€å•çš„çº¿æ€§å›å½’æ¥å»ºæ¨¡ã€‚

+   çœ‹èµ·æ¥ï¼Œè„†æ€§å’Œå¢åŠ å­”éš™ç‡éƒ½æœ‰ä¸€ä¸ªæœ€ä½³ç‚¹ï¼Œå¯¹ç”Ÿäº§æ€»æ˜¯æœ‰ç›Šçš„ã€‚

## ä½¿ç”¨ k æœ€è¿‘é‚»å®ä¾‹åŒ–ã€æ‹Ÿåˆå’Œé¢„æµ‹

è®©æˆ‘ä»¬å®ä¾‹åŒ–ã€æ‹Ÿåˆå¹¶ä½¿ç”¨ k æœ€è¿‘é‚»æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚

+   ä½¿ç”¨è¶…å‚æ•°å’Œ k æœ€è¿‘é‚»å®ä¾‹åŒ–å®ƒ

+   ä½¿ç”¨è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn çš„æ ‡å‡† fit å‡½æ•°ã€‚

```py
n_neighbours = 1; p = 2; weights = 'uniform'                 # model hyperparameters
neigh = KNeighborsRegressor(weights = weights, n_neighbors=n_neighbours, p = p) # instantiate the prediction model 
```

æˆ‘ä»¬å·²ç»è®¾ç½®äº†è¶…å‚æ•°ï¼š

+   weights = æ ¹æ®æœ€è¿‘é‚»å±…ç»™å‡ºçš„é¢„æµ‹çš„å¹³å‡æƒé‡ã€‚â€˜uniformâ€™æ˜¯ç®—æœ¯å¹³å‡ï¼Œè€Œâ€˜distanceâ€™æ˜¯è·ç¦»åŠ æƒã€‚

+   n_neighbours = æœ€å¤§é‚»å±…æ•°é‡ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬é€šè¿‡é™åˆ¶åˆ° 5 ä¸ªæœ€è¿‘é‚»å±…æ¥çº¦æŸæˆ‘ä»¬çš„é¢„æµ‹ã€‚

+   p = è·ç¦»åº¦é‡å¹‚æˆ– Minkowski åº¦é‡ï¼ˆ1 = æ›¼å“ˆé¡¿è·ç¦»ï¼Œ2 ä¸ºæ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰ï¼Œç”¨äºå¯»æ‰¾æœ€è¿‘é‚»å±…ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½æ ¹æ®å­”éš™ç‡å’Œè„†æ€§æ¥é¢„æµ‹ç”Ÿäº§ã€‚

+   æˆ‘ä»¬å°†ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„ä¸¤ä¸ªå‡½æ•°æ¥å¯è§†åŒ–ç‰¹å¾ç©ºé—´ä¸­çš„ k æœ€è¿‘é‚»é¢„æµ‹ä»¥åŠè®­ç»ƒæ•°æ®ä¸­å®é™…å’Œä¼°è®¡ç”Ÿäº§çš„äº¤å‰å›¾ï¼Œä»¥åŠæ¥è‡ª sklearn.metric æ¨¡å—çš„ä¸‰ä¸ªæ¨¡å‹åº¦é‡ã€‚

```py
neigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data

plt.subplot(221)                                              # training data vs. the model predictions
Z = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,
                    'Training Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(222)                                              # testing data vs. the model predictions
visualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,
                'Testing Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

# plt.subplot(223)                                              # model accuracy check
# check_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,
#             y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/8a097688955c9a933b8a09894a1a0ea5.png)

æ¨¡å‹çœ‹èµ·æ¥ä¸é”™ï¼š

+   éå‚æ•°æ–¹æ³•åœ¨æ‹Ÿåˆé¢„æµ‹ç‰¹å¾ç©ºé—´ä¸­çš„éçº¿æ€§å“åº”æ¨¡å¼æ–¹é¢éå¸¸çµæ´»ã€‚

+   æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç”±äº k æœ€è¿‘é‚»æ•°æ®æœ‰é™å’Œä½¿ç”¨å‡åŒ€åŠ æƒè€Œäº§ç”Ÿçš„æœç´¢ä¼ªå½±ã€‚

+   æˆ‘ä»¬å¯¹è¿™ä¸ªä½ç»´é—®é¢˜ï¼ˆåªæœ‰ 2 ä¸ªé¢„æµ‹ç‰¹å¾ï¼‰æœ‰å¯†é›†çš„æ•°æ®ã€‚

+   æµ‹è¯•æ•°æ®å’Œè®­ç»ƒæ•°æ®åœ¨é¢„æµ‹ç‰¹å¾ç©ºé—´ä¸­ä¸€è‡´ä¸”æ¥è¿‘ã€‚

è®©æˆ‘ä»¬å°è¯•é€šè¿‡ä½¿ç”¨éå¸¸å¤§çš„ k è¶…å‚æ•°æ¥è¿‡æ‹Ÿåˆæ¨¡å‹ã€‚

```py
n_neighbours = 5; p = 2; weights = 'uniform'                # model hyperparameters
neigh = KNeighborsRegressor(weights = weights,n_neighbors=n_neighbours,p = p) # instantiate the prediction model

neigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data

plt.subplot(221)                                              # training data vs. the model predictions
Z = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,
                    'Training Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(222)                                              # testing data vs. the model predictions
visualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,
                'Testing Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(223)                                              # model accuracy check
check_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,
            y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/7a8fad6c8ab65ba24660fe87eb558d34.png)

æ³¨æ„ï¼Œè¿™å¹³æ»‘äº†å“åº”ï¼Œé¢„æµ‹æ­£æ¥è¿‘å…¨å±€å‡å€¼ã€‚

+   æˆ‘ä»¬æœ‰ä¸€ä¸ªæ¬ æ‹Ÿåˆæ¨¡å‹ã€‚

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„ k-æœ€è¿‘é‚»é¢„æµ‹æ¨¡å‹ä½¿ç”¨è¾ƒå°çš„ k è¶…å‚æ•°ã€‚

```py
n_neighbours = 1; p = 2; weights = 'uniform'                  # model hyperparameters
neigh = KNeighborsRegressor(weights = weights,n_neighbors=n_neighbours,p = p) # instantiate the prediction model

neigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data

plt.subplot(221)                                              # training data vs. the model predictions
Z = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,
                    'Training Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(222)                                              # testing data vs. the model predictions
visualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,
                'Testing Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(223)                                              # model accuracy check
check_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,
            y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/652c1c3e20ca9e893f0cf17db9352459.png)

ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªæç«¯è¿‡æ‹Ÿåˆæ¨¡å‹ã€‚

+   è®­ç»ƒå‡æ–¹è¯¯å·®ä¸º 0.0ï¼Œè€Œæµ‹è¯•è¯¯å·®ç›¸å½“é«˜ã€‚

+   æ³¨æ„ï¼Œæˆ‘ä»¬è¿‡æ‹Ÿåˆæ¨¡å‹ä¸­çš„æŸäº›é¢„æµ‹è¶…å‡ºäº†ç»˜å›¾çš„æœ€å°å’Œæœ€å¤§å“åº”ç‰¹å¾å€¼ã€‚

è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨ L1ï¼Œæ›¼å“ˆé¡¿è·ç¦»æ¥æ‰¾åˆ° k ä¸ªæœ€è¿‘é‚»ã€‚

```py
n_neighbours = 10; p = 1; weights = 'uniform'                 # model hyperparameters
neigh = KNeighborsRegressor(weights = weights,n_neighbors=n_neighbours,p = p) # instantiate the prediction model

neigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data

plt.subplot(221)                                              # training data vs. the model predictions
Z = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,
                    'Training Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(222)                                              # testing data vs. the model predictions
visualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,
                'Testing Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(223)                                              # model accuracy check
check_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,
            y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/3a4e29a43415592363cfc3314ffd6b4b.png)

å°†è¿™ä¸ªé¢„æµ‹æ¨¡å‹ä¸æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å”¯ä¸€æ”¹å˜çš„æ˜¯å°† k æœ€è¿‘æ ·æœ¬çš„è·ç¦»æœç´¢ä»æ¬§å‡ é‡Œå¾—è·ç¦»æ›´æ”¹ä¸ºæ›¼å“ˆé¡¿è·ç¦»ã€‚

+   æœç´¢ä¼ªå½±ç°åœ¨ä¸ç‰¹å¾å¯¹é½ï¼ˆå°„çº¿åœ¨ x å’Œ y æ–¹å‘ä¸Šå®šå‘ï¼‰ã€‚

## k-æœ€è¿‘é‚»çš„å‚æ•°è°ƒæ•´

è®©æˆ‘ä»¬åœ¨è°ƒæ•´è¶…å‚æ•°æ—¶æ£€æŸ¥è¿™ä¸€ç‚¹ã€‚

é‚£ä¹ˆ$k$çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

+   å°çš„$k$è¶…å‚æ•°å¯¼è‡´åœ¨é¢„æµ‹ç‰¹å¾ç©ºé—´ä¸Šçš„å±€éƒ¨ç‰¹å®šé¢„æµ‹æ¨¡å‹ã€‚

+   å¤§çš„$k$è¶…å‚æ•°å¯¼è‡´åœ¨é¢„æµ‹ç‰¹å¾ç©ºé—´ä¸Šçš„æ›´å¹³æ»‘ã€å…¨å±€æ‹Ÿåˆçš„é¢„æµ‹æ¨¡å‹ã€‚

è¿™ä¸å…¶ä»–æ¨¡å‹ï¼ˆå¦‚å†³ç­–æ ‘ï¼‰è§‚å¯Ÿåˆ°çš„ä»ä½åˆ°é«˜å¤æ‚åº¦ç±»ä¼¼ã€‚

+   å°çš„$k$å¤æ‚ã€‚

+   å¤§çš„$k$ç®€å•ã€‚

æˆ‘ä»¬éœ€è¦è°ƒæ•´å¤æ‚åº¦ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## è¶…å‚æ•°è°ƒæ•´

è®©æˆ‘ä»¬éå†å¤šä¸ª$k$æœ€è¿‘é‚»ï¼Œä»¥å¹³å‡å’Œé€†è·ç¦»ä¼°è®¡æ¥è®¿é—®ä¸æµ‹è¯•ç²¾åº¦ç›¸å…³çš„æœ€ä½³è¶…å‚æ•°ã€‚

```py
k = 1                                                         # set initial, lowest k hyperparameter
dist_error = []; unif_error = []; k_mat = []                  # make lists to store the results
while k <= 150:                                               # loop over the k hyperparameter
    neigh_dist = KNeighborsRegressor(weights = 'distance', n_neighbors=k, p = 2) # instantiate the model
    neigh_dist_fit = neigh_dist.fit(X_train,y_train)          # train the model with the training data
    y_pred = neigh_dist_fit.predict(X_test)                   # predict over the testing cases
    MSE = metrics.mean_squared_error(y_test,y_pred)           # calculate the MSE testing
    dist_error.append(MSE)                                    # add to the list of MSE

    neigh_unif = KNeighborsRegressor(weights = 'uniform', n_neighbors=k, p = 2)
    neigh_unif_fit = neigh_unif.fit(X_train,y_train)          # train the model with the training data
    y_pred = neigh_unif_fit.predict(X_test)                   # predict over the testing cases
    MSE = metrics.mean_squared_error(y_test,y_pred)           # calculate the MSE testing
    unif_error.append(MSE)                                    # add to the list of MSE

    k_mat.append(k)                                           # append k to an array for plotting
    k = k + 1 
```

ç°åœ¨è®©æˆ‘ä»¬ç»˜åˆ¶ç»“æœã€‚

```py
plt.subplot(111)
plt.scatter(k_mat,dist_error,s=None, c='red',label = 'inverse distance weighted', marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors="black")
plt.scatter(k_mat,unif_error,s=None, c='blue',label = 'arithmetic average', marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors="black")
plt.title('Testing Error vs. Number of Nearest Neighbours'); plt.xlabel('Number of Nearest Neighbours'); plt.ylabel('Mean Square Error')
plt.legend(); add_grid()
plt.xlim(0,50); plt.ylim([0,750000])
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8,wspace=0.15,hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/fe795ea54d585888e2632307876aaa72.png)

æˆ‘ä»¬èƒ½ä»è¿™ä¸ªç»“æœè§‚å¯Ÿåˆ°ä»€ä¹ˆï¼Ÿ

+   åœ¨$k = 12$ä¸ªæœ€è¿‘é‚»æ—¶ï¼Œæˆ‘ä»¬æœ€å°åŒ–äº†æµ‹è¯•ä¸­çš„å‡æ–¹è¯¯å·®ã€‚

+   ä¸ç®—æœ¯å¹³å‡ï¼ˆåœ¨é¢„æµ‹ç‰¹å¾ç©ºé—´ä¸­å¯¹ k ä¸ªæœ€è¿‘è®­ç»ƒæ•°æ®è¿›è¡Œå‡åŒ€åŠ æƒï¼‰ç›¸æ¯”ï¼Œé€†è·ç¦»åŠ æƒæœ‰æ›´å¥½çš„æ€§èƒ½ã€‚

æˆ‘ä»¬æ¨¡å‹çš„æœ€ä½³ç‰¹å®šæ€§å’Œå¤æ‚åº¦æ˜¯å­˜åœ¨çš„ã€‚

+   1 ä¸ªæœ€è¿‘é‚»æ˜¯ä¸€ä¸ªéå¸¸å±€éƒ¨ç‰¹å®šçš„æ¨¡å‹ï¼ˆè¿‡æ‹Ÿåˆï¼‰ã€‚

+   åŒ…å«å¤ªå¤šä¿¡æ¯çš„è®¸å¤šæœ€è¿‘é‚»è¿‡äºæ³›åŒ–ï¼ˆæ¬ æ‹Ÿåˆï¼‰ã€‚

æˆ‘ä»¬æ­£åœ¨è§‚å¯Ÿ$k$æœ€è¿‘é‚»æ¨¡å‹çš„ç²¾åº¦ä¸å¤æ‚åº¦ä¹‹é—´çš„æƒè¡¡ã€‚

## k æŠ˜äº¤å‰éªŒè¯

é€šè¿‡è§‚å¯Ÿå‡†ç¡®ç‡ä¸å¤æ‚æ€§çš„æƒè¡¡ï¼Œè¯„ä¼°æˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½æ˜¯æœ‰ç”¨çš„ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬çœŸæ­£æƒ³è¦åšçš„æ˜¯ä¸¥æ ¼æµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åº”è¯¥æ‰§è¡Œæ›´ä¸¥æ ¼çš„äº¤å‰éªŒè¯ï¼Œå®ƒèƒ½æ›´å¥½åœ°è¯„ä¼°ä¸åŒçš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ã€‚scikit learn æœ‰ä¸€ä¸ªå†…ç½®çš„äº¤å‰éªŒè¯æ–¹æ³•å«åš cross_val_scoreï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥ï¼š

1.  åº”ç”¨ k æŠ˜æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£åˆ†ç¦»è®­ç»ƒå’Œæµ‹è¯•æ•°æ®

1.  è‡ªåŠ¨åŒ–æ¨¡å‹æ„å»ºï¼Œå¾ªç¯éå†æŠ˜å å¹¶å¹³å‡æ„Ÿå…´è¶£çš„æŒ‡æ ‡

è®©æˆ‘ä»¬åœ¨å…·æœ‰å¯å˜ $k$ ä¸ªæœ€è¿‘é‚»çš„ k è¿‘é‚»é¢„æµ‹ä¸Šè¯•ä¸€è¯•ã€‚æ³¨æ„äº¤å‰éªŒè¯è®¾ç½®ä¸ºä½¿ç”¨ 4 ä¸ªå¤„ç†å™¨ï¼Œä½†ä»å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ‰èƒ½è¿è¡Œã€‚

```py
score = []                                                  # code modified from StackOverFlow by Dimosthenis
k_mat = []
for k in range(1,150):
    neigh_dist = KNeighborsRegressor(weights = 'distance', n_neighbors=k, p = 1)
    scores = cross_val_score(estimator=neigh_dist, X= np.c_[df['sPor'],df['sBrittle']],y=df['Prod'], cv=4, n_jobs=4,
                             scoring = "neg_mean_squared_error") # Perform 7-fold cross validation
    score.append(abs(scores.mean()))
    k_mat.append(k) 
```

è¾“å‡ºæ˜¯ä¸€ä¸ªæ•°ç»„ï¼ŒåŒ…å«æ¯ä¸ªå¤æ‚åº¦çº§åˆ«ï¼ˆ$k$ ä¸ªæœ€è¿‘é‚»çš„æ•°é‡ï¼‰çš„å¹³å‡åˆ†æ•°ï¼ˆå‡æ–¹è¯¯å·®ï¼‰ï¼Œä»¥åŠä¸€ä¸ªåŒ…å« $k$ å€¼çš„æ•°ç»„ã€‚

```py
plt.figure(figsize=(8,6))
plt.scatter(k_mat,score,s=None, c="red", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.5, edgecolors="black")
plt.title('k-fold Cross Validation Error (MSE) vs. k Nearest Neighbours'); plt.xlabel('Number of Nearest Neighbours'); plt.ylabel('Mean Square Error')
plt.xlim(1,150); plt.ylim([0,1400000]); add_grid()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8,wspace=0.15,hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/d79973afda6b57fc45418841e4ff114d.png)

åœ¨ k æŠ˜äº¤å‰éªŒè¯æ¨¡å‹æµ‹è¯•ä¸­ï¼Œä½¿ç”¨ 10 ä¸ªæœ€è¿‘é‚»çš„å‚æ•°æˆ‘ä»¬å¾—åˆ°äº†æœ€é«˜çš„å‡†ç¡®ç‡ã€‚

## é¢„æµ‹ç‰¹å¾æ ‡å‡†åŒ–

æˆ‘ä»¬å·²ç»å°†é¢„æµ‹ç‰¹å¾æ ‡å‡†åŒ–ï¼Œä»¥æ¶ˆé™¤å®ƒä»¬èŒƒå›´çš„å½±å“ã€‚

+   å¦‚æœæˆ‘ä»¬ä½¿ç”¨åŸå§‹é¢„æµ‹ç‰¹å¾ä¼šæ€æ ·å‘¢ï¼Ÿ

è®©æˆ‘ä»¬è¯•ä¸€è¯•ã€‚

+   æˆ‘ä»¬é¦–å…ˆä½¿ç”¨åŸå§‹ç‰¹å¾è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²ï¼Œè€Œä¸è¿›è¡Œæ ‡å‡†åŒ–

```py
X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(df.loc[:,sel_pred],df.loc[:,resp],test_size=0.25,random_state=73073)

neigh = KNeighborsRegressor(weights = 'distance', n_neighbors=15, p = 1)
neigh_fit = neigh.fit(X_train_orig,y_train_orig)                # train the model with the training data

plt.subplot(121)                                              # training data vs. the model predictions
Z = visualize_model(neigh_fit,X_train_orig[sel_pred[0]],Xmin[0],Xmax[0],X_train_orig[sel_pred[1]],Xmin[1],Xmax[1],y_train[resp[0]],ymin,ymax,
                    'Training Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(122)                                              # testing data vs. the model predictions
visualize_model(neigh_fit,X_test_orig[sel_pred[0]],Xmin[0],Xmax[0],X_test_orig[sel_pred[1]],Xmin[1],Xmax[1],y_test[resp[0]],ymin,ymax,
                'Testing Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/7c1a8ce67e0629a7c8b0a3ceccdf2686.png)

ä½ èƒ½çœ‹åˆ°æ°´å¹³çš„æ¡çº¹å—ï¼Ÿè„†æ€§ä¸å­”éš™ç‡ä¹‹é—´çš„å¤§èŒƒå›´é‡çº§å·®å¼‚å¯¼è‡´äº†è¿™ç§æ¡çº¹ã€‚

+   ç‰¹å¾ç©ºé—´ä¸­çš„è·ç¦»å¯¹è„†æ€§çš„ç›¸å¯¹å˜åŒ–æ¯”å­”éš™ç‡æ›´æ•æ„Ÿ

è®©æˆ‘ä»¬å°†å­”éš™ç‡è½¬æ¢ä¸ºåˆ†æ•°ï¼Œå¹¶è§‚å¯Ÿç”±äºå°†å­”éš™ç‡ä½œä¸ºåˆ†æ•°è€Œä¸æ˜¯ç™¾åˆ†æ¯”å¤„ç†çš„ä»»æ„å†³ç­–ï¼Œæˆ‘ä»¬çš„é¢„æµ‹æŒ‡æ ‡å‘ç”Ÿäº†æ€æ ·çš„å˜åŒ–ã€‚

+   é€šè¿‡å°†æ‰€æœ‰å­”éš™ç‡å€¼ä¹˜ä»¥ $\frac{1}{100}$ çš„ç³»æ•°ã€‚

```py
X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(df.loc[:,sel_pred],df.loc[:,resp],test_size=0.25,random_state=73073)

X_train_orig['Por'] = X_train_orig['Por']/100.0
X_test_orig['Por'] = X_test_orig['Por']/100.0

neigh = KNeighborsRegressor(weights = 'distance', n_neighbors=15, p = 1)
neigh_fit = neigh.fit(X_train_orig,y_train_orig)                # train the model with the training data

plt.subplot(121)                                              # training data vs. the model predictions
Z = visualize_model(neigh_fit,X_train_orig[sel_pred[0]],Xmin[0]/100,Xmax[0]/100,X_train_orig[sel_pred[1]],Xmin[1],Xmax[1],y_train[resp[0]],ymin,ymax,
                    'Training Data and k Nearest Neighbours',False)
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(122)                                              # testing data vs. the model predictions
visualize_model(neigh_fit,X_test_orig[sel_pred[0]],Xmin[0]/100,Xmax[0]/100,X_test_orig[sel_pred[1]],Xmin[1],Xmax[1],y_test[resp[0]],ymin,ymax,
                'Testing Data and k Nearest Neighbours',False)
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/bf1f50f1e62336305668f4aff28ac682.png)

+   å½“æˆ‘ä»¬å°†ç™¾åˆ†æ¯”å­”éš™ç‡è½¬æ¢ä¸ºåˆ†æ•°å­”éš™ç‡æ—¶ï¼Œè¿™ç§æ¡çº¹æ•ˆåº”å˜å¾—æ›´åŠ ä¸¥é‡ï¼Œå› ä¸ºå­”éš™ç‡ä¸­çš„è·ç¦»çœ‹èµ·æ¥æ¯”è„†æ€§ä¸­çš„è·ç¦»è¦è¿‘å¾—å¤šï¼Œè¿™ä»…ä»…æ˜¯å› ä¸ºç‰¹å¾èŒƒå›´çš„ä¸åŒã€‚

æˆ‘ä»¬åˆ†é…æœ€è¿‘é‚»çš„è·ç¦»åº¦é‡å¯¹ç‰¹å¾å•ä½éå¸¸æ•æ„Ÿã€‚åœ¨æˆ‘ä»¬ä½¿ç”¨å®ƒä»¬æ¥æ„å»º $k$-è¿‘é‚»å›å½’æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥å§‹ç»ˆå¯¹æ‰€æœ‰é¢„æµ‹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆä½¿å®ƒä»¬å¤„äºå¹³ç­‰åœ°ä½ï¼‰ï¼

## scikit-learn ä¸­çš„ k è¿‘é‚»å›å½’ä¸ç®¡é“

éœ€è¦æ ‡å‡†åŒ–ç‰¹å¾ã€è®­ç»ƒã€è°ƒæ•´å¹¶ä½¿ç”¨æ‰€æœ‰æ•°æ®é‡æ–°è®­ç»ƒè°ƒæ•´åçš„æ¨¡å‹å¯èƒ½çœ‹èµ·æ¥è¦åšå¾ˆå¤šå·¥ä½œï¼

+   ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨ scikit-learn ä¸­çš„ Pipeline å¯¹è±¡ã€‚

è¿™é‡Œæœ‰ä¸€äº›å…³äºç®¡é“çš„äº®ç‚¹ã€‚

### æœºå™¨å­¦ä¹ å»ºæ¨¡ç®¡é“åŸºç¡€

æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹å¯èƒ½å¾ˆå¤æ‚ï¼Œæœ‰å„ç§æ­¥éª¤ï¼š

+   æ•°æ®å‡†å¤‡ï¼Œç‰¹å¾å·¥ç¨‹è½¬æ¢

+   æ¨¡å‹å‚æ•°æ‹Ÿåˆ

+   æ¨¡å‹è¶…å‚æ•°è°ƒæ•´

+   æ¨¡å‹æ–¹æ³•é€‰æ‹©

+   åœ¨å¤§é‡è¶…å‚æ•°ç»„åˆä¸­è¿›è¡Œæœç´¢

+   è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹è¿è¡Œ

ç®¡é“æ˜¯ scikit-learn ç±»ï¼Œå…è®¸å°è£…ä¸€ç³»åˆ—æ•°æ®å‡†å¤‡å’Œå»ºæ¨¡æ­¥éª¤

+   ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†ç®¡é“è§†ä¸ºæˆ‘ä»¬é«˜åº¦ç²¾ç®€çš„å·¥ä½œæµç¨‹ä¸­çš„ä¸€ä¸ªå¯¹è±¡

ç®¡é“ç±»å…è®¸æˆ‘ä»¬ï¼š

+   æé«˜ä»£ç å¯è¯»æ€§ï¼Œå¹¶ä¿æŒä¸€åˆ‡äº•ç„¶æœ‰åº

+   é¿å…å¸¸è§çš„æµç¨‹é—®é¢˜ï¼Œå¦‚æ•°æ®æ³„éœ²ï¼Œæµ‹è¯•æ•°æ®å‘ŠçŸ¥æ¨¡å‹å‚æ•°è®­ç»ƒ

+   æŠ½è±¡å¸¸è§çš„æœºå™¨å­¦ä¹ å»ºæ¨¡ï¼Œå¹¶ä¸“æ³¨äºæ„å»ºæœ€ä½³æ¨¡å‹

åŸºæœ¬å“²å­¦æ˜¯å°†æœºå™¨å­¦ä¹ è§†ä¸ºç»„åˆæœç´¢ï¼Œä»¥è‡ªåŠ¨åŒ–æœ€ä½³æ¨¡å‹çš„ç¡®å®šï¼ˆAutoMLï¼‰

### ä½¿ç”¨ç®¡é“çš„ k-Nearest Neighbours

è¿™é‡Œæ˜¯ä½¿ç”¨ scikit-learn ç®¡é“è¿›è¡Œæ•´ä¸ªæ¨¡å‹è®­ç»ƒå’Œè°ƒæ•´è¿‡ç¨‹çš„ç´§å‡‘ã€å®‰å…¨ä»£ç 

+   GridSearchCV å¯¹è±¡å®é™…ä¸Šå˜æˆäº†é¢„æµ‹æ¨¡å‹ï¼Œè°ƒæ•´è¿‡çš„è¶…å‚æ•°åœ¨æ‰€æœ‰æ•°æ®ä¸Šé‡æ–°è®­ç»ƒï¼

```py
import os                                                     # to set current working directory 

folds = 4                                                   # number of k folds
k_min = 1; k_max = 150                                       # range of k hyperparameter to consider

X_pipe = df.loc[:,sel_pred]                                 # all the samples for the original features
y_pipe = df.loc[:,resp[0]]                             # warning this becomes a series, 1D ndarray with label

pipe = Pipeline([                                           # the machine learning workflow as a pipeline object
    ('scaler', StandardScaler()),
    ('knear', KNeighborsRegressor())
])

params = {                                                  # the machine learning workflow method's parameters
    'scaler': [StandardScaler()],
    'knear__n_neighbors': np.arange(k_min,k_max,1,dtype = int),
    'knear__metric': ['euclidean'],
    'knear__p': [2],
    'knear__weights': ['distance']
}

grid_cv_tuned = GridSearchCV(pipe, params, scoring = 'neg_mean_squared_error', # grid search cross validation 
                             cv=KFold(n_splits=folds,shuffle=False),
                             refit = True)
grid_cv_tuned.fit(X_pipe,y_pipe)                                      # fit model with tuned hyperparameters

plt.subplot(121)
visualize_tuned_model(grid_cv_tuned.best_params_['knear__n_neighbors'], # visualize the error vs. k 
                      grid_cv_tuned.cv_results_['param_knear__n_neighbors'],
                      abs(grid_cv_tuned.cv_results_['mean_test_score']))              

plt.subplot(122)                                            # visualize the tuned model
visualize_model(grid_cv_tuned,X[sel_pred[0]],Xmin[0],Xmax[0],X[sel_pred[1]],Xmin[1],Xmax[1],df[resp[0]],ymin,ymax,
                'All Data and Tuned and Retrained k-Nearest Neighbours')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/4cddeaa35cf9ea49e9a36b802755dc3a.png)

### æ£€æŸ¥è°ƒæ•´è¿‡çš„è¶…å‚æ•°

åœ¨ GridSearchCV æ¨¡å‹å¯¹è±¡ä¸­ï¼Œæœ‰ä¸€ä¸ªå†…ç½®çš„å­—å…¸ best_params_ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰è°ƒæ•´è¿‡çš„è¶…å‚æ•°ã€‚

+   æ³¨æ„ï¼Œåœ¨ k çš„èŒƒå›´å†…ï¼Œé€‰æ‹©çš„ kï¼Œn_neighbors

+   æ­¤å¤–ï¼Œè¿˜æŒ‡å®šäº†å…¶ä»–è¶…å‚æ•°ï¼Œä½†æˆ‘ä»¬å¯ä»¥æä¾›èŒƒå›´å’Œåœºæ™¯ï¼Œä»¥ä¾¿ä½¿ç”¨ç½‘æ ¼æœç´¢æ–¹æ³•è¿›è¡Œæ¢ç´¢

å½“è°ƒæ•´è¶…è¿‡ 1 ä¸ªè¶…å‚æ•°æ—¶ï¼Œè¿è¡Œæ—¶é—´å°†éšç€è¶…å‚æ•°çš„ç»„åˆä»¥åŠç»“æœæ¨¡å‹æŸå¤±å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œcv_results_['mean_test_score']ï¼‰çš„å¢åŠ è€Œå¢åŠ ï¼Œå¯¹æ‰€æœ‰è¶…å‚æ•°æ¡ˆä¾‹è¿›è¡Œæ’åº

```py
grid_cv_tuned.best_params_ 
```

```py
{'knear__metric': 'euclidean',
 'knear__n_neighbors': 11,
 'knear__p': 2,
 'knear__weights': 'distance',
 'scaler': StandardScaler()} 
```

ä¹Ÿæœ‰åŠ©äºæŸ¥çœ‹æ•´ä¸ªæ¨¡å‹å¯¹è±¡ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚åŒ…æ‹¬ï¼š

+   è¶…å‚æ•°è°ƒæ•´è€ƒè™‘çš„æ‰€æœ‰ç®¡é“å’Œæ¡ˆä¾‹ã€‚

```py
grid_cv_tuned 
```

```py
GridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=False),
             estimator=Pipeline(steps=[('scaler', StandardScaler()),
                                       ('knear', KNeighborsRegressor())]),
             param_grid={'knear__metric': ['euclidean'],
                         'knear__n_neighbors': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,
        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,
        40,  41,  42,  43,  44,  45,...
        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,
        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,
       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,
       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,
       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,
       144, 145, 146, 147, 148, 149]),
                         'knear__p': [2], 'knear__weights': ['distance'],
                         'scaler': [StandardScaler()]},
             scoring='neg_mean_squared_error')
```

**åœ¨ Jupyter ç¯å¢ƒä¸­ï¼Œè¯·é‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼ä»¥æ˜¾ç¤º HTML è¡¨ç¤ºå½¢å¼ï¼Œæˆ–è€…ç›¸ä¿¡ç¬”è®°æœ¬ã€‚**

åœ¨ GitHub ä¸Šï¼ŒHTML è¡¨ç¤ºæ— æ³•æ¸²æŸ“ï¼Œè¯·å°è¯•ä½¿ç”¨ nbviewer.org åŠ è½½æ­¤é¡µé¢ã€‚**

```py
GridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=False),
             estimator=Pipeline(steps=[('scaler', StandardScaler()),
                                       ('knear', KNeighborsRegressor())]),
             param_grid={'knear__metric': ['euclidean'],
                         'knear__n_neighbors': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,
        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,
        40,  41,  42,  43,  44,  45,...
        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,
        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,
       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,
       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,
       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,
       144, 145, 146, 147, 148, 149]),
                         'knear__p': [2], 'knear__weights': ['distance'],
                         'scaler': [StandardScaler()]},
             scoring='neg_mean_squared_error')
```

```py
Pipeline(steps=[('scaler', StandardScaler()), ('knear', KNeighborsRegressor())])
```

```py
StandardScaler()
```

```py
KNeighborsRegressor()
```

## è¯„è®º

è¿™æ˜¯å¯¹ k-Nearest Neighbours çš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šå…¶ä»–èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´ YouTube è®²åº§é“¾æ¥ä¸­çš„èµ„æºé“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![å›¾ç‰‡](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

åœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡çš„ 40 è‹±äº©æ ¡å›­å†…ï¼Œè¿ˆå…‹å°”Â·çš®å°”èŒ¨æ•™æˆçš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡å…¹ï¼ˆMichael Pyrczï¼‰æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ [Cockrell å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p) å’Œ [Jackson åœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/) çš„æ•™æˆï¼Œä»–åœ¨é‚£é‡Œç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics) æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚

+   [è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board) çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š [æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board) çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº†è¶…è¿‡ 70 ç¯‡ [åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„ [Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦ [åœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶ä¸”æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[Python ä¸­çš„åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPy å®æˆ˜æŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) å’Œ [Python ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå®æˆ˜æŒ‡å—ä¸ä»£ç ](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„ [YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures) ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å« 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†å·¥ä½œæµç¨‹é“¾æ¥ï¼Œè¿™äº›å­˜å‚¨åº“ä½äºä»–çš„ [GitHub è´¦æˆ·](https://github.com/GeostatsGuy)ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›æŒç»­æ›´æ–°çš„å†…å®¹ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™ä¸ªå†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æˆ‘å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°ã€‚

æˆ‘æ€»æ˜¯ä¹äºè®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢

## æ›´å¤šèµ„æºè¯·è®¿é—®ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­çš„åº”ç”¨åœ°çƒç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## k è¿‘é‚»å›å½’çš„åŠ¨æœº

è¦†ç›– k è¿‘é‚»å›å½’æœ‰è®¸å¤šå¾ˆå¥½çš„ç†ç”±ã€‚é™¤äº†å®ƒæ˜¯ä¸€ä¸ªç®€å•ã€å¯è§£é‡Šä¸”çµæ´»çš„é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹å¤–ï¼Œå®ƒè¿˜å±•ç¤ºäº†é‡è¦çš„æ¦‚å¿µï¼Œ

+   **éå‚æ•°é¢„æµ‹æ¨¡å‹** - ä»æ•°æ®ä¸­å­¦ä¹ å…³ç³»çš„å½¢çŠ¶ï¼Œå³ä¸å¯¹å…³ç³»çš„å½¢çŠ¶åšå‡ºå…ˆéªŒå‡è®¾

+   **åŸºäºå®ä¾‹çš„ã€æ‡’æƒ°å­¦ä¹ ** - æ¨¡å‹è®­ç»ƒæ¨è¿Ÿåˆ°éœ€è¦é¢„æµ‹æ—¶ï¼Œä¸éœ€è¦é¢„å…ˆè®¡ç®—æ¨¡å‹ã€‚å³ï¼Œé¢„æµ‹éœ€è¦è®¿é—®æ•°æ®

+   **è¶…å‚æ•°è°ƒæ•´** - ä½¿ç”¨å¯ç†è§£çš„è¶…å‚æ•°æ¥æ§åˆ¶æ¨¡å‹æ‹Ÿåˆ

+   **éå¸¸çµæ´»ã€å¤šåŠŸèƒ½çš„é¢„æµ‹æ¨¡å‹** - åœ¨è®¸å¤šæƒ…å†µä¸‹è¡¨ç°è‰¯å¥½

## å·ç§¯

äº‹å®ä¸Šï¼Œk è¿‘é‚»ç±»ä¼¼äºåœ¨å±€éƒ¨é‚»åŸŸå†…é€šè¿‡åŠ æƒå¹³å‡è¿›è¡Œç©ºé—´ä¼°è®¡ã€‚

![](img/0cfb5694654b0332a568b81d0ae25755.png)

åœ¨é¢„æµ‹ç‰¹å¾ç©ºé—´ä¸­å¯¹ç©ºé—´æ’å€¼è¿›è¡Œå»ºæ¨¡ã€‚

k è¿‘é‚»æ–¹æ³•ç±»ä¼¼äºç©ºé—´æ’å€¼çš„å·ç§¯æ–¹æ³•ã€‚å·ç§¯æ˜¯ä¸¤ä¸ªå‡½æ•°çš„ç§¯åˆ†ä¹˜ç§¯ï¼Œå…¶ä¸­ä¸€ä¸ªå‡½æ•°ç»è¿‡åè½¬å¹¶æ²¿$\tau$å¹³ç§»ã€‚

+   ä¸€ç§è§£é‡Šæ˜¯ä½¿ç”¨åŠ æƒå‡½æ•°$ğ‘“(\Delta)$å¯¹å‡½æ•°è¿›è¡Œå¹³æ»‘ï¼Œä»¥è®¡ç®—å‡½æ•°$ğ‘”(x)$çš„åŠ æƒå¹³å‡å€¼ï¼Œ

$$ (f * g)(x) = \int_{-\infty}^{\infty} f(\tau) g(x - \tau) \, d\tau $$

è¿™å¾ˆå®¹æ˜“æ‰©å±•åˆ°å¤šç»´

$$ (f * g)(x, y, z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(\tau_x, \tau_y, \tau_z) g(x - \tau_x, y - \tau_y, z - \tau_z) \, d\tau_x \, d\tau_y \, d\tau_z $$

åœ¨ç§¯åˆ†ä¹‹å‰é€‰æ‹©å“ªä¸ªå‡½æ•°è¿›è¡Œå¹³ç§»ä¸ä¼šæ”¹å˜ç»“æœï¼Œå·ç§¯ç®—å­å…·æœ‰äº¤æ¢æ€§ï¼Œ

$$ (f * g)(x) = \int_{-\infty}^{\infty} f(\tau) g(x - \tau) \, d\tau $$$$ (f * g)(x) = \int_{-\infty}^{\infty} f(x - \tau) g(\tau) \, d\tau $$

+   å¦‚æœä»»ä¸€å‡½æ•°è¢«åå°„ï¼Œåˆ™å·ç§¯ç­‰åŒäºäº’ç›¸å…³ï¼Œå®ƒæ˜¯ä¸¤ä¸ªä¿¡å·ä½œä¸ºä½ç§»å‡½æ•°çš„ç›¸ä¼¼åº¦åº¦é‡ã€‚

ä¸ºäº†æ¼”ç¤ºä½¿ç”¨è¯¦å°½çš„ $g(x)$ å’Œç¨€ç–é‡‡æ ·çš„ $g(x)$ çš„å·ç§¯ï¼Œæˆ‘æ„å»ºäº†ä¸€ä¸ª [äº¤äº’å¼ Python å·ç§¯ä»ªè¡¨æ¿](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Convolution_kNearest.ipynb)ï¼Œ

![](img/f55c37e0f7da98a233affd5fbd5ba38c.png)

ç”¨äºæ¼”ç¤ºå·ç§¯çš„äº¤äº’å¼ Python ä»ªè¡¨æ¿ã€‚

```py
import numpy as np                                            # arrays and matrix math
import pandas as pd                                           # DataFrames
import matplotlib.pyplot as plt                               # for plotting
from scipy.ndimage import convolve1d
seed = 73073                                                  # random number seed

kr = 5; kloc = 45                                             # kernel radius, kernel location for example point

df_denpor = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/1D_por_perm_smooth.csv') # load data from Dr. Pyrcz's GitHub 

plt.subplot(121)                    
plt.plot(df_denpor['Por'],df_denpor['Depth'],color='black',label='f(x)'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)')
plt.ylim([100,0]); plt.xlim([2,22]); plt.title('Exhaustive Case: Original and Convolved Function'); plt.yticks(np.arange(100, -1, -5));

size = 2 * kr + 1                                             # make uniform kernel
kernel = np.ones(size) / size                                 # normalize kernel to sum to one for unbiasedness

convolved = np.convolve(df_denpor['Por'].values, kernel, mode='same') # convolution

k = len(kernel)
trim = k // 2  # how many values to trim from each edge
convolved_valid = convolved[trim:-trim] if trim > 0 else convolved
depth_valid = df_denpor['Depth'].values[trim:-trim] if trim > 0 else df_denpor['Depth'].values
convolved_pt = convolved_valid[np.abs(depth_valid - kloc).argmin()]; depth_pt = depth_valid[np.abs(depth_valid - kloc).argmin()]
plt.plot(convolved_valid,depth_valid,color='red',label=r'$(f * g)(x)$'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)'); 
plt.ylim([100,0]); plt.xlim([2,22]); plt.legend(loc='lower left'); plt.plot([2,22],[depth_pt,depth_pt],color='red',ls='--')
plt.scatter(convolved_pt,depth_pt,color='red',marker='o',edgecolor='black',zorder=10)

plt.subplot(122)
plt.plot(kernel,np.linspace(-1*kr+kloc,kr+kloc,2*kr+1),color='black',label=r'$g(\tau)$'); plt.xlim(0.35,0.0); plt.ylim([100,0]); 
plt.yticks(np.arange(100, -1, -5)); plt.plot([kernel[0],0.0],[-1*kr+kloc,-1*kr+kloc],color='black'); 
plt.plot([kernel[0],0.0],[kr+kloc,kr+kloc],color='black'); plt.legend(loc='lower left'); plt.title('Kernel')
plt.plot([0.35,0.0],[depth_pt,depth_pt],color='red',ls='--'); plt.ylabel('Depth (m)'); plt.xlabel('Weight (unitless)')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/58f0d6ec82ae506386975d87b4fbaa3fa88f321e3ef516e9f956402a156d2751.png](img/83364c0e2e061f041cd64b882996eb7e.png)

```py
import astropy.convolution.convolve as convolve               # sparse data convolution
frac = 0.2

df_denpor['Por_Sparse'] = df_denpor['Por'].copy()

np.random.seed(seed = seed)
nan_indices = np.random.choice(len(df_denpor), size=int(len(df_denpor)*(1.0-frac)), replace=False)
df_denpor.loc[nan_indices, 'Por_Sparse'] = np.nan

plt.subplot(121)                    
plt.scatter(df_denpor['Por_Sparse'],df_denpor['Depth'],color='black',label='f(x) Sparse'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)')
plt.plot(df_denpor['Por'],df_denpor['Depth'],color='black',alpha=0.3,label='f(x) Exhaustive');
plt.ylim([26,0]); plt.xlim([2,22]); plt.title('Sparse Case: Original and Convolved Function'); plt.yticks(np.arange(100, -1, -5));

size = 2 * kr + 1                                             # make uniform kernel
kernel = np.ones(size) / size                                 # normalize kernel to sum to one for unbiasedness

convolved = convolve(df_denpor['Por_Sparse'].values,kernel,boundary='extend',nan_treatment='interpolate',normalize_kernel=True) # convolve

k = len(kernel)
trim = k // 2  # how many values to trim from each edge
convolved_valid = convolved[trim:-trim] if trim > 0 else convolved
depth_valid = df_denpor['Depth'].values[trim:-trim] if trim > 0 else df_denpor['Depth'].values
convolved_pt = convolved_valid[np.abs(depth_valid - kloc).argmin()]; depth_pt = depth_valid[np.abs(depth_valid - kloc).argmin()]
plt.plot(convolved_valid,depth_valid,color='red',label=r'$(f * g)(x)$'); plt.ylabel('Depth (m)'); plt.xlabel('Porosity (%)'); 
plt.ylim([100,0]); plt.xlim([2,22]); plt.legend(loc='lower left'); plt.plot([2,22],[depth_pt,depth_pt],color='red',ls='--')
plt.scatter(convolved_pt,depth_pt,color='red',marker='o',edgecolor='black',zorder=10)

plt.subplot(122)
plt.plot(kernel,np.linspace(-1*kr+kloc,kr+kloc,2*kr+1),color='black',label=r'$g(\tau)$'); plt.xlim(0.35,0.0); plt.ylim([100,0]); 
plt.yticks(np.arange(100, -1, -5)); plt.plot([kernel[0],0.0],[-1*kr+kloc,-1*kr+kloc],color='black'); 
plt.plot([kernel[0],0.0],[kr+kloc,kr+kloc],color='black'); plt.legend(loc='lower left'); plt.title('Kernel')
plt.plot([0.35,0.0],[depth_pt,depth_pt],color='red',ls='--'); plt.ylabel('Depth (m)'); plt.xlabel('Weight (unitless)')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

```py
WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve] 
```

![_images/ae797e2e907bd48f0c7b524dd697136f6a863602289a91dc8e372c930f4a72c1.png](img/2eab3ef179c50a4d663c4b21d6e38124.png)

è™½ç„¶å›é¡¾å’Œè®¨è®ºå·ç§¯å¾ˆæœ‰ç”¨ï¼Œä½† k è¿‘é‚»ä¸å·ç§¯ä¸åŒï¼Œå› ä¸ºå®ƒæŒ‡å®šäº† $k$ ä¸ªæœ€è¿‘é‚»ä»¥åŒ…å«åœ¨åŠ æƒå¹³å‡å€¼ä¸­ï¼Œ

+   æŒ‡å®š $k$ å¯¼è‡´å±€éƒ¨è‡ªé€‚åº”çª—å£å¤§å°ï¼Œå±€éƒ¨é‚»åŸŸæ‰©å±•è¶³å¤Ÿè¿œä»¥æ‰¾åˆ° $k$ ä¸ªè®­ç»ƒæ•°æ®

![](img/2ffab60ca866a945ae26ac4aab566a02.png)

å¯¹äºç»™å®šçš„ $k$ ä¸ªæœ€è¿‘é‚»æ•°æ®ç‚¹ï¼Œä»é¢„æµ‹ç‰¹å¾ç©ºé—´çš„ç¨€ç–æ•°æ®åŒºåŸŸæ”¶é›†æ•°æ®ã€‚

## k è¿‘é‚»è¶…å‚æ•°

ç°åœ¨è®©æˆ‘ä»¬è®¨è®º k è¿‘é‚»è¶…å‚æ•°ã€‚

1.  **k ä¸ªæœ€è¿‘çš„æ•°æ®ç‚¹** - ç”¨äºé¢„æµ‹

1.  **æ•°æ®åŠ æƒ** - ä¾‹å¦‚å‡åŒ€åŠ æƒï¼ˆä½¿ç”¨å±€éƒ¨è®­ç»ƒæ•°æ®å¹³å‡å€¼ï¼‰ï¼Œå€’æ•°è·ç¦»åŠ æƒ

æ³¨æ„ï¼Œå¯¹äºå€’æ•°è·ç¦»åŠ æƒçš„æ¡ˆä¾‹ï¼Œè¯¥æ–¹æ³•ç±»ä¼¼äºå€’æ•°è·ç¦»åŠ æƒæ’å€¼ï¼Œé€šå¸¸åº”ç”¨äºç©ºé—´æ’å€¼ä¸­çš„å±€éƒ¨æ•°æ®æœ€å¤§æ•°é‡çº¦æŸã€‚å€’æ•°è·ç¦»åœ¨ GeostatsPy ä¸­å¯ç”¨äºç©ºé—´æ˜ å°„ã€‚

1.  **è·ç¦»åº¦é‡** - åœ¨é¢„æµ‹ç‰¹å¾ç©ºé—´å†…çš„è®­ç»ƒæ•°æ®æŒ‰è·ç¦»æ’åºï¼Œä»æœ€è¿‘åˆ°æœ€è¿œï¼Œå¯ä»¥åº”ç”¨å¤šç§è·ç¦»åº¦é‡ï¼ŒåŒ…æ‹¬ï¼š

+   æ¬§å‡ é‡Œå¾—è·ç¦»

\begin{equation}

$d_i = \sqrt{\sum_{\alpha = 1}^{m} \left(x_{\alpha,i} - x_{\alpha,0}\right)Â²} \end{equation}$

+   ç±³æ°è·ç¦» - ä¸€ç§è·ç¦»çš„æ¨å¹¿å½¢å¼ï¼Œå…¶ä¸­å·²çŸ¥çš„æ›¼å“ˆé¡¿å’Œæ¬§å‡ é‡Œå¾—è·ç¦»æ˜¯ç‰¹æ®Šæƒ…å†µï¼Œ

$$ d_{(i,i')} = \left( \sum_{j=1}^{m} \left( x_{(j,i)} - x_{(j,i')} \right)^p \right)^{\frac{1}{p}} $$

+   å½“ $p=2$ æ—¶ï¼Œè¿™å˜ä¸ºæ¬§å‡ é‡Œå¾—è·ç¦»

+   å½“ $p=1$ æ—¶ï¼Œå®ƒå˜ä¸ºæ›¼å“ˆé¡¿è·ç¦»

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = True
import os                                                     # to set current working directory 
import math                                                   # square root operator
import numpy as np                                            # arrays and matrix math
import scipy.stats as st                                      # statistical methods
import pandas as pd                                           # DataFrames
import pandas.plotting as pd_plot
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import seaborn as sns                                         # for matrix scatter plots
from sklearn import metrics                                   # measures to check our models
from sklearn.preprocessing import StandardScaler              # standardize the features
from sklearn.neighbors import KNeighborsRegressor             # for nearest k neighbours
from sklearn import metrics                                   # measures to check our models
from sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning
from sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # suppress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœé‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½å¿…é¡»é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„æŸäº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£å¹¶è¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£ï¼Œå¯ä»¥è·å¾—æ›´å¤šå¸®åŠ©ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰ä¸€äº›å‡½æ•°æ¥ç®€åŒ–ç»˜åˆ¶ç›¸å…³çŸ©é˜µã€å†³ç­–æ ‘å›å½’æ¨¡å‹çš„å¯è§†åŒ–ä»¥åŠå°†æŒ‡å®šçš„ç™¾åˆ†ä½æ•°å’Œä¸»æ¬¡ç½‘æ ¼çº¿æ·»åŠ åˆ°æˆ‘ä»¬çš„å›¾è¡¨ä¸­ã€‚

```py
def comma_format(x, pos):
    return f'{int(x):,}'

def feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot
    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1
    plt.plot(pred,metric,color='black',zorder=20)
    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)
    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)
    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  
    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)
    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();
    return

def plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix 
    my_colormap = plt.get_cmap('RdBu_r', 256)          
    newcolors = my_colormap(np.linspace(0, 1, 256))
    white = np.array([256/256, 256/256, 256/256, 1])
    white_low = int(128 - mask*128); white_high = int(128+mask*128)
    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)
    newcmp = ListedColormap(newcolors)
    m = corr_matrix.shape[0]
    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)
    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()
    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()
    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
    cbar = plt.colorbar(im, orientation = 'vertical')
    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.title(title)
    for i in range(0,m):
        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')
        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')
    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])

def visualize_model(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,title,axes_commas = True): # plots the data points and the decision tree prediction 
    n_classes = 10
    cmap_temp = plt.cm.inferno
    xplot_step = (x_max-x_min)/100; yplot_step = (y_max-y_min)/100
    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step),
                     np.arange(y_min, y_max, yplot_step))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    cs = plt.contourf(xx, yy, Z, cmap=cmap_temp,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))

    im = plt.scatter(xfeature,yfeature,s=30, c=response, marker=None, cmap=cmap_temp, norm=None, vmin=z_min, vmax=z_max, 
                     alpha=1.0, linewidths=0.8, edgecolors="black",zorder=10)
    plt.scatter(xfeature,yfeature,s=60, c='white', marker=None, cmap=cmap_temp, norm=None, vmin=z_min, vmax=z_max, 
                     alpha=1.0, linewidths=0.8, edgecolors=None,zorder=8)
    plt.title(title); plt.xlabel(xfeature.name); plt.ylabel(yfeature.name)
    cbar = plt.colorbar(im, orientation = 'vertical'); cbar.set_label(response.name, rotation=270, labelpad=20)
    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))
    if axes_commas == True:
        plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
        plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
    return Z

def visualize_tuned_model(k_tuned,k_mat,score_mat):
    plt.scatter(k_mat,score_mat,s=10.0, c="red", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, 
                linewidths=0.5, edgecolors="black")
    plt.plot([k_tuned,k_tuned],[0,10000000],color='black',linestyle=(6, (2,3)),label='tuned',zorder=1)
    plt.title('k-fold Cross Validation Error (MSE) vs. k Nearest Neighbours'); plt.xlabel('Number of Nearest Neighbours')
    plt.ylabel('Mean Square Error')
    plt.xlim(k_min,k_max); plt.ylim(0,np.max(score_mat))

def check_model(model,xtrain,ytrain,xtest,ytest,ymin,ymax,rtrain,rtest,title): # plots the estimated vs. the actual 
    predict_train = model.predict(np.c_[xtrain,ytrain])
    predict_test = model.predict(np.c_[xtest,ytest])
    plt.scatter(rtrain,predict_train,s=None, c='darkorange',marker=None, cmap=None, norm=None, vmin=None, vmax=None, 
                alpha=0.8, linewidths=0.8, edgecolors="black",label='Train')
    plt.scatter(rtest,predict_test,s=None, c='red',marker=None, cmap=None, norm=None, vmin=None, vmax=None, 
                alpha=0.8, linewidths=0.8, edgecolors="black",label='Test')
    plt.title(title); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')
    plt.xlim(ymin,ymax); plt.ylim(ymin,ymax)
    plt.arrow(ymin,ymin,ymax,ymax,width=0.02,color='black',head_length=0.0,head_width=0.0)
    MSE_train = metrics.mean_squared_error(rtrain,predict_train)
    Var_Explained_train = metrics.explained_variance_score(rtrain,predict_train)
    cor_train = math.sqrt(metrics.r2_score(rtrain,predict_train))
    MSE_test = metrics.mean_squared_error(rtest,predict_test)
    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))
    plt.annotate('Train MSE: ' + str(f'{(np.round(MSE_train,2)):,}'),[0.05*(ymax-ymin)+ymin,0.95*(ymax-ymin)+ymin]) 
    plt.annotate('Test MSE:  ' + str(f'{(np.round(MSE_test,2)):,}'),[0.05*(ymax-ymin)+ymin,0.90*(ymax-ymin)+ymin])
    add_grid(); plt.legend(loc='lower right')
    # print('Mean Squared Error on Training = ', round(MSE_test,2),', Variance Explained =', round(Var_Explained,2),'Cor =', round(cor,2))

def weighted_percentile(data, weights, perc):                 # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) 
    ix = np.argsort(data)
    data = data[ix] 
    weights = weights[ix] 
    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) 
    return np.interp(perc, cdf, data)

def histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram 
    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)
    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')
    plt.plot([avg,avg],[0.0,45],color = color)
    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)
    html_str = ''
    for df in args:
        html_str += df.head().to_html()  # Using .head() for the first few rows
    display(HTML(f'<div style="display: flex;">{html_str}</div>')) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/PGE383")                                        # set the working directory 
```

æ‚¨å°†éœ€è¦æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ä¸ºæ‚¨çš„è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”åœ¨ Mac ä¸Šæ ¼å¼ä¸åŒï¼ˆä¾‹å¦‚ï¼šâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½è¡¨æ ¼æ•°æ®

è¿™æ˜¯å°†æˆ‘ä»¬çš„é€—å·åˆ†éš”æ•°æ®æ–‡ä»¶åŠ è½½åˆ° Pandas DataFrame å¯¹è±¡ä¸­çš„å‘½ä»¤ã€‚

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›† â€˜unconv_MV.csvâ€™ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…å«æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   äº•å¹³å‡å­”éš™åº¦

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   å‰ªåˆ‡æ¯”ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   ç»ç’ƒè´¨åå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹ç”Ÿäº§ 90 å¤©å¹³å‡ï¼ˆMCFPDï¼‰ã€‚

æ³¨æ„ï¼Œæ•°æ®é›†æ˜¯åˆæˆçš„ã€‚

æˆ‘ä»¬ä½¿ç”¨ pandas çš„ â€˜read_csvâ€™ å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸º â€˜my_dataâ€™ çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

## å¯é€‰ï¼šå‘å“åº”ç‰¹å¾æ·»åŠ éšæœºå™ªå£°

æˆ‘ä»¬å¯ä»¥è¿™æ ·åšæ¥è§‚å¯Ÿæ•°æ®å™ªå£°å¯¹è¿‡æ‹Ÿåˆå’Œè¶…å‚æ•°è°ƒæ•´çš„å½±å“ã€‚

+   è¿™æ˜¯ä¸ºäº†ç»éªŒå­¦ä¹ ï¼Œå½“ç„¶æˆ‘ä»¬ä¸ä¼šå‘æˆ‘ä»¬çš„æ•°æ®æ·»åŠ éšæœºå™ªå£°

+   æˆ‘ä»¬è®¾ç½®äº†éšæœºæ•°ç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§

```py
df_load = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub 
df_load = df_load.iloc[:,1:8]                                 # copy all rows and columns 1 through 8, note 0 column is removed

response = 'Prod'                                             # specify the response feature
add_noise = True                                              # set True to add noise to response feature to demonstrate overfit
noise_stdev = 500                                             # amount of noise to add to response feature to demonstrate overfit

np.random.seed(seed = seed)                                   # set the random number seed
if add_noise == True:
    df_load[response] = df_load[response] + np.random.normal(loc=0.0,scale=noise_stdev,size = len(df_load))

X = df_load.copy(deep = False)
X = X.drop([response],axis='columns')                         # make predictor and response DataFrames
y = df_load.loc[:,response]

features = X.columns.values.tolist() + [y.name]               # store the names of the features
pred = X.columns.values.tolist()
resp = [y.name]

Xmin = [6.0,0.0,1.0,10.0,0.0,0.9]; Xmax = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minumum and maximum values for plotting
ymin = 1000.0; ymax = 9000.0

predlabel = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Brittleness Ratio (%)', # set the names for plotting
             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']
resplabel = 'Normalized Initial Production (MCFPD)'

predtitle = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting
             'Total Organic Carbon','Vitrinite Reflectance']
resptitle = 'Initial Production'

featurelabel = predlabel + [resplabel]                        # make feature labels and titles for concise code
featuretitle = predtitle + [resptitle]

m = len(pred) + 1
mpred = len(pred)

df = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) 
```

## å¯è§†åŒ– DataFrame

å¯è§†åŒ– DataFrame æ˜¯æ£€æŸ¥æ•°æ®çš„ç¬¬ä¸€æ­¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ï¼Œç­‰ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ©ç”¨ â€˜headâ€™ DataFrame æˆå‘˜å‡½æ•°æ¥é¢„è§ˆï¼ˆæ ¼å¼æ•´æ´ã€æ¸…æ™°ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚

+   æ·»åŠ å‚æ•° â€˜n=13â€™ ä»¥æŸ¥çœ‹æ•°æ®é›†çš„å‰ 13 è¡Œã€‚

```py
df.head(n=13) 
```

|  | Por | Perm | AI | Brittle | TOC | VR | Prod |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 12.08 | 2.92 | 2.80 | 81.40 | 1.16 | 2.31 | 1339.165488 |
| 1 | 12.38 | 3.53 | 3.22 | 46.17 | 0.89 | 1.88 | 3383.979252 |
| 2 | 14.02 | 2.59 | 4.01 | 72.80 | 0.89 | 2.72 | 2509.686720 |
| 3 | 17.67 | 6.75 | 2.63 | 39.81 | 1.08 | 1.88 | 5514.421023 |
| 4 | 17.52 | 4.57 | 3.18 | 10.94 | 1.51 | 1.90 | 3532.020478 |
| 5 | 14.53 | 4.81 | 2.69 | 53.60 | 0.94 | 1.67 | 4283.543382 |
| 6 | 13.49 | 3.60 | 2.93 | 63.71 | 0.80 | 1.85 | 3627.906723 |
| 7 | 11.58 | 3.03 | 3.25 | 53.00 | 0.69 | 1.93 | 3101.539533 |
| 8 | 12.52 | 2.72 | 2.43 | 65.77 | 0.95 | 1.98 | 3213.391047 |
| 9 | 13.25 | 3.94 | 3.71 | 66.20 | 1.14 | 2.65 | 2200.204701 |
| 10 | 15.04 | 4.39 | 2.22 | 61.11 | 1.08 | 1.77 | 3433.752662 |
| 11 | 16.19 | 6.30 | 2.29 | 49.10 | 1.53 | 1.86 | 4465.007131 |
| 12 | 16.82 | 5.42 | 2.80 | 66.65 | 1.17 | 1.98 | 4373.060709 |

## è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯

åœ¨ DataFrames ä¸­ï¼Œæœ‰è®¸å¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ã€‚describe å‘½ä»¤ä»¥ä¸€ä¸ªæ¼‚äº®çš„æ•°æ®è¡¨å½¢å¼æä¾›è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€‚

+   æˆ‘ä»¬æœ‰ä¸€äº›è´Ÿ TOC å€¼ï¼è®©æˆ‘ä»¬æ£€æŸ¥åˆ†å¸ƒã€‚

```py
df.describe().transpose()                                     # calculate summary statistics for the data 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 200.0 | 14.991150 | 2.971176 | 6.550000 | 12.912500 | 15.070000 | 17.40250 | 23.550000 |
| Perm | 200.0 | 4.330750 | 1.731014 | 1.130000 | 3.122500 | 4.035000 | 5.28750 | 9.870000 |
| AI | 200.0 | 2.968850 | 0.566885 | 1.280000 | 2.547500 | 2.955000 | 3.34500 | 4.630000 |
| Brittle | 200.0 | 48.161950 | 14.129455 | 10.940000 | 37.755000 | 49.510000 | 58.26250 | 84.330000 |
| TOC | 200.0 | 0.990450 | 0.481588 | -0.190000 | 0.617500 | 1.030000 | 1.35000 | 2.180000 |
| VR | 200.0 | 1.964300 | 0.300827 | 0.930000 | 1.770000 | 1.960000 | 2.14250 | 2.870000 |
| Prod | 200.0 | 3842.630027 | 1594.301295 | 803.640483 | 2551.414599 | 3626.229052 | 4739.73408 | 9021.792491 |

+   åªæœ‰ä¸€ä¸¤ä¸ªç¨å¾®è´Ÿå€¼ï¼Œæˆ‘ä»¬åªéœ€å°†å®ƒä»¬æˆªæ–­åˆ°é›¶ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å°† DataFrame ä¸­çš„æ‰€æœ‰ TOC å€¼è®¾ç½®ä¸º 0.0ï¼ˆå¦‚æœå°äº 0.0ï¼‰ï¼Œå¦åˆ™ä¿æŒåŸå§‹ TOC å€¼ã€‚

```py
num = df._get_numeric_data()                                  # get the numerical values
num[num < 0] = 0                                              # truncate negative values to 0.0
df.describe().transpose()                                     # calculate summary statistics for the data 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 200.0 | 14.991150 | 2.971176 | 6.550000 | 12.912500 | 15.070000 | 17.40250 | 23.550000 |
| Perm | 200.0 | 4.330750 | 1.731014 | 1.130000 | 3.122500 | 4.035000 | 5.28750 | 9.870000 |
| AI | 200.0 | 2.968850 | 0.566885 | 1.280000 | 2.547500 | 2.955000 | 3.34500 | 4.630000 |
| Brittle | 200.0 | 48.161950 | 14.129455 | 10.940000 | 37.755000 | 49.510000 | 58.26250 | 84.330000 |
| TOC | 200.0 | 0.991950 | 0.478264 | 0.000000 | 0.617500 | 1.030000 | 1.35000 | 2.180000 |
| VR | 200.0 | 1.964300 | 0.300827 | 0.930000 | 1.770000 | 1.960000 | 2.14250 | 2.870000 |
| Prod | 200.0 | 3842.630027 | 1594.301295 | 803.640483 | 2551.414599 | 3626.229052 | 4739.73408 | 9021.792491 |

æˆ‘ä»¬æ£€æŸ¥äº†æ€»ç»“ç»Ÿè®¡é‡æ˜¯ä»¶å¥½äº‹ã€‚

+   æ²¡æœ‰æ˜æ˜¾çš„é”™è¯¯

+   æ£€æŸ¥æ¯ä¸ªç‰¹å¾å€¼çš„èŒƒå›´ï¼Œä»¥è®¾ç½®å’Œè°ƒæ•´ç»˜å›¾é™åˆ¶ã€‚è§ä¸Šå›¾ã€‚

## è®¡ç®—ç›¸å…³çŸ©é˜µå’Œä¸å“åº”æ’åçš„ç›¸å…³æ€§

è®©æˆ‘ä»¬è¿›è¡Œç›¸å…³æ€§åˆ†æã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¹‹å‰å£°æ˜çš„å‡½æ•°è®¡ç®—å¹¶æŸ¥çœ‹ç›¸å…³çŸ©é˜µä»¥åŠä¸å“åº”ç‰¹å¾çš„ç›¸å…³æ€§ã€‚

+   ç›¸å…³æ€§åˆ†æåŸºäºçº¿æ€§å…³ç³»çš„å‡è®¾ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªè‰¯å¥½çš„å¼€ç«¯

```py
corr_matrix = df.corr()
correlation = corr_matrix.iloc[:,-1].values[:-1]

plt.subplot(121)
plot_corr(corr_matrix,'Correlation Matrix',1.0,0.5)           # using our correlation matrix visualization function
plt.xlabel('Features'); plt.ylabel('Features')

plt.subplot(122)
feature_rank_plot(pred,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp[0],'Correlation',0.5)

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/6194a66796a23a77575acb7c89abb176.png)

æ³¨æ„ç”±äºæ¯ä¸ªå˜é‡ä¸å…¶è‡ªèº«çš„ç›¸å…³æ€§è€Œäº§ç”Ÿçš„ 1.0 å¯¹è§’çº¿ã€‚

è¿™çœ‹èµ·æ¥ä¸é”™ã€‚ç›¸å…³æ€§å¤§å°æœ‰æ··åˆã€‚å½“ç„¶ï¼Œç›¸å…³ç³»æ•°ä»…é™äºçº¿æ€§ç›¸å…³ç¨‹åº¦çš„åº¦é‡ã€‚

+   è®©æˆ‘ä»¬æŸ¥çœ‹çŸ©é˜µæ•£ç‚¹å›¾ï¼Œä»¥äº†è§£ç‰¹å¾ä¹‹é—´çš„æˆå¯¹å…³ç³»ã€‚

```py
pairgrid = sns.PairGrid(df,vars=['Por','Perm','AI','Brittle','TOC','Prod']) # matrix scatter plots
pairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)
pairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle
pairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, 
                              alpha = 1.0, n_levels = 10)
pairgrid.add_legend()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/18798d69cb2a69a3c10006380bba6873.png)

## ä»…ä½¿ç”¨ä¸¤ä¸ªé¢„æµ‹ç‰¹å¾è¿›è¡Œå·¥ä½œ

è®©æˆ‘ä»¬å°†é—®é¢˜ç®€åŒ–ä¸º 2 ä¸ªé¢„æµ‹ç‰¹å¾ï¼Œå­”éš™ç‡å’Œè„†æ€§ï¼Œä»¥é¢„æµ‹äº§é‡ã€‚é€šè¿‡åªä½¿ç”¨ 2 ä¸ªç‰¹å¾ï¼Œå¯è§†åŒ–ç‰¹å¾ç©ºé—´çš„åˆ†å‰²å˜å¾—éå¸¸å®¹æ˜“ï¼ˆå®ƒåªæœ‰ 2 ç»´ï¼Œå¯ä»¥å®Œå…¨æ˜¾ç¤ºåœ¨ä¸€ä¸ªå›¾è¡¨ä¸Šï¼‰ã€‚

## æ ‡å‡†åŒ–é¢„æµ‹ç‰¹å¾

k æœ€è¿‘é‚»æ–¹æ³•åœ¨ç‰¹å¾ç©ºé—´ä¸­ä½¿ç”¨æœ€è¿‘è®­ç»ƒæ ·æœ¬æœç´¢ï¼ˆç±»ä¼¼äº k-means èšç±»ï¼‰ã€‚ä¸ºäº†æ¶ˆé™¤ç‰¹å¾èŒƒå›´å¯¹æ–¹æ³•çš„å½±å“ï¼Œæˆ‘ä»¬æ ‡å‡†åŒ–äº†ç‰¹å¾ã€‚

+   æˆ‘ä»¬å°†æ ‡å‡†åŒ–æˆ‘ä»¬çš„é¢„æµ‹ç‰¹å¾ï¼Œä½¿å…¶å‡å€¼ä¸ºé›¶ï¼Œæ–¹å·®ä¸º 1ã€‚

+   æˆ‘ä»¬ä½¿ç”¨ scikit-learn çš„é¢„å¤„ç†æ¨¡å—æ¥ç®€åŒ–è¿™ä¸€æ­¥ï¼Œå¹¶æä¾›æ–¹ä¾¿ä¸”å®‰å…¨çš„åå‘è½¬æ¢ã€‚

```py
if1 = 0; if2 = 3                                              # selected predictor features

transform = StandardScaler();                                 # instantiate feature standardization method

sel_pred = [pred[if1],pred[if2]]
sel_features = pred + [resp]

spredlabel = ['Standardized ' + element for element in predlabel] # standardized predictors list

sel_spredlabel = [spredlabel[if1]] + [spredlabel[if2]] 

sel_spred = ['s' + element for element in sel_pred]           # standardized predictors list

df[sel_spred[0]] = transform.fit_transform(df.loc[:,sel_pred].values)[:,0] # standardize the data features to mean = 0, var = 1.0
df[sel_spred[1]] = transform.fit_transform(df.loc[:,sel_pred].values)[:,1] # standardize the data features to mean = 0, var = 1.0

print('Selected Predictor Features: ' + str(sel_pred))
print('Standardized Selected Predictor Features: ' + str(sel_spred))
print('Response Feature: ' + str([resp]))
df.head() 
```

```py
Selected Predictor Features: ['Por', 'Brittle']
Standardized Selected Predictor Features: ['sPor', 'sBrittle']
Response Feature: [['Prod']] 
```

|  | Por | Perm | AI | Brittle | TOC | VR | Prod | sPor | sBrittle |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 12.08 | 2.92 | 2.80 | 81.40 | 1.16 | 2.31 | 1339.165488 | -0.982256 | 2.358297 |
| 1 | 12.38 | 3.53 | 3.22 | 46.17 | 0.89 | 1.88 | 3383.979252 | -0.881032 | -0.141332 |
| 2 | 14.02 | 2.59 | 4.01 | 72.80 | 0.89 | 2.72 | 2509.686720 | -0.327677 | 1.748113 |
| 3 | 17.67 | 6.75 | 2.63 | 39.81 | 1.08 | 1.88 | 5514.421023 | 0.903875 | -0.592585 |
| 4 | 17.52 | 4.57 | 3.18 | 10.94 | 1.51 | 1.90 | 3532.020478 | 0.853263 | -2.640962 |

è®©æˆ‘ä»¬æ¼”ç¤ºä»æ ‡å‡†åŒ–ç‰¹å¾åˆ°åŸå§‹ç‰¹å¾çš„é€†å‘è½¬æ¢ã€‚

+   åœ¨æˆ‘ä»¬çš„å·¥ä½œæµç¨‹ä¸­æˆ‘ä»¬ä¸éœ€è¦è¿™ä¸ªï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦å°†é¢„æµ‹ç‰¹å¾è¿›è¡Œæ­£å‘è½¬æ¢ä»¥è®­ç»ƒæ¨¡å‹å’Œåšå‡ºé¢„æµ‹

```py
print('Backtransformed: \n Por    Brittle')
transform.inverse_transform(df.loc[:,sel_spred])[:5,:]        # check the reverse standardization 
```

```py
Backtransformed: 
        Por    Brittle 
```

```py
array([[12.08, 81.4 ],
       [12.38, 46.17],
       [14.02, 72.8 ],
       [17.67, 39.81],
       [17.52, 10.94]]) 
```

æˆ‘ä»¬å¯ä»¥å°†ä¸Šé¢çš„è¾“å‡ºä¸åŸå§‹çš„å­”éš™ç‡å’Œè„†æ€§è¿›è¡Œæ¯”è¾ƒã€‚åå‘è½¬æ¢æ˜¯æœ‰æ•ˆçš„ï¼

+   æˆ‘ä»¬å°†ä½¿ç”¨æ­¤æ–¹æ³•åœ¨éœ€è¦æ—¶è¿”å›åˆ°åŸå§‹ç‰¹å¾å•ä½ã€‚

+   é€šå¸¸ï¼Œå¯¹äºé¢„æµ‹ç‰¹å¾ï¼Œæˆ‘ä»¬ä¸éœ€è¦è¿›è¡Œåå‘è½¬æ¢ï¼Œæˆ‘ä»¬åªéœ€å°†é¢„æµ‹ç‰¹å¾è¿›è¡Œæ­£å‘è½¬æ¢ä»¥é¢„æµ‹å“åº”ç‰¹å¾ã€‚

+   åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åœ¨æ„å»ºæ¨¡å‹æ—¶ä¸éœ€è¦è½¬æ¢å“åº”ç‰¹å¾ã€‚å“åº”ç‰¹å¾çš„åˆ†å¸ƒè¡¨ç°è‰¯å¥½ï¼Œk æœ€è¿‘é‚»ç®—æ³•ä¸­å¹¶æ²¡æœ‰ç†è®ºæœŸæœ›å“åº”ç‰¹å¾æœ‰ç‰¹å®šçš„èŒƒå›´æˆ–åˆ†å¸ƒä»½é¢ã€‚

## ç‰¹å¾èŒƒå›´

è®©æˆ‘ä»¬è®¾å®šä¸€äº›ç»˜å›¾çš„èŒƒå›´ã€‚æ³¨æ„å¯¹äºæ ‡å‡†åŒ–çš„é¢„æµ‹ç‰¹å¾ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨-3.5 åˆ° 3.5 ä½œä¸ºé™åˆ¶ã€‚

```py
Xmin = [5.0, 0.0]; Xmax = [25.0,100.0]                        # selected predictor features min and max 
```

## è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²

ä¸ºäº†æ–¹ä¾¿å’Œç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn çš„éšæœºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²ã€‚

```py
X_train, X_test, y_train, y_test = train_test_split(df.loc[:,sel_spred],df.loc[:,resp],test_size=0.25,random_state=73073)
df_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)
df_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) 
```

è®©æˆ‘ä»¬å…ˆæ£€æŸ¥å­”éš™ç‡ã€è„†æ€§å’Œäº§é‡çš„å•å˜é‡ç»Ÿè®¡ã€‚

```py
nbins = 20                                                    # number of histogram bins

plt.subplot(221)
plt.hist(X_train[sel_spred[0]],alpha = 0.8,color = 'darkorange',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Train')
plt.hist(X_test[sel_spred[0]],alpha = 0.8,color = 'red',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Test')
plt.title(sel_spred[0]); plt.xlim(-3,3); plt.xlabel(sel_spredlabel[0]); plt.ylabel('Frequency'); add_grid(); plt.legend(loc='upper right')

plt.subplot(222)
plt.hist(X_train[sel_spred[1]],alpha = 0.8,color = 'darkorange',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Train')
plt.hist(X_test[sel_spred[1]],alpha = 0.8,color = 'red',edgecolor = 'black',bins=np.linspace(-3,3,nbins),label='Test')
plt.title(sel_spred[1]); plt.xlim(-3,3); plt.xlabel(sel_spredlabel[1]); plt.ylabel('Frequency'); add_grid(); plt.legend(loc='upper right')

plt.subplot(223)
plt.hist(y_train[resp],alpha = 0.8,color = 'darkorange',edgecolor = 'black',bins=np.linspace(ymin,ymax,nbins),label='Train')
plt.hist(y_test[resp],alpha = 0.8,color = 'red',edgecolor = 'black',bins=np.linspace(ymin,ymax,nbins),label='Test')
plt.legend(loc='upper right'); plt.title(resp[0]); plt.xlim(ymin,ymax) 
plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
plt.xlabel(resplabel); plt.ylabel('Frequency'); add_grid()

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/a6e3b01d9e6230c6eb26e1b06c604aa2fc175bb399d25bcd7794d5fb4b1b1e46.png](img/2447bd1e6857b017d16d4197616a3dc1.png)

åˆ†å¸ƒè¡¨ç°è‰¯å¥½ï¼Œ

+   æˆ‘ä»¬æ— æ³•è§‚å¯Ÿåˆ°æ˜æ˜¾çš„ç¼ºå£æˆ–æˆªæ–­ã€‚

+   æ£€æŸ¥è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„è¦†ç›–ç‡

è®©æˆ‘ä»¬çœ‹çœ‹å­”éš™ç‡ä¸è„†æ€§çš„æ•£ç‚¹å›¾ï¼Œç‚¹æ ¹æ®äº§é‡ç€è‰²ã€‚

```py
plt.subplot(121)                                              # train data plot
im = plt.scatter(X_train[sel_spred[0]],X_train[sel_spred[1]],s=None, c=y_train[resp[0]], marker=None, cmap=cmap, norm=None, vmin=ymin, 
                 vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors="black")
plt.title('Train ' + resp[0] + ' vs. ' + sel_spred[1] + ' and ' + sel_spred[1]); plt.xlabel(sel_spredlabel[0]); plt.ylabel(sel_spredlabel[1])
plt.xlim(-3,3); plt.ylim(-3,3)
cbar = plt.colorbar(im, orientation = 'vertical')
cbar.set_label(resplabel, rotation=270, labelpad=20)
cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))
add_grid()

plt.subplot(122)                                               # test data plot
im = plt.scatter(X_test[sel_spred[0]],X_test[sel_spred[1]],s=None, c=y_test[resp[0]], marker=None, cmap=cmap, norm=None, vmin=ymin, 
                 vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors="black")
plt.title('Test ' + resp[0] + ' vs. ' + sel_spred[1] + ' and ' + sel_spred[1]); plt.xlabel(sel_spredlabel[0]); plt.ylabel(sel_spredlabel[1])
plt.xlim(-3,3); plt.ylim(-3,3)
cbar = plt.colorbar(im, orientation = 'vertical')
cbar.set_label(resplabel, rotation=270, labelpad=20)
cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))
add_grid()

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/4c88383d908cf65a54e1601379bf0b36cdbfab09eeae0ff5cc37587387fd084c.png](img/0b1f8f8266faddc853479c3aab599c2e.png)

è¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥æ˜¯éçº¿æ€§çš„ï¼Œä¸èƒ½ç”¨ç®€å•çš„çº¿æ€§å›å½’æ¥å»ºæ¨¡ã€‚

+   çœ‹èµ·æ¥ï¼Œåœ¨è„†æ€§å’Œå¢åŠ å­”éš™ç‡æ–¹é¢æœ‰ä¸€ä¸ªæœ€ä½³ç‚¹ï¼Œå¯¹äºç”Ÿäº§æ¥è¯´ï¼Œå¢åŠ å­”éš™ç‡æ€»æ˜¯æœ‰ç›Šçš„ã€‚

## å®ä¾‹åŒ–ã€æ‹Ÿåˆå’Œé¢„æµ‹ k æœ€è¿‘é‚»æ¨¡å‹ã€‚

è®©æˆ‘ä»¬å®ä¾‹åŒ–ã€æ‹Ÿåˆå’Œé¢„æµ‹ä¸€ä¸ª k æœ€è¿‘é‚»æ¨¡å‹ã€‚

+   ä½¿ç”¨è¶…å‚æ•°ï¼Œå®ä¾‹åŒ– k æœ€è¿‘é‚»ã€‚

+   ä½¿ç”¨è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn çš„æ ‡å‡† fit å‡½æ•°ã€‚

```py
n_neighbours = 1; p = 2; weights = 'uniform'                 # model hyperparameters
neigh = KNeighborsRegressor(weights = weights, n_neighbors=n_neighbours, p = p) # instantiate the prediction model 
```

æˆ‘ä»¬å·²ç»è®¾ç½®äº†è¶…å‚æ•°ï¼š

+   weights = æ ¹æ®æœ€è¿‘é‚»å±…ç»™å‡ºçš„é¢„æµ‹çš„å¹³å‡æƒé‡ã€‚â€˜uniformâ€™æ˜¯ç®—æœ¯å¹³å‡ï¼Œè€Œâ€˜distanceâ€™æ˜¯é€†è·ç¦»åŠ æƒã€‚

+   n_neighbours = æœ€å¤§é‚»å±…æ•°ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬é€šè¿‡é™åˆ¶åˆ° 5 ä¸ªæœ€è¿‘é‚»å±…æ¥çº¦æŸæˆ‘ä»¬çš„é¢„æµ‹ã€‚

+   p = è·ç¦»åº¦é‡å¹‚æˆ– Minkowski åº¦é‡ï¼ˆ1 = æ›¼å“ˆé¡¿è·ç¦»ï¼Œ2 = æ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰ï¼Œç”¨äºå¯»æ‰¾æœ€è¿‘é‚»å±…ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½æ ¹æ®å­”éš™ç‡å’Œè„†æ€§æ¥æ‹Ÿåˆæˆ‘ä»¬çš„ç”Ÿäº§é¢„æµ‹æ¨¡å‹ã€‚

+   æˆ‘ä»¬å°†ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„ä¸¤ä¸ªå‡½æ•°æ¥å¯è§†åŒ–ç‰¹å¾ç©ºé—´ä¸­çš„ k æœ€è¿‘é‚»é¢„æµ‹ä»¥åŠè®­ç»ƒæ•°æ®çš„å®é™…å’Œä¼°è®¡ç”Ÿäº§çš„äº¤å‰å›¾ï¼Œä»¥åŠæ¥è‡ª sklearn.metrics æ¨¡å—çš„ä¸‰ä¸ªæ¨¡å‹åº¦é‡ã€‚

```py
neigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data

plt.subplot(221)                                              # training data vs. the model predictions
Z = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,
                    'Training Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(222)                                              # testing data vs. the model predictions
visualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,
                'Testing Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

# plt.subplot(223)                                              # model accuracy check
# check_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,
#             y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/8a097688955c9a933b8a09894a1a0ea5.png)

æ¨¡å‹çœ‹èµ·æ¥ä¸é”™ï¼š

+   éå‚æ•°æ–¹æ³•åœ¨æ‹Ÿåˆé¢„æµ‹ç‰¹å¾ç©ºé—´ä¸­çš„éçº¿æ€§å“åº”æ¨¡å¼æ–¹é¢éå¸¸çµæ´»ã€‚

+   æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç”±äº k æœ€è¿‘æ•°æ®æœ‰é™å’Œä½¿ç”¨å‡åŒ€åŠ æƒè€Œäº§ç”Ÿçš„æœç´¢ä¼ªå½±ã€‚

+   å¯¹äºè¿™ä¸ªä½ç»´é—®é¢˜ï¼ˆåªæœ‰ 2 ä¸ªé¢„æµ‹ç‰¹å¾ï¼‰ï¼Œæˆ‘ä»¬æ‹¥æœ‰å¯†é›†çš„æ•°æ®ã€‚

+   æµ‹è¯•æ•°æ®å’Œè®­ç»ƒæ•°æ®åœ¨é¢„æµ‹ç‰¹å¾ç©ºé—´ä¸­ä¸€è‡´ä¸”æ¥è¿‘ã€‚

è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨ä¸€ä¸ªéå¸¸å¤§çš„ k è¶…å‚æ•°æ¥è¿‡æ‹Ÿåˆæ¨¡å‹ã€‚

```py
n_neighbours = 5; p = 2; weights = 'uniform'                # model hyperparameters
neigh = KNeighborsRegressor(weights = weights,n_neighbors=n_neighbours,p = p) # instantiate the prediction model

neigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data

plt.subplot(221)                                              # training data vs. the model predictions
Z = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,
                    'Training Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(222)                                              # testing data vs. the model predictions
visualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,
                'Testing Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(223)                                              # model accuracy check
check_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,
            y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/7a8fad6c8ab65ba24660fe87eb558d34.png)

æ³¨æ„ï¼Œè¿™å¹³æ»‘äº†å“åº”ï¼Œé¢„æµ‹æ­£åœ¨æ¥è¿‘å…¨å±€å¹³å‡å€¼ã€‚

+   æˆ‘ä»¬æœ‰ä¸€ä¸ªæ¬ æ‹Ÿåˆçš„æ¨¡å‹ã€‚

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„ k æœ€è¿‘é‚»é¢„æµ‹æ¨¡å‹ä½¿ç”¨è¾ƒå°çš„ k è¶…å‚æ•°ã€‚

```py
n_neighbours = 1; p = 2; weights = 'uniform'                  # model hyperparameters
neigh = KNeighborsRegressor(weights = weights,n_neighbors=n_neighbours,p = p) # instantiate the prediction model

neigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data

plt.subplot(221)                                              # training data vs. the model predictions
Z = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,
                    'Training Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(222)                                              # testing data vs. the model predictions
visualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,
                'Testing Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(223)                                              # model accuracy check
check_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,
            y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/652c1c3e20ca9e893f0cf17db9352459.png)

ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªæç«¯è¿‡æ‹Ÿåˆçš„æ¨¡å‹ã€‚

+   è®­ç»ƒå‡æ–¹è¯¯å·®ä¸º 0.0ï¼Œæµ‹è¯•è¯¯å·®ç›¸å½“é«˜ã€‚

+   æ³¨æ„ï¼Œæˆ‘ä»¬è¿‡æ‹Ÿåˆæ¨¡å‹ä¸­çš„æŸäº›é¢„æµ‹è¶…å‡ºäº†ç»˜å›¾çš„æœ€å°å’Œæœ€å¤§å“åº”ç‰¹å¾å€¼ã€‚

è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨ L1ï¼Œæ›¼å“ˆé¡¿è·ç¦»æ¥æ‰¾åˆ° k æœ€è¿‘é‚»å±…ã€‚

```py
n_neighbours = 10; p = 1; weights = 'uniform'                 # model hyperparameters
neigh = KNeighborsRegressor(weights = weights,n_neighbors=n_neighbours,p = p) # instantiate the prediction model

neigh_fit = neigh.fit(X_train,y_train)                        # train the model with the training data

plt.subplot(221)                                              # training data vs. the model predictions
Z = visualize_model(neigh_fit,X_train[sel_spred[0]],-3.5,3.5,X_train[sel_spred[1]],-3.5,3.5,y_train[resp[0]],ymin,ymax,
                    'Training Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(222)                                              # testing data vs. the model predictions
visualize_model(neigh_fit,X_test[sel_spred[0]],-3.5,3.5,X_test[sel_spred[1]],-3.5,3.5,y_test[resp[0]],ymin,ymax,
                'Testing Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(223)                                              # model accuracy check
check_model(neigh_fit,X_train[sel_spred[0]],X_train[sel_spred[1]],X_test[sel_spred[0]],X_test[sel_spred[1]],ymin,ymax,
            y_train[resp[0]],y_test[resp[0]],'K Nearest Neighbour Regression Model Accuracy')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.25, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/3a4e29a43415592363cfc3314ffd6b4b.png)

å°†è¿™ä¸ªé¢„æµ‹æ¨¡å‹ä¸æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å”¯ä¸€æ”¹å˜çš„æ˜¯å°† k æœ€è¿‘æ ·æœ¬çš„è·ç¦»æœç´¢ä»æ¬§å‡ é‡Œå¾—è·ç¦»æ›´æ”¹ä¸ºæ›¼å“ˆé¡¿è·ç¦»ã€‚

+   æœç´¢ä¼ªå½±ç°åœ¨ä¸ç‰¹å¾å¯¹é½ï¼ˆå°„çº¿åœ¨ x å’Œ y æ–¹å‘ä¸Šå®šå‘ï¼‰

## k-Nearest Neighbours çš„è¶…å‚æ•°è°ƒæ•´

è®©æˆ‘ä»¬åœ¨è°ƒæ•´è¶…å‚æ•°æ—¶æ£€æŸ¥è¿™ä¸€ç‚¹ã€‚

é‚£ä¹ˆ $k$ åšäº†ä»€ä¹ˆï¼Ÿ

+   å°çš„ $k$ è¶…å‚æ•°å¯¼è‡´åœ¨é¢„æµ‹ç‰¹å¾ç©ºé—´ä¸Šäº§ç”Ÿä¸€ä¸ªå±€éƒ¨çš„ç‰¹å®šé¢„æµ‹æ¨¡å‹

+   å¤§çš„ $k$ è¶…å‚æ•°å¯¼è‡´åœ¨é¢„æµ‹ç‰¹å¾ç©ºé—´ä¸Šäº§ç”Ÿä¸€ä¸ªæ›´å¹³æ»‘ã€å…¨å±€æ‹Ÿåˆçš„é¢„æµ‹æ¨¡å‹

è¿™ä¸å…¶ä»–æ¨¡å‹ï¼ˆå¦‚å†³ç­–æ ‘ï¼‰è§‚å¯Ÿåˆ°çš„ä»ä½åˆ°é«˜å¤æ‚åº¦ç±»ä¼¼ã€‚

+   å°çš„ $k$ æ˜¯å¤æ‚çš„

+   å¤§çš„ $k$ æ˜¯ç®€å•çš„

æˆ‘ä»¬éœ€è¦è°ƒæ•´å¤æ‚æ€§ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## è°ƒæ•´è¶…å‚æ•°

è®©æˆ‘ä»¬å¾ªç¯éå†å¤šä¸ª $k$ æœ€è¿‘é‚»ï¼Œå¯¹å¹³å‡å’Œå€’æ•°è·ç¦»ä¼°è®¡è¿›è¡Œå¹³å‡ï¼Œä»¥è·å–æµ‹è¯•å‡†ç¡®æ€§çš„æœ€ä½³è¶…å‚æ•°ã€‚

```py
k = 1                                                         # set initial, lowest k hyperparameter
dist_error = []; unif_error = []; k_mat = []                  # make lists to store the results
while k <= 150:                                               # loop over the k hyperparameter
    neigh_dist = KNeighborsRegressor(weights = 'distance', n_neighbors=k, p = 2) # instantiate the model
    neigh_dist_fit = neigh_dist.fit(X_train,y_train)          # train the model with the training data
    y_pred = neigh_dist_fit.predict(X_test)                   # predict over the testing cases
    MSE = metrics.mean_squared_error(y_test,y_pred)           # calculate the MSE testing
    dist_error.append(MSE)                                    # add to the list of MSE

    neigh_unif = KNeighborsRegressor(weights = 'uniform', n_neighbors=k, p = 2)
    neigh_unif_fit = neigh_unif.fit(X_train,y_train)          # train the model with the training data
    y_pred = neigh_unif_fit.predict(X_test)                   # predict over the testing cases
    MSE = metrics.mean_squared_error(y_test,y_pred)           # calculate the MSE testing
    unif_error.append(MSE)                                    # add to the list of MSE

    k_mat.append(k)                                           # append k to an array for plotting
    k = k + 1 
```

ç°åœ¨è®©æˆ‘ä»¬ç»˜åˆ¶ç»“æœã€‚

```py
plt.subplot(111)
plt.scatter(k_mat,dist_error,s=None, c='red',label = 'inverse distance weighted', marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors="black")
plt.scatter(k_mat,unif_error,s=None, c='blue',label = 'arithmetic average', marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors="black")
plt.title('Testing Error vs. Number of Nearest Neighbours'); plt.xlabel('Number of Nearest Neighbours'); plt.ylabel('Mean Square Error')
plt.legend(); add_grid()
plt.xlim(0,50); plt.ylim([0,750000])
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8,wspace=0.15,hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/fe795ea54d585888e2632307876aaa72.png)

æˆ‘ä»¬å¯ä»¥ä»è¿™ä¸ªç»“æœä¸­è§‚å¯Ÿåˆ°ä»€ä¹ˆï¼Ÿ

+   å½“ $k = 12$ æœ€è¿‘é‚»æ—¶ï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•ä¸­æœ€å°åŒ–å‡æ–¹è¯¯å·®

+   ä¸ç®—æœ¯å¹³å‡ï¼ˆåœ¨é¢„æµ‹ç‰¹å¾ç©ºé—´ä¸­å¯¹ k ä¸ªæœ€è¿‘è®­ç»ƒæ•°æ®å‡åŒ€åŠ æƒï¼‰ç›¸æ¯”ï¼Œä½¿ç”¨å€’æ•°è·ç¦»åŠ æƒæˆ‘ä»¬æœ‰æ›´å¥½çš„æ€§èƒ½

æˆ‘ä»¬æ¨¡å‹çš„æœ€ä½³ç‰¹å®šåº¦/å¤æ‚åº¦æ˜¯å­˜åœ¨çš„ã€‚

+   1 ä¸ªæœ€è¿‘é‚»æ˜¯ä¸€ä¸ªéå¸¸å±€éƒ¨ç‰¹å®šçš„æ¨¡å‹ï¼ˆè¿‡æ‹Ÿåˆï¼‰

+   è®¸å¤šæœ€è¿‘é‚»åŒ…å«å¤ªå¤šä¿¡æ¯ï¼Œè¿‡äºæ³›åŒ–ï¼ˆæ¬ æ‹Ÿåˆï¼‰

æˆ‘ä»¬æ­£åœ¨è§‚å¯Ÿ $k$ æœ€è¿‘é‚»æ¨¡å‹çš„å‡†ç¡®æ€§ä¸å¤æ‚æ€§çš„æƒè¡¡ã€‚

## k æŠ˜äº¤å‰éªŒè¯

é€šè¿‡è§‚å¯Ÿå‡†ç¡®æ€§ä¸å¤æ‚æ€§çš„æƒè¡¡æ¥è¯„ä¼°æˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½æ˜¯æœ‰ç”¨çš„ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬çœŸæ­£æƒ³è¦åšçš„æ˜¯ä¸¥æ ¼æµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åº”è¯¥æ‰§è¡Œæ›´ä¸¥æ ¼çš„äº¤å‰éªŒè¯ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°ä¸åŒçš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ã€‚scikit learn æœ‰ä¸€ä¸ªå†…ç½®çš„äº¤å‰éªŒè¯æ–¹æ³•ï¼Œç§°ä¸º cross_val_scoreï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥ï¼š

1.  ä½¿ç”¨è¿­ä»£åˆ†ç¦»è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„æ–¹æ³•åº”ç”¨ k æŠ˜æ³•

1.  è‡ªåŠ¨åŒ–æ¨¡å‹æ„å»ºï¼Œå¾ªç¯éå†æŠ˜å å¹¶å¹³å‡æ„Ÿå…´è¶£çš„æŒ‡æ ‡

è®©æˆ‘ä»¬åœ¨å…·æœ‰å¯å˜æ•°é‡ $k$ ä¸ªæœ€è¿‘é‚»çš„ k æœ€è¿‘é‚»é¢„æµ‹ä¸Šå°è¯•ä¸€ä¸‹ã€‚æ³¨æ„äº¤å‰éªŒè¯è®¾ç½®ä¸ºä½¿ç”¨ 4 ä¸ªå¤„ç†å™¨ï¼Œä½†ä»å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ‰èƒ½è¿è¡Œã€‚

```py
score = []                                                  # code modified from StackOverFlow by Dimosthenis
k_mat = []
for k in range(1,150):
    neigh_dist = KNeighborsRegressor(weights = 'distance', n_neighbors=k, p = 1)
    scores = cross_val_score(estimator=neigh_dist, X= np.c_[df['sPor'],df['sBrittle']],y=df['Prod'], cv=4, n_jobs=4,
                             scoring = "neg_mean_squared_error") # Perform 7-fold cross validation
    score.append(abs(scores.mean()))
    k_mat.append(k) 
```

è¾“å‡ºæ˜¯ä¸€ä¸ªæ•°ç»„ï¼ŒåŒ…å«æ¯ä¸ªå¤æ‚åº¦çº§åˆ«ï¼ˆ$k$ ä¸ªæœ€è¿‘é‚»çš„æ•°é‡ï¼‰çš„å¹³å‡åˆ†æ•°ï¼ˆMSEï¼‰ï¼Œä»¥åŠä¸€ä¸ªåŒ…å« $k$ å€¼çš„æ•°ç»„ã€‚

```py
plt.figure(figsize=(8,6))
plt.scatter(k_mat,score,s=None, c="red", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.5, edgecolors="black")
plt.title('k-fold Cross Validation Error (MSE) vs. k Nearest Neighbours'); plt.xlabel('Number of Nearest Neighbours'); plt.ylabel('Mean Square Error')
plt.xlim(1,150); plt.ylim([0,1400000]); add_grid()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8,wspace=0.15,hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/d79973afda6b57fc45418841e4ff114d.png)

åœ¨ 10 ä¸ªæœ€è¿‘é‚»çš„è¶…å‚æ•°ä¸‹ï¼Œæˆ‘ä»¬åœ¨ k æŠ˜äº¤å‰éªŒè¯æ¨¡å‹æµ‹è¯•ä¸­è·å¾—äº†æœ€é«˜çš„å‡†ç¡®ç‡ã€‚

## é¢„æµ‹ç‰¹å¾æ ‡å‡†åŒ–

æˆ‘ä»¬å·²ç»æ ‡å‡†åŒ–äº†é¢„æµ‹ç‰¹å¾ï¼Œä»¥æ¶ˆé™¤å®ƒä»¬èŒƒå›´çš„å½±å“ã€‚

+   å¦‚æœæˆ‘ä»¬ä½¿ç”¨åŸå§‹é¢„æµ‹ç‰¹å¾ä¼šæ€æ ·å‘¢ï¼Ÿ

è®©æˆ‘ä»¬è¯•è¯•çœ‹ã€‚

+   æˆ‘ä»¬é¦–å…ˆä½¿ç”¨åŸå§‹ç‰¹å¾è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²ï¼Œè€Œä¸è¿›è¡Œæ ‡å‡†åŒ–

```py
X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(df.loc[:,sel_pred],df.loc[:,resp],test_size=0.25,random_state=73073)

neigh = KNeighborsRegressor(weights = 'distance', n_neighbors=15, p = 1)
neigh_fit = neigh.fit(X_train_orig,y_train_orig)                # train the model with the training data

plt.subplot(121)                                              # training data vs. the model predictions
Z = visualize_model(neigh_fit,X_train_orig[sel_pred[0]],Xmin[0],Xmax[0],X_train_orig[sel_pred[1]],Xmin[1],Xmax[1],y_train[resp[0]],ymin,ymax,
                    'Training Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(122)                                              # testing data vs. the model predictions
visualize_model(neigh_fit,X_test_orig[sel_pred[0]],Xmin[0],Xmax[0],X_test_orig[sel_pred[1]],Xmin[1],Xmax[1],y_test[resp[0]],ymin,ymax,
                'Testing Data and k Nearest Neighbours')
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/7c1a8ce67e0629a7c8b0a3ceccdf2686.png)

ä½ çœ‹åˆ°æ°´å¹³å¸¦çŠ¶äº†å—ï¼Ÿè„†æ€§ä¸å­”éš™ç‡ç›¸æ¯”ï¼Œå¹…åº¦èŒƒå›´æ›´å¤§ï¼Œå¯¼è‡´äº†è¿™ç§å¸¦çŠ¶ã€‚

+   ç‰¹å¾ç©ºé—´ä¸­çš„è·ç¦»å¯¹è„†æ€§çš„ç›¸å¯¹å˜åŒ–æ¯”å­”éš™ç‡æ›´æ•æ„Ÿ

è®©æˆ‘ä»¬å°†å­”éš™ç‡è½¬æ¢ä¸ºåˆ†æ•°ï¼Œå¹¶è§‚å¯Ÿç”±äºå°†å­”éš™ç‡ä½œä¸ºåˆ†æ•°è€Œä¸æ˜¯ç™¾åˆ†æ¯”è¿›è¡Œå¤„ç†çš„ä»»æ„å†³ç­–ï¼Œæˆ‘ä»¬çš„é¢„æµ‹å› å­çš„å˜åŒ–ã€‚

+   é€šè¿‡å°†æ‰€æœ‰å­”éš™ç‡å€¼ä¹˜ä»¥$\frac{1}{100}$å› å­ã€‚

```py
X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(df.loc[:,sel_pred],df.loc[:,resp],test_size=0.25,random_state=73073)

X_train_orig['Por'] = X_train_orig['Por']/100.0
X_test_orig['Por'] = X_test_orig['Por']/100.0

neigh = KNeighborsRegressor(weights = 'distance', n_neighbors=15, p = 1)
neigh_fit = neigh.fit(X_train_orig,y_train_orig)                # train the model with the training data

plt.subplot(121)                                              # training data vs. the model predictions
Z = visualize_model(neigh_fit,X_train_orig[sel_pred[0]],Xmin[0]/100,Xmax[0]/100,X_train_orig[sel_pred[1]],Xmin[1],Xmax[1],y_train[resp[0]],ymin,ymax,
                    'Training Data and k Nearest Neighbours',False)
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplot(122)                                              # testing data vs. the model predictions
visualize_model(neigh_fit,X_test_orig[sel_pred[0]],Xmin[0]/100,Xmax[0]/100,X_test_orig[sel_pred[1]],Xmin[1],Xmax[1],y_test[resp[0]],ymin,ymax,
                'Testing Data and k Nearest Neighbours',False)
plt.annotate('Hyperparameters',[1.5,3.2],color='white'); plt.annotate('weights: ' + weights,[1.5,2.9],color='white')
plt.annotate('n neighbours: ' + str(n_neighbours),[1.5,2.6],color='white'); plt.annotate('distance norm: ' + str(p),[1.5,2.3],color='white')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/bf1f50f1e62336305668f4aff28ac682.png)

+   å½“æˆ‘ä»¬å°†ç™¾åˆ†æ¯”è½¬æ¢ä¸ºåˆ†æ•°å­”éš™ç‡æ—¶ï¼Œè¿™ç§å¸¦çŠ¶æ•ˆåº”å˜å¾—æ›´åŠ ä¸¥é‡ï¼Œå› ä¸ºå­”éš™ç‡ä¸­çš„è·ç¦»çœ‹èµ·æ¥æ¯”è„†æ€§ä¸­çš„è·ç¦»è¦è¿‘å¾—å¤šï¼Œä»…ä»…æ˜¯å› ä¸ºç‰¹å¾èŒƒå›´çš„ä¸åŒã€‚

æˆ‘ä»¬ç”¨äºåˆ†é…æœ€è¿‘é‚»çš„è·ç¦»åº¦é‡å¯¹ç‰¹å¾å•ä½éå¸¸æ•æ„Ÿã€‚åœ¨ä½¿ç”¨å®ƒä»¬æ„å»ºæˆ‘ä»¬çš„$k$æœ€è¿‘é‚»å›å½’æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥å§‹ç»ˆæ ‡å‡†åŒ–æ‰€æœ‰é¢„æµ‹ç‰¹å¾ï¼ˆä½¿å®ƒä»¬å¤„äºå¹³ç­‰åœ°ä½ï¼‰ï¼

## ä½¿ç”¨ç®¡é“çš„ scikit-learn ä¸­çš„ k æœ€è¿‘é‚»å›å½’

éœ€è¦æ ‡å‡†åŒ–ç‰¹å¾ã€è®­ç»ƒã€è°ƒæ•´å¹¶ä½¿ç”¨æ‰€æœ‰æ•°æ®é‡æ–°è®­ç»ƒè°ƒæ•´åçš„æ¨¡å‹å¯èƒ½çœ‹èµ·æ¥è¦åšå¾ˆå¤šå·¥ä½œï¼

+   ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨ scikit-learn ä¸­çš„ Pipeline å¯¹è±¡ã€‚

è¿™é‡Œæ˜¯å…³äºç®¡é“çš„ä¸€äº›äº®ç‚¹ã€‚

### æœºå™¨å­¦ä¹ å»ºæ¨¡ç®¡é“åŸºç¡€

æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹å¯èƒ½å¾ˆå¤æ‚ï¼Œæœ‰å„ç§æ­¥éª¤ï¼š

+   æ•°æ®å‡†å¤‡ã€ç‰¹å¾å·¥ç¨‹è½¬æ¢

+   æ¨¡å‹å‚æ•°æ‹Ÿåˆ

+   æ¨¡å‹è¶…å‚æ•°è°ƒæ•´

+   å»ºæ¨¡æ–¹æ³•é€‰æ‹©

+   åœ¨å¤§é‡è¶…å‚æ•°ç»„åˆä¸­è¿›è¡Œæœç´¢

+   è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹è¿è¡Œ

ç®¡é“ï¼ˆPipelinesï¼‰æ˜¯ scikit-learn ä¸­çš„ä¸€ä¸ªç±»ï¼Œå…è®¸å°è£…ä¸€ç³»åˆ—æ•°æ®å‡†å¤‡å’Œå»ºæ¨¡æ­¥éª¤

+   ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†ç®¡é“è§†ä¸ºæˆ‘ä»¬å¤§å¤§ç®€åŒ–çš„å·¥ä½œæµç¨‹ä¸­çš„ä¸€ä¸ªå¯¹è±¡

ç®¡é“ç±»å…è®¸æˆ‘ä»¬ï¼š

+   æé«˜ä»£ç å¯è¯»æ€§å¹¶ä¿æŒä¸€åˆ‡äº•ç„¶æœ‰åº

+   é¿å…å¸¸è§çš„æµç¨‹é—®é¢˜ï¼Œå¦‚æ•°æ®æ³„éœ²ã€æµ‹è¯•æ•°æ®å½±å“æ¨¡å‹å‚æ•°è®­ç»ƒ

+   æŠ½è±¡é€šç”¨æœºå™¨å­¦ä¹ å»ºæ¨¡ï¼Œå¹¶ä¸“æ³¨äºæ„å»ºå°½å¯èƒ½å¥½çš„æ¨¡å‹

åŸºæœ¬å“²å­¦æ˜¯å°†æœºå™¨å­¦ä¹ è§†ä¸ºç»„åˆæœç´¢ï¼Œä»¥è‡ªåŠ¨åŒ–æœ€ä½³æ¨¡å‹çš„ç¡®å®šï¼ˆAutoMLï¼‰

### ä½¿ç”¨ç®¡é“çš„ k-æœ€è¿‘é‚»

è¿™é‡Œæ˜¯ä½¿ç”¨ scikit-learn ç®¡é“è¿›è¡Œæ•´ä¸ªæ¨¡å‹è®­ç»ƒå’Œè°ƒæ•´è¿‡ç¨‹çš„ç´§å‡‘ã€å®‰å…¨ä»£ç 

+   GridSearchCV å¯¹è±¡å®é™…ä¸Šå˜æˆäº†é¢„æµ‹æ¨¡å‹ï¼Œè°ƒæ•´åçš„è¶…å‚æ•°åœ¨æ‰€æœ‰æ•°æ®ä¸Šé‡æ–°è®­ç»ƒï¼

```py
import os                                                     # to set current working directory 

folds = 4                                                   # number of k folds
k_min = 1; k_max = 150                                       # range of k hyperparameter to consider

X_pipe = df.loc[:,sel_pred]                                 # all the samples for the original features
y_pipe = df.loc[:,resp[0]]                             # warning this becomes a series, 1D ndarray with label

pipe = Pipeline([                                           # the machine learning workflow as a pipeline object
    ('scaler', StandardScaler()),
    ('knear', KNeighborsRegressor())
])

params = {                                                  # the machine learning workflow method's parameters
    'scaler': [StandardScaler()],
    'knear__n_neighbors': np.arange(k_min,k_max,1,dtype = int),
    'knear__metric': ['euclidean'],
    'knear__p': [2],
    'knear__weights': ['distance']
}

grid_cv_tuned = GridSearchCV(pipe, params, scoring = 'neg_mean_squared_error', # grid search cross validation 
                             cv=KFold(n_splits=folds,shuffle=False),
                             refit = True)
grid_cv_tuned.fit(X_pipe,y_pipe)                                      # fit model with tuned hyperparameters

plt.subplot(121)
visualize_tuned_model(grid_cv_tuned.best_params_['knear__n_neighbors'], # visualize the error vs. k 
                      grid_cv_tuned.cv_results_['param_knear__n_neighbors'],
                      abs(grid_cv_tuned.cv_results_['mean_test_score']))              

plt.subplot(122)                                            # visualize the tuned model
visualize_model(grid_cv_tuned,X[sel_pred[0]],Xmin[0],Xmax[0],X[sel_pred[1]],Xmin[1],Xmax[1],df[resp[0]],ymin,ymax,
                'All Data and Tuned and Retrained k-Nearest Neighbours')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/4cddeaa35cf9ea49e9a36b802755dc3a.png)

### æ£€æŸ¥è°ƒæ•´åçš„è¶…å‚æ•°

åœ¨ GridSearchCV æ¨¡å‹å¯¹è±¡ä¸­ï¼Œæœ‰ä¸€ä¸ªå†…ç½®çš„å­—å…¸ç§°ä¸º best_params_ï¼Œå®ƒåŒ…å«æ‰€æœ‰è°ƒæ•´è¿‡çš„è¶…å‚æ•°ã€‚

+   æ³¨æ„ï¼Œåœ¨ k çš„èŒƒå›´ä¸­é€‰æ‹©çš„ kï¼Œn_neighborsã€‚

+   æ­¤å¤–ï¼Œè¿˜æŒ‡å®šäº†å…¶ä»–è¶…å‚æ•°ï¼Œä½†æˆ‘ä»¬æœ¬å¯ä»¥æä¾›æ¯ä¸ªè¶…å‚æ•°çš„èŒƒå›´å’Œåœºæ™¯ï¼Œä»¥ä¾¿ä½¿ç”¨ç½‘æ ¼æœç´¢æ–¹æ³•è¿›è¡Œæ¢ç´¢ã€‚

å½“è°ƒæ•´è¶…è¿‡ 1 ä¸ªè¶…å‚æ•°æ—¶ï¼Œè¿è¡Œæ—¶é—´å°†éšç€è¶…å‚æ•°çš„ç»„åˆä»¥åŠç»“æœæ¨¡å‹æŸå¤±å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œcv_results_['mean_test_score']ï¼‰çš„å¢åŠ è€Œå¢åŠ ï¼Œè¯¥å‡½æ•°å¯¹æ‰€æœ‰è¶…å‚æ•°æ¡ˆä¾‹è¿›è¡Œæ’åºã€‚

```py
grid_cv_tuned.best_params_ 
```

```py
{'knear__metric': 'euclidean',
 'knear__n_neighbors': 11,
 'knear__p': 2,
 'knear__weights': 'distance',
 'scaler': StandardScaler()} 
```

ä¹Ÿæœ‰å¿…è¦æŸ¥çœ‹æ•´ä¸ªæ¨¡å‹å¯¹è±¡ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚åŒ…æ‹¬ï¼š

+   è¶…å‚æ•°è°ƒæ•´è€ƒè™‘çš„æ‰€æœ‰ç®¡é“å’Œæ¡ˆä¾‹ã€‚

```py
grid_cv_tuned 
```

```py
GridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=False),
             estimator=Pipeline(steps=[('scaler', StandardScaler()),
                                       ('knear', KNeighborsRegressor())]),
             param_grid={'knear__metric': ['euclidean'],
                         'knear__n_neighbors': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,
        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,
        40,  41,  42,  43,  44,  45,...
        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,
        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,
       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,
       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,
       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,
       144, 145, 146, 147, 148, 149]),
                         'knear__p': [2], 'knear__weights': ['distance'],
                         'scaler': [StandardScaler()]},
             scoring='neg_mean_squared_error')
```

**åœ¨ Jupyter ç¯å¢ƒä¸­ï¼Œè¯·é‡æ–°è¿è¡Œæ­¤å•å…ƒä»¥æ˜¾ç¤º HTML è¡¨ç¤ºæˆ–ä¿¡ä»»ç¬”è®°æœ¬ã€‚**

åœ¨ GitHub ä¸Šï¼ŒHTML è¡¨ç¤ºæ— æ³•æ¸²æŸ“ï¼Œè¯·å°è¯•ä½¿ç”¨ nbviewer.org åŠ è½½æ­¤é¡µé¢ã€‚**

```py
GridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=False),
             estimator=Pipeline(steps=[('scaler', StandardScaler()),
                                       ('knear', KNeighborsRegressor())]),
             param_grid={'knear__metric': ['euclidean'],
                         'knear__n_neighbors': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,
        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,
        40,  41,  42,  43,  44,  45,...
        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,
        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,
       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,
       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,
       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,
       144, 145, 146, 147, 148, 149]),
                         'knear__p': [2], 'knear__weights': ['distance'],
                         'scaler': [StandardScaler()]},
             scoring='neg_mean_squared_error')
```

```py
Pipeline(steps=[('scaler', StandardScaler()), ('knear', KNeighborsRegressor())])
```

```py
StandardScaler()
```

```py
KNeighborsRegressor()
```

### æœºå™¨å­¦ä¹ å»ºæ¨¡ç®¡é“åŸºç¡€ã€‚

æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹å¯èƒ½å¾ˆå¤æ‚ï¼ŒåŒ…å«å„ç§æ­¥éª¤ï¼š

+   æ•°æ®å‡†å¤‡ã€ç‰¹å¾å·¥ç¨‹è½¬æ¢ã€‚

+   æ¨¡å‹å‚æ•°æ‹Ÿåˆã€‚

+   æ¨¡å‹è¶…å‚æ•°è°ƒæ•´ã€‚

+   å»ºæ¨¡æ–¹æ³•é€‰æ‹©ã€‚

+   åœ¨å¤§é‡è¶…å‚æ•°ç»„åˆä¸­è¿›è¡Œæœç´¢ã€‚

+   è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹è¿è¡Œã€‚

ç®¡é“æ˜¯ scikit-learn ä¸­çš„ä¸€ä¸ªç±»ï¼Œå…è®¸å°è£…ä¸€ç³»åˆ—æ•°æ®å‡†å¤‡å’Œå»ºæ¨¡æ­¥éª¤ã€‚

+   ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†ç®¡é“è§†ä¸ºæˆ‘ä»¬é«˜åº¦ç²¾ç®€çš„å·¥ä½œæµç¨‹ä¸­çš„ä¸€ä¸ªå¯¹è±¡ã€‚

ç®¡é“ç±»å…è®¸æˆ‘ä»¬ï¼š

+   æé«˜ä»£ç å¯è¯»æ€§å¹¶ä¿æŒä¸€åˆ‡æ¸…æ™°ã€‚

+   é¿å…å¸¸è§çš„æµç¨‹é—®é¢˜ï¼Œå¦‚æ•°æ®æ³„éœ²ã€æµ‹è¯•æ•°æ®å½±å“æ¨¡å‹å‚æ•°è®­ç»ƒã€‚

+   æŠ½è±¡é€šç”¨æœºå™¨å­¦ä¹ å»ºæ¨¡ï¼Œä¸“æ³¨äºæ„å»ºå°½å¯èƒ½å¥½çš„æ¨¡å‹ã€‚

åŸºæœ¬å“²å­¦æ˜¯å°†æœºå™¨å­¦ä¹ è§†ä¸ºç»„åˆæœç´¢ï¼Œä»¥è‡ªåŠ¨åŒ–æœ€ä½³æ¨¡å‹çš„ç¡®å®šï¼ˆAutoMLï¼‰ã€‚

### ä½¿ç”¨ç®¡é“çš„ k-æœ€è¿‘é‚»ã€‚

è¿™é‡Œæ˜¯ä½¿ç”¨ scikit-learn ç®¡é“è¿›è¡Œæ•´ä¸ªæ¨¡å‹è®­ç»ƒå’Œè°ƒæ•´è¿‡ç¨‹çš„ç´§å‡‘ã€å®‰å…¨ä»£ç ã€‚

+   GridSearchCV å¯¹è±¡å®é™…ä¸Šæˆä¸ºé¢„æµ‹æ¨¡å‹ï¼Œè°ƒæ•´è¿‡çš„è¶…å‚æ•°åœ¨æ‰€æœ‰æ•°æ®ä¸Šé‡æ–°è®­ç»ƒï¼

```py
import os                                                     # to set current working directory 

folds = 4                                                   # number of k folds
k_min = 1; k_max = 150                                       # range of k hyperparameter to consider

X_pipe = df.loc[:,sel_pred]                                 # all the samples for the original features
y_pipe = df.loc[:,resp[0]]                             # warning this becomes a series, 1D ndarray with label

pipe = Pipeline([                                           # the machine learning workflow as a pipeline object
    ('scaler', StandardScaler()),
    ('knear', KNeighborsRegressor())
])

params = {                                                  # the machine learning workflow method's parameters
    'scaler': [StandardScaler()],
    'knear__n_neighbors': np.arange(k_min,k_max,1,dtype = int),
    'knear__metric': ['euclidean'],
    'knear__p': [2],
    'knear__weights': ['distance']
}

grid_cv_tuned = GridSearchCV(pipe, params, scoring = 'neg_mean_squared_error', # grid search cross validation 
                             cv=KFold(n_splits=folds,shuffle=False),
                             refit = True)
grid_cv_tuned.fit(X_pipe,y_pipe)                                      # fit model with tuned hyperparameters

plt.subplot(121)
visualize_tuned_model(grid_cv_tuned.best_params_['knear__n_neighbors'], # visualize the error vs. k 
                      grid_cv_tuned.cv_results_['param_knear__n_neighbors'],
                      abs(grid_cv_tuned.cv_results_['mean_test_score']))              

plt.subplot(122)                                            # visualize the tuned model
visualize_model(grid_cv_tuned,X[sel_pred[0]],Xmin[0],Xmax[0],X[sel_pred[1]],Xmin[1],Xmax[1],df[resp[0]],ymin,ymax,
                'All Data and Tuned and Retrained k-Nearest Neighbours')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/4cddeaa35cf9ea49e9a36b802755dc3a.png)

### æ£€æŸ¥è°ƒæ•´è¿‡çš„è¶…å‚æ•°ã€‚

åœ¨ GridSearchCV æ¨¡å‹å¯¹è±¡ä¸­ï¼Œæœ‰ä¸€ä¸ªå†…ç½®çš„å­—å…¸ç§°ä¸º best_params_ï¼Œå®ƒåŒ…å«æ‰€æœ‰è°ƒæ•´è¿‡çš„è¶…å‚æ•°ã€‚

+   æ³¨æ„ï¼Œåœ¨ k çš„èŒƒå›´ä¸­é€‰æ‹©çš„ kï¼Œn_neighborsã€‚

+   æ­¤å¤–ï¼Œè¿˜æŒ‡å®šäº†å…¶ä»–è¶…å‚æ•°ï¼Œä½†æˆ‘ä»¬æœ¬å¯ä»¥æä¾›æ¯ä¸ªè¶…å‚æ•°çš„èŒƒå›´å’Œåœºæ™¯ï¼Œä»¥ä¾¿ä½¿ç”¨ç½‘æ ¼æœç´¢æ–¹æ³•è¿›è¡Œæ¢ç´¢ã€‚

å½“è°ƒæ•´è¶…è¿‡ 1 ä¸ªè¶…å‚æ•°æ—¶ï¼Œè¿è¡Œæ—¶é—´å°†éšç€è¶…å‚æ•°çš„ç»„åˆä»¥åŠç»“æœæ¨¡å‹æŸå¤±å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œcv_results_['mean_test_score']ï¼‰çš„å¢åŠ è€Œå¢åŠ ï¼Œè¯¥å‡½æ•°å¯¹æ‰€æœ‰è¶…å‚æ•°æ¡ˆä¾‹è¿›è¡Œæ’åºã€‚

```py
grid_cv_tuned.best_params_ 
```

```py
{'knear__metric': 'euclidean',
 'knear__n_neighbors': 11,
 'knear__p': 2,
 'knear__weights': 'distance',
 'scaler': StandardScaler()} 
```

ä¹Ÿæœ‰å¿…è¦æŸ¥çœ‹æ•´ä¸ªæ¨¡å‹å¯¹è±¡ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚åŒ…æ‹¬ï¼š

+   è¶…å‚æ•°è°ƒæ•´è€ƒè™‘çš„æ‰€æœ‰ç®¡é“å’Œæ¡ˆä¾‹ã€‚

```py
grid_cv_tuned 
```

```py
GridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=False),
             estimator=Pipeline(steps=[('scaler', StandardScaler()),
                                       ('knear', KNeighborsRegressor())]),
             param_grid={'knear__metric': ['euclidean'],
                         'knear__n_neighbors': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,
        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,
        40,  41,  42,  43,  44,  45,...
        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,
        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,
       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,
       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,
       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,
       144, 145, 146, 147, 148, 149]),
                         'knear__p': [2], 'knear__weights': ['distance'],
                         'scaler': [StandardScaler()]},
             scoring='neg_mean_squared_error')
```

**åœ¨ Jupyter ç¯å¢ƒä¸­ï¼Œè¯·é‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼ä»¥æ˜¾ç¤º HTML è¡¨ç¤ºå½¢å¼æˆ–ä¿¡ä»»ç¬”è®°æœ¬ã€‚**

åœ¨ GitHub ä¸Šï¼ŒHTML è¡¨ç¤ºå½¢å¼æ— æ³•æ¸²æŸ“ï¼Œè¯·å°è¯•ä½¿ç”¨ nbviewer.org åŠ è½½æ­¤é¡µé¢ã€‚**

```py
GridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=False),
             estimator=Pipeline(steps=[('scaler', StandardScaler()),
                                       ('knear', KNeighborsRegressor())]),
             param_grid={'knear__metric': ['euclidean'],
                         'knear__n_neighbors': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,
        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,
        40,  41,  42,  43,  44,  45,...
        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,
        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,
       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,
       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,
       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,
       144, 145, 146, 147, 148, 149]),
                         'knear__p': [2], 'knear__weights': ['distance'],
                         'scaler': [StandardScaler()]},
             scoring='neg_mean_squared_error')
```

```py
Pipeline(steps=[('scaler', StandardScaler()), ('knear', KNeighborsRegressor())])
```

```py
StandardScaler()
```

```py
KNeighborsRegressor()
```

## è¯„è®º

è¿™æ˜¯å¯¹ k è¿‘é‚»ç®—æ³•çš„åŸºæœ¬ä»‹ç»ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´ YouTube è®²åº§ä¸­çš„èµ„æºé“¾æ¥ï¼Œè§†é¢‘æè¿°ä¸­åŒ…å«èµ„æºé“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![å›¾ç‰‡](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡çš„ 40 è‹±äº©æ ¡å›­å†…ï¼Œè¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆçš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œä»–åœ¨è¯¥æ ¡ç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜ï¼Œ

+   èƒ½æºåˆ†ææ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å¸ˆã€‚

+   [ã€Šè®¡ç®—æœºä¸åœ°çƒç§‘å­¦ã€‹](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼šçš„[ã€Šæ•°å­¦åœ°çƒç§‘å­¦ã€‹](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²æ’°å†™è¶…è¿‡ 70 ç¯‡[åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[ã€Šåœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡ã€‹](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[ã€ŠPython ä¸­çš„åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPy å®æˆ˜æŒ‡å—ã€‹](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[ã€ŠPython ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå®æˆ˜æŒ‡å—ä¸ä»£ç ã€‹](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å« 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†å·¥ä½œæµç¨‹é“¾æ¥ï¼Œè¿™äº›å­˜å‚¨åº“ä½äºä»–çš„[GitHub è´¦æˆ·](https://github.com/GeostatsGuy)ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œä¸“ä¸šäººå£«ï¼Œæä¾›å¸¸é’å†…å®¹ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³è¦ä¸€èµ·å·¥ä½œå—ï¼Ÿ

å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æä»¥åŠæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚ä¸ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡ä»¥åŠ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æƒ³è¦åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼Œæ³¨å†Œå·¥ç¨‹å¸ˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢æ•™æˆ

## æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥è·å–ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°è´¨ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
