# è´å¶æ–¯çº¿æ€§å›å½’

> åŸæ–‡ï¼š[`geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_Bayesian_linear_regression.html`](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_Bayesian_linear_regression.html)

Michael J. Pyrczï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

ç”µå­ä¹¦â€œã€ŠPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œã€‹â€çš„ä¸€ç« ã€‚

Cite this e-Book as:

Pyrcz, M.J., 2024, *ã€ŠPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œã€‹* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138 ![DOI](https://doi.org/10.5281/zenodo.15169138)

æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠæ›´å¤šå†…å®¹å‡å¯åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š

Cite the MachineLearningDemos GitHub Repository as:

Pyrcz, M.J., 2024, *ã€ŠMachineLearningDemos: Python æœºå™¨å­¦ä¹ æ¼”ç¤ºå·¥ä½œæµç¨‹å­˜å‚¨åº“ã€‹* (0.0.3) [è½¯ä»¶]. Zenodo. DOI: 10.5281/zenodo.13835312\. GitHub ä»“åº“ï¼š[GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos) ![DOI](https://zenodo.org/doi/10.5281/zenodo.13835312)

ä½œè€…ï¼šMichael J. Pyrcz

Â© ç‰ˆæƒæ‰€æœ‰ 2024ã€‚

æœ¬ç« æ˜¯å…³äº/æ¼”ç¤º**è´å¶æ–¯çº¿æ€§å›å½’**çš„æ•™ç¨‹ã€‚

**YouTube è®²åº§**ï¼šæŸ¥çœ‹ä»¥ä¸‹è®²åº§ï¼š

+   [æœºå™¨å­¦ä¹ ç®€ä»‹](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)

+   [çº¿æ€§å›å½’](https://youtu.be/0fzbyhWiP84)

+   [å²­å›å½’](https://youtu.be/pMGO40yXZ5Y?si=ygJAheyX-v2BmSiR)

+   [è´å¶æ–¯æ¦‚ç‡](https://www.youtube.com/watch?v=Ppwfr8H177M&list=PLG19vXLQHvSB-D4XKYieEku9GQMQyAzjJ&index=6)

+   [è´å¶æ–¯çº¿æ€§å›å½’](https://youtu.be/LzZ5b3wdZQk?si=DkhYrgmDXzrPFQyr)

+   [æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨](https://youtu.be/BDvyLrH3cLI?si=D6boOpoVpyo-6TqK)

è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘ YouTube ä¸Šçš„[æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«æœ‰è‰¯å¥½æ–‡æ¡£è®°å½•çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºè·å–ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœä½ æƒ³çŸ¥é“æˆ‘çš„åŠ¨æœºï¼Œè¯·æŸ¥çœ‹[Michael çš„æ•…äº‹](https://michaelpyrcz.com/my-story)ã€‚

## è´å¶æ–¯çº¿æ€§å›å½’çš„åŠ¨æœº

è´å¶æ–¯æœºå™¨å­¦ä¹ æ–¹æ³•åº”ç”¨æ¦‚ç‡æ¥é¢„æµ‹å…·æœ‰å†…åœ¨ä¸ç¡®å®šæ€§æ¨¡å‹çš„é¢„æµ‹ã€‚æ­¤å¤–ï¼Œè´å¶æ–¯æ–¹æ³•æ•´åˆäº†è´å¶æ–¯æ›´æ–°çš„æ¦‚å¿µï¼Œå³ä½¿ç”¨æ•°æ®ä¸­çš„ä¼¼ç„¶æ¨¡å‹æ›´æ–°å…ˆéªŒæ¨¡å‹æ¥è®¡ç®—åéªŒæ¨¡å‹ã€‚

ç”±äºä½¿ç”¨è¿™äº›æ¦‚ç‡ï¼Œæ¨¡å‹è®­ç»ƒå­˜åœ¨é¢å¤–çš„å¤æ‚æ€§ï¼Œè¿™äº›å¤æ‚æ€§ç”±è¿ç»­æ¦‚ç‡å¯†åº¦å‡½æ•°è¡¨ç¤ºã€‚ç”±äºç”±æ­¤äº§ç”Ÿçš„é«˜å¤æ‚æ€§ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥æ±‚è§£æ¨¡å‹å‚æ•°ï¼Œè€Œå¿…é¡»åº”ç”¨å¦‚é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›ï¼ˆMCMCï¼‰æ¨¡æ‹Ÿè¿™æ ·çš„é‡‡æ ·æ–¹æ³•ã€‚

è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ MCMC Metropolis-Hastings é‡‡æ ·è¿›è¡Œè´å¶æ–¯çº¿æ€§å›å½’çš„ç®€å•ã€æ˜“äºç†è§£çš„æ¼”ç¤ºã€‚æˆ‘çš„å­¦ç”Ÿåœ¨è¿™æ–¹é¢çš„æ¦‚å¿µä¸Šé‡åˆ°äº†å›°éš¾ï¼Œæ‰€ä»¥æˆ‘æŒ‘æˆ˜è‡ªå·±ä»å¤´å¼€å§‹æ„å»ºä¸€ä¸ªå·¥ä½œæµç¨‹ï¼Œå¹¶æ¸…æ¥šåœ°è§£é‡Šæ‰€æœ‰ç»†èŠ‚ã€‚è®©æˆ‘ä»¬ä»è´å¶æ–¯æ›´æ–°å¼€å§‹è§£é‡Šå…³é”®çš„å‰ææ¡ä»¶ã€‚

## çº¿æ€§å›å½’

è®©æˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹ç”¨äºé¢„æµ‹çš„çº¿æ€§å›å½’çš„æ˜¾è‘—è¦ç‚¹ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹ä¸€ç»„æ•°æ®æ‹Ÿåˆçš„çº¿æ€§æ¨¡å‹ã€‚

![å›¾ç‰‡](img/806bf5f702f9bb5a63e30d6e1f7969d9.png)

ç¤ºä¾‹çº¿æ€§å›å½’æ¨¡å‹ã€‚

**å‚æ•°æ¨¡å‹**

è¿™æ˜¯ä¸€ä¸ªå‚æ•°é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬æ¥å—ä¸€ä¸ªå…ˆéªŒå‡è®¾çš„çº¿æ€§ï¼Œç„¶åè·å¾—ä¸€ä¸ªéå¸¸ä½çš„å‚æ•°è¡¨ç¤ºï¼Œè¿™ä½¿å¾—åœ¨æ²¡æœ‰å¤§é‡æ•°æ®çš„æƒ…å†µä¸‹æ˜“äºè®­ç»ƒã€‚

+   æ‹Ÿåˆæ¨¡å‹æ˜¯ä¸€ä¸ªåŸºäºæ‰€æœ‰å¯ç”¨ç‰¹å¾ $x_1,\ldots,x_m$ çš„ç®€å•åŠ æƒçº¿æ€§åŠ æ€§æ¨¡å‹ã€‚

+   å‚æ•°æ¨¡å‹çš„å½¢å¼å¦‚ä¸‹ï¼š

$$ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 $$

è¿™æ˜¯çº¿æ€§æ¨¡å‹å‚æ•°çš„å¯è§†åŒ–ï¼Œ

![å›¾ç‰‡](img/ada2fcc2740c48478e79404563c91061.png)

çº¿æ€§æ¨¡å‹å‚æ•°ã€‚

**æœ€å°äºŒä¹˜æ³•**

å¯¹äº L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼Œæ¨¡å‹å‚æ•° $b_1,\ldots,b_m,b_0$ çš„è§£æè§£æ˜¯å¯ç”¨çš„ï¼Œè¯¯å·®æ˜¯æ±‚å’Œå¹¶å¹³æ–¹çš„ï¼Œè‡³å°‘æ˜¯å¹³æ–¹çš„ã€‚

+   æˆ‘ä»¬æœ€å°åŒ–è®­ç»ƒæ•°æ®ä¸Šçš„è¯¯å·®ï¼Œæ®‹å·®å¹³æ–¹å’Œï¼ˆRSSï¼‰ï¼š

$$ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)Â² $$

å…¶ä¸­ $y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾å€¼ï¼Œè€Œ $\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ æ˜¯æ¨¡å‹é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹æ˜¯åœ¨ $\alpha = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œçš„ã€‚

è¿™é‡Œæ˜¯ L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰çš„å¯è§†åŒ–ã€‚

![å›¾ç‰‡](img/835541b16e1038a4606f7d97b628c4f9.png)

çº¿æ€§æ¨¡å‹æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®ã€‚

+   è¿™å¯ä»¥ç®€åŒ–ä¸ºè®­ç»ƒæ•°æ®ä¸Šçš„å¹³æ–¹è¯¯å·®ä¹‹å’Œï¼Œ

$$ \sum_{i=1}^n (\Delta y_i)Â² $$

å…¶ä¸­ $\Delta y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾è§‚å¯Ÿ $y_i$ å‡å»æ¨¡å‹é¢„æµ‹ $\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ï¼Œåœ¨ $i = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šã€‚

**å‡è®¾**

æˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹æœ‰ä¸€äº›é‡è¦çš„å‡è®¾ï¼Œ

+   **æ— è¯¯å·®** - é¢„æµ‹å˜é‡æ— è¯¯å·®ï¼Œä¸æ˜¯éšæœºå˜é‡

+   **çº¿æ€§** - å“åº”æ˜¯ç‰¹å¾ï¼ˆä»¬ï¼‰çš„çº¿æ€§ç»„åˆ

+   **å¸¸æ•°æ–¹å·®** - å“åº”ä¸­çš„è¯¯å·®åœ¨é¢„æµ‹å€¼ä¸Šä¿æŒæ’å®š

+   **è¯¯å·®ç‹¬ç«‹æ€§** - å“åº”ä¸­çš„è¯¯å·®å½¼æ­¤ä¸ç›¸å…³

+   **æ— å¤šé‡å…±çº¿æ€§** - æ²¡æœ‰ç‰¹å¾ä¸å…¶ä»–ç‰¹å¾å†—ä½™

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å›é¡¾è´å¶æ–¯æ›´æ–°ï¼Œç„¶åæ›´æ–°æˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹åˆ°è´å¶æ–¯çº¿æ€§å›å½’ã€‚

## è´å¶æ–¯æ›´æ–°

è´å¶æ–¯æ¦‚ç‡æ–¹æ³•åŸºäºå¯¹äº‹ä»¶ä¿¡å¿µï¼ˆä¸“å®¶ç»éªŒï¼‰çš„æŸç§ç¨‹åº¦ï¼Œéšç€æ–°ä¿¡æ¯çš„å‡ºç°è€Œæ›´æ–°

+   è¿™ç§æ¦‚ç‡æ–¹æ³•éå¸¸å¼ºå¤§ï¼Œå¯ä»¥åº”ç”¨äºè§£å†³æˆ‘ä»¬æ— æ³•ç”¨é¢‘ç‡ä¸»ä¹‰æ¦‚ç‡æ–¹æ³•è§£å†³çš„é—®é¢˜

è´å¶æ–¯æ›´æ–°ç”±è´å¶æ–¯å®šç†è¡¨ç¤ºï¼Œ

$$ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} $$

å…¶ä¸­ $P(A)$ æ˜¯å…ˆéªŒï¼Œ$P(B|A)$ æ˜¯ä¼¼ç„¶ï¼Œ$P(B)$ æ˜¯è´Ÿè´£æ¦‚ç‡å°é—­çš„è¯æ®é¡¹ï¼Œ$P(A|B)$ æ˜¯åéªŒã€‚

## è´å¶æ–¯çº¿æ€§å›å½’

çº¿æ€§å›å½’æ¨¡å‹çš„é¢‘ç‡ä¸»ä¹‰å…¬å¼æ˜¯ï¼Œ

$$ y = b_1 \times x + b_0 + \sigma $$

å…¶ä¸­ $x$ æ˜¯é¢„æµ‹ç‰¹å¾ï¼Œ$b_1$ æ˜¯æ–œç‡å‚æ•°ï¼Œ$b_0$ æ˜¯æˆªè·å‚æ•°ï¼Œ$\sigma$ æ˜¯è¯¯å·®æˆ–å™ªå£°ã€‚å­˜åœ¨ä¸€ä¸ªè§£æå½¢å¼ï¼Œç”¨äºæ‹Ÿåˆå¯ç”¨æ•°æ®ï¼ŒåŒæ—¶æœ€å°åŒ–æ•°æ®è¯¯å·®å‘é‡çš„ L2 èŒƒæ•°ã€‚

æˆ‘ä»¬å¯ä»¥ç”¨çŸ©é˜µç¬¦å·è¡¨ç¤ºå¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹ï¼Œ

$$ y = \beta^{T} X + \sigma $$

å…¶ä¸­ $\beta$ æ˜¯æ¨¡å‹å‚æ•°çš„å‘é‡ï¼Œ$\beta_1,\ldots,\beta_m$ã€‚æˆ‘ä»¬å¯ä»¥å‘å‘é‡ä¸­æ·»åŠ å¦ä¸€ä¸ªé¡¹ $\beta_0$ ä½œä¸ºæ¨¡å‹çš„æˆªè·ã€‚

+   å½“æ·»åŠ  $\beta_0$ æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿåœ¨æ•°æ® $X$ çŸ©é˜µä¸­æ·»åŠ ä¸€åˆ— $1$s

å¯¹äºçº¿æ€§å›å½’çš„è´å¶æ–¯å…¬å¼ï¼Œæˆ‘ä»¬å°†æ¨¡å‹è®¾å®šä¸ºå“åº”åˆ†å¸ƒ $Y$ çš„é¢„æµ‹ï¼Œç°åœ¨æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼š

$$ Y \sim N(\beta^{T}X, \sigma^{2} I) $$

æˆ‘ä»¬é€šè¿‡è´å¶æ–¯æ›´æ–°æ¥ä¼°è®¡æ¨¡å‹å‚æ•°åˆ†å¸ƒï¼Œä»å…ˆéªŒå’Œè®­ç»ƒæ•°æ®ä¸­çš„ä¼¼ç„¶æ¨æ–­æ¨¡å‹å‚æ•°ã€‚

$$ p(\beta | y, X) = \frac{p(y,X| \beta) p(\beta)}{p(y,X)} $$

è®©æˆ‘ä»¬é€ä¸ªè®¨è®ºæ¯ä¸ªæœ¯è¯­ï¼Œ

1.  å…ˆéªŒï¼Œ$ğ‘ƒ(\beta)$ - åœ¨æ–°æ•°æ®ä¹‹å‰ï¼ŒåŸºäºé¢„æµ‹ç‰¹å¾å¯¹æ¨¡å‹å‚æ•°çš„åˆå§‹æ¨æ–­ï¼Œ

+   ä¸“å®¶çŸ¥è¯†

+   ç®€å•å…ˆéªŒï¼Œåœ¨éä¿¡æ¯æ€§å…ˆéªŒçš„æƒ…å†µä¸‹ï¼Œä¼¼ç„¶å°†å ä¸»å¯¼åœ°ä½

+   ä¿¡æ¯æ€§å…ˆéªŒï¼Œåœ¨ä½ä¸ç¡®å®šæ€§å…ˆéªŒçš„æƒ…å†µä¸‹ï¼Œå…ˆéªŒå°†å ä¸»å¯¼åœ°ä½

+   åœ¨æ•´åˆè®­ç»ƒæ•°æ®ä¹‹å‰ï¼Œä¸èƒ½åŒ…æ‹¬æ–°çš„è®­ç»ƒæ•°æ®

1.  ä¼¼ç„¶ï¼Œ$ğ‘ƒ(ğ‘¦,ğ‘‹â”‚\beta)$ - åœ¨å‡è®¾çš„æ¨¡å‹å‚æ•°åŸºç¡€ä¸Šï¼Œç»™å®šè®­ç»ƒæ•°æ®çš„æ¡ä»¶åˆ†å¸ƒï¼Œ

+   æ–°æ•°æ®ï¼Œæ•°æ®é©±åŠ¨

+   éšç€æ ·æœ¬æ•°æ®æ•°é‡çš„å¢åŠ ï¼Œä¼¼ç„¶å°†è¶…è¿‡å…ˆéªŒåˆ†å¸ƒ

1.  è¯æ®ï¼Œ$P(y,X)$ - å½’ä¸€åŒ–å¸¸æ•°ï¼Œä»¥ç¡®ä¿æ¦‚ç‡å°é—­ï¼Œå³æœ‰æ•ˆçš„åéªŒæ¦‚ç‡å¯†åº¦å‡½æ•°

+   ä¸æ¨¡å‹å‚æ•°ç‹¬ç«‹

+   é€šè¿‡å¯¹åˆ†å­è¿›è¡Œè¾¹ç¼˜åŒ–æ¥è§£å†³ï¼Œé€šè¿‡å¯¹æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹å‚æ•°è¿›è¡Œç§¯åˆ†è€Œç®€åŒ–

+   å¯¹äºæŸäº›è§£å†³æ–¹æ¡ˆæ–¹æ³•ï¼Œä¾‹å¦‚ MCMC Metropolis-Hastingsï¼Œè¯æ®é¡¹ä¼šæŠµæ¶ˆ

1.  åéªŒï¼Œ$P(\betaâ”‚y,X)$ - ç»™å®šæ•°æ®é©±åŠ¨çš„ä¼¼ç„¶å’ŒåŸºäºä¸“å®¶çŸ¥è¯†å’Œä¿¡å¿µçš„å…ˆéªŒçš„æ¨¡å‹å‚æ•°çš„æ¡ä»¶åˆ†å¸ƒ

+   éšç€æ•°æ®æ•°é‡çš„å¢åŠ ï¼Œ$ğ‘› \rightarrow \infty$ï¼Œæ¨¡å‹å‚æ•° $\beta$ ä¼šæ”¶æ•›åˆ°æ™®é€šæœ€å°äºŒä¹˜çº¿æ€§å›å½’ï¼Œå…ˆéªŒè¢«æ•°æ®æ‰€æ·¹æ²¡ã€‚

é€šå¸¸å¯¹äºè¿ç»­ç‰¹å¾ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥è®¡ç®—åéªŒï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨æŠ½æ ·æ–¹æ³•ï¼Œå¦‚é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›ï¼ˆMcMCï¼‰æ¥æŠ½æ ·åéªŒã€‚

è¿™é‡Œæ˜¯ä¸€ä¸ª MCMC Metropolis Hastings å·¥ä½œæµç¨‹å’Œæ›´å¤šç»†èŠ‚ã€‚

## é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›ï¼ˆMCMCï¼‰

æ˜¯ä¸€å¥—ç®—æ³•ï¼Œç”¨äºä»æ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡ŒæŠ½æ ·ï¼Œä½¿å¾—æ ·æœ¬åŒ¹é…åˆ†å¸ƒç»Ÿè®¡ã€‚

+   **é©¬å°”å¯å¤«** - å±è”½å‡è®¾ï¼Œä¸‹ä¸€ä¸ªæ ·æœ¬åªä¾èµ–äºå‰ä¸€ä¸ªæ ·æœ¬

+   **é“¾** - æ ·æœ¬å½¢æˆä¸€ä¸ªåºåˆ—ï¼Œé€šå¸¸è¡¨æ˜ä»çƒ§æ¯é“¾ï¼ˆå…·æœ‰ä¸å‡†ç¡®ç»Ÿè®¡çš„é“¾ï¼‰åˆ°å¹³è¡¡é“¾ï¼ˆå…·æœ‰å‡†ç¡®ç»Ÿè®¡çš„é“¾ï¼‰çš„è½¬å˜

+   **è’™ç‰¹å¡æ´›** - ä½¿ç”¨è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼Œä»ç»Ÿè®¡åˆ†å¸ƒä¸­è¿›è¡ŒéšæœºæŠ½æ ·

è¿™æœ‰ä»€ä¹ˆç”¨ï¼Ÿ

+   æˆ‘ä»¬é€šå¸¸æ²¡æœ‰ç›®æ ‡åˆ†å¸ƒï¼Œå®ƒæ˜¯æœªçŸ¥çš„

+   ä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡å…¶ä»–å½¢å¼çš„ä¿¡æ¯ï¼Œå¦‚æ¡ä»¶æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œä»¥æ­£ç¡®çš„é¢‘ç‡è¿›è¡ŒæŠ½æ ·

## åŸºäº Metropolis-Hastings æŠ½æ ·çš„è´å¶æ–¯çº¿æ€§å›å½’

è¿™é‡Œæ˜¯ Metropolis-Hastings MCMC æŠ½æ ·çš„åŸºæœ¬æ­¥éª¤ï¼š

å¯¹äº $\ell = 1, \ldots, L$:

1.  ä¸ºæ¨¡å‹å‚æ•°çš„åˆå§‹æ ·æœ¬åˆ†é…éšæœºå€¼ï¼Œ$\beta(\ell = 1) = b_1(\ell = 1)$ï¼Œ$b_0(\ell = 1)$ å’Œ $\sigmaÂ²(\ell = 1)$ã€‚

1.  æ ¹æ®å»ºè®®å‡½æ•°æå‡ºæ–°çš„æ¨¡å‹å‚æ•°ï¼Œ$\beta^{\prime} = b_1$ï¼Œ$b_0$ å’Œ $\sigmaÂ²$ã€‚

1.  è®¡ç®—æ–°å»ºè®®çš„æ¥å—æ¦‚ç‡ï¼Œå³æ ¹æ®æ•°æ®ç»™å‡ºçš„æ–°æ¨¡å‹å‚æ•°çš„åéªŒæ¦‚ç‡ä¸æ ¹æ®æ•°æ®ç»™å‡ºçš„å‰ä¸€ä¸ªæ¨¡å‹å‚æ•°çš„æ¦‚ç‡çš„æ¯”å€¼ï¼Œä¹˜ä»¥æ ¹æ®æ–°æ­¥éª¤ç»™å‡ºçš„æ—§æ­¥éª¤çš„æ¦‚ç‡é™¤ä»¥æ ¹æ®æ—§æ­¥éª¤ç»™å‡ºæ–°æ­¥éª¤çš„æ¦‚ç‡ã€‚

$$ P(\beta \rightarrow \beta^{\prime}) = min\left(\frac{p(\beta^{\prime}|y,X) }{ p(\beta | y,X)} \cdot \frac{p(\beta^{\prime}|\beta) }{ p(\beta | \beta^{\prime})},1\right) $$

1.  åº”ç”¨è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿä»¥æ¡ä»¶æ¥å—å»ºè®®ï¼Œå¦‚æœè¢«æ¥å—ï¼Œ$\ell = \ell + 1$ï¼Œå¹¶æŠ½æ · $\beta(\ell) = \beta^{\prime}$

1.  è½¬åˆ°ç¬¬äºŒæ­¥ã€‚

è®©æˆ‘ä»¬è°ˆè°ˆè¿™ä¸ªç³»ç»Ÿã€‚é¦–å…ˆçœ‹å·¦è¾¹ï¼š

$$ \frac{p(\beta^{\prime}|y,X) }{ p(\beta | y,X)} $$

æˆ‘ä»¬æ­£åœ¨è®¡ç®—ç»™å®šæ•°æ®å’Œå…ˆéªŒæ¨¡å‹çš„åéªŒæ¦‚ç‡ï¼ˆä¼¼ç„¶ä¹˜ä»¥å…ˆéªŒï¼‰ä¸å»ºè®®æ ·æœ¬çš„åéªŒæ¦‚ç‡ä¹‹æ¯”ã€‚

+   æ­£å¦‚æ‚¨ä¸‹é¢å°†çœ‹åˆ°çš„ï¼Œè®¡ç®—è¿™ä¸ªæ¯”ç‡æ˜¯éå¸¸å®ç”¨çš„ã€‚

+   å¦‚æœæè®®çš„æ ·æœ¬æ¯”å½“å‰æ ·æœ¬çš„å¯èƒ½æ€§é«˜ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªå¤§äº 1.0 çš„å€¼ï¼Œå®ƒå°†è¢«æˆªæ–­ä¸º 1.0ï¼Œå¹¶ä¸”æˆ‘ä»¬æ¥å—æè®®çš„æ ·æœ¬ã€‚

+   å¦‚æœæè®®çš„æ ·æœ¬æ¯”å½“å‰æ ·æœ¬çš„å¯èƒ½æ€§ä½ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªå°äº 1.0 çš„å€¼ï¼Œç„¶åæˆ‘ä»¬å°†ä½¿ç”¨è’™ç‰¹å¡æ´›æŠ½æ ·ä»¥è¿™ä¸ªæ¥å—æ¦‚ç‡éšæœºé€‰æ‹©æè®®çš„æ ·æœ¬ã€‚

è¿™ä¸ªç¨‹åºå…è®¸æˆ‘ä»¬åœ¨çƒ§æ¯é“¾ä¹‹åï¼Œä»¥å½“å‰é€Ÿç‡éå†æ¨¡å‹å‚æ•°ç©ºé—´å¹¶é‡‡æ ·å‚æ•°ã€‚

ç°åœ¨ï¼Œå…³äºæ–¹ç¨‹çš„è¿™ä¸€éƒ¨åˆ†å‘¢ï¼Ÿ

$$ \frac{p(\beta^{\prime}|\beta) }{ p(\beta | \beta^{\prime})} $$

å¦‚æœæˆ‘ä»¬ä¸ºæ¨¡å‹å‚æ•°ä½¿ç”¨éå¯¹ç§°æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ä¸ªç¨‹åºå°†å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼

+   ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨æ­£åæ–œåˆ†å¸ƒï¼ˆä¾‹å¦‚ï¼Œå¯¹æ•°æ­£æ€åˆ†å¸ƒï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ›´æœ‰å¯èƒ½å› ä¸ºè¿™ç§åˆ†å¸ƒè€Œèµ°å‘æ›´å¤§çš„å€¼ï¼Œè€Œä¸æ˜¯å› ä¸ºå…ˆéªŒæˆ–ä¼¼ç„¶ã€‚

+   è¿™ä¸ªé¡¹æ¶ˆé™¤äº†è¿™ç§åå·®ï¼Œå› æ­¤æˆ‘ä»¬å¾—åˆ°äº†å…¬å¹³çš„æ ·æœ¬ï¼

ä½ å°†åœ¨ä¸‹é¢çœ‹åˆ°ï¼Œæˆ‘ä»¬é€šè¿‡å‡è®¾æ¨¡å‹å‚æ•°åˆ†å¸ƒå¯¹ç§°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°½ç®¡è®¸å¤šäººä½¿ç”¨éå¯¹ç§°çš„ä¼½é©¬åˆ†å¸ƒæ¥è¡¨ç¤º sigmaï¼Œå› ä¸ºå®ƒä¸èƒ½æœ‰è´Ÿå€¼ã€‚

+   æˆ‘çš„ç›®çš„æ˜¯è¿›è¡Œå°½å¯èƒ½ç®€å•çš„æ¼”ç¤ºã€‚

## æˆ‘ä»¬ç®€åŒ–çš„ Metropolis æŠ½æ ·æ¼”ç¤º

è®©æˆ‘ä»¬è¿›ä¸€æ­¥å…·ä½“åŒ–è¿™ä¸ªå·¥ä½œæµç¨‹ï¼Œä»¥ä¾›æˆ‘ä»¬çš„ç®€å•æ¼”ç¤ºä½¿ç”¨ã€‚

+   æˆ‘å‡è®¾æ‰€æœ‰æ¨¡å‹å‚æ•°éƒ½éµå¾ªé«˜æ–¯å¯¹ç§°åˆ†å¸ƒï¼Œå› æ­¤è¿™ä¸ªå…³ç³»å¯¹æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹å‚æ•°ï¼ŒåŒ…æ‹¬å½“å‰çš„å’Œæè®®çš„ï¼Œéƒ½æˆç«‹ã€‚

$$ \frac{p(\beta^{\prime}|\beta) }{ p(\beta | \beta^{\prime})} = 1.0 $$

å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰äº†è¿™ä¸ªç®€åŒ–çš„æè®®æ¥å—æ¦‚ç‡ï¼Œæ³¨æ„è¿™è¢«ç§°ä¸º Metropolis æŠ½æ ·ã€‚

$$ P(\beta \rightarrow \beta^{\prime}) = min \left( \frac{p(\beta^{\prime}|y,X) }{ p(\beta | y,X)},1 \right) $$

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†è´å¶æ–¯å…¬å¼æ›¿æ¢åˆ°æˆ‘ä»¬çš„è´å¶æ–¯çº¿æ€§å›å½’æ¨¡å‹ä¸­ã€‚

$$ p(\beta^{\prime} | y, X) = \frac{p(y,X| \beta^{\prime}) p(\beta^{\prime})}{p(y,X)} \quad \text{ and } \quad p(\beta | y, X) = \frac{p(y,X| \beta) p(\beta)}{p(y,X)} $$

å¦‚æœæˆ‘ä»¬å°†è¿™äº›å€¼ä»£å…¥æˆ‘ä»¬ä¸Šé¢çš„æ¥å—æ¦‚ç‡ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°è¿™ä¸ªç»“æœã€‚

$$ P(\beta \rightarrow \beta^{\prime}) = min \left( \frac{p(\beta^{\prime}|y,X) }{ p(\beta | y,X)},1 \right) = min \left( \frac{ \left( \frac{p(y,X| \beta_{new}) p(\beta_{new}) } {p(y,X)} \right) }{ \left( \frac{ p(y,X| \beta) p(\beta)}{p(y,X)} \right) },1 \right) $$

æ³¨æ„ï¼Œè¯æ®é¡¹è¢«æŠµæ¶ˆäº†ã€‚

$$ P(\beta \rightarrow \beta^{\prime}) = min \left( \frac{ p(y,X| \beta_{new}) p(\beta_{new}) }{ p(y,X| \beta) p(\beta)},1 \right) $$

ç”±äºæˆ‘ä»¬å¤„ç†çš„æ˜¯ä¼¼ç„¶æ¯”ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¯†åº¦è€Œä¸æ˜¯æ¦‚ç‡ã€‚

$$ P(\beta \rightarrow \beta^{\prime}) = min \left( \frac{ f(y,X| \beta_{new}) f(\beta_{new}) }{ f(y,X| \beta) f(\beta) } ,1 \right) $$

æœ€åï¼Œä¸ºäº†æé«˜è§£çš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—è‡ªç„¶å¯¹æ•°æ¯”ï¼š

$$ ln(P(\beta \rightarrow \beta^{\prime})) = min \left( ln \left[ \frac{ f(y,X| \beta_{new}) f(\beta_{new}) }{ f(y,X| \beta) f(\beta) } \right],0 \right) $$$$ = min \left( \left[ln(f(y,X| \beta_{new})) + ln(f(\beta_{new})) \right] - \left[ ln(f(y,X| \beta)) + ln(f(\beta)) \right],0 \right) $$

æˆ‘ä»¬è®¡ç®—æè®®æ¥å—çš„æ¦‚ç‡ï¼Œå³ä¸Šè¿°å…¬å¼çš„æŒ‡æ•°åŒ–ã€‚

æˆ‘ä»¬å¦‚ä½•è®¡ç®—ä¼¼ç„¶å¯†åº¦ï¼Ÿå¦‚æœæˆ‘ä»¬å‡è®¾æ‰€æœ‰æ•°æ®ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œæˆ‘ä»¬å¯ä»¥å–æ‰€æœ‰å“åº”å€¼ï¼ˆå¯†åº¦ï¼‰åœ¨é¢„æµ‹å€¼å’Œæ¨¡å‹å‚æ•°æ ·æœ¬ä¸‹çš„ä¹˜ç§¯å’Œï¼ç»™å®šå“åº”ç‰¹å¾çš„é«˜æ–¯å‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥ä»é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°è®¡ç®—æ¯ä¸ªæ•°æ®çš„å¯†åº¦ã€‚

$$ f_{y,X | \beta}(y) \sim N [ b_1 \cdot X + b_0, \sigma ] $$

åœ¨ç‹¬ç«‹æ€§çš„å‡è®¾ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ‰€æœ‰è®­ç»ƒæ•°æ®è¿›è¡Œä¹˜ç§¯å’Œã€‚

$$ f(y,X| \beta) = \prod_{\alpha = 1}^{n} f_{y,X | \beta}(y_{\alpha}) $$

æ³¨æ„ï¼Œæ­¤å·¥ä½œæµç¨‹æ˜¯åœ¨ Fortunato Nucera çš„ Medium æ–‡ç« [ä»å¤´å¼€å§‹æŒæ¡è´å¶æ–¯çº¿æ€§å›å½’ï¼šPython ä¸­çš„ Metropolis-Hastings å®ç°](https://medium.com/@tinonucera/bayesian-linear-regression-from-scratch-a-metropolis-hastings-implementation-63526857f191)çš„å¸®åŠ©ä¸‹å¼€å‘çš„ã€‚æˆ‘å¼ºçƒˆæ¨èè¿™ç¯‡æ˜“äºç†è§£çš„æè¿°å’Œæ¼”ç¤ºã€‚æ„Ÿè°¢ï¼ŒFortunatoã€‚

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = True
import os                                                     # to set current working directory 
import math                                                   # square root operator
import numpy as np                                            # arrays and matrix math
import scipy.stats as stats                                   # statistical methods
import pandas as pd                                           # DataFrames
import pandas.plotting as pd_plot
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.patches import Rectangle                      # build a custom legend
from matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import seaborn as sns                                         # for matrix scatter plots
from sklearn.linear_model import LinearRegression             # frequentist model for comparison
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # suppress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœé‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½éœ€è¦é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„å‡ ä¸ªã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ï¼Œç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£å¯ä»¥æä¾›æ›´å¤šå¸®åŠ©ã€‚

## å£°æ˜å‡½æ•°

ä»¥ä¸‹å‡½æ•°åŒ…æ‹¬ï¼š

+   **ä¸‹ä¸€ä¸ªæè®®** - ä»å…ˆå‰çš„æ¨¡å‹å‚æ•°ä¸­æè®®ä¸‹ä¸€ä¸ªæ¨¡å‹å‚æ•°ï¼Œè¿™æ˜¯æè®®æ–¹æ³•ï¼Œç”±æ­¥é•¿æ ‡å‡†å·®å’Œé«˜æ–¯åˆ†å¸ƒå‚æ•°åŒ–ã€‚

+   **ä¼¼ç„¶å¯†åº¦** - æ ¹æ®æ¨¡å‹å‚æ•°è®¡ç®—æ‰€æœ‰æ•°æ®çš„å¯†åº¦ä¹˜ç§¯ã€‚ç”±äºæˆ‘ä»¬å¤„ç†çš„æ˜¯å¯¹æ•°å¯†åº¦ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œæ±‚å’Œã€‚

+   **å…ˆéªŒå¯†åº¦** - æ ¹æ®å…ˆéªŒæ¨¡å‹è®¡ç®—æ‰€æœ‰æ¨¡å‹å‚æ•°çš„å¯†åº¦ä¹˜ç§¯ã€‚ç”±äºæˆ‘ä»¬å¤„ç†çš„æ˜¯å¯¹æ•°å¯†åº¦ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯¹æ‰€æœ‰æ¨¡å‹å‚æ•°è¿›è¡Œæ±‚å’Œã€‚è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹æ€§çš„å‡è®¾ã€‚

+   **æ·»åŠ ç½‘æ ¼** - æ”¹è¿›çš„ç»˜å›¾ç½‘æ ¼ã€‚

```py
def next_proposal(prev_theta, step_stdev = 0.5):              # assuming a Gaussian distribution centered on previous theta and step stdev 
    out_theta = stats.multivariate_normal(mean=prev_theta,cov=np.eye(3)*step_stdev**2).rvs(1)
    return out_theta

def likelihood_density(x,y,theta):                            # likelihood - probability (density) of the data given the model
    density = np.sum(stats.norm.logpdf(y, loc=theta[0]*x+theta[1],scale=theta[2])) # assume independence, sum is product in log space
    return density

def prior_density(theta,prior):                               # prior - probability (density) of the model parameters given the prior 
    mean = np.array([prior[0,0],prior[1,0],prior[2,0]]); cov = np.zeros([3,3]); cov[0,0] = prior[0,1]; cov[1,1] = prior[1,1]; cov[2,2] = prior[2,1]
    prior_out = stats.multivariate_normal.logpdf(theta,mean=mean,cov=cov,allow_singular=True)
    return prior_out

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œä»¥å…ä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥æ“ä½œï¼ˆæ¯æ¬¡é¿å…åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/PGE383")                                        # set the working directory 
```

## ä½¿ç”¨å·²çŸ¥çš„çº¿æ€§å›å½’æ¨¡å‹å‚æ•°æ„å»ºåˆæˆæ•°æ®é›†

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªç®€å•çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å·²çŸ¥çš„çº¿æ€§å›å½’æ¨¡å‹å‚æ•°ï¼Œ$b_1$æ˜¯æ–œç‡å‚æ•°ï¼Œ$b_0$æ˜¯æˆªè·å‚æ•°ï¼Œ$\sigma$æ˜¯è¯¯å·®æˆ–å™ªå£°ã€‚

+   æˆ‘ä»¬åŸºäºè¿™äº›å‚æ•°æ„å»ºæ•°æ®ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶å±•ç¤ºæˆ‘ä»¬å¾—åˆ°äº†æ­£ç¡®çš„æ–œç‡å’Œæˆªè·æ¨¡å‹å‚æ•°ã€‚

```py
np.random.seed(seed = seed)                                   # set random number seed

data_b1 = 3; data_b0 = 20; data_sigma = 5; n = 100            # set data model parameters

X = np.random.rand(n)*30                                      # random x values
y = data_b1*X+data_b0 + np.random.normal(loc=0.0,scale=data_sigma,size=n) # y as a linear function of x + random noise 

Xname = [r'$X_1$']; yname = [r'$y$']                          # specify the predictor features (x2) and response feature (x1)
Xmin = 0.0; Xmax = 30; ymin = 0.0; ymax = 120                 # set minimums and maximums for visualization 
Xunit = ['none']; yunit = ['none'] 
Xlabelunit = Xname[0] + ' (' + Xunit[0] + ')'
ylabelunit = yname[0] + ' (' + yunit[0] + ')'

xhat = np.linspace(Xmin,Xmax,100)                             # set of x values to predict and visualize the model
linear_model = LinearRegression().fit(X.reshape(-1, 1),y)     # instantiate and train the frequentist linear regression model
yhat = linear_model.predict(xhat.reshape(-1, 1))              # make predictions for model plotting

slope = linear_model.coef_[0]
intercept = linear_model.intercept_

plt.subplot(111)                                              # plot the data and model
plt.scatter(X, y,c='red',s=10,edgecolor='black')
plt.plot(xhat,yhat,c='red'); add_grid()
plt.title('Linear Regression Model, Regression of ' + yname[0] + ' on ' + Xname[0])
plt.xlabel(Xlabelunit); plt.ylabel(ylabelunit)
add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])
plt.annotate('Linear Regression Model',[18.0,38])
plt.annotate(r'    $\beta_1$ :' + str(round(slope,2)),[16.0,30])
plt.annotate(r'    $\beta_0$ :' + str(round(intercept,2)),[16.0,20])
plt.annotate(r'$N[\phi] = \beta_1 \times z + \beta_0$',[21.0,30])
plt.annotate(r'$N[\phi] = $' + str(round(slope,2)) + r' $\times$ $z$ + (' + str(round(intercept,2)) + ')',[21.0,20])
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/a8e886d92fdcac447a29e1df3ac8e5e2.png)

çº¿æ€§å›å½’æ¨¡å‹å‚æ•°éå¸¸æ¥è¿‘ç”¨äºç”Ÿæˆåˆæˆæ•°æ®çš„ç³»æ•°ã€‚ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ MCMC è®­ç»ƒä¸€ä¸ªè´å¶æ–¯çº¿æ€§å›å½’æ¨¡å‹ã€‚

## è´å¶æ–¯çº¿æ€§å›å½’

æ‰€æœ‰è¿™ä¸€åˆ‡éƒ½å§‹äºä¸€ä¸ªå…ˆéªŒæ¨¡å‹ï¼Œä¸ºæ­¤æˆ‘ä»¬å‡è®¾ä¸€ä¸ªåˆç†çš„å…ˆéªŒæ¨¡å‹ã€‚

### å‡è®¾å…ˆéªŒæ¨¡å‹

æˆ‘ä»¬å‡è®¾æ–œç‡å’Œæˆªè·å‚æ•°æœä»å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼Œ$f_{b_1,b_0,\sigma}(b_1,b_0,\sigma)$ï¼Œå…¶ä¸­$b_1$ã€$b_0$å’Œ$\sigma$ä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚

+   å¯¹äºä¸€ä¸ªæœ´ç´ å…ˆéªŒï¼Œå‡è®¾ä¸€ä¸ªéå¸¸å¤§çš„æ ‡å‡†å·®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¯¹æ•°æ¦‚ç‡å’Œå¯¹æ•°å¯†åº¦æ¥æé«˜ç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚

+   æˆ‘ä»¬å¸Œæœ›é¿å…è®¸å¤šæ¥è¿‘é›¶çš„å€¼çš„ä¹˜ç§¯å’Œï¼Œå› ä¸ºæ¦‚ç‡å°†å› è®¡ç®—æœºæµ®ç‚¹ç²¾åº¦è€Œæ¶ˆå¤±ã€‚

+   åœ¨å¯¹æ•°ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ±‚å’Œï¼ˆè€Œä¸æ˜¯ä¹˜æ³•ï¼‰è¿™äº›è´Ÿå€¼æ¥è®¡ç®—ç‹¬ç«‹äº‹ä»¶çš„æ¦‚ç‡ã€‚

```py
prior = np.zeros([3,2])                                       # prior distributions
prior[0,:] = [5.0,1.0]                                        # Gaussian prior model for slope, mean and standard deviation
prior[1,:] = [18.0,2.0]                                       # Gaussian prior model for intercept, mean and standard deviation
prior[2,:] = [7.0,1.0]                                        # Gaussian prior model for sigma, k (shape) and phi (scale), recall mean = k x phi, var = k x phiÂ² 

plt.subplot(131)                                              # slope prior distribution
plt.plot(np.arange(0,10,0.01),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[0,0],scale=prior[0,1]),c='red'); ylim = plt.gca().get_ylim() 
plt.fill_between(np.arange(0,10,0.01),np.full(1000,-1.0e20),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[0,0],scale=prior[0,1]),color='black',alpha=0.05,zorder=1)
plt.xlabel(r'$b_1$'); plt.ylabel('Natural Log Density'); plt.title("Gaussian Prior Distribution, $b_1$"); add_grid(); plt.xlim([0,10]); plt.ylim(ylim)

plt.subplot(132)                                              # intercept prior distribution
plt.plot(np.arange(0,100,0.01),stats.norm.logpdf(np.arange(0,100,0.01),loc=prior[1,0],scale=prior[1,1]),c='red'); ylim = plt.gca().get_ylim() 
plt.fill_between(np.arange(0,100,0.01),np.full(10000,-1.0e20),stats.norm.logpdf(np.arange(0,100,0.01),loc=prior[1,0],scale=prior[1,1]),color='black',alpha=0.05,zorder=1)
plt.xlabel(r'$b_0$'); plt.ylabel('Natural Log Density'); plt.title("Gaussian Prior Distribution, $b_1$"); add_grid(); plt.xlim([0,100]); plt.ylim(ylim)

plt.subplot(133)                                              # noise prior distribution
plt.plot(np.arange(0,10,0.01),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[2,0],scale=prior[2,1]),c='red'); ylim = plt.gca().get_ylim() 
plt.fill_between(np.arange(0,10,0.01),np.full(1000,-1.0e20),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[2,0],scale=prior[2,1]),color='black',alpha=0.05,zorder=1)
plt.xlabel(r'$\sigma$'); plt.ylabel('Natural Log Density'); plt.title("Gamma Prior Distribution, $\sigma$"); add_grid(); plt.xlim([0,10]); plt.ylim(ylim)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.35, hspace=0.5); plt.show() 
```

![å›¾ç‰‡](img/d6b7bfe8279df066ea241de8c0e3a483.png)

### è´å¶æ–¯çº¿æ€§å›å½’ä¸ MCMC Metropolis-Hastings

è´å¶æ–¯çº¿æ€§å›å½’ä¸ MCMC Metropolis-Hastings å·¥ä½œæµç¨‹ã€‚

1.  åˆ†é…ä¸€ä¸ªéšæœºçš„åˆå§‹æ¨¡å‹å‚æ•°é›†ã€‚

1.  åº”ç”¨æè®®è§„åˆ™ï¼Œæ ¹æ®å‰ä¸€ä¸ªæ¨¡å‹å‚æ•°é›†åˆ†é…ä¸€ä¸ªæ–°çš„æ¨¡å‹å‚æ•°é›†ã€‚

1.  è®¡ç®—æ–°æ¨¡å‹å‚æ•°çš„ä¼¼ç„¶æ¯”ï¼Œç»™å®šæ•°æ®ç›¸å¯¹äºå‰ä¸€ä¸ªæ¨¡å‹å‚æ•°ã€‚

1.  æ ¹æ®è¿™ä¸ªæ¯”ç‡æœ‰æ¡ä»¶åœ°æ¥å—æè®®ï¼Œå³å¦‚æœæè®®æ›´æ¥è¿‘æ¥å—ï¼Œåˆ™æ ¹æ®æ¯”ç‡è®¡ç®—çš„æ¦‚ç‡è¾ƒå°ã€‚

1.  è¿”å›åˆ°æ­¥éª¤ 2ã€‚

```py
np.random.seed(seed = seed)
step_stdev = 0.2

thetas = np.random.rand(3).reshape(1,-1)                      # seed a random first step
accepted = 0

n = 10000                                                     # number of attempts, include accepted and rejected

for i in range(n):
    theta_new = next_proposal(thetas[-1,:],step_stdev=step_stdev) # next proposal

    log_like_new = likelihood_density(X,y,theta_new)          # new and prior likelihoods, log of density
    log_like = likelihood_density(X,y,thetas[-1,:])

    log_prior_new = prior_density(theta_new,prior)            # new and prior, log of density
    log_prior = prior_density(thetas[-1,:],prior)

    likelihood_prior_proposal_ratio = np.exp((log_like_new + log_prior_new) - (log_like + log_prior)) # calculate log ratio

    if likelihood_prior_proposal_ratio > np.random.rand(1):   # conditionally accept by likelihood ratio
        thetas = np.vstack((thetas,theta_new)); accepted += 1

print('Accepted ' + str(accepted) + ' samples of ' + str(n) + ' attempts')

df = pd.DataFrame(np.vstack([thetas[:,0],thetas[:,1],thetas[:,2]]).T, columns= ['Slope','Intercept','Sigma']) 
```

```py
Accepted 1603 samples of 10000 attempts 
```

### å¯è§†åŒ–ç»“æœ

è®©æˆ‘ä»¬å¯è§†åŒ–æ¯ä¸ªæ¨¡å‹å‚æ•°çš„ MCMC Metropolis æŠ½æ ·é“¾ã€‚

+   æ³¨æ„çƒ§å½•é“¾å’Œå¹³è¡¡é“¾ã€‚

+   å°†æ–œç‡å’Œæˆªè·æ¨¡å‹å‚æ•°ä¸ä»…åŸºäºæ™®é€šæœ€å°äºŒä¹˜è§£çš„é¢‘ç‡ä¸»ä¹‰çº¿æ€§å›å½’è§£å†³æ–¹æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼Œæœ€å°åŒ–æ‰€æœ‰æ ·æœ¬æ•°æ®é”™è¯¯å‘é‡çš„ L2 èŒƒæ•°ã€‚

+   å°†å™ªå£°å‚æ•°ä¸æ·»åŠ åˆ°åˆæˆæ•°æ®ä¸­çš„å™ªå£°è¿›è¡Œæ¯”è¾ƒã€‚

```py
alpha = 0.1                                                   # alpha level for displayed confidence intervals 

plt.subplot(131)                                              # plot slope, b_1, samples
plt.plot(np.arange(0,accepted+1,1),thetas[:,0],c='red',zorder=100) 
plt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$b_1$'); plt.title("McMC Bayesian Linear Regression Slope"); add_grid()
plt.plot([0,accepted],[linear_model.coef_[0],linear_model.coef_[0]],c='blue',zorder=200)
plt.annotate('Linear Regression',[30,linear_model.coef_[0]+0.05],color='blue',zorder=200)
plt.plot([0,accepted],[prior[0,0],prior[0,0]],c='black',zorder=10)
plt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[0,0]+0.07])
lower = stats.norm.ppf(alpha/2.0,loc=prior[0,0],scale=prior[0,1])
plt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)
upper = stats.norm.ppf(1-alpha/2.0,loc=prior[0,0],scale=prior[0,1])
plt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)
plt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)
plt.xlim([0,accepted]); plt.ylim([0,10])

plt.subplot(132)                                              # plot intercept, b_0, samples
plt.plot(np.arange(0,accepted+1,1),thetas[:,1],c='red',zorder=100) 
plt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$b_0$'); plt.title("McMC Bayesian Linear Regression Intercept"); add_grid()
plt.plot([0,accepted],[linear_model.intercept_,linear_model.intercept_],c='blue',zorder=200)
plt.annotate('Linear Regression',[30,linear_model.intercept_+0.25],color='blue',zorder=200)
plt.plot([0,accepted],[prior[1,0],prior[1,0]],c='black',zorder=10)
plt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[1,0]+0.07])
lower = stats.norm.ppf(alpha/2.0,loc=prior[1,0],scale=prior[1,1])
plt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)
upper = stats.norm.ppf(1-alpha/2.0,loc=prior[1,0],scale=prior[1,1])
plt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)
plt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)
plt.xlim([0,accepted]); plt.ylim([0,30])

plt.subplot(133)                                              # plot noise, sigma, samples
plt.plot(np.arange(0,accepted+1,1),thetas[:,2],c='red',zorder=100) 
plt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$\sigma$'); plt.title("McMC Bayesian Linear Regression Sigma"); add_grid()
plt.plot([0,accepted],[data_sigma,data_sigma],color='blue',zorder=200)
plt.annotate('Synthetic Data Noise',[30,data_sigma+0.06],color='blue',zorder=200)
plt.plot([0,accepted],[prior[2,0],prior[2,0]],c='black',zorder=10)
plt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[2,0]+0.07])
lower = stats.norm.ppf(alpha/2.0,loc=prior[2,0],scale=prior[2,1])
plt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)
upper = stats.norm.ppf(1-alpha/2.0,loc=prior[2,0],scale=prior[2,1])
plt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)
plt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)
plt.xlim([0,accepted]); plt.ylim([0,10])

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.2, wspace=0.2, hspace=0.5); plt.show() 
```

![å›¾ç‰‡](img/0afb4e1bf5f96613b6f86806ca52ebc3.png)

### åœ¨æ¨¡å‹å‚æ•°ç©ºé—´ä¸­å¯è§†åŒ–é“¾

å°†çƒ§å½•é“¾è®¾ç½®ä¸ºçƒ§å½•é“¾æœ«ç«¯çš„æ¥å—æ ·æœ¬ï¼Œå¹¶è§‚å¯Ÿ MCMC æŠ½æ ·ã€‚

```py
burn_chain = 250
plt.scatter(thetas[:burn_chain,0],thetas[:burn_chain,1],s=5,marker = 'x',c='black',alpha=0.8,linewidth=0.3,cmap=plt.cm.inferno,zorder=10)
plt.scatter(thetas[burn_chain:,0],thetas[burn_chain:,1],s=5,c=np.arange(burn_chain,accepted+1,1),alpha=1.0,edgecolor='black',linewidth=0.1,cmap=plt.cm.inferno,zorder=10)
sns.kdeplot(data=df[burn_chain:],x='Slope',y='Intercept',color='grey',linewidths=1.00,alpha=0.9,levels=np.logspace(-4,-0,3),zorder=100)
plt.plot(thetas[:burn_chain,0],thetas[:burn_chain,1],color='black',linewidth=0.1,zorder=1)
add_grid(); plt.xlabel('Slope, $b_1$'); plt.ylabel('Intercept, $b_0$'); plt.title('McMC Metropolis Samples Bayesian Linear Regression') 
plt.xlim([0,5]); plt.ylim([0,25])
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8, wspace=0.2, hspace=0.5); plt.show() 
```

![å›¾ç‰‡](img/ad916426750d37902655104ee4efeeb0.png)

## æ³¨é‡Š

è¿™æ˜¯å¯¹è´å¶æ–¯çº¿æ€§å›å½’çš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´ YouTube è®²åº§çš„é“¾æ¥ï¼Œè§†é¢‘æè¿°ä¸­åŒ…å«èµ„æºé“¾æ¥ã€‚

å¸Œæœ›è¿™èƒ½æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œåœ¨é‚£é‡Œä»–ç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚

+   [ã€Šè®¡ç®—æœºä¸åœ°å­¦ã€‹](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°å­¦åä¼š[ã€Šæ•°å­¦åœ°å­¦ã€‹](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº†è¶…è¿‡ 70 ç¯‡[åŒè¡Œè¯„å®¡çš„å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦ã€Š[åœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ã€‹ï¼Œå¹¶ä¸”æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œåˆ†åˆ«æ˜¯ã€Š[Python ä¸­çš„åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)ã€‹å’Œã€Š[Python ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„å®è·µæŒ‡å—](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‹ã€‚

è¿ˆå…‹å°”åœ¨å¤§å­¦çš„æ‰€æœ‰è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å« 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ª GitHub ä»“åº“ä¸­çš„è¯¦ç»†å·¥ä½œæµç¨‹é“¾æ¥ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³è¦ä¸€èµ·å·¥ä½œå—ï¼Ÿ

å¸Œæœ›è¿™ä¸ªå†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æˆ‘å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢

æ›´å¤šèµ„æºè¯·è®¿é—®ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## è´å¶æ–¯çº¿æ€§å›å½’çš„åŠ¨æœº

è´å¶æ–¯æœºå™¨å­¦ä¹ æ–¹æ³•å°†æ¦‚ç‡åº”ç”¨äºå…·æœ‰å†…åœ¨ä¸ç¡®å®šæ€§æ¨¡å‹çš„é¢„æµ‹ã€‚æ­¤å¤–ï¼Œè´å¶æ–¯æ–¹æ³•æ•´åˆäº†è´å¶æ–¯æ›´æ–°çš„æ¦‚å¿µï¼Œå³ä½¿ç”¨æ•°æ®ä¸­çš„ä¼¼ç„¶æ¨¡å‹æ›´æ–°å…ˆéªŒæ¨¡å‹æ¥è®¡ç®—åéªŒæ¨¡å‹ã€‚

ç”±äºä½¿ç”¨è¿™äº›æ¦‚ç‡ï¼ˆç”±è¿ç»­æ¦‚ç‡å¯†åº¦å‡½æ•°è¡¨ç¤ºï¼‰è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå­˜åœ¨é¢å¤–çš„å¤æ‚æ€§ã€‚ç”±äºç”±æ­¤äº§ç”Ÿçš„é«˜å¤æ‚æ€§ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥æ±‚è§£æ¨¡å‹å‚æ•°ï¼Œè€Œå¿…é¡»åº”ç”¨å¦‚é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›ï¼ˆMCMCï¼‰æ¨¡æ‹Ÿè¿™æ ·çš„æŠ½æ ·æ–¹æ³•ã€‚

è¿™æ˜¯ä¸€ä¸ªç®€å•ã€éå¸¸æ˜“äºç†è§£çš„è´å¶æ–¯çº¿æ€§å›å½’ä¸ McMC Metropolis-Hastings æŠ½æ ·çš„æ¼”ç¤ºã€‚æˆ‘çš„å­¦ç”Ÿåœ¨è¿™æ–¹é¢çš„æ¦‚å¿µä¸Šé‡åˆ°äº†å›°éš¾ï¼Œæ‰€ä»¥æˆ‘æŒ‘æˆ˜è‡ªå·±ä»å¤´å¼€å§‹æ„å»ºä¸€ä¸ªå·¥ä½œæµç¨‹ï¼Œå¹¶æ¸…æ™°åœ°è§£é‡Šæ‰€æœ‰ç»†èŠ‚ã€‚è®©æˆ‘ä»¬è§£é‡Šå…³é”®çš„å‰ææ¡ä»¶ï¼Œä»è´å¶æ–¯æ›´æ–°å¼€å§‹ã€‚

## çº¿æ€§å›å½’

è®©æˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹çº¿æ€§å›å½’é¢„æµ‹çš„è¦ç‚¹ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹ä¸€ä¸ªæ‹Ÿåˆåˆ°ä¸€ç»„æ•°æ®çš„çº¿æ€§æ¨¡å‹ã€‚

![](img/806bf5f702f9bb5a63e30d6e1f7969d9.png)

ç¤ºä¾‹çº¿æ€§å›å½’æ¨¡å‹ã€‚

**å‚æ•°æ¨¡å‹**

è¿™æ˜¯ä¸€ä¸ªå‚æ•°é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬æ¥å—ä¸€ä¸ªå…ˆéªŒå‡è®¾çš„çº¿æ€§ï¼Œç„¶åè·å¾—ä¸€ä¸ªéå¸¸ä½çš„å‚æ•°è¡¨ç¤ºï¼Œæ˜“äºè®­ç»ƒï¼Œæ— éœ€å¤§é‡æ•°æ®ã€‚

+   é€‚é…æ¨¡å‹æ˜¯ä¸€ä¸ªåŸºäºæ‰€æœ‰å¯ç”¨ç‰¹å¾ $x_1,\ldots,x_m$ çš„ç®€å•åŠ æƒçº¿æ€§åŠ æ€§æ¨¡å‹ã€‚

+   å‚æ•°æ¨¡å‹çš„å½¢å¼ä¸ºï¼š

$$ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 $$

è¿™é‡Œæ˜¯çº¿æ€§æ¨¡å‹å‚æ•°çš„å¯è§†åŒ–ï¼Œ

![](img/ada2fcc2740c48478e79404563c91061.png)

çº¿æ€§æ¨¡å‹å‚æ•°ã€‚

**æœ€å°äºŒä¹˜æ³•**

å¯¹äº L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼Œæ¨¡å‹å‚æ•° $b_1,\ldots,b_m,b_0$ çš„è§£æè§£æ˜¯å¯ç”¨çš„ï¼Œè¯¯å·®æ˜¯æ±‚å’Œå¹¶å¹³æ–¹çš„å·²çŸ¥æœ€å°äºŒä¹˜æ³•ã€‚

+   æˆ‘ä»¬æœ€å°åŒ–è®­ç»ƒæ•°æ®ä¸Šçš„è¯¯å·®ï¼Œæ®‹å·®å¹³æ–¹å’Œï¼ˆRSSï¼‰ï¼š

$$ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)Â² $$

å…¶ä¸­ $y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾å€¼ï¼Œ$\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ æ˜¯æ¨¡å‹é¢„æµ‹ï¼Œåœ¨ $\alpha = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šã€‚

è¿™é‡Œæ˜¯ L2 èŒƒæ•°æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰çš„å¯è§†åŒ–ï¼Œ

![](img/835541b16e1038a4606f7d97b628c4f9.png)

çº¿æ€§æ¨¡å‹çš„æŸå¤±å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®ã€‚

+   è¿™å¯ä»¥ç®€åŒ–ä¸ºè®­ç»ƒæ•°æ®ä¸Šå¹³æ–¹è¯¯å·®çš„æ€»å’Œï¼Œ

\begin{equation} \sum_{i=1}^n (\Delta y_i)Â² \end{equation}

å…¶ä¸­ $\Delta y_i$ æ˜¯å®é™…å“åº”ç‰¹å¾è§‚å¯Ÿ $y_i$ å‡å»æ¨¡å‹é¢„æµ‹ $\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ï¼Œåœ¨ $i = 1,\ldots,n$ çš„è®­ç»ƒæ•°æ®ä¸Šã€‚

**å‡è®¾**

æˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹æœ‰ä¸€äº›é‡è¦çš„å‡è®¾ï¼Œ

+   **æ— è¯¯å·®** - é¢„æµ‹å˜é‡æ— è¯¯å·®ï¼Œä¸æ˜¯éšæœºå˜é‡

+   **çº¿æ€§** - å“åº”æ˜¯ç‰¹å¾ï¼ˆsï¼‰çš„çº¿æ€§ç»„åˆ

+   **å¸¸æ•°æ–¹å·®** - å“åº”è¯¯å·®åœ¨é¢„æµ‹å€¼ä¸Šæ˜¯å¸¸æ•°

+   **è¯¯å·®ç‹¬ç«‹æ€§** - å“åº”è¯¯å·®å½¼æ­¤ä¸ç›¸å…³

+   **æ— å¤šé‡å…±çº¿æ€§** - æ²¡æœ‰ç‰¹å¾ä¸å…¶ä»–ç‰¹å¾å†—ä½™

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å›é¡¾è´å¶æ–¯æ›´æ–°ï¼Œç„¶åæ›´æ–°æˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹åˆ°è´å¶æ–¯çº¿æ€§å›å½’ã€‚

## è´å¶æ–¯æ›´æ–°

è´å¶æ–¯æ¦‚ç‡æ–¹æ³•åŸºäºå¯¹äº‹ä»¶ä¿¡å¿µï¼ˆä¸“å®¶ç»éªŒï¼‰çš„åº¦ï¼Œéšç€æ–°ä¿¡æ¯çš„å‡ºç°è€Œæ›´æ–°

+   è¿™ç§å¤„ç†æ¦‚ç‡çš„æ–¹æ³•å¼ºå¤§ä¸”å¯ä»¥åº”ç”¨äºè§£å†³æˆ‘ä»¬æ— æ³•ç”¨é¢‘ç‡ä¸»ä¹‰æ¦‚ç‡æ–¹æ³•è§£å†³çš„é—®é¢˜

è´å¶æ–¯æ›´æ–°ç”±è´å¶æ–¯å®šç†è¡¨ç¤ºï¼Œ

$$ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} $$

å…¶ä¸­ $P(A)$ æ˜¯å…ˆéªŒæ¦‚ç‡ï¼Œ$P(B|A)$ æ˜¯ä¼¼ç„¶å‡½æ•°ï¼Œ$P(B)$ æ˜¯è´Ÿè´£æ¦‚ç‡å°é—­çš„è¯è¯é¡¹ï¼Œè€Œ $P(A|B)$ æ˜¯åéªŒæ¦‚ç‡ã€‚

## è´å¶æ–¯çº¿æ€§å›å½’

çº¿æ€§å›å½’æ¨¡å‹çš„é¢‘ç‡ä¸»ä¹‰å…¬å¼ä¸ºï¼Œ

$$ y = b_1 \times x + b_0 + \sigma $$

å…¶ä¸­ $x$ æ˜¯é¢„æµ‹ç‰¹å¾ï¼Œ$b_1$ æ˜¯æ–œç‡å‚æ•°ï¼Œ$b_0$ æ˜¯æˆªè·å‚æ•°ï¼Œ$\sigma$ æ˜¯è¯¯å·®æˆ–å™ªå£°ã€‚å­˜åœ¨ä¸€ä¸ªè§£æå½¢å¼æ¥æ‹Ÿåˆå¯ç”¨æ•°æ®ï¼ŒåŒæ—¶æœ€å°åŒ–æ•°æ®è¯¯å·®å‘é‡çš„ L2 èŒƒæ•°ã€‚

æˆ‘ä»¬å¯ä»¥ç”¨çŸ©é˜µè¡¨ç¤ºæ³•è¡¨ç¤ºå¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹ï¼Œ

$$ y = \beta^{T} X + \sigma $$

å…¶ä¸­ $\beta$ æ˜¯æ¨¡å‹å‚æ•°çš„å‘é‡ï¼Œ$\beta_1,\ldots,\beta_m$ã€‚æˆ‘ä»¬å¯ä»¥å‘å‘é‡ä¸­æ·»åŠ å¦ä¸€ä¸ªé¡¹ $\beta_0$ ä½œä¸ºæ¨¡å‹çš„æˆªè·ã€‚

+   å½“æ·»åŠ  $\beta_0$ æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿåœ¨æ•°æ® $X$ çŸ©é˜µä¸­æ·»åŠ ä¸€åˆ— $1$s

å¯¹äºçº¿æ€§å›å½’çš„è´å¶æ–¯å…¬å¼ï¼Œæˆ‘ä»¬å°†æ¨¡å‹è®¾å®šä¸ºå¯¹å“åº”åˆ†å¸ƒ $Y$ çš„é¢„æµ‹ï¼Œç°åœ¨æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼š

$$ Y \sim N(\beta^{T}X, \sigma^{2} I) $$

æˆ‘ä»¬é€šè¿‡è´å¶æ–¯æ›´æ–°æ¥ä¼°è®¡æ¨¡å‹å‚æ•°åˆ†å¸ƒï¼Œä»å…ˆéªŒå’Œä¼¼ç„¶ä¸­æ¨æ–­æ¨¡å‹å‚æ•°ï¼Œè¿™äº›ä¼¼ç„¶å’Œå…ˆéªŒæ¥è‡ªè®­ç»ƒæ•°æ®ã€‚

$$ p(\beta | y, X) = \frac{p(y,X| \beta) p(\beta)}{p(y,X)} $$

è®©æˆ‘ä»¬é€ä¸ªè®¨è®ºæ¯ä¸ªæœ¯è¯­ï¼Œ

1.  å…ˆéªŒï¼Œ$ğ‘ƒ(\beta)$ - åœ¨æ–°æ•°æ®ä¹‹å‰ï¼ŒåŸºäºé¢„æµ‹ç‰¹å¾å¯¹æ¨¡å‹å‚æ•°çš„å…ˆéªŒæ¨æ–­ï¼Œ

+   ä¸“å®¶çŸ¥è¯†

+   æ— ä¿¡æ¯å…ˆéªŒï¼Œåœ¨éä¿¡æ¯å…ˆéªŒçš„æƒ…å†µä¸‹ï¼Œä¼¼ç„¶å°†ä¸»å¯¼

+   ä¿¡æ¯å…ˆéªŒï¼Œåœ¨ä½ä¸ç¡®å®šæ€§å…ˆéªŒçš„æƒ…å†µä¸‹ï¼Œå…ˆéªŒå°†ä¸»å¯¼ä¼¼ç„¶

+   åœ¨æ•´åˆè®­ç»ƒæ•°æ®ä¹‹å‰ï¼Œä¸èƒ½åŒ…æ‹¬æ–°çš„è®­ç»ƒæ•°æ®

1.  ä¼¼ç„¶ï¼Œ$ğ‘ƒ(ğ‘¦,ğ‘‹â”‚\beta)$ - åŸºäºå‡è®¾çš„æ¨¡å‹å‚æ•°ï¼Œå¯¹è®­ç»ƒæ•°æ®çš„æ¡ä»¶åˆ†å¸ƒï¼Œ

+   æ–°æ•°æ®ï¼Œæ•°æ®é©±åŠ¨

+   éšç€æ ·æœ¬æ•°æ®æ•°é‡çš„å¢åŠ ï¼Œä¼¼ç„¶å‡½æ•°ä¼šå‹å€’å…ˆéªŒåˆ†å¸ƒ

1.  è¯æ®ï¼Œ$P(y,X)$ - å½’ä¸€åŒ–å¸¸æ•°ï¼Œä»¥ç¡®ä¿æ¦‚ç‡å°é—­ï¼Œå³æœ‰æ•ˆçš„åéªŒæ¦‚ç‡å¯†åº¦å‡½æ•°

+   ä¸æ¨¡å‹å‚æ•°æ— å…³

+   é€šè¿‡å¯¹åˆ†å­è¿›è¡Œè¾¹ç¼˜åŒ–æ¥è§£å†³ï¼Œé€šè¿‡å¯¹æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹å‚æ•°è¿›è¡Œç§¯åˆ†è€Œç®€åŒ–

+   å¯¹äºæŸäº›è§£å†³æ–¹æ¡ˆï¼Œä¾‹å¦‚ MCMC Metropolis-Hastingsï¼Œè¯æ®é¡¹ä¼šæŠµæ¶ˆ

1.  åéªŒï¼Œ$P(\betaâ”‚y,X)$ - ç»™å®šæ•°æ®é©±åŠ¨çš„ä¼¼ç„¶å’Œå…ˆéªŒï¼ˆåŸºäºä¸“å®¶çŸ¥è¯†å’Œä¿¡å¿µï¼‰çš„æ¨¡å‹å‚æ•°çš„æ¡ä»¶åˆ†å¸ƒ

+   éšç€æ•°æ®æ•°é‡ $ğ‘› \rightarrow \infty$ çš„å¢åŠ ï¼Œæ¨¡å‹å‚æ•° $\beta$ æ”¶æ•›åˆ°æ™®é€šæœ€å°äºŒä¹˜çº¿æ€§å›å½’ï¼Œå…ˆéªŒè¢«æ•°æ®å‹å€’ã€‚

é€šå¸¸å¯¹äºè¿ç»­ç‰¹å¾ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥è®¡ç®—åéªŒï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨é‡‡æ ·æ–¹æ³•ï¼Œä¾‹å¦‚é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›ï¼ˆMcMCï¼‰æ¥é‡‡æ ·åéªŒã€‚

è¿™é‡Œæœ‰ä¸€ä¸ª McMC Metropolis Hastings å·¥ä½œæµç¨‹å’Œæ›´å¤šç»†èŠ‚ã€‚

## é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´› (MCMC)

æ˜¯ä¸€ç»„ç®—æ³•ï¼Œç”¨äºä»æ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œä½¿å¾—æ ·æœ¬åŒ¹é…åˆ†å¸ƒç»Ÿè®¡é‡ã€‚

+   **é©¬å°”å¯å¤«** - å±è”½å‡è®¾ï¼Œä¸‹ä¸€ä¸ªæ ·æœ¬åªä¾èµ–äºå‰ä¸€ä¸ªæ ·æœ¬

+   **é“¾** - æ ·æœ¬å½¢æˆåºåˆ—ï¼Œé€šå¸¸è¡¨æ˜ä»å…·æœ‰ä¸å‡†ç¡®ç»Ÿè®¡é‡çš„çƒ§æ¯é“¾åˆ°å…·æœ‰å‡†ç¡®ç»Ÿè®¡é‡çš„å¹³è¡¡é“¾çš„è½¬æ¢

+   **è’™ç‰¹å¡æ´›** - ä½¿ç”¨è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼Œä»ç»Ÿè®¡åˆ†å¸ƒä¸­è¿›è¡Œéšæœºé‡‡æ ·

è¿™æœ‰ä»€ä¹ˆç”¨ï¼Ÿ

+   æˆ‘ä»¬é€šå¸¸æ²¡æœ‰ç›®æ ‡åˆ†å¸ƒï¼Œå®ƒæ˜¯æœªçŸ¥çš„

+   ä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡å…¶ä»–å½¢å¼çš„ä¿¡æ¯ï¼Œå¦‚æ¡ä»¶æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œä»¥æ­£ç¡®çš„é¢‘ç‡è¿›è¡Œé‡‡æ ·

## ä½¿ç”¨ Metropolis-Hastings æŠ½æ ·å™¨çš„è´å¶æ–¯çº¿æ€§å›å½’

è¿™é‡Œæ˜¯ Metropolis-Hastings MCMC æŠ½æ ·å™¨çš„åŸºæœ¬æ­¥éª¤ï¼š

å¯¹äº $\ell = 1, \ldots, L$:

1.  ä¸ºæ¨¡å‹å‚æ•°çš„åˆå§‹æ ·æœ¬åˆ†é…éšæœºå€¼ï¼Œ$\beta(\ell = 1) = b_1(\ell = 1)$ï¼Œ$b_0(\ell = 1)$ å’Œ $\sigmaÂ²(\ell = 1)$ã€‚

1.  æ ¹æ®æè®®å‡½æ•°æå‡ºæ–°çš„æ¨¡å‹å‚æ•°ï¼Œ$\beta^{\prime} = b_1$ï¼Œ$b_0$ å’Œ $\sigmaÂ²$ã€‚

1.  è®¡ç®—æ–°æè®®çš„æ¥å—æ¦‚ç‡ï¼Œä½œä¸ºç»™å®šæ•°æ®çš„åéªŒæ¦‚ç‡ä¸å…ˆå‰çš„æ¨¡å‹å‚æ•°çš„æ¯”ï¼Œä¹˜ä»¥ç»™å®šæ–°æ­¥éª¤çš„è€æ­¥éª¤çš„æ¦‚ç‡é™¤ä»¥ç»™å®šæ—§æ­¥éª¤çš„æ–°æ­¥éª¤çš„æ¦‚ç‡ã€‚

$$ P(\beta \rightarrow \beta^{\prime}) = min\left(\frac{p(\beta^{\prime}|y,X) }{ p(\beta | y,X)} \cdot \frac{p(\beta^{\prime}|\beta) }{ p(\beta | \beta^{\prime})},1\right) $$

1.  åº”ç”¨è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿä»¥æ¡ä»¶æ¥å—æè®®ï¼Œå¦‚æœæ¥å—ï¼Œ$\ell = \ell + 1$ï¼Œå¹¶é‡‡æ · $\beta(\ell) = \beta^{\prime}$

1.  è½¬åˆ°æ­¥éª¤ 2ã€‚

è®©æˆ‘ä»¬è®¨è®ºè¿™ä¸ªç³»ç»Ÿã€‚é¦–å…ˆï¼Œå·¦è¾¹ï¼š

$$ \frac{p(\beta^{\prime}|y,X) }{ p(\beta | y,X)} $$

æˆ‘ä»¬æ­£åœ¨è®¡ç®—ç»™å®šæ•°æ®å’Œå…ˆéªŒæ¨¡å‹çš„åéªŒæ¦‚ç‡ï¼ˆä¼¼ç„¶ä¹˜ä»¥å…ˆéªŒï¼‰çš„æ¯”ç‡ï¼Œå¯¹äºæè®®æ ·æœ¬å’Œå½“å‰æ ·æœ¬ã€‚

+   å¦‚æ‚¨ä¸‹é¢æ‰€è§ï¼Œè®¡ç®—è¿™ä¸ªæ¯”ç‡æ˜¯éå¸¸å®ç”¨çš„ã€‚

+   å¦‚æœæè®®çš„æ ·æœ¬æ¯”å½“å‰æ ·æœ¬æ›´æœ‰å¯èƒ½ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªå¤§äº 1.0 çš„å€¼ï¼Œå®ƒå°†è¢«æˆªæ–­ä¸º 1.0ï¼Œå¹¶ä¸”æˆ‘ä»¬æ¥å—æè®®çš„æ ·æœ¬ã€‚

+   å¦‚æœæè®®çš„æ ·æœ¬æ¯”å½“å‰æ ·æœ¬ä¸å¤ªå¯èƒ½ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªå°äº 1.0 çš„å€¼ï¼Œç„¶åæˆ‘ä»¬å°†ä½¿ç”¨è’™ç‰¹å¡æ´›æŠ½æ ·ä»¥è¿™ä¸ªæ¥å—æ¦‚ç‡éšæœºé€‰æ‹©æè®®çš„æ ·æœ¬ã€‚

è¿™ä¸ªç¨‹åºä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡æ¨¡å‹å‚æ•°ç©ºé—´è¿›è¡Œéå†ï¼Œå¹¶åœ¨çƒ§å½•é“¾ä¹‹åä»¥å½“å‰é€Ÿç‡é‡‡æ ·å‚æ•°ã€‚

é‚£ä¹ˆï¼Œè¿™ä¸ªæ–¹ç¨‹çš„éƒ¨åˆ†æ˜¯ä»€ä¹ˆï¼Ÿ

$$ \frac{p(\beta^{\prime}|\beta) }{ p(\beta | \beta^{\prime})} $$

å¦‚æœæˆ‘ä»¬ä¸ºæ¨¡å‹å‚æ•°ä½¿ç”¨éå¯¹ç§°æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ä¸ªç¨‹åºå°±ä¼šæœ‰é—®é¢˜ï¼

+   ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨æ­£åæ–œåˆ†å¸ƒï¼ˆä¾‹å¦‚ï¼Œå¯¹æ•°æ­£æ€åˆ†å¸ƒï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ›´æœ‰å¯èƒ½å› ä¸ºè¿™ç§åˆ†å¸ƒè€Œèµ°å‘æ›´å¤§çš„å€¼ï¼Œè€Œä¸æ˜¯å› ä¸ºå…ˆéªŒæˆ–ä¼¼ç„¶ã€‚

+   è¿™ä¸ªé¡¹æ¶ˆé™¤äº†è¿™ç§åå·®ï¼Œå› æ­¤æˆ‘ä»¬å¾—åˆ°å…¬å¹³çš„æ ·æœ¬ï¼

æ‚¨å°†åœ¨ä¸‹é¢çœ‹åˆ°ï¼Œæˆ‘ä»¬é€šè¿‡å‡è®¾å¯¹ç§°æ¨¡å‹å‚æ•°åˆ†å¸ƒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°½ç®¡è®¸å¤šäººä½¿ç”¨éå¯¹ç§°ä¼½é©¬åˆ†å¸ƒæ¥è¡¨ç¤º sigmaï¼Œå› ä¸ºå®ƒä¸èƒ½æœ‰è´Ÿå€¼ã€‚

+   æˆ‘çš„ç›®çš„æ˜¯å°½å¯èƒ½ç®€å•çš„æ¼”ç¤ºã€‚

## æˆ‘ä»¬çš„ç®€åŒ–æ¼”ç¤º Metropolis æŠ½æ ·

è®©æˆ‘ä»¬è¿›ä¸€æ­¥å…·ä½“è¯´æ˜è¿™ä¸ªå·¥ä½œæµç¨‹ï¼Œä»¥ä¾›æˆ‘ä»¬çš„ç®€å•æ¼”ç¤ºã€‚

+   æˆ‘å‡è®¾æ‰€æœ‰æ¨¡å‹å‚æ•°éƒ½éµå¾ªé«˜æ–¯ã€å¯¹ç§°åˆ†å¸ƒï¼Œå› æ­¤è¿™ä¸ªå…³ç³»å¯¹æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹å‚æ•°ï¼ŒåŒ…æ‹¬å½“å‰å’Œæè®®çš„å‚æ•°éƒ½æˆç«‹ã€‚

$$ \frac{p(\beta^{\prime}|\beta) }{ p(\beta | \beta^{\prime})} = 1.0 $$

å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰äº†è¿™ä¸ªç®€åŒ–çš„æè®®æ¥å—æ¦‚ç‡ï¼Œæ³¨æ„è¿™è¢«ç§°ä¸º Metropolis æŠ½æ ·ã€‚

$$ P(\beta \rightarrow \beta^{\prime}) = min \left( \frac{p(\beta^{\prime}|y,X) }{ p(\beta | y,X)},1 \right) $$

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†è´å¶æ–¯å…¬å¼ä»£å…¥æˆ‘ä»¬çš„è´å¶æ–¯çº¿æ€§å›å½’æ¨¡å‹ä¸­ã€‚

$$ p(\beta^{\prime} | y, X) = \frac{p(y,X| \beta^{\prime}) p(\beta^{\prime})}{p(y,X)} \quad \text{å’Œ} \quad p(\beta | y, X) = \frac{p(y,X| \beta) p(\beta)}{p(y,X)} $$

å¦‚æœæˆ‘ä»¬å°†è¿™äº›ä»£å…¥æˆ‘ä»¬ä¸Šé¢çš„æ¥å—æ¦‚ç‡ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°è¿™ä¸ªç»“æœã€‚

$$ P(\beta \rightarrow \beta^{\prime}) = min \left( \frac{p(\beta^{\prime}|y,X) }{ p(\beta | y,X)},1 \right) = min \left( \frac{ \left( \frac{p(y,X| \beta_{new}) p(\beta_{new}) } {p(y,X)} \right) }{ \left( \frac{ p(y,X| \beta) p(\beta)}{p(y,X)} \right) },1 \right) $$

æ³¨æ„ï¼Œè¯æ®é¡¹ç›¸äº’æŠµæ¶ˆã€‚

$$ P(\beta \rightarrow \beta^{\prime}) = min \left( \frac{ p(y,X| \beta_{new}) p(\beta_{new}) }{ p(y,X| \beta) p(\beta)},1 \right) $$

ç”±äºæˆ‘ä»¬å¤„ç†çš„æ˜¯ä¼¼ç„¶æ¯”ç‡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¯†åº¦è€Œä¸æ˜¯æ¦‚ç‡ã€‚

$$ P(\beta \rightarrow \beta^{\prime}) = min \left( \frac{ f(y,X| \beta_{new}) f(\beta_{new}) }{ f(y,X| \beta) f(\beta) } ,1 \right) $$

æœ€åï¼Œä¸ºäº†æé«˜è§£çš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—è‡ªç„¶å¯¹æ•°æ¯”ç‡ï¼š

$$ ln(P(\beta \rightarrow \beta^{\prime})) = min \left( ln \left[ \frac{ f(y,X| \beta_{new}) f(\beta_{new}) }{ f(y,X| \beta) f(\beta) } \right],0 \right) $$$$ = min \left( \left[ln(f(y,X| \beta_{new})) + ln(f(\beta_{new})) \right] - \left[ ln(f(y,X| \beta)) + ln(f(\beta)) \right],0 \right) $$

æˆ‘ä»¬è®¡ç®—æè®®æ¥å—çš„æ¦‚ç‡ï¼Œä½œä¸ºä¸Šé¢çš„æŒ‡æ•°ã€‚

æˆ‘ä»¬å¦‚ä½•è®¡ç®—ä¼¼ç„¶å¯†åº¦ï¼Ÿå¦‚æœæˆ‘ä»¬å‡è®¾æ‰€æœ‰æ•°æ®ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œæˆ‘ä»¬å¯ä»¥å–æ‰€æœ‰å“åº”å€¼ç»™å®šé¢„æµ‹å™¨å’Œæ¨¡å‹å‚æ•°æ ·æœ¬çš„æ¦‚ç‡ï¼ˆå¯†åº¦ï¼‰çš„ä¹˜ç§¯å’Œï¼ç»™å®šå“åº”ç‰¹å¾çš„é«˜æ–¯å‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥ä»é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸­è®¡ç®—æ¯ä¸ªæ•°æ®çš„å¯†åº¦ã€‚

$$ f_{y,X | \beta}(y) \sim N [ b_1 \cdot X + b_0, \sigma ] $$

åœ¨ç‹¬ç«‹æ€§çš„å‡è®¾ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ‰€æœ‰è®­ç»ƒæ•°æ®è¿›è¡Œä¹˜ç§¯å’Œã€‚

$$ f(y,X| \beta) = \prod_{\alpha = 1}^{n} f_{y,X | \beta}(y_{\alpha}) $$

æ³¨æ„ï¼Œè¿™ä¸ªå·¥ä½œæµç¨‹æ˜¯åœ¨ Fortunato Nucera çš„ Medium æ–‡ç« [ä»å¤´å¼€å§‹æŒæ¡è´å¶æ–¯çº¿æ€§å›å½’ï¼šPython ä¸­çš„ Metropolis-Hastings å®ç°](https://medium.com/@tinonucera/bayesian-linear-regression-from-scratch-a-metropolis-hastings-implementation-63526857f191)çš„å¸®åŠ©ä¸‹å¼€å‘çš„ã€‚æˆ‘å¼ºçƒˆæ¨èè¿™ç¯‡æ˜“äºç†è§£çš„æè¿°å’Œæ¼”ç¤ºã€‚è°¢è°¢ï¼ŒFortunatoã€‚

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = True
import os                                                     # to set current working directory 
import math                                                   # square root operator
import numpy as np                                            # arrays and matrix math
import scipy.stats as stats                                   # statistical methods
import pandas as pd                                           # DataFrames
import pandas.plotting as pd_plot
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.patches import Rectangle                      # build a custom legend
from matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import seaborn as sns                                         # for matrix scatter plots
from sklearn.linear_model import LinearRegression             # frequentist model for comparison
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # suppress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½éœ€è¦é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ï¼Œç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£ä¸­æä¾›äº†æ›´å¤šå¸®åŠ©ã€‚

## å£°æ˜å‡½æ•°

ä»¥ä¸‹å‡½æ•°åŒ…æ‹¬ï¼š

+   **next_proposal** - ä»å…ˆå‰æ¨¡å‹å‚æ•°æå‡ºä¸‹ä¸€ä¸ªæ¨¡å‹å‚æ•°ï¼Œè¿™æ˜¯æè®®æ–¹æ³•ï¼Œç”±æ­¥é•¿æ ‡å‡†å·®å’Œé«˜æ–¯åˆ†å¸ƒå‚æ•°åŒ–ã€‚

+   **ä¼¼ç„¶å¯†åº¦** - æ ¹æ®æ¨¡å‹å‚æ•°è®¡ç®—æ‰€æœ‰æ•°æ®çš„å¯†åº¦ä¹˜ç§¯ã€‚ç”±äºæˆ‘ä»¬ä½¿ç”¨å¯¹æ•°å¯†åº¦ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œæ±‚å’Œã€‚

+   **å…ˆéªŒå¯†åº¦** - æ ¹æ®å…ˆéªŒæ¨¡å‹è®¡ç®—æ‰€æœ‰æ¨¡å‹å‚æ•°çš„å¯†åº¦ä¹˜ç§¯ã€‚ç”±äºæˆ‘ä»¬ä½¿ç”¨å¯¹æ•°å¯†åº¦ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯¹æ‰€æœ‰æ¨¡å‹å‚æ•°è¿›è¡Œæ±‚å’Œã€‚è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹æ€§çš„å‡è®¾ã€‚

+   **add_grid** - æ”¹è¿›çš„ç»˜å›¾ç½‘æ ¼ã€‚

```py
def next_proposal(prev_theta, step_stdev = 0.5):              # assuming a Gaussian distribution centered on previous theta and step stdev 
    out_theta = stats.multivariate_normal(mean=prev_theta,cov=np.eye(3)*step_stdev**2).rvs(1)
    return out_theta

def likelihood_density(x,y,theta):                            # likelihood - probability (density) of the data given the model
    density = np.sum(stats.norm.logpdf(y, loc=theta[0]*x+theta[1],scale=theta[2])) # assume independence, sum is product in log space
    return density

def prior_density(theta,prior):                               # prior - probability (density) of the model parameters given the prior 
    mean = np.array([prior[0,0],prior[1,0],prior[2,0]]); cov = np.zeros([3,3]); cov[0,0] = prior[0,1]; cov[1,1] = prior[1,1]; cov[2,2] = prior[2,1]
    prior_out = stats.multivariate_normal.logpdf(theta,mean=mean,cov=cov,allow_singular=True)
    return prior_out

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥æ“ä½œï¼ˆæ¯æ¬¡éƒ½é¿å…åŒ…å«å®Œæ•´çš„åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/PGE383")                                        # set the working directory 
```

## ä½¿ç”¨å·²çŸ¥çš„çº¿æ€§å›å½’æ¨¡å‹å‚æ•°æ„å»ºåˆæˆæ•°æ®é›†

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªç®€å•çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å·²çŸ¥çš„çº¿æ€§å›å½’æ¨¡å‹å‚æ•°ï¼Œ$b_1$ æ˜¯æ–œç‡å‚æ•°ï¼Œ$b_0$ æ˜¯æˆªè·å‚æ•°ï¼Œ$\sigma$ æ˜¯è¯¯å·®æˆ–å™ªå£°ã€‚

+   æˆ‘ä»¬åŸºäºè¿™äº›å‚æ•°æ„å»ºæ•°æ®ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶å±•ç¤ºæˆ‘ä»¬å¾—åˆ°äº†æ­£ç¡®çš„æ–œç‡å’Œæˆªè·æ¨¡å‹å‚æ•°ã€‚

```py
np.random.seed(seed = seed)                                   # set random number seed

data_b1 = 3; data_b0 = 20; data_sigma = 5; n = 100            # set data model parameters

X = np.random.rand(n)*30                                      # random x values
y = data_b1*X+data_b0 + np.random.normal(loc=0.0,scale=data_sigma,size=n) # y as a linear function of x + random noise 

Xname = [r'$X_1$']; yname = [r'$y$']                          # specify the predictor features (x2) and response feature (x1)
Xmin = 0.0; Xmax = 30; ymin = 0.0; ymax = 120                 # set minimums and maximums for visualization 
Xunit = ['none']; yunit = ['none'] 
Xlabelunit = Xname[0] + ' (' + Xunit[0] + ')'
ylabelunit = yname[0] + ' (' + yunit[0] + ')'

xhat = np.linspace(Xmin,Xmax,100)                             # set of x values to predict and visualize the model
linear_model = LinearRegression().fit(X.reshape(-1, 1),y)     # instantiate and train the frequentist linear regression model
yhat = linear_model.predict(xhat.reshape(-1, 1))              # make predictions for model plotting

slope = linear_model.coef_[0]
intercept = linear_model.intercept_

plt.subplot(111)                                              # plot the data and model
plt.scatter(X, y,c='red',s=10,edgecolor='black')
plt.plot(xhat,yhat,c='red'); add_grid()
plt.title('Linear Regression Model, Regression of ' + yname[0] + ' on ' + Xname[0])
plt.xlabel(Xlabelunit); plt.ylabel(ylabelunit)
add_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])
plt.annotate('Linear Regression Model',[18.0,38])
plt.annotate(r'    $\beta_1$ :' + str(round(slope,2)),[16.0,30])
plt.annotate(r'    $\beta_0$ :' + str(round(intercept,2)),[16.0,20])
plt.annotate(r'$N[\phi] = \beta_1 \times z + \beta_0$',[21.0,30])
plt.annotate(r'$N[\phi] = $' + str(round(slope,2)) + r' $\times$ $z$ + (' + str(round(intercept,2)) + ')',[21.0,20])
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/a8e886d92fdcac447a29e1df3ac8e5e2.png)

çº¿æ€§å›å½’æ¨¡å‹å‚æ•°éå¸¸æ¥è¿‘ç”¨äºç”Ÿæˆåˆæˆæ•°æ®çš„ç³»æ•°ã€‚ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ MCMC è®­ç»ƒä¸€ä¸ªè´å¶æ–¯çº¿æ€§å›å½’æ¨¡å‹ã€‚

## è´å¶æ–¯çº¿æ€§å›å½’

æ‰€æœ‰è¿™ä¸€åˆ‡éƒ½å§‹äºä¸€ä¸ªå…ˆéªŒæ¨¡å‹ï¼Œä¸ºæ­¤æˆ‘ä»¬å‡è®¾ä¸€ä¸ªåˆç†çš„å…ˆéªŒæ¨¡å‹ã€‚

### å‡è®¾å…ˆéªŒæ¨¡å‹

æˆ‘ä»¬å‡è®¾æ–œç‡å’Œæˆªè·å‚æ•°æœä»å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼Œ$f_{b_1,b_0,\sigma}(b_1,b_0,\sigma)$ï¼Œå…¶ä¸­ $b_1$ã€$b_0$ å’Œ $\sigma$ ä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚

+   å¯¹äºä¸€ä¸ªæœ´ç´ å…ˆéªŒï¼Œå‡è®¾ä¸€ä¸ªéå¸¸å¤§çš„æ ‡å‡†å·®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¯¹æ•°æ¦‚ç‡å’Œå¯¹æ•°å¯†åº¦æ¥æé«˜ç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚

+   æˆ‘ä»¬æƒ³è¦é¿å…æ¥è¿‘é›¶çš„å¤šä¸ªå€¼çš„ä¹˜ç§¯å’Œï¼Œå› ä¸ºæ¦‚ç‡ä¼šç”±äºè®¡ç®—æœºæµ®ç‚¹ç²¾åº¦è€Œæ¶ˆå¤±ã€‚

+   åœ¨å¯¹æ•°ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ±‚å’Œï¼ˆè€Œä¸æ˜¯ä¹˜æ³•ï¼‰è¿™äº›è´Ÿå€¼æ¥è®¡ç®—ç‹¬ç«‹äº‹ä»¶çš„æ¦‚ç‡

```py
prior = np.zeros([3,2])                                       # prior distributions
prior[0,:] = [5.0,1.0]                                        # Gaussian prior model for slope, mean and standard deviation
prior[1,:] = [18.0,2.0]                                       # Gaussian prior model for intercept, mean and standard deviation
prior[2,:] = [7.0,1.0]                                        # Gaussian prior model for sigma, k (shape) and phi (scale), recall mean = k x phi, var = k x phiÂ² 

plt.subplot(131)                                              # slope prior distribution
plt.plot(np.arange(0,10,0.01),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[0,0],scale=prior[0,1]),c='red'); ylim = plt.gca().get_ylim() 
plt.fill_between(np.arange(0,10,0.01),np.full(1000,-1.0e20),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[0,0],scale=prior[0,1]),color='black',alpha=0.05,zorder=1)
plt.xlabel(r'$b_1$'); plt.ylabel('Natural Log Density'); plt.title("Gaussian Prior Distribution, $b_1$"); add_grid(); plt.xlim([0,10]); plt.ylim(ylim)

plt.subplot(132)                                              # intercept prior distribution
plt.plot(np.arange(0,100,0.01),stats.norm.logpdf(np.arange(0,100,0.01),loc=prior[1,0],scale=prior[1,1]),c='red'); ylim = plt.gca().get_ylim() 
plt.fill_between(np.arange(0,100,0.01),np.full(10000,-1.0e20),stats.norm.logpdf(np.arange(0,100,0.01),loc=prior[1,0],scale=prior[1,1]),color='black',alpha=0.05,zorder=1)
plt.xlabel(r'$b_0$'); plt.ylabel('Natural Log Density'); plt.title("Gaussian Prior Distribution, $b_1$"); add_grid(); plt.xlim([0,100]); plt.ylim(ylim)

plt.subplot(133)                                              # noise prior distribution
plt.plot(np.arange(0,10,0.01),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[2,0],scale=prior[2,1]),c='red'); ylim = plt.gca().get_ylim() 
plt.fill_between(np.arange(0,10,0.01),np.full(1000,-1.0e20),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[2,0],scale=prior[2,1]),color='black',alpha=0.05,zorder=1)
plt.xlabel(r'$\sigma$'); plt.ylabel('Natural Log Density'); plt.title("Gamma Prior Distribution, $\sigma$"); add_grid(); plt.xlim([0,10]); plt.ylim(ylim)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.35, hspace=0.5); plt.show() 
```

![å›¾ç‰‡](img/d6b7bfe8279df066ea241de8c0e3a483.png)

### åŸºäº MCMC Metropolis-Hastings çš„è´å¶æ–¯çº¿æ€§å›å½’

åŸºäº MCMC Metropolis-Hastings çš„è´å¶æ–¯çº¿æ€§å›å½’å·¥ä½œæµç¨‹ã€‚

1.  åˆ†é…ä¸€ä¸ªéšæœºçš„åˆå§‹æ¨¡å‹å‚æ•°é›†

1.  åº”ç”¨ä¸€ä¸ªæè®®è§„åˆ™ï¼Œæ ¹æ®å…ˆå‰çš„ä¸€ç»„æ¨¡å‹å‚æ•°åˆ†é…ä¸€ç»„æ–°çš„æ¨¡å‹å‚æ•°

1.  è®¡ç®—ç»™å®šæ•°æ®çš„æ–°æ¨¡å‹å‚æ•°ç›¸å¯¹äºå…ˆå‰æ¨¡å‹å‚æ•°çš„ä¼¼ç„¶æ¯”

1.  æ ¹æ®è¿™ä¸ªæ¯”ç‡æœ‰æ¡ä»¶åœ°æ¥å—æè®®ï¼Œå³å¦‚æœæè®®æ›´æ¥è¿‘æ¥å—ï¼Œåˆ™æ ¹æ®æ¯”ç‡è®¡ç®—çš„æ¦‚ç‡è¾ƒå°ã€‚

1.  è¿”å›åˆ°æ­¥éª¤ 2ã€‚

```py
np.random.seed(seed = seed)
step_stdev = 0.2

thetas = np.random.rand(3).reshape(1,-1)                      # seed a random first step
accepted = 0

n = 10000                                                     # number of attempts, include accepted and rejected

for i in range(n):
    theta_new = next_proposal(thetas[-1,:],step_stdev=step_stdev) # next proposal

    log_like_new = likelihood_density(X,y,theta_new)          # new and prior likelihoods, log of density
    log_like = likelihood_density(X,y,thetas[-1,:])

    log_prior_new = prior_density(theta_new,prior)            # new and prior, log of density
    log_prior = prior_density(thetas[-1,:],prior)

    likelihood_prior_proposal_ratio = np.exp((log_like_new + log_prior_new) - (log_like + log_prior)) # calculate log ratio

    if likelihood_prior_proposal_ratio > np.random.rand(1):   # conditionally accept by likelihood ratio
        thetas = np.vstack((thetas,theta_new)); accepted += 1

print('Accepted ' + str(accepted) + ' samples of ' + str(n) + ' attempts')

df = pd.DataFrame(np.vstack([thetas[:,0],thetas[:,1],thetas[:,2]]).T, columns= ['Slope','Intercept','Sigma']) 
```

```py
Accepted 1603 samples of 10000 attempts 
```

### å¯è§†åŒ–ç»“æœ

è®©æˆ‘ä»¬å¯è§†åŒ–æ¯ä¸ªæ¨¡å‹å‚æ•°çš„ Metropolis é‡‡æ ·é“¾ã€‚

+   æ³¨æ„åˆ°çƒ§å½•å’Œå¹³è¡¡é“¾ã€‚

+   å°†æ–œç‡å’Œæˆªè·æ¨¡å‹å‚æ•°ä¸ä»…åŸºäºæ™®é€šæœ€å°äºŒä¹˜è§£çš„é¢‘ç‡ä¸»ä¹‰çº¿æ€§å›å½’è§£è¿›è¡Œæ¯”è¾ƒï¼Œæœ€å°åŒ–æ‰€æœ‰æ ·æœ¬æ•°æ®ä¸­è¯¯å·®å‘é‡çš„ L2 èŒƒæ•°ã€‚

+   å°†å™ªå£°å‚æ•°ä¸æ·»åŠ åˆ°åˆæˆæ•°æ®ä¸­çš„å™ªå£°è¿›è¡Œæ¯”è¾ƒã€‚

```py
alpha = 0.1                                                   # alpha level for displayed confidence intervals 

plt.subplot(131)                                              # plot slope, b_1, samples
plt.plot(np.arange(0,accepted+1,1),thetas[:,0],c='red',zorder=100) 
plt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$b_1$'); plt.title("McMC Bayesian Linear Regression Slope"); add_grid()
plt.plot([0,accepted],[linear_model.coef_[0],linear_model.coef_[0]],c='blue',zorder=200)
plt.annotate('Linear Regression',[30,linear_model.coef_[0]+0.05],color='blue',zorder=200)
plt.plot([0,accepted],[prior[0,0],prior[0,0]],c='black',zorder=10)
plt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[0,0]+0.07])
lower = stats.norm.ppf(alpha/2.0,loc=prior[0,0],scale=prior[0,1])
plt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)
upper = stats.norm.ppf(1-alpha/2.0,loc=prior[0,0],scale=prior[0,1])
plt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)
plt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)
plt.xlim([0,accepted]); plt.ylim([0,10])

plt.subplot(132)                                              # plot intercept, b_0, samples
plt.plot(np.arange(0,accepted+1,1),thetas[:,1],c='red',zorder=100) 
plt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$b_0$'); plt.title("McMC Bayesian Linear Regression Intercept"); add_grid()
plt.plot([0,accepted],[linear_model.intercept_,linear_model.intercept_],c='blue',zorder=200)
plt.annotate('Linear Regression',[30,linear_model.intercept_+0.25],color='blue',zorder=200)
plt.plot([0,accepted],[prior[1,0],prior[1,0]],c='black',zorder=10)
plt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[1,0]+0.07])
lower = stats.norm.ppf(alpha/2.0,loc=prior[1,0],scale=prior[1,1])
plt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)
upper = stats.norm.ppf(1-alpha/2.0,loc=prior[1,0],scale=prior[1,1])
plt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)
plt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)
plt.xlim([0,accepted]); plt.ylim([0,30])

plt.subplot(133)                                              # plot noise, sigma, samples
plt.plot(np.arange(0,accepted+1,1),thetas[:,2],c='red',zorder=100) 
plt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$\sigma$'); plt.title("McMC Bayesian Linear Regression Sigma"); add_grid()
plt.plot([0,accepted],[data_sigma,data_sigma],color='blue',zorder=200)
plt.annotate('Synthetic Data Noise',[30,data_sigma+0.06],color='blue',zorder=200)
plt.plot([0,accepted],[prior[2,0],prior[2,0]],c='black',zorder=10)
plt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[2,0]+0.07])
lower = stats.norm.ppf(alpha/2.0,loc=prior[2,0],scale=prior[2,1])
plt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)
upper = stats.norm.ppf(1-alpha/2.0,loc=prior[2,0],scale=prior[2,1])
plt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)
plt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)
plt.xlim([0,accepted]); plt.ylim([0,10])

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.2, wspace=0.2, hspace=0.5); plt.show() 
```

![å›¾ç‰‡](img/0afb4e1bf5f96613b6f86806ca52ebc3.png)

### åœ¨æ¨¡å‹å‚æ•°ç©ºé—´ä¸­å¯è§†åŒ–é“¾

å°†çƒ§å½•é“¾çš„ç»“æŸå¤„çš„æ¥å—æ ·æœ¬ä½œä¸ºçƒ§å½•é“¾çš„æ¥å—æ ·æœ¬ï¼Œå¹¶è§‚å¯Ÿ MCMC é‡‡æ ·ã€‚

```py
burn_chain = 250
plt.scatter(thetas[:burn_chain,0],thetas[:burn_chain,1],s=5,marker = 'x',c='black',alpha=0.8,linewidth=0.3,cmap=plt.cm.inferno,zorder=10)
plt.scatter(thetas[burn_chain:,0],thetas[burn_chain:,1],s=5,c=np.arange(burn_chain,accepted+1,1),alpha=1.0,edgecolor='black',linewidth=0.1,cmap=plt.cm.inferno,zorder=10)
sns.kdeplot(data=df[burn_chain:],x='Slope',y='Intercept',color='grey',linewidths=1.00,alpha=0.9,levels=np.logspace(-4,-0,3),zorder=100)
plt.plot(thetas[:burn_chain,0],thetas[:burn_chain,1],color='black',linewidth=0.1,zorder=1)
add_grid(); plt.xlabel('Slope, $b_1$'); plt.ylabel('Intercept, $b_0$'); plt.title('McMC Metropolis Samples Bayesian Linear Regression') 
plt.xlim([0,5]); plt.ylim([0,25])
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8, wspace=0.2, hspace=0.5); plt.show() 
```

![å›¾ç‰‡](img/ad916426750d37902655104ee4efeeb0.png)

### å‡è®¾å…ˆéªŒæ¨¡å‹

æˆ‘ä»¬å‡è®¾æ–œç‡å’Œæˆªè·å‚æ•°æœä»å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼Œ$f_{b_1,b_0,\sigma}(b_1,b_0,\sigma)$ï¼Œå…¶ä¸­$b_1$ã€$b_0$å’Œ$\sigma$ä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚

+   å¯¹äºä¸€ä¸ªæœ´ç´ å…ˆéªŒï¼Œå‡è®¾ä¸€ä¸ªéå¸¸å¤§çš„æ ‡å‡†å·®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¯¹æ•°æ¦‚ç‡å’Œå¯¹æ•°å¯†åº¦æ¥æé«˜ç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚

+   æˆ‘ä»¬å¸Œæœ›é¿å…æ¥è¿‘é›¶çš„å¤šä¸ªå€¼çš„ä¹˜ç§¯å’Œï¼Œå› ä¸ºæ¦‚ç‡å°†å› è®¡ç®—æœºæµ®ç‚¹ç²¾åº¦è€Œæ¶ˆå¤±ã€‚

+   åœ¨å¯¹æ•°ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ±‚å’Œï¼ˆè€Œä¸æ˜¯ä¹˜æ³•ï¼‰è¿™äº›è´Ÿå€¼æ¥è®¡ç®—ç‹¬ç«‹äº‹ä»¶çš„æ¦‚ç‡

```py
prior = np.zeros([3,2])                                       # prior distributions
prior[0,:] = [5.0,1.0]                                        # Gaussian prior model for slope, mean and standard deviation
prior[1,:] = [18.0,2.0]                                       # Gaussian prior model for intercept, mean and standard deviation
prior[2,:] = [7.0,1.0]                                        # Gaussian prior model for sigma, k (shape) and phi (scale), recall mean = k x phi, var = k x phiÂ² 

plt.subplot(131)                                              # slope prior distribution
plt.plot(np.arange(0,10,0.01),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[0,0],scale=prior[0,1]),c='red'); ylim = plt.gca().get_ylim() 
plt.fill_between(np.arange(0,10,0.01),np.full(1000,-1.0e20),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[0,0],scale=prior[0,1]),color='black',alpha=0.05,zorder=1)
plt.xlabel(r'$b_1$'); plt.ylabel('Natural Log Density'); plt.title("Gaussian Prior Distribution, $b_1$"); add_grid(); plt.xlim([0,10]); plt.ylim(ylim)

plt.subplot(132)                                              # intercept prior distribution
plt.plot(np.arange(0,100,0.01),stats.norm.logpdf(np.arange(0,100,0.01),loc=prior[1,0],scale=prior[1,1]),c='red'); ylim = plt.gca().get_ylim() 
plt.fill_between(np.arange(0,100,0.01),np.full(10000,-1.0e20),stats.norm.logpdf(np.arange(0,100,0.01),loc=prior[1,0],scale=prior[1,1]),color='black',alpha=0.05,zorder=1)
plt.xlabel(r'$b_0$'); plt.ylabel('Natural Log Density'); plt.title("Gaussian Prior Distribution, $b_1$"); add_grid(); plt.xlim([0,100]); plt.ylim(ylim)

plt.subplot(133)                                              # noise prior distribution
plt.plot(np.arange(0,10,0.01),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[2,0],scale=prior[2,1]),c='red'); ylim = plt.gca().get_ylim() 
plt.fill_between(np.arange(0,10,0.01),np.full(1000,-1.0e20),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[2,0],scale=prior[2,1]),color='black',alpha=0.05,zorder=1)
plt.xlabel(r'$\sigma$'); plt.ylabel('Natural Log Density'); plt.title("Gamma Prior Distribution, $\sigma$"); add_grid(); plt.xlim([0,10]); plt.ylim(ylim)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.35, hspace=0.5); plt.show() 
```

![å›¾ç‰‡](img/d6b7bfe8279df066ea241de8c0e3a483.png)

### è´å¶æ–¯çº¿æ€§å›å½’ä¸ MCMC Metropolis-Hastings

è´å¶æ–¯çº¿æ€§å›å½’ä¸ MCMC Metropolis-Hastings å·¥ä½œæµç¨‹ã€‚

1.  åˆ†é…ä¸€ä¸ªéšæœºçš„åˆå§‹æ¨¡å‹å‚æ•°é›†

1.  åº”ç”¨ä¸€ä¸ªæè®®è§„åˆ™ï¼Œæ ¹æ®å…ˆå‰çš„ä¸€ç»„æ¨¡å‹å‚æ•°åˆ†é…ä¸€ç»„æ–°çš„æ¨¡å‹å‚æ•°

1.  è®¡ç®—ç»™å®šæ•°æ®çš„æ–°æ¨¡å‹å‚æ•°ç›¸å¯¹äºå…ˆå‰æ¨¡å‹å‚æ•°çš„ä¼¼ç„¶æ¯”

1.  æ ¹æ®è¿™ä¸ªæ¯”ç‡æœ‰æ¡ä»¶åœ°æ¥å—æè®®ï¼Œå³å¦‚æœæè®®æ›´æ¥è¿‘æ¥å—ï¼Œåˆ™æ ¹æ®æ¯”ç‡è®¡ç®—çš„æ¦‚ç‡è¾ƒå°ã€‚

1.  è¿”å›åˆ°æ­¥éª¤ 2ã€‚

```py
np.random.seed(seed = seed)
step_stdev = 0.2

thetas = np.random.rand(3).reshape(1,-1)                      # seed a random first step
accepted = 0

n = 10000                                                     # number of attempts, include accepted and rejected

for i in range(n):
    theta_new = next_proposal(thetas[-1,:],step_stdev=step_stdev) # next proposal

    log_like_new = likelihood_density(X,y,theta_new)          # new and prior likelihoods, log of density
    log_like = likelihood_density(X,y,thetas[-1,:])

    log_prior_new = prior_density(theta_new,prior)            # new and prior, log of density
    log_prior = prior_density(thetas[-1,:],prior)

    likelihood_prior_proposal_ratio = np.exp((log_like_new + log_prior_new) - (log_like + log_prior)) # calculate log ratio

    if likelihood_prior_proposal_ratio > np.random.rand(1):   # conditionally accept by likelihood ratio
        thetas = np.vstack((thetas,theta_new)); accepted += 1

print('Accepted ' + str(accepted) + ' samples of ' + str(n) + ' attempts')

df = pd.DataFrame(np.vstack([thetas[:,0],thetas[:,1],thetas[:,2]]).T, columns= ['Slope','Intercept','Sigma']) 
```

```py
Accepted 1603 samples of 10000 attempts 
```

### å¯è§†åŒ–ç»“æœ

è®©æˆ‘ä»¬å¯è§†åŒ–æ¯ä¸ªæ¨¡å‹å‚æ•°çš„ Metropolis é‡‡æ ·é“¾ã€‚

+   æ³¨æ„åˆ°çƒ§å½•å’Œå¹³è¡¡é“¾ã€‚

+   å°†æ–œç‡å’Œæˆªè·æ¨¡å‹å‚æ•°ä¸ä»…åŸºäºæ™®é€šæœ€å°äºŒä¹˜è§£çš„é¢‘ç‡ä¸»ä¹‰çº¿æ€§å›å½’è§£è¿›è¡Œæ¯”è¾ƒï¼Œæœ€å°åŒ–æ‰€æœ‰æ ·æœ¬æ•°æ®ä¸­è¯¯å·®å‘é‡çš„ L2 èŒƒæ•°ã€‚

+   å°†å™ªå£°å‚æ•°ä¸æ·»åŠ åˆ°åˆæˆæ•°æ®ä¸­çš„å™ªå£°è¿›è¡Œæ¯”è¾ƒã€‚

```py
alpha = 0.1                                                   # alpha level for displayed confidence intervals 

plt.subplot(131)                                              # plot slope, b_1, samples
plt.plot(np.arange(0,accepted+1,1),thetas[:,0],c='red',zorder=100) 
plt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$b_1$'); plt.title("McMC Bayesian Linear Regression Slope"); add_grid()
plt.plot([0,accepted],[linear_model.coef_[0],linear_model.coef_[0]],c='blue',zorder=200)
plt.annotate('Linear Regression',[30,linear_model.coef_[0]+0.05],color='blue',zorder=200)
plt.plot([0,accepted],[prior[0,0],prior[0,0]],c='black',zorder=10)
plt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[0,0]+0.07])
lower = stats.norm.ppf(alpha/2.0,loc=prior[0,0],scale=prior[0,1])
plt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)
upper = stats.norm.ppf(1-alpha/2.0,loc=prior[0,0],scale=prior[0,1])
plt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)
plt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)
plt.xlim([0,accepted]); plt.ylim([0,10])

plt.subplot(132)                                              # plot intercept, b_0, samples
plt.plot(np.arange(0,accepted+1,1),thetas[:,1],c='red',zorder=100) 
plt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$b_0$'); plt.title("McMC Bayesian Linear Regression Intercept"); add_grid()
plt.plot([0,accepted],[linear_model.intercept_,linear_model.intercept_],c='blue',zorder=200)
plt.annotate('Linear Regression',[30,linear_model.intercept_+0.25],color='blue',zorder=200)
plt.plot([0,accepted],[prior[1,0],prior[1,0]],c='black',zorder=10)
plt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[1,0]+0.07])
lower = stats.norm.ppf(alpha/2.0,loc=prior[1,0],scale=prior[1,1])
plt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)
upper = stats.norm.ppf(1-alpha/2.0,loc=prior[1,0],scale=prior[1,1])
plt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)
plt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)
plt.xlim([0,accepted]); plt.ylim([0,30])

plt.subplot(133)                                              # plot noise, sigma, samples
plt.plot(np.arange(0,accepted+1,1),thetas[:,2],c='red',zorder=100) 
plt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$\sigma$'); plt.title("McMC Bayesian Linear Regression Sigma"); add_grid()
plt.plot([0,accepted],[data_sigma,data_sigma],color='blue',zorder=200)
plt.annotate('Synthetic Data Noise',[30,data_sigma+0.06],color='blue',zorder=200)
plt.plot([0,accepted],[prior[2,0],prior[2,0]],c='black',zorder=10)
plt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[2,0]+0.07])
lower = stats.norm.ppf(alpha/2.0,loc=prior[2,0],scale=prior[2,1])
plt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)
upper = stats.norm.ppf(1-alpha/2.0,loc=prior[2,0],scale=prior[2,1])
plt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)
plt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)
plt.xlim([0,accepted]); plt.ylim([0,10])

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.2, wspace=0.2, hspace=0.5); plt.show() 
```

![å›¾ç‰‡](img/0afb4e1bf5f96613b6f86806ca52ebc3.png)

### åœ¨æ¨¡å‹å‚æ•°ç©ºé—´ä¸­å¯è§†åŒ–é“¾

å°†çƒ§é“¾è®¾ç½®ä¸ºçƒ§é“¾ç»“æŸæ—¶çš„æ¥å—æ ·æœ¬ï¼Œå¹¶è§‚å¯Ÿäº† MCMC æŠ½æ ·ã€‚

```py
burn_chain = 250
plt.scatter(thetas[:burn_chain,0],thetas[:burn_chain,1],s=5,marker = 'x',c='black',alpha=0.8,linewidth=0.3,cmap=plt.cm.inferno,zorder=10)
plt.scatter(thetas[burn_chain:,0],thetas[burn_chain:,1],s=5,c=np.arange(burn_chain,accepted+1,1),alpha=1.0,edgecolor='black',linewidth=0.1,cmap=plt.cm.inferno,zorder=10)
sns.kdeplot(data=df[burn_chain:],x='Slope',y='Intercept',color='grey',linewidths=1.00,alpha=0.9,levels=np.logspace(-4,-0,3),zorder=100)
plt.plot(thetas[:burn_chain,0],thetas[:burn_chain,1],color='black',linewidth=0.1,zorder=1)
add_grid(); plt.xlabel('Slope, $b_1$'); plt.ylabel('Intercept, $b_0$'); plt.title('McMC Metropolis Samples Bayesian Linear Regression') 
plt.xlim([0,5]); plt.ylim([0,25])
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8, wspace=0.2, hspace=0.5); plt.show() 
```

![å›¾ç‰‡](img/ad916426750d37902655104ee4efeeb0.png)

## è¯„è®º

è¿™æ˜¯å¯¹è´å¶æ–¯çº¿æ€§å›å½’çš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„ YouTube è®²åº§é“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![å›¾ç‰‡](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œåœ¨é‚£é‡Œä»–ç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„è´Ÿè´£äººï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚

+   [è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº† 70 å¤šç¯‡åŒè¡Œè¯„å®¡çš„å‡ºç‰ˆç‰©ï¼ˆ[åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼‰ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[åœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[Python ä¸­çš„åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[Python ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå®è·µæŒ‡å—ä¸ä»£ç ](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å« 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ª GitHub ä»“åº“ä¸­çš„è¯¦ç»†å·¥ä½œæµç¨‹é“¾æ¥ï¼Œè¿™äº›ä»“åº“ä½äºä»–çš„[GitHub è´¦æˆ·](https://github.com/GeostatsGuy)ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›æŒç»­æ›´æ–°çš„å†…å®¹ã€‚æƒ³äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³è¦ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™ä»½å†…å®¹å¯¹é‚£äº›æƒ³è¦äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æä»¥åŠæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚ä¸ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°æˆ‘ã€‚

æˆ‘æ€»æ˜¯ä¹äºè®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼Œæ³¨å†Œå·¥ç¨‹å¸ˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢æ•™æˆ

æ›´å¤šèµ„æºè¯·è®¿é—®ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
