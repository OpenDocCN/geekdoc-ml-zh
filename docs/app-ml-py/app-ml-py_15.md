# ä¸»æˆåˆ†åˆ†æ

> åŸæ–‡ï¼š[`geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_PCA.html`](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_PCA.html)

Michael J. Pyrczï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡æ•™æˆ

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

æœ¬ä¹¦æ˜¯ç”µå­ä¹¦â€œPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œâ€çš„ä¸€ç« ã€‚

è¯·å°†æ­¤ç”µå­ä¹¦å¼•ç”¨å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *Python åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œ* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138 ![DOI](https://doi.org/10.5281/zenodo.15169138)

æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠå…¶ä»–å·¥ä½œæµç¨‹éƒ½å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š

è¯·å°† MachineLearningDemos GitHub ä»“åº“å¼•ç”¨å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *MachineLearningDemos: Python æœºå™¨å­¦ä¹ æ¼”ç¤ºå·¥ä½œæµç¨‹ä»“åº“* (0.0.3) [è½¯ä»¶]. Zenodo. DOI: 10.5281/zenodo.13835312\. GitHub ä»“åº“ï¼š[GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos) ![DOI](https://zenodo.org/doi/10.5281/zenodo.13835312)

ä½œè€…ï¼šMichael J. Pyrcz

Â© ç‰ˆæƒæ‰€æœ‰ 2024ã€‚

æœ¬ç« æ˜¯å…³äº/æ¼”ç¤º**ä¸»æˆåˆ†åˆ†æ**çš„æ•™ç¨‹ã€‚

**YouTube è®²åº§**ï¼šæŸ¥çœ‹æˆ‘åœ¨ä»¥ä¸‹æ–¹é¢çš„è®²åº§ï¼š

+   [æœºå™¨å­¦ä¹ ç®€ä»‹](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)

+   [ç»´åº¦è¯…å’’ã€é™ç»´ã€ä¸»æˆåˆ†åˆ†æ](https://youtu.be/-to3JXiae9Y?si=W1j2CwR9t0t8hxIB)

+   [å¤šç»´å°ºåº¦åˆ†æå’ŒéšæœºæŠ•å½±](https://youtu.be/Yt0o8ukIOKU?si=_ri1NPwKVdhYzgO3)

è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘ YouTube ä¸Šçš„[æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«æœ‰è‰¯å¥½æ–‡æ¡£è®°å½•çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºè·å–ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœä½ æƒ³çŸ¥é“æˆ‘çš„åŠ¨åŠ›ï¼Œè¯·æŸ¥çœ‹[Michael çš„æ•…äº‹](https://michaelpyrcz.com/my-story)ã€‚

## ä¸»æˆåˆ†åˆ†æçš„åŠ¨åŠ›

ä¸æ›´å¤šç‰¹å¾/å˜é‡ä¸€èµ·å·¥ä½œæ›´å›°éš¾ï¼

1.  æ›´éš¾å¯è§†åŒ–å’Œæ¨¡å‹æ•°æ®

1.  éœ€è¦æ›´å¤šæ•°æ®æ¥æ¨æ–­è”åˆæ¦‚ç‡

1.  ç‰¹å¾ç©ºé—´çš„æ•°æ®è¦†ç›–èŒƒå›´æ›´å°‘

1.  æ›´éš¾å¯¹æ¨¡å‹è¿›è¡Œè´¨è¯¢/æ£€æŸ¥

1.  æ›´å¯èƒ½å­˜åœ¨å†—ä½™ç‰¹å¾ï¼Œä¾‹å¦‚å¤šé‡å…±çº¿æ€§ï¼Œå¯¼è‡´æ¨¡å‹ä¸ç¨³å®š

1.  éœ€è¦æ›´å¤šçš„è®¡ç®—åŠªåŠ›ã€æ›´å¤šçš„è®¡ç®—èµ„æºå’Œæ›´é•¿çš„è¿è¡Œæ—¶é—´

1.  æ›´å¤æ‚çš„æ¨¡å‹æ›´æœ‰å¯èƒ½è¿‡æ‹Ÿåˆ

1.  æ¨¡å‹æ„å»ºéœ€è¦æ›´å¤šä¸“ä¸šæ—¶é—´

æˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ›´å°‘ã€æ›´æœ‰ä¿¡æ¯é‡çš„ç‰¹å¾æ¥è·å¾—æ›´å¥½çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯å°†æ‰€æœ‰ç‰¹å¾éƒ½æŠ•å…¥æ¨¡å‹ï¼è¿™éƒ¨åˆ†åŠ¨æœºå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±ç»´åº¦ç¾éš¾é©±åŠ¨çš„ã€‚

## ç»´åº¦ç¾éš¾

1.  **æ•°æ®å’Œæ¨¡å‹å¯è§†åŒ–** - æˆ‘ä»¬æ— æ³•å¯è§†åŒ–è¶…è¿‡ä¸‰ç»´ï¼Œå³æ— æ³•è®¿é—®æ•°æ®æ‹Ÿåˆæ¨¡å‹ï¼Œè¯„ä¼°å†…æ’ä¸å¤–æ¨ã€‚

+   è€ƒè™‘ä¸€ä¸ª 5 ç»´ç¤ºä¾‹ï¼Œä»¥çŸ©é˜µæ•£ç‚¹å›¾çš„å½¢å¼å±•ç¤ºï¼Œå³ä½¿åœ¨è¿™ç§æƒ…å†µä¸­ï¼Œæ¯ä¸ªå›¾ä¹Ÿæç«¯åœ°è¾¹ç¼˜åŒ–åˆ°äºŒç»´ï¼Œ

![](img/ecf50f66114aec17ea35fde1342d66c4.png)

5 ç»´æ•°æ®çš„ç¤ºä¾‹ï¼Œä»¥çŸ©é˜µæ•£ç‚¹å›¾çš„å½¢å¼å±•ç¤ºã€‚

1.  **é‡‡æ ·** - è¶³å¤Ÿçš„æ ·æœ¬æ•°é‡ä»¥æ¨æ–­è”åˆæ¦‚ç‡ç­‰ç»Ÿè®¡é‡ï¼Œ$P(x_1,\ldots,x_m)$ã€‚

+   å›å¿†ä¸€ä¸‹ç›´æ–¹å›¾æˆ–å½’ä¸€åŒ–ç›´æ–¹å›¾çš„è®¡ç®—ï¼šæˆ‘ä»¬å»ºç«‹ç®±å­å¹¶è®¡ç®—æ¯ä¸ªç®±å­ä¸­çš„é¢‘ç‡æˆ–æ¦‚ç‡ã€‚

+   æˆ‘ä»¬éœ€è¦æ¯ä¸ªç®±å­ä¸€ä¸ªåä¹‰ä¸Šçš„æ•°æ®æ ·æœ¬æ•°ï¼Œå› æ­¤åœ¨ä¸€ç»´ç©ºé—´ä¸­æˆ‘ä»¬éœ€è¦$ğ‘›=ğ‘›_{ğ‘ /ğ‘ğ‘–ğ‘›} \cdot ğ‘›_{ğ‘ğ‘–ğ‘›ğ‘ }$ä¸ªæ ·æœ¬ã€‚

+   ä½†åœ¨ mD ä¸­ï¼Œæˆ‘ä»¬éœ€è¦$n$ä¸ªæ ·æœ¬æ¥è®¡ç®—ç¦»æ•£åŒ–çš„è”åˆæ¦‚ç‡ï¼Œ

$$ ğ‘›=ğ‘›_{ğ‘ /ğ‘ğ‘–ğ‘›} \cdot ğ‘›_{ğ‘ğ‘–ğ‘›ğ‘ }^m $$

+   ä¾‹å¦‚ï¼Œæ¯ä¸ªç®±å­ 10 ä¸ªæ ·æœ¬ï¼Œå…±æœ‰ 35 ä¸ªç®±å­ï¼Œåœ¨äºŒç»´ç©ºé—´ä¸­éœ€è¦ 12,250 ä¸ªæ ·æœ¬ï¼Œåœ¨ä¸‰ç»´ç©ºé—´ä¸­éœ€è¦ 428,750 ä¸ªæ ·æœ¬

![](img/bc8823819263f4497ef6baab93a9ee38.png)

æ¯ä¸ªç‰¹å¾æœ‰ 35 ä¸ªç®±å­çš„ 2 ç»´æ•°æ®ç¤ºä¾‹ã€‚

1.  **æ ·æœ¬è¦†ç›–ç‡** - æ ·æœ¬å€¼èŒƒå›´è¦†ç›–é¢„æµ‹ç‰¹å¾ç©ºé—´ã€‚

+   æ ·æœ¬ç©ºé—´ä¸­å¯èƒ½è§£çš„åˆ†æ•°ï¼Œå¯¹äº 1 ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬å‡è®¾è¦†ç›–ç‡ä¸º 80%

+   è®°ä½ï¼Œæˆ‘ä»¬é€šå¸¸åªç›´æ¥é‡‡æ ·åœ°ä¸‹ä½“ç§¯çš„$\frac{1}{10â·}$ã€‚

+   æ˜¯çš„ï¼Œè¦†ç›–ç‡çš„è§‚å¿µæ˜¯ä¸»è§‚çš„ï¼Œéœ€è¦è¦†ç›–å¤šå°‘æ•°æ®ï¼Ÿå…³äºé—´éš™å‘¢ï¼Ÿç­‰ç­‰ã€‚

![](img/d8058511a88a482ed34b0cbd9eb34fec.png)

æ¯ä¸ªç‰¹å¾æœ‰ 35 ä¸ªç®±å­çš„ 2 ç»´æ•°æ®ç¤ºä¾‹ã€‚

+   ç°åœ¨å¦‚æœæœ‰ä¸¤ä¸ªç‰¹å¾å„æœ‰ 80%çš„è¦†ç›–ç‡ï¼ŒäºŒç»´è¦†ç›–ç‡æ˜¯ 64%

![](img/8d96453b3f6c2a92a160fe4329a13d4a.png)

æ¯ä¸ªç‰¹å¾æœ‰ 35 ä¸ªç®±å­çš„ 2 ç»´æ•°æ®ç¤ºä¾‹ã€‚

+   è¦†ç›–ç‡æ˜¯ï¼Œ

$$ c = c_1^m $$

1.  **æ‰­æ›²ç©ºé—´** - é«˜ç»´ç©ºé—´æ˜¯æ‰­æ›²çš„ã€‚

+   è®¡ç®—è¶…ç«‹æ–¹ä½“å†…åµŒè¶…çƒä½“çš„ä½“ç§¯æ¯”ï¼Œ

$$ \frac{\pi^{\frac{m}{2}}}{m 2^{m-1} \Gamma\left(\frac{m}{2}\right)} \to 0 \quad \text{as} \quad m \to \infty $$

+   å›å¿†ä¸€ä¸‹ï¼Œ$\Gamma(ğ‘›)=(ğ‘›âˆ’1)!$ã€‚

+   é«˜ç»´ç©ºé—´å…¨æ˜¯è§’è€Œæ²¡æœ‰ä¸­é—´éƒ¨åˆ†ï¼Œå¤§éƒ¨åˆ†é«˜ç»´ç©ºé—´ç¦»ä¸­é—´éƒ¨åˆ†å¾ˆè¿œï¼ˆå…¨æ˜¯è§’ï¼ï¼‰ã€‚

+   ç»“æœï¼Œé«˜ç»´ç©ºé—´ä¸­çš„è·ç¦»å¤±å»äº†æ•æ„Ÿæ€§ï¼Œå³å¯¹äºç©ºé—´ä¸­çš„ä»»ä½•éšæœºç‚¹ï¼ŒæœŸæœ›çš„æˆå¯¹è·ç¦»éƒ½å˜å¾—ç›¸åŒï¼Œ

$$ \lim_{m \to \infty} \left( \mathbb{E}\left[\text{dist}_{\text{max}}(m) - \text{dist}_{\text{min}}(m)\right] \right) \to 0 $$

+   åœ¨è¶…ç©ºé—´ä¸­éšæœºç‚¹å¯¹è·ç¦»èŒƒå›´çš„æœŸæœ›æé™è¶‹äºé›¶ã€‚å¦‚æœè·ç¦»å‡ ä¹éƒ½ç›¸åŒï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»å°±ä¸å†æœ‰æ„ä¹‰äº†ï¼

![](img/8c8d512cca4eb330150d1ba298831543.png)

è¶…ç«‹æ–¹ä½“å†…è¶…çƒä½“çš„ä½“ç§¯æ¯”ã€‚

+   è¿™é‡Œæ˜¯å„ç§ç»´åº¦ä¸‹æ‰­æ›²çš„ä¸¥é‡ç¨‹åº¦ï¼Œ

| m | nD / 2D |
| --- | --- |
| 2 | 1.0 |
| 5 | 0.28 |
| 10 | 0.003 |
| 20 | 0.00000003 |

1.  **å¤šé‡å…±çº¿æ€§** - é«˜ç»´æ•°æ®é›†æ›´å¯èƒ½å‡ºç°å…±çº¿æ€§æˆ–å¤šé‡å…±çº¿æ€§ã€‚

+   ç”±å…¶ä»–ç‰¹å¾çº¿æ€§æè¿°çš„ç‰¹å¾å¯¼è‡´æ¨¡å‹æ–¹å·®é«˜ã€‚

## æ¨æ–­æ€§æœºå™¨å­¦ä¹ 

ä¸»æˆåˆ†åˆ†ææ˜¯ä¸€ç§æ¨æ–­æ€§ã€æ— ç›‘ç£çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚

+   æ²¡æœ‰å“åº”ç‰¹å¾ï¼Œ$y$ï¼Œåªæœ‰é¢„æµ‹ç‰¹å¾ï¼Œ

$$ ğ‘‹_1,\ldots,ğ‘‹_ğ‘š $$

+   é€šè¿‡æ¨¡ä»¿æ•°æ®çš„ç´§å‡‘è¡¨ç¤ºæ¥å­¦ä¹ æœºå™¨

+   é€šè¿‡æ•æ‰ç‰¹å¾æŠ•å½±ã€åˆ†ç»„åˆ†é…ã€ç¥ç»ç½‘ç»œæ½œåœ¨ç‰¹å¾ç­‰æ¨¡å¼

+   æˆ‘ä»¬ä¸“æ³¨äºå¯¹äººç¾¤ã€è‡ªç„¶ç³»ç»Ÿçš„æ¨æ–­ï¼Œè€Œä¸æ˜¯å¯¹å“åº”ç‰¹å¾çš„é¢„æµ‹ã€‚

## ä¸»æˆåˆ†åˆ†æ

ä¸»æˆåˆ†åˆ†ææ˜¯å¤šç§é™ç»´æ–¹æ³•ä¹‹ä¸€ï¼š

é™ç»´å°†æ•°æ®è½¬æ¢åˆ°è¾ƒä½ç»´åº¦

+   ç»™å®šç‰¹å¾ï¼Œ$ğ‘‹_1,\dots,ğ‘‹_ğ‘š$ï¼Œæˆ‘ä»¬éœ€è¦ ${m \choose 2}=\frac{ğ‘š \cdot (ğ‘šâˆ’1)}{2}$ ä¸ªæ•£ç‚¹å›¾æ¥å¯è§†åŒ–äºŒç»´æ•£ç‚¹å›¾ã€‚

+   ä¸€æ—¦æˆ‘ä»¬æœ‰ 4 ä¸ªæˆ–æ›´å¤šå˜é‡ï¼Œç†è§£æ•°æ®å°±å˜å¾—éå¸¸å›°éš¾ã€‚

+   å›å¿†ç»´åº¦è¯…å’’ï¼Œå®ƒå½±å“æ¨æ–­ã€å»ºæ¨¡å’Œå¯è§†åŒ–ã€‚

ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯æ‰¾åˆ°ä¸€ä¸ªå¥½çš„ä½ç»´ $ğ‘$ è¡¨ç¤ºï¼Œä»¥è¡¨ç¤ºåŸå§‹ç»´åº¦ $ğ‘š$

åœ¨é™ç»´è¡¨ç¤ºä¸­çš„å¥½å¤„ï¼š

1.  æ•°æ®å­˜å‚¨/è®¡ç®—æ—¶é—´

1.  æ›´å®¹æ˜“å¯è§†åŒ–

1.  è¿˜å¤„ç†äº†å¤šé‡å…±çº¿æ€§

## æ­£äº¤å˜æ¢

å°†ä¸€ç»„è§‚æµ‹å€¼è½¬æ¢ä¸ºä¸€ç»„çº¿æ€§ä¸ç›¸å…³çš„å˜é‡ï¼Œç§°ä¸ºä¸»æˆåˆ†

+   å¯ç”¨çš„ä¸»æˆåˆ†æ•° ($k$) ä¸º minâ¡($ğ‘›âˆ’1,ğ‘š$)

+   å—å˜é‡/ç‰¹å¾ $ğ‘š$ å’Œæ•°æ®æ•°é‡çš„é™åˆ¶

ç»„ä»¶æŒ‰é¡ºåºæ’åˆ—ï¼Œ

+   ç¬¬ä¸€ä¸ªæˆåˆ†æè¿°äº†æœ€å¤§çš„å¯èƒ½æ–¹å·® / å°½å¯èƒ½è§£é‡Šæœ€å¤šçš„å˜å¼‚æ€§

+   ä¸‹ä¸€ä¸ªæˆåˆ†æè¿°äº†æœ€å¤§çš„å¯èƒ½å‰©ä½™æ–¹å·®

+   æœ€å¤šåˆ°æœ€å¤§ä¸»æˆåˆ†æ•°

å¯¹ä¸»æˆåˆ†åˆ†ææœ‰å¤šç§è§£é‡Šæ–¹å¼ï¼Œ

## æœ€ä½³æ‹Ÿåˆè§£é‡Š

æœ€å°åŒ–æ•°æ®ä¸ä¸»æˆåˆ†ä¹‹é—´çš„æ­£äº¤æŠ•å½±è¯¯å·®ï¼Œ

$$ \min \sum_{i=1}^{n} \left( \left( X_i - \bar{X} \right) - \left( X_i - \bar{X} \right) V_p V_p^T \right)Â² $$

å…¶ä¸­ $ğ‘½_ğ’‘$ æ˜¯æˆ‘ä»¬å‰ $ğ’‘$ ä¸ªå‘é‡çš„çŸ©é˜µï¼Œ$ğ‘¿_ğ’Š$ æ˜¯æ ·æœ¬ $ğ‘–$ åœ¨æ‰€æœ‰ $ğ‘$ ä¸ªç‰¹å¾ä¸Šçš„å‘é‡ï¼Œ$\overline{X}$ æ˜¯å‡å€¼å‘é‡ï¼Œ

![](img/c9c37a5643c5eca21190ee3fa4c30880.png)

å°†äºŒç»´æ•°æ®æŠ•å½±åˆ°ä¸€ç»´ï¼ˆå·¦ï¼‰å’Œä¸‰ç»´æ•°æ®æŠ•å½±åˆ°äºŒç»´ï¼ˆå³ï¼‰çš„æ­£äº¤è¯¯å·®ï¼ˆå¾…æ·»åŠ å¼•ç”¨ï¼‰ã€‚

å…¶ä¸­ä¸»æˆåˆ†æè¿°äº†ä¸€ç»´ä¸­çš„å‘é‡å’Œå¹³é¢ä¸­çš„äºŒç»´ï¼Œä»¥åŠæŠ•å½±ç©ºé—´ä¸­çš„ä¸»æˆåˆ†å¾—åˆ†ï¼Œ

$$ (ğ‘¿_ğ’Šâˆ’\overline{ğ‘¿})ğ‘½_ğ’‘ $$

å¹¶ä¸”åœ¨åŸå§‹ç©ºé—´ä¸­å…·æœ‰é™ä½ç»´åº¦çš„åå˜æ¢æ˜¯ï¼Œ

$$ (X_i-\overline{X})V_p V_p^T $$

æ³¨æ„ï¼Œç»™å®š $V_p$ çŸ©é˜µæ˜¯æ­£äº¤çš„ï¼Œ

$$ V_p^T = V_p^{-1} $$

## åŸºäºæ—‹è½¬çš„è§£é‡Š

æ­£äº¤å˜æ¢æ˜¯ä¸€ç§æ—‹è½¬ï¼Œå®ƒæœ€å¤§åŒ–äº†ç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®ï¼Œæœ€å¤§åŒ–ç¬¬äºŒä¸»æˆåˆ†çš„å‰©ä½™æ–¹å·®ï¼Œç­‰ç­‰ã€‚

å¦‚æœä½ æƒ³çœ‹åˆ° PCA ä½œä¸ºæ—‹è½¬çš„å®é™…æ“ä½œï¼Œè¯·æŸ¥çœ‹æˆ‘çš„[PCA æ—‹è½¬äº¤äº’å¼ Python ä»ªè¡¨æ¿](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_PCA_Rotation.ipynb)ï¼Œ

![](img/e1292179f35c2427c0914445105c302d.png)

æˆ‘çš„äº¤äº’å¼ä»ªè¡¨æ¿å±•ç¤ºäº† PCA ä½œä¸ºæ•°æ®çš„æ—‹è½¬ã€‚

ä»è¿™ä¸ªä»ªè¡¨æ¿ä¸­å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œæœ‰ä¸€ä¸ªæ—‹è½¬æœ€å¤§åŒ–äº†ç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®ï¼ŒåŒæ—¶æ¶ˆé™¤äº†ç¬¬ä¸€å’Œç¬¬äºŒä¸»æˆåˆ†ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

## ç‰¹å¾å€¼ / ç‰¹å¾å‘é‡è§£é‡Š

å¯¹äºä¸»æˆåˆ†åˆ†æï¼Œæˆ‘ä»¬è®¡ç®—æ•°æ®åæ–¹å·®çŸ©é˜µï¼Œç‰¹å¾çš„ç»„åˆçš„æˆå¯¹åæ–¹å·®ã€‚

+   æˆ‘ä»¬ä»åæ–¹å·®çŸ©é˜µä¸­è®¡ç®—ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ã€‚

+   ç‰¹å¾å€¼æ˜¯æ¯ä¸ªç»„ä»¶è§£é‡Šçš„æ–¹å·®ã€‚

+   æ•°æ®åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡æ˜¯ä¸»æˆåˆ†ã€‚

## ä¸»æˆåˆ†åˆ†æå·¥ä½œæµç¨‹

1.  æ ‡å‡†åŒ–ç‰¹å¾

$$ X^s=\frac{X-\overline{X}}{\sigma_X} $$$$ X_1,\ldots,X_m \quad \rightarrow X_1^s,\ldots,X_m^s $$

```py
where $X_i$ are original features and $X^s_i$ are transformed features. 
```

+   æ ‡å‡†åŒ–æ˜¯å¿…è¦çš„ï¼Œä»¥é˜²æ­¢å…·æœ‰è¾ƒå¤§æ–¹å·®çš„ç‰¹å¾ä¸»å¯¼è§£å†³æ–¹æ¡ˆï¼Œå³ç¬¬ä¸€ä¸»æˆåˆ†ä¸æ–¹å·®æœ€å¤§çš„ç‰¹å¾å¯¹é½

1.  è®¡ç®—æ ‡å‡†åŒ–çš„ç‰¹å¾åæ–¹å·®çŸ©é˜µ

$$ C_{(X_{m_1}, X_{m_2})} = \frac{\sum_{i=1}^{n} \left( (x_{m_1} - \bar{x}_{m_1})(x_{m_2} - \bar{x}_{m_2}) \right)}{n - 1} $$

```py
given the features are standardized the matrix is a correlation matrix 
```

$$\begin{split} C = \begin{bmatrix} C(X_1, X_1) & \cdots & C(X_1, X_m) \\ \vdots & \ddots & \vdots \\ C(X_m, X_1) & \cdots & C(X_m, X_m) \end{bmatrix} \end{split}$$

```py
given the features are standardized the matrix is a correlation matrix, 
```

$$\begin{split} C = \begin{bmatrix} \rho(X_1, X_1) & \cdots & \rho(X_1, X_m) \\ \vdots & \ddots & \vdots \\ \rho(X_m, X_1) & \cdots & \rho(X_m, X_m) \end{bmatrix} \end{split}$$

1.  è®¡ç®—åæ–¹å·®çŸ©é˜µ $C$ çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œ

    ç»™å®š $C$ æ˜¯ä¸€ä¸ªæ–¹é˜µ $(m \times m)$ï¼Œ$v (m \times 1)$ æ˜¯ä¸€ä¸ªå‘é‡ï¼Œ$\lambda$ æ˜¯ä¸€ä¸ªæ ‡é‡ ($1$)ï¼Œ

$$ Cv=\lambda v $$

```py
we can reorder to, 
```

$$ (C- \lambda \cdot I)âˆ™v=0 $$

```py
where $I$ is an identity matrix. By Cramerâ€™s rule, we have a solution if the determinant is 0, 
```

$$ |C- \lambda \cdot I|=0 $$

```py
find the possible Eigenvalues, $\lambda_ğ›¼$, and solve for eigenvectors, $ğ’—_ğœ¶, \quad \alpha=ğŸ,\ldots,ğ’$ 
```

+   ç»“æœåœ¨çŸ©é˜µ $V_m$ ä¸­æœ‰ $\text{min}(m,n-1)$ ä¸ªç‰¹å¾å‘é‡

![](img/149885b8478ebe255e67e3781a68b054.png)

ç‰¹å¾å‘é‡ä½œä¸ºä¸»æˆåˆ†ã€‚

```py
that form a basis on which the data are projected for dimensionality reduction, 
```

![](img/99275c247c63e53876ec6c9dd844b7b9.png)

ç‰¹å¾å‘é‡ä½œä¸ºå®šä¹‰æ–°æ—‹è½¬åŸºçš„ä¸»æˆåˆ†ã€‚

å¦‚æœä½ æƒ³æŸ¥çœ‹ä¸»æˆåˆ†åŠ è½½å’Œæˆåˆ†ä¹‹é—´çš„æ–¹å·®åˆ†é…ï¼Œè¯·æŸ¥çœ‹æˆ‘çš„[PCA åŠ è½½äº¤äº’å¼ Python ä»ªè¡¨æ¿](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_PCA_Eigen.ipynb),

![](img/8a7fbf602c24c192b7d999a9a3faaf43.png)

æˆ‘çš„äº¤äº’å¼ä»ªè¡¨æ¿å±•ç¤ºäº† PCA åŠ è½½å’Œæ–¹å·®è§£é‡Šï¼Œæ¯ä¸ªä¸»æˆåˆ†çš„ç›¸å…³æ€§å˜åŒ–éƒ½åœ¨ç‰¹å¾ 1ã€2 å’Œ 3 ä¹‹é—´ã€‚

## åŠ è½½æ‰€éœ€çš„åº“

ä»¥ä¸‹ä»£ç åŠ è½½æ‰€éœ€çš„åº“ã€‚è¿™äº›åº“åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
ignore_warnings = True                                        # ignore warnings?
from sklearn.preprocessing import MinMaxScaler                # min/max normalization
from sklearn.decomposition import PCA                         # PCA program from scikit learn (package for machine learning)
from sklearn.preprocessing import StandardScaler              # standardize variables to mean of 0.0 and variance of 1.0
import numpy as np                                            # ndarrays for gridded data
import pandas as pd                                           # DataFrames for tabular data
import pandas.plotting as pd_plot                             # pandas plotting functions
import copy                                                   # for deep copies
import os                                                     # set working directory, run executables
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks
import matplotlib.ticker as mtick                             # control tick label formatting
from matplotlib.ticker import PercentFormatter                # percentage axis label formatting
import seaborn as sns                                         # advanced plotting
plt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements
if ignore_warnings == True:                                   
    import warnings
    warnings.filterwarnings('ignore')
cmap = plt.cm.inferno                                         # color map
seed = 42                                                     # random number seed 
```

å¦‚æœä½ é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½éœ€è¦é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æ›´å¤šå¸®åŠ©å¯ä»¥åœ¨ç›¸åº”åŒ…çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå•ç‹¬çš„å‡½æ•°æ¥ç®€åŒ–ç»˜åˆ¶ç›¸å…³çŸ©é˜µã€‚æˆ‘è¿˜æ·»åŠ äº†ä¸€ä¸ªæ–¹ä¾¿çš„å‡½æ•°æ¥æ·»åŠ ä¸»ç½‘æ ¼çº¿å’Œå‰¯ç½‘æ ¼çº¿ï¼Œä»¥æé«˜ç»˜å›¾çš„å¯è§£é‡Šæ€§ã€‚

```py
def plot_corr(df,size=10):                                    # plots a graphical correlation matrix 
    from matplotlib.colors import ListedColormap              # make a custom colormap
    my_colormap = plt.cm.get_cmap('RdBu_r', 256)          
    newcolors = my_colormap(np.linspace(0, 1, 256))
    white = np.array([256/256, 256/256, 256/256, 1])
    newcolors[65:191, :] = white                              # mask all correlations less than abs(0.8)
    newcmp = ListedColormap(newcolors)
    m = len(df.columns)
    corr = df.corr()
    fig, ax = plt.subplots(figsize=(size, size))
    im = ax.matshow(corr,vmin = -1.0, vmax = 1.0,cmap = newcmp)
    plt.xticks(range(len(corr.columns)), corr.columns);
    plt.yticks(range(len(corr.columns)), corr.columns);
    plt.colorbar(im, orientation = 'vertical')
    plt.title('Correlation Matrix')
    for i in range(0,m):
        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')
        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')
    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def add_grid2(sub_plot):
    sub_plot.grid(True, which='major',linewidth = 1.0); sub_plot.grid(True, which='minor',linewidth = 0.2) # add y grids
    sub_plot.tick_params(which='major',length=7); sub_plot.tick_params(which='minor', length=4)
    sub_plot.xaxis.set_minor_locator(AutoMinorLocator()); sub_plot.yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/Local")                                 # set the working directory 
```

ä½ å°†éœ€è¦æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ä¸ºä½ çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”åœ¨ Mac ä¸Šæ ¼å¼ä¸åŒï¼ˆä¾‹å¦‚ï¼šâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½è¡¨æ ¼æ•°æ®

è¿™æ˜¯å°†æˆ‘ä»¬çš„é€—å·åˆ†éš”æ•°æ®æ–‡ä»¶åŠ è½½åˆ° Pandas DataFrame å¯¹è±¡çš„å‘½ä»¤ã€‚

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›†â€˜unconv_MV.csvâ€™ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…æ‹¬æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   ä¼˜è‰¯çš„å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   å‰ªåˆ‡æ¯”ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   ç»ç’ƒè´¨åå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹ç”Ÿäº§ 90 å¤©å¹³å‡ï¼ˆMCFPDï¼‰ã€‚

æ³¨æ„ï¼Œæ•°æ®é›†æ˜¯åˆæˆçš„ã€‚

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸ºâ€˜my_dataâ€™çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

```py
#my_data = pd.read_csv("unconv_MV.csv") 
my_data = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv") # load the comma delimited data file
my_data = my_data.iloc[:,1:]                              # remove the well index 
```

## å¯è§†åŒ– DataFrame

å¯è§†åŒ– DataFrame æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åˆæ­¥æ£€æŸ¥ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ‡ç‰‡ DataFrame æ¥é¢„è§ˆã€‚

+   æˆ‘ä»¬æ˜¾ç¤ºä» 0 åˆ° 7 çš„æ‰€æœ‰è®°å½•ï¼Œä½†ä¸åŒ…æ‹¬ 7ã€‚

```py
my_data[:7]                                               # preview the first 7 rows of the dataframe 
```

|  | å­”éš™ç‡ | LogPerm | AI | å‰ªåˆ‡æ¯” | TOC | VR | ç”Ÿäº§ |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 15.91 | 1.67 | 3.06 | 14.05 | 1.36 | 1.85 | 177.381958 |
| 1 | 15.34 | 1.65 | 2.60 | 31.88 | 1.37 | 1.79 | 1479.767778 |
| 2 | 20.45 | 2.02 | 3.13 | 63.67 | 1.79 | 2.53 | 4421.221583 |
| 3 | 11.95 | 1.14 | 3.90 | 58.81 | 0.40 | 2.03 | 1488.317629 |
| 4 | 19.53 | 1.83 | 2.57 | 43.75 | 1.40 | 2.11 | 5261.094919 |
| 5 | 19.47 | 2.04 | 2.73 | 54.37 | 1.42 | 2.12 | 5497.005506 |
| 6 | 12.70 | 1.30 | 3.70 | 43.03 | 0.45 | 1.95 | 1784.266285 |

## æè¿°æ€§ç»Ÿè®¡è¡¨æ•°æ®

åœ¨ DataFrames ä¸­ï¼Œæœ‰å¾ˆå¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥ä»è¡¨æ ¼æ•°æ®ä¸­è®¡ç®—æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯ã€‚describe å‘½ä»¤æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„æ•°æ®è¡¨ï¼Œæä¾›äº†è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼å’Œå››åˆ†ä½æ•°ã€‚

+   æˆ‘ä»¬ä½¿ç”¨è½¬ç½®åªæ˜¯ä¸ºäº†ç¿»è½¬è¡¨æ ¼ï¼Œä½¿å¾—ç‰¹å¾åœ¨è¡Œä¸Šï¼Œè€Œç»Ÿè®¡ä¿¡æ¯åœ¨åˆ—ä¸Šã€‚

```py
my_data.describe().transpose()                            # calculate summary statistics for the data 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 1000.0 | 14.950460 | 3.029634 | 5.400000 | 12.85750 | 14.98500 | 17.080000 | 24.65000 |
| LogPerm | 1000.0 | 1.398880 | 0.405966 | 0.120000 | 1.13000 | 1.39000 | 1.680000 | 2.58000 |
| AI | 1000.0 | 2.982610 | 0.577629 | 0.960000 | 2.57750 | 3.01000 | 3.360000 | 4.70000 |
| Brittle | 1000.0 | 49.719480 | 15.077006 | -10.500000 | 39.72250 | 49.68000 | 59.170000 | 93.47000 |
| TOC | 1000.0 | 1.003810 | 0.504978 | -0.260000 | 0.64000 | 0.99500 | 1.360000 | 2.71000 |
| VR | 1000.0 | 1.991170 | 0.308194 | 0.900000 | 1.81000 | 2.00000 | 2.172500 | 2.90000 |
| Production | 1000.0 | 2247.295809 | 1464.256312 | 2.713535 | 1191.36956 | 1976.48782 | 3023.594214 | 12568.64413 |

å¾ˆå¥½ï¼Œæˆ‘ä»¬å·²ç»æ£€æŸ¥äº†æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯ï¼Œå¯¹äºè„†æ€§å’Œæ€»æœ‰æœºç¢³ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›è´Ÿå€¼ã€‚è¿™æ˜¯åœ¨ç‰©ç†ä¸Šä¸å¯èƒ½çš„ã€‚

+   è¿™äº›å€¼å¿…é¡»å­˜åœ¨è¯¯å·®ã€‚æˆ‘ä»¬çŸ¥é“å¯èƒ½çš„æœ€å°å€¼æ˜¯ 0.0ï¼Œå› æ­¤æˆ‘ä»¬å°†æˆªæ–­åˆ° 0.0ã€‚

æˆ‘ä»¬ä½¿ç”¨ï¼š

```py
df.get_numerical_data() 
```

DataFrame æˆå‘˜å‡½æ•°ç”¨äºä» DataFrame è·å–æ•°æ®çš„æµ…æ‹·è´ã€‚

ç”±äºè¿™æ˜¯ä¸€ä¸ªæµ…æ‹·è´ï¼Œæˆ‘ä»¬å¯¹å‰¯æœ¬æ‰€åšçš„ä»»ä½•æ›´æ”¹éƒ½ä¼šå½±å“åˆ°åŸå§‹ DataFrame ä¸­çš„æ•°æ®ã€‚

+   è¿™å…è®¸æˆ‘ä»¬ä¸€æ¬¡æ€§å°†è¿™ä¸ªç®€å•çš„æ¡ä»¶è¯­å¥åº”ç”¨åˆ° DataFrame ä¸­çš„æ‰€æœ‰æ•°æ®å€¼ã€‚

```py
num = my_data._get_numeric_data()                         # get the numerical values
num[num < 0] = 0                                          # truncate negative values to 0.0
my_data.describe().transpose()                            # calculate summary statistics for the data 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 1000.0 | 14.950460 | 3.029634 | 5.400000 | 12.85750 | 14.98500 | 17.080000 | 24.65000 |
| LogPerm | 1000.0 | 1.398880 | 0.405966 | 0.120000 | 1.13000 | 1.39000 | 1.680000 | 2.58000 |
| AI | 1000.0 | 2.982610 | 0.577629 | 0.960000 | 2.57750 | 3.01000 | 3.360000 | 4.70000 |
| Brittle | 1000.0 | 49.731480 | 15.033593 | 0.000000 | 39.72250 | 49.68000 | 59.170000 | 93.47000 |
| TOC | 1000.0 | 1.006170 | 0.499838 | 0.000000 | 0.64000 | 0.99500 | 1.360000 | 2.71000 |
| VR | 1000.0 | 1.991170 | 0.308194 | 0.900000 | 1.81000 | 2.00000 | 2.172500 | 2.90000 |
| Production | 1000.0 | 2247.295809 | 1464.256312 | 2.713535 | 1191.36956 | 1976.48782 | 3023.594214 | 12568.64413 |

## è®¡ç®—ç›¸å…³çŸ©é˜µ

å¯¹äºé™ç»´ï¼Œæ•°æ®å¯è§†åŒ–æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç¬¬ä¸€æ­¥ã€‚

è®©æˆ‘ä»¬ä»ç›¸å…³çŸ©é˜µå¼€å§‹ã€‚

æˆ‘ä»¬å¯ä»¥è®¡ç®—å®ƒï¼Œå¹¶é€šè¿‡ä»¥ä¸‹å‘½ä»¤åœ¨æ§åˆ¶å°ä¸­æŸ¥çœ‹ã€‚

```py
corr_matrix = np.corrcoef(my_data, rowvar = False) 
```

è¾“å…¥æ•°æ®æ˜¯ä¸€ä¸ª 2D ndarrayï¼Œ$rowvar$æŒ‡å®šå˜é‡æ˜¯å¦åœ¨è¡Œä¸Šè€Œä¸æ˜¯åˆ—ä¸Šã€‚

```py
corr_matrix = np.corrcoef(my_data, rowvar = False)
print(np.around(corr_matrix,2))                           # print the correlation matrix to 2 decimals 
```

```py
[[ 1\.    0.81 -0.51 -0.25  0.71  0.08  0.69]
 [ 0.81  1\.   -0.32 -0.15  0.51  0.05  0.57]
 [-0.51 -0.32  1\.    0.17 -0.55  0.49 -0.33]
 [-0.25 -0.15  0.17  1\.   -0.24  0.3  -0.07]
 [ 0.71  0.51 -0.55 -0.24  1\.    0.31  0.5 ]
 [ 0.08  0.05  0.49  0.3   0.31  1\.    0.14]
 [ 0.69  0.57 -0.33 -0.07  0.5   0.14  1\.  ]] 
```

æ³¨æ„ç”±äºæ¯ä¸ªå˜é‡ä¸å…¶è‡ªèº«çš„ç›¸å…³æ€§è€Œäº§ç”Ÿçš„ 1.0 å¯¹è§’çº¿ã€‚

è®©æˆ‘ä»¬ä½¿ç”¨ä¸Šé¢å£°æ˜çš„å‡½æ•°æ¥åˆ¶ä½œä¸€ä¸ªå›¾å½¢ç›¸å…³çŸ©é˜µå¯è§†åŒ–ã€‚

+   è¿™å¯èƒ½æé«˜æˆ‘ä»¬è¯†åˆ«ç‰¹å¾çš„èƒ½åŠ›ã€‚å®ƒä¾èµ–äº Numpy DataFrames å†…ç½®çš„ç›¸å…³çŸ©é˜µæ–¹æ³•å’Œ MatPlotLib è¿›è¡Œç»˜å›¾ã€‚

```py
plot_corr(my_data,7)                                      # using our correlation matrix visualization function
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/93b5d485eb7760d8aa68200eb8d5b65a.png)

è¿™çœ‹èµ·æ¥ä¸é”™ã€‚å­˜åœ¨å¤šç§åŒå˜é‡ã€çº¿æ€§ç›¸å…³ç¨‹åº¦ã€‚å½“ç„¶ï¼Œç›¸å…³ç³»æ•°ä»…é™äºçº¿æ€§ç›¸å…³ç¨‹åº¦ã€‚

## æ£€æŸ¥çŸ©é˜µæ•£ç‚¹å›¾

ä¸ºäº†è·å–æ›´å®Œæ•´çš„ä¿¡æ¯ï¼Œè®©æˆ‘ä»¬æŸ¥çœ‹ Pandas åŒ…ä¸­çš„çŸ©é˜µæ•£ç‚¹å›¾ã€‚

+   åæ–¹å·®å’Œç›¸å…³ç³»æ•°å¯¹å¼‚å¸¸å€¼å’Œéçº¿æ€§æ•æ„Ÿ

```py
pd_plot.scatter_matrix(my_data) 
```

`alpha` å…è®¸æˆ‘ä»¬ä½¿ç”¨åŠé€æ˜ç‚¹ï¼Œä»¥ä¾¿åœ¨å¯†é›†æ•£ç‚¹å›¾ä¸­æ›´å®¹æ˜“å¯è§†åŒ–ã€‚

`hist_kwds` æ˜¯å¯¹è§’çº¿å…ƒç´ ä¸Šç›´æ–¹å›¾çš„å‚æ•°é›†ã€‚

```py
pd_plot.scatter_matrix(my_data, alpha = 0.1,              # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/018895042e5a686505eab9280b272aa5.png)

## ç®€å•çš„åŒå˜é‡ç¤ºä¾‹

è®©æˆ‘ä»¬å°†é—®é¢˜ç®€åŒ–ä¸ºåŒå˜é‡ï¼ˆ2 ä¸ªç‰¹å¾ï¼‰ï¼Œå­”éš™ç‡å’Œæ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼Œå¹¶å°†äº•çš„æ•°é‡ä» 1,000 å‡å°‘åˆ° 100ã€‚

```py
my_data_por_perm = my_data.iloc[0:100,0:2]                # extract just por and logperm, 100 samples
my_data_por_perm.describe().transpose()                   # calculate summary statistics for the data 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 100.0 | 14.9856 | 2.823016 | 9.23 | 12.9275 | 14.720 | 16.705 | 21.00 |
| LogPerm | 100.0 | 1.3947 | 0.390947 | 0.36 | 1.1475 | 1.365 | 1.650 | 2.48 |

è®©æˆ‘ä»¬é¦–å…ˆæ£€æŸ¥ Por å’Œ LogPerm çš„å•å˜é‡ç»Ÿè®¡ä¿¡æ¯ã€‚

```py
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.hist(my_data_por_perm["Por"], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20)
ax1.set_title('Porosity'); ax1.set_xlabel('Porosity (%)'); ax1.set_ylabel('Frequency'); add_grid2(ax1)
ax2.hist(my_data_por_perm["LogPerm"], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20)
ax2.set_title('Log Transformed Permeability'); ax2.set_xlabel('Log[Permeability] (log(mD)'); add_grid2(ax2)
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/5905d69c02168314e84b6bf1c5e7d169.png)

å®é™…ä¸Šï¼Œåˆ†å¸ƒå¯èƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒçš„ï¼Œæ— è®ºå®ƒä»¬çš„è¡Œä¸ºå¦‚ä½•è‰¯å¥½ï¼Œæˆ‘ä»¬æ— æ³•è§‚å¯Ÿåˆ°æ˜æ˜¾çš„ç¼ºå£æˆ–æˆªæ–­ã€‚

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å­”éš™ç‡ä¸å¯¹æ•°æ¸—é€ç‡çš„æ•£ç‚¹å›¾ã€‚

è¿™å°†æ˜¯æ¥è‡ª *matplotlib* çš„åŸºæœ¬å‘½ä»¤ï¼Œç”¨äºåˆ¶ä½œæ•£ç‚¹å›¾ã€‚

```py
plt.scatter(my_data_por_perm["Por"],my_data_por_perm["LogPerm"] 
```

+   é¢å¤–çš„å‚æ•°ç”¨äºæ ¼å¼åŒ–å’Œæ ‡ç­¾

```py
plt.scatter(my_data_por_perm["Por"],my_data_por_perm["LogPerm"],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
plt.title('Log Transformed Permeability vs. Porosity'); plt.xlabel('Porosity (%)'); plt.ylabel('Log(Permeability (Log(mD))'); add_grid()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.7, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/9c8f27583a828639b3ad04b5051376a2.png)

## ä¸»æˆåˆ†è®¡ç®—

ä½¿ç”¨æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼Œæˆ‘ä»¬ä¸å­”éš™ç‡æœ‰ä¸€ä¸ªéå¸¸å¥½çš„çº¿æ€§å…³ç³»ï¼ŒPCA åº”è¯¥åœ¨è¿™ç»„æ•°æ®ä¸Šå·¥ä½œå¾—å¾ˆå¥½ã€‚

+   æˆ‘ä»¬å‡†å¤‡å¥½ä½¿ç”¨å­”éš™ç‡å’Œæ¸—é€ç‡çš„å¯¹æ•°è¿›è¡Œ PCAã€‚

## æ ‡å‡†åŒ–ç‰¹å¾

æˆ‘ä»¬å¿…é¡»æ ‡å‡†åŒ–æˆ‘ä»¬çš„å˜é‡ï¼Œä½¿å…¶å‡å€¼ä¸ºé›¶ï¼Œ$\bar{x} = 0.0$ï¼Œæ–¹å·®ä¸º 1ï¼Œ$\sigma^{2}_{x} = 1.0$ã€‚

+   å¦åˆ™ï¼Œå­”éš™ç‡å’Œæ¸—é€ç‡çš„æ¯”ä¾‹å·®å¼‚å°†äº§ç”Ÿé‡å¤§å½±å“ã€‚æ³¨æ„ï¼Œå•ä½é€‰æ‹©å¯¹æ–¹å·®çš„å½±å“ï¼Œä¾‹å¦‚ï¼Œæ¸—é€ç‡ä½¿ç”¨è¾¾è¥¿ï¼ˆDï¼‰è€Œä¸æ˜¯æ¯«è¾¾è¥¿ï¼ˆmDï¼‰ï¼Œæˆ–è€…å­”éš™ç‡ä½¿ç”¨åˆ†æ•°è€Œä¸æ˜¯ç™¾åˆ†æ¯”ã€‚è¿™æ˜¯ç›¸å½“ä»»æ„çš„ï¼

ä¸ºäº†æ¶ˆé™¤è¿™ç§å½±å“ï¼Œæˆ‘ä»¬åº”è¯¥å§‹ç»ˆæ ‡å‡†åŒ–ï¼Œé™¤éä¸¤ä¸ªå˜é‡å…·æœ‰ç›¸åŒçš„å•ä½ï¼Œå¹¶ä¸”å®ƒä»¬ä¹‹é—´çš„èŒƒå›´å’Œæ–¹å·®æœ‰æ„ä¹‰ï¼Œç„¶åæ ‡å‡†åŒ–å¯èƒ½ä¼šåˆ é™¤é‡è¦ä¿¡æ¯ã€‚

```py
features = ['Por','LogPerm']
x = my_data_por_perm.loc[:,features].values
mu = np.mean(x, axis=0)
sd = np.std(x, axis=0)
x = StandardScaler().fit_transform(x)                     # standardize the data features to mean = 0, var = 1.0

print("Original Mean Por", np.round(mu[0],2), ', Original Mean LogPerm = ', np.round(mu[1],2)) 
print("Original StDev Por", np.round(sd[0],2), ', Original StDev LogPerm = ', np.round(sd[1],2)) 
print('Mean Transformed Por =',np.round(np.mean(x[:,0]),2),', Mean Transformed LogPerm =',np.round(np.mean(x[:,1]),2))
print('Variance Transformed Por =',np.var(x[:,0]),', Variance Transformed LogPerm =',np.var(x[:,1])) 
```

```py
Original Mean Por 14.99 , Original Mean LogPerm =  1.39
Original StDev Por 2.81 , Original StDev LogPerm =  0.39
Mean Transformed Por = 0.0 , Mean Transformed LogPerm = -0.0
Variance Transformed Por = 1.0000000000000002 , Variance Transformed LogPerm = 1.0 
```

```py
cov = np.cov(x,rowvar = False)
cov 
```

```py
array([[1.01010101, 0.80087707],
       [0.80087707, 1.01010101]]) 
```

â€œxâ€æ˜¯ä¸€ä¸ªæ¥è‡ª Numpy åŒ…çš„äºŒç»´ ndarrayï¼Œå…¶ç‰¹å¾åœ¨åˆ—ä¸Šï¼Œæ ·æœ¬åœ¨è¡Œä¸Šã€‚

+   å¦‚ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬ç¡®è®¤â€œxâ€äºŒç»´æ•°ç»„ä¸­çš„ç‰¹å¾å·²ç»æ ‡å‡†åŒ–ã€‚

æ£€æŸ¥æˆ‘ä»¬æ ‡å‡†åŒ–å˜é‡çš„å•å˜é‡å’Œå¤šå˜é‡åˆ†å¸ƒä¸æ˜¯ä¸€ä¸ªåä¸»æ„ã€‚

```py
dfS = pd.DataFrame({'sPor': x[:,0], 'sLogPerm': x[:,1]})
sns.jointplot(data=dfS,x='sPor',y='sLogPerm',marginal_kws=dict(bins=30),color='darkorange',edgecolor='black')
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/fabb8a2ef820e71361695cf1232eccd7.png)

ä¸€åˆ‡çœ‹èµ·æ¥éƒ½å¾ˆæ­£å¸¸ï¼Œæˆ‘ä»¬å‡†å¤‡åº”ç”¨ä¸»æˆåˆ†åˆ†æã€‚

## ä¸»æˆåˆ†åˆ†æ (PCA)

è¦åœ¨ Python ä¸­ä½¿ç”¨ SciKitLearn æœºå™¨å­¦ä¹ åŒ…è¿è¡Œ PCAï¼Œæˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªæŒ‡å®šç»„ä»¶æ•°é‡çš„ PCA æ¨¡å‹ï¼Œç„¶åå°†å…¶â€œæ‹Ÿåˆâ€åˆ°æˆ‘ä»¬çš„æ•°æ®ã€‚

```py
n_components = 2
pca = PCA(n_components=n_components)
pca.fit(x) 
```

ä½ å°†åœ¨åé¢çš„é™ç»´ä¸­çœ‹åˆ°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨çŸ©é˜µæ•°å­¦æ¥ä½¿ç”¨è¿™ä¸ªæ¨¡å‹ï¼Œå¹¶å°†æˆ‘ä»¬çš„æ•°æ®å‡å°‘åˆ°ä» 1 åˆ°ç‰¹å¾æ•°é‡ m çš„ä»»ä½•ç»´åº¦ã€‚è®©æˆ‘ä»¬ä»¥ä¸ç‰¹å¾æ•°é‡ m ç›¸ç­‰çš„ç»„ä»¶æ•°é‡è¿è¡Œæ¨¡å‹ã€‚

```py
n_components = 2
pca = PCA(n_components=n_components).fit(x) 
```

## ç»„ä»¶è½½è·

æˆ‘ä»¬åº”è¯¥åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯æŸ¥çœ‹ç»„ä»¶è½½è·ã€‚è®©æˆ‘ä»¬æŸ¥çœ‹å®ƒä»¬å¹¶è§£é‡Šæˆ‘ä»¬çš„ç»“æœã€‚

```py
print(np.round(pca.components_,3))
print('First Principal Component = ' + str(np.round(pca.components_[0,:],3)))
print('Second Principal Component = ' + str(np.round(pca.components_[1,:],3))) 
```

```py
[[ 0.707  0.707]
 [ 0.707 -0.707]]
First Principal Component = [0.707 0.707]
Second Principal Component = [ 0.707 -0.707] 
```

ç»„ä»¶è¢«åˆ—ä¸ºä¸€ä¸ªäºŒç»´æ•°ç»„ï¼ˆndarrayï¼‰ï¼Œå…¶ä¸­ï¼š

+   ä¸»æˆåˆ†åœ¨è¡Œä¸Š

+   ç‰¹å¾åœ¨åˆ—ä¸Š

+   è¡Œå·²æ’åºï¼Œä»¥ä¾¿ç¬¬ä¸€ä¸ªä¸»æˆåˆ†æ˜¯é¡¶è¡Œï¼Œæœ€åä¸€ä¸ªä¸»æˆåˆ†æ˜¯æœ€åä¸€è¡Œã€‚

## ä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹

ä¹Ÿå¾ˆé‡è¦çš„æ˜¯è¦æŸ¥çœ‹æ¯ä¸ªä¸»æˆåˆ†æè¿°çš„æ–¹å·®æ¯”ä¾‹ã€‚

```py
print('Variance explained by PC1 and PC2 =', np.round(pca.explained_variance_ratio_,3))
print('First Principal Component explains ' + str(np.round(pca.explained_variance_ratio_[0],3)) + ' of the total variance.')
print('Second Principal Component explains ' + str(np.round(pca.explained_variance_ratio_[1],3)) + ' of the total variance.') 
```

```py
Variance explained by PC1 and PC2 = [0.896 0.104]
First Principal Component explains 0.896 of the total variance.
Second Principal Component explains 0.104 of the total variance. 
```

## ä¸»æˆåˆ†å¾—åˆ†ï¼Œæ­£å‘å’Œåå‘æŠ•å½±

æˆ‘ä»¬å¯ä»¥è®¡ç®—åŸå§‹æ•°æ®çš„ä¸»æˆåˆ†å¾—åˆ†ã€‚

+   è¿™å®é™…ä¸Šæ˜¯å¯¹æ•°æ®çš„æ—‹è½¬ï¼Œä¸ PC1 å¯¹æœ€å¤§æ–¹å·®çš„æ–¹å‘å¯¹é½ã€‚

+   æˆ‘ä»¬å°†ä½¿ç”¨ PCA å†…ç½®çš„â€œtransformâ€å‡½æ•°è®¡ç®—ä¸»æˆåˆ†å¾—åˆ†ï¼Œç„¶åå°†å…¶å¯è§†åŒ–ä¸ºä¸€ä¸ªæ•£ç‚¹å›¾ã€‚

+   ç„¶åä¸ºäº†â€œé—­åˆå¾ªç¯â€å¹¶æ£€æŸ¥æˆ‘ä»¬æ‰€åšçš„å·¥ä½œï¼ˆä»¥åŠæˆ‘ä»¬çš„çŸ¥è¯†ï¼‰ï¼Œæˆ‘ä»¬å°†è¿›è¡Œ PCA çš„åå‘æ“ä½œï¼Œä»ä¸»æˆåˆ†å¾—åˆ†å›åˆ°æ ‡å‡†åŒ–ç‰¹å¾ã€‚

```py
f, (ax101, ax102, ax103) = plt.subplots(1, 3,figsize=(12,3))
f.subplots_adjust(wspace=0.7)

ax101.scatter(x[:,0],x[:,1], s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax101.set_title('Standardized LogPerm vs. Por'); ax101.set_xlabel('Standardized Por'); ax101.set_ylabel('Standardized LogPerm')
ax101.set_xlim([-3,3]); ax101.set_ylim([-3,3]); add_grid2(ax101)

x_trans = pca.transform(x)                                # calculate the principal component scores
ax102.scatter(x_trans[:,0],-1*x_trans[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax102.set_title('Principal Component Scores'); ax102.set_xlabel('PC1'); ax102.set_ylabel('PC2')
ax102.set_xlim([-3,3]); ax102.set_ylim([-3,3]); add_grid2(ax102)

x_reverse = pca.inverse_transform(x_trans)                        # reverse the principal component scores to standardized values
ax103.scatter(x_reverse[:,0],x_reverse[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax103.set_title('Reverse PCA'); ax103.set_xlabel('Standardized Por'); ax103.set_ylabel('Standardized LogPerm')
ax103.set_xlim([-3,3]); ax103.set_ylim([-3,3]); add_grid2(ax103)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/a58857e0fc1f11730774c37f6998fb6d.png)

æ ‡å‡†åŒ–çš„åŸå§‹å’Œåå‘ PCA äº¤å‰å›¾åº”è¯¥çœ‹èµ·æ¥å®Œå…¨ç›¸åŒã€‚å¦‚æœæ˜¯è¿™æ ·ï¼Œé‚£ä¹ˆæ–¹æ³•å°±æ˜¯æœ‰æ•ˆçš„ã€‚

## æ–¹å·®å®ˆæ’

è®©æˆ‘ä»¬æ£€æŸ¥ä¸»æˆåˆ†å¾—åˆ†çš„æ–¹å·®ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»è®¡ç®—äº†å®ƒä»¬ã€‚

+   æˆ‘ä»¬è®¡ç®—åŸå§‹ç‰¹å¾çš„æ–¹å·®

+   ç„¶åæ±‚å’Œä»¥å¾—åˆ°åŸå§‹çš„æ€»æ–¹å·®

+   æˆ‘ä»¬è®¡ç®—æ¯ä¸ªè½¬æ¢åçš„ä¸»æˆåˆ†å¾—åˆ†çš„æ–¹å·®

+   ç„¶åæˆ‘ä»¬æ±‚å’Œä»¥å¾—åˆ°è½¬æ¢åçš„æ€»æ–¹å·®

æˆ‘ä»¬æ³¨æ„åˆ°ï¼š

+   ç¬¬ä¸€ä¸ªä¸»æˆåˆ†å¾—åˆ†æ¯”ç¬¬äºŒä¸ªæˆåˆ†å¾—åˆ†æœ‰æ›´å¤§çš„æ–¹å·®

+   å˜æ¢è¿‡ç¨‹ä¸­æ€»æ–¹å·®å¾—åˆ°ä¿ç•™ï¼ŒåŸå§‹ç‰¹å¾å’Œ m ä¸ªä¸»æˆåˆ†å¾—åˆ†æ–¹å·®ä¹‹å’Œç›¸åŒ

```py
print('Variance of the 2 features:')
print(np.var(x, axis = 0))

print('\nTotal Variance from Original Features:')
print(np.sum(np.var(x, axis = 0)))

print('\nVariance of the 2 principle components:')
print(np.round(np.var(x_trans, axis = 0),2))

print('\nTotal Variance from Original Features:')
print(round(np.sum(np.var(x_trans, axis = 0)),2)) 
```

```py
Variance of the 2 features:
[1\. 1.]

Total Variance from Original Features:
2.0

Variance of the 2 principle components:
[1.79 0.21]

Total Variance from Original Features:
2.0 
```

## ä¸»æˆåˆ†å¾—åˆ†ç‹¬ç«‹æ€§

è®©æˆ‘ä»¬æ£€æŸ¥åŸå§‹ç‰¹å¾ä¸æˆ‘ä»¬çš„æŠ•å½±ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

```py
print('\nCorrelation Matrix of the 2 original features components:')
print(np.round(np.corrcoef(x, rowvar = False),2))

print('\nCorrelation Matrix of the 2 principle components\' scores:')
print(np.round(np.corrcoef(x_trans, rowvar = False),2)) 
```

```py
Correlation Matrix of the 2 original features components:
[[1\.   0.79]
 [0.79 1\.  ]]

Correlation Matrix of the 2 principle components' scores:
[[ 1\. -0.]
 [-0\.  1.]] 
```

æˆ‘ä»¬å·²ç»å°†å…·æœ‰é«˜ç›¸å…³æ€§çš„åŸå§‹ç‰¹å¾æŠ•å½±åˆ° 2 ä¸ªæ–°ç‰¹å¾ä¸Šï¼Œè¿™äº›æ–°ç‰¹å¾ä¹‹é—´æ²¡æœ‰ç›¸å…³æ€§ã€‚

## é€šè¿‡ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡è®¡ç®—å™¨æ‰‹åŠ¨è¿›è¡Œä¸»æˆåˆ†åˆ†æ

è®©æˆ‘ä»¬æ‰‹åŠ¨è®¡ç®— PCAï¼Œä½¿ç”¨æ ‡å‡†åŒ–ç‰¹å¾å’Œç‰¹å¾å€¼è®¡ç®—ï¼Œå¹¶ä¸ä¸Šé¢çš„ scikit-learn ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚

+   æˆ‘ä»¬ç¡®è®¤ç»“æœæ˜¯ä¸€è‡´çš„ã€‚

```py
from numpy.linalg import eig
eigen_values,eigen_vectors = eig(cov)
print('Eigen Vectors:\n' +  str(np.round(eigen_vectors,2)))
print('First Eigen Vector: ' + str(eigen_vectors[:,0]))
print('Second Eigen Vector: ' + str(eigen_vectors[:,1]))
print('Eigen Values:\n' +  str(np.round(eigen_values,2)))
PC = eigen_vectors.T.dot(x.T)
plt.subplot(121)
plt.scatter(PC[0,:],PC[1,:],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
plt.title('Principal Component Scores By-hand with numpy.linalg Eig Function'); plt.xlabel('PC1'); plt.ylabel('PC2')
plt.xlim([-3,3]); plt.ylim([-3,3]); add_grid()

plt.subplot(122)
plt.scatter(x_trans[:,0],-1*x_trans[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
plt.title('Principal Component Scores with scikit-learn PCA'); plt.xlabel('PC1'); plt.ylabel('PC2')
plt.xlim([-3,3]); plt.ylim([-3,3]); add_grid()

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.2, hspace=0.3); plt.show() 
```

```py
Eigen Vectors:
[[ 0.71 -0.71]
 [ 0.71  0.71]]
First Eigen Vector: [0.70710678 0.70710678]
Second Eigen Vector: [-0.70710678  0.70710678]
Eigen Values:
[1.81 0.21] 
```

![å›¾ç‰‡](img/5f1afcb9beed01f67ca256f8ea4acb30.png)

## é™ç»´æ¼”ç¤º

ç°åœ¨æˆ‘ä»¬å°è¯•é€šè¿‡ä»…ä¿ç•™ç¬¬ä¸€ä¸ªä¸»æˆåˆ†æ¥è¿›è¡Œ**é™ç»´**ã€‚æˆ‘ä»¬å°†ä»åŸå§‹å€¼è¿‡æ¸¡åˆ°åŸå§‹å€¼çš„é¢„æµ‹ã€‚

+   å›æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç”¨ç¬¬ä¸€ä¸ªä¸»æˆåˆ†è§£é‡Šå¤§çº¦ 90%çš„æ–¹å·®ï¼Œæ‰€ä»¥ç»“æœåº”è¯¥çœ‹èµ·æ¥â€œç›¸å½“ä¸é”™â€ï¼Œå¯¹å§ï¼Ÿ

æˆ‘ä»¬å°†æ‰‹åŠ¨å®Œæˆæ•´ä¸ªè¿‡ç¨‹ï¼Œä½¿å…¶å°½å¯èƒ½ç®€å•æ˜“æ‡‚ã€‚ç¨åæˆ‘ä»¬å°†æ›´åŠ ç´§å‡‘ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š

1.  ä»åŸå§‹å­”éš™ç‡å’Œæ¸—é€ç‡æ•°æ®å¼€å§‹

1.  æ ‡å‡†åŒ–ï¼Œä½¿å¾— Por å’Œ LogPerm çš„å‡å€¼ä¸º 0.0ï¼Œæ–¹å·®ä¸º 1.0

1.  è®¡ç®—ä¸¤ä¸ªä¸»æˆåˆ†æ¨¡å‹ï¼Œå¯è§†åŒ–ä¸»æˆåˆ†å¾—åˆ†

1.  é€šè¿‡å°†ç›¸å…³æˆåˆ†å¾—åˆ†è®¾ç½®ä¸º 0.0 æ¥ç§»é™¤ç¬¬äºŒä¸ªä¸»æˆåˆ†

1.  é€šè¿‡çŸ©é˜µä¹˜ä»¥å¾—åˆ†å’Œæˆåˆ†è´Ÿè½½æ¥åè½¬ä¸»æˆåˆ†

1.  å°†çŸ©é˜µæ•°å­¦åº”ç”¨äºæ¢å¤åŸå§‹å‡å€¼å’Œæ–¹å·®

```py
nComp = 1
f, ((ax201, ax202, ax203), (ax206, ax205, ax204)) = plt.subplots(2, 3,figsize=(15,10))
#f, ((ax201, ax202), (ax203, ax204), (ax205, ax206)) = plt.subplots(3, 2,figsize=(10,15))
f.subplots_adjust(wspace=0.5,hspace = 0.3)

ax201.scatter(my_data_por_perm["Por"],my_data_por_perm["LogPerm"],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax201.set_title('1\. LogPerm vs. Por'); ax201.set_xlabel('Por'); ax201.set_ylabel('LogPerm')
ax201.set_xlim([8,22]); ax201.set_ylim([0,2.5]); add_grid2(ax201)

mu = np.mean(np.vstack((my_data_por_perm["Por"].values,my_data_por_perm["LogPerm"].values)), axis=1)
sd = np.std(np.vstack((my_data_por_perm["Por"].values,my_data_por_perm["LogPerm"].values)), axis=1)
x = StandardScaler().fit_transform(x)                     # standardize the data features to mean = 0, var = 1.0

ax202.scatter(x[:,0],x[:,1], s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax202.set_title('2\. Standardized LogPerm vs. Por'); ax202.set_xlabel('Standardized Por'); ax202.set_ylabel('Standardized LogPerm')
ax202.set_xlim([-3.5,3.5]); ax202.set_ylim([-3.5,3.5]); add_grid2(ax202)

n_components = 2                                          # build principal component model with 2 components
pca = PCA(n_components=n_components)
pca.fit(x)

x_trans = pca.transform(x)                                # calculate principal component scores
ax203.scatter(x_trans[:,0],-1*x_trans[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax203.set_title('3\. Principal Component Scores'); ax203.set_xlabel('PC1'); ax203.set_ylabel('PC2')
ax203.set_xlim([-3.5,3.5]); ax203.set_ylim([-3.5,3.5]); add_grid2(ax203)

x_trans[:,1] = 0.0                                         # zero / remove the 2nd principal component 

ax204.scatter(x_trans[:,0],x_trans[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax204.set_title('4\. Only 1st Principal Component Scores'); ax204.set_xlabel('PC1'); ax204.set_ylabel('PC2')
ax204.set_xlim([-3.5,3.5]); ax204.set_ylim([-3.5,3.5]); add_grid2(ax204)

xhat = pca.inverse_transform(x_trans)                             # reverse the principal component scores to standardized values
ax205.scatter(xhat[:,0],xhat[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax205.set_title('5\. Reverse PCA'); ax205.set_xlabel('Standardized Por'); ax205.set_ylabel('Standardized LogPerm')
ax205.set_xlim([-3.5,3.5]); ax205.set_ylim([-3.5,3.5]); add_grid2(ax205)

xhat = np.dot(pca.inverse_transform(x)[:,:nComp], pca.components_[:nComp,:])
xhat = sd*xhat + mu                                       # remove the standardization

ax206.scatter(my_data_por_perm["Por"],my_data_por_perm["LogPerm"],s=None, c="blue", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.6, linewidths=1.0, edgecolors="black")
ax206.scatter(xhat[:,0],xhat[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax206.set_title('6\. De-standardized Reverse PCA'); ax206.set_xlabel('Por'); ax206.set_ylabel('LogPerm')
ax206.set_xlim([8,22]); ax206.set_ylim([0,2.5]); add_grid2(ax206)

plt.show() 
```

![å›¾ç‰‡](img/bf554436f2aadf97e6226c941ec9202d.png)

è®©æˆ‘ä»¬å°†åŸå§‹æ•°æ®å’Œç»“æœä½ç»´æ¨¡å‹å¹¶æ’æ”¾ç½®ï¼Œå¹¶æ£€æŸ¥ç»“æœæ–¹å·®ã€‚

```py
f, (ax201, ax206) = plt.subplots(1, 2,figsize=(10,6))
f.subplots_adjust(wspace=0.5,hspace = 0.3)

ax201.scatter(my_data_por_perm["Por"],my_data_por_perm["LogPerm"],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax201.set_title('1\. LogPerm vs. Por'); ax201.set_xlabel('Por'); ax201.set_ylabel('LogPerm')
ax201.set_xlim([8,22]); ax201.set_ylim([0,2.5]); add_grid2(ax201)

ax206.scatter(xhat[:,0],xhat[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax206.set_title('6\. De-standardized Reverse PCA'); ax206.set_xlabel('Por'); ax206.set_ylabel('LogPerm')
ax206.set_xlim([8,22]); ax206.set_ylim([0,2.5]); add_grid2(ax206)
plt.show()

var_por = np.var(my_data_por_perm["Por"]); var_por_hat = np.var(xhat[:,0]);
var_logperm = np.var(my_data_por_perm["LogPerm"]); var_logperm_hat = np.var(xhat[:,1]);
print('Variance Por =',np.round(var_por,3),', Variance Reduced Dimensional Por =',np.round(var_por_hat,3),'Fraction = ',np.round(var_por_hat/var_por,3))
print('Variance LogPerm =',np.round(var_logperm,3),', Variance Reduced Dimensional LogPerm =',np.round(var_logperm_hat,3),'Fraction = ',np.round(var_logperm_hat/var_logperm,3))
print('Total Variance =',np.round(var_por + var_logperm,3), ', Total Variance Reduced Dimension =',np.round(var_por_hat+var_logperm_hat,3),'Fraction = ',np.round((var_por_hat+var_logperm_hat)/(var_por+var_logperm),3)) 
```

![å›¾ç‰‡](img/240c16e5f3411a754b89857959797e42.png)

```py
Variance Por = 7.89 , Variance Reduced Dimensional Por = 7.073 Fraction =  0.896
Variance LogPerm = 0.151 , Variance Reduced Dimensional LogPerm = 0.136 Fraction =  0.896
Total Variance = 8.041 , Total Variance Reduced Dimension = 7.208 Fraction =  0.896 
```

## æ‰€æœ‰é¢„æµ‹ç‰¹å¾

æˆ‘ä»¬å°†å›åˆ°åŸå§‹æ•°æ®æ–‡ä»¶ï¼Œè¿™æ¬¡æå–æ‰€æœ‰ 6 ä¸ªé¢„æµ‹å˜é‡å’Œå‰ 500 ä¸ªæ ·æœ¬ã€‚

```py
my_data_f6 = my_data.iloc[0:500,0:6]                      # extract the 6 predictors, 500 samples 
```

ä»æˆ‘ä»¬çš„æ•°æ®å¼€å§‹ï¼Œå…ˆè®¡ç®—æ€»ç»“ç»Ÿè®¡é‡æ˜¯ä¸ªå¥½ä¸»æ„ã€‚

```py
my_data_f6.describe().transpose()                         # calculate summary statistics for the data 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 500.0 | 14.89936 | 2.985967 | 5.40 | 12.8500 | 14.900 | 17.0125 | 23.85 |
| LogPerm | 500.0 | 1.40010 | 0.409616 | 0.18 | 1.1475 | 1.380 | 1.6700 | 2.58 |
| AI | 500.0 | 2.99244 | 0.563674 | 1.21 | 2.5900 | 3.035 | 3.3725 | 4.70 |
| Brittle | 500.0 | 49.74682 | 15.212123 | 0.00 | 39.3125 | 49.595 | 59.2075 | 93.47 |
| TOC | 500.0 | 0.99800 | 0.503635 | 0.00 | 0.6400 | 0.960 | 1.3500 | 2.71 |
| VR | 500.0 | 1.99260 | 0.307434 | 0.90 | 1.8200 | 2.010 | 2.1725 | 2.84 |

è®©æˆ‘ä»¬å†è®¡ç®—ä¸€ä¸ªç›¸å…³çŸ©é˜µå¹¶æŸ¥çœ‹å®ƒã€‚

```py
corr_matrix = np.corrcoef(my_data_f6, rowvar = False)
print(np.around(corr_matrix,2))                           # print the correlation matrix to 2 decimals 
```

```py
[[ 1\.    0.79 -0.49 -0.25  0.71  0.12]
 [ 0.79  1\.   -0.32 -0.13  0.48  0.04]
 [-0.49 -0.32  1\.    0.14 -0.53  0.47]
 [-0.25 -0.13  0.14  1\.   -0.24  0.24]
 [ 0.71  0.48 -0.53 -0.24  1\.    0.35]
 [ 0.12  0.04  0.47  0.24  0.35  1\.  ]] 
```

æˆ‘ä»¬éœ€è¦å°†æ¯ä¸ªå˜é‡æ ‡å‡†åŒ–ï¼Œä½¿å…¶å‡å€¼ä¸ºé›¶ï¼Œæ–¹å·®ä¸º 1ã€‚è®©æˆ‘ä»¬è¿™æ ·åšå¹¶æ£€æŸ¥ç»“æœã€‚åœ¨ä¸‹é¢çš„æ§åˆ¶å°ä¸­ï¼Œæˆ‘ä»¬æ‰“å°å‡ºæ‰€æœ‰ 6 ä¸ªé¢„æµ‹å˜é‡çš„åˆå§‹å€¼å’Œæ ‡å‡†åŒ–å‡å€¼å’Œæ–¹å·®ã€‚

```py
features = ['Por','LogPerm','AI','Brittle','TOC','VR']
x_f6 = my_data_f6.loc[:,features].values
mu_f6 = np.mean(x_f6, axis=0)
sd_f6 = np.std(x_f6, axis=0)
x_f6 = StandardScaler().fit_transform(x_f6)

print("Original Means", features[:], np.round(mu_f6[:],2)) 
print("Original StDevs", features[:],np.round(sd_f6[:],2)) 
print('Mean Transformed =',features[:],np.round(x.mean(axis=0),2))
print('Variance Transformed Por =',features[:],np.round(x.var(axis=0),2)) 
```

```py
Original Means ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [14.9   1.4   2.99 49.75  1\.    1.99]
Original StDevs ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [ 2.98  0.41  0.56 15.2   0.5   0.31]
Mean Transformed = ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [0\. 0.]
Variance Transformed Por = ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [1\. 1.] 
```

æˆ‘ä»¬è¿˜åº”è¯¥æ£€æŸ¥æ¯ä¸ªå˜é‡çš„å•å˜é‡åˆ†å¸ƒã€‚

```py
f, (ax6,ax7,ax8,ax9,ax10,ax11) = plt.subplots(1, 6, sharey=True, figsize=(15,2))
ax6.hist(x_f6[:,0], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax6.set_title('Std. Porosity'); ax6.set_xlim(-5,5)
ax7.hist(x_f6[:,1], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax7.set_title('Std. Log[Perm.]'); ax7.set_xlim(-5,5)
ax8.hist(x_f6[:,2], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax8.set_title('Std. Acoustic Imped.'); ax8.set_xlim(-5,5)
ax9.hist(x_f6[:,3], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax9.set_title('Std. Brittleness'); ax9.set_xlim(-5,5)
ax10.hist(x_f6[:,4], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax10.set_title('Std. Total Organic C'); ax10.set_xlim(-5,5)
ax11.hist(x_f6[:,5], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax11.set_title('Std. Vit. Reflectance'); ax11.set_xlim(-5,5)
plt.show() 
```

![cb70ebc58a6161c91e168f37c51faf16ad0c7a73cb23c7741794ee731d2470a4.png](img/c9eb87ba8e54d0f26a95ce04a0f2751a.png)

æ¦‚ç‡ç»Ÿè®¡å’Œåˆ†å¸ƒçœ‹èµ·æ¥å¾ˆå¥½ã€‚æ²¡æœ‰æ˜æ˜¾çš„ç¼ºå¤±æ•°æ®ã€ç¼ºå£ã€æ˜¾è‘—çš„æˆªæ–­ã€å³°å€¼æˆ–å¼‚å¸¸å€¼ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥å¯¹æˆ‘ä»¬ 6 ä¸ªç‰¹å¾è¿›è¡Œä¸»æˆåˆ†åˆ†æäº†ã€‚

```py
n_components = 6
pca_f6 = PCA(n_components=n_components)
pca_f6.fit(x_f6)

print(np.round(pca_f6.components_,3))                     # visualize the component loadings 
```

```py
[[ 0.558  0.476 -0.405 -0.211  0.504  0.01 ]
 [-0.117 -0.114 -0.432 -0.323 -0.229 -0.794]
 [-0.019 -0.124  0.384 -0.898  0.07   0.157]
 [-0.214 -0.674 -0.424 -0.006  0.526  0.21 ]
 [-0.784  0.522 -0.031 -0.046  0.331 -0.019]
 [ 0.12  -0.138  0.566  0.206  0.55  -0.549]] 
```

è®©æˆ‘ä»¬å…ˆçœ‹çœ‹æˆåˆ†è½½è·ã€‚æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæˆåˆ†ï¼Œæœ€ä¸Šé¢ä¸€è¡Œæ˜¯ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ï¼ˆPC1ï¼‰ï¼Œä¸‹ä¸€è¡Œæ˜¯ç¬¬äºŒä¸ªä¸»æˆåˆ†ï¼ˆPC2ï¼‰ï¼Œç›´åˆ°æœ€åä¸€è¡Œæ˜¯ç¬¬å…­ä¸ªä¸»æˆåˆ†ï¼ˆPC6ï¼‰ã€‚åˆ—æ˜¯æŒ‰â€˜Porâ€™ã€â€˜LogPermâ€™ã€â€˜AIâ€™ã€â€˜Brittleâ€™ã€â€˜TOCâ€™åˆ°â€˜VRâ€™çš„é¡ºåºæ’åˆ—çš„ç‰¹å¾ã€‚

ç¬¬ä¸€ä¸»æˆåˆ†ä¸»è¦ç”±å­”éš™ç‡ã€å¯¹æ•°æ¸—é€ç‡ã€å£°é˜»æŠ—å’Œæ€»æœ‰æœºç¢³ç»„æˆï¼Œè¡¨æ˜å®ƒä»¬å…±åŒå˜åŒ–çš„æ–¹å¼æ˜¯é€ æˆå¤§éƒ¨åˆ†æ–¹å·®çš„åŸå› ã€‚ä¸‹ä¸€ä¸ªä¸»æˆåˆ†ä¸»è¦ç”±é•œç…¤åå°„ç‡ç»„æˆã€‚ç¬¬ä¸‰ä¸ªä¸»åæ ‡ä¸»è¦ç”±è„†æ€§ç­‰ç»„æˆã€‚

## åˆ‡ç‰‡å›¾

ä¸ºäº†å¸®åŠ©è§£é‡Šè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åº”è¯¥è€ƒè™‘æ¯ä¸ªä¸»æˆåˆ†çš„æ–¹å·®è´¡çŒ®ã€‚

```py
print('Variance explained by PC1 thru PC6 =', np.round(pca_f6.explained_variance_ratio_,3))

f, (ax10, ax11) = plt.subplots(1, 2,figsize=(10,6))
f.subplots_adjust(wspace=0.5,hspace = 0.3)

ax10.plot(np.arange(1,7,1),pca_f6.explained_variance_ratio_*100,color='darkorange',alpha=0.8)
ax10.scatter(np.arange(1,7,1),pca_f6.explained_variance_ratio_*100,color='darkorange',alpha=0.8,edgecolor='black')
ax10.set_xlabel('Principal Component'); ax10.set_ylabel('Variance Explained'); ax10.set_title('Variance Explained by Principal Component')
fmt = '%.0f%%' # Format you want the ticks, e.g. '40%'
yticks = mtick.FormatStrFormatter(fmt); ax10.set_xlim(1,6); ax10.set_ylim(0,100.0)
ax10.yaxis.set_major_formatter(yticks); add_grid2(ax10)

ax11.plot(np.arange(1,7,1),np.cumsum(pca_f6.explained_variance_ratio_*100),color='darkorange',alpha=0.8)
ax11.scatter(np.arange(1,7,1),np.cumsum(pca_f6.explained_variance_ratio_*100),color='darkorange',alpha=0.8,edgecolor='black')
ax11.plot([1,6],[95,95], color='black',linestyle='dashed')
ax11.set_xlabel('Principal Component'); ax11.set_ylabel('Cumulative Variance Explained'); ax11.set_title('Cumulative Variance Explained by Principal Component')
fmt = '%.0f%%' # Format you want the ticks, e.g. '40%'
yticks = mtick.FormatStrFormatter(fmt); ax11.set_xlim(1,6); ax11.set_ylim(0,100.0); ax11.annotate('95% variance explained',[4.05,90])
ax11.yaxis.set_major_formatter(yticks); add_grid2(ax11)

plt.show() 
```

```py
Variance explained by PC1 thru PC6 = [0.462 0.246 0.149 0.11  0.024 0.009] 
```

![f13a2759a5a3a9ba079c2c90976c1d01b7e4e03c073aeb9780c2e4db83e7bbbf.png](img/6f96fbe837665bd4b49b22deee4579f9.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¤§çº¦ 46%çš„æ–¹å·®ç”±ç¬¬ä¸€ä¸ªä¸»æˆåˆ†æè¿°ï¼Œç„¶åå¤§çº¦ 25%ç”±ç¬¬äºŒä¸ªä¸»æˆåˆ†æè¿°ç­‰ç­‰ã€‚

## ä¸»æˆåˆ†å¾—åˆ†ä¹‹é—´çš„ç‹¬ç«‹æ€§

åœ¨æŠ•å½±å‰åæ£€æŸ¥ç‰¹å¾å¯¹ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

```py
print('\nCorrelation Matrix of the 6 original features components:')
print(np.round(np.corrcoef(x_f6, rowvar = False),2))

print('\nCorrelation Matrix of the 6 principle components\' scores:')
print(np.round(np.corrcoef(pca_f6.transform(x_f6), rowvar = False),2)) 
```

```py
Correlation Matrix of the 6 original features components:
[[ 1\.    0.79 -0.49 -0.25  0.71  0.12]
 [ 0.79  1\.   -0.32 -0.13  0.48  0.04]
 [-0.49 -0.32  1\.    0.14 -0.53  0.47]
 [-0.25 -0.13  0.14  1\.   -0.24  0.24]
 [ 0.71  0.48 -0.53 -0.24  1\.    0.35]
 [ 0.12  0.04  0.47  0.24  0.35  1\.  ]]

Correlation Matrix of the 6 principle components' scores:
[[ 1\.  0\. -0\.  0\.  0\. -0.]
 [ 0\.  1\. -0\. -0\. -0\. -0.]
 [-0\. -0\.  1\. -0\. -0\.  0.]
 [ 0\. -0\. -0\.  1\.  0\.  0.]
 [ 0\. -0\. -0\.  0\.  1\.  0.]
 [-0\. -0\.  0\.  0\.  0\.  1.]] 
```

æ–°çš„æŠ•å½±ç‰¹å¾ï¼ˆå³ä½¿æ²¡æœ‰é™ç»´ï¼Œ$p=m$ï¼‰æ‰€æœ‰æˆå¯¹çš„ç›¸å…³æ€§éƒ½æ˜¯ 0.0ï¼

+   æ‰€æœ‰æŠ•å½±ç‰¹å¾å½¼æ­¤ä¹‹é—´çº¿æ€§ç‹¬ç«‹

## é™ç»´å¯¹ä¸¤ä¸ªç‰¹å¾å…³ç³»çš„å½±å“

å½“æˆ‘ä»¬ä¿ç•™ 1 åˆ° 6 ä¸ªä¸»æˆåˆ†æ—¶ï¼Œä»…æŸ¥çœ‹å­”éš™ç‡ä¸å¯¹æ•°æ¸—é€ç‡çš„åŒå˜é‡å…³ç³»å°†å¾ˆæœ‰è¶£ã€‚

+   è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨çŸ©é˜µæ•°å­¦æ¥é€šè¿‡ PCA è¿›è¡Œåè½¬ï¼Œä»¥åŠç”¨å„ç§ä¸»æˆåˆ†æ•°é‡è¿›è¡Œæ ‡å‡†åŒ–ï¼Œç„¶åç»˜åˆ¶å¯¹æ•°æ¸—é€ç‡ä¸å­”éš™ç‡çš„æ•£ç‚¹å›¾ã€‚

```py
nComp = 6
xhat_6d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])
xhat_6d = sd_f6*xhat_6d + mu_f6

nComp = 5
xhat_5d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])
xhat_5d = sd_f6*xhat_5d + mu_f6

nComp = 4
xhat_4d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])
xhat_4d = sd_f6*xhat_4d + mu_f6

nComp = 3
xhat_3d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])
xhat_3d = sd_f6*xhat_3d + mu_f6

nComp = 2
xhat_2d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])
xhat_2d = sd_f6*xhat_2d + mu_f6

nComp = 1
xhat_1d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])
xhat_1d = sd_f6*xhat_1d + mu_f6

f, (ax12, ax13, ax14, ax15, ax16, ax17, ax18) = plt.subplots(1, 7,figsize=(20,20))
f.subplots_adjust(wspace=0.7)

ax12.scatter(my_data_f6["Por"],my_data_f6["LogPerm"],s=None, c="darkorange",marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax12.set_title('Original Data'); ax12.set_xlabel('Por'); ax12.set_ylabel('LogPerm')
ax12.set_ylim(0.0,3.0); ax12.set_xlim(8,22); ax12.set_aspect(4.0); 

ax13.scatter(xhat_1d[:,0],xhat_1d[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax13.set_title('1 Principal Component'); ax13.set_xlabel('Por'); ax13.set_ylabel('LogPerm')
ax13.set_ylim(0.0,3.0); ax13.set_xlim(8,22); ax13.set_aspect(4.0)

ax14.scatter(xhat_2d[:,0],xhat_2d[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax14.set_title('2 Principal Components'); ax14.set_xlabel('Por'); ax14.set_ylabel('LogPerm')
ax14.set_ylim(0.0,3.0); ax14.set_xlim(8,22); ax14.set_aspect(4.0)

ax15.scatter(xhat_3d[:,0],xhat_3d[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax15.set_title('3 Principal Components'); ax15.set_xlabel('Por'); ax15.set_ylabel('LogPerm')
ax15.set_ylim(0.0,3.0); ax15.set_xlim(8,22); ax15.set_aspect(4.0)

ax16.scatter(xhat_4d[:,0],xhat_4d[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax16.set_title('4 Principal Components'); ax16.set_xlabel('Por'); ax16.set_ylabel('LogPerm')
ax16.set_ylim(0.0,3.0); ax16.set_xlim(8,22); ax16.set_aspect(4.0)

ax17.scatter(xhat_5d[:,0],xhat_5d[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax17.set_title('5 Principal Components'); ax17.set_xlabel('Por'); ax17.set_ylabel('LogPerm')
ax17.set_ylim(0.0,3.0); ax17.set_xlim(8,22); ax17.set_aspect(4.0)

ax18.scatter(xhat_6d[:,0],xhat_6d[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax18.set_title('6 Principal Components'); ax18.set_xlabel('Por'); ax18.set_ylabel('LogPerm')
ax18.set_ylim(0.0,3.0); ax18.set_xlim(8,22); ax18.set_aspect(4.0)

plt.show() 
```

![2c07008ca4c1cad616bfb73f5ffed082b67c565a0bbab5e216624e59e8c949ff.png](img/5a69f72819a4fda450143b47c8b81454.png)

éå¸¸æœ‰è¶£åœ°è§‚å¯Ÿéšç€æˆ‘ä»¬åŒ…å«æ›´å¤šæˆåˆ†ï¼Œå¯¹æ•°æ¸—é€ç‡ä¸å­”éš™ç‡ä¹‹é—´çš„åŒå˜é‡å…³ç³»çš„å‡†ç¡®æ€§å¦‚ä½•æé«˜ã€‚è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ–¹å·®ã€‚

```py
print('1 Principal Component : Variance Por =',np.round(np.var(xhat_1d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_1d[:,1])/(sd_f6[1]*sd_f6[1]),2))

print('2 Principal Components: Variance Por =',np.round(np.var(xhat_2d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_2d[:,1])/(sd_f6[1]*sd_f6[1]),2))

print('3 Principal Components: Variance Por =',np.round(np.var(xhat_3d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_3d[:,1])/(sd_f6[1]*sd_f6[1]),2))

print('4 Principal Components: Variance Por =',np.round(np.var(xhat_4d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_4d[:,1])/(sd_f6[1]*sd_f6[1]),2))

print('5 Principal Components: Variance Por =',np.round(np.var(xhat_5d[:,0])/(sd_f6[0]*sd_f6[0]),2),'  Variance Log Perm = ',np.round(np.var(xhat_5d[:,1])/(sd_f6[1]*sd_f6[1]),2))

print('6 Principal Components: Variance Por =',np.round(np.var(xhat_6d[:,0])/(sd_f6[0]*sd_f6[0]),2),'  Variance Log Perm = ',np.round(np.var(xhat_6d[:,1])/(sd_f6[1]*sd_f6[1]),2)) 
```

```py
1 Principal Component : Variance Por = 0.86  Variance Log Perm =  0.63
2 Principal Components: Variance Por = 0.88  Variance Log Perm =  0.65
3 Principal Components: Variance Por = 0.88  Variance Log Perm =  0.66
4 Principal Components: Variance Por = 0.91  Variance Log Perm =  0.96
5 Principal Components: Variance Por = 1.0   Variance Log Perm =  1.0
6 Principal Components: Variance Por = 1.0   Variance Log Perm =  1.0 
```

è¿™å¾ˆæœ‰è¶£ã€‚ä½¿ç”¨ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ï¼Œæˆ‘ä»¬æè¿°äº† 86%çš„å­”éš™ç‡æ–¹å·®ã€‚æ¥ä¸‹æ¥çš„ä¸¤ä¸ªä¸»æˆåˆ†æ²¡æœ‰æä¾›å¤ªå¤šå¸®åŠ©ã€‚ç„¶åç¬¬å››å’Œç¬¬äº”ä¸ªä¸»æˆåˆ†å‡ºç°äº†ä¸€ä¸ªè·³è·ƒã€‚

+   å½“ç„¶ï¼Œé—®é¢˜æ˜¯ 6 ç»´çš„ï¼Œä¸ä»…ä»…æ˜¯å­”éš™ç‡ä¸å¯¹æ•°æ¸—é€ç‡ï¼Œä½†æ˜¯çœ‹åˆ°ä¸»æˆåˆ†æ•°é‡ä¸æ¯ä¸ªåŸå§‹ç‰¹å¾ä¿ç•™çš„æ–¹å·®ä¹‹é—´çš„å…³ç³»æ˜¯å¦æœ‰è¶£

+   ä¸»æˆåˆ†å¹¶æ²¡æœ‰å‡åŒ€åœ°æè¿°æ¯ä¸ªç‰¹å¾

## é™ç»´å¯¹æ‰€æœ‰ç‰¹å¾çŸ©é˜µæ•£ç‚¹å›¾çš„å½±å“

è®©æˆ‘ä»¬çœ‹çœ‹çŸ©é˜µæ•£ç‚¹å›¾ï¼Œä»¥æŸ¥çœ‹æ‰€æœ‰åŒå˜é‡ç»„åˆã€‚

+   é¦–å…ˆï¼Œåšä¸€äº›è®°å½•ï¼Œæˆ‘ä»¬å¿…é¡»å°† 6D é™ç»´æ¨¡å‹æ”¾å…¥ DataFrames ä¸­ï¼ˆç›®å‰æ˜¯ Numpy ndarraysï¼‰ã€‚

```py
df_1d = pd.DataFrame(data=xhat_1d,columns=features)   
df_2d = pd.DataFrame(data=xhat_2d,columns=features)
df_3d = pd.DataFrame(data=xhat_3d,columns=features)
df_4d = pd.DataFrame(data=xhat_4d,columns=features)
df_5d = pd.DataFrame(data=xhat_5d,columns=features)
df_6d = pd.DataFrame(data=xhat_6d,columns=features) 
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº› DataFrames ç”ŸæˆçŸ©é˜µæ•£ç‚¹å›¾ã€‚å½“æˆ‘ä»¬æ·»åŠ ä¸»æˆåˆ†æ—¶ï¼Œçœ‹åˆ°åŒå˜é‡å›¾çš„å‡†ç¡®æ€§æé«˜æ˜¯éå¸¸æœ‰è¶£çš„ã€‚è€Œä¸”ï¼Œä»…ä½¿ç”¨ä¸¤ä¸ªä¸»æˆåˆ†ï¼Œæˆ‘ä»¬å°±å¾ˆå¥½åœ°æ•æ‰åˆ°ä¸€äº›å˜é‡å¯¹çš„åŒå˜é‡å…³ç³»ã€‚

```py
fig = plt.figure()

pd_plot.scatter_matrix(my_data_f6, alpha = 0.1,           # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('Original Data')

pd_plot.scatter_matrix(df_1d, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('1 Principal Component')

pd_plot.scatter_matrix(df_2d, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('2 Principal Components')

pd_plot.scatter_matrix(df_3d, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('3 Principal Components')

pd_plot.scatter_matrix(df_4d, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('4 Principal Components')

pd_plot.scatter_matrix(df_5d, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('5 Principal Components')

pd_plot.scatter_matrix(df_6d, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('6 Principal Components')

plt.show() 
```

```py
<Figure size 640x480 with 0 Axes> 
```

![å›¾ç‰‡](img/e78345fd784cdb36b14f282fe91180d5.png) ![å›¾ç‰‡](img/73bc7cc3f14b45b2024f8ac9631eeff6.png) ![å›¾ç‰‡](img/58965505867559377436105bcfe2bd2f.png) ![å›¾ç‰‡](img/edc87f7af42bdbab50ece3313c71f72d.png) ![å›¾ç‰‡](img/deadaf11c865b21029ccb2121e4df45b.png) ![å›¾ç‰‡](img/3847c4e4cad0f11b7497cac6c1234968.png) ![å›¾ç‰‡](img/ffde399e1f56a914a61abaada97a2abb.png)

## å¯¹ä¸ç›¸å…³æ•°æ®è¿›è¡Œä¸»æˆåˆ†åˆ†æ

è®©æˆ‘ä»¬å†è¯•ä¸€æ¬¡æµ‹è¯•ï¼Œå¯¹ä¸ç›¸å…³æ•°æ®è¿›è¡Œä¸»æˆåˆ†åˆ†æã€‚

+   æˆ‘ä»¬ä¸º 5 ä¸ªç‰¹å¾ç”Ÿæˆäº†å¤§é‡éšæœºæ ·æœ¬ï¼ˆn å¾ˆå¤§ï¼‰ã€‚

+   æˆ‘ä»¬å°†å‡è®¾ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒ

```py
x_rand = np.random.rand(10000,5); df_x_rand = pd.DataFrame(x_rand)
print('Variance of original features: ', np.round(np.var(x_rand, axis = 0),2))
print('Proportion of variance of original features: ', np.round(np.var(x_rand, axis = 0)/np.sum(np.var(x_rand, axis = 0)),2))
print('Correlation Matrix of original features:\n'); print(np.round(np.cov(x_rand, rowvar = False),2)); print()

pd_plot.scatter_matrix(df_x_rand, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('Original Features')

pca_rand = PCA(n_components=5)
pca_rand.fit(x_rand)
print('PCA Variance Explained ', np.round(pca_rand.explained_variance_ratio_,2))  

scores_x_rand = pca_rand.transform(x_rand); df_scores_x_rand = pd.DataFrame(scores_x_rand)

print('\nCorrelation Matrix of scores:\n'); print(np.round(np.cov(scores_x_rand, rowvar = False),2)); print()

pd_plot.scatter_matrix(df_scores_x_rand, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('Principal Component Scores') 
```

```py
Variance of original features:  [0.08 0.08 0.08 0.08 0.08]
Proportion of variance of original features:  [0.2 0.2 0.2 0.2 0.2]
Correlation Matrix of original features:

[[ 0.08 -0\.    0\.   -0\.    0\.  ]
 [-0\.    0.08  0\.    0\.   -0\.  ]
 [ 0\.    0\.    0.08  0\.   -0\.  ]
 [-0\.    0\.    0\.    0.08  0\.  ]
 [ 0\.   -0\.   -0\.    0\.    0.08]] 
```

```py
PCA Variance Explained  [0.21 0.2  0.2  0.2  0.19]

Correlation Matrix of scores:

[[ 0.09 -0\.   -0\.   -0\.    0\.  ]
 [-0\.    0.08  0\.   -0\.   -0\.  ]
 [-0\.    0\.    0.08  0\.    0\.  ]
 [-0\.   -0\.    0\.    0.08  0\.  ]
 [ 0\.   -0\.    0\.    0\.    0.08]] 
```

```py
Text(0.5, 0.98, 'Principal Component Scores') 
```

![å›¾ç‰‡](img/3e13125b9ca06c498e74bfcd5ae47b64.png) ![å›¾ç‰‡](img/67ae027cd9095c50fbc22e0a2f176a6b.png)

å½“ä¸»æˆåˆ†åˆ†æåº”ç”¨äºä¸ç›¸å…³ã€å‡åŒ€åˆ†å¸ƒçš„ç‰¹å¾æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

+   æ‰€æœ‰ä¸»æˆåˆ†æè¿°äº†ç›¸åŒæ•°é‡çš„æ–¹å·®

+   æ— æ³•é€šè¿‡ç‰¹å¾æŠ•å½±è¿›è¡Œé™ç»´

+   ç‹¬ç«‹éšæœºå˜é‡çš„çº¿æ€§ç»„åˆå¼•å‘ä¸­å¿ƒæé™å®šç†ï¼Œä¸»æˆåˆ†å¾—åˆ†è¶‹å‘äºé«˜æ–¯åˆ†å¸ƒï¼ˆè§ä¸Šé¢çŸ©é˜µæ•£ç‚¹å›¾ä¸­ç‚¹çš„å››èˆäº”å…¥ï¼‰

## åœ¨æ–°æ•°æ®é›†ä¸Šè¿›è¡Œå®è·µ

å¥½çš„ï¼Œæ˜¯æ—¶å€™å¼€å§‹å·¥ä½œäº†ã€‚è®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªæ•°æ®é›†å¹¶æ‰§è¡Œ PCAï¼Œ

+   ç´§å‡‘çš„ä»£ç 

+   åŸºæœ¬å¯è§†åŒ–

+   ä¿å­˜è¾“å‡º

æ‚¨å¯ä»¥é€‰æ‹©è¿™äº›æ•°æ®é›†ä¹‹ä¸€æˆ–ä¿®æ”¹ä»£ç å¹¶æ·»åŠ æ‚¨è‡ªå·±çš„æ•°æ®é›†æ¥æ‰§è¡Œæ­¤æ“ä½œã€‚

### æ•°æ®é›† 0ï¼Œéå¸¸è§„å¤šå…ƒ v4

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒæ•°æ®é›† [unconv_MV.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v4.csv)ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…æ‹¬æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   äº•å¹³å‡å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°é˜»æŠ— (kg/mÂ³ x m/s x 10â¶)

+   å²©è„†æ€§æ¯” (%)

+   æ€»æœ‰æœºç¢³ (%) 

+   ç»ç’ƒè´¨åå°„ç‡ (%)

+   åˆå§‹ç”Ÿäº§ 90 å¤©å¹³å‡ (MCFPD)ã€‚

### æ•°æ®é›† 1ï¼ŒåäºŒï¼Œ12

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€äºŒç»´ç©ºé—´æ•°æ®é›† [12_sample_data.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/12_sample_data.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 480 å£éå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   X (m), Y (m) ä½ç½®åæ ‡

+   å²©æ€§ (0 - ç²˜åœŸï¼Œ1 - ç ‚)

+   å•ä½è½¬æ¢åçš„å­”éš™ç‡ (%)

+   æ¸—é€ç‡ (mD)

+   å£°é˜»æŠ— (kg/mÂ³ x m/s x 10â¶)

### æ•°æ®é›† 2ï¼Œå‚¨å±‚ 21

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ä¸‰ç»´ç©ºé—´æ•°æ®é›† [res21_wells.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/res21_wells.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 10,000m x 10,000m x 50 m å‚¨å±‚å•å…ƒçš„ 73 å£å‚ç›´äº•çš„å˜é‡ï¼š

+   äº•ï¼ˆIDï¼‰

+   X (m), Y (m), æ·±åº¦ (m) ä½ç½®åæ ‡

+   å•ä½è½¬æ¢åçš„å­”éš™ç‡ (%)

+   æ¸—é€ç‡ (mD)

+   å•ä½è½¬æ¢åçš„å£°é˜»æŠ— (kg/m2s*10â¶)

+   å²©æ€§ï¼ˆåˆ†ç±»ï¼‰- ä»ç²˜åœŸã€ç ‚è´¨ç²˜åœŸã€ç²˜åœŸè´¨ç ‚åˆ°ç ‚å²©çš„é¡ºåº

+   å¯†åº¦ (g/cmÂ³)

+   å‹ç¼©æ³¢é€Ÿåº¦ (m/s)

+   æ¨æ°æ¨¡é‡ (GPa)

+   å‰ªåˆ‡æ³¢é€Ÿåº¦ (m/s)

+   å‰ªåˆ‡æ¨¡é‡ (GPa)

æˆ‘ä»¬ä½¿ç”¨ pandas çš„ â€˜read_csvâ€™ å‡½æ•°å°†è¡¨æ ¼æ•°æ®åŠ è½½åˆ°åä¸º â€˜my_dataâ€™ çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

+   æˆ‘ä»¬è¿˜ç”¨æ•°æ®èŒƒå›´å’Œæ ‡ç­¾å¡«å……åˆ—è¡¨ï¼Œä»¥ä¾¿äºç»˜å›¾

åŠ è½½æ•°æ®å¹¶æ ¼å¼åŒ–ï¼Œ

+   åˆ é™¤å“åº”ç‰¹å¾

+   æ ¹æ®éœ€è¦é‡æ–°æ ¼å¼åŒ–ç‰¹å¾

+   æ­¤å¤–ï¼Œæˆ‘ä¹Ÿå–œæ¬¢å°†å…ƒæ•°æ®å­˜å‚¨åœ¨åˆ—è¡¨ä¸­

```py
idata = 0                                                     # select the dataset

if idata == 0:
    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['Well','Prod'],axis=1,inplace=True)          # remove well index and response feature

    features = df_new.columns.values.tolist()                 # store the names of the features

    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting

    flabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Brittleness Ratio (%)', # set the names for plotting
             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']

    ftitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting
             'Total Organic Carbon','Vitrinite Reflectance']

elif idata == 1:
    names = {'Porosity':'Por'}

    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['X','Y','Unnamed: 0'],axis=1,inplace=True)   # remove response feature
    df_new = df_new.rename(columns=names)

    features = df_new.columns.values.tolist()                 # store the names of the features

    xmin_new = [0.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [10000.0,10000.0,1.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting

    flabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Facies (categorical)',
              'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting

    ftitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',
              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']

elif idata == 2:  
    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['Well_ID','X','Y','CumulativeOil'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature
    df_new = df_new.dropna(how='any',inplace=False)

    features = df_new.columns.values.tolist()                 # store the names of the features

    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting

    flabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Facies (categorical)',
              'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting

    ftitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',
              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']

df_new[df_new.columns] = MinMaxScaler().fit_transform(df_new) # min/max normalize all the features
df_new.head(n=13) 
```

|  | Por | Perm | AI | Brittle | TOC | VR |
| --- | --- | --- | --- | --- | --- | --- |
| 0 | 0.325294 | 0.204805 | 0.453731 | 0.960076 | 0.569620 | 0.711340 |
| 1 | 0.342941 | 0.274600 | 0.579104 | 0.480038 | 0.455696 | 0.489691 |
| 2 | 0.439412 | 0.167048 | 0.814925 | 0.842894 | 0.455696 | 0.922680 |
| 3 | 0.654118 | 0.643021 | 0.402985 | 0.393378 | 0.535865 | 0.489691 |
| 4 | 0.645294 | 0.393593 | 0.567164 | 0.000000 | 0.717300 | 0.500000 |
| 5 | 0.469412 | 0.421053 | 0.420896 | 0.581278 | 0.476793 | 0.381443 |
| 6 | 0.408235 | 0.282609 | 0.492537 | 0.719035 | 0.417722 | 0.474227 |
| 7 | 0.295882 | 0.217391 | 0.588060 | 0.573103 | 0.371308 | 0.515464 |
| 8 | 0.351176 | 0.181922 | 0.343284 | 0.747105 | 0.481013 | 0.541237 |
| 9 | 0.394118 | 0.321510 | 0.725373 | 0.752964 | 0.561181 | 0.886598 |
| 10 | 0.499412 | 0.372998 | 0.280597 | 0.683608 | 0.535865 | 0.432990 |
| 11 | 0.567059 | 0.591533 | 0.301493 | 0.519962 | 0.725738 | 0.479381 |
| 12 | 0.604118 | 0.490847 | 0.453731 | 0.759095 | 0.573840 | 0.541237 |

### æ‰§è¡Œä¸»æˆåˆ†åˆ†æ

æ‰§è¡Œä¸»æˆåˆ†åˆ†æï¼Œ

1.  è®¡ç®—ä¸»æˆåˆ†è½½è·

1.  é€‰æ‹©ä¸»æˆåˆ†çš„æ•°é‡ä»¥æè¿°ç›®æ ‡æ–¹å·®è§£é‡Š

1.  åˆ›å»ºä¸€ä¸ªæ–°çš„ DataFrameï¼ŒåŒ…å«ä¸»æˆåˆ†å¾—åˆ†

```py
var_explained = 0.95                                          # select the minimum variance explained

n_components = min(len(df_new.columns),len(df_new)-1)         # max components is min of number of features or number of data - 1
pca_new = PCA(n_components=n_components).fit(df_new.values)   # calculate PCA
pca_scores = pca_new.fit_transform(df_new.values)

cumulative_variance = np.cumsum(pca_new.explained_variance_ratio_) # calculate cumulative explained variance

n_selected = np.argmax(cumulative_variance >= var_explained) + 1 # find number of components to retain 95% variance

df_new_projected = pd.DataFrame(pca_scores[:, :n_selected],columns=[f'PC{i+1}' for i in range(n_selected)],
            index=df_new.index)                               # project data to that many principal components

sns.pairplot(df_new_projected.iloc[:,:], plot_kws={'alpha':1.0,'s':50}, palette = 'colorblind', corner=True) # matrix scatter plot
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.6, top=0.7, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/5c93c2ee554f6df8bbbb8936c7fec98e.png)

### æ£€æŸ¥ç´¯ç§¯æ–¹å·®è§£é‡Š

```py
plt.plot(np.arange(1,len(pca_new.explained_variance_ratio_)+1,1),np.cumsum(pca_new.explained_variance_ratio_*100),color='darkorange',alpha=0.8)
plt.scatter(np.arange(1,len(pca_new.explained_variance_ratio_)+1,1),np.cumsum(pca_new.explained_variance_ratio_*100),color='darkorange',alpha=0.8,edgecolor='black')
plt.plot([1,len(df_new.columns)],[95,95], color='black',linestyle='dashed'); plt.plot([n_selected,n_selected],[0,100],color='red',zorder=-1)
plt.annotate('Selected Number of Components = '+ str(n_selected),[n_selected,10],rotation=270,color='red')
plt.xlabel('Principal Component'); plt.ylabel('Cumulative Variance Explained'); plt.title('Cumulative Variance Explained by Principal Component')
fmt = '%.0f%%' # Format you want the ticks
plt.xticks(range(1, len(cumulative_variance) + 1))
yticks = mtick.FormatStrFormatter(fmt); plt.xlim(1,len(pca_new.explained_variance_ratio_)); plt.ylim(0,100.0) 
plt.annotate('95% variance explained',[4.05,90]); add_grid()
plt.gca().yaxis.set_major_formatter(PercentFormatter(100.0))  # 1.0 = 100%
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/4810ab4ed9ff9b4c3c7270ea4fce631d.png)

### ä¿å­˜ä¸»æˆåˆ†

ç°åœ¨æˆ‘ä»¬å¯ä»¥é€‰æ‹©å†™å‡ºå…·æœ‰é™ç»´ä¸»æˆåˆ†å¾—åˆ†çš„ DataFrameã€‚

```py
save_PCA = True                                        # save the imputed DataFrame?

if save_PCA == True:
    folder = r'C:\Local'
    file_name = r'dataframe_PCA.csv'

    df_new_projected.to_csv(folder + "/" + file_name, index=False) 
```

```py
---------------------------------------------------------------------------
OSError  Traceback (most recent call last)
Cell In[42], line 7
  4 folder = r'C:\Local'
  5 file_name = r'dataframe_PCA.csv'
----> 7 df_new_projected.to_csv(folder + "/" + file_name, index=False)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\core\generic.py:3772, in NDFrame.to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)
  3761 df = self if isinstance(self, ABCDataFrame) else self.to_frame()
  3763 formatter = DataFrameFormatter(
  3764     frame=df,
  3765     header=header,
   (...)
  3769     decimal=decimal,
  3770 )
-> 3772 return DataFrameRenderer(formatter).to_csv(
  3773     path_or_buf,
  3774     lineterminator=lineterminator,
  3775     sep=sep,
  3776     encoding=encoding,
  3777     errors=errors,
  3778     compression=compression,
  3779     quoting=quoting,
  3780     columns=columns,
  3781     index_label=index_label,
  3782     mode=mode,
  3783     chunksize=chunksize,
  3784     quotechar=quotechar,
  3785     date_format=date_format,
  3786     doublequote=doublequote,
  3787     escapechar=escapechar,
  3788     storage_options=storage_options,
  3789 )

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\formats\format.py:1186, in DataFrameRenderer.to_csv(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)
  1165     created_buffer = False
  1167 csv_formatter = CSVFormatter(
  1168     path_or_buf=path_or_buf,
  1169     lineterminator=lineterminator,
   (...)
  1184     formatter=self.fmt,
  1185 )
-> 1186 csv_formatter.save()
  1188 if created_buffer:
  1189     assert isinstance(path_or_buf, StringIO)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\formats\csvs.py:240, in CSVFormatter.save(self)
  236  """
  237 Create the writer & save.
  238 """
  239 # apply compression and byte/text conversion
--> 240 with get_handle(
  241     self.filepath_or_buffer,
  242     self.mode,
  243     encoding=self.encoding,
  244     errors=self.errors,
  245     compression=self.compression,
  246     storage_options=self.storage_options,
  247 ) as handles:
  248     # Note: self.encoding is irrelevant here
  249     self.writer = csvlib.writer(
  250         handles.handle,
  251         lineterminator=self.lineterminator,
   (...)
  256         quotechar=self.quotechar,
  257     )
  259     self._save()

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:737, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)
  735 # Only for write methods
  736 if "r" not in mode and is_path:
--> 737     check_parent_directory(str(handle))
  739 if compression:
  740     if compression != "zstd":
  741         # compression libraries do not like an explicit text-mode

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:600, in check_parent_directory(path)
  598 parent = Path(path).parent
  599 if not parent.is_dir():
--> 600     raise OSError(rf"Cannot save file into a non-existent directory: '{parent}'")

OSError: Cannot save file into a non-existent directory: 'C:\Local' 
```

## è¯„è®º

è¿™æ˜¯å¯¹ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è¿›è¡Œé™ç»´çš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå¾—æ›´å¤šï¼Œè®¨è®ºæ›´å¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´ YouTube è®²åº§ä¸­çš„èµ„æºé“¾æ¥ï¼Œè§†é¢‘æè¿°ä¸­åŒ…å«èµ„æºé“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![å›¾ç‰‡](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­å†…åŠå…¬å®¤çš„è¿ˆå…‹å°”Â·çš®å°”å¥‡å…¹æ•™æˆã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡å…¹æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[Cockrell å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œä»–åœ¨è¯¥æ ¡ä»äº‹å’Œæ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜

+   æ‹…ä»»[è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²æ’°å†™è¶…è¿‡ 70 ç¯‡[åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[åœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬è¿‘æœŸå‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[Python åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[Python åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå®è·µæŒ‡å—ä¸ä»£ç ](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å« 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ª GitHub ä»“åº“ä¸­çš„è¯¦ç»†å·¥ä½œæµç¨‹ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œä¸“ä¸šäººå£«ï¼Œæä¾›æŒç»­æ›´æ–°çš„å†…å®¹ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

å¸Œæœ›è¿™ä¸ªå†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   å¯¹åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰æ„Ÿå…´è¶£å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æˆ‘å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°æ‚¨ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”å¥‡å…¹ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢

æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥è·å–ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰çš„åŠ¨æœº

ä¸æ›´å¤šç‰¹å¾/å˜é‡ä¸€èµ·å·¥ä½œæ›´å›°éš¾ï¼

1.  å¯è§†åŒ–æ•°æ®å’Œæ¨¡å‹æ›´å›°éš¾

1.  éœ€è¦æ›´å¤šæ•°æ®æ¥æ¨æ–­è”åˆæ¦‚ç‡

1.  ç‰¹å¾ç©ºé—´çš„æ•°æ®è¦†ç›–èŒƒå›´æ›´å°‘

1.  æ£€æŸ¥/éªŒè¯æ¨¡å‹æ›´å›°éš¾

1.  æ›´å¯èƒ½å­˜åœ¨å†—ä½™ç‰¹å¾ï¼Œä¾‹å¦‚å¤šé‡å…±çº¿æ€§ï¼Œå¯¼è‡´æ¨¡å‹ä¸ç¨³å®š

1.  è€—è´¹æ›´å¤šçš„è®¡ç®—åŠªåŠ›ã€æ›´å¤šçš„è®¡ç®—èµ„æºå’Œæ›´é•¿çš„è¿è¡Œæ—¶é—´

1.  æ›´å¤æ‚çš„æ¨¡å‹æ›´æœ‰å¯èƒ½è¿‡æ‹Ÿåˆ

1.  æ¨¡å‹æ„å»ºéœ€è¦æ›´å¤šä¸“ä¸šæ—¶é—´

ä¸æ›´å°‘çš„ã€å…·æœ‰ä¿¡æ¯é‡çš„ç‰¹å¾ç›¸æ¯”ï¼Œå°†æ‰€æœ‰ç‰¹å¾éƒ½æŠ•å…¥æ¨¡å‹ä¸­å¯ä»¥å¾—åˆ°æ›´å¥½çš„æ¨¡å‹ï¼è¿™éƒ¨åˆ†åŠ¨æœºå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±ç»´åº¦è¯…å’’é©±åŠ¨çš„ã€‚

## ç»´åº¦è¯…å’’

1.  **æ•°æ®å’Œæ¨¡å‹å¯è§†åŒ–** - æˆ‘ä»¬æ— æ³•å¯è§†åŒ–è¶…è¿‡ 3Dï¼Œå³æ— æ³•è®¿é—®æ•°æ®æ‹Ÿåˆï¼Œè¯„ä¼°å†…æ’ä¸å¤–æ¨ã€‚

+   è€ƒè™‘ä¸€ä¸ªä»¥çŸ©é˜µæ•£ç‚¹å›¾å½¢å¼å±•ç¤ºçš„ 5D ç¤ºä¾‹ï¼Œå³ä½¿åœ¨åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªå›¾ä¹Ÿæœ‰æç«¯çš„è¾¹ç¼˜åŒ–åˆ° 2Dï¼Œ

![å›¾ç‰‡](img/ecf50f66114aec17ea35fde1342d66c4.png)

5D æ•°æ®çš„çŸ©é˜µæ•£ç‚¹å›¾ç¤ºä¾‹ã€‚

1.  **é‡‡æ ·** - è¶³å¤Ÿçš„æ ·æœ¬æ•°é‡ï¼Œä»¥æ¨æ–­è¯¸å¦‚è”åˆæ¦‚ç‡ $P(x_1,\ldots,x_m)$ è¿™æ ·çš„ç»Ÿè®¡é‡ã€‚

+   å›å¿†ä¸€ä¸‹ç›´æ–¹å›¾æˆ–å½’ä¸€åŒ–ç›´æ–¹å›¾çš„è®¡ç®—ï¼šæˆ‘ä»¬å»ºç«‹åŒºé—´å¹¶è®¡ç®—æ¯ä¸ªåŒºé—´çš„é¢‘ç‡æˆ–æ¦‚ç‡ã€‚

+   æˆ‘ä»¬éœ€è¦æ¯ä¸ªåŒºé—´çš„åä¹‰æ•°æ®æ ·æœ¬æ•°ï¼Œå› æ­¤åœ¨ä¸€ç»´ä¸­æˆ‘ä»¬éœ€è¦ $ğ‘›=ğ‘›_{ğ‘ /ğ‘ğ‘–ğ‘›} \cdot ğ‘›_{ğ‘ğ‘–ğ‘›ğ‘ }$ ä¸ªæ ·æœ¬

+   ä½†åœ¨ mD ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ $n$ ä¸ªæ ·æœ¬æ¥è®¡ç®—ç¦»æ•£åŒ–è”åˆæ¦‚ç‡ï¼Œ

$$ ğ‘›=ğ‘›_{ğ‘ /ğ‘ğ‘–ğ‘›} \cdot ğ‘›_{ğ‘ğ‘–ğ‘›ğ‘ }^m $$

+   ä¾‹å¦‚ï¼Œæ¯ä¸ªåŒºé—´æœ‰ 10 ä¸ªæ ·æœ¬ï¼Œ35 ä¸ªåŒºé—´ï¼Œåœ¨ 2D ä¸­éœ€è¦ 12,250 ä¸ªæ ·æœ¬ï¼Œåœ¨ 3D ä¸­éœ€è¦ 428,750 ä¸ªæ ·æœ¬

![](img/bc8823819263f4497ef6baab93a9ee38.png)

ç¤ºä¾‹ 2D æ•°æ®ï¼Œæ¯ä¸ªç‰¹å¾æœ‰ 35 ä¸ªåŒºé—´ã€‚

1.  **æ ·æœ¬è¦†ç›–ç‡** - æ ·æœ¬å€¼èŒƒå›´è¦†ç›–é¢„æµ‹ç‰¹å¾ç©ºé—´ã€‚

+   æ ·æœ¬è¦†ç›–çš„åˆ†æ•°ï¼Œå¯¹äº 1 ä¸ªç‰¹å¾æˆ‘ä»¬å‡è®¾ 80%çš„è¦†ç›–ç‡

+   è®°ä½ï¼Œæˆ‘ä»¬é€šå¸¸åªç›´æ¥é‡‡æ ·åœ°ä¸‹ä½“ç§¯çš„ $\frac{1}{10â·}$ã€‚

+   æ˜¯çš„ï¼Œè¦†ç›–çš„æ¦‚å¿µæ˜¯ä¸»è§‚çš„ï¼Œè¦†ç›–å¤šå°‘æ•°æ®ï¼Ÿå…³äºé—´éš™å‘¢ï¼Ÿç­‰ç­‰ã€‚

![](img/d8058511a88a482ed34b0cbd9eb34fec.png)

ç¤ºä¾‹ 2D æ•°æ®ï¼Œæ¯ä¸ªç‰¹å¾æœ‰ 35 ä¸ªåŒºé—´ã€‚

+   ç°åœ¨å¦‚æœæœ‰ä¸¤ä¸ªç‰¹å¾çš„ 80%è¦†ç›–ç‡ï¼Œ2D è¦†ç›–ç‡æ˜¯ 64%

![](img/8d96453b3f6c2a92a160fe4329a13d4a.png)

ç¤ºä¾‹ 2D æ•°æ®ï¼Œæ¯ä¸ªç‰¹å¾æœ‰ 35 ä¸ªåŒºé—´ã€‚

+   è¦†ç›–ç‡æ˜¯ï¼Œ

$$ c = c_1^m $$

1.  **æ‰­æ›²ç©ºé—´** - é«˜ç»´ç©ºé—´æ˜¯æ‰­æ›²çš„ã€‚

+   å–è¶…ç«‹æ–¹ä½“å†…å†…æ¥è¶…çƒä½“çš„ä½“ç§¯æ¯”ï¼Œ

$$ \frac{\pi^{\frac{m}{2}}}{m 2^{m-1} \Gamma\left(\frac{m}{2}\right)} \to 0 \quad \text{as} \quad m \to \infty $$

+   å›å¿†ä¸€ä¸‹ï¼Œ$\Gamma(ğ‘›)=(ğ‘›âˆ’1)!$ã€‚

+   é«˜ç»´ç©ºé—´å…¨æ˜¯è§’è€Œæ²¡æœ‰ä¸­é—´éƒ¨åˆ†ï¼Œè€Œä¸”å¤§å¤šæ•°é«˜ç»´ç©ºé—´ç¦»ä¸­é—´éƒ¨åˆ†å¾ˆè¿œï¼ˆå…¨æ˜¯è§’ï¼ï¼‰ã€‚

+   å› æ­¤ï¼Œé«˜ç»´ç©ºé—´ä¸­çš„è·ç¦»æ•æ„Ÿæ€§é™ä½ï¼Œå³å¯¹äºç©ºé—´ä¸­çš„ä»»ä½•éšæœºç‚¹ï¼Œæˆå¯¹è·ç¦»çš„æœŸæœ›éƒ½å˜å¾—ç›¸åŒã€‚

$$ \lim_{m \to \infty} \left( \mathbb{E}\left[\text{dist}_{\text{max}}(m) - \text{dist}_{\text{min}}(m)\right] \right) \to 0 $$

+   éšæœºç‚¹åœ¨è¶…ç©ºé—´ä¸­æˆå¯¹è·ç¦»èŒƒå›´çš„æœŸæœ›æé™è¶‹äºé›¶ã€‚å¦‚æœè·ç¦»å‡ ä¹éƒ½ç›¸åŒï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»å°±ä¸å†æœ‰æ„ä¹‰äº†ï¼

![](img/8c8d512cca4eb330150d1ba298831543.png)

è¶…ç«‹æ–¹ä½“å†…è¶…çƒä½“çš„ä½“ç§¯æ¯”ã€‚

+   è¿™é‡Œæ˜¯ä¸åŒç»´åº¦ä¸‹çš„æ‰­æ›²ç¨‹åº¦ï¼Œ

| m | nD / 2D |
| --- | --- |
| 2 | 1.0 |
| 5 | 0.28 |
| 10 | 0.003 |
| 20 | 0.00000003 |

1.  **å¤šé‡å…±çº¿æ€§** - é«˜ç»´æ•°æ®é›†æ›´æœ‰å¯èƒ½å­˜åœ¨å…±çº¿æ€§æˆ–å¤šé‡å…±çº¿æ€§ã€‚

+   ç”±å…¶ä»–ç‰¹å¾çº¿æ€§æè¿°çš„ç‰¹å¾å¯¼è‡´æ¨¡å‹æ–¹å·®é«˜ã€‚

## æ¨æ–­æ€§æœºå™¨å­¦ä¹ 

ä¸»æˆåˆ†åˆ†ææ˜¯ä¸€ç§æ¨æ–­æ€§ã€æ— ç›‘ç£çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚

+   æ²¡æœ‰å“åº”ç‰¹å¾ï¼Œ$y$ï¼Œåªæœ‰é¢„æµ‹ç‰¹å¾ï¼Œ

$$ ğ‘‹_1,\ldots,ğ‘‹_ğ‘š $$

+   æœºå™¨é€šè¿‡æ¨¡ä»¿æ•°æ®çš„ç´§å‡‘è¡¨ç¤ºæ¥å­¦ä¹ ã€‚

+   é€šè¿‡ç‰¹å¾æŠ•å½±ã€åˆ†ç»„åˆ†é…ã€ç¥ç»ç½‘ç»œæ½œåœ¨ç‰¹å¾ç­‰æ–¹å¼æ•æ‰æ¨¡å¼ã€‚

+   æˆ‘ä»¬ä¸“æ³¨äºå¯¹æ€»ä½“ã€è‡ªç„¶ç³»ç»Ÿçš„æ¨ç†ï¼Œè€Œä¸æ˜¯å¯¹å“åº”ç‰¹å¾çš„é¢„æµ‹ã€‚

## ä¸»æˆåˆ†åˆ†æ

ä¸»æˆåˆ†åˆ†ææ˜¯å¤šç§é™ç»´æ–¹æ³•ä¹‹ä¸€ï¼š

é™ç»´å°†æ•°æ®è½¬æ¢åˆ°æ›´ä½ç»´åº¦

+   ç»™å®šç‰¹å¾ï¼Œ$ğ‘‹_1,\dots,ğ‘‹_ğ‘š$ï¼Œæˆ‘ä»¬éœ€è¦ ${m \choose 2}=\frac{ğ‘š \cdot (ğ‘šâˆ’1)}{2}$ ä¸ªæ•£ç‚¹å›¾æ¥å¯è§†åŒ–äºŒç»´æ•£ç‚¹å›¾ã€‚

+   ä¸€æ—¦æˆ‘ä»¬æœ‰ 4 ä¸ªæˆ–æ›´å¤šå˜é‡ï¼Œç†è§£æ•°æ®å°±å˜å¾—éå¸¸å›°éš¾ã€‚

+   å›å¿†ç»´åº¦è¯…å’’ï¼Œå½±å“æ¨ç†ã€å»ºæ¨¡å’Œå¯è§†åŒ–ã€‚

ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯æ‰¾åˆ°ä¸€ä¸ªå¥½çš„ä½ç»´è¡¨ç¤ºï¼Œ$ğ‘$ï¼Œæ¥è¡¨ç¤ºåŸå§‹ç»´åº¦ $ğ‘š$

åœ¨é™ç»´è¡¨ç¤ºä¸­çš„å¥½å¤„ï¼š

1.  æ•°æ®å­˜å‚¨/è®¡ç®—æ—¶é—´

1.  æ›´å®¹æ˜“çš„å¯è§†åŒ–

1.  è¿˜è€ƒè™‘äº†å¤šé‡å…±çº¿æ€§

## æ­£äº¤å˜æ¢

å°†ä¸€ç»„è§‚æµ‹å€¼è½¬æ¢ä¸ºä¸€ç»„çº¿æ€§ä¸ç›¸å…³çš„å˜é‡ï¼Œç§°ä¸ºä¸»æˆåˆ†

+   å¯ç”¨çš„ä¸»æˆåˆ†æ•°é‡ï¼ˆ$k$ï¼‰æ˜¯ minâ¡($ğ‘›âˆ’1,ğ‘š$)

+   å—é™äºå˜é‡/ç‰¹å¾ï¼Œ$ğ‘š$ï¼Œå’Œæ•°æ®æ•°é‡

æˆåˆ†æ˜¯æœ‰åºçš„ï¼Œ

+   ç¬¬ä¸€æˆåˆ†æè¿°äº†æœ€å¤§çš„å¯èƒ½æ–¹å·®/å°½å¯èƒ½å¤šåœ°è§£é‡Šå˜å¼‚æ€§

+   ä¸‹ä¸€ä¸ªæˆåˆ†æè¿°äº†å¯èƒ½çš„æœ€å¤§å‰©ä½™æ–¹å·®

+   æœ€å¤šåˆ°æœ€å¤§æ•°é‡çš„ä¸»æˆåˆ†

æœ‰å¤šç§æ–¹å¼æ¥è§£é‡Šä¸»æˆåˆ†åˆ†æï¼Œ

## æœ€ä½³æ‹Ÿåˆè§£é‡Š

æœ€å°åŒ–æ•°æ®ä¸ä¸»æˆåˆ†ä¹‹é—´çš„æ­£äº¤æŠ•å½±è¯¯å·®ï¼Œ

$$ \min \sum_{i=1}^{n} \left( \left( X_i - \bar{X} \right) - \left( X_i - \bar{X} \right) V_p V_p^T \right)Â² $$

å…¶ä¸­ $ğ‘½_ğ’‘$ æ˜¯æˆ‘ä»¬å‰ $ğ’‘$ ä¸ªå‘é‡çš„çŸ©é˜µï¼Œ$ğ‘¿_ğ’Š$ æ˜¯æ ·æœ¬ $ğ‘–$ åœ¨æ‰€æœ‰ $ğ‘$ ä¸ªç‰¹å¾ä¸Šçš„å‘é‡ï¼Œ$\overline{X}$ æ˜¯å‡å€¼å‘é‡ï¼Œ

![å›¾ç‰‡](img/c9c37a5643c5eca21190ee3fa4c30880.png)

å°†äºŒç»´æ•°æ®æŠ•å½±åˆ°ä¸€ç»´ï¼ˆå·¦ï¼‰å’Œä¸‰ç»´æ•°æ®æŠ•å½±åˆ°äºŒç»´ï¼ˆå³ï¼‰çš„æ­£äº¤è¯¯å·®ï¼ˆå¾…æ·»åŠ å¼•ç”¨ï¼‰ã€‚

å…¶ä¸­ä¸»æˆåˆ†æè¿°äº†ä¸€ç»´ä¸­çš„å‘é‡å’Œå¹³é¢ä¸­çš„äºŒç»´ï¼Œä»¥åŠæŠ•å½±ç©ºé—´ä¸­çš„ä¸»æˆåˆ†å¾—åˆ†ï¼Œ

$$ (ğ‘¿_ğ’Šâˆ’\overline{ğ‘¿})ğ‘½_ğ’‘ $$

å¹¶ä¸”åœ¨åŸå§‹ç©ºé—´ä¸­å…·æœ‰é™ç»´çš„åå˜æ¢æ˜¯ï¼Œ

$$ (ğ‘¿_ğ’Šâˆ’\overline{X})ğ‘½_ğ’‘ ğ‘½_ğ’‘^ğ‘» $$

æ³¨æ„ï¼Œç”±äº $V_p$ çŸ©é˜µæ˜¯æ­£äº¤çš„ï¼Œ

$$ V_p^T = V_p^{-1} $$

## åŸºäºæ—‹è½¬çš„è§£é‡Š

æ­£äº¤å˜æ¢æ˜¯ä¸€ç§æ—‹è½¬ï¼Œå®ƒæœ€å¤§åŒ–äº†ç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®ï¼Œæœ€å¤§åŒ–ç¬¬äºŒä¸»æˆåˆ†çš„å‰©ä½™æ–¹å·®ï¼Œç­‰ç­‰ã€‚

å¦‚æœä½ æƒ³çœ‹åˆ° PCA ä½œä¸ºæ—‹è½¬çš„å®é™…æ“ä½œï¼Œè¯·æŸ¥çœ‹æˆ‘çš„[PCA æ—‹è½¬äº¤äº’å¼ Python ä»ªè¡¨æ¿](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_PCA_Rotation.ipynb)ã€‚

![å›¾ç‰‡](img/e1292179f35c2427c0914445105c302d.png)

æˆ‘çš„äº¤äº’å¼ä»ªè¡¨æ¿å±•ç¤ºäº† PCA ä½œä¸ºæ•°æ®æ—‹è½¬ã€‚

ä»è¿™ä¸ªä»ªè¡¨æ¿å¯ä»¥çœ‹å‡ºï¼Œå­˜åœ¨ä¸€ä¸ªæ—‹è½¬ï¼Œå®ƒæœ€å¤§åŒ–äº†ç¬¬ä¸€ä¸ªä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®ï¼ŒåŒæ—¶æ¶ˆé™¤äº†ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªä¸»æˆåˆ†ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

## ç‰¹å¾å€¼/ç‰¹å¾å‘é‡è§£é‡Š

å¯¹äºä¸»æˆåˆ†åˆ†æï¼Œæˆ‘ä»¬è®¡ç®—æ•°æ®åæ–¹å·®çŸ©é˜µï¼Œç‰¹å¾ç»„åˆçš„æˆå¯¹åæ–¹å·®ã€‚

+   ç„¶åæˆ‘ä»¬ä»åæ–¹å·®çŸ©é˜µä¸­è®¡ç®—ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ã€‚

+   ç‰¹å¾å€¼æ˜¯æ¯ä¸ªæˆåˆ†è§£é‡Šçš„æ–¹å·®ã€‚

+   æ•°æ®åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡æ˜¯ä¸»æˆåˆ†ã€‚

## ä¸»æˆåˆ†åˆ†æå·¥ä½œæµç¨‹

1.  æ ‡å‡†åŒ–ç‰¹å¾

$$ ğ‘‹^ğ‘ =\frac{ğ‘‹âˆ’\overline{X}}{\sigma_ğ‘‹} $$$$ ğ‘‹_1,\ldots,ğ‘‹_ğ‘š \quad \rightarrow ğ‘‹_1^ğ‘ ,\ldots,ğ‘‹_ğ‘š^ğ‘  $$

```py
where $X_i$ are original features and $X^s_i$ are transformed features. 
```

+   æ ‡å‡†åŒ–æ˜¯å¿…è¦çš„ï¼Œä»¥é˜²æ­¢å…·æœ‰è¾ƒå¤§æ–¹å·®çš„ç‰¹å¾ä¸»å¯¼è§£å†³æ–¹æ¡ˆï¼Œå³ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ä¸æ–¹å·®æœ€å¤§çš„ç‰¹å¾å¯¹é½

1.  è®¡ç®—æ ‡å‡†åŒ–çš„ç‰¹å¾åæ–¹å·®çŸ©é˜µ

$$ C_{(X_{m_1}, X_{m_2})} = \frac{\sum_{i=1}^{n} \left( (x_{m_1} - \bar{x}_{m_1})(x_{m_2} - \bar{x}_{m_2}) \right)}{n - 1} $$

```py
given the features are standardized the matrix is a correlation matrix 
```

$$\begin{split} C = \begin{bmatrix} C(X_1, X_1) & \cdots & C(X_1, X_m) \\ \vdots & \ddots & \vdots \\ C(X_m, X_1) & \cdots & C(X_m, X_m) \end{bmatrix} \end{split}$$

```py
given the features are standardized the matrix is a correlation matrix, 
```

$$\begin{split} C = \begin{bmatrix} \rho(X_1, X_1) & \cdots & \rho(X_1, X_m) \\ \vdots & \ddots & \vdots \\ \rho(X_m, X_1) & \cdots & \rho(X_m, X_m) \end{bmatrix} \end{split}$$

1.  è®¡ç®—åæ–¹å·®çŸ©é˜µ $ğ‘ª$ çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œ

    ç»™å®š $ğ¶$ æ˜¯ä¸€ä¸ªæ–¹é˜µ $(ğ‘š \times ğ‘š)$ï¼Œ$ğ‘£ (ğ‘š \times 1)$ æ˜¯ä¸€ä¸ªå‘é‡ï¼Œ$\lambda$ æ˜¯ä¸€ä¸ªæ ‡é‡ ($1$)ï¼Œ

$$ ğ¶ğ‘£=\lambda ğ‘£ $$

```py
we can reorder to, 
```

$$ (ğ¶âˆ’ \lambda \cdot ğ¼)âˆ™ğ‘£=0 $$

```py
where $I$ is an identity matrix. By Cramerâ€™s rule, we have a solution if the determinant is 0, 
```

$$ |ğ¶âˆ’ \lambda \cdot ğ¼|=0 $$

```py
find the possible Eigenvalues, $\lambda_ğ›¼$, and solve for eigenvectors, $ğ’—_ğœ¶, \quad \alpha=ğŸ,\ldots,ğ’$ 
```

+   ç»“æœæ˜¯çŸ©é˜µ $ğ‘½_ğ’$ ä¸­çš„ $\text{ğ’ğ’Šğ’}â¡(ğ’,ğ’âˆ’ğŸ)$ ä¸ªç‰¹å¾å‘é‡ã€‚

![](img/149885b8478ebe255e67e3781a68b054.png)

ç‰¹å¾å‘é‡ä½œä¸ºä¸»æˆåˆ†ã€‚

```py
that form a basis on which the data are projected for dimensionality reduction, 
```

![](img/99275c247c63e53876ec6c9dd844b7b9.png)

ç‰¹å¾å‘é‡ä½œä¸ºå®šä¹‰æ–°æ—‹è½¬åŸºçš„ä¸»æˆåˆ†ã€‚

å¦‚æœä½ æƒ³æŸ¥çœ‹ä¸»æˆåˆ†è½½è·å’Œå„æˆåˆ†ä¹‹é—´çš„æ–¹å·®åˆ†é…ï¼Œè¯·æŸ¥çœ‹æˆ‘çš„ [PCA è½½è·äº¤äº’å¼ Python ä»ªè¡¨æ¿](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_PCA_Eigen.ipynb)ï¼Œ

![](img/8a7fbf602c24c192b7d999a9a3faaf43.png)

æˆ‘çš„äº¤äº’å¼ä»ªè¡¨æ¿å±•ç¤ºäº†ä¸»æˆåˆ†è½½è·å’Œæ¯ä¸ªä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®ï¼Œéšç€ç‰¹å¾ 1ã€2 å’Œ 3 ä¹‹é—´çš„ç›¸å…³æ€§å˜åŒ–ã€‚

## åŠ è½½æ‰€éœ€çš„åº“

ä»¥ä¸‹ä»£ç åŠ è½½æ‰€éœ€çš„åº“ã€‚è¿™äº›åº“åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
ignore_warnings = True                                        # ignore warnings?
from sklearn.preprocessing import MinMaxScaler                # min/max normalization
from sklearn.decomposition import PCA                         # PCA program from scikit learn (package for machine learning)
from sklearn.preprocessing import StandardScaler              # standardize variables to mean of 0.0 and variance of 1.0
import numpy as np                                            # ndarrays for gridded data
import pandas as pd                                           # DataFrames for tabular data
import pandas.plotting as pd_plot                             # pandas plotting functions
import copy                                                   # for deep copies
import os                                                     # set working directory, run executables
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks
import matplotlib.ticker as mtick                             # control tick label formatting
from matplotlib.ticker import PercentFormatter                # percentage axis label formatting
import seaborn as sns                                         # advanced plotting
plt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements
if ignore_warnings == True:                                   
    import warnings
    warnings.filterwarnings('ignore')
cmap = plt.cm.inferno                                         # color map
seed = 42                                                     # random number seed 
```

å¦‚æœä½ é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½éœ€è¦é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ï¼Œç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£å¯ä»¥æä¾›æ›´å¤šå¸®åŠ©ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå•ç‹¬çš„å‡½æ•°æ¥ç®€åŒ–ç»˜åˆ¶ç›¸å…³çŸ©é˜µã€‚æˆ‘è¿˜æ·»åŠ äº†ä¸€ä¸ªæ–¹ä¾¿çš„å‡½æ•°æ¥æ·»åŠ ä¸»ç½‘æ ¼çº¿å’Œå‰¯ç½‘æ ¼çº¿ï¼Œä»¥æé«˜å›¾è¡¨çš„å¯è§£é‡Šæ€§ã€‚

```py
def plot_corr(df,size=10):                                    # plots a graphical correlation matrix 
    from matplotlib.colors import ListedColormap              # make a custom colormap
    my_colormap = plt.cm.get_cmap('RdBu_r', 256)          
    newcolors = my_colormap(np.linspace(0, 1, 256))
    white = np.array([256/256, 256/256, 256/256, 1])
    newcolors[65:191, :] = white                              # mask all correlations less than abs(0.8)
    newcmp = ListedColormap(newcolors)
    m = len(df.columns)
    corr = df.corr()
    fig, ax = plt.subplots(figsize=(size, size))
    im = ax.matshow(corr,vmin = -1.0, vmax = 1.0,cmap = newcmp)
    plt.xticks(range(len(corr.columns)), corr.columns);
    plt.yticks(range(len(corr.columns)), corr.columns);
    plt.colorbar(im, orientation = 'vertical')
    plt.title('Correlation Matrix')
    for i in range(0,m):
        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')
        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')
    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def add_grid2(sub_plot):
    sub_plot.grid(True, which='major',linewidth = 1.0); sub_plot.grid(True, which='minor',linewidth = 0.2) # add y grids
    sub_plot.tick_params(which='major',length=7); sub_plot.tick_params(which='minor', length=4)
    sub_plot.xaxis.set_minor_locator(AutoMinorLocator()); sub_plot.yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/Local")                                 # set the working directory 
```

æ‚¨å°†ä¸å¾—ä¸æ›´æ–°å¼•å·å†…çš„éƒ¨åˆ†ä»¥åŒ…å«æ‚¨è‡ªå·±çš„å·¥ä½œç›®å½•ï¼Œå¹¶ä¸”æ ¼å¼åœ¨ Mac ä¸Šä¸åŒï¼ˆä¾‹å¦‚ï¼šâ€œ~/PGEâ€ï¼‰ã€‚

## åŠ è½½è¡¨æ ¼æ•°æ®

è¿™æ˜¯åŠ è½½æˆ‘ä»¬çš„é€—å·åˆ†éš”æ•°æ®æ–‡ä»¶åˆ° Pandas DataFrame å¯¹è±¡çš„å‘½ä»¤ã€‚

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›†â€˜unconv_MV.csvâ€™ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…æ‹¬æ¥è‡ª 1000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼š

+   ä¼˜è‰¯çš„å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   å²©è„†æ€§æ¯”ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   ç»ç’ƒè´¨åå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹ç”Ÿäº§ 90 å¤©å¹³å‡ï¼ˆMCFPDï¼‰ã€‚

æ³¨æ„ï¼Œæ•°æ®é›†æ˜¯åˆæˆçš„ã€‚

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸ºâ€˜my_dataâ€™çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

```py
#my_data = pd.read_csv("unconv_MV.csv") 
my_data = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv") # load the comma delimited data file
my_data = my_data.iloc[:,1:]                              # remove the well index 
```

## å¯è§†åŒ– DataFrame

å¯è§†åŒ– DataFrame æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åˆæ­¥æ£€æŸ¥ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ‡ç‰‡ DataFrame æ¥é¢„è§ˆã€‚

+   æˆ‘ä»¬æ˜¾ç¤ºä» 0 åˆ° 7 çš„æ‰€æœ‰è®°å½•ï¼Œä½†ä¸åŒ…æ‹¬ 7

```py
my_data[:7]                                               # preview the first 7 rows of the dataframe 
```

|  | Por | LogPerm | AI | Brittle | TOC | VR | ç”Ÿäº§ |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 15.91 | 1.67 | 3.06 | 14.05 | 1.36 | 1.85 | 177.381958 |
| 1 | 15.34 | 1.65 | 2.60 | 31.88 | 1.37 | 1.79 | 1479.767778 |
| 2 | 20.45 | 2.02 | 3.13 | 63.67 | 1.79 | 2.53 | 4421.221583 |
| 3 | 11.95 | 1.14 | 3.90 | 58.81 | 0.40 | 2.03 | 1488.317629 |
| 4 | 19.53 | 1.83 | 2.57 | 43.75 | 1.40 | 2.11 | 5261.094919 |
| 5 | 19.47 | 2.04 | 2.73 | 54.37 | 1.42 | 2.12 | 5497.005506 |
| 6 | 12.70 | 1.30 | 3.70 | 43.03 | 0.45 | 1.95 | 1784.266285 |

## è¡¨æ ¼æ•°æ®çš„æ‘˜è¦ç»Ÿè®¡

åœ¨ DataFrames ä¸­ä»è¡¨æ ¼æ•°æ®è®¡ç®—æ‘˜è¦ç»Ÿè®¡æœ‰å¾ˆå¤šé«˜æ•ˆçš„æ–¹æ³•ã€‚describe å‘½ä»¤æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„æ•°æ®è¡¨ï¼Œæä¾›äº†è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼å’Œå››åˆ†ä½æ•°ã€‚

+   æˆ‘ä»¬ä½¿ç”¨è½¬ç½®åªæ˜¯ç¿»è½¬è¡¨æ ¼ï¼Œä»¥ä¾¿ç‰¹å¾åœ¨è¡Œä¸Šï¼Œç»Ÿè®¡ä¿¡æ¯åœ¨åˆ—ä¸Šã€‚

```py
my_data.describe().transpose()                            # calculate summary statistics for the data 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 1000.0 | 14.950460 | 3.029634 | 5.400000 | 12.85750 | 14.98500 | 17.080000 | 24.65000 |
| LogPerm | 1000.0 | 1.398880 | 0.405966 | 0.120000 | 1.13000 | 1.39000 | 1.680000 | 2.58000 |
| AI | 1000.0 | 2.982610 | 0.577629 | 0.960000 | 2.57750 | 3.01000 | 3.360000 | 4.70000 |
| Brittle | 1000.0 | 49.719480 | 15.077006 | -10.500000 | 39.72250 | 49.68000 | 59.170000 | 93.47000 |
| TOC | 1000.0 | 1.003810 | 0.504978 | -0.260000 | 0.64000 | 0.99500 | 1.360000 | 2.71000 |
| VR | 1000.0 | 1.991170 | 0.308194 | 0.900000 | 1.81000 | 2.00000 | 2.172500 | 2.90000 |
| Production | 1000.0 | 2247.295809 | 1464.256312 | 2.713535 | 1191.36956 | 1976.48782 | 3023.594214 | 12568.64413 |

å¾ˆå¥½ï¼Œæˆ‘ä»¬å·²ç»æ£€æŸ¥äº†æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ï¼Œå¯¹äºè„†æ€§å’Œæ€»æœ‰æœºç¢³ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›è´Ÿå€¼ã€‚è¿™æ˜¯åœ¨ç‰©ç†ä¸Šä¸å¯èƒ½çš„ã€‚

+   è¿™äº›å€¼è‚¯å®šæœ‰è¯¯ã€‚æˆ‘ä»¬çŸ¥é“æœ€ä½å¯èƒ½çš„å€¼æ˜¯ 0.0ï¼Œå› æ­¤æˆ‘ä»¬å°†æˆªæ–­åˆ° 0.0ã€‚

æˆ‘ä»¬ä½¿ç”¨ï¼š

```py
df.get_numerical_data() 
```

DataFrame æˆå‘˜å‡½æ•°ç”¨äºä» DataFrame è·å–æ•°æ®çš„æµ…æ‹·è´ã€‚

ç”±äºè¿™æ˜¯ä¸€ä¸ªæµ…æ‹·è´ï¼Œæˆ‘ä»¬å¯¹æ‹·è´æ‰€åšçš„ä»»ä½•æ›´æ”¹éƒ½ä¼šå½±å“åˆ°åŸå§‹ DataFrame ä¸­çš„æ•°æ®ã€‚

+   è¿™å…è®¸æˆ‘ä»¬ä¸€æ¬¡æ€§å°†è¿™ä¸ªç®€å•çš„æ¡ä»¶è¯­å¥åº”ç”¨åˆ° DataFrame ä¸­çš„æ‰€æœ‰æ•°æ®å€¼ã€‚

```py
num = my_data._get_numeric_data()                         # get the numerical values
num[num < 0] = 0                                          # truncate negative values to 0.0
my_data.describe().transpose()                            # calculate summary statistics for the data 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 1000.0 | 14.950460 | 3.029634 | 5.400000 | 12.85750 | 14.98500 | 17.080000 | 24.65000 |
| LogPerm | 1000.0 | 1.398880 | 0.405966 | 0.120000 | 1.13000 | 1.39000 | 1.680000 | 2.58000 |
| AI | 1000.0 | 2.982610 | 0.577629 | 0.960000 | 2.57750 | 3.01000 | 3.360000 | 4.70000 |
| Brittle | 1000.0 | 49.731480 | 15.033593 | 0.000000 | 39.72250 | 49.68000 | 59.170000 | 93.47000 |
| TOC | 1000.0 | 1.006170 | 0.499838 | 0.000000 | 0.64000 | 0.99500 | 1.360000 | 2.71000 |
| VR | 1000.0 | 1.991170 | 0.308194 | 0.900000 | 1.81000 | 2.00000 | 2.172500 | 2.90000 |
| Production | 1000.0 | 2247.295809 | 1464.256312 | 2.713535 | 1191.36956 | 1976.48782 | 3023.594214 | 12568.64413 |

## è®¡ç®—ç›¸å…³çŸ©é˜µ

å¯¹äºé™ç»´ï¼Œæ•°æ®å¯è§†åŒ–æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç¬¬ä¸€æ­¥ã€‚

è®©æˆ‘ä»¬ä»ç›¸å…³çŸ©é˜µå¼€å§‹ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›å‘½ä»¤è®¡ç®—å¹¶æŸ¥çœ‹æ§åˆ¶å°ä¸­çš„ç»“æœã€‚

```py
corr_matrix = np.corrcoef(my_data, rowvar = False) 
```

è¾“å…¥æ•°æ®æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œ$rowvar$ æŒ‡å®šå˜é‡æ˜¯å¦ä½äºè¡Œè€Œä¸æ˜¯åˆ—ä¸­ã€‚

```py
corr_matrix = np.corrcoef(my_data, rowvar = False)
print(np.around(corr_matrix,2))                           # print the correlation matrix to 2 decimals 
```

```py
[[ 1\.    0.81 -0.51 -0.25  0.71  0.08  0.69]
 [ 0.81  1\.   -0.32 -0.15  0.51  0.05  0.57]
 [-0.51 -0.32  1\.    0.17 -0.55  0.49 -0.33]
 [-0.25 -0.15  0.17  1\.   -0.24  0.3  -0.07]
 [ 0.71  0.51 -0.55 -0.24  1\.    0.31  0.5 ]
 [ 0.08  0.05  0.49  0.3   0.31  1\.    0.14]
 [ 0.69  0.57 -0.33 -0.07  0.5   0.14  1\.  ]] 
```

æ³¨æ„ç”±äºæ¯ä¸ªå˜é‡ä¸å…¶è‡ªèº«ç›¸å…³è€Œäº§ç”Ÿçš„ 1.0 å¯¹è§’çº¿ã€‚

è®©æˆ‘ä»¬ä½¿ç”¨ä¸Šé¢å£°æ˜çš„å‡½æ•°æ¥åˆ¶ä½œå›¾å½¢ç›¸å…³çŸ©é˜µå¯è§†åŒ–ã€‚

+   è¿™å¯èƒ½æé«˜æˆ‘ä»¬è¯†åˆ«ç‰¹å¾çš„èƒ½åŠ›ã€‚å®ƒä¾èµ–äºå†…ç½®çš„ Numpy DataFrames ç›¸å…³çŸ©é˜µæ–¹æ³•å’Œ Matplotlib è¿›è¡Œç»˜å›¾ã€‚

```py
plot_corr(my_data,7)                                      # using our correlation matrix visualization function
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/93b5d485eb7760d8aa68200eb8d5b65a.png)

è¿™çœ‹èµ·æ¥ä¸é”™ã€‚å­˜åœ¨å¤šç§åŒå˜é‡ã€çº¿æ€§ç›¸å…³ç¨‹åº¦ã€‚å½“ç„¶ï¼Œç›¸å…³ç³»æ•°ä»…é™äºçº¿æ€§ç›¸å…³ç¨‹åº¦ã€‚

## æ£€æŸ¥çŸ©é˜µæ•£ç‚¹å›¾

ä¸ºäº†è·å–æ›´å®Œæ•´çš„ä¿¡æ¯ï¼Œè®©æˆ‘ä»¬æŸ¥çœ‹ Pandas åŒ…ä¸­çš„çŸ©é˜µæ•£ç‚¹å›¾ã€‚

+   åæ–¹å·®å’Œç›¸å…³ç³»æ•°å¯¹å¼‚å¸¸å€¼å’Œéçº¿æ€§æ•æ„Ÿ

```py
pd_plot.scatter_matrix(my_data) 
```

$alpha$ å…è®¸æˆ‘ä»¬ä½¿ç”¨åŠé€æ˜ç‚¹ï¼Œä»¥ä¾¿åœ¨å¯†é›†æ•£ç‚¹å›¾ä¸­æ›´å®¹æ˜“å¯è§†åŒ–ã€‚

$hist_kwds$ æ˜¯å¯¹è§’çº¿å…ƒç´ ç›´æ–¹å›¾çš„å‚æ•°é›†ã€‚

```py
pd_plot.scatter_matrix(my_data, alpha = 0.1,              # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/018895042e5a686505eab9280b272aa5.png)

## ç®€å•çš„åŒå˜é‡ç¤ºä¾‹

è®©æˆ‘ä»¬å°†é—®é¢˜ç®€åŒ–ä¸ºåŒå˜é‡ï¼ˆ2 ä¸ªç‰¹å¾ï¼‰ï¼Œå­”éš™ç‡å’Œæ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼Œå¹¶å°†äº•çš„æ•°é‡ä» 1,000 å‡å°‘åˆ° 100ã€‚

```py
my_data_por_perm = my_data.iloc[0:100,0:2]                # extract just por and logperm, 100 samples
my_data_por_perm.describe().transpose()                   # calculate summary statistics for the data 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 100.0 | 14.9856 | 2.823016 | 9.23 | 12.9275 | 14.720 | 16.705 | 21.00 |
| LogPerm | 100.0 | 1.3947 | 0.390947 | 0.36 | 1.1475 | 1.365 | 1.650 | 2.48 |

è®©æˆ‘ä»¬é¦–å…ˆæ£€æŸ¥ Por å’Œ LogPerm çš„å•å˜é‡ç»Ÿè®¡ä¿¡æ¯ã€‚

```py
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.hist(my_data_por_perm["Por"], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20)
ax1.set_title('Porosity'); ax1.set_xlabel('Porosity (%)'); ax1.set_ylabel('Frequency'); add_grid2(ax1)
ax2.hist(my_data_por_perm["LogPerm"], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20)
ax2.set_title('Log Transformed Permeability'); ax2.set_xlabel('Log[Permeability] (log(mD)'); add_grid2(ax2)
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/5905d69c02168314e84b6bf1c5e7d169.png)

åˆ†å¸ƒå®é™…ä¸Šå¯èƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒçš„ï¼Œæ— è®ºå®ƒä»¬çš„è¡Œä¸ºæ˜¯å¦è‰¯å¥½ï¼Œæˆ‘ä»¬æ— æ³•è§‚å¯Ÿåˆ°æ˜æ˜¾çš„ç¼ºå£æˆ–æˆªæ–­ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹å­”éš™ç‡ä¸å¯¹æ•°æ¸—é€ç‡çš„æ•£ç‚¹å›¾ã€‚

è¿™å°†æ˜¯æ¥è‡ª *matplotlib* çš„åŸºæœ¬å‘½ä»¤ï¼Œç”¨äºåˆ¶ä½œæ•£ç‚¹å›¾ã€‚

```py
plt.scatter(my_data_por_perm["Por"],my_data_por_perm["LogPerm"] 
```

+   é¢å¤–çš„å‚æ•°ç”¨äºæ ¼å¼åŒ–å’Œæ ‡ç­¾

```py
plt.scatter(my_data_por_perm["Por"],my_data_por_perm["LogPerm"],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
plt.title('Log Transformed Permeability vs. Porosity'); plt.xlabel('Porosity (%)'); plt.ylabel('Log(Permeability (Log(mD))'); add_grid()
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.7, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/9c8f27583a828639b3ad04b5051376a2.png)

## ä¸»æˆåˆ†è®¡ç®—

é€šè¿‡æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼Œæˆ‘ä»¬ä¸å­”éš™ç‡æœ‰ä¸€ä¸ªéå¸¸å¥½çš„çº¿æ€§å…³ç³»ï¼ŒPCA åº”è¯¥å¯ä»¥å¾ˆå¥½åœ°å¤„ç†è¿™äº›æ•°æ®ã€‚

+   æˆ‘ä»¬å‡†å¤‡ä½¿ç”¨å­”éš™ç‡å’Œæ¸—é€ç‡çš„å¯¹æ•°è¿›è¡Œ PCAã€‚

## æ ‡å‡†åŒ–ç‰¹å¾

æˆ‘ä»¬å¿…é¡»æ ‡å‡†åŒ–æˆ‘ä»¬çš„å˜é‡ï¼Œä½¿å…¶å‡å€¼ç­‰äºé›¶ï¼Œ$\bar{x} = 0.0$ï¼Œæ–¹å·®ç­‰äºä¸€ï¼Œ$\sigma^{2}_{x} = 1.0$ã€‚

+   å¦åˆ™ï¼Œå­”éš™ç‡å’Œæ¸—é€ç‡çš„æ¯”ä¾‹å·®å¼‚å°†äº§ç”Ÿé‡å¤§å½±å“ã€‚æ³¨æ„ï¼Œç”±äºå•ä½é€‰æ‹©å¯¹æ–¹å·®çš„å½±å“ï¼Œä¾‹å¦‚ï¼Œæ¸—é€ç‡ä½¿ç”¨è¾¾è¥¿ï¼ˆDï¼‰è€Œä¸æ˜¯æ¯«è¾¾è¥¿ï¼ˆmDï¼‰æˆ–ä½¿ç”¨åˆ†æ•°è€Œä¸æ˜¯ç™¾åˆ†æ¯”æ¥è¡¨ç¤ºå­”éš™ç‡ã€‚è¿™æ˜¯ç›¸å½“ä»»æ„çš„ï¼

è¦æ¶ˆé™¤è¿™ç§å½±å“ï¼Œé™¤éä¸¤ä¸ªå˜é‡å…·æœ‰ç›¸åŒçš„å•ä½å¹¶ä¸”å®ƒä»¬ä¹‹é—´çš„èŒƒå›´å’Œæ–¹å·®æœ‰æ„ä¹‰ï¼Œå¦åˆ™æˆ‘ä»¬åº”è¯¥å§‹ç»ˆè¿›è¡Œæ ‡å‡†åŒ–ï¼Œå› ä¸ºæ ‡å‡†åŒ–å¯èƒ½ä¼šåˆ é™¤é‡è¦ä¿¡æ¯ã€‚

```py
features = ['Por','LogPerm']
x = my_data_por_perm.loc[:,features].values
mu = np.mean(x, axis=0)
sd = np.std(x, axis=0)
x = StandardScaler().fit_transform(x)                     # standardize the data features to mean = 0, var = 1.0

print("Original Mean Por", np.round(mu[0],2), ', Original Mean LogPerm = ', np.round(mu[1],2)) 
print("Original StDev Por", np.round(sd[0],2), ', Original StDev LogPerm = ', np.round(sd[1],2)) 
print('Mean Transformed Por =',np.round(np.mean(x[:,0]),2),', Mean Transformed LogPerm =',np.round(np.mean(x[:,1]),2))
print('Variance Transformed Por =',np.var(x[:,0]),', Variance Transformed LogPerm =',np.var(x[:,1])) 
```

```py
Original Mean Por 14.99 , Original Mean LogPerm =  1.39
Original StDev Por 2.81 , Original StDev LogPerm =  0.39
Mean Transformed Por = 0.0 , Mean Transformed LogPerm = -0.0
Variance Transformed Por = 1.0000000000000002 , Variance Transformed LogPerm = 1.0 
```

```py
cov = np.cov(x,rowvar = False)
cov 
```

```py
array([[1.01010101, 0.80087707],
       [0.80087707, 1.01010101]]) 
```

â€œxâ€æ˜¯æ¥è‡ª Numpy åŒ…çš„ 2D ndarrayï¼Œç‰¹å¾åœ¨åˆ—ä¸­ï¼Œæ ·æœ¬åœ¨è¡Œä¸­ã€‚

+   å¦‚ä¸Šæ‰€ç¤ºï¼Œæˆ‘ä»¬ç¡®è®¤â€œxâ€äºŒç»´æ•°ç»„ä¸­çš„ç‰¹å¾å·²ç»æ ‡å‡†åŒ–ã€‚

æ£€æŸ¥æˆ‘ä»¬æ ‡å‡†åŒ–å˜é‡çš„å•å˜é‡å’Œå¤šå˜é‡åˆ†å¸ƒä¸æ˜¯ä¸€ä¸ªåä¸»æ„ã€‚

```py
dfS = pd.DataFrame({'sPor': x[:,0], 'sLogPerm': x[:,1]})
sns.jointplot(data=dfS,x='sPor',y='sLogPerm',marginal_kws=dict(bins=30),color='darkorange',edgecolor='black')
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/fabb8a2ef820e71361695cf1232eccd7.png)

ä¸€åˆ‡çœ‹èµ·æ¥éƒ½å¾ˆæ­£å¸¸ï¼Œæˆ‘ä»¬å‡†å¤‡åº”ç”¨ä¸»æˆåˆ†åˆ†æã€‚

## ä¸»æˆåˆ†åˆ†æ (PCA)

è¦åœ¨ Python ä¸­ä½¿ç”¨ SciKitLearn æœºå™¨å­¦ä¹ åŒ…è¿è¡Œ PCAï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨æŒ‡å®šæ•°é‡çš„ç»„ä»¶åˆ›å»ºä¸€ä¸ª PCA æ¨¡å‹ï¼Œç„¶åå°†å…¶â€œæ‹Ÿåˆâ€åˆ°æˆ‘ä»¬çš„æ•°æ®ä¸Šã€‚

```py
n_components = 2
pca = PCA(n_components=n_components)
pca.fit(x) 
```

æ­£å¦‚ä½ ç¨åé€šè¿‡é™ç»´å°†çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨çŸ©é˜µæ•°å­¦æ¥å¤„ç†è¿™ä¸ªæ¨¡å‹ï¼Œå¹¶å°†æˆ‘ä»¬çš„æ•°æ®å‡å°‘åˆ°ä» 1 åˆ°ç‰¹å¾æ•°é‡ m çš„ä»»ä½•ç»´åº¦ã€‚è®©æˆ‘ä»¬ä»¥ç‰¹å¾æ•°é‡ m ç­‰äºç»„ä»¶æ•°é‡çš„æ–¹å¼è¿è¡Œæ¨¡å‹ã€‚

```py
n_components = 2
pca = PCA(n_components=n_components).fit(x) 
```

## ç»„ä»¶è½½è·

æˆ‘ä»¬é¦–å…ˆåº”è¯¥åšçš„æ˜¯æŸ¥çœ‹æˆåˆ†åŠ è½½ã€‚è®©æˆ‘ä»¬æŸ¥çœ‹å®ƒä»¬å¹¶è§£é‡Šæˆ‘ä»¬çš„ç»“æœã€‚

```py
print(np.round(pca.components_,3))
print('First Principal Component = ' + str(np.round(pca.components_[0,:],3)))
print('Second Principal Component = ' + str(np.round(pca.components_[1,:],3))) 
```

```py
[[ 0.707  0.707]
 [ 0.707 -0.707]]
First Principal Component = [0.707 0.707]
Second Principal Component = [ 0.707 -0.707] 
```

ç»„ä»¶è¢«åˆ—ä¸ºä¸€ä¸ªäºŒç»´æ•°ç»„ï¼ˆndarrayï¼‰ï¼ŒåŒ…æ‹¬ï¼š

+   ä¸»æˆåˆ†åœ¨è¡Œä¸Š

+   ç‰¹å¾åœ¨åˆ—ä¸Š

+   è¡Œæ˜¯æŒ‰é¡ºåºæ’åˆ—çš„ï¼Œä»¥ä¾¿ç¬¬ä¸€ä¸ªä¸»æˆåˆ†æ˜¯ç¬¬ä¸€è¡Œï¼Œæœ€åä¸€ä¸ªä¸»æˆåˆ†æ˜¯æœ€åä¸€è¡Œã€‚

## ä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹

ä¹Ÿå¾ˆé‡è¦çš„æ˜¯è¦æŸ¥çœ‹æ¯ä¸ªä¸»æˆåˆ†æè¿°çš„æ–¹å·®æ¯”ä¾‹ã€‚

```py
print('Variance explained by PC1 and PC2 =', np.round(pca.explained_variance_ratio_,3))
print('First Principal Component explains ' + str(np.round(pca.explained_variance_ratio_[0],3)) + ' of the total variance.')
print('Second Principal Component explains ' + str(np.round(pca.explained_variance_ratio_[1],3)) + ' of the total variance.') 
```

```py
Variance explained by PC1 and PC2 = [0.896 0.104]
First Principal Component explains 0.896 of the total variance.
Second Principal Component explains 0.104 of the total variance. 
```

## ä¸»æˆåˆ†å¾—åˆ†ï¼Œæ­£å‘å’Œé€†å‘æŠ•å½±

æˆ‘ä»¬å¯ä»¥è®¡ç®—åŸå§‹æ•°æ®çš„ä¸»æˆåˆ†å¾—åˆ†ã€‚

+   è¿™å®é™…ä¸Šæ˜¯å¯¹æ•°æ®çš„æ—‹è½¬ï¼Œä¸ PC1 çš„æ–¹å‘å¯¹é½ï¼Œè¿™æ˜¯æœ€å¤§æ–¹å·®çš„æ–¹å‘ã€‚

+   æˆ‘ä»¬å°†ä½¿ç”¨ PCA å†…ç½®çš„â€œtransformâ€å‡½æ•°è®¡ç®—ä¸»æˆåˆ†å¾—åˆ†ï¼Œç„¶åå°†å…¶å¯è§†åŒ–ä¸ºæ•£ç‚¹å›¾ã€‚

+   ç„¶åä¸ºäº†â€œé—­ç¯â€å¹¶æ£€æŸ¥æˆ‘ä»¬æ‰€åšçš„å·¥ä½œï¼ˆä»¥åŠæˆ‘ä»¬çš„çŸ¥è¯†ï¼‰ï¼Œæˆ‘ä»¬å°†è¿›è¡Œé€† PCAï¼Œä»ä¸»æˆåˆ†å¾—åˆ†å›åˆ°æ ‡å‡†åŒ–ç‰¹å¾ã€‚

```py
f, (ax101, ax102, ax103) = plt.subplots(1, 3,figsize=(12,3))
f.subplots_adjust(wspace=0.7)

ax101.scatter(x[:,0],x[:,1], s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax101.set_title('Standardized LogPerm vs. Por'); ax101.set_xlabel('Standardized Por'); ax101.set_ylabel('Standardized LogPerm')
ax101.set_xlim([-3,3]); ax101.set_ylim([-3,3]); add_grid2(ax101)

x_trans = pca.transform(x)                                # calculate the principal component scores
ax102.scatter(x_trans[:,0],-1*x_trans[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax102.set_title('Principal Component Scores'); ax102.set_xlabel('PC1'); ax102.set_ylabel('PC2')
ax102.set_xlim([-3,3]); ax102.set_ylim([-3,3]); add_grid2(ax102)

x_reverse = pca.inverse_transform(x_trans)                        # reverse the principal component scores to standardized values
ax103.scatter(x_reverse[:,0],x_reverse[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax103.set_title('Reverse PCA'); ax103.set_xlabel('Standardized Por'); ax103.set_ylabel('Standardized LogPerm')
ax103.set_xlim([-3,3]); ax103.set_ylim([-3,3]); add_grid2(ax103)

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/a58857e0fc1f11730774c37f6998fb6d.png)

æ ‡å‡†åŒ–åŸå§‹æ•°æ®å’Œé€† PCA äº¤å‰å›¾åº”è¯¥çœ‹èµ·æ¥å®Œå…¨ç›¸åŒã€‚å¦‚æœæ˜¯è¿™æ ·ï¼Œé‚£ä¹ˆè¯¥æ–¹æ³•å°±æ˜¯æœ‰æ•ˆçš„ã€‚

## æ–¹å·®çš„å®ˆæ’

è®©æˆ‘ä»¬æ£€æŸ¥ä¸»æˆåˆ†å¾—åˆ†çš„æ–¹å·®ï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨å·²ç»è®¡ç®—äº†å®ƒä»¬ã€‚

+   æˆ‘ä»¬è®¡ç®—æ¯ä¸ªåŸå§‹ç‰¹å¾çš„æ–¹å·®

+   ç„¶åæ±‚å’Œä»¥å¾—åˆ°åŸå§‹çš„æ€»æ–¹å·®

+   æˆ‘ä»¬è®¡ç®—æ¯ä¸ªè½¬æ¢åçš„ã€ä¸»æˆåˆ†å¾—åˆ†çš„æ–¹å·®

+   ç„¶åæˆ‘ä»¬æ±‚å’Œä»¥å¾—åˆ°è½¬æ¢åçš„æ€»æ–¹å·®

æˆ‘ä»¬æ³¨æ„åˆ°ï¼š

+   ç¬¬ä¸€ä¸ªä¸»æˆåˆ†å¾—åˆ†æ¯”ç¬¬äºŒä¸ªæˆåˆ†å¾—åˆ†æœ‰æ›´å¤§çš„æ–¹å·®

+   æ€»æ–¹å·®åœ¨è½¬æ¢è¿‡ç¨‹ä¸­ä¿æŒä¸å˜ï¼ŒåŸå§‹ç‰¹å¾å’Œ m ä¸ªä¸»æˆåˆ†å¾—åˆ†æ–¹å·®ä¹‹å’Œç›¸åŒ

```py
print('Variance of the 2 features:')
print(np.var(x, axis = 0))

print('\nTotal Variance from Original Features:')
print(np.sum(np.var(x, axis = 0)))

print('\nVariance of the 2 principle components:')
print(np.round(np.var(x_trans, axis = 0),2))

print('\nTotal Variance from Original Features:')
print(round(np.sum(np.var(x_trans, axis = 0)),2)) 
```

```py
Variance of the 2 features:
[1\. 1.]

Total Variance from Original Features:
2.0

Variance of the 2 principle components:
[1.79 0.21]

Total Variance from Original Features:
2.0 
```

## ä¸»æˆåˆ†å¾—åˆ†çš„ç‹¬ç«‹æ€§

è®©æˆ‘ä»¬æ£€æŸ¥åŸå§‹ç‰¹å¾ä¸æˆ‘ä»¬çš„æŠ•å½±ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

```py
print('\nCorrelation Matrix of the 2 original features components:')
print(np.round(np.corrcoef(x, rowvar = False),2))

print('\nCorrelation Matrix of the 2 principle components\' scores:')
print(np.round(np.corrcoef(x_trans, rowvar = False),2)) 
```

```py
Correlation Matrix of the 2 original features components:
[[1\.   0.79]
 [0.79 1\.  ]]

Correlation Matrix of the 2 principle components' scores:
[[ 1\. -0.]
 [-0\.  1.]] 
```

æˆ‘ä»¬å°†åŸå§‹ç‰¹å¾æŠ•å½±åˆ° 2 ä¸ªæ–°ç‰¹å¾ä¸Šï¼Œè¿™äº›æ–°ç‰¹å¾ä¹‹é—´æ²¡æœ‰ç›¸å…³æ€§ã€‚

## ä½¿ç”¨ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡è®¡ç®—å™¨æ‰‹åŠ¨è¿›è¡Œä¸»æˆåˆ†åˆ†æ

è®©æˆ‘ä»¬é€šè¿‡æ ‡å‡†åŒ–çš„ç‰¹å¾å’Œç‰¹å¾å€¼è®¡ç®—æ¥æ‰‹åŠ¨å±•ç¤º PCAï¼Œå¹¶ä¸ä¸Šé¢çš„ scikit-learn ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚

+   æˆ‘ä»¬ç¡®è®¤ç»“æœæ˜¯ä¸€è‡´çš„ã€‚

```py
from numpy.linalg import eig
eigen_values,eigen_vectors = eig(cov)
print('Eigen Vectors:\n' +  str(np.round(eigen_vectors,2)))
print('First Eigen Vector: ' + str(eigen_vectors[:,0]))
print('Second Eigen Vector: ' + str(eigen_vectors[:,1]))
print('Eigen Values:\n' +  str(np.round(eigen_values,2)))
PC = eigen_vectors.T.dot(x.T)
plt.subplot(121)
plt.scatter(PC[0,:],PC[1,:],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
plt.title('Principal Component Scores By-hand with numpy.linalg Eig Function'); plt.xlabel('PC1'); plt.ylabel('PC2')
plt.xlim([-3,3]); plt.ylim([-3,3]); add_grid()

plt.subplot(122)
plt.scatter(x_trans[:,0],-1*x_trans[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
plt.title('Principal Component Scores with scikit-learn PCA'); plt.xlabel('PC1'); plt.ylabel('PC2')
plt.xlim([-3,3]); plt.ylim([-3,3]); add_grid()

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.2, hspace=0.3); plt.show() 
```

```py
Eigen Vectors:
[[ 0.71 -0.71]
 [ 0.71  0.71]]
First Eigen Vector: [0.70710678 0.70710678]
Second Eigen Vector: [-0.70710678  0.70710678]
Eigen Values:
[1.81 0.21] 
```

![å›¾ç‰‡](img/5f1afcb9beed01f67ca256f8ea4acb30.png)

## ç»´åº¦é™ä½çš„æ¼”ç¤º

ç°åœ¨æˆ‘ä»¬å°è¯•é€šè¿‡ä»…ä¿ç•™ç¬¬ä¸€ä¸ªä¸»æˆåˆ†æ¥è¿›è¡Œ**ç»´åº¦é™ä½**ã€‚æˆ‘ä»¬å°†ä»åŸå§‹å€¼åˆ°åŸå§‹å€¼çš„é¢„æµ‹ã€‚

+   å›æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç”¨ç¬¬ä¸€ä¸ªä¸»æˆåˆ†è§£é‡Šå¤§çº¦ 90%çš„æ–¹å·®ï¼Œæ‰€ä»¥ç»“æœåº”è¯¥çœ‹èµ·æ¥â€œç›¸å½“ä¸é”™â€ï¼Œå¯¹å§ï¼Ÿ

æˆ‘ä»¬å°†æ‰‹åŠ¨å®Œæˆæ•´ä¸ªè¿‡ç¨‹ï¼Œä»¥ä¾¿åœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶å°½å¯èƒ½ç®€å•æ˜“æ‡‚ã€‚ä»¥åæˆ‘ä»¬ä¼šæ›´åŠ ç´§å‡‘ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š

1.  ä»åŸå§‹çš„å­”éš™ç‡å’Œæ¸—é€ç‡æ•°æ®å¼€å§‹

1.  æ ‡å‡†åŒ–å¤„ç†ï¼Œä½¿å¾— Por å’Œ LogPerm çš„å‡å€¼ä¸º 0.0ï¼Œæ–¹å·®ä¸º 1.0

1.  è®¡ç®—ä¸¤ä¸ªä¸»æˆåˆ†æ¨¡å‹ï¼Œå¯è§†åŒ–ä¸»æˆåˆ†å¾—åˆ†

1.  é€šè¿‡å°†ç›¸å…³çš„æˆåˆ†å¾—åˆ†è®¾ç½®ä¸º 0.0 æ¥ç§»é™¤ç¬¬äºŒä¸ªä¸»æˆåˆ†

1.  é€šè¿‡çŸ©é˜µä¹˜ä»¥å¾—åˆ†å’Œæˆåˆ†è´Ÿè½½æ¥åè½¬ä¸»æˆåˆ†

1.  åº”ç”¨çŸ©é˜µæ•°å­¦æ¥æ¢å¤åŸå§‹çš„å‡å€¼å’Œæ–¹å·®

```py
nComp = 1
f, ((ax201, ax202, ax203), (ax206, ax205, ax204)) = plt.subplots(2, 3,figsize=(15,10))
#f, ((ax201, ax202), (ax203, ax204), (ax205, ax206)) = plt.subplots(3, 2,figsize=(10,15))
f.subplots_adjust(wspace=0.5,hspace = 0.3)

ax201.scatter(my_data_por_perm["Por"],my_data_por_perm["LogPerm"],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax201.set_title('1\. LogPerm vs. Por'); ax201.set_xlabel('Por'); ax201.set_ylabel('LogPerm')
ax201.set_xlim([8,22]); ax201.set_ylim([0,2.5]); add_grid2(ax201)

mu = np.mean(np.vstack((my_data_por_perm["Por"].values,my_data_por_perm["LogPerm"].values)), axis=1)
sd = np.std(np.vstack((my_data_por_perm["Por"].values,my_data_por_perm["LogPerm"].values)), axis=1)
x = StandardScaler().fit_transform(x)                     # standardize the data features to mean = 0, var = 1.0

ax202.scatter(x[:,0],x[:,1], s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax202.set_title('2\. Standardized LogPerm vs. Por'); ax202.set_xlabel('Standardized Por'); ax202.set_ylabel('Standardized LogPerm')
ax202.set_xlim([-3.5,3.5]); ax202.set_ylim([-3.5,3.5]); add_grid2(ax202)

n_components = 2                                          # build principal component model with 2 components
pca = PCA(n_components=n_components)
pca.fit(x)

x_trans = pca.transform(x)                                # calculate principal component scores
ax203.scatter(x_trans[:,0],-1*x_trans[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax203.set_title('3\. Principal Component Scores'); ax203.set_xlabel('PC1'); ax203.set_ylabel('PC2')
ax203.set_xlim([-3.5,3.5]); ax203.set_ylim([-3.5,3.5]); add_grid2(ax203)

x_trans[:,1] = 0.0                                         # zero / remove the 2nd principal component 

ax204.scatter(x_trans[:,0],x_trans[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax204.set_title('4\. Only 1st Principal Component Scores'); ax204.set_xlabel('PC1'); ax204.set_ylabel('PC2')
ax204.set_xlim([-3.5,3.5]); ax204.set_ylim([-3.5,3.5]); add_grid2(ax204)

xhat = pca.inverse_transform(x_trans)                             # reverse the principal component scores to standardized values
ax205.scatter(xhat[:,0],xhat[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax205.set_title('5\. Reverse PCA'); ax205.set_xlabel('Standardized Por'); ax205.set_ylabel('Standardized LogPerm')
ax205.set_xlim([-3.5,3.5]); ax205.set_ylim([-3.5,3.5]); add_grid2(ax205)

xhat = np.dot(pca.inverse_transform(x)[:,:nComp], pca.components_[:nComp,:])
xhat = sd*xhat + mu                                       # remove the standardization

ax206.scatter(my_data_por_perm["Por"],my_data_por_perm["LogPerm"],s=None, c="blue", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.6, linewidths=1.0, edgecolors="black")
ax206.scatter(xhat[:,0],xhat[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax206.set_title('6\. De-standardized Reverse PCA'); ax206.set_xlabel('Por'); ax206.set_ylabel('LogPerm')
ax206.set_xlim([8,22]); ax206.set_ylim([0,2.5]); add_grid2(ax206)

plt.show() 
```

![_images/99fce5c8ff4962b84e1d76599a56e25a485eb8a4462b4fa608f8d5a9af64a8dd.png](img/bf554436f2aadf97e6226c941ec9202d.png)

è®©æˆ‘ä»¬æŠŠåŸå§‹æ•°æ®å’Œå¾—åˆ°çš„ä½ç»´æ¨¡å‹å¹¶æ’æ”¾ç½®ï¼Œå¹¶æ£€æŸ¥ç»“æœæ–¹å·®ã€‚

```py
f, (ax201, ax206) = plt.subplots(1, 2,figsize=(10,6))
f.subplots_adjust(wspace=0.5,hspace = 0.3)

ax201.scatter(my_data_por_perm["Por"],my_data_por_perm["LogPerm"],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax201.set_title('1\. LogPerm vs. Por'); ax201.set_xlabel('Por'); ax201.set_ylabel('LogPerm')
ax201.set_xlim([8,22]); ax201.set_ylim([0,2.5]); add_grid2(ax201)

ax206.scatter(xhat[:,0],xhat[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors="black")
ax206.set_title('6\. De-standardized Reverse PCA'); ax206.set_xlabel('Por'); ax206.set_ylabel('LogPerm')
ax206.set_xlim([8,22]); ax206.set_ylim([0,2.5]); add_grid2(ax206)
plt.show()

var_por = np.var(my_data_por_perm["Por"]); var_por_hat = np.var(xhat[:,0]);
var_logperm = np.var(my_data_por_perm["LogPerm"]); var_logperm_hat = np.var(xhat[:,1]);
print('Variance Por =',np.round(var_por,3),', Variance Reduced Dimensional Por =',np.round(var_por_hat,3),'Fraction = ',np.round(var_por_hat/var_por,3))
print('Variance LogPerm =',np.round(var_logperm,3),', Variance Reduced Dimensional LogPerm =',np.round(var_logperm_hat,3),'Fraction = ',np.round(var_logperm_hat/var_logperm,3))
print('Total Variance =',np.round(var_por + var_logperm,3), ', Total Variance Reduced Dimension =',np.round(var_por_hat+var_logperm_hat,3),'Fraction = ',np.round((var_por_hat+var_logperm_hat)/(var_por+var_logperm),3)) 
```

![_images/dd23216e8863e8d206d5ad4311ffe9586147d99dbb5d2ad0581d859f68582c1d.png](img/240c16e5f3411a754b89857959797e42.png)

```py
Variance Por = 7.89 , Variance Reduced Dimensional Por = 7.073 Fraction =  0.896
Variance LogPerm = 0.151 , Variance Reduced Dimensional LogPerm = 0.136 Fraction =  0.896
Total Variance = 8.041 , Total Variance Reduced Dimension = 7.208 Fraction =  0.896 
```

## æ‰€æœ‰é¢„æµ‹ç‰¹å¾

æˆ‘ä»¬å°†å›åˆ°åŸå§‹æ•°æ®æ–‡ä»¶ï¼Œè¿™æ¬¡æå–æ‰€æœ‰ 6 ä¸ªé¢„æµ‹å˜é‡å’Œå‰ 500 ä¸ªæ ·æœ¬ã€‚

```py
my_data_f6 = my_data.iloc[0:500,0:6]                      # extract the 6 predictors, 500 samples 
```

ä»æ•°æ®çš„åŸºæœ¬ç»Ÿè®¡å¼€å§‹æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚

```py
my_data_f6.describe().transpose()                         # calculate summary statistics for the data 
```

|  | count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Por | 500.0 | 14.89936 | 2.985967 | 5.40 | 12.8500 | 14.900 | 17.0125 | 23.85 |
| LogPerm | 500.0 | 1.40010 | 0.409616 | 0.18 | 1.1475 | 1.380 | 1.6700 | 2.58 |
| AI | 500.0 | 2.99244 | 0.563674 | 1.21 | 2.5900 | 3.035 | 3.3725 | 4.70 |
| Brittle | 500.0 | 49.74682 | 15.212123 | 0.00 | 39.3125 | 49.595 | 59.2075 | 93.47 |
| TOC | 500.0 | 0.99800 | 0.503635 | 0.00 | 0.6400 | 0.960 | 1.3500 | 2.71 |
| VR | 500.0 | 1.99260 | 0.307434 | 0.90 | 1.8200 | 2.010 | 2.1725 | 2.84 |

è®©æˆ‘ä»¬ä¹Ÿè®¡ç®—ä¸€ä¸ªç›¸å…³çŸ©é˜µå¹¶æŸ¥çœ‹å®ƒã€‚

```py
corr_matrix = np.corrcoef(my_data_f6, rowvar = False)
print(np.around(corr_matrix,2))                           # print the correlation matrix to 2 decimals 
```

```py
[[ 1\.    0.79 -0.49 -0.25  0.71  0.12]
 [ 0.79  1\.   -0.32 -0.13  0.48  0.04]
 [-0.49 -0.32  1\.    0.14 -0.53  0.47]
 [-0.25 -0.13  0.14  1\.   -0.24  0.24]
 [ 0.71  0.48 -0.53 -0.24  1\.    0.35]
 [ 0.12  0.04  0.47  0.24  0.35  1\.  ]] 
```

æˆ‘ä»¬éœ€è¦å°†æ¯ä¸ªå˜é‡æ ‡å‡†åŒ–ï¼Œä½¿å…¶å‡å€¼ä¸ºé›¶ï¼Œæ–¹å·®ä¸º 1ã€‚è®©æˆ‘ä»¬è¿™æ ·åšå¹¶æ£€æŸ¥ç»“æœã€‚åœ¨ä¸‹é¢çš„æ§åˆ¶å°ä¸­ï¼Œæˆ‘ä»¬æ‰“å°å‡ºæ‰€æœ‰ 6 ä¸ªé¢„æµ‹å˜é‡çš„åˆå§‹å’Œæ ‡å‡†åŒ–å‡å€¼å’Œæ–¹å·®ã€‚

```py
features = ['Por','LogPerm','AI','Brittle','TOC','VR']
x_f6 = my_data_f6.loc[:,features].values
mu_f6 = np.mean(x_f6, axis=0)
sd_f6 = np.std(x_f6, axis=0)
x_f6 = StandardScaler().fit_transform(x_f6)

print("Original Means", features[:], np.round(mu_f6[:],2)) 
print("Original StDevs", features[:],np.round(sd_f6[:],2)) 
print('Mean Transformed =',features[:],np.round(x.mean(axis=0),2))
print('Variance Transformed Por =',features[:],np.round(x.var(axis=0),2)) 
```

```py
Original Means ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [14.9   1.4   2.99 49.75  1\.    1.99]
Original StDevs ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [ 2.98  0.41  0.56 15.2   0.5   0.31]
Mean Transformed = ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [0\. 0.]
Variance Transformed Por = ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [1\. 1.] 
```

æˆ‘ä»¬è¿˜åº”è¯¥æ£€æŸ¥æ¯ä¸ªå˜é‡çš„å•å˜é‡åˆ†å¸ƒã€‚

```py
f, (ax6,ax7,ax8,ax9,ax10,ax11) = plt.subplots(1, 6, sharey=True, figsize=(15,2))
ax6.hist(x_f6[:,0], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax6.set_title('Std. Porosity'); ax6.set_xlim(-5,5)
ax7.hist(x_f6[:,1], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax7.set_title('Std. Log[Perm.]'); ax7.set_xlim(-5,5)
ax8.hist(x_f6[:,2], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax8.set_title('Std. Acoustic Imped.'); ax8.set_xlim(-5,5)
ax9.hist(x_f6[:,3], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax9.set_title('Std. Brittleness'); ax9.set_xlim(-5,5)
ax10.hist(x_f6[:,4], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax10.set_title('Std. Total Organic C'); ax10.set_xlim(-5,5)
ax11.hist(x_f6[:,5], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax11.set_title('Std. Vit. Reflectance'); ax11.set_xlim(-5,5)
plt.show() 
```

![_images/cb70ebc58a6161c91e168f37c51faf16ad0c7a73cb23c7741794ee731d2470a4.png](img/c9eb87ba8e54d0f26a95ce04a0f2751a.png)

åŸºæœ¬ç»Ÿè®¡å’Œåˆ†å¸ƒçœ‹èµ·æ¥å¾ˆå¥½ã€‚æ²¡æœ‰æ˜æ˜¾çš„ç¼ºå¤±æ•°æ®ã€ç¼ºå£ã€æ˜¾è‘—çš„æˆªæ–­ã€å°–å³°æˆ–å¼‚å¸¸å€¼ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥å¯¹æˆ‘ä»¬ 6 ä¸ªç‰¹å¾æ‰§è¡Œä¸»æˆåˆ†åˆ†æäº†ã€‚

```py
n_components = 6
pca_f6 = PCA(n_components=n_components)
pca_f6.fit(x_f6)

print(np.round(pca_f6.components_,3))                     # visualize the component loadings 
```

```py
[[ 0.558  0.476 -0.405 -0.211  0.504  0.01 ]
 [-0.117 -0.114 -0.432 -0.323 -0.229 -0.794]
 [-0.019 -0.124  0.384 -0.898  0.07   0.157]
 [-0.214 -0.674 -0.424 -0.006  0.526  0.21 ]
 [-0.784  0.522 -0.031 -0.046  0.331 -0.019]
 [ 0.12  -0.138  0.566  0.206  0.55  -0.549]] 
```

é¦–å…ˆè®©æˆ‘ä»¬çœ‹çœ‹æˆåˆ†è´Ÿè½½ã€‚æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæˆåˆ†ï¼Œæœ€ä¸Šé¢ä¸€è¡Œæ˜¯ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ï¼ˆPC1ï¼‰ï¼Œæ¥ä¸‹æ¥ä¸€è¡Œæ˜¯ç¬¬äºŒä¸ªä¸»æˆåˆ†ï¼ˆPC2ï¼‰ï¼Œä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ°æœ€åä¸€è¡Œæ˜¯ç¬¬å…­ä¸ªä¸»æˆåˆ†ï¼ˆPC6ï¼‰ã€‚åˆ—æ˜¯æŒ‰ç…§â€˜Porâ€™ï¼Œâ€˜LogPermâ€™ï¼Œâ€˜AIâ€™ï¼Œâ€˜Brittleâ€™ï¼Œâ€˜TOCâ€™ï¼Œåˆ°â€˜VRâ€™çš„é¡ºåºæ’åˆ—çš„ç‰¹å¾ã€‚

ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ä¸»è¦ç”±å­”éš™ç‡ã€å¯¹æ•°æ¸—é€ç‡ã€å£°é˜»æŠ—å’Œæ€»æœ‰æœºç¢³ç»„æˆï¼Œè¡¨æ˜å®ƒä»¬å…±åŒå˜åŒ–çš„æ–¹å¼æ˜¯é€ æˆå¤§éƒ¨åˆ†æ–¹å·®çš„åŸå› ã€‚ä¸‹ä¸€ä¸ªä¸»æˆåˆ†ä¸»è¦ç”±é•œç…¤åå°„ç‡ç»„æˆã€‚ç¬¬ä¸‰ä¸ªä¸»åæ ‡ä¸»è¦ç”±è„†æ€§ç­‰ç»„æˆã€‚

## Scree Plots

ä¸ºäº†å¸®åŠ©è§£é‡Šï¼Œæˆ‘ä»¬åº”è¯¥è€ƒè™‘æ¯ä¸ªä¸»æˆåˆ†çš„æ–¹å·®è´¡çŒ®ã€‚

```py
print('Variance explained by PC1 thru PC6 =', np.round(pca_f6.explained_variance_ratio_,3))

f, (ax10, ax11) = plt.subplots(1, 2,figsize=(10,6))
f.subplots_adjust(wspace=0.5,hspace = 0.3)

ax10.plot(np.arange(1,7,1),pca_f6.explained_variance_ratio_*100,color='darkorange',alpha=0.8)
ax10.scatter(np.arange(1,7,1),pca_f6.explained_variance_ratio_*100,color='darkorange',alpha=0.8,edgecolor='black')
ax10.set_xlabel('Principal Component'); ax10.set_ylabel('Variance Explained'); ax10.set_title('Variance Explained by Principal Component')
fmt = '%.0f%%' # Format you want the ticks, e.g. '40%'
yticks = mtick.FormatStrFormatter(fmt); ax10.set_xlim(1,6); ax10.set_ylim(0,100.0)
ax10.yaxis.set_major_formatter(yticks); add_grid2(ax10)

ax11.plot(np.arange(1,7,1),np.cumsum(pca_f6.explained_variance_ratio_*100),color='darkorange',alpha=0.8)
ax11.scatter(np.arange(1,7,1),np.cumsum(pca_f6.explained_variance_ratio_*100),color='darkorange',alpha=0.8,edgecolor='black')
ax11.plot([1,6],[95,95], color='black',linestyle='dashed')
ax11.set_xlabel('Principal Component'); ax11.set_ylabel('Cumulative Variance Explained'); ax11.set_title('Cumulative Variance Explained by Principal Component')
fmt = '%.0f%%' # Format you want the ticks, e.g. '40%'
yticks = mtick.FormatStrFormatter(fmt); ax11.set_xlim(1,6); ax11.set_ylim(0,100.0); ax11.annotate('95% variance explained',[4.05,90])
ax11.yaxis.set_major_formatter(yticks); add_grid2(ax11)

plt.show() 
```

```py
Variance explained by PC1 thru PC6 = [0.462 0.246 0.149 0.11  0.024 0.009] 
```

![_images/f13a2759a5a3a9ba079c2c90976c1d01b7e4e03c073aeb9780c2e4db83e7bbbf.png](img/6f96fbe837665bd4b49b22deee4579f9.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¤§çº¦ 46%çš„æ–¹å·®ç”±ç¬¬ä¸€ä¸ªä¸»æˆåˆ†æè¿°ï¼Œç„¶åå¤§çº¦ 25%ç”±ç¬¬äºŒä¸ªä¸»æˆåˆ†æè¿°ç­‰ç­‰ã€‚

## ä¸»æˆåˆ†å¾—åˆ†ä¹‹é—´çš„ç‹¬ç«‹æ€§

åœ¨æŠ•å½±å‰åï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹ç‰¹å¾å¯¹çš„æˆå¯¹ç‰¹å¾ç›¸å…³æ€§ã€‚

```py
print('\nCorrelation Matrix of the 6 original features components:')
print(np.round(np.corrcoef(x_f6, rowvar = False),2))

print('\nCorrelation Matrix of the 6 principle components\' scores:')
print(np.round(np.corrcoef(pca_f6.transform(x_f6), rowvar = False),2)) 
```

```py
Correlation Matrix of the 6 original features components:
[[ 1\.    0.79 -0.49 -0.25  0.71  0.12]
 [ 0.79  1\.   -0.32 -0.13  0.48  0.04]
 [-0.49 -0.32  1\.    0.14 -0.53  0.47]
 [-0.25 -0.13  0.14  1\.   -0.24  0.24]
 [ 0.71  0.48 -0.53 -0.24  1\.    0.35]
 [ 0.12  0.04  0.47  0.24  0.35  1\.  ]]

Correlation Matrix of the 6 principle components' scores:
[[ 1\.  0\. -0\.  0\.  0\. -0.]
 [ 0\.  1\. -0\. -0\. -0\. -0.]
 [-0\. -0\.  1\. -0\. -0\.  0.]
 [ 0\. -0\. -0\.  1\.  0\.  0.]
 [ 0\. -0\. -0\.  0\.  1\.  0.]
 [-0\. -0\.  0\.  0\.  0\.  1.]] 
```

æ–°çš„æŠ•å½±ç‰¹å¾ï¼ˆå³ä½¿æ²¡æœ‰é™ç»´ï¼Œ$p=m$ï¼‰æ‰€æœ‰æˆå¯¹ç›¸å…³æ€§éƒ½æ˜¯ 0.0ï¼

+   æ‰€æœ‰æŠ•å½±ç‰¹å¾ä¹‹é—´éƒ½æ˜¯çº¿æ€§ç‹¬ç«‹çš„

## é™ç»´å¯¹ 2 ä¸ªç‰¹å¾å…³ç³»çš„å½±å“

å½“æˆ‘ä»¬ä¿ç•™$1,\ldots,6$ä¸ªä¸»æˆåˆ†æ—¶ï¼Œä»…è§‚å¯Ÿå­”éš™ç‡ä¸å¯¹æ•°æ¸—é€ç‡çš„åŒå˜é‡å…³ç³»å°†å¾ˆæœ‰è¶£ã€‚

+   è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨çŸ©é˜µæ•°å­¦æ¥é€† PCA å’Œæ ‡å‡†åŒ–ï¼Œç„¶åç»˜åˆ¶å¯¹æ•°æ¸—é€ç‡ä¸å­”éš™ç‡çš„æ•£ç‚¹å›¾ã€‚

```py
nComp = 6
xhat_6d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])
xhat_6d = sd_f6*xhat_6d + mu_f6

nComp = 5
xhat_5d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])
xhat_5d = sd_f6*xhat_5d + mu_f6

nComp = 4
xhat_4d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])
xhat_4d = sd_f6*xhat_4d + mu_f6

nComp = 3
xhat_3d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])
xhat_3d = sd_f6*xhat_3d + mu_f6

nComp = 2
xhat_2d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])
xhat_2d = sd_f6*xhat_2d + mu_f6

nComp = 1
xhat_1d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])
xhat_1d = sd_f6*xhat_1d + mu_f6

f, (ax12, ax13, ax14, ax15, ax16, ax17, ax18) = plt.subplots(1, 7,figsize=(20,20))
f.subplots_adjust(wspace=0.7)

ax12.scatter(my_data_f6["Por"],my_data_f6["LogPerm"],s=None, c="darkorange",marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax12.set_title('Original Data'); ax12.set_xlabel('Por'); ax12.set_ylabel('LogPerm')
ax12.set_ylim(0.0,3.0); ax12.set_xlim(8,22); ax12.set_aspect(4.0); 

ax13.scatter(xhat_1d[:,0],xhat_1d[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax13.set_title('1 Principal Component'); ax13.set_xlabel('Por'); ax13.set_ylabel('LogPerm')
ax13.set_ylim(0.0,3.0); ax13.set_xlim(8,22); ax13.set_aspect(4.0)

ax14.scatter(xhat_2d[:,0],xhat_2d[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax14.set_title('2 Principal Components'); ax14.set_xlabel('Por'); ax14.set_ylabel('LogPerm')
ax14.set_ylim(0.0,3.0); ax14.set_xlim(8,22); ax14.set_aspect(4.0)

ax15.scatter(xhat_3d[:,0],xhat_3d[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax15.set_title('3 Principal Components'); ax15.set_xlabel('Por'); ax15.set_ylabel('LogPerm')
ax15.set_ylim(0.0,3.0); ax15.set_xlim(8,22); ax15.set_aspect(4.0)

ax16.scatter(xhat_4d[:,0],xhat_4d[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax16.set_title('4 Principal Components'); ax16.set_xlabel('Por'); ax16.set_ylabel('LogPerm')
ax16.set_ylim(0.0,3.0); ax16.set_xlim(8,22); ax16.set_aspect(4.0)

ax17.scatter(xhat_5d[:,0],xhat_5d[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax17.set_title('5 Principal Components'); ax17.set_xlabel('Por'); ax17.set_ylabel('LogPerm')
ax17.set_ylim(0.0,3.0); ax17.set_xlim(8,22); ax17.set_aspect(4.0)

ax18.scatter(xhat_6d[:,0],xhat_6d[:,1],s=None, c="darkorange", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors="black")
ax18.set_title('6 Principal Components'); ax18.set_xlabel('Por'); ax18.set_ylabel('LogPerm')
ax18.set_ylim(0.0,3.0); ax18.set_xlim(8,22); ax18.set_aspect(4.0)

plt.show() 
```

![_images/2c07008ca4c1cad616bfb73f5ffed082b67c565a0bbab5e216624e59e8c949ff.png](img/5a69f72819a4fda450143b47c8b81454.png)

è§‚å¯Ÿåˆ°éšç€æˆ‘ä»¬åŒ…å«æ›´å¤šæˆåˆ†ï¼Œå¯¹æ•°æ¸—é€ç‡å’Œå­”éš™ç‡ä¹‹é—´çš„åŒå˜é‡å…³ç³»å‡†ç¡®æ€§æé«˜ï¼Œè¿™éå¸¸æœ‰è¶£ã€‚è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ–¹å·®ã€‚

```py
print('1 Principal Component : Variance Por =',np.round(np.var(xhat_1d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_1d[:,1])/(sd_f6[1]*sd_f6[1]),2))

print('2 Principal Components: Variance Por =',np.round(np.var(xhat_2d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_2d[:,1])/(sd_f6[1]*sd_f6[1]),2))

print('3 Principal Components: Variance Por =',np.round(np.var(xhat_3d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_3d[:,1])/(sd_f6[1]*sd_f6[1]),2))

print('4 Principal Components: Variance Por =',np.round(np.var(xhat_4d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_4d[:,1])/(sd_f6[1]*sd_f6[1]),2))

print('5 Principal Components: Variance Por =',np.round(np.var(xhat_5d[:,0])/(sd_f6[0]*sd_f6[0]),2),'  Variance Log Perm = ',np.round(np.var(xhat_5d[:,1])/(sd_f6[1]*sd_f6[1]),2))

print('6 Principal Components: Variance Por =',np.round(np.var(xhat_6d[:,0])/(sd_f6[0]*sd_f6[0]),2),'  Variance Log Perm = ',np.round(np.var(xhat_6d[:,1])/(sd_f6[1]*sd_f6[1]),2)) 
```

```py
1 Principal Component : Variance Por = 0.86  Variance Log Perm =  0.63
2 Principal Components: Variance Por = 0.88  Variance Log Perm =  0.65
3 Principal Components: Variance Por = 0.88  Variance Log Perm =  0.66
4 Principal Components: Variance Por = 0.91  Variance Log Perm =  0.96
5 Principal Components: Variance Por = 1.0   Variance Log Perm =  1.0
6 Principal Components: Variance Por = 1.0   Variance Log Perm =  1.0 
```

è¿™å¾ˆæœ‰è¶£ã€‚ç¬¬ä¸€ä¸ªä¸»æˆåˆ†æè¿°äº† 86%çš„å­”éš™ç‡æ–¹å·®ã€‚æ¥ä¸‹æ¥çš„ä¸¤ä¸ªä¸»æˆåˆ†æ²¡æœ‰æä¾›å¤ªå¤šå¸®åŠ©ã€‚ç„¶åæ˜¯ç¬¬å››å’Œç¬¬äº”ä¸ªä¸»æˆåˆ†çš„è·³è·ƒã€‚

+   å½“ç„¶ï¼Œé—®é¢˜æ˜¯ 6 ç»´çš„ï¼Œä¸ä»…ä»…æ˜¯å­”éš™ç‡ä¸å¯¹æ•°æ¸—é€ç‡ï¼Œä½†æ˜¯çœ‹åˆ°ä¸»æˆåˆ†æ•°é‡ä¸ä¿ç•™çš„æ¯ä¸ªåŸå§‹ç‰¹å¾æ–¹å·®ä¹‹é—´çš„å…³ç³»æ˜¯å¦æœ‰è¶£

+   ä¸»æˆåˆ†å¹¶ä¸å‡åŒ€åœ°æè¿°æ¯ä¸ªç‰¹å¾

## æ‰€æœ‰ç‰¹å¾çŸ©é˜µæ•£ç‚¹å›¾çš„é™ç»´å½±å“

è®©æˆ‘ä»¬çœ‹çœ‹çŸ©é˜µæ•£ç‚¹å›¾ï¼Œä»¥æŸ¥çœ‹æ‰€æœ‰åŒå˜é‡ç»„åˆã€‚

+   é¦–å…ˆï¼Œæœ‰ä¸€äº›è´¦ç›®éœ€è¦å¤„ç†ï¼Œæˆ‘ä»¬å¿…é¡»å°† 6D é™ç»´æ¨¡å‹æ”¾å…¥ DataFrame ä¸­ï¼ˆç›®å‰æ˜¯ Numpy ndarraysï¼‰ã€‚

```py
df_1d = pd.DataFrame(data=xhat_1d,columns=features)   
df_2d = pd.DataFrame(data=xhat_2d,columns=features)
df_3d = pd.DataFrame(data=xhat_3d,columns=features)
df_4d = pd.DataFrame(data=xhat_4d,columns=features)
df_5d = pd.DataFrame(data=xhat_5d,columns=features)
df_6d = pd.DataFrame(data=xhat_6d,columns=features) 
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº› DataFrame ç”ŸæˆçŸ©é˜µæ•£ç‚¹å›¾ã€‚å½“æˆ‘ä»¬æ·»åŠ ä¸»æˆåˆ†æ—¶ï¼Œçœ‹åˆ°åŒå˜é‡å›¾çš„å‡†ç¡®æ€§æé«˜éå¸¸æœ‰è¶£ã€‚è€Œä¸”ï¼Œä»…ç”¨ä¸¤ä¸ªä¸»æˆåˆ†ï¼Œæˆ‘ä»¬å°±èƒ½å¾ˆå¥½åœ°æ•æ‰åˆ°ä¸€äº›å˜é‡å¯¹çš„åŒå˜é‡å…³ç³»ã€‚

```py
fig = plt.figure()

pd_plot.scatter_matrix(my_data_f6, alpha = 0.1,           # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('Original Data')

pd_plot.scatter_matrix(df_1d, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('1 Principal Component')

pd_plot.scatter_matrix(df_2d, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('2 Principal Components')

pd_plot.scatter_matrix(df_3d, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('3 Principal Components')

pd_plot.scatter_matrix(df_4d, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('4 Principal Components')

pd_plot.scatter_matrix(df_5d, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('5 Principal Components')

pd_plot.scatter_matrix(df_6d, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('6 Principal Components')

plt.show() 
```

```py
<Figure size 640x480 with 0 Axes> 
```

![å›¾ç‰‡](img/e78345fd784cdb36b14f282fe91180d5.png) ![å›¾ç‰‡](img/73bc7cc3f14b45b2024f8ac9631eeff6.png) ![å›¾ç‰‡](img/58965505867559377436105bcfe2bd2f.png) ![å›¾ç‰‡](img/edc87f7af42bdbab50ece3313c71f72d.png) ![å›¾ç‰‡](img/deadaf11c865b21029ccb2121e4df45b.png) ![å›¾ç‰‡](img/3847c4e4cad0f11b7497cac6c1234968.png) ![å›¾ç‰‡](img/ffde399e1f56a914a61abaada97a2abb.png)

## å¯¹éç›¸å…³æ•°æ®çš„ä¸»æˆåˆ†åˆ†æ

è®©æˆ‘ä»¬å†è¿›è¡Œä¸€æ¬¡æµ‹è¯•ï¼Œå¯¹éç›¸å…³æ•°æ®è¿›è¡Œä¸»æˆåˆ†åˆ†æã€‚

+   æˆ‘ä»¬ä¸º 5 ä¸ªç‰¹å¾ç”Ÿæˆå¤§é‡éšæœºæ ·æœ¬ï¼ˆn å¾ˆå¤§ï¼‰ã€‚

+   æˆ‘ä»¬å°†å‡è®¾å‡åŒ€åˆ†å¸ƒ

```py
x_rand = np.random.rand(10000,5); df_x_rand = pd.DataFrame(x_rand)
print('Variance of original features: ', np.round(np.var(x_rand, axis = 0),2))
print('Proportion of variance of original features: ', np.round(np.var(x_rand, axis = 0)/np.sum(np.var(x_rand, axis = 0)),2))
print('Correlation Matrix of original features:\n'); print(np.round(np.cov(x_rand, rowvar = False),2)); print()

pd_plot.scatter_matrix(df_x_rand, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('Original Features')

pca_rand = PCA(n_components=5)
pca_rand.fit(x_rand)
print('PCA Variance Explained ', np.round(pca_rand.explained_variance_ratio_,2))  

scores_x_rand = pca_rand.transform(x_rand); df_scores_x_rand = pd.DataFrame(scores_x_rand)

print('\nCorrelation Matrix of scores:\n'); print(np.round(np.cov(scores_x_rand, rowvar = False),2)); print()

pd_plot.scatter_matrix(df_scores_x_rand, alpha = 0.1,                # pandas matrix scatter plot
    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})
plt.suptitle('Principal Component Scores') 
```

```py
Variance of original features:  [0.08 0.08 0.08 0.08 0.08]
Proportion of variance of original features:  [0.2 0.2 0.2 0.2 0.2]
Correlation Matrix of original features:

[[ 0.08 -0\.    0\.   -0\.    0\.  ]
 [-0\.    0.08  0\.    0\.   -0\.  ]
 [ 0\.    0\.    0.08  0\.   -0\.  ]
 [-0\.    0\.    0\.    0.08  0\.  ]
 [ 0\.   -0\.   -0\.    0\.    0.08]] 
```

```py
PCA Variance Explained  [0.21 0.2  0.2  0.2  0.19]

Correlation Matrix of scores:

[[ 0.09 -0\.   -0\.   -0\.    0\.  ]
 [-0\.    0.08  0\.   -0\.   -0\.  ]
 [-0\.    0\.    0.08  0\.    0\.  ]
 [-0\.   -0\.    0\.    0.08  0\.  ]
 [ 0\.   -0\.    0\.    0\.    0.08]] 
```

```py
Text(0.5, 0.98, 'Principal Component Scores') 
```

![å›¾ç‰‡](img/3e13125b9ca06c498e74bfcd5ae47b64.png) ![å›¾ç‰‡](img/67ae027cd9095c50fbc22e0a2f176a6b.png)

å½“ä¸»æˆåˆ†åˆ†æåº”ç”¨äºéç›¸å…³ã€å‡åŒ€åˆ†å¸ƒçš„ç‰¹å¾æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

+   æ‰€æœ‰ä¸»æˆåˆ†æè¿°ç›¸åŒæ•°é‡çš„æ–¹å·®

+   é€šè¿‡ç‰¹å¾æŠ•å½±æ²¡æœ‰é™ç»´çš„æœºä¼š

+   ç‹¬ç«‹éšæœºå˜é‡çš„çº¿æ€§ç»„åˆå¼•å‘ä¸­å¿ƒæé™å®šç†ï¼Œä¸»æˆåˆ†å¾—åˆ†è¶‹å‘äºé«˜æ–¯åˆ†å¸ƒï¼ˆå‚è§ä¸Šé¢çŸ©é˜µæ•£ç‚¹å›¾ä¸­ç‚¹çš„å››èˆäº”å…¥ï¼‰

## åœ¨æ–°æ•°æ®é›†ä¸Šçš„å®è·µ

å¥½çš„ï¼Œæ˜¯æ—¶å€™å¼€å§‹å·¥ä½œäº†ã€‚è®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªæ•°æ®é›†å¹¶ä½¿ç”¨ PCA è¿›è¡Œåˆ†æï¼Œ

+   ç´§å‡‘çš„ä»£ç 

+   åŸºæœ¬å¯è§†åŒ–

+   ä¿å­˜è¾“å‡º

æ‚¨å¯ä»¥é€‰æ‹©è¿™äº›æ•°æ®é›†ä¹‹ä¸€æˆ–ä¿®æ”¹ä»£ç å¹¶æ·»åŠ æ‚¨è‡ªå·±çš„æ•°æ®é›†æ¥å®Œæˆæ­¤æ“ä½œã€‚

### æ•°æ®é›† 0ï¼Œéå¸¸è§„å¤šå…ƒå˜é‡ v4

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒæ•°æ®é›† [unconv_MV.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v4.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   ä¼˜è‰¯çš„å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   å²©æ€§æ¯”ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   ç»ç’ƒè´¨åå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹ç”Ÿäº§ 90 å¤©å¹³å‡ï¼ˆMCFPDï¼‰ã€‚

### æ•°æ®é›† 1ï¼ŒåäºŒä¸ªï¼Œ12

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒ 2D ç©ºé—´æ•°æ®é›† [12_sample_data.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/12_sample_data.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 480 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   X (m), Y (m) ä½ç½®åæ ‡

+   å²©æ€§ï¼ˆ0 - é¡µå²©ï¼Œ1 - ç ‚å²©ï¼‰

+   å•ä½è½¬æ¢åçš„å­”éš™ç‡ï¼ˆ%ï¼‰

+   æ¸—é€ç‡ï¼ˆmDï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

### æ•°æ®é›† 2ï¼Œå‚¨å±‚ 21

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒ 3D ç©ºé—´æ•°æ®é›† [res21_wells.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/res21_wells.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 73 ä¸ªå‚ç›´äº•åœ¨ 10,000m x 10,000m x 50 m å‚¨å±‚å•å…ƒçš„å˜é‡ï¼š

+   äº•ï¼ˆIDï¼‰

+   Xï¼ˆmï¼‰ï¼ŒYï¼ˆmï¼‰ï¼Œæ·±åº¦ï¼ˆmï¼‰ä½ç½®åæ ‡

+   å•ä½è½¬æ¢åçš„å­”éš™ç‡ï¼ˆ%ï¼‰

+   æ¸—é€ç‡ï¼ˆmDï¼‰

+   å•ä½è½¬æ¢åçš„å£°æ³¢é˜»æŠ—ï¼ˆkg/m2s*10â¶ï¼‰

+   ç›¸ï¼ˆåˆ†ç±»ï¼‰ - ä»é¡µå²©ã€ç ‚è´¨é¡µå²©ã€é¡µå²©ç ‚åˆ°ç ‚å²©çš„é¡ºåºã€‚

+   å¯†åº¦ï¼ˆg/cmÂ³ï¼‰

+   å‹ç¼©æ³¢é€Ÿåº¦ï¼ˆm/sï¼‰

+   æ¨æ°æ¨¡é‡ï¼ˆGPaï¼‰

+   å‰ªåˆ‡æ³¢é€Ÿåº¦ï¼ˆm/sï¼‰

+   å‰ªåˆ‡æ¨¡é‡ï¼ˆGPaï¼‰

æˆ‘ä»¬ä½¿ç”¨ pandas çš„ â€˜read_csvâ€™ å‡½æ•°å°†è¡¨æ ¼æ•°æ®åŠ è½½åˆ°åä¸º â€˜my_dataâ€™ çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

+   æˆ‘ä»¬è¿˜ç”¨æ•°æ®èŒƒå›´å’Œæ ‡ç­¾å¡«å……åˆ—è¡¨ï¼Œä»¥ä¾¿äºç»˜å›¾

åŠ è½½æ•°æ®å¹¶æ ¼å¼åŒ–ï¼Œ

+   åˆ é™¤å“åº”ç‰¹å¾

+   æ ¹æ®éœ€è¦é‡æ–°æ ¼å¼åŒ–ç‰¹å¾

+   æ­¤å¤–ï¼Œæˆ‘ä¹Ÿå–œæ¬¢å°†å…ƒæ•°æ®å­˜å‚¨åœ¨åˆ—è¡¨ä¸­

```py
idata = 0                                                     # select the dataset

if idata == 0:
    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['Well','Prod'],axis=1,inplace=True)          # remove well index and response feature

    features = df_new.columns.values.tolist()                 # store the names of the features

    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting

    flabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Brittleness Ratio (%)', # set the names for plotting
             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']

    ftitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting
             'Total Organic Carbon','Vitrinite Reflectance']

elif idata == 1:
    names = {'Porosity':'Por'}

    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['X','Y','Unnamed: 0'],axis=1,inplace=True)   # remove response feature
    df_new = df_new.rename(columns=names)

    features = df_new.columns.values.tolist()                 # store the names of the features

    xmin_new = [0.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [10000.0,10000.0,1.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting

    flabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Facies (categorical)',
              'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting

    ftitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',
              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']

elif idata == 2:  
    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['Well_ID','X','Y','CumulativeOil'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature
    df_new = df_new.dropna(how='any',inplace=False)

    features = df_new.columns.values.tolist()                 # store the names of the features

    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting

    flabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Facies (categorical)',
              'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting

    ftitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',
              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']

df_new[df_new.columns] = MinMaxScaler().fit_transform(df_new) # min/max normalize all the features
df_new.head(n=13) 
```

|  | å­”éš™ç‡ | æ¸—é€ç‡ | AI | å²©è„†æ€§ | TOC | VR |
| --- | --- | --- | --- | --- | --- | --- |
| 0 | 0.325294 | 0.204805 | 0.453731 | 0.960076 | 0.569620 | 0.711340 |
| 1 | 0.342941 | 0.274600 | 0.579104 | 0.480038 | 0.455696 | 0.489691 |
| 2 | 0.439412 | 0.167048 | 0.814925 | 0.842894 | 0.455696 | 0.922680 |
| 3 | 0.654118 | 0.643021 | 0.402985 | 0.393378 | 0.535865 | 0.489691 |
| 4 | 0.645294 | 0.393593 | 0.567164 | 0.000000 | 0.717300 | 0.500000 |
| 5 | 0.469412 | 0.421053 | 0.420896 | 0.581278 | 0.476793 | 0.381443 |
| 6 | 0.408235 | 0.282609 | 0.492537 | 0.719035 | 0.417722 | 0.474227 |
| 7 | 0.295882 | 0.217391 | 0.588060 | 0.573103 | 0.371308 | 0.515464 |
| 8 | 0.351176 | 0.181922 | 0.343284 | 0.747105 | 0.481013 | 0.541237 |
| 9 | 0.394118 | 0.321510 | 0.725373 | 0.752964 | 0.561181 | 0.886598 |
| 10 | 0.499412 | 0.372998 | 0.280597 | 0.683608 | 0.535865 | 0.432990 |
| 11 | 0.567059 | 0.591533 | 0.301493 | 0.519962 | 0.725738 | 0.479381 |
| 12 | 0.604118 | 0.490847 | 0.453731 | 0.759095 | 0.573840 | 0.541237 |

### æ‰§è¡Œä¸»æˆåˆ†åˆ†æ

æ‰§è¡Œä¸»æˆåˆ†åˆ†æï¼Œ

1.  è®¡ç®—ä¸»æˆåˆ†è½½è·

1.  é€‰æ‹©ä¸»æˆåˆ†çš„æ•°é‡ä»¥æè¿°ç›®æ ‡æ–¹å·®è§£é‡Š

1.  åˆ›å»ºä¸€ä¸ªæ–°çš„ DataFrameï¼ŒåŒ…å«ä¸»æˆåˆ†å¾—åˆ†

```py
var_explained = 0.95                                          # select the minimum variance explained

n_components = min(len(df_new.columns),len(df_new)-1)         # max components is min of number of features or number of data - 1
pca_new = PCA(n_components=n_components).fit(df_new.values)   # calculate PCA
pca_scores = pca_new.fit_transform(df_new.values)

cumulative_variance = np.cumsum(pca_new.explained_variance_ratio_) # calculate cumulative explained variance

n_selected = np.argmax(cumulative_variance >= var_explained) + 1 # find number of components to retain 95% variance

df_new_projected = pd.DataFrame(pca_scores[:, :n_selected],columns=[f'PC{i+1}' for i in range(n_selected)],
            index=df_new.index)                               # project data to that many principal components

sns.pairplot(df_new_projected.iloc[:,:], plot_kws={'alpha':1.0,'s':50}, palette = 'colorblind', corner=True) # matrix scatter plot
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.6, top=0.7, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/c849a0643237d2cfdd80d77be1638fac7ede9092059c7cf673a61db49ae9519a.png](img/5c93c2ee554f6df8bbbb8936c7fec98e.png)

### æ£€æŸ¥ç´¯ç§¯æ–¹å·®è§£é‡Š

```py
plt.plot(np.arange(1,len(pca_new.explained_variance_ratio_)+1,1),np.cumsum(pca_new.explained_variance_ratio_*100),color='darkorange',alpha=0.8)
plt.scatter(np.arange(1,len(pca_new.explained_variance_ratio_)+1,1),np.cumsum(pca_new.explained_variance_ratio_*100),color='darkorange',alpha=0.8,edgecolor='black')
plt.plot([1,len(df_new.columns)],[95,95], color='black',linestyle='dashed'); plt.plot([n_selected,n_selected],[0,100],color='red',zorder=-1)
plt.annotate('Selected Number of Components = '+ str(n_selected),[n_selected,10],rotation=270,color='red')
plt.xlabel('Principal Component'); plt.ylabel('Cumulative Variance Explained'); plt.title('Cumulative Variance Explained by Principal Component')
fmt = '%.0f%%' # Format you want the ticks
plt.xticks(range(1, len(cumulative_variance) + 1))
yticks = mtick.FormatStrFormatter(fmt); plt.xlim(1,len(pca_new.explained_variance_ratio_)); plt.ylim(0,100.0) 
plt.annotate('95% variance explained',[4.05,90]); add_grid()
plt.gca().yaxis.set_major_formatter(PercentFormatter(100.0))  # 1.0 = 100%
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![_images/2ff3b417eb8d29bf00f83b4ed697d6e6112a30ee3d2545d1079c3b838c4d2933.png](img/4810ab4ed9ff9b4c3c7270ea4fce631d.png)

### ä¿å­˜ä¸»æˆåˆ†

ç°åœ¨æˆ‘ä»¬å¯ä»¥é€‰æ‹©å°†é™ç»´ä¸»æˆåˆ†å¾—åˆ†å†™å…¥ DataFrameã€‚

```py
save_PCA = True                                        # save the imputed DataFrame?

if save_PCA == True:
    folder = r'C:\Local'
    file_name = r'dataframe_PCA.csv'

    df_new_projected.to_csv(folder + "/" + file_name, index=False) 
```

```py
---------------------------------------------------------------------------
OSError  Traceback (most recent call last)
Cell In[42], line 7
  4 folder = r'C:\Local'
  5 file_name = r'dataframe_PCA.csv'
----> 7 df_new_projected.to_csv(folder + "/" + file_name, index=False)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\core\generic.py:3772, in NDFrame.to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)
  3761 df = self if isinstance(self, ABCDataFrame) else self.to_frame()
  3763 formatter = DataFrameFormatter(
  3764     frame=df,
  3765     header=header,
   (...)
  3769     decimal=decimal,
  3770 )
-> 3772 return DataFrameRenderer(formatter).to_csv(
  3773     path_or_buf,
  3774     lineterminator=lineterminator,
  3775     sep=sep,
  3776     encoding=encoding,
  3777     errors=errors,
  3778     compression=compression,
  3779     quoting=quoting,
  3780     columns=columns,
  3781     index_label=index_label,
  3782     mode=mode,
  3783     chunksize=chunksize,
  3784     quotechar=quotechar,
  3785     date_format=date_format,
  3786     doublequote=doublequote,
  3787     escapechar=escapechar,
  3788     storage_options=storage_options,
  3789 )

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\formats\format.py:1186, in DataFrameRenderer.to_csv(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)
  1165     created_buffer = False
  1167 csv_formatter = CSVFormatter(
  1168     path_or_buf=path_or_buf,
  1169     lineterminator=lineterminator,
   (...)
  1184     formatter=self.fmt,
  1185 )
-> 1186 csv_formatter.save()
  1188 if created_buffer:
  1189     assert isinstance(path_or_buf, StringIO)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\formats\csvs.py:240, in CSVFormatter.save(self)
  236  """
  237 Create the writer & save.
  238 """
  239 # apply compression and byte/text conversion
--> 240 with get_handle(
  241     self.filepath_or_buffer,
  242     self.mode,
  243     encoding=self.encoding,
  244     errors=self.errors,
  245     compression=self.compression,
  246     storage_options=self.storage_options,
  247 ) as handles:
  248     # Note: self.encoding is irrelevant here
  249     self.writer = csvlib.writer(
  250         handles.handle,
  251         lineterminator=self.lineterminator,
   (...)
  256         quotechar=self.quotechar,
  257     )
  259     self._save()

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:737, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)
  735 # Only for write methods
  736 if "r" not in mode and is_path:
--> 737     check_parent_directory(str(handle))
  739 if compression:
  740     if compression != "zstd":
  741         # compression libraries do not like an explicit text-mode

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:600, in check_parent_directory(path)
  598 parent = Path(path).parent
  599 if not parent.is_dir():
--> 600     raise OSError(rf"Cannot save file into a non-existent directory: '{parent}'")

OSError: Cannot save file into a non-existent directory: 'C:\Local' 
```

### æ•°æ®é›† 0ï¼Œéå¸¸è§„å¤šå˜é‡ v4

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒæ•°æ®é›† [unconv_MV.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v4.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å«æ¥è‡ª 1,000 ä¸ªéå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   äº•å¹³å‡å­”éš™ç‡

+   æ¸—é€ç‡çš„å¯¹æ•°å˜æ¢ï¼ˆä»¥çº¿æ€§åŒ–ä¸å…¶ä»–å˜é‡çš„å…³ç³»ï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

+   å‰ªåˆ‡æ¯”ï¼ˆ%ï¼‰

+   æ€»æœ‰æœºç¢³ï¼ˆ%ï¼‰

+   ç»ç’ƒè´¨åå°„ç‡ï¼ˆ%ï¼‰

+   åˆå§‹äº§é‡ 90 å¤©å¹³å‡ï¼ˆMCFPDï¼‰ã€‚

### æ•°æ®é›† 1ï¼ŒåäºŒï¼Œ12

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€2D ç©ºé—´æ•°æ®é›† [12_sample_data.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/12_sample_data.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å« 480 å£éå¸¸è§„äº•çš„å˜é‡ï¼ŒåŒ…æ‹¬ï¼š

+   Xï¼ˆmï¼‰ï¼ŒYï¼ˆmï¼‰ä½ç½®åæ ‡

+   ç›¸ï¼ˆ0 - é¡µå²©ï¼Œ1 - ç ‚å²©ï¼‰

+   å­”éš™ç‡ï¼ˆ%ï¼‰å•ä½è½¬æ¢å

+   æ¸—é€ç‡ï¼ˆmDï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆkg/mÂ³ x m/s x 10â¶ï¼‰

### æ•°æ®é›† 2ï¼Œå‚¨å±‚ 21

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€3D ç©ºé—´æ•°æ®é›† [res21_wells.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/res21_wells.csv)ã€‚æ­¤æ•°æ®é›†åŒ…å« 73 å£å‚ç›´äº•åœ¨ 10,000m x 10,000m x 50 m å‚¨å±‚å•å…ƒä¸­çš„å˜é‡ï¼š

+   äº•ï¼ˆIDï¼‰

+   Xï¼ˆmï¼‰ï¼ŒYï¼ˆmï¼‰ï¼Œæ·±åº¦ï¼ˆmï¼‰ä½ç½®åæ ‡

+   å­”éš™ç‡ï¼ˆ%ï¼‰å•ä½è½¬æ¢å

+   æ¸—é€ç‡ï¼ˆmDï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆkg/mÂ²s*10â¶ï¼‰å•ä½è½¬æ¢å

+   ç›¸ï¼ˆåˆ†ç±»ï¼‰ - ä»é¡µå²©ã€ç ‚è´¨é¡µå²©ã€é¡µå²©ç ‚åˆ°ç ‚å²©çš„é¡ºåºã€‚

+   å¯†åº¦ï¼ˆg/cmÂ³ï¼‰

+   å¯å‹ç¼©æ³¢é€Ÿï¼ˆm/sï¼‰

+   æ¨æ°æ¨¡é‡ï¼ˆGPaï¼‰

+   å‰ªåˆ‡æ³¢é€Ÿï¼ˆm/sï¼‰

+   å‰ªåˆ‡æ¨¡é‡ï¼ˆGPaï¼‰

æˆ‘ä»¬ä½¿ç”¨ pandas çš„ â€˜read_csvâ€™ å‡½æ•°å°†è¡¨æ ¼æ•°æ®åŠ è½½åˆ°åä¸º â€˜my_dataâ€™ çš„ DataFrame ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

+   æˆ‘ä»¬è¿˜ç”¨æ•°æ®èŒƒå›´å’Œæ ‡ç­¾å¡«å……åˆ—è¡¨ï¼Œä»¥ä¾¿äºç»˜å›¾

åŠ è½½æ•°æ®å¹¶æ ¼å¼åŒ–ï¼Œ

+   åˆ é™¤å“åº”ç‰¹å¾

+   æ ¹æ®éœ€è¦é‡æ–°æ ¼å¼åŒ–ç‰¹å¾

+   æ­¤å¤–ï¼Œæˆ‘è¿˜å–œæ¬¢å°†å…ƒæ•°æ®å­˜å‚¨åœ¨åˆ—è¡¨ä¸­

```py
idata = 0                                                     # select the dataset

if idata == 0:
    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['Well','Prod'],axis=1,inplace=True)          # remove well index and response feature

    features = df_new.columns.values.tolist()                 # store the names of the features

    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting

    flabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Brittleness Ratio (%)', # set the names for plotting
             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']

    ftitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting
             'Total Organic Carbon','Vitrinite Reflectance']

elif idata == 1:
    names = {'Porosity':'Por'}

    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['X','Y','Unnamed: 0'],axis=1,inplace=True)   # remove response feature
    df_new = df_new.rename(columns=names)

    features = df_new.columns.values.tolist()                 # store the names of the features

    xmin_new = [0.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [10000.0,10000.0,1.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting

    flabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Facies (categorical)',
              'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting

    ftitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',
              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']

elif idata == 2:  
    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository 
    df_new.drop(['Well_ID','X','Y','CumulativeOil'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature
    df_new = df_new.dropna(how='any',inplace=False)

    features = df_new.columns.values.tolist()                 # store the names of the features

    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting

    flabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10â¶)','Facies (categorical)',
              'Density (g/cmÂ³)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting

    ftitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',
              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']

df_new[df_new.columns] = MinMaxScaler().fit_transform(df_new) # min/max normalize all the features
df_new.head(n=13) 
```

|  | Por | Perm | AI | Brittle | TOC | VR |
| --- | --- | --- | --- | --- | --- | --- |
| 0 | 0.325294 | 0.204805 | 0.453731 | 0.960076 | 0.569620 | 0.711340 |
| 1 | 0.342941 | 0.274600 | 0.579104 | 0.480038 | 0.455696 | 0.489691 |
| 2 | 0.439412 | 0.167048 | 0.814925 | 0.842894 | 0.455696 | 0.922680 |
| 3 | 0.654118 | 0.643021 | 0.402985 | 0.393378 | 0.535865 | 0.489691 |
| 4 | 0.645294 | 0.393593 | 0.567164 | 0.000000 | 0.717300 | 0.500000 |
| 5 | 0.469412 | 0.421053 | 0.420896 | 0.581278 | 0.476793 | 0.381443 |
| 6 | 0.408235 | 0.282609 | 0.492537 | 0.719035 | 0.417722 | 0.474227 |
| 7 | 0.295882 | 0.217391 | 0.588060 | 0.573103 | 0.371308 | 0.515464 |
| 8 | 0.351176 | 0.181922 | 0.343284 | 0.747105 | 0.481013 | 0.541237 |
| 9 | 0.394118 | 0.321510 | 0.725373 | 0.752964 | 0.561181 | 0.886598 |
| 10 | 0.499412 | 0.372998 | 0.280597 | 0.683608 | 0.535865 | 0.432990 |
| 11 | 0.567059 | 0.591533 | 0.301493 | 0.519962 | 0.725738 | 0.479381 |
| 12 | 0.604118 | 0.490847 | 0.453731 | 0.759095 | 0.573840 | 0.541237 |

### æ‰§è¡Œä¸»æˆåˆ†åˆ†æ

æ‰§è¡Œä¸»æˆåˆ†åˆ†æï¼Œ

1.  è®¡ç®—ä¸»æˆåˆ†è½½è·

1.  é€‰æ‹©ä¸»æˆåˆ†çš„æ•°é‡ä»¥æè¿°ç›®æ ‡æ–¹å·®è§£é‡Š

1.  åˆ›å»ºä¸€ä¸ªæ–°çš„ DataFrameï¼ŒåŒ…å«ä¸»æˆåˆ†å¾—åˆ†

```py
var_explained = 0.95                                          # select the minimum variance explained

n_components = min(len(df_new.columns),len(df_new)-1)         # max components is min of number of features or number of data - 1
pca_new = PCA(n_components=n_components).fit(df_new.values)   # calculate PCA
pca_scores = pca_new.fit_transform(df_new.values)

cumulative_variance = np.cumsum(pca_new.explained_variance_ratio_) # calculate cumulative explained variance

n_selected = np.argmax(cumulative_variance >= var_explained) + 1 # find number of components to retain 95% variance

df_new_projected = pd.DataFrame(pca_scores[:, :n_selected],columns=[f'PC{i+1}' for i in range(n_selected)],
            index=df_new.index)                               # project data to that many principal components

sns.pairplot(df_new_projected.iloc[:,:], plot_kws={'alpha':1.0,'s':50}, palette = 'colorblind', corner=True) # matrix scatter plot
plt.subplots_adjust(left=0.0, bottom=0.0, right=0.6, top=0.7, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/5c93c2ee554f6df8bbbb8936c7fec98e.png)

### æ£€æŸ¥ç´¯ç§¯æ–¹å·®è§£é‡Š

```py
plt.plot(np.arange(1,len(pca_new.explained_variance_ratio_)+1,1),np.cumsum(pca_new.explained_variance_ratio_*100),color='darkorange',alpha=0.8)
plt.scatter(np.arange(1,len(pca_new.explained_variance_ratio_)+1,1),np.cumsum(pca_new.explained_variance_ratio_*100),color='darkorange',alpha=0.8,edgecolor='black')
plt.plot([1,len(df_new.columns)],[95,95], color='black',linestyle='dashed'); plt.plot([n_selected,n_selected],[0,100],color='red',zorder=-1)
plt.annotate('Selected Number of Components = '+ str(n_selected),[n_selected,10],rotation=270,color='red')
plt.xlabel('Principal Component'); plt.ylabel('Cumulative Variance Explained'); plt.title('Cumulative Variance Explained by Principal Component')
fmt = '%.0f%%' # Format you want the ticks
plt.xticks(range(1, len(cumulative_variance) + 1))
yticks = mtick.FormatStrFormatter(fmt); plt.xlim(1,len(pca_new.explained_variance_ratio_)); plt.ylim(0,100.0) 
plt.annotate('95% variance explained',[4.05,90]); add_grid()
plt.gca().yaxis.set_major_formatter(PercentFormatter(100.0))  # 1.0 = 100%
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/4810ab4ed9ff9b4c3c7270ea4fce631d.png)

### ä¿å­˜ä¸»æˆåˆ†

ç°åœ¨æˆ‘ä»¬å¯ä»¥é€‰æ‹©å°†å…·æœ‰é™ç»´ä¸»æˆåˆ†å¾—åˆ†çš„ DataFrame å†™å‡ºæ¥ã€‚

```py
save_PCA = True                                        # save the imputed DataFrame?

if save_PCA == True:
    folder = r'C:\Local'
    file_name = r'dataframe_PCA.csv'

    df_new_projected.to_csv(folder + "/" + file_name, index=False) 
```

```py
---------------------------------------------------------------------------
OSError  Traceback (most recent call last)
Cell In[42], line 7
  4 folder = r'C:\Local'
  5 file_name = r'dataframe_PCA.csv'
----> 7 df_new_projected.to_csv(folder + "/" + file_name, index=False)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\core\generic.py:3772, in NDFrame.to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)
  3761 df = self if isinstance(self, ABCDataFrame) else self.to_frame()
  3763 formatter = DataFrameFormatter(
  3764     frame=df,
  3765     header=header,
   (...)
  3769     decimal=decimal,
  3770 )
-> 3772 return DataFrameRenderer(formatter).to_csv(
  3773     path_or_buf,
  3774     lineterminator=lineterminator,
  3775     sep=sep,
  3776     encoding=encoding,
  3777     errors=errors,
  3778     compression=compression,
  3779     quoting=quoting,
  3780     columns=columns,
  3781     index_label=index_label,
  3782     mode=mode,
  3783     chunksize=chunksize,
  3784     quotechar=quotechar,
  3785     date_format=date_format,
  3786     doublequote=doublequote,
  3787     escapechar=escapechar,
  3788     storage_options=storage_options,
  3789 )

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\formats\format.py:1186, in DataFrameRenderer.to_csv(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)
  1165     created_buffer = False
  1167 csv_formatter = CSVFormatter(
  1168     path_or_buf=path_or_buf,
  1169     lineterminator=lineterminator,
   (...)
  1184     formatter=self.fmt,
  1185 )
-> 1186 csv_formatter.save()
  1188 if created_buffer:
  1189     assert isinstance(path_or_buf, StringIO)

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\formats\csvs.py:240, in CSVFormatter.save(self)
  236  """
  237 Create the writer & save.
  238 """
  239 # apply compression and byte/text conversion
--> 240 with get_handle(
  241     self.filepath_or_buffer,
  242     self.mode,
  243     encoding=self.encoding,
  244     errors=self.errors,
  245     compression=self.compression,
  246     storage_options=self.storage_options,
  247 ) as handles:
  248     # Note: self.encoding is irrelevant here
  249     self.writer = csvlib.writer(
  250         handles.handle,
  251         lineterminator=self.lineterminator,
   (...)
  256         quotechar=self.quotechar,
  257     )
  259     self._save()

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:737, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)
  735 # Only for write methods
  736 if "r" not in mode and is_path:
--> 737     check_parent_directory(str(handle))
  739 if compression:
  740     if compression != "zstd":
  741         # compression libraries do not like an explicit text-mode

File C:\ProgramData\anaconda3\envs\MachineLearningBook\lib\site-packages\pandas\io\common.py:600, in check_parent_directory(path)
  598 parent = Path(path).parent
  599 if not parent.is_dir():
--> 600     raise OSError(rf"Cannot save file into a non-existent directory: '{parent}'")

OSError: Cannot save file into a non-existent directory: 'C:\Local' 
```

## è¯„è®º

è¿™æ˜¯å¯¹ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è¿›è¡Œé™ç»´çš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå¾—æ›´å¤šï¼Œä¹Ÿå¯ä»¥è®¨è®ºæ›´å¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„è§†é¢‘è®²åº§é“¾æ¥ã€‚

æˆ‘å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![å›¾ç‰‡](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ˜¯[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œåœ¨[å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡](https://www.utexas.edu/)è¿›è¡Œç ”ç©¶ä¸æ•™å­¦ï¼Œç ”ç©¶å†…å®¹åŒ…æ‹¬åœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚

+   [è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº†è¶…è¿‡ 70 ç¯‡[åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[ç©ºé—´æ•°æ®ç»Ÿè®¡åˆ†æ](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶ä¸”æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[Python ä¸­çš„åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPy å®æˆ˜æŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[Python ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„å®æˆ˜æŒ‡å—](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å« 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ª GitHub ä»“åº“ä¸­çš„è¯¦ç»†æ–‡æ¡£å·¥ä½œæµç¨‹ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›æŒç»­æ›´æ–°çš„å†…å®¹ã€‚æƒ³äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

å¸Œæœ›è¿™ä¸ªå†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œå­¦ä¹ æœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æƒ³è¦åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°æˆ‘ã€‚

æˆ‘æ€»æ˜¯ä¹äºè®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼Œåšå£«ï¼Œå·¥ç¨‹å¸ˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢æ•™æˆ

æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
