# è‡ªç¼–ç å™¨

> åŸæ–‡ï¼š[`geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_autoencoder.html`](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_autoencoder.html)

Michael J. Pyrczï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡æ•™æˆ

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

ç”µå­ä¹¦â€œPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œâ€çš„ç« èŠ‚ã€‚

è¯·å°†æ­¤ç”µå­ä¹¦å¼•ç”¨å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *ã€ŠPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹å†Œã€‹* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138 ![DOI](https://doi.org/10.5281/zenodo.15169138)

æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠæ›´å¤šå†…å®¹å‡å¯åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š

è¯·å°† MachineLearningDemos GitHub ä»“åº“å¼•ç”¨å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, *ã€ŠMachineLearningDemos: Python æœºå™¨å­¦ä¹ æ¼”ç¤ºå·¥ä½œæµç¨‹å­˜å‚¨åº“ã€‹* (0.0.3) [è½¯ä»¶]. Zenodo. DOI: 10.5281/zenodo.13835312\. GitHub ä»“åº“ï¼š[GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos) ![DOI](https://zenodo.org/doi/10.5281/zenodo.13835312)

ç”± Michael J. Pyrcz ç¼–å†™

Â© ç‰ˆæƒæ‰€æœ‰ 2024ã€‚

æœ¬ç« æ˜¯å…³äº/æ¼”ç¤º**è‡ªç¼–ç å™¨**çš„æ•™ç¨‹ã€‚

**YouTube è®²åº§**ï¼šæŸ¥çœ‹æˆ‘åœ¨ä»¥ä¸‹ä¸»é¢˜ä¸Šçš„è®²åº§ï¼š

+   [äººå·¥ç¥ç»ç½‘ç»œ](https://youtu.be/A9PiCMY_6nM?si=NxWSU_5RgQ4w55EL)

+   [å·ç§¯ç¥ç»ç½‘ç»œ](https://youtu.be/za2my_XDoOs?si=LeHU6p2_fc9dX4Yt)

è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘[æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)çš„ä¸€éƒ¨åˆ†ï¼Œåœ¨ YouTube ä¸Šæä¾›äº†æœ‰è‰¯å¥½æ–‡æ¡£è®°å½•çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºç†è§£ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœæ‚¨æƒ³äº†è§£æˆ‘çš„åŠ¨æœºï¼Œè¯·æŸ¥çœ‹[Michael çš„æ•…äº‹](https://michaelpyrcz.com/my-story)ã€‚

## åŠ¨æœº

è‡ªç¼–ç å™¨æ˜¯ä¸€ç§éå¸¸å¼ºå¤§ã€çµæ´»çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå‹ç¼©ä¿¡æ¯ï¼Œ

+   å°†è®­ç»ƒæ•°æ®æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´

+   å°†é«˜ç»´æ•°æ®é™ç»´åˆ°æ›´ä½çš„ç»´åº¦

+   éçº¿æ€§ã€é€šç”¨æ–¹æ³•

## è‡ªç¼–ç å™¨æ¶æ„

è¿™é‡Œæ˜¯æˆ‘ä»¬çš„ç®€å•è‡ªç¼–ç å™¨ï¼Œ

![å›¾ç‰‡](img/ed815fe23f4bd258b278f7aa6f0dd58e.png)

ç®€å•æ¼”ç¤ºè‡ªç¼–ç å™¨ã€‚

è¿™å®é™…ä¸Šæ˜¯[äººå·¥ç¥ç»ç½‘ç»œ](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ANN.html)çš„é•œåƒã€‚

æˆ‘ä¸è®¨è®ºé€šè¿‡ç½‘ç»œçš„æ­£å‘ä¼ é€’ï¼Œå¦‚æœä½ ä¸ç†Ÿæ‚‰è¿™ä¸ªè¿‡ç¨‹ï¼Œä¾‹å¦‚ï¼Œ

+   åœ¨èŠ‚ç‚¹ä¸Šåº”ç”¨æ¿€æ´»åˆ°çº¿æ€§åŠ æƒå’Œåå·®ï¼Œ

ç„¶åï¼Œè¯·æŸ¥é˜…äººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ã€‚

æˆ‘å†³å®šä¸ºæ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨ç‹¬ç‰¹çš„æ•°å€¼ç´¢å¼•ä»¥ç®€æ´åœ°è¡¨ç¤ºè¿æ¥æƒé‡ï¼Œä¾‹å¦‚ $\lambda_{1,4}$ï¼Œä»¥åŠåå·®ï¼Œä¾‹å¦‚ $b_4$ï¼Œ$I$ ç”¨äºè¾“å…¥èŠ‚ç‚¹ï¼Œ$L$ ç”¨äºç¼–ç å™¨éšè—å±‚ï¼ˆâ€œå·¦â€ï¼‰ï¼Œ$M$ ç”¨äºæ½œåœ¨èŠ‚ç‚¹ï¼ˆâ€œä¸­â€ï¼‰ï¼Œ$R$ ç”¨äºè§£ç å™¨éšè—å±‚ï¼ˆâ€œå³â€ï¼‰ï¼Œæœ€å $O$ ç”¨äºè¾“å‡ºèŠ‚ç‚¹ã€‚

è‡ªåŠ¨ç¼–ç å™¨çš„éƒ¨åˆ†å¦‚ä¸‹æ‰€ç¤ºï¼Œ

![](img/652eb880f88b2d046adcc751aa2d62f6.png)

å¸¦æœ‰æ ‡ç­¾éƒ¨åˆ†çš„ç®€å•æ¼”ç¤ºè‡ªåŠ¨ç¼–ç å™¨ã€‚

é€šè¿‡è‡ªåŠ¨ç¼–ç å™¨ä¼ é€’çš„ä¿¡å·åŠå…¶è¡¨ç¤ºåŒ…æ‹¬ï¼Œ

+   **è¾“å…¥** â€“ è®­ç»ƒæ ·æœ¬ï¼Œ

$$ z $$

+   **ç¼–ç å™¨** â€“ å­¦ä¹ å°†è®­ç»ƒæ ·æœ¬å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼Œ

$$ x = f_{\theta} (z) $$

+   **æ½œåœ¨ç©ºé—´** â€“ ç“¶é¢ˆæ€»ç»“äº†è®­ç»ƒæ•°æ®ä¸­çš„æ¨¡å¼ï¼Œ

$$ ğ‘¥ $$

+   **è§£ç å™¨** â€“ å­¦ä¹ å¯¹æ½œåœ¨ç©ºé—´è¿›è¡Œè§£å‹ç¼©ä»¥é‡å»ºåŸå§‹è®­ç»ƒæ•°æ®ï¼Œ

$$ \hat{z} = g_{\phi} (x) = g_{\phi} (f_{\theta}(z) ) $$

+   **é‡å»º** â€“ å°è¯•é‡ç°è¾“å…¥ï¼Œ

$$ \hat{z} \sim z $$

## è®­ç»ƒæ¨¡å‹å‚æ•°

è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨é€šè¿‡ä»¥ä¸‹æ­¥éª¤è¿­ä»£è¿›è¡Œã€‚

![](img/c3a5bc8956f8ceda05ddf9b582cd141d.png)

è®­ç»ƒäººå·¥ç¥ç»ç½‘ç»œé€šè¿‡ä»¥ä¸‹è¿­ä»£è¿‡ç¨‹è¿›è¡Œï¼Œ1. æ­£å‘ä¼ é€’è¿›è¡Œé¢„æµ‹ï¼Œ2. æ ¹æ®é¢„æµ‹å’Œè®­ç»ƒæ•°æ®ä¸­çš„çœŸå®å€¼è®¡ç®—è¯¯å·®å¯¼æ•°ï¼Œ3. å°†è¯¯å·®å¯¼æ•°åå‘ä¼ æ’­é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œä»¥è®¡ç®—æ‰€æœ‰æ¨¡å‹æƒé‡å’Œåå·®å‚æ•°çš„è¯¯å·®å¯¼æ•°ï¼Œ4. æ ¹æ®å¯¼æ•°å’Œå­¦ä¹ ç‡æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œ5. é‡å¤ç›´åˆ°æ”¶æ•›ã€‚

ä¸‹é¢æ˜¯æ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯ï¼Œ

1.  **åˆå§‹åŒ–æ¨¡å‹å‚æ•°** - é€šå¸¸ä½¿ç”¨æ¥è¿‘é›¶çš„å°éšæœºå€¼åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹å‚æ•°ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§æ–¹æ³•ï¼Œ

+   **Xavier æƒé‡åˆå§‹åŒ–** - ä»ç”± $U[\text{min}, \text{max}]$ æŒ‡å®šçš„å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–çš„éšæœºå®ç°ï¼Œ

$$ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}}, \frac{1}{\sqrt{p}} \right] (p^\ell) $$

+   å…¶ä¸­ $F^{-1}_U$ æ˜¯ CDF çš„é€†ï¼Œ$p$ æ˜¯è¾“å…¥æ•°é‡ï¼Œ$p^{\ell}$ æ˜¯ä»å‡åŒ€åˆ†å¸ƒ $U[0,1]$ ä¸­æŠ½å–çš„éšæœºç´¯ç§¯æ¦‚ç‡å€¼ã€‚

+   **å½’ä¸€åŒ– Xavier æƒé‡åˆå§‹åŒ–** - ä»ç”± $U[\text{min}, \text{max}]$ æŒ‡å®šçš„å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–çš„éšæœºå®ç°ï¼Œ

$$ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] (p^\ell) $$

+   å…¶ä¸­ $F^{-1}_U$ æ˜¯ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„é€†ï¼Œ$p$ æ˜¯è¾“å…¥æ•°é‡ï¼Œ$k$ æ˜¯è¾“å‡ºæ•°é‡ï¼Œè€Œ $p^{\ell}$ æ˜¯ä»å‡åŒ€åˆ†å¸ƒ $U[0,1]$ ä¸­æŠ½å–çš„éšæœºç´¯ç§¯æ¦‚ç‡å€¼ã€‚

+   ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªéšè—å±‚èŠ‚ç‚¹ï¼Œ

![](img/b2f8e46ea497049f4b95c03b8812eea7.png)

ç¬¬ä¸€ä¸ªéšè—å±‚èŠ‚ç‚¹æœ‰ 3 ä¸ªè¾“å…¥å’Œ 1 ä¸ªè¾“å‡ºã€‚

+   æˆ‘ä»¬æœ‰ $p = 3$ å’Œ $k = 1$ï¼Œå¹¶ä¸”æˆ‘ä»¬ä»å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–ï¼Œ

$$ U \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] = U \left[ \frac{-1}{\sqrt{3}+1}, \frac{1}{\sqrt{3}+1} \right] $$

1.  **æ­£å‘ä¼ é€’** - å°†è®­ç»ƒæ ·æœ¬ $z$ ä¼ é€’è¿‡å»ï¼Œè®¡ç®—é‡å»º $\hat{z}$ã€‚åˆå§‹é¢„æµ‹åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£å°†æ˜¯éšæœºçš„ï¼Œä½†ä¼šæ”¹è¿›ã€‚

1.  **è®¡ç®—è¯¯å·®å¯¼æ•°** - åŸºäºè¾“å…¥è®­ç»ƒæ ·æœ¬ $z$ å’Œé‡å»º $\hat{z}$ ä¹‹é—´çš„ä¸åŒ¹é…ã€‚

1.  **åå‘ä¼ æ’­è¯¯å·®å¯¼æ•°** - æˆ‘ä»¬é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œåå‘ç§»åŠ¨ä»¥è®¡ç®—æ‰€æœ‰æ¨¡å‹æƒé‡å’Œåç½®å‚æ•°çš„è¯¯å·®å¯¼æ•°ï¼Œä¸ºæ­¤æˆ‘ä»¬ä½¿ç”¨é“¾å¼æ³•åˆ™ï¼Œ

$$ \frac{\partial}{\partial x} f(g(h(x))) = \frac{\partial f}{\partial g} \cdot \frac{\partial g}{\partial h} \cdot \frac{\partial h}{\partial x} $$

1.  **åœ¨æ‰¹æ¬¡ä¸­å¾ªç¯å¹¶å¹³å‡è¯¯å·®å¯¼æ•°** - å¯¹æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰è®­ç»ƒæ•°æ®è¿›è¡Œæ­¥éª¤ 1ï¼Œç„¶åè®¡ç®—è¯¯å·®å¯¼æ•°çš„å¹³å‡å€¼ï¼Œä¾‹å¦‚ï¼Œ

1.  **æ›´æ–°æ¨¡å‹å‚æ•°** - åŸºäºå¯¼æ•° $\frac{\partial P}{\partial \lambda_{i,j}}$ å’Œå­¦ä¹ ç‡ $\eta$ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œ

$$ \lambda_{1,4}^{\ell} = \lambda_{1,4}^{\ell-1} - \eta \cdot \frac{1}{B} \sum_{i=1}^{B} \frac{\partial \mathcal{L}^{(i)}}{\partial \lambda_{1,4}} $$

1.  **é‡å¤ç›´åˆ°æ”¶æ•›** - è¿”å›æ­¥éª¤ 1ï¼Œç›´åˆ°è¯¯å·® $P$ é™ä½åˆ°å¯æ¥å—çš„æ°´å¹³ï¼Œå³æ¨¡å‹æ”¶æ•›æ˜¯åœæ­¢è¿­ä»£çš„æ¡ä»¶

## è‡ªåŠ¨ç¼–ç å™¨æŸå¤±

åœ¨æ¯ä¸ªè¾“å‡º-è¾“å…¥èŠ‚ç‚¹å¯¹ä¸­éƒ½æœ‰ä¸€ä¸ªæŸå¤±å’ŒæŸå¤±æ¢¯åº¦ã€‚è¯¯å·®æŸå¤±å‡½æ•°ï¼Œ

![](img/701ec6c7b420f85dae65e62285e83b13.png)

æ¯ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„è‡ªåŠ¨ç¼–ç å™¨æŸå¤±ï¼Œç›®æ ‡æ˜¯ä½¿è¾“å‡ºä¸è¾“å…¥åŒ¹é…ã€‚

æˆ‘ä»¬å¯ä»¥æ¦‚æ‹¬ä¸ºï¼Œ

$$ L = \frac{1}{2} \sum_{i=1}Â³ \left(O_{i+8} - I_i \right)Â² $$

æ³¨æ„ï¼Œä¸è§„åˆ™çš„ç´¢å¼•æ˜¯ç”±äºæˆ‘é€‰æ‹©åœ¨æ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨å”¯ä¸€çš„èŠ‚ç‚¹ç´¢å¼•ã€‚

æ¯ä¸ªèŠ‚ç‚¹çš„è¯¯å·®å¯¼æ•°æ˜¯ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial O_9} = O_9 - I_1 $$$$ \frac{\partial \mathcal{L}}{\partial O_{10}} = O_{10} - I_2 $$$$ \frac{\partial \mathcal{L}}{\partial O_{11}} = O_{11} - I_3 $$

## è‡ªåŠ¨ç¼–ç å™¨åå‘ä¼ æ’­

è®©æˆ‘ä»¬é€šè¿‡æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨çš„åå‘ä¼ æ’­æ¥é€æ­¥åˆ†æï¼Œè®©æˆ‘ä»¬ä»ä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„åç½®å¼€å§‹ï¼Œ$\frac{\partial \mathcal{L}}{\partial b_{9}}$ã€‚

![](img/8a6b2383ff34c83e1de1a609373cc653.png)

åå‘ä¼ æ’­åˆ°éšè—è§£ç èŠ‚ç‚¹ $ğ‘‚_9$ ä¸­çš„åç½® $ğ‘_9$ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_9} = \frac{\partial O_{9_{\mathrm{in}}}}{\partial b_9} \cdot \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_9} = 1 \cdot 1 \cdot (O_9 - I_1) $$

è®©æˆ‘ä»¬è§£é‡Šæ¯ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬ä»ä¸€ä¸ªè¾“å‡ºæ¢¯åº¦ $\frac{\partial \mathcal{L}}{\partial O_9}$ å¼€å§‹ï¼Œå¹¶è·¨è¿‡è¾“å‡ºèŠ‚ç‚¹ $O_9$ï¼Œå› ä¸ºè¾“å‡ºèŠ‚ç‚¹åº”ç”¨äº†çº¿æ€§æ¿€æ´»ï¼Œ

$$ \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} = 1.0 $$

ç°åœ¨æˆ‘ä»¬å¯ä»¥è®¡ç®—åç½® $b_9$ å…³äºèŠ‚ç‚¹è¾“å…¥çš„å¯¼æ•°ï¼Œ

$$ \frac{\partial 0_{9_{\mathrm{in}}}}{\partial b_9} = \frac{\partial}{\partial b_9} \left( \lambda_{7,9} R_7 + \lambda_{8,9} R_8 + b_9 \right) = 1 $$

ç°åœ¨æˆ‘ä»¬å¯ä»¥ç»§ç»­åˆ°è¿æ¥æƒé‡ï¼Œğœ†_7,9ã€‚

![](img/80eaca0166d0cf02f98e140c090fca18.png)

åå‘ä¼ æ’­åˆ°è¿æ¥æƒé‡ï¼Œ$\lambda_{7,9}$ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial \lambda_{7,9}} = \frac{\partial O_{9_{\mathrm{in}}}}{\partial \lambda_{7,9}} \cdot \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_9} = R_7 \cdot 1 \cdot (O_9 - I_1) $$

å†æ¬¡ï¼Œç”±äºè¾“å‡ºèŠ‚ç‚¹åº”ç”¨äº†çº¿æ€§æ¿€æ´»ï¼Œ

$$ \frac{\partial O_9}{\partial O_{9_{in}}} = 1.0 $$

å¹¶ä¸” $\frac{\partial O^{\text{in}}_9}{\partial \lambda_{7,9}}$ ç®€å•åœ°æ˜¯ $ğ‘…_7$ çš„è¾“å‡ºï¼Œ

$$ \frac{\partial O^{\text{in}}_9}{\partial \lambda_{7,9}} = \frac{\partial}{\partial \lambda_{7,9}} \left( \lambda_{7,9} R_7 + \lambda_{8,9} R_8 + b_9 \right) = R_7 $$

è®©æˆ‘ä»¬ç»§ç»­ä» $\partial \lambda_{7,9}$ åˆ°è§£ç å™¨éšè—èŠ‚ç‚¹ $ğ‘…_7$ çš„è¾“å‡ºï¼Œ

![](img/1c85ce96ca6f0999b7bc167c32d65b89.png)

åå‘ä¼ æ’­åˆ°è§£ç å™¨éšè—å±‚èŠ‚ç‚¹ $R_7$ çš„è¾“å‡ºã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial R_7} = \frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7} \cdot \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_9} + \frac{\partial O_{10_{\mathrm{in}}}}{\partial R_7} \cdot \frac{\partial O_{10}}{\partial O_{10_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_{10}} + \frac{\partial O_{11_{\mathrm{in}}}}{\partial R_7} \cdot \frac{\partial O_{11}}{\partial O_{11_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_{11}} $$

æˆ‘ä»¬å¯ä»¥è¯„ä¼°ä¸ºï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial R_7} = \lambda_{7,9} \cdot 1 \cdot (O_9 - I_1) + \lambda_{7,10} \cdot 1 \cdot (O_{10} - I_2) + \lambda_{7,11} \cdot 1 \cdot (O_{11} - I_3) $$

æˆ‘ä»¬å°†æ¯ä¸ªè¿æ¥çš„å¯¼æ•°ç›¸åŠ ã€‚ç”±äº $ğ‘‚_{9}$ï¼Œ$ğ‘‚_{10}$ å’Œ $ğ‘‚_{11}$ å¤„åº”ç”¨äº†çº¿æ€§æ¿€æ´»ï¼Œ

$$ \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} = 1, \quad \frac{\partial O_{10}}{\partial O_{10_{\mathrm{in}}}} = 1, \quad \frac{\partial O_{11}}{\partial O_{11_{\mathrm{in}}}} = 1 $$

æ­¤å¤–ï¼Œæ²¿ç€è¿æ¥ï¼Œå¯¼æ•°ç®€å•åœ°æ˜¯æƒé‡ï¼Œ

$$ \frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7} = \lambda_{7,9}, \quad \frac{\partial O_{10_{\mathrm{in}}}}{\partial R_7} = \lambda_{7,10}, \quad \frac{\partial O_{11_{\mathrm{in}}}}{\partial R_7} = \lambda_{7,11} $$

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä¸º $\frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7}$ å±•ç¤ºè¿™ä¸€ç‚¹ï¼Œ

$$ \frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7} = \frac{\partial}{\partial R_7} \left( \lambda_{7,9} R_7 + \lambda_{8,9} R_8 + b_9 \right) = \lambda_{7,9} $$

è®©æˆ‘ä»¬ä»æˆ‘ä»¬çš„è§£ç å™¨éšè—å±‚èŠ‚ç‚¹ï¼Œ$ğ‘…_7$ï¼Œçš„è¾“å‡ºç»§ç»­è®¡ç®—èŠ‚ç‚¹åç½® $b_7$ çš„å¯¼æ•°ã€‚

![](img/604e4fcf99d1c41dd899458f80a67179.png)

åå‘ä¼ æ’­åˆ°éšè—è§£ç å™¨èŠ‚ç‚¹ $R_7$ ä¸­çš„åç½®ï¼Œ$b_7$ã€‚

ä»é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_7} = \frac{\partial R_{7_{\mathrm{in}}}}{\partial b_7} \cdot \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial R_7} $$

ç”±äºåœ¨ $R_7$ å¤„çš„ sigmoid æ¿€æ´»ï¼Œè¦è·¨è¿‡èŠ‚ç‚¹ï¼Œ

$$ \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} = \sigma' (R_7) = R_7 (1 - R_7) $$

ä»¥åŠå¯¹äºç»™å®šåç½®çš„èŠ‚ç‚¹è¾“å…¥çš„åå¯¼æ•°ï¼Œ

$$ \frac{R_{7_{\mathrm{in}}}}{\partial b_7} = \frac{\partial}{\partial b_7} \left( \lambda_{6,7} M_6 + b_7 \right) = 1 $$

å› æ­¤ç°åœ¨æˆ‘ä»¬æœ‰ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_7} = 1 \cdot R_7 (1 - R_7) \cdot \overbrace{ \left[ \lambda_{7,9} \cdot 1 \cdot (O_9 - I_1) + \lambda_{7,10} \cdot 1 \cdot (O_{10} - I_2) + \lambda_{7,11} \cdot 1 \cdot (O_{11} - I_3) \right] }^{\frac{\partial L}{\partial R_7}} $$

ç°åœ¨æˆ‘ä»¬å¯ä»¥ç»§ç»­åˆ°è¿æ¥æƒé‡ï¼Œ$\lambda_{6,7}$ã€‚

![](img/1559af01deb817828f382cd89480ff41.png)

åå‘ä¼ æ’­åˆ°è¿æ¥æƒé‡ï¼Œ$\lambda_{6,7}$ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial \lambda_{6,7}} = \frac{\partial R_{7_{\mathrm{in}}}}{\partial \lambda_{6,7}} \cdot \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial R_7} $$

å†æ¬¡ï¼Œç”±äºåœ¨éšè—å±‚èŠ‚ç‚¹ä¸­åº”ç”¨äº† sigmoid æ¿€æ´»ï¼Œ

$$ \frac{\partial R_7}{\partial R_{7_{in}}} = 1.0 $$

å¹¶ä¸” $\frac{\partial R_{7_{\mathrm{in}}}}{\partial \lambda_{6,7}}$ ç®€å•åœ°æ˜¯ $M_6$ çš„è¾“å‡ºï¼Œ

$$ \frac{\partial R_{7_{\mathrm{in}}}}{\partial \lambda_{6,7}} = \frac{\partial}{\partial \lambda_{6,7}} \left( \lambda_{6,7} M_6 + b_6 \right) = M_6 $$

å› æ­¤ç°åœ¨æˆ‘ä»¬æœ‰ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_7} = M_6 \cdot R_7 (1 - R_7) \cdot \overbrace{ \left[ \lambda_{7,9} \cdot 1 \cdot (O_9 - I_1) + \lambda_{7,10} \cdot 1 \cdot (O_{10} - I_2) + \lambda_{7,11} \cdot 1 \cdot (O_{11} - I_3) \right] }^{\frac{\partial \mathcal{L}}{\partial R_7}} $$

è®©æˆ‘ä»¬ç»§ç»­ä»æˆ‘ä»¬çš„æ½œåœ¨èŠ‚ç‚¹ï¼Œğ‘€_6ï¼Œè¾“å‡ºã€‚

![](img/f4cc7dbc1493a36ab0eb828c1422d1f2.png)

åå‘ä¼ æ’­åˆ°æ½œåœ¨èŠ‚ç‚¹çš„è¾“å‡ºï¼Œ$M_6$ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial M_6} = \frac{\partial R_{7_{\mathrm{in}}}}{\partial M_6} \cdot \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial R_7} + \frac{\partial R_{8_{\mathrm{in}}}}{\partial M_6} \cdot \frac{\partial R_8}{\partial R_{8_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial R_8} $$

æˆ‘ä»¬å¯ä»¥å°†å…¶è¡¨ç¤ºä¸ºï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial M_6} = \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} + \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8} $$

ä¸€æ¬¡åˆä¸€æ¬¡ï¼Œç”±äºä½¿ç”¨äº† sigmoid æ¿€æ´»å‡½æ•°ï¼Œ

$$ \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} = R_7 (1 - R_7), \quad \frac{\partial R_8}{\partial R_{8_{\mathrm{in}}}} = R_8 (1 - R_8) $$

å¹¶ä¸”æ²¿ç€è¿æ¥ï¼Œ

$$\begin{split} \begin{aligned} \frac{\partial R_{7_{\mathrm{in}}}}{\partial M_6} &= \frac{\partial}{\partial M_6} \left( \lambda_{6,7} M_6 + b_7 \right) = \lambda_{6,7} \\ \frac{\partial R_{8_{\mathrm{in}}}}{\partial M_6} &= \frac{\partial}{\partial M_6} \left( \lambda_{6,8} M_6 + b_8 \right) = \lambda_{6,8} \end{aligned} \end{split}$$

è®©æˆ‘ä»¬ä»æ½œåœ¨èŠ‚ç‚¹$M_6$çš„è¾“å‡ºç»§ç»­è®¡ç®—èŠ‚ç‚¹åç½®$b_6$çš„å¯¼æ•°ã€‚

![](img/90618005b205c6c5ceb09965c36cf2e1.png)

åœ¨æ½œåœ¨èŠ‚ç‚¹$M_6$ä¸­çš„åç½®$b_6$çš„åå‘ä¼ æ’­ï¼Œæ³¨æ„å›¾åƒå·²ç§»åŠ¨ä»¥è…¾å‡ºç©ºé—´ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial M_{6_{\mathrm{in}}}}{\partial b_6} \cdot \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial M_6} $$

ç”±äºåœ¨$M_6$å¤„åº”ç”¨äº† sigmoid æ¿€æ´»å‡½æ•°ï¼Œä»¥è·¨è¿‡èŠ‚ç‚¹ï¼Œ

$$ \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} = \sigma' (M_6) = M_6 \cdot (1 - M_6) $$

å¹¶ä¸”å¯¹äºç»™å®šåç½®çš„èŠ‚ç‚¹è¾“å…¥çš„åå¯¼æ•°ï¼Œ

$$ \frac{\partial M_{6_{\mathrm{in}}}}{\partial b_6} = \frac{\partial}{\partial b_6} \left( \lambda_{4,6} L_4 + b_6 \right) = 1 $$

æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_6} = 1 \cdot M_6 (1 - M_6) \cdot \overbrace{ \left[ \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} + \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8} \right] }^{\frac{\partial \mathcal{L}}{\partial M_6}} $$

ç°åœ¨æˆ‘ä»¬å¯ä»¥ç»§ç»­åˆ°è¿æ¥æƒé‡ï¼Œ$\lambda_{4,6}$ã€‚

![](img/f5770d05672cfe3c14c6973f2775d2de.png)

å°†åå‘ä¼ æ’­åˆ°è¿æ¥æƒé‡$\lambda_{4,6}$ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial M_{6_{\mathrm{in}}}}{\partial \lambda_{4,6}} \cdot \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial M_6} $$

ä¸€æ¬¡åˆä¸€æ¬¡ï¼Œç”±äºåœ¨éšè—å±‚èŠ‚ç‚¹ä¸­åº”ç”¨äº† sigmoid æ¿€æ´»å‡½æ•°ï¼Œ

$$ \frac{\partial M_6}{\partial M_{6_{in}}} = M_6 \cdot (1 - M_6) $$

å¹¶ä¸” $\frac{\partial M_{6_{\mathrm{in}}}}{\partial \lambda_{4,6}}$ ç®€å•åœ°æ˜¯ $L_4$ çš„è¾“å‡ºï¼Œ

$$ \frac{\partial M_{6_{\mathrm{in}}}}{\partial \lambda_{4,6}} = \frac{\partial}{\partial \lambda_{4,6}} \left( \lambda_{4,6} L_4 + \lambda_{5,6} L_5 + b_6 \right) = L_4 $$

å› æ­¤ç°åœ¨æˆ‘ä»¬æœ‰ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = L_4 \cdot M_6 (1 - M_6) \cdot \overbrace{ \left[ \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} + \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8} \right] }^{\frac{\partial \mathcal{L}}{\partial M_6}} $$

ç°åœ¨æˆ‘ä»¬å¯ä»¥ç»§ç»­åˆ°ç¼–ç å™¨éšè—å±‚èŠ‚ç‚¹çš„è¾“å‡ºï¼Œ$L_4$ã€‚

![](img/1e5148ec01b8276d13a3ac564a201ab3.png)

å‘ç¼–ç å™¨éšè—èŠ‚ç‚¹çš„è¾“å‡ºè¿›è¡Œåå‘ä¼ æ’­ï¼Œ$ğ¿_4$ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°è¿™ä¸ªå¹¶å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial L_4} = \frac{\partial M_{6_{\mathrm{in}}}}{\partial L_4} \cdot \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial M_6} = \lambda_{4,6} \cdot M_6 (1 - M_6) \cdot \frac{\partial \mathcal{L}}{\partial M_6} $$

å†æ¬¡ï¼Œç”±äºåœ¨æ½œåœ¨èŠ‚ç‚¹ä¸­åº”ç”¨äº† sigmoid æ¿€æ´»ï¼Œ

$$ \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} = M_6 (1 - M_6) $$

å¹¶ä¸” $\frac{\partial M_{6_{\mathrm{in}}}}{\partial L_4}$ ç®€å•åœ°æ˜¯æƒé‡ï¼Œ$\lambda_{4,6}$ï¼Œ

$$ \frac{\partial M_{6_{\mathrm{in}}}}{\partial L_4} = \frac{\partial}{\partial L_4} \left( \lambda_{4,6} L_4 + b_6 \right) = \lambda_{4,6} $$

è®©æˆ‘ä»¬ä»ç¼–ç å™¨éšè—å±‚èŠ‚ç‚¹çš„è¾“å‡º $L_4$ å¼€å§‹ï¼Œè®¡ç®—èŠ‚ç‚¹ä¸­åç½® $b_4$ çš„å¯¼æ•°ã€‚

![](img/cf8f925e7a89e3d992b323edfd45034e.png)

å‘ç¼–ç å™¨éšè—å±‚èŠ‚ç‚¹ $L_4$ ä¸­çš„åç½® $b_4$ è¿›è¡Œåå‘ä¼ æ’­ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_4} = \frac{\partial L_{4_{\mathrm{in}}}}{\partial b_4} \cdot \frac{\partial L_4}{\partial L_{4_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial L_4} = 1 \cdot L_4 (1 - L_4) \cdot \frac{\partial \mathcal{L}}{\partial L_4} $$

ç”±äº $M_6$ å¤„çš„ sigmoid æ¿€æ´»ï¼Œè¦ç©¿è¿‡èŠ‚ç‚¹ï¼Œ

$$ \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} = \sigma' (M_6) = M_6 \cdot (1 - M_6) $$

å¯¹äºç»™å®šåç½®çš„èŠ‚ç‚¹è¾“å…¥çš„åå¯¼æ•°ï¼Œ

$$ \frac{\partial M_{6_{\mathrm{in}}}}{\partial b_6} = \frac{\partial}{\partial b_6} \left( \lambda_{4,6} L_4 + b_6 \right) = 1 $$

å› æ­¤ç°åœ¨æˆ‘ä»¬æœ‰ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_6} = 1 \cdot M_6 (1 - M_6) \cdot \overbrace{ \left[ \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} + \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8} \right] }^{\frac{\partial \mathcal{L}}{\partial M_6}} $$

æœ€åï¼Œæˆ‘ä»¬ç»§ç»­åˆ°è¿æ¥æƒé‡ï¼Œ$\lambda_{1,4}$ã€‚

![](img/3623ed192b17eb44b8f6f8c59b1dc0d0.png)

å‘è¿æ¥æƒé‡ $\lambda_{1,4}$ è¿›è¡Œåå‘ä¼ æ’­ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} = \frac{\partial L^{\text{in}}_4}{\partial \lambda_{1,4}} \cdot \frac{\partial L_4}{\partial L^{\text{in}}_4} \cdot \frac{\partial \mathcal{L}}{\partial L_4} $$

å†æ¬¡å¼ºè°ƒï¼Œç”±äºåœ¨éšè—å±‚èŠ‚ç‚¹ä¸­åº”ç”¨äº† sigmoid æ¿€æ´»å‡½æ•°ï¼Œ

$$ \frac{\partial L_4}{\partial L_{4_{in}}} = L_4 \cdot (1 - L_4) $$

å¹¶ä¸” $\frac{\partial L^{\text{in}}_4}{\partial \lambda_{1,4}}$ ç®€å•åœ°æ˜¯ $I_1$ çš„è¾“å‡ºï¼Œ

$$ \frac{\partial L^{\text{in}}_4}{\partial \lambda_{1,4}} = \frac{\partial}{\partial \lambda_{1,4}} \left( \lambda_{1,4} I_1 + \lambda_{2,4} I_2 + \lambda_{3,4} I_3 + b_4 \right) = I_1 $$

å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ï¼Œ

$$ \frac{\partial L}{\partial \lambda_{1,4}} = I_1 \cdot L_4 (1 - L_4) \cdot \underbrace{\left[ \lambda_{4,6} \cdot M_6 (1 - M_6) \cdot \frac{\partial L}{\partial M_6} \right]}_{\frac{\partial L}{\partial L_4}} $$

ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä»…ä½¿ç”¨ NumPy Python åŒ…å’Œ Python å†…ç½®æ•°æ®ç»“æ„å­—å…¸ä»å¤´å¼€å§‹æ„å»ºè¿™ä¸ªè‡ªåŠ¨ç¼–ç å™¨ã€‚

## å¯¼å…¥æ‰€éœ€çš„åŒ…

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»é€šè¿‡ Anaconda 3 å®‰è£…ã€‚

```py
ignore_warnings = True                                        # ignore warnings?
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, AutoLocator) # control of axes ticks
plt.rc('axes', axisbelow=True)                                # set axes and grids in the background for all plots
from scipy.stats import rankdata                              # to assist with plot label placement
from sklearn.linear_model import LinearRegression             # fit the relationship between latent and training data slope 
seed = 13                                                     # random number seed
cmap = plt.cm.tab20                                           # default colormap
plt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements
if ignore_warnings == True:                                   
    import warnings
    warnings.filterwarnings('ignore') 
```

å¦‚æœé‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½éœ€è¦é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£å¹¶è¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æ›´å¤šå¸®åŠ©å¯ä»¥åœ¨ç›¸åº”åŒ…çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ã€‚

## å£°æ˜å‡½æ•°

è¿™é‡Œæä¾›äº†è®­ç»ƒå’Œå¯è§†åŒ–æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨çš„å‡½æ•°ã€‚

```py
def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def xavier(n_in, n_out):                                      # Xavier initializer function
    limit = np.sqrt(6 / (n_in + n_out))
    return np.random.uniform(-limit, limit)

def sigmoid(x):                                               # sigmoid activation
    return 1 / (1 + np.exp(-x))

def initialize_parameters():                                  # initialize all weights and biases and build dictionaries of both
    weights = {                            
        'w14': xavier(3, 2),
        'w24': xavier(3, 2),
        'w34': xavier(3, 2),
        'w15': xavier(3, 2),
        'w25': xavier(3, 2),
        'w35': xavier(3, 2),
        'w46': xavier(2, 1),
        'w56': xavier(2, 1),
        'w67': xavier(1, 2),
        'w68': xavier(1, 2),
        'w79': xavier(2, 3),
        'w89': xavier(2, 3),
        'w710': xavier(2, 3),
        'w810': xavier(2, 3),
        'w711': xavier(2, 3),
        'w811': xavier(2, 3),
    }
    biases = {                                                # biases (one per neuron, excluding input)
        'b4': 0.0,
        'b5': 0.0,
        'b6': 0.0,
        'b7': 0.0,
        'b8': 0.0,
        'b9': 0.0,
        'b10': 0.0,
        'b11': 0.0
    }
    return weights, biases 

def forward_pass(input_vec, weights, biases):                 # forward pass of the autoencoder
    I1, I2, I3 = input_vec.flatten()                               # input nodes (I1, I2, I3)
    z4 = weights['w14'] * I1 + weights['w24'] * I2 + weights['w34'] * I3 + biases['b4'] # encoder
    a4 = sigmoid(z4)

    z5 = weights['w15'] * I1 + weights['w25'] * I2 + weights['w35'] * I3 + biases['b5']
    a5 = sigmoid(z5)

    z6 = weights['w46'] * a4 + weights['w56'] * a5 + biases['b6'] # bottlekneck
    a6 = sigmoid(z6)

    z7 = weights['w67'] * a6 + biases['b7']                   # decoder
    a7 = sigmoid(z7)

    z8 = weights['w68'] * a6 + biases['b8']
    a8 = sigmoid(z8)

    z9 = weights['w79'] * a7 + weights['w89'] * a8 + biases['b9']
    a9 = z9  

    z10 = weights['w710'] * a7 + weights['w810'] * a8 + biases['b10']
    a10 = z10  # linear

    z11 = weights['w711'] * a7 + weights['w811'] * a8 + biases['b11']
    a11 = z11  # linear

    return {                                                  # return all activations as a dictionary
        'I1': I1, 'I2': I2, 'I3': I3,
        'L4': a4, 'L5': a5,
        'M6': a6,
        'R7': a7, 'R8': a8,
        'O9': a9, 'O10': a10, 'O11': a11
    }

def mse_loss_and_derivative(output_vec, input_vec):           # MSE loss and error derivative given output and input
    diff = output_vec - input_vec
    loss = np.mean(diff**2)
    dloss_dout = (2/3) * diff  # shape (3,1)
    return loss, dloss_dout

def sigmoid_derivative(x):                                    # derivative of sigmoid activation
    return x * (1 - x)

def backpropagate(activations, weights, biases, dloss_dout):  # backpropagate the error derivatives
    I1, I2, I3 = activations['I1'], activations['I2'], activations['I3']
    a4, a5 = activations['L4'], activations['L5']
    a6 = activations['M6']
    a7, a8 = activations['R7'], activations['R8']
    O9, O10, O11 = activations['O9'], activations['O10'], activations['O11']

    delta9 = dloss_dout[0, 0]                                 # error terms (delta) for output nodes = dLoss/dOutput
    delta10 = dloss_dout[1, 0]
    delta11 = dloss_dout[2, 0]

    grad_weights = {}                                         # gradients for weights from R7, R8 to O9, O10, O11
    grad_biases = {}

    grad_weights['w79'] = delta9 * a7
    grad_weights['w89'] = delta9 * a8
    grad_weights['w710'] = delta10 * a7
    grad_weights['w810'] = delta10 * a8
    grad_weights['w711'] = delta11 * a7
    grad_weights['w811'] = delta11 * a8

    grad_biases['b9'] = delta9
    grad_biases['b10'] = delta10
    grad_biases['b11'] = delta11

    delta_r7 = (delta9 * weights['w79'] + delta10 * weights['w710'] + delta11 * weights['w711']) * sigmoid_derivative(a7) # gradients for R7 and R8
    delta_r8 = (delta9 * weights['w89'] + delta10 * weights['w810'] + delta11 * weights['w811']) * sigmoid_derivative(a8)

    grad_weights['w67'] = delta_r7 * a6                       # gradients for weights from M6 to R7, R8
    grad_weights['w68'] = delta_r8 * a6

    grad_biases['b7'] = delta_r7
    grad_biases['b8'] = delta_r8

    delta_m6 = (delta_r7 * weights['w67'] + delta_r8 * weights['w68']) * sigmoid_derivative(a6) # backpropagate delta to M6 (sigmoid)

    grad_weights['w46'] = delta_m6 * a4                       # gradients for weights from L4, L5 to M6
    grad_weights['w56'] = delta_m6 * a5

    grad_biases['b6'] = delta_m6

    delta_l4 = delta_m6 * weights['w46'] * sigmoid_derivative(a4) # backpropagate delta to L4, L5 (sigmoid)
    delta_l5 = delta_m6 * weights['w56'] * sigmoid_derivative(a5)

    grad_weights['w14'] = delta_l4 * I1                       # gradients for weights from I1, I2, I3 to L4
    grad_weights['w24'] = delta_l4 * I2
    grad_weights['w34'] = delta_l4 * I3

    grad_biases['b4'] = delta_l4

    grad_weights['w15'] = delta_l5 * I1                       # gradients for weights from I1, I2, I3 to L5
    grad_weights['w25'] = delta_l5 * I2
    grad_weights['w35'] = delta_l5 * I3

    grad_biases['b5'] = delta_l5
    return grad_weights, grad_biases

def update_parameters(weights, biases, grad_weights, grad_biases, learning_rate): # update the weights and biased by derivatives and learning rate
    for key in grad_weights:                                  # update weights
        weights[key] -= learning_rate * grad_weights[key]
    for key in grad_biases:                                   # update biases
        biases[key] -= learning_rate * grad_biases[key]
    return weights, biases 
```

## å¯è§†åŒ–è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œ

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŒ‡å®šè‡ªåŠ¨ç¼–ç å™¨çš„æ ‡ç­¾ã€ä½ç½®ã€è¿æ¥å’Œé¢œè‰²ï¼Œç„¶åç»˜åˆ¶è‡ªåŠ¨ç¼–ç å™¨ã€‚

+   è™½ç„¶è¿™ä¸ªä»£ç æ˜¯é€šç”¨çš„ï¼Œä½†å®é™…çš„è‡ªåŠ¨ç¼–ç å™¨ä»£ç å¹¶æ²¡æœ‰æ¨å¹¿åˆ°ä¸å…¶ä»–æ¶æ„ä¸€èµ·å·¥ä½œï¼Œä¾‹å¦‚æ”¹å˜ç½‘ç»œçš„æ·±åº¦æˆ–å®½åº¦

+   ä»…æ›´æ”¹æ˜¾ç¤ºå‚æ•°ï¼Œä½†ä¸è¦æ›´æ”¹è‡ªåŠ¨ç¼–ç å™¨æ¶æ„

```py
positions = {                                                 # node positions
    'I1': (0, 2), 'I2': (0, 1), 'I3': (0, 0),
    'L4': (1, 1.5), 'L5': (1, 0.5),
    'M6': (2, 1),
    'R7': (3, 1.5), 'R8': (3, 0.5),
    'O9': (4, 2), 'O10': (4, 1), 'O11': (4, 0),
}

node_colors = {                                               # node colors
    'I1': 'white', 'I2': 'white', 'I3': 'white',
    'L4': 'white', 'L5': 'white',
    'M6': 'white',
    'R7': 'white', 'R8': 'white',
    'O9': 'white', 'O10': 'white', 'O11': 'white',
}

edges = [                                                     # edges and weight labels
    ('I1', 'L4', 'lightcoral'), ('I2', 'L4', 'red'), ('I3', 'L4', 'darkred'),
    ('I1', 'L5', 'dodgerblue'), ('I2', 'L5', 'blue'), ('I3', 'L5', 'darkblue'),
    ('L4', 'M6', 'orange'), ('L5', 'M6', 'darkorange'),
    ('M6', 'R7', 'orange'), ('M6', 'R8', 'darkorange'),
    ('R7', 'O9', 'lightcoral'), ('R7', 'O10', 'red'), ('R7', 'O11', 'darkred'),
    ('R8', 'O9', 'dodgerblue'), ('R8', 'O10', 'blue'), ('R8', 'O11', 'darkblue'),
]

weight_labels = { (src, dst,): f"$\\lambda_{{{src[1]}{dst[1:]}}}$" for (src, dst, color) in edges }

bias_offsets = {                                              # bias vector offsets
    'L4': (0.06, 0.12), 'L5': (0.06, 0.12),
    'M6': (0.0, 0.15),
    'R7': (-0.06, 0.12), 'R8': (-0.06, 0.12),
    'O9': (0.0, 0.15), 'O10': (0.0, 0.15), 'O11': (0.0, 0.15),
}

bias_labels = { node: f"$b_{{{node[1:]}}}$" for node in bias_offsets.keys() }
# Plot
fig, ax = plt.subplots(figsize=(11, 6))

custom_weight_offsets = {                                     # custom label offsets for select overlapping weights
    ('I2', 'L4'): (-0.20, 0.0),
    ('I2', 'L5'): (-0.2, 0.20),
    ('R8', 'O9'): (0.15, 0.35),
    ('R8', 'O10'): (0.15, 0.16),
}

for (src, dst, color) in edges:                               # plot edges and weight labels
    x0, y0 = positions[src]
    x1, y1 = positions[dst]
    ax.plot([x0, x1], [y0, y1], color=color, linewidth=1, zorder=1)
    xm, ym = (x0 + x1) / 2, (y0 + y1) / 2
    dx, dy = custom_weight_offsets.get((src, dst), (0, 0.08))
    ax.text(xm + dx, ym + dy, weight_labels[(src, dst)],
            fontsize=9, ha='center', va='center', color = color, zorder=5)

for node, (x, y) in positions.items():                        # white back circles
    ax.scatter(x, y, s=1000, color='white', zorder=2)

for node, (x, y) in positions.items():                        # node circles and labels
    ax.scatter(x, y, s=500, color=node_colors[node], edgecolors='black', zorder=3)
    ax.text(x, y, node, ha='center', va='center', fontsize=9, zorder=4)

for node, (dx, dy) in bias_offsets.items():                   # bias arrows and tighter label placement
    nx, ny = positions[node]
    bx, by = nx + dx, ny + dy
    ax.annotate("", xy=(nx, ny), xytext=(bx, by),
                arrowprops=dict(arrowstyle="->", color='black'), zorder=2)
    ax.text(bx, by, bias_labels[node], ha='right', va='bottom', fontsize=10)

# Final formatting
ax.set_xlim(-0.5, 4.5)
ax.set_ylim(-0.5, 2.7)
ax.axis('off'); plt.tight_layout(); plt.show() 
```

![_images/333249f6a43bbad84e15a2423db3b9cc8670650c55532adfe9fea6ac7c992872.png](img/330a264f2ed0fefaff128fb34a83b1e7.png)

## åˆ›å»ºä¸€ä¸ªæœ‰è¶£çš„åˆæˆæ•°æ®é›†

ç”Ÿæˆä¸€ä¸ª 1D é•¿åº¦ä¸º 3 å‘é‡çš„éšæœºæ•°æ®é›†ï¼Œå…¶æ¨¡å¼å¯ä»¥è¢«æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨æ€»ç»“ã€‚

+   å¦‚æœæˆ‘ä»¬ç”Ÿæˆé•¿åº¦ä¸º 3 çš„éšæœº 1D å‘é‡ï¼Œæˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨å°†æ— æ³•æ€»ç»“ï¼Œå³ï¼Œæ— æ³•ä»åŸå§‹çš„ 3 ä¸ªå€¼ä¸­å‹ç¼©ä¿¡æ¯

+   æˆ‘ä»¬å¿…é¡»åŒ…æ‹¬ä¸€ä¸ªå¯ä»¥è¢«è‡ªåŠ¨ç¼–ç å™¨å­¦ä¹ çš„æ¨¡å¼ï¼Œä»¥é€šè¿‡æ½œåœ¨èŠ‚ç‚¹è§‚å¯Ÿé€šè¿‡è‰¯å¥½çš„æ•°æ®é‡å»ºå®ç°çš„é™ç»´

ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘å·²ç»å°†æ•°æ®é›†ä½œä¸ºä¸€ä¸ªæ··åˆæ¨¡å‹è®¡ç®—ï¼Œçº¿æ€§åŠ å°çš„éšæœºæ®‹å·®ã€‚æ•°æ®ç”Ÿæˆæ­¥éª¤åŒ…æ‹¬ï¼Œ

1.  ç»˜åˆ¶ä¸€ä¸ªéšæœºæ–œç‡ $\sim N\left[-2.0, 2.0 \right]$

1.  åœ¨ä½ç½® $\left[-1, 0, 1 \right]$, $f(\left[-1, 0, 1 \right])$ è®¡ç®—ä¸‰ä¸ªç‚¹

1.  å‘æ¯ä¸ªä½ç½®æ·»åŠ éšæœºã€ç‹¬ç«‹çš„æ®‹å·®ï¼Œ$f(\left[-1, 0, 1 \right]) + N\left[0.0,\sigma \right]$ï¼Œå…¶ä¸­ sigma æ˜¯æ®‹å·®æ ‡å‡†å·®

æ³¨æ„ï¼Œæ–œç‡è¢«ä¿ç•™ä½œä¸ºä¸€ä¸ªæ ‡ç­¾ï¼Œå°†ç”¨äºä¸æ½œåœ¨èŠ‚ç‚¹ $M_6$ è¾“å‡ºè¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ£€æŸ¥ï¼Œæˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ

+   æˆ‘ä»¬çš„å‡è®¾æ˜¯ï¼Œè‡ªåŠ¨ç¼–ç å™¨å°†å­¦ä¹ ä¸€ä¸ªå€¼ï¼Œè¯¥å€¼ç›´æ¥æ˜ å°„åˆ°æ–œç‡ä»¥æè¿°è¿™ä¸ªæ•°æ®é›†ã€‚

+   æ³¨æ„ï¼Œè™½ç„¶è¿™ä¸ªæ ‡ç­¾ç”¨äºå±•ç¤ºè‡ªåŠ¨ç¼–ç å™¨å­¦ä¹ çš„èƒ½åŠ›ï¼Œä½†å®ƒå¹¶æ²¡æœ‰ç”¨äºè®­ç»ƒæ¨¡å‹ï¼

```py
np.random.seed(seed = seed+1)                                 # set random seed
nbatch = 12; nnodes = 3; sigma = 0.1                          # set number of data (total number of data), number of nodes (must be 3), error st.dev.
ymat = np.zeros(nbatch); x = np.arange(1,nnodes+1,1); Xmat = np.zeros([nbatch,nnodes])
data = []
for ibatch in range(0,nbatch):                                # loop over synthetic data
    m = np.random.uniform(low = -2.0, high = 2.0)
    Xmat[ibatch] = (x-2.0)*m + np.random.normal(loc = 0.0, scale=sigma,size=nnodes)
    ymat[ibatch] = np.dot(x, Xmat[ibatch]) / np.dot(x, x)
    data.append(Xmat[ibatch].reshape(3,1))

rank = rankdata(Xmat[:,-1])                                   # rank data to improve (alternate) adjacent labels' locations
plt.subplot(111)                                              # plot the synthetic data
for ibatch in range(0,nbatch):                                
    plt.scatter(Xmat[ibatch],x,color=cmap(ibatch/(nbatch)),edgecolor='black',lw=1,zorder=10)
    plt.plot(Xmat[ibatch],x,color=cmap(ibatch/(nbatch)),lw=2,zorder=1)
    custom_positions = [1,2,3,3.2]
    custom_labels = ['I1','I2','I3','Y']
    if rank[ibatch] % 2 == 0:
        plt.annotate(np.round(ymat[ibatch],2),[Xmat[ibatch][-1],3.18],size=9,color='black',ha='center')
    else:
        plt.annotate(np.round(ymat[ibatch],2),[Xmat[ibatch][-1],3.25],size=9,color='black',ha='center') 
    plt.annotate(ibatch+1,[Xmat[ibatch][0],0.9],size=9,color='black',ha='center')
    plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)
    plt.ylim([3.4,0.8]); plt.xlim([-1.5,1.5]); plt.ylabel('Input Nodes'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Data and Labels')
plt.annotate('Data Index: ',[-1.4,0.9])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/9ce15f05dfa887ce6ee1f02619cb004d.png)

## è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨

æˆ‘ä»¬ä¹‹å‰å·²ç»å®šä¹‰äº†æˆ‘ä»¬è‡ªåŠ¨ç¼–ç å™¨æ‰€éœ€çš„æ‰€æœ‰åŸºæœ¬å‡½æ•°ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°æ¥ç»„åˆæˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨è®­ç»ƒæ­¥éª¤ï¼Œ

1.  **åˆå§‹åŒ–å‚æ•°** - åˆå§‹åŒ–æƒé‡å’Œåå·®

1.  **å‰å‘ä¼ é€’** - é€šè¿‡æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œå‰å‘ä¼ é€’ä»¥è®¡ç®—èŠ‚ç‚¹è¾“å‡ºå’Œæ•°æ®é‡å»º

1.  **å‡æ–¹è¯¯å·®æŸå¤±å’Œå¯¼æ•°** - è®¡ç®—æ¯ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„ L2 æŸå¤±å’Œç›¸å…³è¯¯å·®å¯¼æ•°ï¼Œè¿™äº›èŠ‚ç‚¹æ¥è‡ªè®­ç»ƒæ•°æ®å’Œé‡å»º

1.  **åå‘ä¼ æ’­** - æ ¹æ®è¯¯å·®å¯¼æ•°å’ŒèŠ‚ç‚¹è¾“å‡ºï¼Œé€šè¿‡ç½‘ç»œåå‘ä¼ æ’­è¯¯å·®å¯¼æ•°ï¼Œç„¶ååœ¨æ¯ä¸ªæƒé‡å’Œåå·®ä¸Šå¹³å‡æ¢¯åº¦

1.  **æ›´æ–°å‚æ•°** - ä½¿ç”¨æ‰¹æ¬¡çš„å¹³å‡æ¢¯åº¦å’Œå­¦ä¹ ç‡æ›´æ–°æƒé‡å’Œåå·®

1.  è¿›è¡Œåˆ° 2 ç›´åˆ°æ”¶æ•›ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯ä¸€ä¸ªå›ºå®šçš„è®­ç»ƒå‘¨æœŸæ•°

```py
epochs = 10000                                                # set hyperparameters
batch_size = nbatch
learning_rate = 0.1
seed = 13
np.random.seed(seed=seed)

output_mat = np.zeros((batch_size,epochs,3)); loss_mat = np.zeros((epochs)); M6_mat = np.zeros((batch_size,epochs))

weights, biases = initialize_parameters()                     # initialize weights and biases

for epoch in range(epochs):
    sum_grad_w = {k: 0 for k in weights.keys()}               # initialize zero dictionary to average backpropogated gradients
    sum_grad_b = {k: 0 for k in biases.keys()}
    epoch_loss = 0
    for idata,input_vec in enumerate(data):
        activations = forward_pass(input_vec, weights, biases) # forward pass
        M6_mat[idata,epoch] = activations['M6']
        output_vec = np.array([[activations['O9']], [activations['O10']], [activations['O11']]])
        output_mat[idata,epoch,:] = output_vec.reshape(3)
        loss, dloss_dout = mse_loss_and_derivative(output_vec, input_vec) # compute loss and derivative
        epoch_loss += loss
        grad_w, grad_b = backpropagate(activations, weights, biases, dloss_dout) # backpropagation the derivative
        for k in grad_w:                                      # accumulate gradients
            sum_grad_w[k] += grad_w[k]
        for k in grad_b:
            sum_grad_b[k] += grad_b[k]
    avg_grad_w = {k: v / batch_size for k, v in sum_grad_w.items()} # average gradients over batch
    avg_grad_b = {k: v / batch_size for k, v in sum_grad_b.items()}
    epoch_loss /= batch_size
    loss_mat[epoch] = epoch_loss
    weights, biases = update_parameters(weights, biases, avg_grad_w, avg_grad_b, learning_rate) # update parameters
    # if epoch % 500 == 0:                                    # print loss every 100 training epochs
    #     print(f"Epoch {epoch}, Loss: {epoch_loss:.6f}")

plt.subplot(111)                                              # plot training error vs. training epoch
plt.plot(np.arange(0,epoch+1,1),loss_mat,color='red',label=r'MSE'); plt.xlim([1,epoch]); plt.ylim([0,1])
plt.xlabel('Epochs'); plt.ylabel(r'Mean Square Error (L2 loss)'); plt.title('Autoencoder Average Batch L2 Loss vs. Training Epoch')
add_grid(); plt.legend(loc='upper right'); plt.xscale('linear')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/3ab1c8fef6098b7c75943615555e53e5.png)

å¹³å‡ L2 æŸå¤±ä¸è®­ç»ƒå‘¨æœŸæ›²çº¿çœ‹èµ·æ¥éå¸¸å¥½ã€‚

+   æˆ‘ä»¬çœ‹åˆ°å­¦ä¹ æš‚åœï¼Œç„¶åçªç„¶è®­ç»ƒè¯¯å·®å¿«é€Ÿå‡å°‘ï¼Œç„¶åç¼“æ…¢æ”¶æ•›

+   æˆ‘ä¸ºäº†æ•ˆç‡åœåœ¨äº† 10,000 ä¸ªè®­ç»ƒå‘¨æœŸå¤„

## è¯„ä¼°æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œ

è®©æˆ‘ä»¬çœ‹çœ‹ç½‘ç»œç“¶é¢ˆå¤„çš„æ½œåœ¨èŠ‚ç‚¹è¾“å‡ºï¼Œå³èŠ‚ç‚¹ M6 çš„è¾“å‡ºã€‚

+   æ³¨æ„ï¼Œæˆ‘ä»¬è®°å½•äº†æ‰€æœ‰è®­ç»ƒå‘¨æœŸå’Œæ‰€æœ‰æ•°æ®çš„ M6 è¾“å‡ºï¼ˆç§°ä¸ºèŠ‚ç‚¹æ¿€æ´»ï¼‰ã€‚

+   è®©æˆ‘ä»¬æŸ¥çœ‹æœ€ç»ˆè®­ç»ƒå¥½çš„ç½‘ç»œï¼Œæœ€åä¸€ä¸ªè®­ç»ƒå‘¨æœŸï¼Œå¹¶éå†æ‰€æœ‰æ•°æ®

è¿™é‡Œæ˜¯æœ€ç»ˆè®­ç»ƒå‘¨æœŸ M6 è¾“å‡ºä¸æ ·æœ¬æ–œç‡çš„å¯¹æ¯”å›¾ï¼Œ

```py
linear_model = LinearRegression().fit(ymat.reshape(-1, 1), M6_mat[:,-1]) # fit linear model to regress latent on training data slope

plt.subplot(111)                                              # plot latent vs. training data slope
plt.plot(np.linspace(-0.4,0.4,100),linear_model.predict(np.linspace(-0.4,0.4,100).reshape(-1,1)),color='red',zorder=-1)
for ibatch,input_vec in enumerate(data):                      # plot and label training data
    plt.scatter(ymat[ibatch],M6_mat[ibatch,-1],color=cmap(ibatch/(nbatch)),edgecolor='black',marker='o',s=30,zorder=10)
    plt.annotate(ibatch+1,[ymat[ibatch]-0.01,M6_mat[ibatch,-1]+0.01],size=9,color='black',ha='center',zorder=100) 
plt.ylabel('M6 Output'); plt.xlabel(r'Sample Slope, $m_i$'); plt.title('Latent Node Output vs. Sample Slope')
plt.ylim([0.1,0.8]); plt.xlim([-0.4,0.4]); add_grid()
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/7815f0d074113f20a6f77a446f1f83d2.png)

å¦‚é¢„æœŸï¼Œæˆ‘ä»¬çš„ç½‘ç»œç“¶é¢ˆå¤„çš„æ½œåœ¨èŠ‚ç‚¹è¾“å‡ºä¸ç”¨äºç”Ÿæˆæ•°æ®çš„æ ·æœ¬æ–œç‡ä¹‹é—´å­˜åœ¨è‰¯å¥½çš„å…³ç³»ï¼

+   æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨å·²ç»å­¦ä¹ äº†ä¸€ä¸ªå€¼æ¥è¡¨ç¤ºæ•°æ®é›†ä¸­ 3 ä¸ªå€¼çš„å‘é‡ï¼

+   è¿™æ˜¯å¯¹ä¿¡æ¯å‹ç¼©çš„ç»ä½³å±•ç¤ºï¼Œ3:1ï¼

## æ£€æŸ¥è®­ç»ƒæ•°æ®é‡å»º

è®©æˆ‘ä»¬å¯è§†åŒ–ä½¿ç”¨æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œé‡å»ºçš„ 1D æ•°æ®ï¼Œç¼–ç åå†è§£ç ã€‚

+   å¯¹äºæ‰€æœ‰è®­ç»ƒæ•°æ®ï¼Œæˆ‘åŒ…æ‹¬åŸå§‹æ•°æ®å’Œé‡å»ºæ•°æ®ï¼Œå³ç”±æˆ‘ä»¬è®­ç»ƒå¥½çš„è‡ªåŠ¨ç¼–ç å™¨ç¼–ç å’Œè§£ç çš„æ•°æ®

+   å¯¹äºæ¯ä¸ªæ•°æ®è®­ç»ƒæ ·æœ¬ï¼Œæˆ‘åŒ…æ‹¬æ ·æœ¬æ–œç‡ä»¥ä¾›å‚è€ƒï¼Œä½†è¿™ä¸ªæ ‡ç­¾åœ¨è®­ç»ƒä¸­ã€ç¼–ç å™¨æˆ–è§£ç å™¨ä¸­éƒ½æ²¡æœ‰ä½¿ç”¨

```py
for idata,input_vec in enumerate(data):                       # plot training data and reconstructions 
    plt.subplot(4,3,idata+1)
    plt.scatter(Xmat[idata],x,color=cmap(idata/(nbatch+2)),edgecolor='black',lw=1,zorder=10)
    plt.plot(Xmat[idata],x,lw=1,zorder=1,color=cmap(idata/(nbatch+2)),label='data')
    custom_positions = [1,2,3,3.2]
    custom_labels = ['I1','I2','I3','Y']
    plt.annotate(np.round(ymat[idata],2),[Xmat[idata][-1],3.25],size=9,color='black',ha='center')  
    plt.scatter(output_mat[idata,-1,:],x,lw=1,color=cmap(idata/(nbatch+2)))
    plt.plot(output_mat[idata,-1,:],x,lw=1,ls='--',color=cmap(idata/(nbatch+2)),label='reconstruction')
    plt.legend(loc='upper left')
    plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)
    plt.ylim([3.5,0.8]); plt.xlim([-2.5,2.5]); plt.ylabel('index'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Training Data #' + str(idata+1))

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=4.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/7203aaa35d3b4fe06560d8885fa0bc78.png)

è®­ç»ƒæ•°æ®é‡å»ºç›¸å½“ä¸é”™ï¼

+   æˆ‘ä»¬çš„è‡ªç¼–ç å™¨å·²ç»å­¦ä¼šäº†ç¼–ç å’Œè§£ç è®­ç»ƒæ•°æ®

+   å±•ç¤ºäº†ä» 3 ç»´åˆ° 1 ç»´çš„è‰¯å¥½é™ç»´æ•ˆæœï¼

## æ£€æŸ¥æµ‹è¯•æ•°æ®é‡å»º

è®©æˆ‘ä»¬ç”Ÿæˆæ›´å¤šæ•°æ®å¹¶æµ‹è¯•é‡å»ºæ•ˆæœã€‚

+   ä½¿ç”¨æœªç”¨äºè®­ç»ƒè‡ªç¼–ç å™¨çš„æ•°æ®è¿›è¡Œæ€§èƒ½æ£€æŸ¥ï¼Œè¿™è¢«ç§°ä¸ºæ¨¡å‹æ³›åŒ–

```py
np.random.seed(seed = seed+7)
nbatch_test = 12; nnodes = 3; sigma = 0.1
ymat_test = np.zeros(nbatch); x = np.arange(1,nnodes+1,1); Xmat_test = np.zeros([nbatch,nnodes])
data_test = []
for ibatch in range(0,nbatch):
    m = np.random.uniform(low = -2.0, high = 2.0)
    Xmat_test[ibatch] = (x-2.0)*m + np.random.normal(loc = 0.0, scale=sigma,size=nnodes)
    ymat_test[ibatch] = np.dot(x, Xmat_test[ibatch]) / np.dot(x, x)
    data_test.append(Xmat_test[ibatch].reshape(3,1))

rank = rankdata(Xmat_test[:,-1])
plt.subplot(111)
for ibatch in range(0,nbatch_test):
    plt.scatter(Xmat_test[ibatch],x,color=cmap(ibatch/(nbatch)),edgecolor='black',lw=1,zorder=10)
    plt.plot(Xmat_test[ibatch],x,color=cmap(ibatch/(nbatch)),lw=2,zorder=1)
    custom_positions = [1,2,3,3.2]
    custom_labels = ['I1','I2','I3','Y']
    if rank[ibatch] % 2 == 0:
        plt.annotate(np.round(ymat_test[ibatch],2),[Xmat_test[ibatch][-1],3.18],size=9,color='black',ha='center')
    else:
        plt.annotate(np.round(ymat_test[ibatch],2),[Xmat_test[ibatch][-1],3.25],size=9,color='black',ha='center') 
    plt.annotate(ibatch+13,[Xmat_test[ibatch][0],0.9],size=9,color='black',ha='center')
    plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)
    plt.ylim([3.4,0.8]); plt.xlim([-1.5,1.5]); plt.ylabel('Input Nodes'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Data and Labels')
plt.annotate('Test Data Index: ',[-1.45,0.9])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/28e4aff8d55696fd326194c3a79007d1.png)

å°†è®­ç»ƒå¥½çš„è‡ªç¼–ç å™¨åº”ç”¨äºé‡å»ºæµ‹è¯•æ•°æ®ã€‚

```py
output_vec_test = np.zeros((len(data_test),3))
for idata_test,input_vec_test in enumerate(data_test):
    activations = forward_pass(input_vec_test, weights, biases)                                                    # forward pass
    output_vec_test[idata_test,:] = np.array([[activations['O9']], [activations['O10']], [activations['O11']]]).reshape(-1) 
```

ç°åœ¨å¯è§†åŒ–æµ‹è¯•æ•°æ®é‡å»ºï¼Œ

```py
for idata,input_vec_test in enumerate(data_test):
    plt.subplot(4,3,idata+1)
    plt.scatter(input_vec_test,x,color=cmap(idata/(nbatch)),edgecolor='black',lw=1,zorder=10)
    plt.plot(input_vec_test,x,lw=1,zorder=1,color=cmap(idata/(nbatch)),label='data')
    custom_positions = [1,2,3,3.2]
    custom_labels = ['I1','I2','I3','Y']
    # plt.annotate(np.round(ymat[idata],2),[Xmat[idata][-1],3.25],size=8,color='black',ha='center') 
    plt.scatter(output_vec_test[idata,:],x,lw=1,color=cmap(idata/(nbatch)))
    plt.plot(output_vec_test[idata,:],x,lw=1,ls='--',color=cmap(idata/(nbatch)),label='reconstruction')
    plt.legend(loc='upper left'); plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)
    plt.ylim([3.5,0.8]); plt.xlim([-1.5,1.5]); plt.ylabel('index'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Test Image #' + str(idata+13))

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=4.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/abda6b15ea724a35119dc1c5cf9554e9.png)

æˆ‘ä»¬è®­ç»ƒå¥½çš„è‡ªç¼–ç å™¨ä¼¼ä¹æ³›åŒ–å¾—å¾ˆå¥½ï¼Œåœ¨é‡å»ºè®­ç»ƒæ•°æ®å’Œä¿ç•™çš„æµ‹è¯•æ¡ˆä¾‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚

+   ä¸ºäº†æ›´å®Œæ•´çš„æµç¨‹ï¼Œæˆ‘ä»¬å°†åœ¨è®­ç»ƒå‘¨æœŸå†…å¹¶è¡Œè¯„ä¼°è®­ç»ƒå’Œæµ‹è¯•é”™è¯¯ï¼Œä»¥æ£€æŸ¥æ¨¡å‹è¿‡æ‹Ÿåˆã€‚

+   æˆ‘å°†è¿™äº›ç»„ä»¶åˆ†å¼€ï¼Œä»¥ä¾¿åœ¨æ¼”ç¤ºä¸­æ›´ç®€æ´ã€æ›´æ¸…æ™°

## è¯„è®º

è¿™æ˜¯å¯¹è‡ªç¼–ç å™¨æ·±åº¦å­¦ä¹ ç½‘ç»œçš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[èµ„æºå…±äº«æ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„ YouTube è®²åº§é“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![å›¾ç‰‡](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

åœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡çš„ 40 è‹±äº©æ ¡å›­å†…ï¼Œè¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆçš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”å¥‡æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œä»–åœ¨[å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡](https://www.utexas.edu/)ä»äº‹å’Œæ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚

+   [è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº† 70 å¤šç¯‡[åŒè¡Œè¯„å®¡çš„å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[åœ°çƒç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æ–°å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[Python ä¸­åº”ç”¨åœ°çƒç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ï¼šä»£ç å®è·µæŒ‡å—](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œé™„æœ‰ 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†å·¥ä½œæµç¨‹é“¾æ¥ï¼Œè¿™äº›å­˜å‚¨åº“ä½äºä»–çš„[GitHub è´¦æˆ·](https://github.com/GeostatsGuy)ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›å¸¸é’å†…å®¹ã€‚äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™ä¸ªå†…å®¹å¯¹é‚£äº›æƒ³è¦äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æä»¥åŠæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚ä¸ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢

æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹ä½ç½®è·å–ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°çƒç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [Python ä¸­åº”ç”¨åœ°çƒç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## åŠ¨æœº

è‡ªåŠ¨ç¼–ç å™¨æ˜¯ä¸€ç§éå¸¸å¼ºå¤§ã€çµæ´»çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå‹ç¼©ä¿¡æ¯ï¼Œ

+   å°†è®­ç»ƒæ•°æ®æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´

+   å°†é«˜ç»´æ•°æ®é™ç»´åˆ°æ›´ä½çš„ç»´åº¦

+   éçº¿æ€§ï¼Œé€šç”¨æ–¹æ³•

## è‡ªåŠ¨ç¼–ç å™¨æ¶æ„

è¿™é‡Œæ˜¯æˆ‘ä»¬çš„ç®€å•è‡ªåŠ¨ç¼–ç å™¨ï¼Œ

![](img/ed815fe23f4bd258b278f7aa6f0dd58e.png)

ç®€å•æ¼”ç¤ºè‡ªåŠ¨ç¼–ç å™¨ã€‚

è¿™å®é™…ä¸Šæ˜¯æ¥è‡ª [äººå·¥ç¥ç»ç½‘ç»œ](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ANN.html) çš„é•œåƒã€‚

æˆ‘ä¸ä¼šè®¨è®ºé€šè¿‡ç½‘ç»œçš„æ­£å‘ä¼ é€’ï¼Œå¦‚æœä½ ä¸ç†Ÿæ‚‰è¿™ä¸ªè¿‡ç¨‹ï¼Œä¾‹å¦‚ï¼Œ

+   åº”ç”¨åˆ°èŠ‚ç‚¹çº¿æ€§åŠ æƒå’Œåç½®ä¸Šçš„æ¿€æ´»å‡½æ•°

ç„¶åè¯·å›é¡¾äººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ã€‚

æˆ‘å†³å®šä¸ºæ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨ç‹¬ç‰¹çš„æ•°å€¼ç´¢å¼•ï¼Œä»¥ä¾¿äºç®€æ´åœ°è¡¨ç¤ºè¿æ¥æƒé‡ï¼Œä¾‹å¦‚ $\lambda_{1,4}$ï¼Œä»¥åŠåç½®ï¼Œä¾‹å¦‚ $b_4$ï¼Œ$I$ ç”¨äºè¾“å…¥èŠ‚ç‚¹ï¼Œ$L$ ç”¨äºç¼–ç å™¨éšè—å±‚ï¼ˆâ€˜å·¦â€™ï¼‰ï¼Œ$M$ ç”¨äºæ½œåœ¨èŠ‚ç‚¹ï¼ˆâ€˜ä¸­é—´â€™ï¼‰ï¼Œ$R$ ç”¨äºè§£ç å™¨éšè—å±‚ï¼ˆâ€˜å³â€™ï¼‰ï¼Œæœ€å $O$ ç”¨äºè¾“å‡ºèŠ‚ç‚¹ã€‚

è‡ªåŠ¨ç¼–ç å™¨çš„å„ä¸ªéƒ¨åˆ†å¦‚ä¸‹æ‰€ç¤ºï¼Œ

![](img/652eb880f88b2d046adcc751aa2d62f6.png)

å¸¦æœ‰éƒ¨åˆ†æ ‡ç­¾çš„ç®€å•æ¼”ç¤ºè‡ªåŠ¨ç¼–ç å™¨ã€‚

é€šè¿‡è‡ªåŠ¨ç¼–ç å™¨ä¼ é€’çš„ä¿¡å·åŠå…¶è¡¨ç¤ºåŒ…æ‹¬ï¼Œ

+   **è¾“å…¥** â€“ è®­ç»ƒæ ·æœ¬ï¼Œ

$$ z $$

+   **ç¼–ç å™¨** â€“ å°†è®­ç»ƒæ ·æœ¬å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´çš„å­¦ä¹ å‹ç¼©ï¼Œ

$$ x = f_{\theta} (z) $$

+   **æ½œåœ¨ç©ºé—´** â€“ çª„é¢ˆéƒ¨åˆ†æ€»ç»“äº†è®­ç»ƒæ•°æ®ä¸­çš„æ¨¡å¼ï¼Œ

$$ ğ‘¥ $$

+   **è§£ç å™¨** â€“ å°†æ½œåœ¨ç©ºé—´è§£å‹ç¼©ä¸ºåŸå§‹è®­ç»ƒæ•°æ®çš„é‡å»ºï¼Œ

$$ \hat{z} = g_{\phi} (x) = g_{\phi} (f_{\theta}(z) ) $$

+   é‡å»º â€“ å°è¯•é‡ç°è¾“å…¥ï¼Œ

$$ \hat{z} \sim z $$

## è®­ç»ƒæ¨¡å‹å‚æ•°

è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨é€šè¿‡ä»¥ä¸‹æ­¥éª¤è¿­ä»£è¿›è¡Œã€‚

![](img/c3a5bc8956f8ceda05ddf9b582cd141d.png)

è®­ç»ƒäººå·¥ç¥ç»ç½‘ç»œé€šè¿‡ä»¥ä¸‹è¿­ä»£è¿‡ç¨‹è¿›è¡Œï¼Œ1. å‰å‘ä¼ é€’ä»¥è¿›è¡Œé¢„æµ‹ï¼Œ2. æ ¹æ®é¢„æµ‹å’Œè®­ç»ƒæ•°æ®ä¸­çš„çœŸå®å€¼è®¡ç®—è¯¯å·®å¯¼æ•°ï¼Œ3. å°†è¯¯å·®å¯¼æ•°åå‘ä¼ æ’­é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œä»¥è®¡ç®—æ‰€æœ‰æ¨¡å‹æƒé‡å’Œåç½®å‚æ•°çš„è¯¯å·®å¯¼æ•°ï¼Œ4. æ ¹æ®å¯¼æ•°å’Œå­¦ä¹ ç‡æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œ5. é‡å¤ç›´åˆ°æ”¶æ•›ã€‚

è¿™é‡Œæ˜¯æ¯ä¸ªæ­¥éª¤çš„ä¸€äº›ç»†èŠ‚ï¼Œ

1.  **åˆå§‹åŒ–æ¨¡å‹å‚æ•°** - é€šå¸¸ä½¿ç”¨æ¥è¿‘é›¶çš„å°éšæœºå€¼åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹å‚æ•°ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§æ–¹æ³•ï¼Œ

+   **Xavier æƒé‡åˆå§‹åŒ–** - ç”± $U[\text{min}, \text{max}]$ æŒ‡å®šçš„å‡åŒ€åˆ†å¸ƒçš„éšæœºå®ç°ï¼Œ

$$ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}}, \frac{1}{\sqrt{p}} \right] (p^\ell) $$

+   å…¶ä¸­ $F^{-1}_U$ æ˜¯ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„é€†ï¼Œ$p$ æ˜¯è¾“å…¥çš„æ•°é‡ï¼Œè€Œ $p^{\ell}$ æ˜¯ä»å‡åŒ€åˆ†å¸ƒ $U[0,1]$ ä¸­æŠ½å–çš„éšæœºç´¯ç§¯æ¦‚ç‡å€¼ã€‚

+   **å½’ä¸€åŒ– Xavier æƒé‡åˆå§‹åŒ–** - ä»ç”± $U[\text{min}, \text{max}]$ æŒ‡å®šçš„å‡åŒ€åˆ†å¸ƒä¸­éšæœºå®ç°ï¼Œ

$$ \lambda_{i,j} = F_U^{-1} \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] (p^\ell) $$

+   å…¶ä¸­ $F^{-1}_U$ æ˜¯ CDF çš„é€†ï¼Œ$p$ æ˜¯è¾“å…¥æ•°é‡ï¼Œ$k$ æ˜¯è¾“å‡ºæ•°é‡ï¼Œ$p^{\ell}$ æ˜¯ä»å‡åŒ€åˆ†å¸ƒ $U[0,1]$ ä¸­æŠ½å–çš„éšæœºç´¯ç§¯æ¦‚ç‡å€¼ã€‚

+   ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªéšè—å±‚èŠ‚ç‚¹ï¼Œ

![](img/b2f8e46ea497049f4b95c03b8812eea7.png)

ç¬¬ä¸€ä¸ªéšè—å±‚èŠ‚ç‚¹æœ‰ 3 ä¸ªè¾“å…¥ï¼Œ1 ä¸ªè¾“å‡ºã€‚

+   æˆ‘ä»¬æœ‰ $p = 3$ å’Œ $k = 1$ï¼Œå¹¶ä»å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–ï¼Œ

$$ U \left[ \frac{-1}{\sqrt{p}+k}, \frac{1}{\sqrt{p}+k} \right] = U \left[ \frac{-1}{\sqrt{3}+1}, \frac{1}{\sqrt{3}+1} \right] $$

1.  **æ­£å‘ä¼ æ’­** - å°†è®­ç»ƒæ ·æœ¬ $z$ ä¼ é€’è¿‡å»ï¼Œè®¡ç®—é‡å»º $\hat{z}$ã€‚åˆå§‹é¢„æµ‹åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£å°†æ˜¯éšæœºçš„ï¼Œä½†ä¼šæ”¹è¿›ã€‚

1.  **è®¡ç®—è¯¯å·®å¯¼æ•°** - åŸºäºè¾“å…¥è®­ç»ƒæ ·æœ¬ $z$ å’Œé‡å»º $\hat{z}$ ä¹‹é—´çš„ä¸åŒ¹é…ã€‚

1.  **åå‘ä¼ æ’­è¯¯å·®å¯¼æ•°** - æˆ‘ä»¬é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œå›æº¯ä»¥è®¡ç®—æ‰€æœ‰æ¨¡å‹æƒé‡å’Œåå·®å‚æ•°çš„è¯¯å·®å¯¼æ•°ï¼Œä¸ºæ­¤æˆ‘ä»¬ä½¿ç”¨é“¾å¼æ³•åˆ™ï¼Œ

$$ \frac{\partial}{\partial x} f(g(h(x))) = \frac{\partial f}{\partial g} \cdot \frac{\partial g}{\partial h} \cdot \frac{\partial h}{\partial x} $$

1.  **éå†æ‰¹é‡å¹¶å¹³å‡è¯¯å·®å¯¼æ•°** - å¯¹æ‰¹é‡ä¸­çš„æ‰€æœ‰è®­ç»ƒæ•°æ®è¿›è¡Œæ­¥éª¤ 1ï¼Œç„¶åè®¡ç®—è¯¯å·®å¯¼æ•°çš„å¹³å‡å€¼ï¼Œä¾‹å¦‚ï¼Œ

1.  **æ ¹æ®å¯¼æ•°å’Œå­¦ä¹ ç‡æ›´æ–°æ¨¡å‹å‚æ•°** - å¦‚æ­¤ï¼Œ

$$ \lambda_{1,4}^{\ell} = \lambda_{1,4}^{\ell-1} - \eta \cdot \frac{1}{B} \sum_{i=1}^{B} \frac{\partial \mathcal{L}^{(i)}}{\partial \lambda_{1,4}} $$

1.  **é‡å¤ç›´åˆ°æ”¶æ•›** - è¿”å›æ­¥éª¤ 1ï¼Œç›´åˆ°è¯¯å·® $P$ é™ä½åˆ°å¯æ¥å—çš„æ°´å¹³ï¼Œå³æ¨¡å‹æ”¶æ•›æ˜¯åœæ­¢è¿­ä»£çš„æ¡ä»¶

## è‡ªç¼–ç å™¨æŸå¤±

åœ¨æ¯ä¸ªè¾“å‡º-è¾“å…¥èŠ‚ç‚¹å¯¹ä¸­éƒ½æœ‰ä¸€ä¸ªæŸå¤±å’ŒæŸå¤±æ¢¯åº¦ã€‚è¯¯å·®æŸå¤±å‡½æ•°ï¼Œ

![](img/701ec6c7b420f85dae65e62285e83b13.png)

æ¯ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„è‡ªç¼–ç å™¨æŸå¤±ï¼Œç›®æ ‡æ˜¯ä½¿è¾“å‡ºä¸è¾“å…¥åŒ¹é…ã€‚

æˆ‘ä»¬å¯ä»¥æ¦‚æ‹¬ä¸ºï¼Œ

$$ L = \frac{1}{2} \sum_{i=1}Â³ \left(O_{i+8} - I_i \right)Â² $$

æ³¨æ„ï¼Œä¸è§„åˆ™çš„ç´¢å¼•æ˜¯ç”±äºæˆ‘é€‰æ‹©åœ¨æ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨å”¯ä¸€çš„èŠ‚ç‚¹ç´¢å¼•ã€‚

æ¯ä¸ªèŠ‚ç‚¹çš„è¯¯å·®å¯¼æ•°æ˜¯ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial O_9} = O_9 - I_1 $$$$ \frac{\partial \mathcal{L}}{\partial O_{10}} = O_{10} - I_2 $$$$ \frac{\partial \mathcal{L}}{\partial O_{11}} = O_{11} - I_3 $$

## è‡ªç¼–ç å™¨åå‘ä¼ æ’­

è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹æˆ‘ä»¬çš„è‡ªç¼–ç å™¨çš„åå‘ä¼ æ’­ï¼Œè®©æˆ‘ä»¬ä»ä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„åå·®å¼€å§‹ï¼Œ$\frac{\partial \mathcal{L}}{\partial b_{9}}$ã€‚

![](img/8a6b2383ff34c83e1de1a609373cc653.png)

å‘éšè—è§£ç èŠ‚ç‚¹ $ğ‘‚_9$ ä¸­çš„åå·® $ğ‘_9$ åå‘ä¼ æ’­ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_9} = \frac{\partial O_{9_{\mathrm{in}}}}{\partial b_9} \cdot \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_9} = 1 \cdot 1 \cdot (O_9 - I_1) $$

è®©æˆ‘ä»¬è§£é‡Šæ¯ä¸ªéƒ¨åˆ†ã€‚æˆ‘ä»¬é¦–å…ˆä»è¾“å‡ºæ¢¯åº¦ $\frac{\partial \mathcal{L}}{\partial O_9}$ å¼€å§‹ï¼Œå¹¶è·¨è¿‡è¾“å‡ºèŠ‚ç‚¹ $O_9$ï¼Œå› ä¸ºè¾“å‡ºèŠ‚ç‚¹åº”ç”¨äº†çº¿æ€§æ¿€æ´»ï¼Œ

$$ \frac{\partial O_9}{\partial O_{9_{in}}} = 1.0 $$

ç°åœ¨æˆ‘ä»¬å¯ä»¥è®¡ç®—åå·® $b_9$ å…³äºèŠ‚ç‚¹è¾“å…¥çš„å¯¼æ•°ï¼Œ

$$ \frac{\partial 0_{9_{\mathrm{in}}}}{\partial b_9} = \frac{\partial}{\partial b_9} \left( \lambda_{7,9} R_7 + \lambda_{8,9} R_8 + b_9 \right) = 1 $$

ç°åœ¨æˆ‘ä»¬å¯ä»¥ç»§ç»­åˆ°è¿æ¥æƒé‡ $ğœ†_7,9$ã€‚

![](img/80eaca0166d0cf02f98e140c090fca18.png)

å‘è¿æ¥æƒé‡ $\lambda_{7,9}$ åå‘ä¼ æ’­ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial \lambda_{7,9}} = \frac{\partial O_{9_{\mathrm{in}}}}{\partial \lambda_{7,9}} \cdot \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_9} = R_7 \cdot 1 \cdot (O_9 - I_1) $$

å†æ¬¡å¼ºè°ƒï¼Œç”±äºè¾“å‡ºèŠ‚ç‚¹åº”ç”¨äº†çº¿æ€§æ¿€æ´»ï¼Œ

$$ \frac{\partial O_9}{\partial O_{9_{in}}} = 1.0 $$

å¹¶ä¸” $\frac{\partial O^{\text{in}}_9}{\partial \lambda_{7,9}}$ ç®€å•åœ°æ˜¯ $ğ‘…_7$ çš„è¾“å‡ºï¼Œ

$$ \frac{\partial O^{\text{in}}_9}{\partial \lambda_{7,9}} = \frac{\partial}{\partial \lambda_{7,9}} \left( \lambda_{7,9} R_7 + \lambda_{8,9} R_8 + b_9 \right) = R_7 $$

è®©æˆ‘ä»¬ç»§ç»­åˆ° $\partial \lambda_{7,9}$ å¹¶åˆ°è¾¾è§£ç éšè—èŠ‚ç‚¹ $ğ‘…_7$ çš„è¾“å‡º

![](img/1c85ce96ca6f0999b7bc167c32d65b89.png)

å‘è§£ç éšè—å±‚èŠ‚ç‚¹ $R_7$ çš„è¾“å‡ºåå‘ä¼ æ’­ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial R_7} = \frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7} \cdot \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_9} + \frac{\partial O_{10_{\mathrm{in}}}}{\partial R_7} \cdot \frac{\partial O_{10}}{\partial O_{10_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_{10}} + \frac{\partial O_{11_{\mathrm{in}}}}{\partial R_7} \cdot \frac{\partial O_{11}}{\partial O_{11_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial O_{11}} $$

æˆ‘ä»¬å¯ä»¥å°†å…¶è¯„ä¼°ä¸ºï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial R_7} = \lambda_{7,9} \cdot 1 \cdot (O_9 - I_1) + \lambda_{7,10} \cdot 1 \cdot (O_{10} - I_2) + \lambda_{7,11} \cdot 1 \cdot (O_{11} - I_3) $$

æˆ‘ä»¬å°†æ¯ä¸ªè¿æ¥çš„å¯¼æ•°ç›¸åŠ ã€‚å†æ¬¡å¼ºè°ƒï¼Œç”±äº $ğ‘‚_{9}$ï¼Œ$ğ‘‚_{10}$ å’Œ $ğ‘‚_{11}$ å¤„çš„çº¿æ€§æ¿€æ´»ï¼Œ

$$ \frac{\partial O_9}{\partial O_{9_{\mathrm{in}}}} = 1, \quad \frac{\partial O_{10}}{\partial O_{10_{\mathrm{in}}}} = 1, \quad \frac{\partial O_{11}}{\partial O_{11_{\mathrm{in}}}} = 1 $$

æ­¤å¤–ï¼Œæ²¿ç€è¿æ¥ï¼Œå¯¼æ•°å°±æ˜¯æƒé‡ï¼Œ

$$ \frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7} = \lambda_{7,9}, \quad \frac{\partial O_{10_{\mathrm{in}}}}{\partial R_7} = \lambda_{7,10}, \quad \frac{\partial O_{11_{\mathrm{in}}}}{\partial R_7} = \lambda_{7,11} $$

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æ¼”ç¤º $\frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7}$ çš„å¯¼æ•°ï¼Œ

$$ \frac{\partial O_{9_{\mathrm{in}}}}{\partial R_7} = \frac{\partial}{\partial R_7} \left( \lambda_{7,9} R_7 + \lambda_{8,9} R_8 + b_9 \right) = \lambda_{7,9} $$

ä»æˆ‘ä»¬è§£ç å™¨éšè—å±‚èŠ‚ç‚¹ $ğ‘…_7$ çš„è¾“å‡ºç»§ç»­è®¡ç®—èŠ‚ç‚¹åç½® $b_7$ çš„å¯¼æ•°ã€‚

![](img/604e4fcf99d1c41dd899458f80a67179.png)

åå‘ä¼ æ’­åˆ°éšè—è§£ç èŠ‚ç‚¹ $R_7$ ä¸­çš„åç½® $b_7$ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_7} = \frac{\partial R_{7_{\mathrm{in}}}}{\partial b_7} \cdot \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial R_7} $$

ç”±äºåœ¨ $R_7$ å¤„åº”ç”¨äº† sigmoid æ¿€æ´»å‡½æ•°ï¼Œä¸ºäº†è·¨è¿‡èŠ‚ç‚¹ï¼Œ

$$ \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} = \sigma' (R_7) = R_7 (1 - R_7) $$

å¯¹äºç»™å®šåç½®çš„èŠ‚ç‚¹è¾“å…¥çš„åå¯¼æ•°ï¼Œ

$$ \frac{R_{7_{\mathrm{in}}}}{\partial b_7} = \frac{\partial}{\partial b_7} \left( \lambda_{6,7} M_6 + b_7 \right) = 1 $$

å› æ­¤ç°åœ¨æˆ‘ä»¬æœ‰ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_7} = 1 \cdot R_7 (1 - R_7) \cdot \overbrace{ \left[ \lambda_{7,9} \cdot 1 \cdot (O_9 - I_1) + \lambda_{7,10} \cdot 1 \cdot (O_{10} - I_2) + \lambda_{7,11} \cdot 1 \cdot (O_{11} - I_3) \right] }^{\frac{\partial L}{\partial R_7}} $$

ç°åœ¨æˆ‘ä»¬å¯ä»¥ç»§ç»­åˆ°è¿æ¥æƒé‡ $\lambda_{6,7}$ã€‚

![](img/1559af01deb817828f382cd89480ff41.png)

åå‘ä¼ æ’­åˆ°è¿æ¥æƒé‡ $\lambda_{6,7}$ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial \lambda_{6,7}} = \frac{\partial R_{7_{\mathrm{in}}}}{\partial \lambda_{6,7}} \cdot \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial R_7} $$

å†æ¬¡ï¼Œç”±äºåœ¨éšè—å±‚èŠ‚ç‚¹ä¸­åº”ç”¨äº† sigmoid æ¿€æ´»å‡½æ•°ï¼Œ

$$ \frac{\partial R_7}{\partial R_{7_{in}}} = 1.0 $$

å¹¶ä¸” $\frac{\partial R_{7_{\mathrm{in}}}}{\partial \lambda_{6,7}}$ ç®€å•åœ°æ˜¯ $M_6$ çš„è¾“å‡ºï¼Œ

$$ \frac{\partial R_{7_{\mathrm{in}}}}{\partial \lambda_{6,7}} = \frac{\partial}{\partial \lambda_{6,7}} \left( \lambda_{6,7} M_6 + b_6 \right) = M_6 $$

å› æ­¤ç°åœ¨æˆ‘ä»¬æœ‰ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_7} = M_6 \cdot R_7 (1 - R_7) \cdot \overbrace{ \left[ \lambda_{7,9} \cdot 1 \cdot (O_9 - I_1) + \lambda_{7,10} \cdot 1 \cdot (O_{10} - I_2) + \lambda_{7,11} \cdot 1 \cdot (O_{11} - I_3) \right] }^{\frac{\partial \mathcal{L}}{\partial R_7}} $$

è®©æˆ‘ä»¬ç»§ç»­ä»æˆ‘ä»¬çš„æ½œåœ¨èŠ‚ç‚¹ $M_6$ çš„è¾“å‡ºå¼€å§‹ã€‚

![](img/f4cc7dbc1493a36ab0eb828c1422d1f2.png)

å‘æ½œåœ¨èŠ‚ç‚¹è¾“å‡º $M_6$ åå‘ä¼ æ’­ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial M_6} = \frac{\partial R_{7_{\mathrm{in}}}}{\partial M_6} \cdot \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial R_7} + \frac{\partial R_{8_{\mathrm{in}}}}{\partial M_6} \cdot \frac{\partial R_8}{\partial R_{8_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial R_8} $$

æˆ‘ä»¬å¯ä»¥å°†å…¶è§£æä¸ºï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial M_6} = \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} + \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8} $$

å†æ¬¡ï¼Œç”±äº sigmoid æ¿€æ´»ï¼Œ

$$ \frac{\partial R_7}{\partial R_{7_{\mathrm{in}}}} = R_7 (1 - R_7), \quad \frac{\partial R_8}{\partial R_{8_{\mathrm{in}}}} = R_8 (1 - R_8) $$

å¹¶æ²¿ç€è¿æ¥ï¼Œ

$$\begin{split} \begin{aligned} \frac{\partial R_{7_{\mathrm{in}}}}{\partial M_6} &= \frac{\partial}{\partial M_6} \left( \lambda_{6,7} M_6 + b_7 \right) = \lambda_{6,7} \\ \frac{\partial R_{8_{\mathrm{in}}}}{\partial M_6} &= \frac{\partial}{\partial M_6} \left( \lambda_{6,8} M_6 + b_8 \right) = \lambda_{6,8} \end{aligned} \end{split}$$

è®©æˆ‘ä»¬ä»æ½œåœ¨èŠ‚ç‚¹ $M_6$ çš„è¾“å‡ºç»§ç»­è®¡ç®—èŠ‚ç‚¹ $b_6$ çš„åå¯¼æ•°ã€‚

![](img/90618005b205c6c5ceb09965c36cf2e1.png)

å‘æ½œåœ¨èŠ‚ç‚¹ $M_6$ ä¸­çš„åç½® $b_6$ åå‘ä¼ æ’­ã€‚æ³¨æ„å›¾åƒå·²ç§»åŠ¨ä»¥è…¾å‡ºç©ºé—´ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_6} = \frac{\partial M_{6_{\mathrm{in}}}}{\partial b_6} \cdot \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial M_6} $$

ç”±äº $M_6$ å¤„çš„ sigmoid æ¿€æ´»ï¼Œè¦ç©¿è¿‡èŠ‚ç‚¹ï¼Œ

$$ \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} = \sigma' (M_6) = M_6 \cdot (1 - M_6) $$

ä»¥åŠå¯¹äºç»™å®šåç½®çš„èŠ‚ç‚¹è¾“å…¥çš„åå¯¼æ•°ï¼Œ

$$ \frac{\partial M_{6_{\mathrm{in}}}}{\partial b_6} = \frac{\partial}{\partial b_6} \left( \lambda_{4,6} L_4 + b_6 \right) = 1 $$

å› æ­¤ç°åœ¨æˆ‘ä»¬æœ‰ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_6} = 1 \cdot M_6 (1 - M_6) \cdot \overbrace{ \left[ \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} + \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8} \right] }^{\frac{\partial \mathcal{L}}{\partial M_6}} $$

ç°åœ¨æˆ‘ä»¬å¯ä»¥ç»§ç»­åˆ°è¿æ¥æƒé‡ï¼Œ$\lambda_{4,6}$ã€‚

![](img/f5770d05672cfe3c14c6973f2775d2de.png)

å‘è¿æ¥æƒé‡åå‘ä¼ æ’­ï¼Œ$\lambda_{4,6}$ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = \frac{\partial M_{6_{\mathrm{in}}}}{\partial \lambda_{4,6}} \cdot \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial M_6} $$

å†æ¬¡å¼ºè°ƒï¼Œç”±äºåœ¨éšè—å±‚èŠ‚ç‚¹ä¸­åº”ç”¨äº† Sigmoid æ¿€æ´»å‡½æ•°ï¼Œ

$$ \frac{\partial M_6}{\partial M_{6_{in}}} = M_6 \cdot (1 - M_6) $$

å¹¶ä¸” $\frac{\partial M_{6_{\mathrm{in}}}}{\partial \lambda_{4,6}}$ ç®€å•åœ°æ˜¯ $L_4$ çš„è¾“å‡ºï¼Œ

$$ \frac{\partial M_{6_{\mathrm{in}}}}{\partial \lambda_{4,6}} = \frac{\partial}{\partial \lambda_{4,6}} \left( \lambda_{4,6} L_4 + \lambda_{5,6} L_5 + b_6 \right) = L_4 $$

å› æ­¤ç°åœ¨æˆ‘ä»¬æœ‰ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial \lambda_{4,6}} = L_4 \cdot M_6 (1 - M_6) \cdot \overbrace{ \left[ \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} + \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8} \right] }^{\frac{\partial \mathcal{L}}{\partial M_6}} $$

ç°åœ¨æˆ‘ä»¬å¯ä»¥ç»§ç»­åˆ°ç¼–ç å™¨éšè—å±‚èŠ‚ç‚¹çš„è¾“å‡º $L_4$ã€‚

![](img/1e5148ec01b8276d13a3ac564a201ab3.png)

å‘åä¼ æ’­åˆ°ç¼–ç å™¨éšè—èŠ‚ç‚¹çš„è¾“å‡º $ğ¿_4$ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°è¿™ä¸ªç»“æœå¹¶è¯„ä¼°å®ƒï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial L_4} = \frac{\partial M_{6_{\mathrm{in}}}}{\partial L_4} \cdot \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial M_6} = \lambda_{4,6} \cdot M_6 (1 - M_6) \cdot \frac{\partial \mathcal{L}}{\partial M_6} $$

å†æ¬¡å¼ºè°ƒï¼Œç”±äºåœ¨æ½œåœ¨èŠ‚ç‚¹ä¸­åº”ç”¨äº† Sigmoid æ¿€æ´»å‡½æ•°ï¼Œ

$$ \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} = M_6 (1 - M_6) $$

å¹¶ä¸” $\frac{\partial M_{6_{\mathrm{in}}}}{\partial L_4}$ ç®€å•åœ°æ˜¯æƒé‡ï¼Œ$\lambda_{4,6}$ï¼Œ

$$ \frac{\partial M_{6_{\mathrm{in}}}}{\partial L_4} = \frac{\partial}{\partial L_4} \left( \lambda_{4,6} L_4 + b_6 \right) = \lambda_{4,6} $$

è®©æˆ‘ä»¬ä»ç¼–ç å™¨éšè—å±‚èŠ‚ç‚¹çš„è¾“å‡º $L_4$ å¼€å§‹ï¼Œè®¡ç®—èŠ‚ç‚¹åç½® $b_4$ çš„å¯¼æ•°ã€‚

![](img/cf8f925e7a89e3d992b323edfd45034e.png)

å‘åä¼ æ’­åˆ°ç¼–ç å™¨éšè—å±‚èŠ‚ç‚¹ $L_4$ ä¸­çš„åç½® $b_4$ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_4} = \frac{\partial L_{4_{\mathrm{in}}}}{\partial b_4} \cdot \frac{\partial L_4}{\partial L_{4_{\mathrm{in}}}} \cdot \frac{\partial \mathcal{L}}{\partial L_4} = 1 \cdot L_4 (1 - L_4) \cdot \frac{\partial \mathcal{L}}{\partial L_4} $$

ç”±äº $M_6$ å¤„çš„ Sigmoid æ¿€æ´»å‡½æ•°ï¼Œè¦ç©¿è¿‡èŠ‚ç‚¹ï¼Œ

$$ \frac{\partial M_6}{\partial M_{6_{\mathrm{in}}}} = \sigma' (M_6) = M_6 \cdot (1 - M_6) $$

ä»¥åŠå¯¹äºç»™å®šåç½®çš„èŠ‚ç‚¹è¾“å…¥çš„åå¯¼æ•°ï¼Œ

$$ \frac{\partial M_{6_{\mathrm{in}}}}{\partial b_6} = \frac{\partial}{\partial b_6} \left( \lambda_{4,6} L_4 + b_6 \right) = 1 $$

å› æ­¤ç°åœ¨æˆ‘ä»¬æœ‰ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial b_6} = 1 \cdot M_6 (1 - M_6) \cdot \overbrace{ \left[ \lambda_{6,7} \cdot R_7 (1 - R_7) \cdot \frac{\partial \mathcal{L}}{\partial R_7} + \lambda_{6,8} \cdot R_8 (1 - R_8) \cdot \frac{\partial \mathcal{L}}{\partial R_8} \right] }^{\frac{\partial \mathcal{L}}{\partial M_6}} $$

æœ€åï¼Œæˆ‘ä»¬ç»§ç»­åˆ°è¿æ¥æƒé‡ï¼Œ$\lambda_{1,4}$ã€‚

![å›¾ç‰‡](img/3623ed192b17eb44b8f6f8c59b1dc0d0.png)

åå‘ä¼ æ’­åˆ°è¿æ¥æƒé‡ï¼Œ$\lambda_{1,4}$ã€‚

é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼Œ

$$ \frac{\partial \mathcal{L}}{\partial \lambda_{1,4}} = \frac{\partial L^{\text{in}}_4}{\partial \lambda_{1,4}} \cdot \frac{\partial L_4}{\partial L^{\text{in}}_4} \cdot \frac{\partial \mathcal{L}}{\partial L_4} $$

å†æ¬¡å¼ºè°ƒï¼Œç”±äºåœ¨éšè—å±‚èŠ‚ç‚¹ä¸­åº”ç”¨äº† sigmoid æ¿€æ´»å‡½æ•°ï¼Œ

$$ \frac{\partial L_4}{\partial L_{4_{in}}} = L_4 \cdot (1 - L_4) $$

å¹¶ä¸” $\frac{\partial L^{\text{in}}_4}{\partial \lambda_{1,4}}$ ç®€å•åœ°æ˜¯ $I_1$ çš„è¾“å‡ºï¼Œ

$$ \frac{\partial L^{\text{in}}_4}{\partial \lambda_{1,4}} = \frac{\partial}{\partial \lambda_{1,4}} \left( \lambda_{1,4} I_1 + \lambda_{2,4} I_2 + \lambda_{3,4} I_3 + b_4 \right) = I_1 $$

å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ï¼Œ

$$ \frac{\partial L}{\partial \lambda_{1,4}} = I_1 \cdot L_4 (1 - L_4) \cdot \underbrace{\left[ \lambda_{4,6} \cdot M_6 (1 - M_6) \cdot \frac{\partial L}{\partial M_6} \right]}_{\frac{\partial L}{\partial L_4}} $$

ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä»…ä½¿ç”¨ NumPy Python åŒ…å’Œ Python å†…ç½®æ•°æ®ç»“æ„å­—å…¸ä»å¤´å¼€å§‹æ„å»ºè¿™ä¸ªè‡ªåŠ¨ç¼–ç å™¨ã€‚

## å¯¼å…¥æ‰€éœ€çš„åŒ…

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
ignore_warnings = True                                        # ignore warnings?
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, AutoLocator) # control of axes ticks
plt.rc('axes', axisbelow=True)                                # set axes and grids in the background for all plots
from scipy.stats import rankdata                              # to assist with plot label placement
from sklearn.linear_model import LinearRegression             # fit the relationship between latent and training data slope 
seed = 13                                                     # random number seed
cmap = plt.cm.tab20                                           # default colormap
plt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements
if ignore_warnings == True:                                   
    import warnings
    warnings.filterwarnings('ignore') 
```

å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½å¿…é¡»é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£å¹¶è¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£ä¸­æä¾›äº†æ›´å¤šå¸®åŠ©ã€‚

## å£°æ˜å‡½æ•°

è¿™é‡Œæ˜¯è®­ç»ƒå’Œå¯è§†åŒ–æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨çš„å‡½æ•°ã€‚

```py
def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def xavier(n_in, n_out):                                      # Xavier initializer function
    limit = np.sqrt(6 / (n_in + n_out))
    return np.random.uniform(-limit, limit)

def sigmoid(x):                                               # sigmoid activation
    return 1 / (1 + np.exp(-x))

def initialize_parameters():                                  # initialize all weights and biases and build dictionaries of both
    weights = {                            
        'w14': xavier(3, 2),
        'w24': xavier(3, 2),
        'w34': xavier(3, 2),
        'w15': xavier(3, 2),
        'w25': xavier(3, 2),
        'w35': xavier(3, 2),
        'w46': xavier(2, 1),
        'w56': xavier(2, 1),
        'w67': xavier(1, 2),
        'w68': xavier(1, 2),
        'w79': xavier(2, 3),
        'w89': xavier(2, 3),
        'w710': xavier(2, 3),
        'w810': xavier(2, 3),
        'w711': xavier(2, 3),
        'w811': xavier(2, 3),
    }
    biases = {                                                # biases (one per neuron, excluding input)
        'b4': 0.0,
        'b5': 0.0,
        'b6': 0.0,
        'b7': 0.0,
        'b8': 0.0,
        'b9': 0.0,
        'b10': 0.0,
        'b11': 0.0
    }
    return weights, biases 

def forward_pass(input_vec, weights, biases):                 # forward pass of the autoencoder
    I1, I2, I3 = input_vec.flatten()                               # input nodes (I1, I2, I3)
    z4 = weights['w14'] * I1 + weights['w24'] * I2 + weights['w34'] * I3 + biases['b4'] # encoder
    a4 = sigmoid(z4)

    z5 = weights['w15'] * I1 + weights['w25'] * I2 + weights['w35'] * I3 + biases['b5']
    a5 = sigmoid(z5)

    z6 = weights['w46'] * a4 + weights['w56'] * a5 + biases['b6'] # bottlekneck
    a6 = sigmoid(z6)

    z7 = weights['w67'] * a6 + biases['b7']                   # decoder
    a7 = sigmoid(z7)

    z8 = weights['w68'] * a6 + biases['b8']
    a8 = sigmoid(z8)

    z9 = weights['w79'] * a7 + weights['w89'] * a8 + biases['b9']
    a9 = z9  

    z10 = weights['w710'] * a7 + weights['w810'] * a8 + biases['b10']
    a10 = z10  # linear

    z11 = weights['w711'] * a7 + weights['w811'] * a8 + biases['b11']
    a11 = z11  # linear

    return {                                                  # return all activations as a dictionary
        'I1': I1, 'I2': I2, 'I3': I3,
        'L4': a4, 'L5': a5,
        'M6': a6,
        'R7': a7, 'R8': a8,
        'O9': a9, 'O10': a10, 'O11': a11
    }

def mse_loss_and_derivative(output_vec, input_vec):           # MSE loss and error derivative given output and input
    diff = output_vec - input_vec
    loss = np.mean(diff**2)
    dloss_dout = (2/3) * diff  # shape (3,1)
    return loss, dloss_dout

def sigmoid_derivative(x):                                    # derivative of sigmoid activation
    return x * (1 - x)

def backpropagate(activations, weights, biases, dloss_dout):  # backpropagate the error derivatives
    I1, I2, I3 = activations['I1'], activations['I2'], activations['I3']
    a4, a5 = activations['L4'], activations['L5']
    a6 = activations['M6']
    a7, a8 = activations['R7'], activations['R8']
    O9, O10, O11 = activations['O9'], activations['O10'], activations['O11']

    delta9 = dloss_dout[0, 0]                                 # error terms (delta) for output nodes = dLoss/dOutput
    delta10 = dloss_dout[1, 0]
    delta11 = dloss_dout[2, 0]

    grad_weights = {}                                         # gradients for weights from R7, R8 to O9, O10, O11
    grad_biases = {}

    grad_weights['w79'] = delta9 * a7
    grad_weights['w89'] = delta9 * a8
    grad_weights['w710'] = delta10 * a7
    grad_weights['w810'] = delta10 * a8
    grad_weights['w711'] = delta11 * a7
    grad_weights['w811'] = delta11 * a8

    grad_biases['b9'] = delta9
    grad_biases['b10'] = delta10
    grad_biases['b11'] = delta11

    delta_r7 = (delta9 * weights['w79'] + delta10 * weights['w710'] + delta11 * weights['w711']) * sigmoid_derivative(a7) # gradients for R7 and R8
    delta_r8 = (delta9 * weights['w89'] + delta10 * weights['w810'] + delta11 * weights['w811']) * sigmoid_derivative(a8)

    grad_weights['w67'] = delta_r7 * a6                       # gradients for weights from M6 to R7, R8
    grad_weights['w68'] = delta_r8 * a6

    grad_biases['b7'] = delta_r7
    grad_biases['b8'] = delta_r8

    delta_m6 = (delta_r7 * weights['w67'] + delta_r8 * weights['w68']) * sigmoid_derivative(a6) # backpropagate delta to M6 (sigmoid)

    grad_weights['w46'] = delta_m6 * a4                       # gradients for weights from L4, L5 to M6
    grad_weights['w56'] = delta_m6 * a5

    grad_biases['b6'] = delta_m6

    delta_l4 = delta_m6 * weights['w46'] * sigmoid_derivative(a4) # backpropagate delta to L4, L5 (sigmoid)
    delta_l5 = delta_m6 * weights['w56'] * sigmoid_derivative(a5)

    grad_weights['w14'] = delta_l4 * I1                       # gradients for weights from I1, I2, I3 to L4
    grad_weights['w24'] = delta_l4 * I2
    grad_weights['w34'] = delta_l4 * I3

    grad_biases['b4'] = delta_l4

    grad_weights['w15'] = delta_l5 * I1                       # gradients for weights from I1, I2, I3 to L5
    grad_weights['w25'] = delta_l5 * I2
    grad_weights['w35'] = delta_l5 * I3

    grad_biases['b5'] = delta_l5
    return grad_weights, grad_biases

def update_parameters(weights, biases, grad_weights, grad_biases, learning_rate): # update the weights and biased by derivatives and learning rate
    for key in grad_weights:                                  # update weights
        weights[key] -= learning_rate * grad_weights[key]
    for key in grad_biases:                                   # update biases
        biases[key] -= learning_rate * grad_biases[key]
    return weights, biases 
```

## å¯è§†åŒ–è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œ

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŒ‡å®šè‡ªåŠ¨ç¼–ç å™¨çš„æ ‡ç­¾ã€ä½ç½®ã€è¿æ¥å’Œé¢œè‰²ï¼Œç„¶åç»˜åˆ¶è‡ªåŠ¨ç¼–ç å™¨ã€‚

+   è™½ç„¶æ­¤ä»£ç æ˜¯é€šç”¨çš„ï¼Œä½†å®é™…çš„è‡ªåŠ¨ç¼–ç å™¨ä»£ç å¹¶æ²¡æœ‰æ¨å¹¿åˆ°ä¸å…¶ä»–æ¶æ„ä¸€èµ·å·¥ä½œï¼Œä¾‹å¦‚æ”¹å˜ç½‘ç»œçš„æ·±åº¦æˆ–å®½åº¦

+   æ”¹å˜æ˜¾ç¤ºå‚æ•°ï¼Œä½†ä¸æ”¹å˜è‡ªåŠ¨ç¼–ç å™¨æ¶æ„

```py
positions = {                                                 # node positions
    'I1': (0, 2), 'I2': (0, 1), 'I3': (0, 0),
    'L4': (1, 1.5), 'L5': (1, 0.5),
    'M6': (2, 1),
    'R7': (3, 1.5), 'R8': (3, 0.5),
    'O9': (4, 2), 'O10': (4, 1), 'O11': (4, 0),
}

node_colors = {                                               # node colors
    'I1': 'white', 'I2': 'white', 'I3': 'white',
    'L4': 'white', 'L5': 'white',
    'M6': 'white',
    'R7': 'white', 'R8': 'white',
    'O9': 'white', 'O10': 'white', 'O11': 'white',
}

edges = [                                                     # edges and weight labels
    ('I1', 'L4', 'lightcoral'), ('I2', 'L4', 'red'), ('I3', 'L4', 'darkred'),
    ('I1', 'L5', 'dodgerblue'), ('I2', 'L5', 'blue'), ('I3', 'L5', 'darkblue'),
    ('L4', 'M6', 'orange'), ('L5', 'M6', 'darkorange'),
    ('M6', 'R7', 'orange'), ('M6', 'R8', 'darkorange'),
    ('R7', 'O9', 'lightcoral'), ('R7', 'O10', 'red'), ('R7', 'O11', 'darkred'),
    ('R8', 'O9', 'dodgerblue'), ('R8', 'O10', 'blue'), ('R8', 'O11', 'darkblue'),
]

weight_labels = { (src, dst,): f"$\\lambda_{{{src[1]}{dst[1:]}}}$" for (src, dst, color) in edges }

bias_offsets = {                                              # bias vector offsets
    'L4': (0.06, 0.12), 'L5': (0.06, 0.12),
    'M6': (0.0, 0.15),
    'R7': (-0.06, 0.12), 'R8': (-0.06, 0.12),
    'O9': (0.0, 0.15), 'O10': (0.0, 0.15), 'O11': (0.0, 0.15),
}

bias_labels = { node: f"$b_{{{node[1:]}}}$" for node in bias_offsets.keys() }
# Plot
fig, ax = plt.subplots(figsize=(11, 6))

custom_weight_offsets = {                                     # custom label offsets for select overlapping weights
    ('I2', 'L4'): (-0.20, 0.0),
    ('I2', 'L5'): (-0.2, 0.20),
    ('R8', 'O9'): (0.15, 0.35),
    ('R8', 'O10'): (0.15, 0.16),
}

for (src, dst, color) in edges:                               # plot edges and weight labels
    x0, y0 = positions[src]
    x1, y1 = positions[dst]
    ax.plot([x0, x1], [y0, y1], color=color, linewidth=1, zorder=1)
    xm, ym = (x0 + x1) / 2, (y0 + y1) / 2
    dx, dy = custom_weight_offsets.get((src, dst), (0, 0.08))
    ax.text(xm + dx, ym + dy, weight_labels[(src, dst)],
            fontsize=9, ha='center', va='center', color = color, zorder=5)

for node, (x, y) in positions.items():                        # white back circles
    ax.scatter(x, y, s=1000, color='white', zorder=2)

for node, (x, y) in positions.items():                        # node circles and labels
    ax.scatter(x, y, s=500, color=node_colors[node], edgecolors='black', zorder=3)
    ax.text(x, y, node, ha='center', va='center', fontsize=9, zorder=4)

for node, (dx, dy) in bias_offsets.items():                   # bias arrows and tighter label placement
    nx, ny = positions[node]
    bx, by = nx + dx, ny + dy
    ax.annotate("", xy=(nx, ny), xytext=(bx, by),
                arrowprops=dict(arrowstyle="->", color='black'), zorder=2)
    ax.text(bx, by, bias_labels[node], ha='right', va='bottom', fontsize=10)

# Final formatting
ax.set_xlim(-0.5, 4.5)
ax.set_ylim(-0.5, 2.7)
ax.axis('off'); plt.tight_layout(); plt.show() 
```

![å›¾ç‰‡](img/333249f6a43bbad84e15a2423db3b9cc8670650c55532adfe9fea6ac7c992872.png)

## åˆ¶ä½œä¸€ä¸ªæœ‰è¶£çš„åˆæˆæ•°æ®é›†

ç”Ÿæˆä¸€ä¸ªå…·æœ‰ 3 ä¸ªå‘é‡ 1D é•¿åº¦çš„éšæœºæ•°æ®é›†ï¼Œè¯¥æ¨¡å¼å¯ä»¥é€šè¿‡æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨æ€»ç»“ã€‚

+   å¦‚æœæˆ‘ä»¬ç”Ÿæˆé•¿åº¦ä¸º 3 çš„éšæœº 1D å‘é‡ï¼Œæˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨å°†æ— æ³•æ€»ç»“ï¼Œå³ï¼Œæ— æ³•ä»åŸå§‹ 3 ä¸ªå€¼ä¸­å‹ç¼©ä¿¡æ¯

+   æˆ‘ä»¬å¿…é¡»åŒ…æ‹¬ä¸€ä¸ªè‡ªåŠ¨ç¼–ç å™¨å¯ä»¥å­¦ä¹ çš„æ¨¡å¼ï¼Œé€šè¿‡æ½œåœ¨èŠ‚ç‚¹è§‚å¯Ÿé€šè¿‡è‰¯å¥½çš„æ•°æ®é‡å»ºè¿›è¡Œé™ç»´

è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘å·²ç»å°†æ•°æ®é›†è®¡ç®—ä¸ºä¸€ä¸ªæ··åˆæ¨¡å‹ï¼Œçº¿æ€§åŠ å°éšæœºæ®‹å·®ã€‚æ•°æ®ç”Ÿæˆæ­¥éª¤åŒ…æ‹¬ï¼Œ

1.  ç”Ÿæˆä¸€ä¸ªéšæœºæ–œç‡ $\sim N\left[-2.0, 2.0 \right]$

1.  åœ¨ä½ç½® $\left[-1, 0, 1 \right]$ ä¸Šè®¡ç®— 3 ä¸ªç‚¹ï¼Œ$f(\left[-1, 0, 1 \right])$

1.  ä¸ºæ¯ä¸ªä½ç½®æ·»åŠ éšæœºçš„ã€ç‹¬ç«‹çš„æ®‹å·®ï¼Œ$f(\left[-1, 0, 1 \right]) + N\left[0.0,\sigma \right]$ï¼Œå…¶ä¸­ sigma æ˜¯æ®‹å·®æ ‡å‡†å·®

æ³¨æ„ï¼Œæ–œç‡è¢«ä¿ç•™ä½œä¸ºæ ‡ç­¾ï¼Œå°†ç”¨äºä¸æ½œåœ¨èŠ‚ç‚¹$M_6$è¾“å‡ºè¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ£€æŸ¥æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ

+   æˆ‘ä»¬çš„å‡è®¾æ˜¯è‡ªåŠ¨ç¼–ç å™¨å°†å­¦ä¼šä¸€ä¸ªå€¼ï¼Œè¯¥å€¼ç›´æ¥æ˜ å°„åˆ°æ–œç‡ä»¥æè¿°è¿™ä¸ªæ•°æ®é›†ã€‚

+   æ³¨æ„ï¼Œè™½ç„¶è¿™ä¸ªæ ‡ç­¾ç”¨äºå±•ç¤ºè‡ªåŠ¨ç¼–ç å™¨å­¦ä¹ çš„èƒ½åŠ›ï¼Œä½†å®ƒå¹¶æ²¡æœ‰ç”¨äºè®­ç»ƒæ¨¡å‹ï¼

```py
np.random.seed(seed = seed+1)                                 # set random seed
nbatch = 12; nnodes = 3; sigma = 0.1                          # set number of data (total number of data), number of nodes (must be 3), error st.dev.
ymat = np.zeros(nbatch); x = np.arange(1,nnodes+1,1); Xmat = np.zeros([nbatch,nnodes])
data = []
for ibatch in range(0,nbatch):                                # loop over synthetic data
    m = np.random.uniform(low = -2.0, high = 2.0)
    Xmat[ibatch] = (x-2.0)*m + np.random.normal(loc = 0.0, scale=sigma,size=nnodes)
    ymat[ibatch] = np.dot(x, Xmat[ibatch]) / np.dot(x, x)
    data.append(Xmat[ibatch].reshape(3,1))

rank = rankdata(Xmat[:,-1])                                   # rank data to improve (alternate) adjacent labels' locations
plt.subplot(111)                                              # plot the synthetic data
for ibatch in range(0,nbatch):                                
    plt.scatter(Xmat[ibatch],x,color=cmap(ibatch/(nbatch)),edgecolor='black',lw=1,zorder=10)
    plt.plot(Xmat[ibatch],x,color=cmap(ibatch/(nbatch)),lw=2,zorder=1)
    custom_positions = [1,2,3,3.2]
    custom_labels = ['I1','I2','I3','Y']
    if rank[ibatch] % 2 == 0:
        plt.annotate(np.round(ymat[ibatch],2),[Xmat[ibatch][-1],3.18],size=9,color='black',ha='center')
    else:
        plt.annotate(np.round(ymat[ibatch],2),[Xmat[ibatch][-1],3.25],size=9,color='black',ha='center') 
    plt.annotate(ibatch+1,[Xmat[ibatch][0],0.9],size=9,color='black',ha='center')
    plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)
    plt.ylim([3.4,0.8]); plt.xlim([-1.5,1.5]); plt.ylabel('Input Nodes'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Data and Labels')
plt.annotate('Data Index: ',[-1.4,0.9])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/9ce15f05dfa887ce6ee1f02619cb004d.png)

## è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨

æˆ‘ä»¬å·²ç»ä¸ºæˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨å®šä¹‰äº†æ‰€æœ‰åŸºæœ¬å‡½æ•°ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ç»„åˆæˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨è®­ç»ƒæ­¥éª¤ï¼Œ

1.  **initialize_parameters** - åˆå§‹åŒ–æƒé‡å’Œåç½®

1.  **forward_pass** - é€šè¿‡æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œå‰å‘ä¼ é€’ä»¥è®¡ç®—èŠ‚ç‚¹è¾“å‡ºå’Œæ•°æ®é‡å»º

1.  **mse_loss_and_derivative** - è®¡ç®—è®­ç»ƒæ•°æ®å’Œé‡å»ºæ•°æ®ä¸­æ¯ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„ L2 æŸå¤±å’Œç›¸å…³è¯¯å·®å¯¼æ•°

1.  **backpropagate** - æ ¹æ®è¯¯å·®å¯¼æ•°å’ŒèŠ‚ç‚¹è¾“å‡ºé€šè¿‡ç½‘ç»œåå‘ä¼ æ’­è¯¯å·®å¯¼æ•°ï¼Œç„¶ååœ¨æ¯ä¸ªæƒé‡å’Œåç½®ä¸Šå¹³å‡æ‰¹æ¬¡çš„æ¢¯åº¦

1.  **update_parameters** - ä½¿ç”¨æ‰¹æ¬¡çš„å¹³å‡æ¢¯åº¦å’Œå­¦ä¹ ç‡æ›´æ–°æƒé‡å’Œåç½®

1.  è¿›è¡Œåˆ°æ”¶æ•›ï¼Œåœ¨è®­ç»ƒ epoch è¾¾åˆ°ä¸€å®šæ•°é‡æ—¶

```py
epochs = 10000                                                # set hyperparameters
batch_size = nbatch
learning_rate = 0.1
seed = 13
np.random.seed(seed=seed)

output_mat = np.zeros((batch_size,epochs,3)); loss_mat = np.zeros((epochs)); M6_mat = np.zeros((batch_size,epochs))

weights, biases = initialize_parameters()                     # initialize weights and biases

for epoch in range(epochs):
    sum_grad_w = {k: 0 for k in weights.keys()}               # initialize zero dictionary to average backpropogated gradients
    sum_grad_b = {k: 0 for k in biases.keys()}
    epoch_loss = 0
    for idata,input_vec in enumerate(data):
        activations = forward_pass(input_vec, weights, biases) # forward pass
        M6_mat[idata,epoch] = activations['M6']
        output_vec = np.array([[activations['O9']], [activations['O10']], [activations['O11']]])
        output_mat[idata,epoch,:] = output_vec.reshape(3)
        loss, dloss_dout = mse_loss_and_derivative(output_vec, input_vec) # compute loss and derivative
        epoch_loss += loss
        grad_w, grad_b = backpropagate(activations, weights, biases, dloss_dout) # backpropagation the derivative
        for k in grad_w:                                      # accumulate gradients
            sum_grad_w[k] += grad_w[k]
        for k in grad_b:
            sum_grad_b[k] += grad_b[k]
    avg_grad_w = {k: v / batch_size for k, v in sum_grad_w.items()} # average gradients over batch
    avg_grad_b = {k: v / batch_size for k, v in sum_grad_b.items()}
    epoch_loss /= batch_size
    loss_mat[epoch] = epoch_loss
    weights, biases = update_parameters(weights, biases, avg_grad_w, avg_grad_b, learning_rate) # update parameters
    # if epoch % 500 == 0:                                    # print loss every 100 training epochs
    #     print(f"Epoch {epoch}, Loss: {epoch_loss:.6f}")

plt.subplot(111)                                              # plot training error vs. training epoch
plt.plot(np.arange(0,epoch+1,1),loss_mat,color='red',label=r'MSE'); plt.xlim([1,epoch]); plt.ylim([0,1])
plt.xlabel('Epochs'); plt.ylabel(r'Mean Square Error (L2 loss)'); plt.title('Autoencoder Average Batch L2 Loss vs. Training Epoch')
add_grid(); plt.legend(loc='upper right'); plt.xscale('linear')

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/3ab1c8fef6098b7c75943615555e53e5.png)

å¹³å‡ L2 æŸå¤±ä¸è®­ç»ƒ epoch æ›²çº¿çœ‹èµ·æ¥éå¸¸å¥½ã€‚

+   æˆ‘ä»¬çœ‹åˆ°å­¦ä¹ æš‚åœï¼Œç„¶åçªç„¶è®­ç»ƒé”™è¯¯å¿«é€Ÿå‡å°‘ï¼Œç„¶åç¼“æ…¢æ”¶æ•›

+   ä¸ºäº†æ•ˆç‡ï¼Œæˆ‘åœæ­¢åœ¨ 10,000 ä¸ª epoch

## è¯„ä¼°æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œ

è®©æˆ‘ä»¬çœ‹çœ‹ç½‘ç»œç“¶é¢ˆå¤„çš„æ½œåœ¨èŠ‚ç‚¹è¾“å‡ºï¼Œå³èŠ‚ç‚¹ M6 çš„è¾“å‡ºã€‚

+   æ³¨æ„ï¼Œæˆ‘ä»¬è®°å½•äº†æ‰€æœ‰è®­ç»ƒ epoch å’Œæ‰€æœ‰æ•°æ®çš„ M6 è¾“å‡ºï¼ˆç§°ä¸ºèŠ‚ç‚¹æ¿€æ´»ï¼‰ã€‚

+   è®©æˆ‘ä»¬çœ‹çœ‹æœ€ç»ˆè®­ç»ƒçš„ç½‘ç»œï¼Œæœ€åä¸€ä¸ª epochï¼Œå¹¶éå†æ‰€æœ‰æ•°æ®

è¿™é‡Œæ˜¯æœ€ç»ˆ epoch M6 è¾“å‡ºä¸æ ·æœ¬æ–œç‡çš„å¯¹æ¯”å›¾ï¼Œ

```py
linear_model = LinearRegression().fit(ymat.reshape(-1, 1), M6_mat[:,-1]) # fit linear model to regress latent on training data slope

plt.subplot(111)                                              # plot latent vs. training data slope
plt.plot(np.linspace(-0.4,0.4,100),linear_model.predict(np.linspace(-0.4,0.4,100).reshape(-1,1)),color='red',zorder=-1)
for ibatch,input_vec in enumerate(data):                      # plot and label training data
    plt.scatter(ymat[ibatch],M6_mat[ibatch,-1],color=cmap(ibatch/(nbatch)),edgecolor='black',marker='o',s=30,zorder=10)
    plt.annotate(ibatch+1,[ymat[ibatch]-0.01,M6_mat[ibatch,-1]+0.01],size=9,color='black',ha='center',zorder=100) 
plt.ylabel('M6 Output'); plt.xlabel(r'Sample Slope, $m_i$'); plt.title('Latent Node Output vs. Sample Slope')
plt.ylim([0.1,0.8]); plt.xlim([-0.4,0.4]); add_grid()
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/7815f0d074113f20a6f77a446f1f83d2.png)

å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œç½‘ç»œç“¶é¢ˆå¤„çš„æ½œåœ¨èŠ‚ç‚¹è¾“å‡ºä¸ç”¨äºç”Ÿæˆæ•°æ®çš„æ ·æœ¬æ–œç‡ä¹‹é—´å­˜åœ¨è‰¯å¥½çš„å…³ç³»ï¼

+   æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨å·²ç»å­¦ä¼šäº† 1 ä¸ªå€¼æ¥è¡¨ç¤ºæ•°æ®é›†ä¸­ 3 ä¸ªå€¼çš„å‘é‡ï¼

+   è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¿¡æ¯å‹ç¼©æ¼”ç¤ºï¼Œ3:1ï¼

## æ£€æŸ¥è®­ç»ƒæ•°æ®é‡å»º

è®©æˆ‘ä»¬å¯è§†åŒ–ä½¿ç”¨æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œé‡æ„çš„ 1D æ•°æ®ï¼Œç¼–ç ç„¶åè§£ç ã€‚

+   å¯¹äºæ‰€æœ‰è®­ç»ƒæ•°æ®ï¼Œæˆ‘åŒ…æ‹¬åŸå§‹æ•°æ®å’Œé‡å»ºæ•°æ®ï¼Œå³ç”±æˆ‘ä»¬è®­ç»ƒçš„è‡ªåŠ¨ç¼–ç å™¨ç¼–ç å’Œè§£ç çš„æ•°æ®

+   å¯¹äºæ¯ä¸ªæ•°æ®è®­ç»ƒæ ·æœ¬ï¼Œæˆ‘åŒ…æ‹¬æ ·æœ¬æ–œç‡ä»¥ä¾›å‚è€ƒï¼Œä½†è¿™ä¸ªæ ‡ç­¾åœ¨è®­ç»ƒä¸­ã€ç¼–ç å™¨æˆ–è§£ç å™¨ä¸­éƒ½æ²¡æœ‰ä½¿ç”¨

```py
for idata,input_vec in enumerate(data):                       # plot training data and reconstructions 
    plt.subplot(4,3,idata+1)
    plt.scatter(Xmat[idata],x,color=cmap(idata/(nbatch+2)),edgecolor='black',lw=1,zorder=10)
    plt.plot(Xmat[idata],x,lw=1,zorder=1,color=cmap(idata/(nbatch+2)),label='data')
    custom_positions = [1,2,3,3.2]
    custom_labels = ['I1','I2','I3','Y']
    plt.annotate(np.round(ymat[idata],2),[Xmat[idata][-1],3.25],size=9,color='black',ha='center')  
    plt.scatter(output_mat[idata,-1,:],x,lw=1,color=cmap(idata/(nbatch+2)))
    plt.plot(output_mat[idata,-1,:],x,lw=1,ls='--',color=cmap(idata/(nbatch+2)),label='reconstruction')
    plt.legend(loc='upper left')
    plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)
    plt.ylim([3.5,0.8]); plt.xlim([-2.5,2.5]); plt.ylabel('index'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Training Data #' + str(idata+1))

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=4.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/7203aaa35d3b4fe06560d8885fa0bc78.png)

è®­ç»ƒæ•°æ®é‡å»ºç›¸å½“ä¸é”™ï¼

+   æˆ‘ä»¬çš„è‡ªç¼–ç å™¨å·²ç»å­¦ä¼šäº†ç¼–ç å’Œè§£ç è®­ç»ƒæ•°æ®

+   ä» 3 ç»´åˆ° 1 ç»´å±•ç¤ºäº†è‰¯å¥½çš„é™ç»´æ•ˆæœï¼

## æ£€æŸ¥æµ‹è¯•æ•°æ®é‡å»º

è®©æˆ‘ä»¬ç”Ÿæˆæ›´å¤šæ•°æ®å¹¶æµ‹è¯•é‡å»ºã€‚

+   æ£€æŸ¥æˆ‘ä»¬è®­ç»ƒçš„è‡ªåŠ¨ç¼–ç å™¨åœ¨æœªç”¨äºè®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨çš„æ•°æ®ä¸Šçš„æ€§èƒ½ï¼Œè¿™è¢«ç§°ä¸ºæ¨¡å‹æ³›åŒ–

```py
np.random.seed(seed = seed+7)
nbatch_test = 12; nnodes = 3; sigma = 0.1
ymat_test = np.zeros(nbatch); x = np.arange(1,nnodes+1,1); Xmat_test = np.zeros([nbatch,nnodes])
data_test = []
for ibatch in range(0,nbatch):
    m = np.random.uniform(low = -2.0, high = 2.0)
    Xmat_test[ibatch] = (x-2.0)*m + np.random.normal(loc = 0.0, scale=sigma,size=nnodes)
    ymat_test[ibatch] = np.dot(x, Xmat_test[ibatch]) / np.dot(x, x)
    data_test.append(Xmat_test[ibatch].reshape(3,1))

rank = rankdata(Xmat_test[:,-1])
plt.subplot(111)
for ibatch in range(0,nbatch_test):
    plt.scatter(Xmat_test[ibatch],x,color=cmap(ibatch/(nbatch)),edgecolor='black',lw=1,zorder=10)
    plt.plot(Xmat_test[ibatch],x,color=cmap(ibatch/(nbatch)),lw=2,zorder=1)
    custom_positions = [1,2,3,3.2]
    custom_labels = ['I1','I2','I3','Y']
    if rank[ibatch] % 2 == 0:
        plt.annotate(np.round(ymat_test[ibatch],2),[Xmat_test[ibatch][-1],3.18],size=9,color='black',ha='center')
    else:
        plt.annotate(np.round(ymat_test[ibatch],2),[Xmat_test[ibatch][-1],3.25],size=9,color='black',ha='center') 
    plt.annotate(ibatch+13,[Xmat_test[ibatch][0],0.9],size=9,color='black',ha='center')
    plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)
    plt.ylim([3.4,0.8]); plt.xlim([-1.5,1.5]); plt.ylabel('Input Nodes'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Data and Labels')
plt.annotate('Test Data Index: ',[-1.45,0.9])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/28e4aff8d55696fd326194c3a79007d1.png)

å°†è®­ç»ƒå¥½çš„è‡ªåŠ¨ç¼–ç å™¨åº”ç”¨äºé‡å»ºæµ‹è¯•æ•°æ®ã€‚

```py
output_vec_test = np.zeros((len(data_test),3))
for idata_test,input_vec_test in enumerate(data_test):
    activations = forward_pass(input_vec_test, weights, biases)                                                    # forward pass
    output_vec_test[idata_test,:] = np.array([[activations['O9']], [activations['O10']], [activations['O11']]]).reshape(-1) 
```

ç°åœ¨å¯è§†åŒ–æµ‹è¯•æ•°æ®çš„é‡å»ºï¼Œ

```py
for idata,input_vec_test in enumerate(data_test):
    plt.subplot(4,3,idata+1)
    plt.scatter(input_vec_test,x,color=cmap(idata/(nbatch)),edgecolor='black',lw=1,zorder=10)
    plt.plot(input_vec_test,x,lw=1,zorder=1,color=cmap(idata/(nbatch)),label='data')
    custom_positions = [1,2,3,3.2]
    custom_labels = ['I1','I2','I3','Y']
    # plt.annotate(np.round(ymat[idata],2),[Xmat[idata][-1],3.25],size=8,color='black',ha='center') 
    plt.scatter(output_vec_test[idata,:],x,lw=1,color=cmap(idata/(nbatch)))
    plt.plot(output_vec_test[idata,:],x,lw=1,ls='--',color=cmap(idata/(nbatch)),label='reconstruction')
    plt.legend(loc='upper left'); plt.gca().set_yticks(custom_positions); plt.gca().set_yticklabels(custom_labels)
    plt.ylim([3.5,0.8]); plt.xlim([-1.5,1.5]); plt.ylabel('index'); plt.xlabel('z'); add_grid(); plt.title('Synthetic 1D Test Image #' + str(idata+13))

plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=4.1, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/abda6b15ea724a35119dc1c5cf9554e9.png)

æˆ‘ä»¬çš„è®­ç»ƒå¥½çš„è‡ªåŠ¨ç¼–ç å™¨ä¼¼ä¹æ³›åŒ–å¾—å¾ˆå¥½ï¼Œåœ¨é‡å»ºè®­ç»ƒæ•°æ®å’Œä¿ç•™çš„æµ‹è¯•æ¡ˆä¾‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚

+   ä¸ºäº†æ›´å®Œæ•´çš„æµç¨‹ï¼Œæˆ‘ä»¬å°†åœ¨è®­ç»ƒå‘¨æœŸå†…å¹¶è¡Œè¯„ä¼°è®­ç»ƒå’Œæµ‹è¯•é”™è¯¯ï¼Œä»¥æ£€æŸ¥æ¨¡å‹è¿‡æ‹Ÿåˆã€‚

+   æˆ‘å°†è¿™äº›ç»„ä»¶åˆ†å¼€ï¼Œä»¥åœ¨æ¼”ç¤ºä¸­ä¿æŒç®€æ´å’Œæ¸…æ™°

## è¯„è®º

è¿™æ˜¯å¯¹è‡ªåŠ¨ç¼–ç å™¨æ·±åº¦å­¦ä¹ ç½‘ç»œçš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„ YouTube è®²åº§é“¾æ¥ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ

*è¿ˆå…‹å°”*

## å…³äºä½œè€…

![å›¾ç‰‡](img/eb709b2c0a0c715da01ae0165efdf3b2.png)

è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ 40 è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚

è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ˜¯[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œåœ¨[å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡](https://www.utexas.edu/)ï¼Œåœ¨é‚£é‡Œä»–ç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ

+   [èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚

+   [è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚

è¿ˆå…‹å°”å·²ç»æ’°å†™äº†è¶…è¿‡ 70 ç¯‡[åŒè¡Œè¯„å®¡çš„å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[Python åŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦ã€Š[åœ°è´¨ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ã€‹ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œåˆ†åˆ«æ˜¯ã€Š[Python ä¸­çš„åº”ç”¨åœ°è´¨ç»Ÿè®¡å­¦ï¼šGeostatsPy å®è·µæŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)ã€‹å’Œã€Š[Python ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„å®è·µæŒ‡å—](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‹ã€‚

è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯åœ¨ä»–çš„[YouTube é¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œé™„æœ‰ 100 å¤šä¸ª Python äº¤äº’å¼ä»ªè¡¨æ¿å’Œ 40 å¤šä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†è®°å½•å·¥ä½œæµç¨‹ï¼Œè¿™äº›å­˜å‚¨åº“ä½äºä»–çš„[GitHub è´¦æˆ·](https://github.com/GeostatsGuy)ï¼Œä»¥æ”¯æŒä»»ä½•æœ‰å…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›æŒç»­æ›´æ–°çš„å†…å®¹ã€‚æƒ³äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚ä¸ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œï¼Œæ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶ç»“åˆæ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µï¼Œå¼€å‘æ–°é¢–çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ä»¥å¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢

æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°è´¨ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åº”ç”¨åœ°è´¨ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
