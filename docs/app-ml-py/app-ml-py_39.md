# 机器学习术语表

> 原文：[`geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_glossary.html`](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_glossary.html)

迈克尔·J·皮尔茨，教授，德克萨斯大学奥斯汀分校

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [网站](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [地统计学书籍](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python 中应用地统计学电子书](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python 中应用机器学习电子书](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

电子书“Python 中应用地统计学：GeostatsPy 实践指南”的一章。

请将此电子书引用如下：

Pyrcz, M.J., 2024, Python 中应用地统计学：GeostatsPy 实践指南，[`geostatsguy.github.io/GeostatsPyDemos_Book`](https://geostatsguy.github.io/GeostatsPyDemos_Book)。

本书中的工作流程以及更多内容可在以下链接找到：

请将 GeostatsPyDemos GitHub 仓库引用如下：

Pyrcz, M.J., 2024, GeostatsPyDemos: GeostatsPy Python Package for Spatial Data Analytics and Geostatistics Demonstration Workflows Repository (0.0.1). Zenodo. [`zenodo.org/doi/10.5281/zenodo.12667035`](https://zenodo.org/doi/10.5281/zenodo.12667035)

![DOI](https://zenodo.org/doi/10.5281/zenodo.12667035)

作者：迈克尔·J·皮尔茨

© 版权所有 2024。

本章是**机器学习术语**的摘要。

## 机器学习概念的动力

首先，为什么要做这件事呢？我收到了来自我的**地下机器学习**本科和研究生课程的学生的课程术语表请求。虽然我通常在讲义中为显著术语提供定义幻灯片，但一些学生要求课程术语表，以便他们进行课程复习。这本电子书提供了一个很好的工具和动力，最终完成了这项工作。

让我从一项坦白开始。有一个由谷歌开发者编写的[机器学习术语表](https://developers.google.com/machine-learning/glossary)。对于那些寻求深入、全面的地理统计术语列表，请使用这本书！

+   通过编写自己的术语表，我可以将范围和描述限制在课程内容内。我担心许多学生会因为标准机器学习术语表的大小和数学符号而感到不知所措。

+   此外，通过在电子书中包含术语表，我可以从术语表条目链接到电子书中的章节，以便于查阅。我最终将所有章节都添加到术语表的超链接，以便在章节和术语表之间来回移动。

最后，像本书的其他部分一样，我希望术语表成为一个永葆青春的活文档。

## **邻接矩阵**（谱聚类）

谱聚类：表示图中所有节点成对组合之间成对连接的矩阵。

+   这些值是指示符，如果不连接则为 0，如果连接则为 1

注意，在邻接矩阵中，节点自连接被设置为 0

## **加法规则**（概率）

概率概念：当我们添加概率（结果的并集）时，$A$ 或 $B$ 的概率是根据概率加法规则计算的，

$$ P(A \cup B) = P(A) + P(B) - P(A,B) $$

对于互斥事件，我们可以将加法规则推广为，

$$ P\left( \bigcup_{i=1}^k A_i \right) = \sum_{i=1}^k P(A_i) $$

## **仿射校正**

特征变换：一种分布缩放，可以将其视为单变量分布（例如，*直方图*）的平移、拉伸或压缩。对于将 $X$ 向 $Y$ 进行仿射校正的情况，

$$ y_i = \frac{\sigma_y}{\sigma_x}(x_i - \overline{x}) + \overline{y}, \quad \forall \quad i, \ldots, n $$

其中 $\overline{x}$ 和 $\sigma_x$ 是原始均值和方差，而 $\overline{y}$ 和 $\sigma_y$ 是新的均值和方差。

我们可以看到，仿射相关方法首先将分布中心化（通过减去原始均值），然后通过新标准差与原始标准差的比例来缩放分散度（分布范围），然后将分布平移到新的均值中心。

+   仿射校正没有形状变化。对于形状变化，可以考虑像 *高斯变形* 这样的 *分布变换*。

## **亲和矩阵**（谱聚类）

谱聚类：表示图中所有节点成对组合之间成对连接程度的矩阵。

+   这些值表示连接的强度，与指示符邻接矩阵不同，如果不连接则为 0，如果连接则为 1

注意，在邻接矩阵中，节点自连接被设置为 0

## **Bagging 模型**

Bagging 树和随机森林：将自助法应用于获得数据实现，

$$ Y^b, X_1^b, \dots, X_m^b, \quad b = 1, \dots, B $$

以训练预测模型实现，

$\hat{Y}^b = \hat{f}^b (X_1^b, \dots, X_m^b)$

其中，

+   $(X_1^b, \dots, X_m^b)$ - 第 $b$ 个自助数据集中的自助预测特征

+   $\hat{f}^b$ - 第 $b$ 个自助模型

+   $\hat{Y}^b$ - 第 $b^{th}$ 个自助模型的模型预测值

用于计算预测实现。预测实现的集合被聚合以减少模型方差。聚合包括，

+   *回归* - 预测的平均值 $ \hat{Y} = \frac{1}{B} \sum_{b=1}^{B} \hat{Y}^b $ 

+   *分类* - 预测的最小值

$$ \hat{Y} = \text{argmax}(\hat{Y}^b) $$

我们可以使用任何预测模型进行袋装，实际上 scikit-learn 中的 BaggingClassifier 和 BaggingRegressor 函数是接受预测模型作为输入的包装器。

## **基函数展开**

多项式回归：为了增加模型的灵活性，例如，为了捕捉回归、分类模型中的非线性，我们使用一组基函数扩展特征

+   在数学中，基函数展开是将更复杂的函数表示为更简单基函数的线性组合的方法，这使得问题更容易解决

+   使用基函数展开，我们通过原始特征的基函数扩展问题的维度性，但仍然在转换后的特征上使用线性方法。

$$ ℎ(𝑥_𝑖 )=\left( ℎ_1(𝑥_𝑖 ),ℎ_2(𝑥_𝑖 ),\ldots,ℎ_𝑘(𝑥_𝑖 ) \right) $$

这里是一个基函数展开的例子，多项式基函数展开的基函数集合：

$$ h_{i,1}(x_i) = x_i, \quad h_{i,2}(x_i) = x_i², \quad h_{i,3}(x_i) = x_i³, \quad h_{i,4}(x_i) = x_i⁴, \dots, \quad h_{i,k}(x_i) = x_i^k $$

## **基函数**

多项式回归：为了增加模型的灵活性，例如，为了捕捉回归、分类模型中的非线性，我们使用一组基函数扩展特征

+   在数学中，基函数展开是将更复杂的函数表示为更简单基函数的线性组合的方法，这使得问题更容易解决

+   使用基函数展开，我们通过原始特征的基函数扩展问题的维度性，但仍然在转换后的特征上使用线性方法。

$$ ℎ(𝑥_𝑖 )=\left( ℎ_1(𝑥_𝑖 ),ℎ_2(𝑥_𝑖 ),\ldots,ℎ_𝑘(𝑥_𝑖 ) \right) $$

其中 $h_1, \ldots, h_k$ 是基函数。例如，以下是多项式基函数展开的基函数：

$$ h_{i,1}(x_i) = x_i, \quad h_{i,2}(x_i) = x_i², \quad h_{i,3}(x_i) = x_i³, \quad h_{i,4}(x_i) = x_i⁴, \dots, \quad h_{i,k}(x_i) = x_i^k $$

## **贝叶斯定理**（概率）

概率概念：贝叶斯概率的核心数学模型，用于从先验概率进行贝叶斯更新，从新信息到后验概率的似然概率。 

$$ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} $$

其中 $P(A)$ 是先验，$P(B|A)$ 是似然，$P(B)$ 是证据项，$P(A|B)$ 是后验。如果方便，可以用更描述性的标签替换 $A$ 和 $B$ 以更好地理解这种方法，

$$ P(\text{Model} | \text{New Data}) = \frac{P(\text{New Data} | \text{Model}) \cdot P(\text{Model})}{P(\text{New Data})} $$

展示我们正在用新数据更新我们的模型

## **贝叶斯概率**

概率概念：基于对事件发生可能性的信念程度（专家判断和经验）的概率。一般方法，

+   从先验概率开始，在收集新信息之前

+   基于新信息单独制定似然概率

+   使用似然更新先验以计算更新的后验概率

+   当有新信息时继续更新

+   解决我们无法使用简单频率的概率问题，即*频率派概率*方法

+   贝叶斯更新是通过*贝叶斯定理*建模的

## **分类的贝叶斯更新**

朴素贝叶斯：这是我们从贝叶斯更新的角度提出分类预测问题的方法，基于给定 $n$ 个特征 $x_1, \dots , x_n$ 的类别 $k$ 的条件概率。

$$ P(C_k | x_1, \dots , x_n) $$

我们可以通过贝叶斯更新求解这个后验概率，

$$ P(C_k | x_1, \dots , x_n) = \frac{P(x_1, \dots , x_n | C_k) P(C_k)}{P(x_1, \dots , x_n)} $$

让我们暂时将似然和先验结合起来，

$$ P(x_1, \dots , x_n | C_k) P(C_k) = P(x_1, \dots , x_n, C_k) $$

我们可以递归地扩展完整的联合分布如下，

$$ P(x_1, \dots , x_n, C_k) $$

联合分布通过条件分布和先验分布进行扩展，

$$ P(x_1 | x_2, \dots , x_n, C_k) P(x_2, \dots , x_n, C_k) $$

继续递归地扩展，

$$ P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3, \dots , x_n, C_k) $$

我们可以推广如下，

$$ P(C_k | x_1, \dots , x_n) = P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3 | x_4, \dots , x_n, C_k) \ldots P(x_{n-1} | x_n, C_k) (x_{n} | C_k) P(C_k) $$

## **贝叶斯线性回归**

贝叶斯线性回归：线性回归模型的频率派公式为，

$$ y = b_1 \times x + b_0 + \sigma $$

其中 $x$ 是预测特征，$b_1$ 是斜率参数，$b_0$ 是截距参数，$\sigma$ 是误差或噪声。存在一个解析形式，用于拟合可用数据并最小化数据误差向量的 $L²$ 范数。

对于线性回归的贝叶斯公式，我们将模型设定为对响应分布 $Y$ 的预测，现在 $Y$ 是一个随机变量：

$$ Y \sim N(\beta^{T}X, \sigma^{2} I) $$

我们通过贝叶斯更新来估计模型参数分布，以从先验和训练数据的似然中推断模型参数。

$$ P(\beta | y, X) = \frac{P(y,X| \beta) P(\beta)}{P(y,X)} $$

通常对于连续特征，我们无法直接计算后验概率，我们必须使用抽样方法，例如马尔可夫链蒙特卡洛（McMC）来抽样后验。

## **大数据**

机器学习概念：如果你的数据具有以下这些标准的组合，那么你就有大数据：

1.  *数据量* - 许多数据样本和特征，难以存储、传输和可视化

1.  *数据速度* - 高速率收集，相对于决策周期进行连续数据收集，在更新模型的同时保持对新数据的跟进是一个挑战

1.  *数据多样性* - 数据来自各种来源，具有各种类型的数据、信息和规模

1.  *数据可变性* - 在项目期间数据采集发生变化，即使是单个特征也可能有多个版本的数据，具有不同的规模、分布和真实性

1.  *数据真实性* - 数据具有各种级别的准确性，数据是不确定的

对于大多数（如果不是所有）的这些标准，在常见的地下应用中都是满足的。地下工程和地球科学通常与大数据打交道！

## **大数据分析**

机器学习概念：检查大型和多样化的数据集（*大数据*）以发现模式和做出决策的过程，将统计学应用于大数据。

## **二值转换**（也称为指示转换）

特征转换：将随机变量指示编码为相对于类别或阈值的概率。

如果 $i(\bf{u}:z_k)$ 是一个分类变量的指示器，

+   实现等于一个类别的概率是多少？

$$\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{如果 } Z(\bf{u}) = z_k \\ 0, & \text{如果 } Z(\bf{u}) \ne z_k \end{cases} \end{split}$$

例如，

+   给定阈值，$z_2 = 2$, 以及在 $\bf{u}_1$ 的数据，$z(\bf{u}_1) = 2$，那么 $i(bf{u}_1; z_2) = 1$

+   给定阈值，$z_1 = 1$, 以及一个远离数据的随机变量，$Z(\bf{u}_2)$，那么计算为 $F^{-1}_{\bf{u}_2}(z_1)$ 的随机变量，$i(\bf{u}_2; z_1) = 0.23$

如果 $I\{\bf{u}:z_k\}$ 是一个连续变量的指示器，

+   实现小于或等于阈值的概率是多少？

$$\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{如果 } Z(\bf{u}) \le z_k \\ 0, & \text{如果 } Z(\bf{u}) > z_k \end{cases} \end{split}$$

例如，

+   给定阈值，$z_1 = 6\%$, 以及在 $\bf{u}_1$ 的数据，$z(\bf{u}_1) = 8\%$，那么 $i(\bf{u}_1; z_1) = 0$

+   给定阈值，$z_4 = 18\%$, 以及一个远离数据的随机变量，$Z(\bf{u}_2) = N\left[\mu = 16\%,\sigma = 3\%\right]$，那么 $i(\bf{u}_2; z_4) = 0.75$

指示编码可以通过在每个位置的随机变量的指示变换在整个随机函数上应用。

## **提升模型**

梯度提升：添加多个弱学习器以构建更强的学习器。

+   弱学习器是一种仅提供略好于随机选择的预测的预测器

这就是用文字描述的方法，然后用方程表示，

+   构建一个误差率高的简单模型，模型可能非常不准确，但方向是正确的

+   从模型中计算误差

+   将另一个模型拟合到误差上

+   计算第一个和第二个模型添加的误差

+   重复执行，直到获得所需的精度或满足其他停止条件

现在用方程表示，从 $X_1,\ldots,X_m$ 预测 $Y$ 的一般工作流程是，

+   构建一个弱学习器来从 $X_1,\ldots,X_m$ 预测 $Y$，从训练数据 $x_{i,j}$ 预测 $\hat{F}_k(X)$。

+   对所需估计量数量进行循环，$k = 1,\ldots,K$

1.  计算训练数据中的残差，$h_k(x_{i}) = y_i - \hat{F}_k(x_{i})$

1.  将另一个弱学习器拟合到从 $X_1,\ldots,X_m$ 预测 $h_k$，从训练数据 $x_{i,j}$ 预测 $\hat{F}_k(X)$。

+   每个模型都是基于前一个模型来提高精度

回归估计量是对 $K$ 个简单模型的总和，

$$ \hat{Y} =\sum_{k=1}^{K} F_k(X_1,\ldots,X_m) $$

## **重抽样**

袋装树和随机森林：一种统计重抽样过程，用于从样本数据本身计算计算出的统计量的不确定性。一些一般性评论，

+   *有放回的抽样* - 从数据集的累积分布函数中进行 $n$（数据样本数）*蒙特卡洛模拟*，这导致数据的新实现

+   *模拟数据收集过程* - 基本思想是模拟原始数据收集过程。我们不是实际收集新的样本集，而是从数据中随机选择以获取数据实现

+   *重抽样任何统计量* - 这种方法非常灵活，因为我们可以从数据实现中计算出任何统计量的实现

+   *计算成本低* - 重复此方法以获取统计量的实现，以构建不确定性的完整分布。使用大量实现，$L$，以获得可靠的不确定性模型。

+   *计算整个不确定性的分布* - 对于任何统计量，你计算不确定性模型中的任何汇总统计量，例如，均值的均值、P10 和 P90

+   *机器学习的袋装法* - 是将重抽样应用于获取数据实现以训练预测模型实现，对预测模型集合进行聚合预测以减少模型方差

重抽样的局限性是什么？

+   偏差样本数据可能导致偏差的 bootstrapped 不确定性模型，你必须首先去除偏差，例如，*去簇化*

+   您必须拥有足够的样本量

+   仅整合空间样本稀疏性带来的不确定性

+   不考虑数据的空间背景，即样本数据位置、感兴趣区域的体积或空间连续性。存在一种称为[空间自举](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Spatial_Bootstrap.ipynb)的自举变体。

## **分类特征**

机器学习概念：一个只能取有限且通常固定数量的可能值的特征

## **分类名义特征**

机器学习概念：一个没有自然排序的*分类*特征，例如，

+   相岩 = \{灰岩，碎屑岩，砾岩，角砾岩\}

+   矿物 = \{石英，长石，方解石\}

## **分类有序特征**

机器学习概念：一个具有自然排序的*分类*特征，例如，

+   地质年代 = \{中新世，上新世，更新世\} - 从较老到较新的岩石排序

+   摩氏硬度 = $\{1, 2, \ldots, 10\}$ - 从较软到较硬的岩石排序

## **因果关系**

多元分析：一个变化直接导致一个或多个其他特征变化的关系

因果关系的一些重要方面，

1.  *不对称和时间优先* - $A$ 由 $B$ 导致并不表示 $B$ 由 $A$ 导致

1.  *非虚假* - 不是由于随机效应或混杂特征

1.  *机制和解释* - 有一个合理的机制或过程可以解释这种关系

1.  *一致性* - 该关系在一系列条件、时间、地点、人群等条件下是可观察的

1.  *强度* - 更强的关系在所有前 1-5 点都成立的情况下，增加了因果关系的可能性

建立因果关系非常困难，

+   在本课程中，我们通常避免因果关系和因果分析，并通过诸如“相关性不意味着因果关系”之类的陈述来强调这一点

## **基于细胞的解聚**

数据准备：一种将权重分配给基于局部采样密度的空间样本的解聚方法，使得加权统计更有可能代表总体。数据权重分配如下，

+   在密集采样区域中的样本权重较低

+   在稀疏采样区域中的样本权重较高

解聚的目标是使样本统计量独立于样本位置，例如，补充钻探或爆破孔样本不应因局部样本密度增加而改变感兴趣区域的统计量。

基于细胞的解聚过程如下：

1.  在空间数据上放置一个单元格网格，并将权重设置为与单元格中样本数量的倒数成比例

1.  单元网格大小是可变的，选择最小化去簇平均（在样本平均偏高时）或最大化去簇平均（如果样本平均偏低）的单元大小

1.  为了消除单元网格位置的影响，单元网格被随机移动几次，并对每个数据点的去簇权重进行平均

权重计算如下：

$$ w(\bf{u}_j) = \frac{1}{n_l} \cdot \frac{n}{L_o} $$

其中 $n_l$ 是当前单元中的数据数量，$L_o$ 是有数据的单元数量，$n$ 是数据总数。

这里有一些基于单元去簇的亮点，

+   根据名义样本间距（例如，在充填钻探前的数据间距）进行专家判断以分配单元大小，将提高基于最小或最大去簇平均（如上所述）的单元大小选择自动化方法的性能。

+   基于单元的去簇方法没有意识到感兴趣区域的边界；因此，感兴趣区域边界附近的数据可能看起来采样更稀疏，并得到更多的权重

+   基于单元的方法是由安德烈·约内尔教授在 1983 年开发的，[]

## **认知偏差**

机器学习概念：人类大脑用来简化从大量个人经验和学习偏好中获取的信息处理过程的自动化（潜意识）思维过程。虽然这些对于我们在地球上的进化和生存至关重要，但它们可能导致数据科学中的以下问题：

1.  *锚定偏差*，过分强调第一条信息。研究表明，当我们刚开始了解一个主题时，第一条信息可能是无关的，而且一个项目中最早的数据通常具有最大的不确定性。通过整理所有数据、整合不确定性、促进项目团队中的开放讨论和辩论来解决锚定偏差。

1.  *可得性启发式*，高估容易获得的信息的重要性，例如，祖父每天抽 3 包烟，活到 100 岁，即依赖于轶事。通过确保项目团队记录所有可用信息并应用定量分析来超越轶事来解决可得性启发式。

1.  *从众效应*，评估的概率随着持有相同信念的人数增加而增加。注意避免所有人在项目团队中一拥而上或最响亮的声音影响所有人。鼓励项目团队的所有成员贡献，甚至单独开会可能会有所帮助，以解决从众效应。

1.  *盲点效应*，未能看到自己的认知偏差。这是所有认知偏差中最难克服的。一个可能的解决方案是邀请对项目团队的方法、结果和决策进行无偏见审查。

1.  *选择支持偏差*，在做出承诺后概率增加，即做出决定。例如，我购买那辆车的决定通过关注汽车的正信息是好的。这是确认偏差的一个具体案例。

1.  *聚类错觉*，在随机事件中看到模式。是的，这个启发式方法在我们被大型预测者追捕时帮助我们生存下来，即，假阳性比假阴性要好得多！解决方案是建立不确定性置信区间，并测试所有数据和结果与随机效应。

1.  *确认偏差*，只考虑支持当前模型的新信息。选择支持偏差是确认偏差的一个具体案例。解决确认偏差的方法是寻找你可能会不同意的人，并组建具有不同技术观点和不同专家经验的熟练项目团队。我的方法是，如果房间里每个人都同意我，我会感到紧张！

1.  *保守偏差*，倾向于旧数据而非新收集的数据。数据整理和定量分析是有帮助的。

1.  *近期偏差*，倾向于最近收集的数据。确保你的团队记录以前的数据和选择，以增强团队记忆。就像保守偏差一样，数据整理和定量分析是我们的第一道防线。

1.  *幸存者偏差*，只关注成功案例。检查你的团队可用的数据中是否存在任何可能的预选或筛选。

统计学/数据分析的稳健使用可以保护用户免受偏差的影响。

## **互补事件**（概率）

概率概念：概率的 NOT 运算符，如果我们定义 A，那么 A 的补集，$A^c$，不是 A，我们得到这个结果闭包关系，

$$ P(A) + P(A^c) = 1.0 $$

互补事件可以考虑用于超出单变量问题之外，例如考虑这个双变量闭包，

$$ P(A|B) + P(A^c|B) = 1.0 $$

注意，给定的术语必须相同。

## **计算复杂度**

线性回归：表示方法所需的计算机资源，我们在机器学习中使用它来了解我们的机器学习方法如何随着维度、特征数量和训练数据数量的变化而缩放，表示为，

$$ 𝑂(𝑓(𝑛)) $$

其中 $𝑛$ 代表问题的大小。计算复杂度有 2 个组成部分，

+   *时间复杂度* - 指的是计算时间以及对于给定算法，时间随问题规模变化的缩放比例

+   *空间复杂度* - 指的是所需的计算机内存以及对于给定算法，存储随问题规模变化的缩放比例

例如，如果时间复杂度是 $O(n³)$，其中 $n$ 是训练数据数量，那么如果我们加倍数据数量，运行时间将增加八倍。

关于计算复杂度的其他显著点，

+   *默认最坏情况复杂度* - 对于特定问题规模的最坏情况复杂度，提供了一个上限

+   *渐近复杂度* - 其中 $𝑛$ 很大。一些算法对于小数据集有加速，这不被使用

+   假设所有步骤都是必需的，例如，数据没有预排序，等等。

时间复杂度示例，

+   *二次时间*，$𝑶(𝒏𝟐)$ - 例如，整数乘法，冒泡排序

+   *线性时间*，$𝑶(𝒏)$ - 例如，在未排序的数组中找到最小值或最大值

+   *分数幂*，$𝑶(𝒏^𝒄 )$ - 其中 $[0 < c < 1]$，例如，在 kd 树中搜索，$𝑂(𝑛^(\frac{1}{2}))$

+   *指数时间*，$𝑶(𝟐^𝒏)$ - 例如，使用动态规划的旅行商问题

## **条件概率**

概率概念：在另一个事件发生的情况下，事件的概率，

$$ P(A|B) = \frac{P(A,B)}{P(A)} $$

我们将这读作 A 在 B 发生的情况下发生的概率，即联合概率除以边缘概率。我们可以通过向任一组件添加联合来扩展条件概率到任何多元情况。例如，

$$ P(C|B,A) = \frac{P(A,B,C)}{P(B,C)} $$

## **置信区间**

线性回归：将总结统计量或模型参数的不确定性表示为范围，下限和上限，基于称为置信水平的指定概率区间。

我们这样传达置信区间，

+   有 95%的概率（或者说 20 次中的 19 次）模型斜率在 0.5 和 0.7 之间。

关于置信区间的其他显著点，

+   当可用时，通过分析方法计算，或使用更通用和灵活的 bootstrap

+   对于贝叶斯方法，我们参考可信度区间

## **混淆矩阵**

朴素贝叶斯：一个矩阵，表示预测（x 轴）与实际（y 轴）类别频率，以可视化分类模型的性能。

+   通过分类模型可视化并诊断所有正确和错误分类的组合，例如，类别 1 经常被错误分类为类别 3。

+   完美准确率是每个类别在主对角线上的数量，类别 1 总是被预测为类别 1，等等。

+   将分类矩阵应用于计算单个分类准确性的总结，例如，精确度、召回率等。

## **连续特征**

机器学习概念：一个可以取介于下限和上限之间任何值的特征。例如，

+   孔隙率 = $\{13.01\%, 5.23\%, 24.62\%\}$

+   金品位 = $\{4.56 \text{ g/t}, 8.72 \text{ g/t}, 12.45 \text{ g/t} \}$

## **连续，区间特征**

机器学习概念：一个*连续特征*，其中数字之间的间隔是相等的，例如，1.50 和 2.50 之间的差值与 2.50 和 3.50 之间的差值相同，但实际值没有客观的物理现实（存在于任意尺度上），即没有真正的零点，例如，

+   摄氏温度尺度（基于水在 0℃结冰和 100℃沸腾的任意尺度）

+   公历年份（没有真正的零年）

我们可以使用加法和减法运算来比较连续的区间特征。

## **连续的比率特征**

机器学习概念：一个 *连续特征*，其中数字之间的间隔是相等的，例如，1.50 和 2.50 之间的差值与 2.50 和 3.50 之间的差值相同，但数值确实具有客观现实性（衡量实际物理现象），即，确实有真正的零点，例如，

+   开尔文温度尺度

+   孔隙率

+   透水性

+   饱和

由于存在真正的零点，连续的比率特征可以通过乘法和除法数学运算（除了加法和减法）进行比较，例如，孔隙率是两倍。

## **连续可微**

机器学习训练和调优：一个函数如果满足两个关键条件，则它是连续可微的：

1.  函数是可微的，函数在其定义域内的每一点都存在导数，即函数在每一个可能的点上都有一个明确的斜率。

1.  导数是连续的，函数的导数没有跳跃、不连续或突然变化，即，导数函数在其定义域的每一点上都是连续的。

对于一个机器学习示例，

+   $L²$ 范数的导数是连续可微的，因此对于线性回归和岭回归，我们可以对损失函数应用偏导数来计算模型参数训练的闭式解。

+   $L¹$ 范数的导数不是连续可微的，因此对于 LASSO 回归，我们不能对损失函数应用偏导数来计算模型参数训练的闭式解。我们必须使用迭代优化来训练模型参数。

## **卷积**

k-最近邻算法：两个函数的积分乘积，其中一个函数经过反转并平移 $\Delta$。

+   一种解释是将加权函数 $𝑓(\Delta)$ 应用于平滑函数，以计算函数 $𝑔(x)$ 的加权平均值，

$$ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta $$

这很容易扩展到多维

$$ (f * g)(x, y, z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(\Delta_x, \Delta_y, \Delta_z) g(x - \Delta_x, y - \Delta_y, z - \Delta_z) \, d\Delta_x \, d\Delta_y \, d\Delta_z $$

在积分之前选择哪个函数进行平移不会改变结果，卷积算子具有交换性。

$$ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta $$$$ (f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta $$

+   如果任一函数被反射，则卷积等价于互相关，作为位移函数的信号相似度度量。

## **核心数据**

机器学习概念：直接测量地下资源（回收的钻屑也是直接测量，具有更大的不确定性和较小的、不规则的比例）的主要采样方法。对核心数据的评论，

+   对于石油和天然气来说，收集成本高昂/耗时，会中断钻井作业，覆盖稀疏且具有选择性（非常偏颇）

+   在采矿（钻石钻探孔）中非常常见，用于具有规则图案和紧密间距的品位控制

+   重力、活塞等取心方法用于在湖泊和海洋中取样沉积物

我们从核心数据中学到了什么？

+   岩性特征（沉积结构，矿物等级），岩石物理特征（孔隙度，渗透率），以及力学特征（弹性模量，泊松比）

+   通过井和钻探孔之间的插值来获取地层和矿体几何形状

核心数据对于支持地下资源解释至关重要。它们是整个储层概念和预测框架的锚点，

+   例如，与井日志数据同位的核心数据用于校准（地面真实）岩性，从井日志中获取孔隙度

## **相关性**

多元分析：皮尔逊积矩相关系数是线性关系程度的度量，

$$ \rho_{x,y} = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{(n-1)\sigma_x \sigma_y} $$

其中 $\overline{x}$ 和 $\overline{y}$ 是特征 $x$ 和 $y$ 的均值。该度量被限制在 $$$-1,1$$$。

+   相关系数是标准化的协方差

佩尔逊相关系数对异常值和偏离线性行为（在双变量意义上）非常敏感。我们有一个称为斯皮尔曼秩相关系数的替代方案，

$$ \rho_{R_x R_y} = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i} - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le 1.0 $$

排序相关系数在计算相关系数之前将数据应用排序变换。要计算排序变换，只需将数据值替换为排名 $R_x = 1,\dots,n$，其中 $n$ 是最大值，$1$ 是最小值。

$$ x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall \, i \gt j $$$$ R_{x_i} = i $$

## **协方差**

多元分析：衡量两个特征如何一起变化，

$$ C_{x,y} = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{(n-1)} $$

其中 $\overline{x}$ 和 $\overline{y}$ 是特征 $x$ 和 $y$ 的均值。该度量被限制在 $$$-\sigma_x \cdot \sigm_y,\sigma_x \cdot \sigm_y$$$。

+   相关系数是标准化的协方差

佩尔森相关系数对异常值和偏离线性行为（在双变量意义上）非常敏感。我们有一个称为斯皮尔曼秩相关系数的替代方案，

$$ \rho_{R_x R_y} = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i} - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le 1.0 $$

排序相关系数在计算相关系数之前将秩变换应用于数据。要计算秩变换，只需将数据值替换为秩 $R_x = 1,\dots,n$，其中 $n$ 是最大值，$1$ 是最小值。

$$ x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall \, i \gt j $$$$ R_{x_i} = i $$

## **交叉验证**

机器学习概念：在模型参数训练中保留部分数据以测试模型预测未用于训练模型案例的能力

+   这通常通过训练和测试数据分割来完成，其中 15% - 30% 的数据分配给测试

+   作为现实世界模型使用的彩排，训练-测试分割必须是公平的，从而产生与计划使用模型相似的预测难度

+   存在更复杂的设计，如 k 折交叉验证，它允许通过 k 折每折都训练模型来测试所有数据

+   交叉验证可以应用于检查模型性能以评估估计精度（最常见）和不确定性模型的好坏（[Maldonado-Cruz and Pyrcz, 2021](https://www.sciencedirect.com/science/article/pii/S0920410521006343)）

## **累积分布函数** (CDF)

单变量分析：离散概率密度函数（PDF）的总和或连续概率密度函数（PDF）的积分。以下是重要概念，

+   累积分布函数（CDF）表示为 $F_x(x)$，注意概率密度函数（PDF）表示为 $f_x(x)$

+   是随机样本 $X$ 小于或等于特定值 $x$ 的概率；因此，y 轴是累积概率

$$ F_x(x) = P(X \le x) = \int_{-infty}^x f(u) du $$

+   对于累积分布函数（CDF），没有箱假设；因此，箱的分辨率与数据相同。

+   单调不减函数，因为负斜率会表明在区间上有负概率。

有效累积分布函数（CDF）的要求包括，

1.  非负约束：

$$ F_x(x) = P(X \le x) \ge 0.0, \quad \forall x $$

1.  有效概率：

$$ 0.0 \le F_x(x) \le 1.0, \quad \forall x $$

1.  不能有负斜率：

$$ \frac{dF_x(x)}{dx} \ge 0.0, \quad \forall x $$

1.  最小和最大（确保概率封闭）值：

$$ \text{min}(F_x(x)) = 0.0 \quad \text{max}(F_x(x)) = 1.0 $$

## **维度诅咒**

特征排序：与处理许多特征相关的一系列挑战，即高维空间，包括，

+   在高维空间中无法可视化和模型数据

+   通常在广阔的高维空间中，采样不足以进行统计推断

+   高维预测特征空间的低覆盖度

+   扭曲的特征空间，包括由角和距离主导的扭曲空间，距离失去敏感性

+   随着维度的增加，特征之间的多重共线性更可能

## **数据**（数据方面）

特征排序：在描述空间数据集时，这些是基本方面，

*数据覆盖范围* - 对于这次调查，有多少比例的人口被采样？

通常，硬数据具有高分辨率（小规模、体积支持），但数据覆盖范围较差（仅测量人口中的极小比例，例如，

+   *深水油气核心覆盖范围* - 井筒核心仅采样深水储层的一亿五千万分之一到一亿五千万分之一，假设 3 英寸直径的核心在垂直井中以 500 米到 1500 米的间距有 10%的核心覆盖

+   *核心覆盖采矿级控制* - 钻孔核心样品采样矿石体积的八万分之一到三十万分之一，假设 HQ 63.5 毫米直径的核心在垂直钻孔中以 5 米到 10 米的间距有 100%的核心覆盖

软数据往往具有出色的（通常是完整的）覆盖范围，但分辨率低，

+   *地震反射调查和重力测量调查* - 数据通常在整个感兴趣体积中可用，但分辨率低，并且通常随着深度的增加而降低

*数据规模*（支持大小）- 单个样本采样的规模或体积是什么？例如，

+   核样品在孔隙尺度上的核磁共振图像，1 - 50 $\mu m$

+   以 0.3 米间隔在离井筒 1 米处采样的伽马射线测井

+   以 20 米 x20 米 x100 米分辨率的地面重力梯度测量图

*数据信息类型* - 数据告诉我们关于地下结构什么信息？例如，

+   可用于校准渗透率和饱和度的粒度分布

+   流体类型以评估油水接触点位置

+   重要的储层层段的倾角和连续性，以获取连通性

+   矿石品位以绘制高、中、低品位矿石壳体，用于矿山规划

## **数据凸性**

基于密度的聚类：如果欧几里得特征空间的一个子集 $A$ 对于 $A$ 内的任意两点 $𝑥_1$ 和 $𝑥_2$，连接这些点的整个线段都在 $A$ 内，则 $A$ 是凸的，$\left[𝑥_1,𝑥_2\right] \in A$。

## **DataFrame**

机器学习工作流程构建和编码：一个方便的 Pandas 类，用于处理具有每行一个样本和每列一个特征的表格数据，因为，

+   方便的数据结构，用于存储、访问、操作表格数据

+   内置从各种文件类型、Python 类甚至直接从 Excel 加载数据的方法

+   内置计算汇总统计和可视化数据的方法

+   内置的数据查询、排序、数据过滤方法

+   内置的数据操作、清理、重新格式化方法

+   内置属性用于存储有关数据的信息，例如大小、空值数量和空值

## **数据分析**

机器学习概念：使用统计与可视化来支持决策。

+   Pyrcz 博士表示，数据分析与统计学相同。

## **数据准备**

机器学习概念：任何增强、改进原始数据以准备模型的工作流程步骤。

+   数据驱动科学需要数据，数据准备仍然至关重要

+   $\gt >80\%$的任何地下研究都是数据准备和解释

我们继续面临数据挑战：

+   数据整理 - 格式标准、版本控制、存储、传输、安全和文档

+   管理大量数据 - 可视化、可用性和数据挖掘与探索

+   大量的元数据 - 缺乏平台、标准和格式

+   工程集成、数据多样性、规模、解释和不确定性

清洁数据库是所有数据分析和机器学习的先决条件

+   必须从这个基础开始

+   输入垃圾，输出垃圾

## **度矩阵**（谱聚类）

谱聚类：表示图的一个矩阵，每个图节点、样本的连接数。

+   对角矩阵，用整数表示连接数

## **基于密度的聚类 DBSCAN**

基于密度的聚类：一种基于密度的聚类算法，簇在特征空间中以超参数确定的足够点密度位置生成或扩展。

+   $\epsilon$ – 在归一化特征空间中局部邻域的半径。这是簇的规模/分辨率。如果这个值设置得太小，太多的样本会被视为异常值；如果设置得太大，所有簇将合并成一个单一的簇。

+   $min_{Pts}$ – 分配核心点所需的最小点数，其中核心点用于初始化或扩展簇组。

密度通过体积内的样本数量来量化，其中体积基于特征空间所有维度的半径。

通过 k 距离图（在这种情况下是 k 最近邻）可以进行自动或引导的$\epsilon$参数估计。

1.  计算所有样本数据（在这种情况下为 1,700）在归一化特征空间中的最近邻距离。

1.  按升序排序并绘图。

1.  选择最大化正曲率的距离（即拐点）。

这里是 DBSCAN 聚类的显著方面的总结，

+   *DBSCAN* - 代表基于密度的空间聚类应用噪声（Ester 等，1996）。

+   *优点* - 包括最小领域知识来估计超参数，能够表示任意形状的簇组，并且在大数据集上应用高效

+   *自底向上/聚合聚类* – 所有数据样本最初都是独立的组，称为“未访问”，但实际上在分配到组之前被视为异常值，然后聚类组迭代增长。

+   *互斥性* – 与 k-means 聚类一样，所有样本只能属于单个聚类组。

$$ P(C_i \cap C_j | i \ne j) = 0.0 $$

+   *非穷尽性* – 一些样本可能被留下未分配，并假设为聚类组分配的异常值

$$ P(C_1 \cup C_2 \cup \dots C_k) \le 1.0 $$

## **决策标准**

机器学习概念：通过应用传递函数到地下模型（s）来计算的特征，以支持决策。决策标准代表价值、健康、环境和安全。例如：

+   污染物回收率以支持泵和土壤修复项目的工程设计

+   地下油资源以确定是否应开发水库

+   洛伦兹系数异质性度量用于分类水库并确定成熟类比

+   回收因子或生产率以安排生产和确定最佳设施

+   回收的矿物品位和吨数以确定经济最终矿坑壳体

## **决策树**

决策树：一个直观的、回归和分类的预测机器学习模型，它将预测空间 $𝑋_1,…,𝑋_𝑚$ 划分为 $𝐽$ 个互斥、穷尽的区域，$𝑅_𝑗$。

+   *互斥性* – 任何预测因子的任何组合只属于单个区域，$𝑅_𝑗$

+   *穷尽性* – 所有预测因子的所有组合属于一个区域，$𝑅_𝑗$，区域覆盖整个特征空间，考虑的变量的范围

每个区域的相同预测，该区域的训练数据平均值，$\hat{Y}(𝑅_𝑗) = \overline{Y}(𝑅_𝑗)$

+   对于分类，最常见的是基于模式的或 argmax 运算符

决策树的其他显著特点，

+   *监督学习* - 响应特征标签 $Y$ 在训练和测试数据中可用

+   *层次，二进制分割* - 预测特征空间，从 1 个区域开始，然后依次分割，创建新的区域

+   *紧凑、可解释的模型* - 由于分类是基于特征空间的二进制分割的层次结构（每次一个特征），因此模型可以用直观的方式指定为具有二叉分支的树，因此得名决策树。该模型的代码是嵌套的 if 语句，例如，

```py
if porosity > 0.15:
    if brittleness < 20:
        initial_production = 1000
    else:
        initial_production = 7000
else:
    if brittleness < 40:
        initial_production = 500
    else:
        initial_production = 3000 
```

决策树是从上到下构建的。我们从一个覆盖整个特征空间的单个区域开始，然后进行一系列的分割，

+   *扫描所有可能的分割* - 在所有区域和所有特征上。

+   *贪婪优化* - 通过在任意特征中找到最佳分割来减少所有训练数据 $y_i$ 在所有区域 $j = 1,\ldots,J$ 上的残差平方和。后续分割之间没有共享其他信息。

$$ RSS = \sum^{J}_{j=1} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})² $$

超参数包括，

+   *区域数量* – 非常容易理解，你知道模型会是什么样子

+   *最小均方误差减少量* – 可以提前停止，例如，RSS 减少量小的分割可能导致后续分割有更大的 RSS 减少量

+   *每个区域的* **最小训练数据量** – 与区域均值预测的准确性概念相关，即我们需要至少 𝑛 个数据来获得可靠的均值

+   *最大层数* – 强制对称树，到达每个区域的分割数量相似

## **去簇**

数据准备：根据局部采样密度为空间样本分配权重的各种方法，使得加权统计量更有可能代表总体。数据权重分配使得，

+   在密集采样区域采样的权重较低

+   在稀疏采样区域采样的权重较高

存在多种去簇方法：

+   *基于单元格的去簇*

+   *多边形去簇*

+   *基于克里金的去簇*

重要的是要注意，没有任何去簇方法可以证明对于每个数据集，结果加权统计量都会提高总体参数的预测，但在期望中，这些方法往往可以减少偏差。

## **去簇**（统计学）

数据准备：一旦为空间数据集计算了去簇权重，则将去簇统计应用于后续分析或建模的输入。例如，

+   将去簇均值分配为简单克里金法的平稳、全局均值

+   将所有带权重的数据点的累积分布函数应用于顺序高斯模拟，以确保反变换后的实现接近去簇分布

任何统计量都可以加权，包括整个累积分布函数！以下是一些加权统计量的示例，给定去簇权重 $w(\bf{u}_j)$，对于所有数据 $j=1,\ldots,n$。

+   加权样本均值，

$$ \overline{x}_{wt} = \frac{\sum_{i=1}^n w(\bf{u}_j) \cdot z(\bf{u}_j)}{\sum_{i=1}^n w(\bf{u}_j)} $$

其中 $n$ 是数据点的数量。

+   加权样本方差，

$$ s²_{x_{wt}} = \frac{1}{\sum_{i=1}^n w(\bf{u}_j) - 1} \cdot \sum_{i=1}^n w(\bf{u}_j) \cdot \left( x(\bf{u}_j) - \overline{x}_{wt} \right)² $$

其中 $\overline{x}_{wt}$ 是去簇均值。

+   加权协方差，

$$ C_{x,y_{wt}} = \frac{1}{\sum_{i=1}^n w(\bf{u}_j) } \cdot \sum_{i=1}^n w(\bf{u}_j) \cdot \left( x(\bf{u}_j) - \overline{x}_{wt} \right) \cdot \left( y(\bf{u}_j) - \overline{y}_{wt} \right) $$

其中 $\overline{x}_{wt}$ 和 $\overline{y}_{wt}$ 是特征 $X$ 和 $Y$ 的去簇均值。

+   整个累积分布函数，

$$ F_z(z) \approx \sum_{j=1}^{n(Z<z)} w(\bf{u}_j) $$

其中 $n(Z<z)$ 是小于阈值 $z$ 的排序升序数据的数量。我们将其表示为近似值，因为这简化了，并且是在数据分辨率和没有插值模型的情况下。

需要注意的是，没有任何去簇方法可以证明对于每个数据集，结果加权统计量都会提高对总体参数的预测，但在期望中，这些方法倾向于减少偏差。

## **密度连接** (DBSCAN)

基于密度的聚类：如果存在一个点 $Z$，它从点 $A$ 和 $B$ 都密度可达，则点 $A$ 和 $B$ 是密度连接的。

## **基于密度的聚类** (DBSCAN)

基于密度的聚类：一个非空集合，其中所有点都相互密度连接。

## **密度可达** (DBSCAN)

基于密度的聚类：如果 $Y$ 属于一个可以从 $A$ 到达的核心点的邻域，则点 $Y$ 是从 $A$ 密度可达的。这需要一串核心点，每个核心点都属于前一个核心点，最后一个核心点包括点 $Y$。

## **确定性模型**

机器学习概念：假设系统或过程是完全可预测的模型

+   通常基于工程和地球科学物理学以及专家判断

+   例如，数值流动模拟或从地震解释的层序边界表面

+   对于这门课程，我们还声明，数据驱动估计模型如

优点：

+   物理和专家知识的整合

+   各种信息源的整合

缺点：

+   通常相当耗时

+   通常不评估不确定性，专注于构建一个模型

## **降维**

主成分分析：在数据科学工作流程中减少特征数量的方法。有两种主要方法，

+   *特征选择* – 找到对问题最重要的原始特征子集

+   *特征投影* – 将数据从高维空间转换到低维空间

也称为降维或维度缩减

+   由维度的诅咒和多共线性所激发

+   应用于统计学、机器学习和信息理论

## **直接密度可达** (DBSCAN)

基于密度的聚类：如果 $A$ 是核心点且 $X$ 属于 $A$ 的邻域，距离 $A$ 小于等于 $\epsilon$，则点 $X$ 是从 $A$ 直接密度可达的。

## **离散特征**

机器学习概念：一个 *分类特征* 或一个 *连续特征*，它被分箱或分组，例如，

+   孔隙率在 0% 到 20% 之间分配到 10 个区间 = {0 - 2%，2% - 4%，...，20%}

+   摩氏硬度 = $\{1, 2, \ldots, 10\}$（与 *分类特征* 相同）

## **分布变换**

特征变换：通过百分位数从一个分布映射到另一个分布的映射，从而产生新的直方图、PDF 和 CDF。我们在地统计方法和工作流程中执行分布变换，因为，

+   *推断* - 将特征分布校正到期望的形状，例如，校正数据过少或偏差

+   *理论* - 工作流程步骤需要特定的分布假设，例如，对于顺序高斯模拟，需要均值为 0.0 和方差为 1.0 的高斯分布

+   *数据准备或清理* - 校正异常值，变换将异常值映射到目标分布，而不再是异常值

我们如何执行分布变换？

我们将累积分布函数（CDF）$F_{X}$的值转换到新的 CDF $G_{Y}$。这可以通过对所有样本数据应用分位数-分位数变换进行推广：

+   正向变换：

$$ Y = G_{Y}^{-1}(F_{X}(X)) $$

+   逆向变换：

$$ X = F_{X}^{-1}(G_{Y}(Y)) $$

这可以应用于任何数据，包括参数或非参数分布。我们只需要能够通过百分位数将一个分布映射到另一个分布，因此它是一个：

+   保留排名的变换，例如，在分布变换后 P25 仍然是 P25

## **急切学习**

k-最近邻：模型是查询之前构建的训练数据的泛化

+   模型在参数训练和超参数调整后输入独立，即，不需要训练数据即可进行新的预测

相反的是懒惰学习。

## **估计**

机器学习概念：是在未采样位置或时间获取表示特征的单个最佳值的过程。一些额外的概念，

+   局部精度优先于全局空间变异性

+   过于平滑，不适合对异质性敏感的任何变换函数

+   例如，逆距离和克里金法

+   许多预测机器学习模型专注于估计（例如，k-最近邻、决策树、随机森林等）

## **f1 分数**（分类准确度指标）

朴素贝叶斯：分类预测模型的准确性度量，混淆矩阵中每个 $k$ 类别的单一汇总指标。

+   回收率和精度的调和平均值

$$ f1-score_k = \frac{2} { \frac{1}{Precision_k} + \frac{1}{Recall_k} } $$

作为提醒，

+   *回收率* - 测试数据集中该类所有案例中真实正例与所有案例的比率

+   *精度* - 真正例与所有正例（真正例 + 假正例）的比率

$$ Recall_k = \frac{ n_{k, \text{true positives}} }{n_k} $$

## **特征**（也称为变量）

机器学习概念：在研究中测量的或观察到的任何属性

+   例如，孔隙率、渗透率、矿物浓度、饱和度、污染物浓度等。

+   在数据挖掘/机器学习领域，这被称为特征，统计学家将这些变量称为

+   测量通常需要大量的分析、解释等。

+   当特征被修改和组合以提高我们的模型时，我们称之为特征工程

## **特征工程**

特征转换：利用领域专业知识从原始数据中提取改进的预测或响应特征，

+   提高推理或预测机器学习的性能、准确性和收敛性

+   提高模型的可解释性（或者如果我们的工程特征处于不熟悉的单位，可能会降低可解释性）

+   减少异常值和偏差，与高斯性、线性化、维度扩展等假设保持一致

特征转换和特征选择是特征工程两种形式。

## **特征重要性**

特征排名：提供特征排名度量的各种机器学习方法，例如决策树通过包含每个特征来总结均方误差的减少，总结如下，

$$ FI(x) = \sum_{t \in T_f} \frac{N_t}{N} \Delta_{MSE_t} $$

其中 $T_f$ 是所有以特征 $x$ 作为分割的节点，$N_t$ 是达到节点 $t$ 的训练样本数量，$N$ 是数据集中样本的总数，$\Delta_{MSE_t}$ 是 $t$ 分割带来的 MSE 减少量。

注意，特征重要性可以像上面的 MSE 一样以类似的方式计算，对于具有 *基尼不纯度* 的分类树。

特征重要性是模型基于特征排名的一部分，

+   特征重要性的准确性取决于模型的准确性，即不准确的模型可能会提供错误的特征重要性

## **特征插补**

特征插补：在数据表中替换空值，对于所有特征没有值的样本，用合理的值替换，原因有二，

+   使需要完整数据表的统计计算和模型成为可能，即不能处理缺失特征值

+   最大化模型准确性，增加可用于训练和测试模型的可靠样本数量

+   减少可能因特征值中类似删除而发生的模型偏差，当特征值不是随机缺失时

特征插补方法包括，

+   *常量值插补* - 使用特征均值或众数替换空值

+   *基于模型的插补* - 使用相同样本的可用特征值来预测缺失特征，并用预测值替换空值

还有一些依赖于收敛的迭代方法，

+   *链式方程多重插补 (MICE)* - 分配随机值，然后迭代缺失值预测新值

该方法的目标是获得合理的估计值，这些值考虑了所有特征以及所有可用和缺失值之间的关系

## **特征投影**

主成分分析：将原始 $m$ 个特征转换为 $p$ 个特征，其中 $p << m$ 用于降维

+   给定特征 $𝑋_1,\ldots,𝑋_𝑚$，我们需要 $\binom{m}{2} = \frac{𝑚(𝑚−1)}{2}$ 散点图来可视化二维散点图

+   这些表示无法捕捉 $> 2$ 维结构

+   一旦我们有 4 个或更多变量，理解数据就变得非常困难。回忆维度诅咒。

+   主成分分析、多维缩放和随机投影是例子

+   特征选择是降维的另一种方法。

## **特征空间**

特征排序：通常特征空间仅指预测特征，不包括响应特征（即，

+   我们需要做出预测的所有可能的预测特征组合

+   可能被称为预测特征空间。

通常，我们在预测特征空间上训练和测试机器的预测。

+   该空间通常是超立方体，每个轴代表一个预测特征，从每个预测特征的最小值延伸到最大值

+   预测特征空间的更复杂形状是可能的，例如，我们可以屏蔽或删除数据覆盖较差的子集。

## **特征排序**

特征排序：特征工程的一部分，特征排序是一组方法，根据每个特征包含的信息量及其在预测响应特征中的重要性或价值来分配相对重要性或价值。

完成此任务有各种各样的可能方法。我的建议是采用多种度量标准的方法，同时理解每种方法的假设和局限性。

这里是我们将考虑的特征排序的一般类型指标：

+   *视觉检查* - 包括数据分布、散点图和小提琴图

+   *统计摘要* - 相关性分析、互信息

+   *基于模型* - 包括模型参数、特征重要性分数和全局 Shapley 值

+   *递归特征消除* - 以及其他通过保留测试数据交叉验证进行试错以找到最佳参数集的方法

特征排序主要是由维度诅咒驱动的，即，使用最少、最有信息量的预测特征。

## **特征变换**

特征变换：一种特征工程，涉及对特征应用数学运算以改进特征在工作流程中的价值。例如，

+   特征截断

+   特征归一化或标准化

+   特征分布变换

我们可能有多种原因想要执行特征变换。

+   使特征一致以便于可视化和比较

+   避免偏差或为依赖于预测特征空间中计算的距离的方法（例如 k 近邻回归）施加特征权重

+   该方法要求变量具有特定的范围或分布：

    +   人工神经网络可能要求所有特征的范围从 [-1,1]

    +   部分相关系数需要高斯分布。

    +   统计测试可能需要特定的分布

    +   地统计学顺序模拟需要指示器或高斯变换

特征变换是许多机器学习工作流程中的常见基本构建块。

## **第四范式**

机器学习概念：从数据驱动的范式进行科学发现构建，

+   第一范式 - 经验科学 - 实验和观察

+   第二范式 - 理论科学 - 分析表达式

+   第三范式 - 计算科学 - 数值模拟

我们通过增加新的科学范式，而不是替换旧范式。每个先前的范式都由先前的范式支持，例如，

+   理论科学建立在经验科学之上

+   数值模拟将实验中的分析表达式和校准方程整合在一起

## **频率派概率**

概率概念：基于从实验中观察到的频率来衡量事件发生的可能性。对于随机实验和定义良好的设置（例如抛硬币），

$$ \text{Prob}(A) = P(A) = \lim_{n \to \infty} \frac{n(A)}{n} $$

其中：

$n(A)$ = 事件 $A$ 发生的次数 $n$ = 试验次数

例如，下一口井可能干涸的可能性，在某个位置 ($\bf{u}_{\alpha}$) 遇到砂岩，在某个位置 ($\bf{u}_{\alpha}$) 超过 $15 \%$ 的岩石孔隙率。

## **高斯畸变**

特征变换：将分位数变换为高斯分布。

通过它们的累积概率映射特征值。

$$ y = G_y^{-1}\left( F_x(x)\right) $$

其中 $𝐹_𝑥$ 是原始特征累积分布函数 (CDF) 和 $𝐺_𝑦$ 是高斯 CDF 概率密度函数

$$ f(x) = \frac{1}{\sigma \sqrt{2 \pi}} exp \left[-1 \frac{1}{2} \left(\frac{x-\mu}{\sigma} \right)² \right] $$

正态分布的缩写是

$$ N[\mu,\sigma²] $$

例如 $N[0,1]$ 是标准正态分布

+   大部分自然变异或测量误差是高斯的

+   参数化完全由均值、方差和相关性系数（如果多元）决定

+   分布是无界的，没有最小值也没有最大值，极端值非常不可能，通常应用某种截断

警告，许多工作流程应用单变量高斯变形然后假设双变量或多变量高斯，这是不正确的，但通常很难将我们的数据转换为多变量高斯。

需要高斯分布的方法。

+   当数据是多元高斯分布时，皮尔逊积矩相关系数完全描述了多元关系。

+   部分相关需要双变量高斯分布。

+   顺序模拟（地统计学）假设高斯分布以再现全局分布。

+   均值差异的 Student's t 检验。

+   卡方分布是从高斯分布随机变量的平方和推导出来的。

+   高斯朴素贝叶斯分类假设条件为高斯分布。

## **Gibbs 采样器** (MCMC)

贝叶斯线性回归：一组算法，用于从概率分布中采样，使得样本匹配分布统计，基于，

+   依次从条件分布中进行采样。

由于只需要条件概率密度函数，系统简化为不需要完整的联合概率密度函数。

这里是 Gibbs MCMC 采样器双变量情况的基本步骤。

1.  为 $𝑋(0)$、$𝑌(0)$ 分配随机值。

1.  从 $𝑓(𝑋|𝑌(0))$ 中采样以获得 $𝑋(1)$。

1.  从 $𝑓(𝑌|𝑋(1))$ 中采样以获得 $𝑌(1)$。

1.  对样本重复下一步，$\ell = 1,\ldots,𝐿$。

产生的样本将具有正确的联合分布。

$$ 𝑓(𝑋,𝑌) $$

## **梯度提升模型**

梯度提升：将提升模型作为梯度下降问题提出的结果预测模型。

在每个步骤 $k$，一个模型正在被拟合，然后计算误差 $h_k(X_1,\ldots,X_m)$。

我们可以分配一个损失函数。

$$ L\left(y,F(X)\right) = \frac{\left(y - F(X)\right)²}{2} $$

因此，我们希望最小化 $\ell2$ 损失函数：

$$ J = \sum_{i=1}^{n} L\left(y_i, F_k(X) \right) $$

通过调整我们的模型结果来适应我们的训练数据 $F(x_1), F(x_2),\ldots,F(x_n)$。

我们可以取误差相对于我们的模型的偏导数。

$$ \frac{\partial J}{\partial F(x_i)} = F(x_i) - y_i $$

我们可以将残差解释为负梯度。

$$ y_i - F(x_i) = -1 \frac{\partial J}{\partial F(x_i)} $$

因此，我们现在有一个梯度下降问题：

$$ F_{k+1}(X_i) = F_k(X_i) + h(X_i) $$$$ F_{k+1}(X_i) = F_k(X_i) + y_i - F_k(X_i) $$$$ F_{k+1}(X_i) = F_k(X_i) - 1 \frac{\partial J}{\partial F_k(X_i)} $$

通用形式如下：

$$ \phi_{k+1} = \phi_k - \rho \frac{\partial J}{\partial \phi_k} $$

其中 $phi_k$ 是当前状态，$\rho$ 是学习率，$J$ 是损失函数，而 $\phi_{k+1}$ 是我们估计器的下一个状态。

训练数据中的误差残差是梯度，因此我们正在进行梯度下降。

+   对负梯度进行一系列模型的拟合。

通过将问题视为梯度下降问题，我们能够应用各种损失函数，

+   $\ell2$ 是我们的 $\frac{\left(y - F(X)\right)²}{2}$，实际应用中是可行的，但对外部异常值不稳健

$$ - 1 \frac{\partial J}{\partial F_k(X_i)} = y_i - F_k(X_i) $$

+   $\ell1$ 是我们的 $|y - F(X)|$，对外部异常值更稳健

$$ - 1 \frac{\partial J}{\partial F_k(X_i)} = sign(y_i - F_k(X_i)) $$

+   还有其他一些，如 Huber 损失

## **图拉普拉斯算子**（谱聚类）

谱聚类：通过整合图节点之间的连接来表示图的矩阵，包括每个图节点和样本的连接数。计算为度矩阵减去邻接矩阵。其中，

+   *度矩阵*，$𝐷$ - 每个节点的连接度

+   邻接矩阵，$𝐴$ - 节点之间的特定连接

## **地理统计学**

机器学习概念：应用统计学的一个分支，它整合了：

1.  空间（地质）背景

1.  空间关系

1.  体积支持/尺度

1.  不确定性

我将所有空间统计都包含在地理统计学中，有些人不同意我的观点。根据我的经验，任何有用的用于建模空间现象的统计方法都被采用并添加到地理统计学工具包中！地理统计学是一个不断发展和演变的研究领域。

## **基于梯度的优化**

LASSO 回归：通过迭代最小化损失函数来求解模型参数的方法。步骤包括，

1.  从随机的模型参数开始

1.  计算模型参数的损失函数

1.  计算损失函数的梯度，通常没有损失函数的方程，通过数值计算局部损失函数的导数进行采样，

$$ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) = \frac{L(y_{\alpha}, F(X_{\alpha}, b_1 - \epsilon)) - L(y_{\alpha}, F(X_{\alpha}, b_1 + \epsilon))}{2\epsilon} $$

1.  通过沿斜坡/梯度下降来更新参数估计，

$$ \hat{b}_{1,t+1} = \hat{b}_{1,t} - r \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) $$

其中 $r$ 是学习率/步长，$\hat{b}(1,𝑡)$ 是当前模型参数估计，而 $\hat{b}(1,𝑡+1)$ 是更新后的参数估计。

一些关于基于梯度的优化的重要评论，

+   *梯度搜索收敛* - 该方法将找到局部或全局最小值

+   *梯度搜索步长* - 步长的影响，$r$ 太小，需要太长时间收敛到解，而 $r$ 太大，解可能跳过/错过全局最小值或发散

+   *多个模型参数* - 在多个模型参数上计算和分解梯度，以向量表示。

$$ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1, b_2)) = \left[ \begin{matrix} \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) & \nabla L(y_{\alpha}, F(X_{\alpha}, b_2)) \end{matrix} \right] $$

+   *参数空间探索* - 训练机器学习模型参数的优化是探索高维模型参数空间

## **图**（光谱聚类）

光谱聚类：一种以有组织的方式表示数据的图表，每个样本作为一个节点，顶点表示样本之间的成对关系。

+   对于无向图，顶点是双向的，即连接是对称的，两个方向具有相同的强度

## **网格数据**

机器学习工作流程构建和编码：通常在 2D 或 3D 上具有详尽且均匀分布的数据，代表地图和模型

+   存储为逗号分隔的 .csv 文件，包含 $𝑛_𝑦$ 行和 $𝑛_𝑥$ 列

+   也可以保存/加载为二进制格式，以获得更紧凑的文件，但不是人类可读的。

+   通常直接可视化，例如，matplotlib 的 imshow 函数，或作为等高线图

## **硬数据**

机器学习概念：具有高度确定性的数据，通常来自岩石的直接测量

+   例如，基于井筒岩心和基于测井数据的孔隙度和岩性

通常，硬数据具有高分辨率（小尺度、体积支撑），但覆盖范围较差（仅测量人口中的极小比例，例如，

+   *核心覆盖深水油气* - 井筒岩心仅采样深水储层的一千五百万分之一到一十亿分之一，假设使用直径为 3 英寸的岩心，在垂直井中，岩心覆盖率为 10%，井距为 500 米到 1,500 米

+   *核心覆盖采矿级控制* - 钻孔岩心样本占矿石体积的一万八千分之一到三万分之一，假设 HQ 63.5 毫米直径岩心，在垂直钻孔中岩心覆盖率为 100%，井距为 5 米到 10 米

## **赫尔米特多项式**

多项式回归：实数线上的正交多项式族。

| 阶数 | 赫尔米特多项式 $H_e(x)$ |
| --- | --- |
| 零阶 | $H_{e_0}(x) = 1$ |
| 一阶 | $H_{e_1}(x) = x$ |
| 二阶 | $H_{e_2}(x) = x² - 1$ |
| 三阶 | $H_{e_3}(x) = x³ - 3x$ |
| 四阶 | $H_{e_4}(x) = x⁴ - 6x² + 3$ |

这些多项式相对于加权函数是正交的，

$$ 𝑤(𝑥)=𝑒^{−\frac{𝑥²}{2}} $$

这是标准高斯概率密度函数，没有缩放因子，$\frac{1}{\sqrt{2\pi}}$。正交性的定义如下，

$$ \int_{-\infty}^{\infty} H_m(x) H_n(x) w(x) \, dx = 0 $$

赫尔米特多项式在标准正态概率分布的区间 $[−\infty,\infty]$ 上是正交的。

通过在多项式回归中使用赫尔米特多项式而不是常规多项式进行多项式基扩展，可以消除预测特征之间的多重共线性，

+   回想，预测特征之间的独立性是多项式回归中应用多项式基扩展的线性系统的一个假设

## **启发式算法**

聚类分析：解决难题的捷径，为了速度和实用性，在最优性和准确性之间做出妥协。

+   这种通用方法在机器学习、计算机科学和数学优化中很常见，例如，k 均值聚类的 $k^n$ 解空间实际上是通过启发式算法解决的。

## **层次聚类**

聚类分析：所有聚类分组分配都是迭代确定的，与一次性确定所有聚类分组的划分聚类方法相反。包括，

+   *聚合层次聚类* - 从 $n$ 个聚类开始，每个数据样本在其自己的聚类中，然后迭代地将聚类合并成更大的聚类

+   *划分层次聚类* - 从所有数据在一个聚类开始，然后迭代地划分出新的聚类

+   k 均值聚类是划分聚类，而找到解决方案的启发式方法是迭代的，但实际上解决方案是一次性完成的

+   很难更新，一旦进行了一系列的拆分或合并，就很难返回并修改模型

## **直方图**

单变量分析：使用频率图表示单变量统计分布，该图在可能值的范围内对一组完整的箱进行绘制。这些是构建直方图的步骤，

1.  将可能的连续特征值范围划分为 $K$ 个相等大小的箱，$\delta x$:

$$ \Delta x = \left( \frac{x_{max} - x_{min}}{K} \right) $$

或者使用可用的类别标签进行分类特征。

1.  计算每个箱中样本的数量（频率），$n_k$， \quad $\forall \quad k=1,\ldots,K$.

1.  绘制频率与箱标签的关系图（如果连续，则使用箱中心）

注意，直方图通常以柱状图的形式绘制。

## **混合模型**

机器学习概念：包括确定性模型和随机模型组合的系统或过程

+   大多数地统计模型都是混合模型

+   例如，加性确定性趋势模型和随机残差模型

## **独立性**（概率）

概率概念：事件 $A$ 和 $B$ 独立当且仅当以下关系成立，

1.  $P(A \cap B) = P(A) \cdot P(B)$

1.  $P(A|B) = P(A)$

1.  $P(B|A) = P(B)$

如果任何这些条件被违反，我们怀疑存在某种形式的关系。

## **指示转换**（也称为二元转换）

特征转换：将随机变量编码为相对于类别或阈值的概率的指示符。

如果 $i(\bf{u}:z_k)$ 是一个分类变量的指示符，

+   实现等于某一类别的概率是多少？

$$\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) = z_k \\ 0, & \text{if } Z(\bf{u}) \ne z_k \end{cases} \end{split}$$

例如，

+   给定阈值，$z_2 = 2$，以及数据在 $\bf{u}_1$，$z(\bf{u}_1) = 2$，那么 $i(bf{u}_1; z_2) = 1$

+   给定阈值，$z_1 = 1$，以及一个远离数据的随机变量，$Z(\bf{u}_2)$，那么计算为 $F^{-1}_{\bf{u}_2}(z_1)$ 的随机变量，$i(\bf{u}_2; z_1) = 0.23$

如果 $i(\bf{u}:z_k)$ 是一个连续变量的指示符，

+   实现小于或等于阈值的概率是多少？

$$\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) \le z_k \\ 0, & \text{if } Z(\bf{u}) > z_k \end{cases} \end{split}$$

例如，

+   给定阈值，$z_1 = 6\%$，以及数据在 $\bf{u}_1$，$z(\bf{u}_1) = 8\%$，那么 $i(\bf{u}_1; z_1) = 0$

+   给定阈值，$z_4 = 18\%$，以及一个远离数据的随机变量，$Z(\bf{u}_2) = N\left[\mu = 16\%,\sigma = 3\%\right]$，那么 $i(\bf{u}_2; z_4) = 0.75$

指示编码可以通过对每个位置的随机变量的指示变换应用于整个随机函数。

## **指示变异图**

特征变换：指示变异图是从空间数据的**指示变换**计算和建模的，用于指示克立格法。指示变异图是，

$$ \gamma_i(\mathbf{h}; z_k) = \frac{1}{2N(\mathbf{h})} \sum_{\alpha=1}^{N(\mathbf{h})} \left[ i(\mathbf{u}_\alpha; z_k) - i(\mathbf{u}_\alpha + \mathbf{h}; z_k) \right]² $$

其中 $i(\mathbf{u}_\alpha; z_k)$ 和 $i(\mathbf{u}_\alpha + \mathbf{h}; z_k)$ 分别是尾端位置 $\mathbf{u}_\alpha$ 和头部位置 $\mathbf{u}_\alpha + \mathbf{h}$ 的 $z_k$ 阈值的指示变换。

+   对于硬数据，指示变换 $i(\bf{u},z_k)$ 要么是 0，要么是 1，在这种情况下，当头部和尾部的值都 $\le z_k$（对于连续特征）或 $= z_k$（对于分类特征）时，$\left[ i(\mathbf{u}_\alpha; z_k) - i(\mathbf{u}_\alpha + \mathbf{h}; z_k) \right]²$ 等于 0，相对于阈值也是如此，或者当它们不同时为 1。

+   因此，指示变异图是变化对的比例的一半！指示变异图可以与滞后距离 $h$ 上的变化概率相关。

+   指示变异图的块金值是按以下方式计算的指示方差，

$$ \sigma_i² = p \cdot (1 - p) $$

其中 $p$ 是 1 的比例（或零，因为函数在比例上是对称的）

## **推断，推断统计学**

机器学习概念：这是一个很大的主题，但为了这门课程，我提供了这个简化的、功能性的定义，给定来自总体的随机样本，描述总体，例如，

+   给定井样，描述储层

+   给定钻孔样品，描述矿体

## **内点**

一个回归模型准确度指标，表示在模型预测值 $\hat{y}_i$ 的 $\epsilon$ 范围内的测试数据比例，

$$ I_R = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} I(y_i, \hat{y}_i) $$

给定指示变换，

$$\begin{split} I(y_i, \hat{y}_i) = \begin{cases} 1, & \text{if } |y_i - \hat{y}_i| \leq \epsilon \\ 0, & \text{otherwise} \end{cases} \end{split}$$

这是一个有用的、直观的准确度度量，表示具有足够好的预测的训练或测试数据比例。

+   但是，存在一个与特定应用所需的准确度相关的边界大小 $\epsilon$ 的选择

## **基于实例的学习**

k-Nearest Neighbours：也称为基于记忆的学习，比较新的预测问题（作为预测器集合 $𝑥_1,\ldots,𝑥_𝑚$）与训练数据中观察到的案例。

+   模型需要访问训练数据，充当观察的库

+   直接从训练数据中进行预测

+   预测复杂度随着训练数据数量 $𝑛$、邻居数量 $𝑘$ 和特征数量 $𝑚$ 的增加而增长。

+   懒惰学习的特定情况

## **事件交集**（概率）

概率概念：结果的交集，$A$ 和 $B$ 的概率表示为，

$$ P(A \cap B) = P(A,B) $$

在 $A$ 和 $B$ 独立的前提下，$A$ 和 $B$ 的概率是，

$$ P(A,B) = P(A) \cdot P(B) $$

## **不可减少误差**

机器学习概念：由于数据限制导致的误差，包括缺失特征和缺失样本，例如，完整的预测特征空间没有得到充分的采样

+   不可减少误差不受模型复杂度的影响，它是数据的一个限制

+   预期测试平方误差的三个组成部分之一，包括模型方差、模型偏差和不可减少误差

$$ E \left[ \left(y_0 - \hat{f}(x_1⁰, \ldots, x_m,⁰ \right)² \right] = \left(E [\hat{f}(x_1⁰, \ldots, x_m,⁰)] - f(x_1⁰, \ldots, x_m,⁰) \right)² + $$$$ E \left[ \left( \hat{f} \left(x_1⁰, \ldots, x_m,⁰ \right) - E \left[ \hat{f}(x_1⁰, \ldots, x_m,⁰) \right] \right)² \right] + \sigma_e² $$

其中 $\sigma_e²$ 是不可减少误差。

## **惯性**（聚类）

聚类分析：k-means 聚类损失函数总结了所有组内样本之间的差异，

$$ I = \sum_{i=1}^{K} \sum_{x_j \in C_i} \| x_j - \mu_i \|² $$

其中 $K$ 是聚类总数，$C_i$ 表示第 $i$ 个聚类的样本集，$x_j$ 表示 $C_i$ 聚类中的一个数据样本，$\mu_i$ 是 $C_i$ 聚类的原型，$\| x_j - \mu_i \|²$ 是样本 $x_j$ 与聚类原型 $\mu_i$ 之间的平方欧几里得距离。在 mD 空间中，使用 $1,\ldots,m$ 个特征进行样本、原型和距离计算。

+   通过最小化惯性，k-means 聚类最小化组内的差异，同时最大化组间的差异

## **联合概率**

概率概念：考虑多个事件同时发生的概率，$A$ 和 $B$ 的概率表示为，

$$ P(A \cap B) = P(A,B) $$

或者 $A$、$B$ 和 $C$ 的概率表示为，

$$ P(A \cap B \cap C) = P(A,B,C) $$

在假设 $A$、$B$ 和 $C$ 独立的情况下，联合概率可以计算为，

$$ P(A,B,C) = P(A) \cdot P(B) \cdot P(C) $$

## **K 个桶离散化**

特征转换：将特征的取值范围划分为 K 个桶，然后对于每个样本，如果样本在桶内，则分配值为 1，如果不在桶内，则分配值为 0

+   分桶策略包括均匀宽度桶（均匀）和每个桶中均匀数量的数据（分位数）

+   也称为独热编码

需要 K 个桶离散化的方法，

+   基础扩展以在更高维空间中工作

+   将连续特征离散化为分类特征，用于如朴素贝叶斯分类器等分类方法

+   直方图构建和卡方检验用于分布差异

+   互信息分桶

## **K 折交叉验证**

机器学习概念：将数据划分为 K 折，并对每折进行循环，使用剩余的数据训练模型，并在折中的数据上测试模型。然后汇总所有折的测试准确率。

+   训练和测试数据分割基于 K，例如，K = 4 时，每折的测试数据为 25%，K = 5 时，每折的测试数据为 20%

+   这是对交叉验证的一种改进，它只应用一次训练和测试分割来构建一个单一模型。K 折方法允许测试所有数据，并且对所有折的准确率进行汇总往往可以平滑准确率与超参数的图像，从而实现更可靠的超参数调整

+   K 折交叉验证可以应用于检查模型性能以估计准确度（最常见）和不确定性模型的好坏（[Maldonado-Cruz 和 Pyrcz，2021](https://www.sciencedirect.com/science/article/pii/S0920410521006343)）

## **k-Means 聚类**

聚类分析：一种无监督机器学习方法，用于分区聚类，将未标记数据分组分配，其中聚类组内的差异最小化。最小化的损失函数是，

$$ I = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i || $$

其中 $i$ 是聚类索引，$\alpha$ 是数据样本索引，$X$ 是数据样本，$\mu_i$ 是 $i$ 聚类的原型，$k$ 是聚类总数，而 $|| X_m - \mu_m ||$ 是在 $M$ 维空间中从样本到聚类原型的欧几里得距离，计算如下，

$$ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i} \right)² } $$

这里是 k 均值聚类的关键方面总结，

+   *k* - 被作为模型超参数

+   *穷尽且互斥的组* - 所有数据分配到单个组

+   *原型方法* - 在特征空间中用合成案例的数量表示训练数据。对于 K-means 聚类，我们分配并迭代更新 $K$ 个原型。

+   *迭代解法* - 初始原型在特征空间中随机分配，每个训练样本的标签更新为最近的原型，然后原型调整到其分配的训练数据的质心，重复此过程，直到训练数据分配没有进一步更新。

+   *无监督学习* - 训练数据未标记，并根据其在特征空间中与原型的接近程度分配 $K$ 个标签。想法是相似的事物，在特征空间中的接近度，应该属于同一个聚类组。

+   *特征加权* - 该过程取决于训练样本和原型在特征空间中的欧几里得距离。距离被视为相似度的“倒数”。如果特征具有显著不同的幅度，则幅度和范围最大的特征将主导损失函数，聚类组将变得各向异性，与高范围特征垂直对齐。虽然常见的做法是对变量进行标准化/归一化，但可以通过不等方差应用特征加权。注意，在这个演示中，我们将特征归一化到 0.0 到 1.0 的范围。

## **k-最近邻算法**

k-最近邻算法：一个简单、可解释且灵活的非参数预测机器学习模型，基于对 $k$ 个最近训练数据应用局部加权窗口

k-最近邻方法与空间插值的卷积方法类似。卷积是两个函数的积分乘积，其中一个函数被反转并沿 $\Delta$ 平移。

+   一种解释是使用加权函数 $𝑓(\Delta)$ 对函数进行平滑，以计算函数 $𝑔(x)$ 的加权平均值，

$$ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta $$

这很容易扩展到多维

$$ (f * g)(x, y, z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(\Delta_x, \Delta_y, \Delta_z) g(x - \Delta_x, y - \Delta_y, z - \Delta_z) \, d\Delta_x \, d\Delta_y \, d\Delta_z $$

在积分之前选择哪个函数被平移不会改变结果，卷积算子具有交换性。

$$ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta $$$$ (f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta $$

+   如果任一函数被反射，卷积就等同于互相关，它是两个信号相似度的度量，作为位移的函数。

+   对于 k 近邻，使用$k$会导致一个局部自适应窗口大小，这与标准的卷积不同

K 近邻是一种基于实例的懒惰学习方法，模型训练被推迟到需要预测时，不需要预先计算模型。即预测需要访问数据。

+   为了进行新的预测，必须要有训练数据

超参数包括，

+   *k 个最近的数据点*用于预测

+   *数据加权*，例如使用局部训练数据平均值的均匀加权，或逆距离加权

注意，对于逆距离加权的情况，该方法类似于逆距离加权插值，通常应用于空间插值，并施加一个最大局部数据数量约束。

+   逆距离在 GeostatsPy 中可用于空间映射。

要找到最近的 k 个数据点，需要一个距离度量，

+   在预测特征空间内的训练数据按距离排序（从近到远）

+   可以应用各种距离度量，包括：

1.  欧几里得距离

\begin{equation}

d_i = \sqrt{\sum_{\alpha = 1}^{m} \left(x_{\alpha,i} - x_{\alpha,0}\right)²} \end{equation}

1.  Minkowski 距离 - 距离的通用表达式，其中已知的曼哈顿距离和欧几里得距离是特殊情况，

$$ d_{(i,i')} = \left( \sum_{j=1}^{m} \left( x_{(j,i)} - x_{(j,i')} \right)^p \right)^{\frac{1}{p}} $$

+   当$p=2$时，这变为欧几里得距离

+   当$p=1$时，它变为曼哈顿距离

## **核技巧**（支持向量机）

支持向量机：我们可以在我们的方法中包含基函数的扩展，而无需将训练数据转换到这个更高维的空间，

$$ h(x) $$

我们只需要预测特征上的内积，

$$ h(x) \left( h(x') \right)^T = \langle h(x), h(x') \rangle $$

在变换空间中，我们只需要所有可用训练数据在该变换空间中的‘相似性’！

+   我们仅使用将被投影到更高维空间的训练数据之间的相似性矩阵来训练支持向量机

+   我们实际上永远不需要计算更高维空间中的训练数据值

## **克里金法**

数据准备：依赖于线性权重的空间估计方法，这些权重考虑了空间连续性、数据接近性和冗余。克里金估计是，

$$ z^*(\bf{u}) = \sum_{\alpha = 1}^{n} \lambda_{\alpha} \cdot z(\bf{u}_{\alpha}) + \left( 1.0 - \sum_{\alpha=1}^n \lambda_{\alpha} \right) \cdot m_z $$

+   正确的术语是无偏约束，将权重之和减去 1 应用于全局均值。

在去除趋势$t(\bf{u})$的情况下，我们现在有一个残差，$y(\bf{u})$，

$$ y(\bf{u}) = z(\bf{u}) - t(\bf{u}) $$

残差均值为零，因此我们可以简化我们的克里金估计为，

$$ y^*(\bf{u}) = \sum_{\alpha = 1}^{n} \lambda_{\alpha} \cdot y(\bf{u}_{\alpha}) $$

简单克里金权重是通过求解线性方程组来计算的，

$$ \sum_{j=1}^n \lambda_j C(\bf{u}_i,\bf{u}_j) = C(\bf{u},\bf{u}_i), \quad i=1,\ldots,n $$

这可以用矩阵符号表示为，

$$\begin{split} \begin{bmatrix} C(\bf{u}_1,\bf{u}_1) & C(\bf{u}_1,\bf{u}_2) & \dots & C(\bf{u}_1,\bf{u}_n) \\ C(\bf{u}_2,\bf{u}_1) & C(\bf{u}_2,\bf{u}_2) & \dots & C(\bf{u}_2,\bf{u}_n) \\ \vdots & \vdots & \ddots & \vdots \\ C(\bf{u}_n,\bf{u}_1) & C(\bf{u}_n,\bf{u}_2) & \dots & C(\bf{u}_n,\bf{u}_n) \\ \end{bmatrix} \cdot \begin{bmatrix} \lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_n \\ \end{bmatrix} = \begin{bmatrix} C(\bf{u}_1,\bf{u}) \\ C(\bf{u}_2,\bf{u}) \\ \vdots \\ C(\bf{u}_n,\bf{u}) \\ \end{bmatrix} \end{split}$$

该系统可以通过将克里金估计的方程代入估计方差的方程，然后设置关于权重的偏导数为零来推导出来。

+   我们正在优化权重以最小化估计方差

该系统集成了，

+   *空间连续性*，通过变异函数（以及协方差函数来计算协方差，$C$，值）来量化

+   *冗余*，所有可用数据之间的空间连续度，$C(\bf{u}_i,\bf{u}_j)$

+   *邻近度*，即可用数据与估计位置之间的空间连续度，$C(\bf{u}_i,\bf{u})$

克里金提供了一种称为克里金方差（估计方差的一种特殊情况）的估计精度度量。

$$ \sigma^{2}_{E}(\bf{u}) = C(0) - \sum^{n}_{\alpha = 1} \lambda_{\alpha} C(\bf{u}_0 - \bf{u}_{\alpha}) $$

克里金估计之所以最佳，是因为它们最小化了上述估计方差。

克里金估计的性质包括，

+   *精确插值器* - 在数据位置处的数据值进行克里金估计

+   *克里金方差* - 克里金估计中的不确定性度量。在获取样本信息之前就可以计算，因为克里金估计方差不依赖于数据的值也不依赖于克里金估计，即克里金估计量是同方差齐次的。

+   *空间上下文* - 克里金综合了空间连续性、邻近度和冗余；因此，克里金考虑了数据的配置和被估计特征的连续性结构。

+   *尺度* - 克里金默认假设估计和数据位于同一支撑点，即数学上表示为空间中零体积的点。克里金可以推广以考虑数据和估计的支撑体积，

+   *多元* - 克里金可以通过共克里金系统推广，以考虑空间估计中的多个次级数据。我们将在后面介绍这一点。

+   *平滑效应* - 克里金的平滑效应可以预测为缺失方差。局部估计的缺失方差是克里金方差。

## **基于克里金的去聚类**

数据准备：一种基于局部采样密度的降聚方法，为空间样本分配权重，使得加权统计更有可能代表总体。数据权重分配如下，

+   稠密采样区域的样本获得较少权重

+   稀疏采样区域的样本获得更多权重

基于克里金法的降聚过程如下：

1.  计算并建模实验变异函数

1.  在覆盖感兴趣区域的高分辨率网格上应用克里金法进行估计

1.  计算分配给每个数据的权重总和

1.  将数据权重分配与这个权重总和成比例

权重计算如下：

$$ w(\bf{u}_j) = n \cdot \frac{\sum_{iy}^{ny} \sum_{ix}^{nx} \lambda_j}{\sum_{i=1}^n \left[ \sum_{iy}^{ny} \sum_{ix}^{nx} \lambda_{j,ix,iy} \right]} $$

其中 $nx$ 和 $ny$ 是网格中的单元格数，$n$ 是数据数量，$\lambda_{j,ix,iy}$ 是分配给 $ix,iy$ 网格单元格的 $j$ 数据的权重。

这里是关于基于克里金法的降聚的一个重要观点，

+   如多边形降聚，基于克里金法的降聚对感兴趣区域的边界敏感；因此，当感兴趣区域扩展或收缩时，分配给感兴趣区域边界的数据的权重可能会发生根本性的变化

此外，基于克里金法的降聚还整合了变异函数模型中的空间连续性模型。考虑以下变异函数模型对降聚权重的影响，

+   如果有 100%的相对异常值效应，则没有空间连续性，因此，所有数据都获得相等的权重。注意，上述方程会导致除以 0.0 的错误，必须在代码中进行检查。

+   几何各向异性可能会显著影响权重，因为沿特定方位对齐的数据在协方差方面被视为更近或更远

## **柯尔莫哥洛夫的 3 个概率公理**

概率概念：这些是柯尔莫哥洛夫对有效概率的 3 个公理，

1.  事件发生的概率是一个非负数。

$$ P(𝐴) \ge 0 $$

1.  整个样本空间，所有可能结果的概率，$\Omega$，为 1（单位），也称为概率封闭。

$$ P(\Omega) = 1 $$

1.  互斥事件的并集的可加性。

$$ P\left(⋃_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i) $$

例如，互斥事件 $A_1$ 和 $A_2$ 的概率是，$P(A_1 + A_2) = P(A_1) + P(A_2)$

## **$L¹$ 范数**

线性回归：称为曼哈顿范数或绝对残差之和（SAR），

$$ \sum_{i=1}^n |\Delta y_i | $$

也表示为平均绝对误差（MAE）。

$$ \frac{1}{n} \sum_{i=1}^n |\Delta y_i | $$

使用 $L¹$ 范数进行最小化称为最小绝对差异。

## **$L²$ 范数**

线性回归：称为平方残差之和（SSR），

$$ \sum_{i=1}^n \sqrt{\Delta y_i} $$

也表示为均方误差（MSE），

$$ \frac{1}{n} \sum_{i=1}^n \left( \Delta y_i \right)² $$

以及欧几里得范数，

$$ \sqrt{ \sum_{i=1}^n \sqrt{\Delta y_i} } $$

使用 $L²$ 范数进行最小化被称为最小二乘法。

## **$L¹$ 与 $L²$ 范数**

LASSO 回归：在机器学习中，$L¹$ 和 $L²$ 范数的选取非常重要。为了解释这一点，让我们比较在训练模型参数时 $L¹$ 和 $L²$ 范数在损失函数中的性能。

| 属性 | 最小绝对偏差（L1） | 最小二乘（L2） |
| --- | --- | --- |
| 鲁棒性* | 鲁棒 | 不太鲁棒 |
| 解的稳定性 | 不稳定解 | 稳定解 |
| 解的数量 | 可能存在多个解 | 总是有一个解 |
| 特征选择 | 内置特征选择 | 无特征选择 |
| 输出稀疏性 | 稀疏输出 | 非稀疏输出 |
| 解的解析性 | 无解析解 | 解析解 |

这里有一些重要的观点，

+   *鲁棒性* - 对异常值有抵抗力

+   *不稳定* - 对于训练的小变化，训练好的模型预测可能会跳跃

+   *多个解* - 不同的解具有相似或相同的损失，导致解在训练数据的小变化下跳跃

+   *输出稀疏性* 和 *特征选择* - 模型参数趋向于 0.0

+   *解析解* - 存在解析解以求解最优模型参数

## **$L¹$ 或 $L²$ 正则化器**

特征变换：在单个样本的特征间执行以约束总和

L1 范数在样本间有以下约束，

$$ \sum_{\alpha = 1}^m x^{\prime}_{i,\alpha} = 1.0, \quad i = 1, \ldots, n $$

L1 正则化器变换，

$$ x^{\prime}_{i,\alpha} = \frac{x_{i,\alpha}}{\sum_{\alpha=1}^m x_{i,\alpha}} $$

L2 范数在样本间有以下约束，

$$ \sum_{\alpha = 1}^m \left( x^{\prime}_{i,\alpha} \right)² = 1.0, \quad i = 1, \ldots, n $$

L2 正则化器变换，

$$ x^{\prime}_{i,\alpha} = \sqrt{\frac{(x_{i,\alpha})²}{\sum_{\alpha=1}^m (x_{i,\alpha})²}} $$

例如，应用于文本分类和聚类，以及 L1 用于组合数据（总和 1.0 约束）

## **LASSO 回归**

LASSO 回归：具有 $L¹$ 正则化项和正则化超参数 $\lambda$ 的线性回归，

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)² + \lambda \sum_{j=1}^m |b_{\alpha}| $$

因此，LASSO 回归训练集成了两个经常相互竞争的目标来寻找模型参数，

+   寻找使训练数据误差最小的模型参数

+   最小化斜率参数趋向于零

LASSO 和岭回归之间的唯一区别是：

+   对于 LASSO，收缩项被表示为 $\ell_1$ 惩罚项，

$$ \lambda \sum_{\alpha=1}^m |b_{\alpha}| $$

+   对于岭回归，收缩项被表示为 $\ell_2$ 惩罚，

$$ \lambda \sum_{\alpha=1}^m \left(b_{\alpha}\right)² $$

虽然岭回归和 LASSO 都将模型参数 ($b_{\alpha}, \alpha = 1,\ldots,m$) 收缩到零：

+   随着 lambda，$\lambda$ 超参数的增加，LASSO 参数以不同的速率达到零。

+   因此，LASSO 提供了一种特征排序和选择的方法！

lambda，$\lambda$ 超参数控制模型的拟合程度，可能与模型偏差-方差权衡有关。

+   当 $\lambda \rightarrow 0$ 时，预测模型趋近于线性回归，模型偏差较低，但模型方差较高

+   随着 $\lambda$ 的增加，模型方差降低，模型偏差增加

+   当 $\lambda \rightarrow \infty$ 时，所有系数都变为 0.0，模型是训练数据响应特征均值

## **懒惰学习**

k-最近邻：模型是训练数据的泛化，计算在查询模型时才进行

+   模型是训练数据和选定的超参数，要做出新的预测，必须提供训练数据

相反的是积极学习。

## **学习率** (梯度提升)

梯度提升：控制每次新模型更新的速率。

$$ f_m = f_{m-1} - \rho_m \frac{\partial L(y_\alpha, F(X_\alpha))}{\partial F(X_\alpha)} $$

其中 $\rho_m$ 是学习率，$\frac{\partial L(y_\alpha, F(X_\alpha))}{\partial F(X_\alpha)}$ 是梯度，误差，$f_{m-1}$ 是前一个估计，$f_m$ 是新的估计。

关于学习率的一些显著点，

+   没有学习率，提升模型学习得太快，模型方差会很高

+   减慢学习以获得更稳健的模型，平衡以确保良好的性能，过小的速率将需要非常大的模型数量才能达到收敛

## **同样删除** (MRMR)

特征排序：移除任何具有任何缺失特征值的样本

+   如果缺失的特征值不是随机缺失 (MAR)，这可能会在数据中引入偏差

+   将导致有效数据量减少和模型不确定性增加

## **线性回归**

线性回归：一个线性、参数化的预测模型，

$$ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 $$

对于 L2 范数损失函数，模型参数 $b_1,\ldots,b_m,b_0$ 的解析解是可用的，误差是求和并平方的已知最小二乘法。

+   我们在训练数据上最小化误差，残差平方和 (RSS)：

$$ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)² $$

其中 $y_i$ 是实际响应特征值，$\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ 是模型预测，在 $\alpha = 1,\ldots,n$ 的训练数据上。

+   这可以简化为训练数据上的平方误差之和，

$$ \sum_{i=1}^n (\Delta y_i)² $$

其中 $\Delta y_i$ 是实际响应特征观察 $y_i$ 减去模型预测 $\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$，在 $i = 1,\ldots,n$ 的训练数据上。

我们的线性回归模型有一些重要的假设，

+   *无误差* - 预测变量无误差，不是随机变量

+   *线性性* - 响应是特征（s）的线性组合

+   *常数方差* - 响应误差在预测值上是恒定的

+   *误差独立性* - 响应误差之间相互不相关

+   *无多重共线性* - 没有特征与其他特征冗余

## **位置图**

加载数据和绘图模型：一个数据图，其中两个轴是位置，例如 $X$ 和 $Y$，东西方向和南北方向，纬度和经度等，以显示空间数据的位置和大小。

+   通常数据点会被着色以表示特征的比例，以可视化感兴趣区域或体积上的采样特征

+   优点，可以可视化数据，而无需任何可能影响我们对数据印象的模型

+   缺点，可能难以可视化大型数据集和三维数据

## **损失函数**

LASSO 回归：用于训练模型参数的最小化方程。例如，线性回归的损失函数包括残差平方和、$L²$ 错误范数，

$$ \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)² $$

对于 LASSO 回归，损失函数包括残差平方和、$L²$ 错误范数，加上 $L¹$ 正则化项，

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)² + \lambda \sum_{j=1}^m |b_{\alpha}| $$

对于 k-means 聚类，损失函数是，

$$ I = \sum^k_{i=1} \sum_{\alpha \in C_i} \sqrt{ \sum_{j = 1}^m X_{\alpha,m} - \mu_{i,m} } $$

最小化损失函数的方法取决于范数的类型，

+   在 $L²$ 范数下，我们对损失函数相对于模型参数进行微分，并将其设置为等于零

+   在我们的损失函数中使用 $L¹$ 范数时，我们失去了访问解析解，并使用迭代优化，例如最速下降法

## **机器学习工作流程设计**

机器学习工作流程构建和编码：基于以下步骤，

1.  *指定目标* - 例如，

+   建立数值模型

+   评估不同的恢复过程

1.  *指定数据* - 可用的是什么，缺少的是什么？

1.  *设计一组步骤以实现目标* - 常见步骤包括，

+   加载数据

+   格式化、检查和清理数据

+   运行操作，包括统计计算、模型或可视化

+   传递函数

1.  *开发文档* - 包括实现细节、决策的辩护、元数据、限制和未来工作

1.  *流程* - 数据和信息流，通过分支和回环建模时的学习

1.  *不确定性* - 总结所有不确定性来源，包括集成不确定性的方法，捍卫被认为确定的不确定性模型和方面

## **边缘** (支持向量机)

支持向量机：当训练数据包括重叠类别时，不可能也不希望开发一个完美分离这些类别的决策边界，这些类别将满足此条件，

$$ y_i \left( x_i^T \beta + \beta_0 \right) \geq 0 $$

我们需要一个允许某些误分类的模型。

$$ y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i $$

我们引入了边缘的概念 $𝑀$ 和边缘距离（误差，$𝜉_𝑖$）。

$$ \underset{\beta, \beta_0}{\text{min}} \left( \frac{1}{2M²} + C \sum_{i=1}^N \xi_i \right) $$

损失函数包括边缘项 $M$，因此试图在最小化分类错误的同时最小化边缘，分类错误由超参数 $C$ 加权。

## **边缘概率**

概率概念：只考虑一个事件发生的概率，即 $A$ 的概率，

$$ P(A) $$

边缘概率可以通过边缘化过程从联合概率中计算得出，

$$ P(A) = \int_{-\infty}^{\infty} P(A,B) dB $$

其中我们整合其他事件 $B$ 的所有情况，以消除其影响。对于事件 $B$ 的离散可能情况，我们可以简单地对所有可能的 $B$ 的情况求和，

$$ P(A) = \sum_{i=1}^{k_B} P(A,B) dB $$

## **矩阵散点图**

多元分析：包括所有特征成对散点图的组合图。

+   给定 $m$ 个特征，有 $m \times m$ 个散点图

+   散点图是有序的，y 轴特征来自 $X_1,\ldots,X_m$ 的行，x 轴特征来自 $X_1,\ldots,X_m$ 的列

+   对角线是自身特征绘制的，通常被特征直方图或概率密度函数所取代

我们使用矩阵散点图来，

+   寻找双变量线性或非线性结构

+   寻找双变量同方差性（条件方差恒定）和异方差性（条件方差随值变化）

+   寻找双变量约束，例如组合数据的求和约束

记住，其他特征已被边缘化，这并不是一个完整的 m-D 可视化。

## **最大相关性最小冗余** (MRMR)

特征排序：一种基于互信息的特征排序方法，考虑了特征的相关性和冗余。

+   一个例子是相关性减去冗余摘要，

$$ MRMR = max \left[ frac{1}{|S|} \sum_{X_i \in S} I(X_i,Y) - \frac{1}{|S|²} \sum_{X_i \in S} \sum_{X_j, i \ne j} I(X_i,X_j) \right] $$

其中 $𝑆$ 是预测特征子集，$|𝑆|$ 是子集 $𝑆$ 中特征的数量。

## **Metropolis-Hastings MCMC 采样器**

贝叶斯线性回归：Metropolis-Hastings MCMC 采样器的基本步骤：

对于 $\ell = 1, \ldots, L$:

1.  为模型参数的初始样本分配随机值，$\beta(\ell = 1) = b_1(\ell = 1)$, $b_0(\ell = 1)$ 和 $\sigma²(\ell = 1)$.

1.  基于提议函数提出新的模型参数，$\beta^{\prime} = b_1$, $b_0$ 和 $\sigma²$.

1.  计算新提议的接受概率，作为给定数据的后验概率与先前的模型参数的比值，乘以旧步骤给定新步骤的概率除以新步骤给定旧步骤的概率。

$$ P(\beta \rightarrow \beta^{\prime}) = min\left(\frac{P(\beta^{\prime}|y,X) }{ P(\beta | y,X)} \cdot \frac{P(\beta^{\prime}|\beta) }{ P(\beta | \beta^{\prime})},1\right) $$

1.  应用蒙特卡洛模拟以条件接受提议，如果接受，$\ell = \ell + 1$，并采样 $\beta(\ell) = \beta^{\prime}$

1.  转到步骤 2。

## **闵可夫斯基距离**

k-最近邻：距离的一般表达式，其中已知的曼哈顿和欧几里得距离是特殊情况，

$$ d_{(i,i')} = \left( \sum_{j=1}^{m} \left( x_{(j,i)} - x_{(j,i')} \right)^p \right)^{\frac{1}{p}} $$

+   当 $p=2$ 时，这变为欧几里得距离

+   当 $p=1$ 时，它变为曼哈顿距离

## **缺失特征值**

特征插补：数据表中的空值，没有所有特征值的样本

缺失特征值有许多原因，例如，

1.  采样成本，例如，低渗透率测试耗时过长

1.  岩石流变学样本过滤器，例如，无法恢复泥岩样本

1.  采样以减少不确定性并最大化盈利性，而不是统计代表性，双重目的样本用于信息和生产

缺失数据后果，除了减少训练和测试数据量外，缺失数据，如果不是完全随机，可能会导致，

+   偏差样本统计导致偏差的模型训练和测试

+   偏差模型具有偏差预测，可能没有偏差的指示

## **随机缺失** (MAR)

特征插补：缺失特征值分布随机，在预测特征空间中均匀覆盖，即所有值都有缺失的可能性，并且缺失特征值之间没有相关性。

这通常不是情况，因为缺失数据通常有一个混杂特征，例如，

1.  采样成本，例如，低渗透率测试耗时过长

1.  岩石力学样本过滤器，例如，无法恢复泥岩样本

1.  采样以减少不确定性和最大化盈利性，而不是统计代表性，双重目的样本用于信息和生产

缺失数据后果，除了减少训练和测试数据量外，如果缺失数据不是完全随机，还可能导致，

+   偏差样本统计导致模型训练和测试偏差

+   偏差模型具有偏差预测，可能没有偏差的迹象

## **模型偏差**

机器学习概念：是由于不足够的复杂性和灵活性来适应自然设置而产生的错误

+   增加模型复杂性通常会导致模型偏差减少

+   *模型偏差-方差权衡* - 随着复杂性的增加，模型方差增加，模型偏差减少

+   预期测试平方误差的三个组成部分之一，包括模型方差、模型偏差和不可减少误差

$$ E \left[ \left(y_0 - \hat{f}(x_1⁰, \ldots, x_m,⁰ \right)² \right] = \left(E [\hat{f}(x_1⁰, \ldots, x_m,⁰)] - f(x_1⁰, \ldots, x_m,⁰) \right)² + $$$$ E \left[ \left( \hat{f} \left(x_1⁰, \ldots, x_m,⁰ \right) - E \left[ \hat{f}(x_1⁰, \ldots, x_m,⁰) \right] \right)² \right] + \sigma_e² $$

其中 $\left(E [\hat{f}(x_1⁰, \ldots, x_m,⁰)] - f(x_1⁰, \ldots, x_m,⁰) \right)²$ 是模型偏差。

## **模型偏差-方差权衡**

机器学习概念：随着复杂性的增加，模型方差增加，模型偏差减少。

+   由于模型方差和模型偏差都是预期测试平方误差的组成部分，因此模型偏差和模型方差的平衡导致了一个最佳复杂度水平，以最小化测试误差

## **模型检查**

机器学习概念：是任何空间建模工作流程的关键最后一步。以下是模型检查的关键方面，

1.  *模型输入* - 数据和统计数据的整合

+   检查模型以确保模型输入在模型中得到尊重，通常对所有实现进行检查，例如，输出直方图与实现中的输入直方图相匹配

1.  *精确空间估计* - 模型在可用样本数据之外准确预测的能力，在各种配置下，具有准确性

+   通过交叉验证，保留了一些数据，检查模型预测的能力

+   通常，通过真实值与预测交叉图和均方误差等指标进行总结

$$ MSE = \frac{1}{n} \sum_{\alpha = 1}^{n} \left(z^{*}(\bf{u}_{\alpha}) - z(\bf{u}_{\alpha}) \right)² $$

1.  *精确和精确的不确定性模型* - 在给定信息和各种不确定性来源的情况下，不确定性模型是合理的

+   也通过交叉验证进行了检查，保留了一些数据，但通过检查特定概率区间内数据的比例

+   用保留数据在区间内的比例与概率区间来总结

+   45 度线上的点表示准确且精确的模型不确定性

+   45 度线以上的点表示准确和不准确的模型不确定性，不确定性范围太宽

+   45 度线以下的点表示不准确的模型不确定性，不确定性太窄或模型有偏差

## **模型复杂度或灵活性**

机器学习概念：模型拟合数据和可解释的能力。

可以使用各种概念来描述模型复杂度，

+   模型中的特征数量、预测变量，模型的维度，通常导致更多的模型参数

+   参数数量，每个项应用的顺序，例如线性、二次、阈值

+   模型的格式，即，多项式回归的紧凑方程与决策树嵌套条件语句相比，或神经网络成千上万的权重和偏差模型参数

+   例如，高阶多项式、更大的决策树等更复杂的复杂性。

通常，更复杂或灵活的模型更难以解释，

+   线性回归及其相关模型参数可以进行分析，甚至应用于特征排序，而具有径向基函数的支持向量机在 nD 高维空间中是一个线性模型

## **模型泛化**

机器学习概念：模型预测训练数据之外的能力。

+   模型学习数据中的结构，而不仅仅是记住训练数据

不善于泛化的模型，

+   过拟合模型在训练数据上具有高精度，而在训练数据之外具有低精度，以低测试精度为例

+   欠拟合模型对自然现象过于简单或不灵活，训练和测试准确性低

## **模型超参数**

机器学习概念：限制模型复杂度。超参数调整以最大化保留测试数据中的准确性，以防止模型过拟合。

对于从 $4^{th}$ 到 $1^{st}$ 次的多项式模型，

$$ y = b_4 \cdot x⁴ + b_3 \cdot x³ + b_2 \cdot x² + b_1 \cdot x + b_0 $$$$ y = b_3 \cdot x³ + b_2 \cdot x² + b_1 \cdot x + b_0 $$$$ y = b_2 \cdot x² + b_1 \cdot x + b_0 $$$$ y = b_1 \cdot x + b_0 $$

多项式阶数的选择是超参数，即，一阶模型最简单，四阶模型最复杂。

## **模型参数**

机器学习概念：机器学习模型中用于控制对训练数据拟合的可训练系数。

对于多项式模型，

$$ y = b_3 \cdot x³ + b_2 \cdot x² + b_1 \cdot x + b_0 $$

$b_3$、$b_2$、$b_1$ 和 $b_0$ 是模型参数。

+   *训练模型参数* - 模型参数通过优化计算，以最小化训练数据上的误差和正则化项，通过解析解或迭代解，例如，梯度下降优化

## 模型正则化

岭回归：添加信息以防止过拟合（或欠拟合），提高模型泛化能力。

+   这种信息被称为正则化项

+   这代表了一种由正则化超参数调整的复杂度惩罚

考虑岭回归损失函数，

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)² + \lambda \sum_{j=1}^m b_{\alpha}² $$

其中 $\lambda \sum_{j=1}^m b_{\alpha}²$ 是正则化项，而 $\lambda$ 是正则化超参数。

正则化的概念相当通用，在机器学习架构中的选择，例如，

+   使用感受野卷积神经网络（CNNs）

+   限制决策树的最大层数的选择。

关于模型正则化有几个有用的视角，

+   *奥卡姆剃刀原则* - 正则化调整模型复杂度以达到最简单的有效解

+   *贝叶斯视角* - 正则化是对解施加先验。

## **模型方差**

机器学习概念：由于对数据集敏感而产生的误差

+   增加模型复杂度通常会导致模型方差增加

+   集成机器学习，例如，通过在数据集的 bootstrap 实现上训练多个估计器来平均模型方差

+   *模型偏差-方差权衡* - 随着复杂度的增加，模型方差增加，模型偏差减少

+   预期测试平方误差的三个组成部分之一，包括模型方差、模型偏差和不可减少误差

$$ E \left[ \left(y_0 - \hat{f}(x_1⁰, \ldots, x_m,⁰ \right)² \right] = \left(E [\hat{f}(x_1⁰, \ldots, x_m,⁰)] - f(x_1⁰, \ldots, x_m,⁰) \right)² + $$$$ E \left[ \left( \hat{f} \left(x_1⁰, \ldots, x_m,⁰ \right) - E \left[ \hat{f}(x_1⁰, \ldots, x_m,⁰) \right] \right)² \right] + \sigma_e² $$

其中 $E \left[ \left( \hat{f} \left(x_1⁰, \ldots, x_m,⁰ \right) - E \left[ \hat{f}(x_1⁰, \ldots, x_m,⁰) \right] \right)² \right]$ 是模型方差。

## **动量**（优化）

LASSO 回归：用新步更新前一步，动量，$\lambda$ 是应用于前一步的权重，而 $1 - \lambda$ 是应用于当前步的权重，

$$ \left( \left( r \cdot \nabla L \right)_{t-1} \right)^m = \lambda \cdot r \cdot \nabla L_{t-2} + (1 - \lambda) \cdot r \cdot \nabla L_{t-1} $$

+   从每个模型参数的损失函数偏导数计算出的梯度存在噪声。动量平滑，减少这种噪声的影响。

+   动量有助于解决方案沿着损失函数的一般斜率前进，而不是在局部峡谷或凹槽中振荡

## **马尔可夫链蒙特卡洛** (MCMC)

贝叶斯线性回归：一套从概率分布中采样的算法，使得样本匹配分布统计量。

+   *马尔可夫* - 屏蔽假设，下一个样本仅依赖于前一个样本

+   *链* - 样本形成一个序列，通常表明从烧毁链（具有不准确统计量）到平衡链（具有准确统计量）的过渡

+   *蒙特卡洛* - 使用蒙特卡洛模拟，从统计分布中进行随机采样

为什么这很有用？

+   我们通常没有目标分布，它是未知的

+   但我们可以通过其他形式的信息（如条件概率密度函数、Gibbs 采样或候选下一个样本与当前样本的似然比）以正确的频率进行采样，例如 Metropolis-Hastings

## **Metropolis-Hastings Sampling** (MCMC)

贝叶斯线性回归：一套从概率分布中采样的算法，使得样本匹配分布统计量，基于，

+   候选下一个样本和当前样本的似然比

+   基于这个似然比的一个拒绝采样器

由于只需要似然比的比率，系统简化为从贝叶斯概率中取消证据项

下面是 Metropolis-Hastings MCMC 采样器的基本步骤：

对于 $\ell = 1, \ldots, L$:

1.  为模型参数的初始样本分配随机值，$\beta(\ell = 1) = b_1(\ell = 1)$, $b_0(\ell = 1)$ 和 $\sigma²(\ell = 1)$.

1.  基于建议函数提出新的模型参数，$\beta^{\prime} = b_1$, $b_0$ 和 $\sigma²$.

1.  计算新建议的接受概率，即新模型参数在给定数据下的后验概率与旧模型参数在给定数据下的后验概率的比率，乘以旧步骤在给定新步骤下的概率除以新步骤在给定旧步骤下的概率。

$$ P(\beta \rightarrow \beta^{\prime}) = min\left(\frac{P(\beta^{\prime}|y,X) }{ P(\beta | y,X)} \cdot \frac{P(\beta^{\prime}|\beta) }{ P(\beta | \beta^{\prime})},1\right) $$

1.  应用蒙特卡洛模拟以条件接受建议，如果被接受，$\ell = \ell + 1$，并采样 $\beta(\ell) = \beta^{\prime}$

1.  转到步骤 2。

## **蒙特卡洛模拟 (MCS)**

贝叶斯线性回归：从统计分布中随机采样，随机变量。MCS 的步骤如下：

1.  建立特征累积分布函数，$F_x(x)$

1.  从均匀分布 [0,1] 中抽取随机值，这是一个随机累积概率值，称为 p 值，$p^{\ell}$

1.  应用累积分布函数的逆来计算相关的实现

$$ x^{\ell} = F_x^{-1} (p^{\ell}) $$

1.  重复计算后续分析所需的足够实现

蒙特卡洛模拟是随机模拟工作流程的基本构建块，例如，

+   *蒙特卡洛模拟工作流程* - 将蒙特卡洛模拟应用于所有特征到传递函数，以计算决策标准的实现，重复多次实现，以通过传递函数传播不确定性

+   *自助法* - 将蒙特卡洛模拟应用于获取数据的实现以计算样本统计或基于集成机器学习的预测模型集的不确定性

+   *蒙特卡洛方法* - 将蒙特卡洛模拟应用于通过有限的随机样本加速昂贵的计算，随着随机样本数量的增加，解决方案收敛

## **蒙特卡洛模拟工作流程**

贝叶斯线性回归：一种方便的随机工作流程，通过蒙特卡洛模拟（MCS）进行采样来传播传递函数的不确定性。该工作流程包括以下步骤，

1.  模型所有输入特征的分布、累积分布函数，

$$ F_{x_1}(x_1), \quad F_{x_2}(x_2), \quad \dots \quad , F_{x_m}(x_m) $$

1.  对所有输入进行蒙特卡洛模拟，

$$ x_1^{\ell}, \quad x_2^{\ell}, \quad \ldots \quad , x_m^{\ell} $$

1.  应用到传递函数以获得传递函数输出的实现，通常是 *决策标准*

$$ y^{\ell} = f \left(x_1^{\ell},x_2^{\ell}, \quad \ldots \quad, x_m^{\ell} \right) $$

1.  重复步骤 1-3 以计算足够的实现来模拟传递函数输出分布。

$$ F_y(y) $$

## **乘法规则**（概率）

概率概念：我们可以通过 $A$ 给定 $B$ 的条件概率与 $A$ 的边缘概率的乘积来计算 $A$ 和 $B$ 的联合概率，

$$ P(A \cup B) = P(A,B) = P(B|A) \cdot P(A) $$

乘法规则是通过简单操作条件概率的定义推导出来的，在这种情况下，

$$ P(B|A) = \frac{P(A,B)}{P(A)} $$

## **互信息**

特征排序：一种通用方法，量化两个特征之间的相互依赖性。

+   量化从观察一个特征关于另一个特征获得的信息量

+   避免对关系形式的任何假设（例如，没有线性关系的假设）

    单位是香农或比特

+   将联合概率与边缘概率的乘积进行比较

+   +   总结了联合 $P(x,y)$ 与边缘 $P(x)\cdot P(y)$ 的乘积之间的差异，在整个 $x \in 𝑋$ 和 $y \in Y$ 上积分，

对于离散或分箱的连续特征 $X$ 和 $Y$，互信息计算如下：

$$ I(X;Y) = \sum_{y \in Y} \sum_{x \in X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) $$

回想，给定 $X$ 和 $Y$ 之间的独立性：

$$ P_{X,Y}(x,y) = P_X(x) \cdot P_Y(y) $$

因此，如果两个特征是独立的，那么 $log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) = 0$

联合概率 $P_{X,Y}(x,y)$ 是求和的加权项，并强制封闭。

+   联合分布中密度较大的部分对互信息度量有更大的影响。

对于连续（和非分箱）特征，我们可以应用积分形式。

$$ I(X;Y) = \int_{Y} \int_{X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) dx dy $$

## **互斥事件**（概率）

概率概念：事件不相交，即没有共同的结果。我们表示为，

+   使用集合表示法，我们声明事件 $A$ 和 $B$ 是互斥的，即它们没有共同的结果。我们表示为，

$$ A \cap B = \{x: x \in A \text{ and } x \in B \} = \emptyset $$

+   以及互斥事件的概率，

$$ P(A,B) = 0.0 $$

## **多维尺度分析**

多维尺度分析：一种在推断统计学/信息可视化中用于探索/可视化高维数据集中个体样本之间相似性（或相反，差异性）的方法，在低维空间中进行。

多维尺度分析（MDS）将 $m$ 维数据投影到 $p$ 维，使得 $p << m$。

+   在尝试保留数据样本之间的成对相似度的同时

+   理想情况下，我们能够投影到 $p=2$ 以轻松探索样本之间的关系。

当主成分分析（PCA）使用协方差矩阵时，多维尺度分析使用距离或相似度矩阵。对于多维尺度分析，

+   你不需要知道实际的特征值，只需要知道样本之间的距离或相似度

+   就像特征空间中的任何距离一样，我们考虑特征标准化以确保具有较大方差的特征不会主导计算。

+   我们可以使用各种相似度度量

多维尺度分析与主成分分析的比较，

+   主成分分析（PCA）通过所有特征之间的协方差矩阵（$m \times m$）找到线性、正交旋转，使得在有序的主成分上*方差最大化*。

+   多维尺度分析将特征空间中所有样本之间的成对距离矩阵（$n \times n$）找到非线性投影，使得*成对距离误差最小化*。

有些人认为在多维尺度空间中可视化数据或模型是可视化不确定性空间。

## **朴素贝叶斯**

朴素贝叶斯：从贝叶斯更新的角度，将条件独立性假设应用于简化分类预测问题，基于给定 $n$ 个特征 $x_1, \dots , x_n$ 的类别 $k$ 的条件概率，

$$ P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3 | x_4, \dots , x_n, C_k) \ldots P(x_{n-1} | x_n, C_k) (x_{n} | C_k) P(C_k) $$

联合条件概率的似然，即条件概率，难以计算，可能无法计算。它需要关于 $x_1, \dots , x_n$ 特征之间联合关系的信息。随着 $n$ 的增加，这需要大量数据来告知联合分布。

在朴素贝叶斯方法中，我们做出“朴素”的假设，即特征都是**条件独立的**。这包括，

$$ P(x_i | x_{i+1}, \ldots , x_n, C_k) = P(x_i | C_k) $$

对于所有 $i = 1, \ldots, n$ 特征。

我们现在可以解出所需的条件概率如下：

$$ P(C_k | x_1, \dots , x_n) = \frac{P(C_k) \prod_{i=1}^{n} P(x_i | C_k)}{P(x_1, \dots , x_n)} $$

我们只需要先验概率 $P(C_k)$ 和一组条件概率 $P(x_i | C_k)$，对于所有预测特征 $i = 1,\ldots,n$ 和所有类别 $k = 1,\ldots,K$。

证据项 $P(x_1, \dots , x_n)$ 仅基于特征 $x_1, \dots , x_n$；因此，在类别 $k = 1,\ldots,n$ 上是常数。

+   它确保了封闭性 - 所有类别的概率之和为 1

+   我们只需将分子标准化，使其在类别上求和为 1

朴素贝叶斯方法是：

+   容易理解，建立在基本的贝叶斯统计基础之上

+   即使数据集较小，也具有实用性，因为有了条件独立性，我们只需要估计简单的条件分布

## **ndarray**

机器学习工作流程构建和编码：Numpy 提供的方便类，用于处理二维或三维上的网格、穷举、规则间隔数据，表示地图和模型，因为，

+   方便的数据结构来存储、访问、操作网格数据

+   内置从各种文件类型加载的方法，Python 类

+   内置计算多维汇总统计的方法

+   内置数据查询、过滤器方法

+   内置数据操作、清理、重新格式化方法

+   内置属性用于存储关于 nD 数组的信息，例如大小和形状

## **非参数模型**

机器学习概念：一个不假设自然设置函数形式、形状的模型。

+   从训练数据中学习形状，更灵活地适应自然系统的各种形状

+   与参数模型相比，模型与自然设置不匹配的风险更低

通常需要更多的数据来准确估计非参数模型，

+   非参数模型通常具有许多可训练参数，即非参数模型实际上是参数丰富的！

## **范数**

线性回归：向量的范数将向量值映射到表示大小或长度的汇总度量 $[𝟎,\infty)$，

为了训练我们的模型以训练数据为目标，我们需要一个单一的总度量来衡量与训练数据的失配，即训练误差。误差在每个训练数据位置观察到，

$$ \Delta y_i = y_i - \hat{y}_i, \quad \forall \quad i = 1,\ldots,n $$

作为误差向量。我们需要一个单一值来总结所有训练数据，我们可以最小化它！

## **归一化**

特征转换：一种分布重缩放，可以看作是平移、拉伸或压缩一元分布（例如，*直方图*）到最小值为 0.0 和最大值为 1.0。

+   这是对原始属性分布的平移和拉伸/压缩，假设没有形状变化，保持排名

$$ y_i = \frac{x_i - min(x)}{max(x) - min(x)}, \quad \forall \quad i, \ldots, n $$

需要进行标准化和最小/最大归一化的方法：

+   k-means 聚类，k 近邻回归

+   特征排序的 $\beta$ 系数

+   人工神经网络对预测特征的前向转换和对响应特征的反向转换，以提高激活函数的敏感性

## **归一化直方图**

单变量分析：是使用概率图表示单变量统计分布，该图在可能的值范围内对穷举集的每个分箱进行概率绘制。这些是构建归一化直方图的步骤，

1.  将可能的连续特征取值范围划分为 $K$ 个等大小的分箱，$\delta x$：

$$ \Delta x = \left( \frac{x_{max} - x_{min}}{K} \right) $$

或使用可用的类别对分类特征进行分类。

1.  计算每个分箱中样本的数量（频率）$n_k$，$\forall k=1,\ldots,K$，并将其除以总数据量 $n$，以计算每个分箱的概率，

$$ p_k = \frac{n_k}{n}, \forall \quad k = 1,\ldots,L $$

1.  绘制概率与分箱标签的关系图（如果连续则使用分箱中心点）

注意，归一化直方图通常以条形图的形式绘制。

## **独热编码**

特征转换：将特征的取值范围分箱为 K 个，然后对于每个样本，如果样本在分箱内则分配值为 1，如果不在分箱内则分配值为 0

+   分箱策略包括均匀宽度分箱（均匀）和每个分箱中均匀数量的数据（分位数）

+   也称为 K 个分箱离散化

需要进行 K 个分箱离散化的方法，

+   在更高维空间中进行基函数展开

+   将连续特征的取值范围离散化为分类特征，用于如朴素贝叶斯分类器等分类方法

+   构建直方图和用于分布差异的卡方检验

+   互信息分箱

## **袋外样本**

袋装树和随机森林：通过数据重采样，可以证明大约 $\frac{2}{3}$ 的数据将被包括（期望中）。对于基于袋装的集成预测模型，

+   因此，每个模型实现中大约有 $\frac{1}{3}$ 的数据（期望中）未用于训练，这些被称为袋外观测

+   对于每个响应特征观测值 $y_{\alpha}$，有 $\frac{B}{3}$ 个袋外预测，$y^{*,b}_{\alpha}$

+   我们可以汇总这些预测实现的整体，对于回归取平均值，对于分类取众数，以计算单个袋外预测，$y^{*}_{\alpha} = \sum_{\alpha = 1}^{\frac{B}{3}} y^{*,b}_{\alpha}$

+   从所有数据中的这些单个袋外预测中，计算袋外均方误差（MSE），

$$ MSE_{OOB} = \sum_{\alpha = 1}^{\frac{B}{3}} \left[ y^{*}_{\alpha} - y_{\alpha} \right]² $$

对于基于袋装的集成预测机器学习，不需要执行训练和测试分割，可以使用袋外均方误差进行超参数调整。

+   这相当于随机训练和测试分割，可能不公平，难度与计划使用模型相同

+   这将测试比例冻结在大约 $\frac{1}{3}$

## **过拟合模型**

机器学习概念：适合数据噪声或数据特殊性的机器学习模型

+   增加的复杂性通常会在训练数据集上降低误差，但可能会导致测试数据上的误差增加

+   在模型复杂性与测试误差上升、训练误差下降的区域

过拟合机器学习模型的问题，

+   模型复杂性和灵活性超过了可用数据的合理性，数据准确性，频率和覆盖范围

+   训练时高精度，但在测试时精度低，表示模型在远离训练数据案例的真实世界使用中的泛化能力较差

## **参数**（统计学）

机器学习概念：对总体的一个概括性度量

+   例如，总体均值，总体标准差

我们很少有机会访问实际的总体参数，通常我们使用可用的样本统计量来推断总体参数

## **参数**（机器学习）

机器学习概念：机器学习模型中用于控制对训练数据拟合的可训练系数

+   模型参数通过优化计算，以最小化训练数据上的误差，通过解析解或迭代解，例如梯度下降优化

## **参数模型**

机器学习概念：对自然系统功能形式、形状做出假设的模型

+   我们获得了参数较少的简单性和优势

+   对于线性模型，我们只有 $m+1$ 个模型参数

存在一种风险，即我们的模型与自然设置大相径庭，导致模型性能不佳，例如，将线性模型应用于非线性现象。

## **偏相关系数**

多元分析：一种在控制 $𝒁_𝟏,\ldots,𝒁_(𝒎−𝟐)$ 其他特征对 $𝑿$ 和 $𝑌$ 的影响后，计算 $𝑿$ 和 $𝑌$ 之间相关性的方法。注意，我使用 $m-2$ 来考虑 $X$ 和 $Y$ 被移除。

对于 $\rho_(𝑋,𝑌.𝑍_1,…,𝑍_(𝑚−2) )$,

1.  执行线性、最小二乘回归，从 $𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐}$ 预测 $𝑿$。$𝑿$ 通过预测变量进行回归以计算估计值，$𝑿^∗$.

1.  执行线性、最小二乘回归，从 $𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐}$ 预测 $𝒀$。$𝒀$ 通过预测变量进行回归以计算估计值，$𝒀^∗$

1.  在步骤 #1 中计算残差，$𝑿 − 𝑿^∗$，其中 $𝑿^∗=𝒇(𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐})$，线性回归模型

1.  在步骤 #2 中计算残差，$𝒀 − 𝒀^∗$，其中 $𝒀^∗=𝒇(𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐})$，线性回归模型

1.  计算步骤 #3 和 #4 的残差之间的相关系数，$\rho_{𝑿 −𝑿^∗,𝒀 − 𝒀^∗}$

偏相关的假设，对于 $𝝆_(𝑿,𝒀.𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐})$,

+   $𝑿,𝒀,𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐}$ 之间存在线性关系，即所有成对关系都是线性的

+   对于任何单变量分布（单变量异常值）和成对关系（双变量异常值）都没有异常值。偏相关对异常值（如常规相关）非常敏感。

+   高斯分布，单变量和成对的双变量分布都是高斯分布。双变量应该是线性相关的，并且同方差。

## **划分聚类**

聚类分析：所有聚类组分配一次确定，而不是像层次聚类方法那样从 $n$ 个聚类开始，然后迭代地将聚类合并成更大的聚类

+   k-means 聚类是划分聚类，而找到解决方案的启发式方法是迭代的，但实际上解决方案是一下子就完成的

+   容易更新，例如，通过修改原型位置和重新计算组分配

## **多边形去聚类**

数据准备：一种去聚类方法，根据局部采样密度对空间样本分配权重，使得加权统计更有可能代表总体。数据权重分配如下，

+   在密集采样区域采样的样本权重较低

+   在稀疏采样区域采样的样本权重较高

多边形去聚类按以下步骤进行：

1.  使用 Voronoi 多边形划分感兴趣的区域。这些多边形是通过相邻数据点之间的垂直平分线的交点构建的。多边形通过最近的数据点将感兴趣的区域分组

1.  将每个数据点的权重分配与相关 Voronoi 多边形的面积成比例

$$ w(\bf{u}_j) = n \cdot \frac{A_j}{\sum_{j=1}^n} $$

其中 $w(\bf{u}_j)$ 是 $j$ 数据的权重。注意，权重的总和是 $n$；因此，$w(\bf{u}_j)$ 是名义权重 1.0，如果数据在感兴趣区域内均匀分布，则是样本密度。

这里是一些关于多边形去聚类的亮点，

+   多边形去聚类对感兴趣区域的边界敏感；因此，当感兴趣区域扩展或收缩时，分配给感兴趣区域边界附近数据的权重可能会发生根本性的变化

+   多边形去聚类与 1911 年由阿尔弗雷德·H·塞森斯（Afred H. Thiessen）开发的用于计算降水平均值的塞森斯多边形方法相同 []

## **多项式回归**

多项式回归：在线性回归之前将多项式基础扩展应用于预测特征，

$$ y = \sum_{l=1}^{k} \sum_{j=1}^{m} \beta_{j,l} h_l (X_j) + \beta_0 $$

其中 $h$ 在训练数据上转换，$𝑖=1,\ldots,n$，

$$ h_1(x_i) = x_i, \quad h_2(x_i) = x_i², \quad h_3(x_i) = x_i³, \quad h_4(x_i) = x_i⁴, \dots, h_k(x_i) = x_i^k $$

最多到指定的阶数 $𝑘$.

例如，对于单个预测特征，$𝑚 = 1$，最高到 $4^{th}$ 阶，

$$ y = \beta_{1,1} X + \beta_{1,2} X² + \beta_{1,3} X³ + \beta_{1,4} X⁴ + \beta_0 $$

在 $𝒉_𝒍$，$𝑙=1,\ldots,𝑘$ 转换之后，在 $𝑗=1,\dots,𝑚$ 预测特征上，我们有相同的线性方程和利用先前讨论的解析解的能力。

+   我们假设在应用我们的基础扩展之后线性。

现在模型参数，$\beta_(𝒍,𝒊)$，与初始预测特征的转换版本相关，$𝒉_𝒍 (𝑿_𝒋)$。

+   我们失去了解释系数的能力，例如，什么是 $𝑝𝑒𝑟𝑚𝑒𝑎𝑏𝑖𝑙𝑖𝑡𝑦\(⁴$**？

+   通常，模型方差显著更高，即可能存在不稳定的插值和特别是外推

多项式回归模型假设，

+   **无误差** - 预测特征的基础扩展是无误差的，不是随机变量

+   **常数方差** - 响应误差在预测值上是恒定的

+   **线性** - 响应是基础特征的线性组合

+   **多项式** - $X$ 和 $Y$ 之间的关系是多项式

+   **误差独立性** - 响应误差之间是不相关的

+   没有多重共线性* - 基础特征扩展与其它特征之间没有线性冗余

## **人口**

概率概念：在感兴趣区域内对感兴趣属性的详尽、有限列表。

+   例如，在储层内每个位置的孔隙率测量的详尽集合

通常，整个总体并不总是可访问的，我们使用有限的样本来对总体进行推断

## **幂律平均值**

特征转换：基于平均的通用形式，将较小尺度度量在较大体积中的聚合汇总成一个代表较大体积的单一值

$$ \overline{x}_p = \left(\frac{1}{n}\sum_{i=1}^n x_i^p \right)^{\frac{1}{p}} $$

+   有助于计算有效渗透率，其中流动不是平行也不是垂直于不同的渗透率层

+   流体模拟可以应用于校准（计算幂律平均的适当功率）

## **精确度**（分类准确度指标）

朴素贝叶斯：一个分类预测模型的准确度度量，混淆矩阵中每个 $k$ 类别的单一汇总指标。

+   真正阳性与所有阳性的比率，真正阳性 + 假阳性

$$ Precision_k = \frac{ n_{k,\text{true positives}} }{ n_{k,\text{true positives}} + n_{k,\text{false positives}}} = \frac{ n_{k,\text{true positives}} }{ n_{k, \text{all positives}} } $$

## **预测区间**

线性回归：下一次预测的不确定性表示为一个范围，下限和上限，基于一个称为置信水平的指定概率区间。

我们这样传达置信区间，

+   在给定预测特征值的情况下，有 95%的概率（或 20 次中的 19 次）表明真实储层的 NTG 在 13%到 17%之间。

是我们预测的不确定性，对于预测区间，我们进行积分，

+   模型中的不确定性 $𝐸{\hat{𝑌}|𝑋=𝑥}$

+   模型中的误差，条件分布 $\hat{Y}|X=x$

## **预测，预测统计**

机器学习概念：根据对总体或总体模型的假设估计下一个样本（或多个样本）

+   例如，给定我们的储层模型，预测下一个井（预钻评估）样本，例如，孔隙率、渗透率、产量等。

## **预测特征**

机器学习概念：预测机器学习模型的输入特征。我们可以将预测机器学习模型概括为，

$$ y = \hat{f}(x_1,\ldots,x_m) + \epsilon $$

其中响应特征是 $y$，预测特征是 $x_1,\ldots,x_m$，而 $\epsilon$ 是模型误差

+   传统统计学使用术语独立变量

## **预测特征空间**

特征排序：指的是预测特征，不包括响应特征（即，

+   我们需要预测的所有预测特征的组合

+   可能被称为预测特征空间。

通常，我们在预测特征空间上训练和测试机器的预测。

+   该空间通常是超立方体，每个轴代表一个预测特征，从最小值延伸到最大值，覆盖每个预测特征的取值范围

+   预测特征空间的形状可能更复杂，例如，我们可以屏蔽或移除数据覆盖较差的子集。

## **原始数据**

机器学习概念：感兴趣的特征数据样本，用于构建模型的目标特征，例如，

+   从岩心和测井数据中得到的孔隙率测量值，用于构建完整的 3D 孔隙率模型。任何孔隙率样本都是原始数据

+   与次级特征相反，例如，如果我们有用于预测孔隙率的岩性数据，则岩性数据是次级数据

## **主成分分析**

主成分分析：多种降维方法之一，将数据转换到较低维度

+   给定特征 $𝑋_1,\dots,𝑋_𝑚$，我们需要 ${m \choose 2}=\frac{𝑚 \cdot (𝑚−1)}{2}$ 个散点图来可视化二维散点图。

+   一旦我们有 4 个或更多变量，理解数据就变得非常困难。

+   回忆维度诅咒，影响推理、建模和可视化。

一种解决方案是找到一个好的低维 $𝑝$ 表示，以表示原始维度 $𝑚$

在降维表示中的好处：

1.  数据存储/计算时间

1.  更容易可视化

1.  还处理多重共线性问题

主成分分析的主要观点，

+   *正交变换* - 将一组观测值转换为一组线性不相关的变量，称为主成分，这种变换保留了成对距离，即是一种旋转

+   *可用的主成分数 ($k$)* - 是 min⁡($𝑛−1,𝑚$)，受变量/特征 $𝑚$ 和数据数量的限制

组件是有序的，

+   第一成分描述了最大的可能方差 / 解释尽可能多的变异性

+   下一个成分描述了最大的可能剩余方差

+   最多到主成分的最大数量

基于特征值和特征向量，

计算数据协方差矩阵，特征对的协方差，然后从协方差矩阵中计算特征向量和特征值，

+   特征值是每个成分解释的方差。

+   数据协方差矩阵的特征向量是主成分。

## **概率密度函数** (PDF)

单变量分析：使用概率密度函数 $f(x)$ 的函数表示统计分布，该函数在所有可能的特征值范围 $x$ 上，这些是 PDF 的概念，

+   非负约束，密度不能为负，

$$ 0.0 \le f(x) $$

+   对于连续特征，密度可能大于 1.0，因为密度是可能性的度量，而不是概率的度量

+   在 $x$ 的一个范围内积分密度以计算概率，

$$ 0 \le \int_a^b f(x) dx = P(a \le x \le b) \le 1.0 $$

+   概率封闭，PDF 曲线下方的面积之和等于 1.0，

$$ \int_{-\infty}^{\infty} f(x) dx = 1.0 $$

非参数 PDF 通过核函数（通常是小的高斯分布）计算，对所有数据进行求和；因此，在计算 PDF 时存在一个隐含的尺度（平滑度）参数。

+   过大的核函数会平滑掉关于单变量分布的重要信息

+   过窄会导致过于嘈杂的 PDF，难以解释。

这类似于选择直方图或归一化直方图的 bin 大小。

参数 PDF 是可能的，但需要将模型拟合到数据中，步骤如下，

1.  选择一个参数分布，例如高斯分布、对数正态分布等。

1.  根据可用数据，通过最小二乘法或最大似然法等方法计算参数分布的参数。

## **概率非负性，归一化**

概率概念：概率的基本约束包括，

1.  **有界**，$0.0 \le P(A) \le 1.0$

1.  **闭包**，$P(\Omega) = 1.0$

1.  **空集**，$P(\emptyset) = 0.0$

## **接受概率**（MCMC）

贝叶斯线性回归：作为拒绝抽样中候选样本被添加到样本中的似然应用。

+   通过蒙特卡洛模拟执行条件接受，

+   依次从条件分布中进行抽样

接受规则是，

+   如果 $𝑃(𝑎𝑐𝑐𝑒𝑝𝑡) \ge 1$，接受 - 接受

+   如果 $𝑃(𝑎𝑐𝑐𝑒𝑝𝑡) \lt 1$，则条件接受，抽取 $𝑝 ∼ U[0,1]$，如果 $𝑝 \le 𝛼$ 则接受

## **概率算子**

概率概念：与概率和不确定性问题工作相关的常见概率算子，

*事件的并集* - 结果的并集，$A$ 或 $B$ 的概率通过概率加法规则计算，

$$ P(A \cup B) = P(A) + P(B) - P(A,B) $$

*事件的交集* - 结果的交集，$A$ 和 $B$ 的概率表示为，

$$ P(A \cap B) = P(A,B) $$

只有在假设 $A$ 和 $B$ 独立的情况下，才能从 $A$ 和 $B$ 的概率中计算出它，

$$ P(A,B) = P(A) \cdot P(B) $$

如果 $A$ 和 $B$ 之间存在依赖关系，则需要条件概率 $P(A|B)$ 而不是边缘概率 $P(A)$，

$$ P(A,B) = P(A|B) \cdot P(B) $$

*互补事件* - 是概率中的非操作符，如果我们定义 $A$，那么 $A$ 的补集 $A^c$ 就不是 $A$，并且我们有这个结果闭包关系，

$$ P(A) + P(A^c) = 1.0 $$

对于超越单变量问题，可以考虑互补事件，例如考虑这个双变量闭包，

$$ P(A|B) + P(A^c|B) = 1.0 $$

注意，给定的术语必须相同。

*互斥事件* - 指的是不交叉或不具有任何共同结果的事件。我们用集合符号表示为，

$$ \{x: x \in A \text{ and } x \in B \} = \emptyset $$

以及 $A$ 和 $B$ 的联合概率，

$$ P(A \cap B) = P(A,B) = 0 $$

## **概率视角**

概率概念：计算概率的 3 个主要视角：

1.  *长期频率* - 概率作为结果的比率，需要重复观察实验。是*频率主义概率*的基础。

1.  *物理趋势或倾向性* - 从对系统或建模的知识中得出的概率，例如，我们可以知道抛硬币得到正面的概率，而不进行实验。

1.  *信念度* - 反映我们对结果的确定性，非常灵活，可以对任何事物分配概率，并随着新信息的更新。是*贝叶斯概率*的基础。

## **原型**（聚类）

聚类分析：用特征空间中的点集表示样本数据。

+   原型通常是实际样本。

+   样本数据通常被分配到最近的（欧几里得）距离原型。

## **定性特征**

机器学习概念：关于无法直接测量的数量，需要解释测量，并用文字（而不是数字）描述，例如，

+   岩石类型 = 砂岩

+   分带 = 黄铜矿-黄铁矿-金 高级铜矿带

## **定量特征**

机器学习概念：可以测量并由数字表示的特征，例如，

+   年龄 = 10 Ma（百万年）

+   孔隙率 = 0.134（体积的空隙部分）

+   饱和度 = 80.5%（体积百分比）

类似于*定性特征*，通常需要解释，例如，总孔隙率可以测量，但应通过解释或模型转换为有效孔隙率。

## **$r²$**（也称为确定系数）

线性回归：线性回归中模型解释的方差比例

这只适用于线性模型，其中：

$$ \sigma²_{tot} = \sigma²_{reg} + \sigma²_{res} $$

其中 $\sigma²_{tot}$ 是响应特征训练的方差，$y_i$，$\sigma²_{reg}$ 是模型预测的方差，$\hat{y}_i$，$\sigma²_{res}$ 是误差的方差，$\Delta y_i$。

+   对于线性回归，$r² = \left( \rho_{x,y} \right)²$

对于非线性模型，这不太可能成立，那么 $\frac{\sigma²_{𝑟𝑒𝑔}}{\sigma²_{𝑡𝑜𝑡}}$ 可能超过 $[0,1]$，对于我们的非线性模型回归模型，我们将使用更稳健的度量，例如均方误差（MSE）。

## **随机森林**

袋装树和随机森林：一种基于标准袋装方法的集成预测模型，具体来说，

+   使用决策树

+   通过限制每个分割只考虑 $m$ 个可用预测因子中的 $p$ 个随机子集来多样化单个树。

有多种方法可以从 $m$ 个可用特征中计算 $p$，

$$ p = \sqrt{m} $$

是常见的。注意，如果 $p = m$，则随机森林是树袋法。

更多关于集成模型多样性的好处评论，

+   通过集成估计减少模型方差，如标准误差所表示的，

$$ \sigma_{\overline{x}}² = \frac{\sigma_{s}²}{n} $$

假设样本是不相关的。树袋集成的一个问题是集成中的树可能高度相关。

+   当存在主导预测特征时会发生这种情况，因为它将始终应用于顶部分割（s），结果是集成中的所有树都非常相似（即相关）

+   对于高度相关的树，集成在减少模型方差方面显著较少

+   这迫使集成中的每一棵树以不同、去相关的模式进化

## **实现**

机器学习概念：来自 *随机变量* 或来自 *随机函数* 的联合结果。

+   是从随机变量 $X$（或随机函数的联合结果）得出的结果，

+   用小写表示，例如，$x$

+   对于空间设置，通常包括一个位置向量 $\bf{u}$，以描述位置，例如，$x(\bf{u})$，作为 $X(\bf{u})$

+   来自模拟的结果，例如，蒙特卡洛模拟，顺序高斯模拟，从 RV（RF）（联合）采样的方法

+   通常，我们假设所有实现都是等概率的，即具有相同的出现概率

## **实现**（不确定性）

机器学习概念：通过保持输入参数和模型选择不变，仅改变随机数种子进行随机模拟的多个空间、地下模型

+   这些模型代表空间不确定性

+   例如，保持孔隙率平均值不变，并观察多个实现中远离井的孔隙率变化

## **学习一些编码的理由**

机器学习工作流程构建和编码：Pyrcz 教授对所有科学家和工程师学习一些编码的理由，

+   *透明度* – 没有编译器会接受挥手！编码迫使你的逻辑被任何其他科学家或工程师审查。

+   *可重复性* – 运行它并得到答案，交给一个同行，他们运行它并得到相同的答案。这是科学方法的一个原则。

+   *量化* – 程序需要数字，并推动我们从定性到定量。给程序喂食，发现新的看待世界的方式。

+   *开源* – 利用全球的智慧。查看包、代码片段，并惊叹于伟大思想者免费分享的成果。

+   *打破障碍* – 不要把它扔过栅栏。与开发者一起坐在桌旁，分享更多你的专业知识，以获得更好的部署产品。

+   *部署* – 与他人分享你的代码，并扩大你的影响力。性能指标或利他主义，你的好工作使许多人受益。

+   *效率* – 最小化工作中的无聊部分。构建一套用于自动化常见任务的脚本，并花更多时间进行科学和工程！

+   *永远有再次做它的机会!* – 你只做了一次吗？编写和自动化工作流程可能需要 2-4 倍的时间。通常，这是值得的。

像我们一样 – 这将改变你。用户会感到受限，程序员真正利用了他们的应用程序和硬件的力量。

## **召回率**（分类准确度指标）

朴素贝叶斯：一种分类预测模型的准确性度量，混淆矩阵中每个 $k$ 类别的单一汇总指标。

+   测试数据集中该类别的所有案例中真实正例与所有案例的比率

$$ Recall_k = \frac{ n_{k, \text{true positives}} }{n_k} $$

## **递归特征消除**

特征排序：一种通过递归删除特征并使用剩余特征构建模型的方法。

+   使用所有特征构建模型，计算特征排序指标，例如系数或特征重要性，具体取决于建模方法中哪些可用

+   删除特征重要性最低的特征并重新构建模型

重复此过程，直到只剩下一个特征

任何预测模型都可以使用，

+   该方法为所有特征分配 $1,\ldots,𝑚$ 的排名，按删除顺序的逆序，即最后剩下的特征最重要，最先删除的最不重要

## **储层建模工作流程**

机器学习工作流程构建和编码：以下为常见的地质统计学储层建模工作流程：

1.  整合所有可用信息，构建多个地下场景和实现，以采样不确定性空间

1.  将所有模型应用于传递函数以采样决策标准

1.  组装决策标准的分布

1.  考虑此不确定性模型，做出最佳储层开发决策

## **响应特征**

机器学习概念：预测机器学习模型的输出特征。我们可以将预测机器学习模型概括为，

$$ y = \hat{f}(x_1,\ldots,x_m) + \epsilon $$

其中响应特征是 $y$，预测特征是 $x_1,\ldots,x_m$，而 $\epsilon$ 是模型误差

+   传统统计学使用术语因变量

## **岭回归**（Tikhonov 正则化）

岭回归：一种线性、参数化的预测模型，

$$ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 $$

对于 L2 范数损失函数，模型参数 $b_1,\ldots,b_m,b_0$ 的解析解是可用的，误差是总和并平方的已知最小二乘。

+   我们最小化包括误差、训练数据上的残差平方和（RSS）和正则化项的损失函数：

$$ \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)² + \lambda \sum_{\alpha = 1}^m b_{\alpha}² $$

其中 $y_i$ 是实际响应特征值，$\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ 是模型预测，在 $\alpha = 1,\ldots,n$ 的训练数据上，$\lambda \sum_{\alpha = 1}^m b_{\alpha}²$ 是收缩惩罚项。

在岭回归中，我们向最小化中添加一个超参数 $\lambda$，收缩惩罚项 $\sum_{j=1}^m b_{\alpha}²$。

因此，岭回归训练整合了两个经常相互竞争的目标，以找到模型参数，

+   找到使训练数据误差最小的模型参数

+   将斜率参数最小化到零

注意：lambda 不包括截距，$b_0$。

$\lambda$ 是一个控制模型拟合程度的超参数，可能与模型偏差-方差权衡有关。

+   当 $\lambda \rightarrow 0$ 时，解趋近于线性回归，没有偏差（相对于线性模型拟合），但模型方差可能更高

+   随着 $\lambda$ 的增加，模型方差减小，模型偏差增加，模型变得简单

+   当 $\lambda \rightarrow \infty$ 时，模型参数 $b_1,\ldots,b_m$ 收缩到 0.0，模型预测趋近于训练数据响应特征均值

## **样本**

机器学习概念：已测量的值和位置的集合

+   例如，来自储层中井的井日志的 1,000 个孔隙度测量值

+   或者在一个 1000 x 1000 的 2D 网格上对感兴趣储层单元的 1,000,000 个声波阻抗测量值

## **场景**（不确定性）

机器学习概念：通过改变输入参数或其他建模选择进行随机模拟计算出的多个空间、地下模型，以表示由于模型参数和模型选择的推断所引起的不确定性

+   例如，模型三个孔隙度输入分布，孔隙度均值低、中、高，并改变输入分布来计算新的地下模型

## **次级数据**

机器学习概念：为另一个特征提供数据样本，而不是感兴趣的特征，用于构建模型的特征，但用于提高目标特征的预测。

+   需要建立主次级数据之间关系模型

例如，空间中的样本，

+   声波阻抗（次级数据）以支持计算孔隙度模型，感兴趣的特征

+   孔隙度（次级数据）以支持计算渗透率模型，感兴趣的特征

## **地震数据**

机器学习概念：使用遥感进行间接测量，反射地震使用声源（如地震检波器）和接收器（如地震检波器）来绘制高覆盖率和一般低分辨率的声波反射图。一些更详细的说明，

+   地震反射（振幅）数据被反演为岩石属性，例如声阻抗，与井声波测井数据一致且位置锚定

+   提供框架，为储层范围和形状提供边界表面，以及关于储层属性（如孔隙度和岩性）的软信息。

## **Shapley 值**

特征排序：基于模型、局部（针对单个预测）和全局（针对一系列预测）的特征重要性，通过学习每个特征对预测的贡献。

需要一种可解释的机器学习方法来支持复杂的模型，但通常可解释性较低。

有两种方法可以提高模型的可解释性，

1.  减少模型的复杂性，但也可能降低模型精度

1.  开发改进的、无差别的（适用于任何模型）模型诊断，即 Shapley 值

Shapley 值是一种合作博弈论方法，

+   根据对边际贡献的总结来分配资源给玩家，即，在玩家之间分配付款

+   计算每个预测特征对推动响应预测远离响应均值贡献的大小

+   边际贡献和 Shapley 值以响应特征的单位计

+   以响应特征的单位

## **辛普森悖论**

多元分析：当组合（或分离）组时，数据趋势会逆转或消失。在分组数据的相关分析中经常观察到，例如，

+   每组之间都有负相关，但整体上呈正相关

## **软数据**

机器学习概念：具有高度不确定性的数据，因此数据不确定性必须整合到模型中

+   例如，从声阻抗校准的局部孔隙率的概率密度函数

软数据集成需要如*指示克里金*、*指示模拟*和*p 场模拟*或随机化数据的流程，这些流程使用假设硬数据的标准模拟方法，如*顺序高斯模拟*。

+   软数据集成是一个高级主题，也是当前研究的热点，但通常使用标准的、地下建模软件包来完成

## **空间抽样**（有偏）

数据准备：样本选择应确保样本统计量不代表总体参数。例如，

+   样本均值与总体均值不同

+   样本方差与总体方差不同

当然，人口参数是不可获取的，因此我们无法直接计算抽样偏差，即样本统计量与总体参数之间的差异。我们可以用来检查抽样偏差的方法，

+   评估样本以进行优先抽样、聚集、过滤等。

+   应用*去聚集*并检查总结统计量是否有重大变化，这是使用去聚集进行诊断

## **空间抽样**（聚集）

数据准备：空间样本具有优先选择的地点，即聚集，导致统计偏差，

+   通常，空间样本会聚集在具有更高价值样本的地点，例如，高孔隙率和渗透率，优质页岩用于非常规储层，低声阻抗指示更高的孔隙率等。

当然，人口参数是不可获取的，因此我们无法直接计算抽样偏差，即样本统计量与总体参数之间的差异。我们可以用来检查抽样偏差的方法，

+   评估样本以进行优先抽样、聚集、过滤等。

+   应用*去聚集*并检查结果是否有重大变化，这是使用去聚集进行诊断

## **空间抽样**（常见做法）

数据准备：样本地点的选择是为了，

*减少不确定性*——通过回答问题，例如，

+   污染羽流延伸多远？——样本边缘

+   地质断层在哪里？——基于地震解释进行钻井

+   最高矿物品位是多少？——样本最佳部分

+   水库延伸多远？——偏心钻井

*直接最大化净现值*——在收集信息时，例如，

+   最大化生产率

+   最大化提取的矿物吨数

换句话说，我们的样本通常是双用途的，例如，为勘探和评估信息而钻探的井随后用于生产。

## **空间抽样**（代表性）

数据准备：如果我们是为了代表性进行抽样，即样本集和由此产生的样本统计量代表总体，根据抽样理论，我们有 2 种选择：

*随机抽样*——从总体中收集的每个潜在样本被抽样的可能性相等。这包括，

+   选择特定位置不会影响后续位置的选择。

+   假设总体大小远大于样本大小；因此，由于不重复抽样（只能对位置进行一次抽样的约束），样本之间没有显著的关联。注意，由于地下是稀疏抽样的巨大总体，这通常不是问题

*规则抽样*——在相等的空间或时间间隔进行抽样。虽然随机抽样是首选，但只要，

+   常规采样间隔与数据中的自然周期性不一致，例如，峰值是系统性地采样，导致样本统计量偏高

## **谱聚类**

谱聚类：一种分区聚类方法，它利用表示数据成对关系的矩阵的谱、特征值和特征向量。

+   从数据样本成对关系（由图拉普拉斯矩阵表征）进行降维

+   特征值，特征向量等同于通过线性、正交特征投影和旋转进行主成分分析降维，以最佳描述方差

谱聚类的优点，

+   编码成对关系的能力，整合专家知识。

+   特征值提供了关于聚类数量的有用信息，基于创建 k 个聚类所需的“切割”程度

+   样本数据成对关系的低维表示

+   结果特征值和特征向量可以解释，特征值描述了每个组连接的数量，而特征向量被分组形成聚类

## **标准化**

特征变换：一种分布缩放，可以将其视为对单变量分布（例如，*直方图*）的平移、拉伸或压缩，以达到均值为 0.0 和方差为 1.0。

$$ y_i = \frac{1}{\sigma_x}(x_i - \overline{x}), \quad \forall \quad i, \ldots, n $$

其中 $\overline{x}$ 和 $\sigma_x$ 是原始均值和方差。

+   这是对原始属性分布的平移和拉伸/压缩

+   假设形状不变，保持秩

## **基于随机梯度的优化**

LASSO 回归：一种通过迭代最小化损失函数来求解模型参数的方法。通过使用批次，将随机性和提高计算效率添加到梯度下降中，

+   批次是具有指定大小 $n_{batch}$ 的训练数据的随机子集

+   导致损失函数梯度的随机近似，计算速度更快

+   批次减少梯度下降的准确性，但加快计算速度，可以执行更多步骤，通常比梯度下降更快

+   增加 $𝑛_{𝑏𝑎𝑡𝑐ℎ}$ 以提高梯度估计的准确性，并减少 $𝑛_{𝑏𝑎𝑡𝑐ℎ}$ 以加快步骤

Robbins-Siegmund (1971) 定理 - 对于凸损失函数收敛到全局最小值，对于非凸损失函数收敛到全局或局部最小值。

步骤包括，

1.  从随机的模型参数开始

1.  选择训练数据的随机子集，$n_{batch}$

1.  计算模型参数在随机批次上的损失函数和损失函数梯度，

$$ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) = \frac{L(y_{\alpha}, F(X_{\alpha}, b_1 - \epsilon)) - L(y_{\alpha}, F(X_{\alpha}, b_1 + \epsilon))}{2\epsilon} $$

1.  通过沿斜坡/梯度下降来更新参数估计，

$$ \hat{b}_{1,t+1} = \hat{b}_{1,t} - r \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) $$

其中 $r$ 是学习率/步长，$\hat{b}(1,𝑡)$ 是当前模型参数估计，$\hat{b}(1,𝑡+1)$ 是更新的参数估计。

## **随机模型**

机器学习概念：不确定的系统或过程，由多个模型、*实现*和受统计约束的*情景*表示，

+   例如，像地统计模拟模型这样的数据驱动模型，它们整合了不确定性

优点：

+   速度

+   不确定性评估

+   报告显著性、置信/预测区间

+   尊重许多类型的数据

+   数据驱动方法

缺点：

+   使用的物理有限

+   统计模型假设/简化

对于随机模型的替代方案，请参阅 *确定性模型*。

## **统计学** (实践)

机器学习概念：收集、组织、解释数据以及得出结论和做出决策的理论和实践。

## **统计学** (测量)

机器学习概念：样本的汇总度量，例如，

+   样本均值 - $\overline{x}$

+   样本标准差 - $s$,

我们使用统计学作为模型参数的估计，这些参数总结了总体(*推断*)

## **统计分布**

单变量分析：对于一个特征，描述其在可能值范围内的发生概率。我们用以下方式表示单变量统计分布，

+   *直方图*

+   *归一化直方图*

+   *概率密度函数* (PDF)

+   *累积分布函数* (CDF)

我们从统计分布中学到了什么？例如，

+   最小值和最大值是什么？

+   我们是否有大量的低值？

+   我们是否有大量的高值？

+   我们是否有异常值，以及任何其他不合理且需要解释的值？

## **支持向量** (支持向量机)

支持向量机：位于边缘或被错误分类的训练数据更新支持向量机分类模型。

+   在支持向量机模型中，训练数据很好地位于正确区域内，不是支持向量，对模型没有影响

## **支持向量机**

支持向量机：一种预测性的二分类机器学习方法，当组间分离较差时是一种好的分类方法。

+   将原始预测特征投影到更高维空间，然后应用线性、平面或超平面，

$$ 𝑓(𝑥) = 𝑥^𝑇 \beta +\beta_0 $$

其中 $\beta$ 是一个向量，与 $\beta$ 一起是超平面模型参数，而 $x$ 是预测特征矩阵，所有这些都在高维空间中。

$$ 𝐺(𝑥)=\text{𝑠𝑖𝑔𝑛}\left( 𝑓(𝑥) \right) $$

$𝑓(𝑥)$ 与决策边界的符号距离成正比，而 $𝐺(𝑥)$ 是决策边界的侧面，$-$ 一侧和 $+$ 另一侧，$f(x) = 0$ 在决策边界上。

我们通过以下方式表示约束，每个类别的所有数据都必须在边界的正确一侧，

$$ y_i \left( x_i^T \beta + \beta_0 \right) \geq 0 $$

其中，如果类别 $y_i$ 为 -1 或 1，则此条件成立。我们需要一个允许某些误分类的模型，

$$ y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i $$

我们引入了边界 $𝑀$ 和边界距离的概念，错误用 $\xi_i$ 表示。现在我们可以将我们的损失函数表示为，

$$ \underset{\beta, \beta_0}{\text{min}} \left( \frac{1}{2M²} + C \sum_{i=1}^N \xi_i \right) $$

条件是，$\xi_i \geq 0, \quad y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i$.

这是高维空间中的支持向量机损失函数，其中 𝛽,𝛽_0 是多线性模型参数。

训练支持向量机，通过找到最大化边界 $M$ 的模型参数，同时最小化错误 $\sum_{i=1}^N \xi_i$

+   $𝑪$ 超参数加权错误之和 $xi_𝑖$，较高的 $𝐶$ 将导致边界 $M$ 减少，并导致过拟合

+   较小的边界，使用较少的数据来约束边界，称为支持向量

+   训练数据在边界的正确一侧没有影响

这里是支持向量机的一些关键方面，

+   被称为支持向量机，而不是机器，因为使用新的核可以得到新的机器

+   有许多核可用，包括多项式和径向基函数

主要超参数是 $C$，其成本为

超参数与核的选择有关，例如，

+   *多项式* - 多项式阶数

+   *径向基函数* - $\gamma$ 与训练数据的距离影响成反比

## **表格数据**

机器学习工作流程构建和编码：数据表，每行代表一个样本，每列代表一个特征

由于 Pandas 的 DataFrame 是处理表格数据的便捷类，

+   方便的数据结构用于存储、访问和操作表格数据

+   内置方法从各种文件类型、Python 类甚至直接从 Excel 加载数据

+   内置方法用于计算摘要统计和可视化数据

+   内置的数据查询、排序、数据过滤方法

+   内置的数据操作、清理和重新格式化方法

+   内置属性用于存储有关数据的信息，例如大小、空值数量和空值

## **训练和测试分割**

机器学习概念：对于模型交叉验证，在预测模型训练之前，保留一部分数据作为测试数据。

+   训练数据用于训练模型参数，而保留的测试数据用于调整模型超参数

+   超参数调整是选择超参数组合，以最小化保留测试数据上的误差范数

最常见的方法是随机选择，这可能不是公平的测试，

+   测试难度的范围与模型在现实世界中的应用相似

+   太简单了——测试案例与训练案例相同或几乎相同，随机抽样通常过于简单

+   太难了——测试案例与训练案例非常不同，模型预计将严重外推

如 k 折交叉验证等替代方法提供了对所有可用数据进行测试的机会，但需要，

+   在超参数组合上训练 k 个预测机器学习模型

+   对 k 个模型测试错误的聚合，用于选择最佳超参数，超参数调整

此外，还有包括训练、验证和测试数据子集的替代工作流程

## **转移函数**（储层建模工作流程）

机器学习概念：应用于空间、地下模型实现和场景的计算，以计算决策标准，这是一个用于支持决策的价值、健康、环境和安全的指标。一些示例转移函数包括，

+   *运输和生物衰减* - 数值模拟以模拟泵和治处理操作期间随时间推移的土壤污染物浓度

+   *体积计算* - 用于计算原地总油量以计算资源

+   *异质性指标* - 作为采收率指标，从资源中估计储量

+   *流动模拟* - 用于计划井的预钻生产预测

+   *惠特尔的坑优化* - 用于计算矿产资源最终坑壳

## **不确定性建模**

机器学习概念：在样本时间计算一个特征在位置或多个位置的可能的值范围。一些考虑因素，

+   我们对样本和模型预测精度的限制进行量化

+   不确定性是一个模型，没有客观的不确定性

+   不确定性是由我们的无知引起的

+   不确定性是由稀疏采样、测量误差和偏差以及异质性引起的

我们通过多个模型、场景和实现来表示不确定性：

+   场景 - 通过改变输入参数或其他建模选择，通过随机模拟计算多个空间、地下模型，以表示由于模型参数和模型选择的推断引起的不确定性

+   实现方式 - 通过保持输入参数和模型选择不变，仅改变随机数种子，通过随机模拟计算多个空间、地下模型

## **欠拟合模型**

机器学习概念：一个过于简单、复杂性和灵活性太低、无法拟合自然现象的机器学习模型，导致模型偏差非常高。

+   欠拟合模型通常接近响应特征的全球均值

+   欠拟合模型在训练和测试数据上具有高误差

+   增加的复杂性通常会在训练和测试数据集上降低误差

+   在模型复杂度下降的训练和测试误差区域

欠拟合机器学习模型的问题，

+   在给定数据、数据准确性、频率和覆盖范围的情况下，更多的模型复杂性和灵活性是不够的

+   训练和测试中的低精度表示远离训练数据案例的实际应用，表明模型泛化能力差

## **事件联合**（概率）

概率概念：结果的联合，$A$或$B$的概率通过概率加法规则计算，

$$ P(A \cup B) = P(A) + P(B) - P(A,B) $$

## **单变量参数**

单变量分析：基于对总体中一个特征测量的汇总度量

## **单变量统计**

单变量分析：基于样本中一个特征测量的汇总度量

## **无监督学习**

聚类分析：从未标记数据中学习数据模式

+   没有响应特征，$𝑌$，只有预测特征，$𝑋_1,ldots,𝑋_𝑚$

+   机器通过模仿数据的紧凑表示进行学习

+   通过特征投影、分组分配、神经网络潜在特征等捕捉模式

+   专注于对总体、自然系统的推理，而不是对响应特征的预测

在本课程中，我们使用推断和预测机器学习的术语，所有涵盖的推断机器学习方法都是无监督的。

## **变量**（也称为特征）

机器学习概念：在研究中测量的任何属性，例如，

+   孔隙率、渗透率、矿物浓度、饱和度、污染物浓度

+   在数据挖掘/机器学习中这被称为*特征*

+   度量通常需要显著的分析、解释等

## **方差膨胀因子**（VIF）

特征排序：衡量预测特征（$X_i$）与所有其他预测特征（$X_j, \forall j \ne i$）之间的线性多重共线性

首先，我们针对所有其他预测特征计算给定预测特征的线性回归。

$$ X_i = \sum_{j, j \ne i}^m X_j + \epsilon $$

从此模型中我们确定确定系数，$R²$，也称为解释方差

然后我们计算方差膨胀因子如下：

$$ VIF = \frac{1}{1 - R²} $$

## **体积-方差关系**

特征变换：随着 *体积支持*（尺度）的增加，方差减少

预测体积-方差关系是处理数据和多尺度模型多个尺度的基础。一些一般观察和假设，

+   当体积支持、尺度变化时，均值不会改变。只有方差会改变

+   可能会发生形状变化（我们在这里不会处理这个问题）。最佳实践是经验性地检查形状变化。通常假设没有形状变化（*仿射校正*）或使用形状变化模型（间接对数正态校正）。

+   分布中的方差减少与空间连续性的范围成反比。方差减少得更快（在较小的体积增加中）对于较短的空间连续性范围。

在常见的尺度变化中，这种影响可能是显著的；因此，忽略体积-方差关系是不适当的，

+   我们不进行这种放大，体积支持变化完美，这就是为什么它仍然被称为缺失尺度。我们很少有足够的数据来严格建模这一点

+   我们需要一个模型来预测体积支持变化引起的方差变化

存在一些体积支持的变化，尺度模型，

*经验性* - 建立一个小规模、高分辨率模型，并对其进行数值放大。例如，计算渗透率的高分辨率模型，应用流体模拟来计算 $v$ 尺度块上的有效渗透率

*幂律平均* - 有一种称为幂律平均的灵活方法。

$$ z_V = \left[ \frac{1}{n} \sum z_v^{\omega} \right] ^{\frac{1}{\omega}} $$

其中 $\omega$ 是平均的幂。例如：

+   $\omega = 1$ 是常规线性平均

+   $\omega = -1$ 是谐波平均

+   $\omega = 0$ 是几何平均（这在 $\omega \rightarrow 0$ 的极限中得到了证明）

如何计算 $\omega$？

+   对于某些情况，我们从理论中知道正确的 $\omega$ 值，例如，对于与床层正交的流动，我们选择 $\omega = -1.0$ 来放大渗透率

+   流体模拟可以应用于数值放大渗透率，然后反向计算校准的 $\omega$

*模型* - 直接调整尺度变化的统计量。例如，在假设线性平均和静态变差函数和方差的情况下：

$$ f = 1 - \frac{\overline{\gamma}(v,v)}{\sigma²} $$

其中 $f$ 是方差减少因子，

$$ f = \frac{D²(v,V)}{D²(\cdot,V)} = \frac{D²(v,V)}{\sigma²} $$

换句话说，$f$ 是基于 $v$ 尺度下的方差与原始数据点支持尺度下的方差的比值，

+   变差函数模型

+   数据的尺度，$\cdot$ 和 $v$ 的尺度

## **维恩图**

概率概念：一个图表，用于传达概率的视觉工具。我们从维恩图中能学到什么？

+   区域大小 $\propto$ 发生概率

+   $\Omega$ 的比例，所有可能的结果用一个框表示，即 $1.0$ 的概率

+   重叠 $\propto$ 联合发生概率

维恩图是可视化边缘、联合和条件概率的一个优秀工具。

## **井日志数据**

机器学习概念：作为一种更便宜的采样井的方法，它不会中断钻井作业，井日志在井上非常常见。通常所有井都有各种井日志可用。例如，

+   在试验垂直井中进行的伽马射线测量以评估页岩的位置和质量，以针对（着陆）水平井

+   中子孔隙率以评估高孔隙率储层砂的位置

+   在钻孔中的伽马射线用于绘制钍矿化

井日志数据对于支持地下资源解释至关重要。一旦由岩心数据锚定，它们提供了建模整个储层概念/框架以进行预测所必需的覆盖范围和分辨率，例如，

+   使用与井日志数据同位处的岩心数据校准的井日志数据用于绘制关键地层层位，包括储层和封堵单元

+   井日志应用于深度校正从地震中反演出的特征，这些特征由于感兴趣体积内岩石速度的不确定性而存在位置精度问题

## **弱学习器**

梯度提升：预测模型仅略优于随机

$$ 𝑌 = \hat{f}_𝑘(𝑋_1,\ldots,𝑋_𝑚) $$

其中 $\hat{f}_𝑘$ 是第 $𝑘$ 个弱学习器，$𝑋_1,\ldots,𝑋_𝑚$ 是预测特征，$\hat{Y}$ 是响应特征的预测。

常用“弱预测器”这个术语，具体来说，对于分类模型，术语是“弱分类器”。

## **井日志数据，图像日志**

机器学习概念：这是一种特殊的**井日志**情况，其中井日志在井筒内以各种方位角间隔重复，从而产生一个二维（展开）图像，而不是沿着井筒的 1D 线。例如，全孔径地层微成像器（FMI）具有：

+   80% 的井筒覆盖率

+   0.2 英寸（0.5 厘米）分辨率垂直和水平

+   30 英寸（79 厘米）的探测深度

可以应用于观察岩性变化、层倾斜和沉积结构。

## 注释

这是对地统计学的基本介绍。如果您想了解更多关于这些基本概念的信息，我建议您阅读我的教科书《地统计学储层建模》中的介绍、建模原理和建模先决条件章节，[地统计学储层建模](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446){cite}`pyrcz2014’。

希望这有助于，

*迈克尔*

## 作者：

迈克尔·皮尔奇，教授，德克萨斯大学奥斯汀分校 *新颖数据分析、地统计学和机器学习地下解决方案*

在地下咨询、研究和开发方面拥有超过 17 年的经验后，迈克尔因他对教学的热情和对增强工程师和地球科学家在地下资源开发中影响的热情而重返学术界。

想了解更多关于迈克尔的信息，请查看以下链接：

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [网站](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [地统计学书籍](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python 中应用地统计学电子书](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python 中应用机器学习电子书](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## 想一起工作吗？

我希望这份内容对那些想要了解更多关于地下建模、数据分析以及机器学习的人有所帮助。学生和在职专业人士都欢迎参与。

+   想邀请我到贵公司进行培训、辅导、项目审查、工作流程设计和/或咨询吗？我很乐意拜访并与您合作！

+   感兴趣合作、支持我的研究生研究或我的地下数据分析与机器学习联盟（共同负责人包括 Foster 教授、Torres-Verdin 教授和 van Oort 教授）吗？我的研究将数据分析、随机建模和机器学习理论与实践相结合，以开发新的方法和工作流程，增加价值。我们正在解决具有挑战性的地下问题！

+   您可以通过 mpyrcz@austin.utexas.edu 联系我。

我总是很高兴讨论，

*迈克尔*

迈克尔·皮尔茨，博士，P.Eng. 教授，德克萨斯大学奥斯汀分校 Cockrell 工程学院和 Jackson 地球科学学院

更多资源可在以下链接获取：[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [网站](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [地统计学书籍](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python 中应用地统计学电子书](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python 中应用机器学习电子书](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## 机器学习概念的动力

首先，为什么要这样做？我收到了来自我的**地下机器学习**本科和研究生课程的学生的请求，要求提供课程术语表。虽然我通常在讲义中为显著术语提供定义幻灯片，但一些学生要求课程术语表，即课程复习的术语列表。这本电子书提供了一个很好的载体和动力，最终完成这项工作。

让我从一项坦白开始。有一本由谷歌开发者编写的[机器学习术语表](https://developers.google.com/machine-learning/glossary)。对于那些寻求深入、全面的地理统计术语列表的人来说，请使用这本书！

+   通过编写自己的术语表，我可以将范围和描述限制在课程内容内。我担心许多学生会因为标准机器学习术语表的大小和数学符号而感到不知所措。

+   此外，通过在电子书中包含术语表，我可以从术语表条目链接到电子书中的章节，以便方便地访问。我最终将所有章节都添加到术语表的超链接中，以便在章节和术语表之间来回移动。

最后，就像本书的其他部分一样，我希望术语表能够成为一个常青的活文档。

## **邻接矩阵**（谱聚类）

谱聚类：表示图中所有节点成对组合之间成对连接的矩阵。

+   这些值是指标，如果不连接则为 0，如果连接则为 1

注意，在邻接矩阵中，节点自连接被设置为 0

## **加法规则**（概率）

概率概念：当我们添加概率（结果的并集）时，$A$ 或 $B$ 的概率是根据概率加法规则计算的，

$$ P(A \cup B) = P(A) + P(B) - P(A,B) $$

对于互斥事件，我们可以将加法规则推广如下，

$$ P\left( \bigcup_{i=1}^k A_i \right) = \sum_{i=1}^k P(A_i) $$

## **仿射校正**

特征变换：一种分布缩放，可以将其视为单变量分布（例如，*直方图*）的平移、拉伸或压缩。对于将 $X$ 仿射校正为 $Y$ 的情况，

$$ y_i = \frac{\sigma_y}{\sigma_x}(x_i - \overline{x}) + \overline{y}, \quad \forall \quad i, \ldots, n $$

其中 $\overline{x}$ 和 $\sigma_x$ 是原始均值和方差，$\overline{y}$ 和 $\sigma_y$ 是新的均值和方差。

我们可以从上面看到，仿射相关方法首先通过减去原始均值来对分布进行中心化，然后通过新标准差与原始标准差的比例来重新缩放分散度（分布扩散），然后将分布平移到新的均值处。

+   仿射校正没有形状变化。对于形状变化，请考虑*分布变换*，如*高斯畸变*。

## **亲和矩阵**（谱聚类）

谱聚类：表示图中所有成对节点之间连接程度的矩阵，样本。

+   值表示连接的强度，与指示邻接矩阵不同，如果不连接则为 0，如果连接则为 1。

注意，在邻接矩阵中，节点自连接被设置为 0。

## **Bagging 模型**

Bagging 树和随机森林：使用自助法获得数据实现的应用。

$$ Y^b, X_1^b, \dots, X_m^b, \quad b = 1, \dots, B $$

为了训练预测模型实现，

$\hat{Y}^b = \hat{f}^b (X_1^b, \dots, X_m^b)$

其中，

+   $(X_1^b, \dots, X_m^b)$ - 第 $b$ 个自助数据集中的自助预测特征

+   $\hat{f}^b$ - 第 $b$ 个自助模型

+   $\hat{Y}^b$ - 第 $b$ 个自助模型中的模型预测值

为了计算预测实现。预测实现的集成被聚合以减少模型方差。聚合包括，

+   *回归* - 预测的平均值 $ \hat{Y} = \frac{1}{B} \sum_{b=1}^{B} \hat{Y}^b $ 

+   *分类* - 预测的模式

$$ \hat{Y} = \text{argmax}(\hat{Y}^b) $$

我们可以对任何预测模型执行 Bagging，实际上 scikit-learn 中的 BaggingClassifier 和 BaggingRegressor 函数是接受预测模型作为输入的包装器。

## **基函数展开**

多项式回归：为了增加我们模型的灵活性，例如，为了捕捉回归、分类模型中的非线性，我们使用一组基函数扩展特征。

+   在数学中，基函数展开是将更复杂的函数表示为更简单基函数的线性组合的方法，这使得问题更容易解决。

+   使用基函数展开，我们通过原始特征的基函数扩展问题的维度，但仍然在转换后的特征上使用线性方法。

$$ ℎ(𝑥_𝑖 )=\left( ℎ_1(𝑥_𝑖 ),ℎ_2(𝑥_𝑖 ),\ldots,ℎ_𝑘(𝑥_𝑖 ) \right) $$

这里是一个基函数展开的例子，多项式基函数展开的基函数集合：

$$ h_{i,1}(x_i) = x_i, \quad h_{i,2}(x_i) = x_i², \quad h_{i,3}(x_i) = x_i³, \quad h_{i,4}(x_i) = x_i⁴, \dots, \quad h_{i,k}(x_i) = x_i^k $$

## **基函数**

多项式回归：为了增加我们模型的灵活性，例如，为了捕捉回归、分类模型中的非线性，我们使用一组基函数扩展特征。

+   在数学中，基函数展开是将更复杂的函数表示为更简单基函数的线性组合的方法，这使得问题更容易解决。

+   使用基函数展开，我们通过原始特征的基函数扩展问题的维度，但仍然在转换后的特征上使用线性方法。

$$ ℎ(𝑥_𝑖 )=\left( ℎ_1(𝑥_𝑖 ),ℎ_2(𝑥_𝑖 ),\ldots,ℎ_𝑘(𝑥_𝑖 ) \right) $$

其中每个 $h_1$, \ldots, $h_k$ 都是基函数。例如，这里是一些多项式基展开的基函数：

$$ h_{i,1}(x_i) = x_i, \quad h_{i,2}(x_i) = x_i², \quad h_{i,3}(x_i) = x_i³, \quad h_{i,4}(x_i) = x_i⁴, \dots, \quad h_{i,k}(x_i) = x_i^k $$

## **贝叶斯定理**（概率）

概率概念：贝叶斯概率的核心数学模型，用于从先验概率进行贝叶斯更新，通过新信息的似然概率到后验概率。

$$ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} $$

其中 $P(A)$ 是先验，$P(B|A)$ 是似然，$P(B)$ 是证据项，$P(A|B)$ 是后验。如果用更描述性的标签替换 $A$ 和 $B$，将有助于更好地理解这种方法，

$$ P(\text{Model} | \text{New Data}) = \frac{P(\text{New Data} | \text{Model}) \cdot P(\text{Model})}{P(\text{New Data})} $$

证明了我们正在用新数据更新我们的模型

## **贝叶斯概率**

概率概念：基于对事件发生可能性的信念程度（专家判断和经验）的概率。一般方法，

+   从先验概率开始，在收集新信息之前

+   仅基于新信息制定似然概率

+   使用似然更新先验以计算更新的后验概率

+   当新信息可用时继续更新

+   解决那些我们不能使用简单频率解决的问题，即*频率主义概率*方法

+   贝叶斯更新通过**贝叶斯定理**建模

## **分类的贝叶斯更新**

朴素贝叶斯：这是从贝叶斯更新的角度提出分类预测问题，基于类别 $k$ 给定 $n$ 个特征 $x_1, \dots , x_n$ 的条件概率。

$$ P(C_k | x_1, \dots , x_n) $$

我们可以用贝叶斯更新求解这个后验概率，

$$ P(C_k | x_1, \dots , x_n) = \frac{P(x_1, \dots , x_n | C_k) P(C_k)}{P(x_1, \dots , x_n)} $$

让我们暂时将似然和先验结合起来，

$$ P(x_1, \dots , x_n | C_k) P(C_k) = P(x_1, \dots , x_n, C_k) $$

我们可以递归地展开完整的联合分布如下，

$$ P(x_1, \dots , x_n, C_k) $$

联合分布与条件分布和先验分布的展开，

$$ P(x_1 | x_2, \dots , x_n, C_k) P(x_2, \dots , x_n, C_k) $$

继续递归展开，

$$ P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3, \dots , x_n, C_k) $$

我们可以推广为，

$$ P(C_k | x_1, \dots , x_n) = P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3 | x_4, \dots , x_n, C_k) \ldots P(x_{n-1} | x_n, C_k) (x_{n} | C_k) P(C_k) $$

## **贝叶斯线性回归**

贝叶斯线性回归：线性回归模型的频率派公式为，

$$ y = b_1 \times x + b_0 + \sigma $$

其中 $x$ 是预测特征，$b_1$ 是斜率参数，$b_0$ 是截距参数，$\sigma$ 是误差或噪声。存在一个解析形式，用于拟合可用数据的同时最小化数据误差向量的 $L²$ 范数。

对于线性回归的贝叶斯公式，我们将模型设定为响应变量 $Y$ 的分布预测，现在是一个随机变量：

$$ Y \sim N(\beta^{T}X, \sigma^{2} I) $$

我们通过贝叶斯更新来估计模型参数分布，以从先验和训练数据中的似然推断模型参数。

$$ P(\beta | y, X) = \frac{P(y,X| \beta) P(\beta)}{P(y,X)} $$

通常对于连续特征，我们无法直接计算后验，我们必须使用采样方法，例如马尔可夫链蒙特卡洛（McMC）来采样后验。

## **大数据**

机器学习概念：如果你的数据满足以下标准之一，则你有大数据：

1.  *数据量* - 许多数据样本和特征，难以存储、传输和可视化

1.  *数据速度* - 高速收集，相对于决策周期进行连续数据收集，挑战在于在更新模型的同时跟上新的数据

1.  *数据多样性* - 数据来自各种来源，具有各种类型的数据、信息和规模

1.  *数据可变性* - 数据在项目期间发生变化，即使是单一特征也可能有多个不同规模、分布和真实性的数据版本

1.  *数据真实性* - 数据具有各种准确度级别，数据并不确定

对于大多数（如果不是所有）的常规地下应用，这些标准都得到了满足。地下工程和地球科学通常与大数据打交道！

## **大数据分析**

机器学习概念：检查大型和多样化的数据集（大数据）以发现模式和做出决策的过程，将统计学应用于大数据。

## **二进制转换**（也称为指示转换）

特征转换：将随机变量指示编码为相对于类别或阈值的概率。

如果 $i(\bf{u}:z_k)$ 是一个分类变量的指示符，

+   实现等于一个类别的概率是多少？

$$\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) = z_k \\ 0, & \text{if } Z(\bf{u}) \ne z_k \end{cases} \end{split}$$

例如，

+   给定阈值，$z_2 = 2$，以及数据在 $\bf{u}_1$，$z(\bf{u}_1) = 2$，则 $i(\bf{u}_1; z_2) = 1$

+   给定阈值，$z_1 = 1$，以及一个远离数据的随机变量 $Z(\bf{u}_2)$，则计算为 $F^{-1}_{\bf{u}_2}(z_1)$ 的随机变量，$i(\bf{u}_2; z_1) = 0.23$

如果 $I\{\bf{u}:z_k\}$ 是一个连续变量的指示器，

+   实现小于或等于阈值的概率是多少？

$$\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) \le z_k \\ 0, & \text{if } Z(\bf{u}) > z_k \end{cases} \end{split}$$

例如，

+   给定阈值，$z_1 = 6\%$，以及数据在 $\bf{u}_1$，$z(\bf{u}_1) = 8\%$，则 $i(\bf{u}_1; z_1) = 0$

+   给定阈值，$z_4 = 18\%$，以及远离数据的随机变量 $Z(\bf{u}_2) = N\left[\mu = 16\%,\sigma = 3\%\right]$，则 $i(\bf{u}_2; z_4) = 0.75$

指示编码可以通过在每个位置的随机变量的指示变换应用于整个随机函数。

## **提升模型**

梯度提升：通过添加多个弱学习器来构建更强的学习器。

+   弱学习器是指提供预测结果仅略好于随机选择的模型

这就是用文字描述的方法，然后用方程表示，

+   构建一个误差率高的简单模型，模型可能非常不准确，但方向是正确的

+   计算模型的误差

+   将另一个模型拟合到误差

+   计算第一个和第二个模型添加的误差

+   重复直到达到所需的准确度或满足其他停止条件

现在有了方程，从 $X_1,\ldots,X_m$ 预测 $Y$ 的一般工作流程是，

+   构建一个弱学习器来从 $X_1,\ldots,X_m$ 预测 $Y$，从训练数据 $x_{i,j}$ 中预测 $\hat{F}_k(X)$。

+   遍历所需估计器的数量，$k = 1,\ldots,K$

1.  计算训练数据中的残差，$h_k(x_{i}) = y_i - \hat{F}_k(x_{i})$

1.  将另一个弱学习器拟合到预测 $h_k$ 从 $X_1,\ldots,X_m$，从训练数据 $x_{i,j}$ 中预测 $\hat{F}_k(X)$。

+   每个模型都是基于前一个模型来提高准确性的

回归估计量是对 $K$ 个简单模型的求和，

$$ \hat{Y} =\sum_{k=1}^{K} F_k(X_1,\ldots,X_m) $$

## **自举**

袋装树和随机森林：一种统计重采样过程，用于从样本数据本身计算计算统计量中的不确定性。一些一般性评论，

+   *带替换的抽样* - 从数据集的累积分布函数中进行 $n$（数据样本数）*蒙特卡洛模拟*，这导致数据的新实现

+   *模拟数据收集过程* - 基本思想是模拟原始数据收集过程。而不是实际收集新的样本集，我们从数据中随机选择以获取数据实现

+   *自举任何统计量* - 这种方法非常灵活，因为我们可以从数据实现中计算任何统计量的实现

+   *计算成本低* - 重复此方法以获取统计量的实现，从而构建一个完整的不确定性分布。使用大量实现，$L$，以获得可靠的不确定性模型。

+   *计算整个不确定性的分布* - 对于任何统计量，你计算不确定性模型中的任何汇总统计量，例如，均值的 P10 和 P90。

+   *机器学习中的袋装法* - 是将自助法应用于获取数据实现，以训练预测模型实现，对预测模型集合进行聚合预测，以减少模型方差。

自助法的局限性是什么？

+   偏差的样本数据可能导致偏差的自助不确定性模型，你必须首先消除偏差，例如，*解聚*。

+   你必须有足够的样本量。

+   仅整合空间中稀疏样本的不确定性。

+   不考虑数据的空间背景，即样本数据位置、感兴趣体积或空间连续性。有一种名为[空间自助法](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Spatial_Bootstrap.ipynb)的自助法变体。

## **分类特征**

机器学习概念：一个只能取有限且通常固定数量的可能值的特征。

## **分类名义特征**

机器学习概念：一个没有自然排序的*分类*特征，例如，

+   砂岩 = {灰岩，碎屑岩，砂屑岩，泥岩}

+   矿物 = {石英，长石，方解石}

## **分类有序特征**

机器学习概念：一个具有自然排序的*分类*特征，例如，

+   地质年代 = {中新世，上新世，更新世} - 从较老的岩石到较新的岩石排序。

+   摩氏硬度 = $\{1, 2, \ldots, 10\}$ - 从较软到较硬的岩石排序。

## **因果关系**

多元分析：一个变化直接导致一个或多个其他特征变化的关系。

因果关系的一些重要方面，

1.  *不对称和时间优先* - $A$ 由 $B$ 造成并不意味着 $B$ 由 $A$ 造成。

1.  *非虚假* - 不是由于随机效应或混杂特征造成的。

1.  *机制和解释* - 有一个合理的机制或过程可以解释这种关系。

1.  *一致性* - 这种关系在一系列条件、时间、地点、人群等情况下都是可观察的。

1.  *强度* - 更强的关系在所有前 1-5 个条件都成立的情况下，增加了因果关系的可能性。

建立因果关系非常困难，

+   在这门课程中，我们通常避免因果关系和因果分析，并通过诸如“相关性不意味着因果关系”之类的陈述来强调这一点。

## **基于细胞的解聚**

数据准备：一种将权重分配给空间样本的解聚方法，基于局部采样密度，使得加权统计更有可能代表总体。数据权重分配如下，

+   在密集采样区域采样的样本权重较小。

+   稀疏采样区域中的样本获得更多权重。

分簇消除的目标是使样本统计量独立于样本位置，例如，补充钻探或爆破孔样本不应因局部样本密度增加而改变感兴趣区域的统计量。

基于单元格的分簇消除过程如下：

1.  在空间数据上放置单元格网格，并将权重设置为与单元格中样本数量的倒数成比例。

1.  单元网格大小有所变化，选择最小化分簇后均值（样本均值偏高）或最大化分簇后均值（如果样本均值偏低）的单元格大小。

1.  为了消除单元格网格位置的影响，单元格网格被随机移动几次，并对每个数据点平均得到的分簇消除权重。

权重计算如下：

$$ w(\bf{u}_j) = \frac{1}{n_l} \cdot \frac{n}{L_o} $$

其中 $n_l$ 是当前单元格中的数据量，$L_o$ 是有数据的单元格数量，$n$ 是数据总数。

这里是关于基于单元格的分簇消除的一些亮点，

+   根据名义样本间距（例如，补充钻探前的数据间距）分配单元格大小，与基于最小或最大分簇均值的自动化单元格大小选择方法（如上所述）相比，将提高性能。

+   基于单元格的分簇消除不了解感兴趣区域的边界；因此，靠近感兴趣区域边界的附近数据可能看起来采样更稀疏，并获得更多权重。

+   基于单元格的方法是由安德烈·约尼尔教授于 1983 年开发的，[]

## **认知偏差**

机器学习概念：人类大脑用来简化从大量个人经验和学习偏好中获取的信息的自动化（潜意识）思维过程。虽然这些对于我们在地球上的进化和生存至关重要，但它们可能导致数据科学中的以下问题：

1.  *锚定偏差*，过分强调第一条信息。研究表明，当我们刚开始了解一个主题时，第一条信息可能是不相关的，而且项目中最早的数据往往具有最大的不确定性。通过整理所有数据，整合不确定性，促进项目团队中的开放讨论和辩论，来应对锚定偏差。

1.  *可用性启发式*，高估易于获取信息的价值，例如，祖父每天抽 3 包烟，却活到 100 岁，即依赖于轶事。通过确保项目团队记录所有可用信息并应用定量分析来超越轶事，来应对可用性启发式。

1.  *从众效应*，评估概率随着持有相同信念的人数增加而增加。注意你的项目团队中每个人都跳上同一艘船或最响亮的声音影响所有其他人。鼓励项目团队的所有成员贡献，甚至单独的会议可能有助于解决从众效应。

1.  *盲点效应*，未能看到自己的认知偏差。这是所有认知偏差中最难的一个。一个可能的解决方案是邀请对项目团队的方法、结果和决策进行远程审查。

1.  *选择支持偏差*，在做出承诺后概率增加，即，做出决定。例如，我购买那辆车的决定通过关注关于汽车的正信息是好的。这是确认偏差的一个特例。

1.  *聚类错觉*，在随机事件中看到模式。是的，这个启发式方法在我们被大型预测者追捕时帮助我们生存下来，即，假阳性比假阴性要好得多！解决方案是建立不确定性置信区间，并测试所有数据和结果以对抗随机效应。

1.  *确认偏差*，只考虑支持当前模型的新信息。选择支持偏差是确认偏差的一个特例。解决确认偏差的方法是寻找你可能会不同意的人，并组建具有不同技术观点和不同专家经验的熟练项目团队。我的方法是如果房间里每个人都同意我，我会感到紧张！

1.  *保守偏差*，倾向于旧数据而不是新收集的数据。数据管理和定量分析是有帮助的。

1.  *近期偏差*，倾向于最近收集的数据。确保你的团队记录以前的数据和选择，以增强团队记忆。就像保守偏差一样，数据管理和定量分析是我们的第一道防线。

1.  *幸存者偏差*，只关注成功案例。检查团队可用的数据中是否存在任何可能的预选或筛选。

有效地使用统计学/数据分析可以保护我们免受偏差的影响。

## **互补事件**（概率）

概率概念：概率的 NOT 运算符，如果我们定义 A，那么 A 的补集，$A^c$，不是 A，并且我们得到这个结果闭包关系，

$$ P(A) + P(A^c) = 1.0 $$

在多元问题之外，可以考虑互补事件，例如考虑这个二元闭包，

$$ P(A|B) + P(A^c|B) = 1.0 $$

注意，给定的术语必须相同。

## **计算复杂性**

线性回归：表示方法所需的计算机资源，我们在机器学习中使用它来理解当改变维度、特征数量和训练数据数量时，我们的机器学习方法如何扩展，表示为，

$$ 𝑂(𝑓(𝑛)) $$

其中 $𝑛$ 代表问题的大小。计算复杂性有两个组成部分，

+   *时间复杂度* - 指的是计算时间以及给定算法的时间复杂度随问题规模的变化

+   *空间复杂度* - 指的是计算机内存需求以及给定算法的存储随问题规模的变化

例如，如果时间复杂度是 $O(n³)$，其中 $n$ 是训练数据数量，那么如果我们加倍数据数量，运行时间将增加八倍。

关于计算复杂度的其他显著点，

+   *默认为最坏情况复杂度* - 对于特定问题规模的最坏情况复杂度，提供了一个上限

+   *渐近复杂度* - 其中 $𝑛$ 很大。一些算法对于小数据集有加速，这不被使用

+   假设所有步骤都是必需的，例如，数据未预先排序等。

时间复杂度示例，

+   *二次时间*, $𝑶(𝒏𝟐)$ - 例如，整数乘法，冒泡排序

+   *线性时间*, $𝑶(𝒏)$ - 例如，在未排序的数组中查找最小值或最大值

+   *分数幂*, $𝑶(𝒏^𝒄 )$ - 其中 $[0 < c < 1]$，例如，在 kd 树中搜索，$𝑂(𝑛^(\frac{1}{2}))$

+   *指数时间*, $𝑶(𝟐^𝒏)$ - 例如，使用动态规划的旅行商问题

## **条件概率**

概率概念：在另一个事件已经发生的情况下，事件的概率，

$$ P(A|B) = \frac{P(A,B)}{P(A)} $$

我们将其读作在 B 发生的情况下 A 发生的概率，即联合概率除以边缘概率。我们可以通过向任一组件添加联合来扩展条件概率到任何多元情况。例如，

$$ P(C|B,A) = \frac{P(A,B,C)}{P(B,C)} $$

## **置信区间**

线性回归：表示为范围、下限和上限的汇总统计量或模型参数的不确定性，基于指定的概率区间，称为置信水平。

我们这样传达置信区间，

+   有 95%的概率（或 20 次中的 19 次）表明模型斜率在 0.5 和 0.7 之间。

关于置信区间的其他显著点，

+   当有可用时，通过分析方法计算，或者使用更通用和灵活的 bootstrap 方法

+   对于贝叶斯方法，我们参考可信度区间

## **混淆矩阵**

朴素贝叶斯：一个矩阵，表示预测（x 轴）与实际（y 轴）类别频率，以可视化分类模型的性能。

+   通过分类模型可视化并诊断所有正确和错误分类的组合，例如，类别 1 经常被错误分类为类别 3。

+   完美准确度是每个类别在对角线上的数量，类别 1 总是预测为类别 1，等等。

+   分类矩阵用于计算分类准确性的单个总结，例如，精确度、召回率等。

## **连续特征**

机器学习概念：一个可以取介于下限和上限之间任何值的特征。例如，

+   孔隙率 = $\{13.01\%, 5.23\%, 24.62\%\}$

+   金品位 = $\{4.56 \text{ g/t}, 8.72 \text{ g/t}, 12.45 \text{ g/t} \}$

## **连续，区间特征**

机器学习概念：一个 *连续特征*，其中数字之间的间隔相等，例如，1.50 和 2.50 之间的差值与 2.50 和 3.50 之间的差值相同，但实际数值没有客观的物理现实性（存在于一个任意尺度上），即，没有真实的零点，例如，

+   摄氏温度尺度（基于水在 0 度结冰和在 100 度沸腾的任意尺度）

+   日历年份（没有真正的零年）

我们可以使用加法和减法运算来比较连续的区间特征。

## **连续，比率特征**

机器学习概念：一个 *连续特征*，其中数字之间的间隔相等，例如，1.50 和 2.50 之间的差值与 2.50 和 3.50 之间的差值相同，但数值确实具有客观现实性（衡量实际物理现象），即，确实有真实的零点，例如，

+   开尔文温度尺度

+   孔隙率

+   渗透率

+   饱和度

由于存在真实的零点，连续的比率特征可以使用乘法和除法数学运算（除了加法和减法）进行比较，例如，孔隙率的两倍。

## **连续可微**

机器学习训练和调优：如果一个函数满足两个关键条件，则它是连续可微的：

1.  函数是可微的，函数的导数在其定义域的每一点上都存在，即，函数在每一个可能的点上都有一个明确的斜率。

1.  导数是连续的，函数的导数在其定义域的每一点上都没有跳跃、不连续或突然变化，即，导数函数在其定义域的每一点上本身是连续的。

对于一个机器学习示例，

+   $L²$ 范数是连续可微的，因此对于线性回归和岭回归，我们可以对损失函数应用偏导数来计算模型参数训练的闭式解

+   $L¹$ 范数不可连续可微，因此对于 LASSO 回归，我们无法对损失函数应用偏导数来计算模型参数训练的闭式解。我们必须使用迭代优化来训练模型参数。

## **卷积**

k-最近邻：两个函数的积分乘积，其中一个函数经过反转并平移 $\Delta$。

+   一种解释是使用加权函数 $𝑓(\Delta)$ 对函数进行平滑处理，以计算函数 $𝑔(x)$ 的加权平均值，

$$ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta $$

这很容易扩展到多维

$$ (f * g)(x, y, z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(\Delta_x, \Delta_y, \Delta_z) g(x - \Delta_x, y - \Delta_y, z - \Delta_z) \, d\Delta_x \, d\Delta_y \, d\Delta_z $$

在积分之前选择哪个函数被移动不会改变结果，卷积算子具有交换性。

$$ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta $$$$ (f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta $$

+   如果任一函数被反映出来，卷积就等同于互相关，它是两个信号作为位移函数的相似度度量。

## **核心数据**

机器学习概念：直接测量地下资源（回收的钻屑也是直接测量，具有更大的不确定性和较小的、不规则的尺度）的主要采样方法。对核心数据的评论，

+   对于石油和天然气来说，收集成本高昂/耗时，会中断钻探作业，分布稀疏且具有选择性（非常具有偏见）

+   在采矿（钻石钻孔）中非常常见，用于以规则模式和紧密间距进行品位控制

+   重力、活塞等取心方法用于在湖泊和海洋中取样沉积物

我们从核心数据中学到了什么？

+   岩石学特征（沉积结构，矿物等级），岩石物理特征（孔隙率，渗透率），以及力学特征（弹性模量，泊松比）

+   通过井和钻孔之间的插值来研究地层学和矿体几何学

核心数据对于支持地下资源解释至关重要。它们是整个储层概念和预测框架的锚，

+   例如，与井日志数据同位处的核心数据用于校准（地面真实）相，井日志中的孔隙率

## **相关性**

多元分析：皮尔逊积矩相关系数是线性关系程度的度量，

$$ \rho_{x,y} = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{(n-1)\sigma_x \sigma_y} $$

其中 $\overline{x}$ 和 $\overline{y}$ 是特征 $x$ 和 $y$ 的平均值。该度量是界限 $$$-1,1$$$。

+   相关系数是标准化的协方差

皮尔逊相关系数对异常值和偏离线性行为（在双变量意义上）非常敏感。我们有一个称为斯皮尔曼秩相关系数的替代方案，

$$ \rho_{R_x R_y} = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i} - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le 1.0 $$

排序相关系数在计算相关系数之前将排序变换应用于数据。要计算排序变换，只需将数据值替换为排名 $R_x = 1,\dots,n$，其中 $n$ 是最大值，$1$ 是最小值。

$$ x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall \, i \gt j $$$$ R_{x_i} = i $$

## **协方差**

多元分析：衡量两个特征如何一起变化，

$$ C_{x,y} = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{(n-1)} $$

其中 $\overline{x}$ 和 $\overline{y}$ 是特征 $x$ 和 $y$ 的均值。该度量受限于 $$$-\sigma_x \cdot \sigm_y,\sigma_x \cdot \sigm_y$$$。

+   相关系数是标准化的协方差

佩尔森相关系数对异常值和偏离线性行为（在双变量意义上）非常敏感。我们有一个称为斯皮尔曼秩相关系数的替代方案，

$$ \rho_{R_x R_y} = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i} - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le 1.0 $$

排序相关系数在计算相关系数之前将数据值转换为排名变换。要计算排名变换，只需将数据值替换为排名 $R_x = 1,\dots,n$，其中 $n$ 是最大值，$1$ 是最小值。

$$ x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall \, i \gt j $$$$ R_{x_i} = i $$

## **交叉验证**

机器学习概念：保留部分数据不用于模型参数训练，以测试模型预测未用于训练的案例的能力

+   这通常通过训练和测试数据分割进行，其中 15% - 30% 的数据分配给测试

+   作为现实世界模型使用的排练，训练-测试分割必须公平，以产生与计划使用模型相似的预测难度

+   存在更复杂的设计，如 k 折交叉验证，它允许通过 k 折每折都带有训练模型的测试来测试所有数据

+   交叉验证可以应用于检查模型性能以估计准确性（最常见）和不确定性模型的好坏（[Maldonado-Cruz 和 Pyrcz，2021](https://www.sciencedirect.com/science/article/pii/S0920410521006343)）

## **累积分布函数** (CDF)

单变量分析：离散概率密度函数的总和或连续概率密度函数的积分。以下是重要概念，

+   累积分布函数表示为 $F_x(x)$，注意概率密度函数表示为 $f_x(x)$

+   是随机样本 $X$ 小于或等于特定值 $x$ 的概率；因此，y 轴是累积概率

$$ F_x(x) = P(X \le x) = \int_{-infty}^x f(u) du $$

+   对于累积分布函数（CDF），没有箱假设；因此，箱的分辨率与数据的分辨率相同。

+   单调不减函数，因为负斜率会表明区间内的负概率。

有效累积分布函数（CDF）的要求包括，

1.  非负约束：

$$ F_x(x) = P(X \le x) \ge 0.0, \quad \forall x $$

1.  有效概率：

$$ 0.0 \le F_x(x) \le 1.0, \quad \forall x $$

1.  不能有负斜率：

$$ \frac{dF_x(x)}{dx} \ge 0.0, \quad \forall x $$

1.  最小和最大值（确保概率闭合）：

$$ \text{min}(F_x(x)) = 0.0 \quad \text{max}(F_x(x)) = 1.0 $$

## **维度诅咒**

特征排序：与处理许多特征相关的一系列挑战，即高维空间，包括，

+   在高维空间中无法可视化数据和模型

+   在广阔的高维空间中进行统计推断通常采样不足

+   高维预测特征空间的低覆盖率

+   扭曲的特征空间，包括由角和距离主导的扭曲空间，距离失去敏感性

+   随着维度的增加，特征之间的多重共线性更可能

## **数据**（数据方面）

特征排序：在描述空间数据集时，这些是基本方面，

*数据覆盖率* - 对于这次调查，有多少比例的人口被采样？

通常，硬数据具有高分辨率（小尺度，体积支撑），但数据覆盖率较差（例如，仅测量极小比例的人口）

+   *核心覆盖率深海油气* - 井岩心仅占深海储量的五亿分之一到五十亿分之一，假设使用直径为 3 英寸的岩心，在垂直井中实现 10%的岩心覆盖率，井间距为 500 米到 1500 米

+   *核心覆盖率采矿级控制* - 钻石钻探孔岩心样本占矿石体积的八千分之一到三万分之一，假设使用直径为 63.5 毫米的岩心，在垂直钻孔中实现 100%的岩心覆盖率，钻孔间距为 5 米到 10 米

软数据往往具有优秀（通常是完整的）覆盖率，但分辨率低，

+   *地震反射调查和重力梯度调查* - 数据通常在整个感兴趣体积中可用，但分辨率低，并且通常随着深度的增加而降低

*数据规模*（支撑大小）- 单个样本采样的规模或体积是多少？例如，

+   核心样品在孔隙尺度上的断层扫描图像，1 - 50 $\mu m$

+   以 0.3 米间隔采样的伽马射线井测井，从井筒向外渗透 1 米

+   基于地面的重力梯度测量图，分辨率为 20 m x 20 m x 100 m

*数据信息类型* - 数据告诉我们关于地下什么信息？例如，

+   可用于校准渗透率和饱和度的粒度分布

+   流体类型以评估油水接触点位置

+   重要的储层层段的倾角和连续性以获取连通性

+   矿石品位以绘制高、中、低品位矿石壳体以进行矿山规划

## **数据凸性**

基于密度的聚类：如果欧几里得特征空间的一个子集 $A$ 对于 $A$ 内的任意两点 $𝑥_1$ 和 $𝑥_2$，连接这些点的整个线段都在 $A$ 内，则 $A$ 是凸的，$\left[𝑥_1,𝑥_2\right] \in A$。

## **DataFrame**

机器学习工作流程构建和编码：一个方便的 Pandas 类，用于处理数据表，每行代表一个样本，每列代表一个特征，因为，

+   方便的数据结构来存储、访问、操作表格数据

+   从各种文件类型、Python 类甚至直接从 Excel 加载数据的内置方法

+   计算汇总统计和可视化数据的内置方法

+   数据查询、排序、数据过滤的内置方法

+   数据操作、清洗、重新格式化的内置方法

+   存储数据信息的内置属性，例如大小、空值数量和空值

## **数据分析**

机器学习概念：使用可视化统计来支持决策。

+   Pyrcz 博士表示，数据分析与统计学相同。

## **数据准备**

机器学习概念：任何增强、改进原始数据以准备模型的工作流程步骤。

+   数据驱动科学需要数据，数据准备仍然至关重要

+   任何地下研究中有超过 80%是数据准备和解释

我们继续面临数据挑战：

+   数据整理 - 格式标准、版本控制、存储、传输、安全和文档

+   大量数据管理 - 可视化、可用性和数据挖掘与探索

+   大量的元数据 - 缺乏平台、标准和格式

+   工程集成、数据多样性、规模、解释和不确定性

清洁数据库是所有数据分析和机器学习的前提

+   必须从这个基础开始

+   输入垃圾，输出垃圾

## **度矩阵**（谱聚类）

谱聚类：表示图的矩阵，每个图节点、样本的连接数。

+   对角矩阵，用整数表示连接数

## **基于密度的聚类 DBSCAN**

基于密度的聚类：一种基于密度的聚类算法，组在特征空间中以足够点密度的位置播种或增长，该点密度由超参数确定，

+   $\epsilon$ – 在归一化特征度量的局部邻域半径。这是簇的尺度/分辨率。如果这个值设置得太小，太多的样本会被留下作为异常值；如果设置得太大，所有的簇将合并成一个单一的簇。

+   $min_{Pts}$ – 分配核心点所需的最小点数，其中核心点用于初始化或增长簇组。

密度通过样本数量在体积中的数量来量化，其中体积基于特征空间所有维度的半径。

通过 k 距离图（在本例中是 k 最近邻）可以进行自动或引导的 $\epsilon$ 参数估计。

1.  计算所有样本数据（在本例中为 1,700）在归一化特征空间中的最近邻距离。

1.  按升序排序并绘图。

1.  选择最大化正曲率的距离（拐点）。

这里是 DBSCAN 聚类的显著方面的总结，

+   *DBSCAN* - 代表基于密度的空间聚类应用噪声（Ester 等，1996 年）。

+   *优点* - 包括最小领域知识来估计超参数，能够表示任意形状的聚类组，并且在大数据集上应用高效

+   *自底向上的层次聚类/聚合聚类* – 所有数据样本最初都是自己的组，称为“未访问”，但实际上作为异常值，直到分配到组，然后聚类组迭代增长。

+   *互斥性* – 类似于 k-means 聚类，所有样本可能只能属于一个聚类组。

$$ P(C_i \cap C_j | i \ne j) = 0.0 $$

+   *非穷尽性* – 一些样本可能被留下作为未分配的，并假设为聚类分组分配的异常值

$$ P(C_1 \cup C_2 \cup \dots C_k) \le 1.0 $$

## **决策标准**

机器学习概念：通过应用转移函数到地下模型（s）来计算的特征，以支持决策。决策标准代表价值、健康、环境和安全。例如：

+   污染物回收率以支持泵的设计和土壤修复项目的实施

+   原地油气资源以确定是否应该开发储层

+   洛伦兹系数异质性度量用于分类储层并确定成熟类比

+   回收系数或产量以安排生产和确定最佳设施

+   恢复的矿物品位和吨位以确定经济最终矿坑壳体

## **决策树**

决策树：一个直观的、回归和分类的预测机器学习模型，将预测空间 $𝑋_1,…,𝑋_𝑚$ 划分为 $𝐽$ 个互斥、穷尽的区域，$𝑅_𝑗$。

+   *互斥性* – 任何预测变量的组合只属于一个区域，$𝑅_𝑗$

+   *穷尽性* – 所有预测变量的组合属于一个区域，$𝑅_𝑗$，区域覆盖整个特征空间，考虑的变量的范围

每个区域的相同预测，该区域的训练数据平均值，$\hat{Y}(𝑅_𝑗) = \overline{Y}(𝑅_𝑗)$

+   对于分类，最常见的是基于模式的或 argmax 运算符

关于决策树的其它显著点，

+   *监督学习* - 响应特征标签 $Y$ 在训练和测试数据中是可用的

+   *分层、二进制分割* - 预测特征空间，从 1 个区域开始，然后顺序划分，创建新的区域

+   *紧凑、可解释的模型* - 由于分类是基于特征空间二进制分割的层次结构（每次一个特征），该模型可以以直观的方式指定为具有二叉分支的树，因此得名决策树。该模型的代码是嵌套的 if 语句，例如，

```py
if porosity > 0.15:
    if brittleness < 20:
        initial_production = 1000
    else:
        initial_production = 7000
else:
    if brittleness < 40:
        initial_production = 500
    else:
        initial_production = 3000 
```

决策树是从上到下构建的。我们从一个覆盖整个特征空间的单一区域开始，然后进行一系列分割，

+   *扫描所有可能的分割* - 在所有区域和所有特征上。

+   *贪婪优化* - 通过在任意特征中找到最佳分割，最小化所有训练数据 $y_i$ 在所有区域 $j = 1,\ldots,J$ 上的残差平方和。后续分割之间没有其他信息共享。

$$ RSS = \sum^{J}_{j=1} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})² $$

超参数包括，

+   *区域数量* – 非常容易理解，你知道模型会是什么样子

+   *最小化 RSS 的减少* – 可以提前停止，例如，RSS 减少较少的分割可能导致后续分割有更大的 RSS 减少

+   *每个区域中训练数据的最低数量* – 与区域均值预测的准确性概念相关，即我们需要至少 𝑛 个数据来获得可靠的均值

+   *最大层数* – 强制对称树，到达每个区域所需的分割数量相似

## **分散**

数据准备：根据局部采样密度对空间样本分配权重的各种方法，使得加权统计量更有可能代表总体。数据权重分配如此，

+   在密集采样区域中的样本权重较低

+   在稀疏采样区域中的样本权重较高

存在多种分散方法：

+   *基于单元格的分散*

+   *多边形分散*

+   *基于克里金的分散*

重要的是要注意，没有任何分散方法可以证明对于每个数据集，结果加权统计量都会提高总体参数的预测，但在期望中，这些方法倾向于减少偏差。

## **分散**（统计学）

数据准备：一旦为空间数据集计算了分散权重，则将分散后的统计量应用于后续分析或建模的输入。例如，

+   分散后的均值被指定为简单克里金法的稳定、全局均值

+   将所有数据的加权累积分布函数应用于顺序高斯模拟，以确保反变换的实现在趋近于分散分布

任何统计量都可以加权，包括整个累积分布函数！以下是加权统计量的示例，给定分散权重 $w(\bf{u}_j)$，对于所有数据 $j=1,\ldots,n$。

+   加权样本均值，

$$ \overline{x}_{wt} = \frac{\sum_{i=1}^n w(\bf{u}_j) \cdot z(\bf{u}_j)}{\sum_{i=1}^n w(\bf{u}_j)} $$

其中 $n$ 是数据数量。

+   加权样本方差，

$$ s²_{x_{wt}} = \frac{1}{\sum_{i=1}^n w(\bf{u}_j) - 1} \cdot \sum_{i=1}^n w(\bf{u}_j) \cdot \left( x(\bf{u}_j) - \overline{x}_{wt} \right)² $$

其中 $\overline{x}_{wt}$ 是去聚类均值。

+   加权协方差，

$$ C_{x,y_{wt}} = \frac{1}{\sum_{i=1}^n w(\bf{u}_j) } \cdot \sum_{i=1}^n w(\bf{u}_j) \cdot \left( x(\bf{u}_j) - \overline{x}_{wt} \right) \cdot \left( y(\bf{u}_j) - \overline{y}_{wt} \right) $$

其中 $\overline{x}_{wt}$ 和 $\overline{y}_{wt}$ 是特征 $X$ 和 $Y$ 的去聚类均值。

+   整个累积分布函数（CDF），

$$ F_z(z) \approx \sum_{j=1}^{n(Z<z)} w(\bf{u}_j) $$

其中 $n(Z<z)$ 是小于阈值 $z$ 的升序排序数据的数量。我们将其表示为近似值，因为这简化了，并且没有插值模型，且在数据分辨率上。

重要的是要注意，没有任何去聚类方法可以证明对于每个数据集，结果加权统计量将提高对总体参数的预测，但在期望中，这些方法倾向于减少偏差。

## **密度连接** (DBSCAN)

基于密度的聚类：如果存在一个点 $Z$，它可以从点 $A$ 和 $B$ 都密度可达，则点 $A$ 和 $B$ 是密度连接的。

## **基于密度的聚类** (DBSCAN)

基于密度的聚类：一个非空集合，其中所有点都相互密度连接。

## **密度可达** (DBSCAN)

基于密度的聚类：如果 $Y$ 属于一个可以从 $A$ 到达的核心点的邻域，则点 $Y$ 是从 $A$ 密度可达的。这需要一系列属于先前核心点并包括点 $Y$ 的核心点链。

## **确定性模型**

机器学习概念：一个假设系统或过程是完全可预测的模型

+   通常基于工程和地球科学物理学以及专家判断

+   例如，数值流动模拟或从地震解释的层序边界表面

+   对于这门课程，我们也声明了数据驱动估计模型，例如

优点：

+   物理学和专家知识的整合

+   各种信息源的整合

缺点：

+   通常非常耗时

+   通常没有不确定性评估，专注于构建一个模型

## **维度约简**

主成分分析：减少数据科学工作流程中特征数量的方法。有两种主要方法，

+   *特征选择* – 找到对问题最重要的原始特征子集

+   *特征投影* – 将数据从高维空间转换到低维空间

被称为降维或维度约简

+   受维度诅咒和多共线性启发

+   应用于统计学、机器学习和信息理论

## **直接密度可达** (DBSCAN)

基于密度的聚类：如果$A$是一个核心点且$X$属于$A$的邻域，距离$le \epsilon$，则点$X$是直接密度可达的$A$。

## **离散特征**

机器学习概念：是一个将特征在未采样位置或时间表示为单个最佳值的进程，或一些额外的概念，

+   孔隙率在 0 到 20%之间分配到 10 个箱子 = {0 - 2%，2% - 4%，...，20%}

+   摩氏硬度 = $\{1, 2, \ldots, 10\}$（与*分类特征*相同）

## **分布变换**

特征变换：通过百分位数值从一个分布映射到另一个分布，从而得到新的直方图、PDF 和 CDF。我们在地统计方法和工作流程中执行分布变换，因为，

+   *推断* - 为了纠正特征分布到预期的形状，例如，纠正数据过少或偏差

+   *理论* - 工作流程步骤需要特定的分布假设，例如，对于顺序高斯模拟，需要均值为 0.0 和方差为 1.0 的高斯分布

+   *数据准备或清理* - 为了纠正异常值，变换将把异常值映射到目标分布，而不再是异常值

我们如何执行分布变换？

我们将累积分布函数（CDF），$F_{X}$，的值转换成一个新的 CDF，$G_{Y}$。这可以通过对所有样本数据应用分位数-分位数变换进行推广：

+   正向变换：

$$ Y = G_{Y}^{-1}(F_{X}(X)) $$

+   反向变换：

$$ X = F_{X}^{-1}(G_{Y}(Y)) $$

这可以应用于任何数据，包括参数或非参数分布。我们只需要能够通过百分位数将一个分布映射到另一个分布，因此它是一个：

+   保留排名的变换，例如，P25 在分布变换后仍然是 P25

## **急切学习**

k-最近邻：模型是查询之前构建的训练数据的泛化

+   模型在参数训练和超参数调整后是输入独立的，即，不需要训练数据来做出新的预测

相反的是懒惰学习。

## **估计**

机器学习概念：是获得单个最佳值以表示未采样位置或时间的特征的进程。一些额外的概念，

+   局部精度优先于全局空间变异性

+   过于平滑，不适合对异质性敏感的任何变换函数

+   例如，逆距离和克里金法

+   许多预测机器学习模型侧重于估计（例如，k 最近邻、决策树、随机森林等）

## **f1 分数**（分类准确度指标）

朴素贝叶斯：一个分类预测模型的准确度度量，混淆矩阵中每个 $k$ 类别的单一汇总指标。

+   召回率和精度的调和平均值

$$ f1-score_k = \frac{2} { \frac{1}{Precision_k} + \frac{1}{Recall_k} } $$

作为提醒，

+   *召回率* - 测试数据集中该类别所有真实正例与所有案例的比率

+   *精度* - 真实正例与所有正例（真实正例 + 假正例）的比率

$$ Recall_k = \frac{ n_{k, \text{true positives}} }{n_k} $$

## **特征**（也称为变量）

机器学习概念：在研究中测量的或观察到的任何属性

+   例如，孔隙率、渗透率、矿物浓度、饱和度、污染物浓度等。

+   在数据挖掘/机器学习中，这被称为特征，统计学家称这些为变量

+   测量通常需要大量的分析、解释等。

+   当特征被修改和组合以提高我们的模型时，我们称之为特征工程

## **特征工程**

特征转换：利用领域专业知识从原始数据中提取改进的预测或响应特征，

+   提高推理或预测机器学习的性能、准确性和收敛性

+   提高模型可解释性（或者如果我们的工程特征处于不熟悉的单位，可能会降低可解释性）

+   缓解异常值和偏差，与高斯性、线性化、维度扩展等假设的一致性

特征转换和特征选择是特征工程的两种形式。

## **特征重要性**

特征排序：各种机器学习方法提供特征排序的度量，例如决策树通过包含每个特征来总结均方误差的减少，并总结为，

$$ FI(x) = \sum_{t \in T_f} \frac{N_t}{N} \Delta_{MSE_t} $$

其中 $T_f$ 是所有以特征 $x$ 作为分割的节点，$N_t$ 是达到节点 $t$ 的训练样本数量，$N$ 是数据集中样本的总数，$\Delta_{MSE_t}$ 是 $t$ 分割时的 MSE 减少量。

注意，对于具有*基尼不纯度*的分类树，特征重要性可以以与上述 MSE 类似的方式计算。

特征重要性是模型特征排序的一部分，

+   特征重要性的准确性取决于模型的准确性，即不准确的模型可能会提供错误的特征重要性

## **特征插补**

特征插补：用合理值替换数据表中的空值，对于所有特征都没有值的样本，有 2 个原因，

+   使统计计算和需要完整数据表的模型成为可能，即，不能处理缺失特征值

+   最大化模型精度，增加可用于训练和测试模型的可靠样本数量

+   缓解可能因特征值中类似删除而出现的模型偏差，这些特征值不是随机缺失的

特征插补方法包括，

+   *常量值插补* - 用特征均值或众数替换空值

+   *基于模型的插补* - 用具有相同样本的可用特征值预测缺失特征来替换空值

还有依赖于收敛的迭代方法，

+   *链式方程多重插补法 (MICE)* - 分配随机值，然后迭代处理缺失值，预测新值

该方法的目标是获得合理的插补值，这些值考虑了所有特征与所有可用和缺失值之间的关系

## **特征投影**

主成分分析：将原始 $m$ 个特征转换为 $p$ 个特征，其中 $p << m$ 用于降维

+   给定特征 $𝑋_1,\ldots,𝑋_𝑚$，我们需要 $\binom{m}{2} = \frac{𝑚(𝑚−1)}{2}$ 个散点图来可视化二维散点图

+   这些表示无法捕捉 $> 2$ 维结构

+   一旦我们有 4 个或更多变量，理解数据就会变得非常困难。回忆维度诅咒。

+   主成分分析、多维缩放和随机投影是例子

+   特征选择是降维的另一种方法

## **特征空间**

特征排序：通常特征空间仅指预测特征，不包括响应特征，即，

+   我们需要预测的所有预测特征的可能组合

+   可能被称为预测特征空间。

通常，我们在预测特征空间上训练和测试机器的预测。

+   该空间通常是超立方体，每个轴代表一个预测特征，从每个预测特征的最小值延伸到最大值

+   预测特征空间的更复杂形状是可能的，例如，我们可以屏蔽或移除数据覆盖较差的子集。

## **特征排序**

特征排序：特征工程的一部分，特征排序是一组方法，根据每个特征对推理中包含的信息和预测响应特征的重要性或价值分配相对重要性。

完成此任务的方法有很多种。我的建议是采用多种度量指标的方法，同时理解每种方法的假设和局限性。

这是我们将考虑的特征排序的一般类型指标：

+   *视觉检查* - 包括数据分布、散点图和小提琴图

+   *统计摘要* - 相关性分析，互信息

+   *基于模型* - 包括模型参数、特征重要性分数和全局 Shapley 值

+   *递归特征消除* - 以及其他通过保留测试数据交叉验证进行试错以找到最佳参数集的方法

特征排序主要受维度诅咒的驱动，即使用最少、最有信息量的预测特征。

## **特征变换**

特征变换：一种涉及对特征应用数学运算以改进工作流程中特征价值的特征工程类型。例如，

+   特征截断

+   特征归一化或标准化

+   特征分布变换

我们有许多原因想要执行特征变换。

+   使特征一致以便于可视化和比较

+   避免偏差或为依赖于预测特征空间中计算的距离的方法（例如 k 近邻回归）施加特征权重

+   该方法要求变量具有特定的范围或分布：

    +   人工神经网络可能需要所有特征的范围在 [-1,1] 之间

    +   部分相关系数需要高斯分布。

    +   统计测试可能需要特定的分布

    +   地统计学顺序模拟需要指示符或高斯变换

特征变换是许多机器学习工作流程中的常见基本构建块。

## **第四范式**

机器学习概念：从数据驱动的范式构建科学发现的，

+   第一个范式 - 经验科学 - 实验 和 观察

+   第二个范式 - 理论科学 - 分析表达式

+   第三个范式 - 计算科学 - 数值模拟

我们通过增加新的科学范式，而不是取代旧范式。每个先前的范式都由先前的范式支持，例如，

+   理论科学建立在经验科学之上

+   数值模拟整合了实验中的分析表达式和校准方程

## **频率主义概率**

概率概念：基于从实验中观察到的频率来衡量事件发生的可能性。对于随机实验和定义良好的设置（例如抛硬币），

$$ \text{Prob}(A) = P(A) = \lim_{n \to \infty} \frac{n(A)}{n} $$

其中：

$n(A)$ = 事件 $A$ 发生的次数 $n$ = 试验次数

例如，钻探下一个油井可能遇到干井的可能性，在位置 ($\bf{u}_{\alpha}$) 遇到砂岩，在位置 ($\bf{u}_{\alpha}$) 超过 $15 \%$ 的岩石孔隙率。

## **高斯畸变**

特征变换：到高斯分布的量数变换。

通过它们的累积概率映射特征值。

$$ y = G_y^{-1}\left( F_x(x)\right) $$

其中 $𝐹_𝑥$ 是原始特征累积分布函数（CDF）和 $𝐺_𝑦$ 是高斯 CDF 概率密度函数

$$ f(x) = \frac{1}{\sigma \sqrt{2 \pi}} exp \left[-1 \frac{1}{2} \left(\frac{x-\mu}{\sigma} \right)² \right] $$

正态分布的简称是

$$ N[\mu,\sigma²] $$

例如 $N[0,1]$ 是标准正态分布

+   大部分自然变异或测量误差是高斯分布

+   完全由均值、方差和协方差系数（如果多元）参数化

+   分布是无界的，没有最小值也没有最大值，极值非常不可能发生，通常应用某种截断

警告，许多工作流程应用单变量高斯变形然后假设二元或多元高斯，这是不正确的，但通常很难将我们的数据转换为多元高斯。

需要高斯分布的方法，

+   皮尔逊积矩相关系数在数据是多元高斯时完全描述了多元关系

+   部分相关需要二元高斯

+   顺序模拟（地统计学）假设高斯分布以再现全局分布

+   学生 t 检验用于均值差异

+   卡方分布是从高斯分布随机变量的平方和导出的

+   高斯朴素贝叶斯分类假设高斯条件

## **吉布斯采样器**（MCMC）

贝叶斯线性回归：一套从概率分布中采样的算法，使得样本匹配分布统计量，基于，

+   依次从条件分布中采样

由于只需要条件概率密度函数，系统简化为不需要完整的联合概率密度函数

这里是二元情况的吉布斯 MCMC 采样器的基本步骤，

1.  为 $𝑋(0)$、$𝑌(0)$ 分配随机值

1.  从 $𝑓(𝑋|𝑌(0))$ 中采样以获得 $𝑋(1)$

1.  从 $𝑓(𝑌|𝑋(1))$ 中采样以获得 $𝑌(1)$

1.  对样本重复下一步，$\ell = 1,\ldots,𝐿$

结果样本将具有正确的联合分布，

$$ 𝑓(𝑋,𝑌) $$

## **梯度提升模型**

梯度提升：将提升模型作为梯度下降问题提出的结果预测模型

在每一步，$k$，正在拟合一个模型，然后计算误差，$h_k(X_1,\ldots,X_m)$。

我们可以分配一个损失函数，

$$ L\left(y,F(X)\right) = \frac{\left(y - F(X)\right)²}{2} $$

因此，我们想要最小化 $\ell2$ 损失函数：

$$ J = \sum_{i=1}^{n} L\left(y_i, F_k(X) \right) $$

通过调整我们的模型结果来适应我们的训练数据 $F(x_1), F(x_2),\ldots,F(x_n)$。

我们可以取误差相对于我们模型的偏导数，

$$ \frac{\partial J}{\partial F(x_i)} = F(x_i) - y_i $$

我们可以将残差解释为负梯度。

$$ y_i - F(x_i) = -1 \frac{\partial J}{\partial F(x_i)} $$

因此，我们现在有一个梯度下降问题：

$$ F_{k+1}(X_i) = F_k(X_i) + h(X_i) $$$$ F_{k+1}(X_i) = F_k(X_i) + y_i - F_k(X_i) $$$$ F_{k+1}(X_i) = F_k(X_i) - 1 \frac{\partial J}{\partial F_k(X_i)} $$

通用形式为：

$$ \phi_{k+1} = \phi_k - \rho \frac{\partial J}{\partial \phi_k} $$

其中 $phi_k$ 是当前状态，$\rho$ 是学习率，$J$ 是损失函数，而 $\phi_{k+1}$ 是估计器的下一个状态。

训练数据中的误差残差是梯度，然后我们进行梯度下降，

+   将一系列模型拟合到负梯度

将问题作为梯度下降问题来处理，我们能够应用各种损失函数，

+   $\ell2$ 是我们的 $\frac{\left(y - F(X)\right)²}{2}$，在处理实际问题时是可行的，但对外部异常值不够稳健

$$ - 1 \frac{\partial J}{\partial F_k(X_i)} = y_i - F_k(X_i) $$

+   $\ell1$ 是我们的 $|y - F(X)|$，对外部异常值更加稳健

$$ - 1 \frac{\partial J}{\partial F_k(X_i)} = sign(y_i - F_k(X_i)) $$

+   还有其他类似 Huber Loss

## **图拉普拉斯算子**（谱聚类）

谱聚类：通过整合图节点之间的连接来表示图的矩阵，包括每个图节点和样本的连接数。计算为度矩阵减去邻接矩阵。其中，

+   *度矩阵* $𝐷$ - 每个节点的连接度

+   邻接矩阵，$𝐴$ - 节点之间的特定连接

## **地统计学**

机器学习概念：应用统计学的一个分支，它整合了：

1.  空间（地质）背景

1.  空间关系

1.  体积支持/尺度

1.  不确定性

我将所有空间统计，包括地统计学都包含在内，有些人不同意我的观点。根据我的经验，任何有用的空间现象建模统计方法都被采用并添加到地统计学工具包中！地统计学是一个不断发展和演变的研究领域。

## **基于梯度的优化**

LASSO 回归：通过迭代最小化损失函数来求解模型参数的方法。步骤包括，

1.  从随机的模型参数开始

1.  计算模型参数的损失函数

1.  计算损失函数的梯度，通常没有损失函数的方程，通过数值计算局部损失函数的导数进行采样，

$$ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) = \frac{L(y_{\alpha}, F(X_{\alpha}, b_1 - \epsilon)) - L(y_{\alpha}, F(X_{\alpha}, b_1 + \epsilon))}{2\epsilon} $$

1.  通过向下步进/梯度来更新参数估计

$$ \hat{b}_{1,t+1} = \hat{b}_{1,t} - r \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) $$

其中 $r$ 是学习率/步长，$\hat{b}(1,𝑡)$ 是当前模型参数估计，而 $\hat{b}(1,𝑡+1)$ 是更新的参数估计。

关于基于梯度的优化的一些重要评论，

+   *梯度搜索收敛* - 该方法将找到一个局部或全局最小值

+   *梯度搜索步长* - 步长的影响，$r$ 太小，需要太长时间收敛到解，而 $r$ 太大，解可能会跳过/错过全局最小值或发散

+   *多个模型参数* - 在多个模型参数上计算和分解梯度，以向量形式表示。

$$ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1, b_2)) = \left[ \begin{matrix} \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) & \nabla L(y_{\alpha}, F(X_{\alpha}, b_2)) \end{matrix} \right] $$

+   *参数空间探索* - 训练机器学习模型参数的优化是探索高维模型参数空间

## **图**（谱聚类）

谱聚类：以有组织的方式表示数据的图表，每个样本作为一个节点，顶点表示样本之间的成对关系。

+   对于无向图，顶点是双向的，即连接是对称的，两个方向具有相同的强度

## **网格数据**

机器学习工作流程构建和编码：通常在 2D 或 3D 上具有详尽且均匀分布的数据，代表地图和模型

+   以逗号分隔的 .csv 文件形式存储，有 $𝑛_𝑦$ 行和 $𝑛_𝑥$ 列

+   也可以保存/加载为二进制格式，以更紧凑，但不是人类可读的文件。

+   通常直接可视化，例如，matplotlib 的 imshow 函数，或作为等高线图

## **硬数据**

机器学习概念：具有高度确定性的数据，通常来自岩石的直接测量。

+   例如，基于岩心和基于测井的孔隙度和岩性

通常，硬数据具有高分辨率（小尺度、体积支持），但覆盖率较差（仅测量人口中的极小比例，例如，

+   *核心覆盖率深海油气* - 岩心仅采样深海储量的五亿分之一到五十亿分之一，假设 3 英寸直径岩心在垂直井中具有 10% 的岩心覆盖率，间距为 500 米到 1500 米

+   *核心覆盖率采矿质量控制* - 钻孔岩心样本占矿石体积的八千分之一到三万分之一，假设 HQ 63.5 毫米直径岩心在垂直钻孔中具有 100% 的岩心覆盖率，间距为 5 米到 10 米

## **Hermite 多项式**

多项式回归：在实数线上的正交多项式族。

| 阶数 | Hermite 多项式 $H_e(x)$ |
| --- | --- |
| 零阶 | $H_{e_0}(x) = 1$ |
| 一阶 | $H_{e_1}(x) = x$ |
| 二阶 | $H_{e_2}(x) = x² - 1$ |
| 三阶 | $H_{e_3}(x) = x³ - 3x$ |
| 四阶 | $H_{e_4}(x) = x⁴ - 6x² + 3$ |

这些多项式相对于加权函数是正交的，

$$ 𝑤(𝑥)=𝑒^{−\frac{𝑥²}{2}} $$

这是标准高斯概率密度函数，没有缩放因子，$\frac{1}{\sqrt{2\pi}}$。正交性的定义如下，

$$ \int_{-\infty}^{\infty} H_m(x) H_n(x) w(x) \, dx = 0 $$

埃尔米特多项式在标准正态概率分布的区间 $[−\infty,\infty]$ 上是正交的。

在多项式回归中，通过使用埃尔米特多项式而不是常规多项式进行多项式基扩展，我们消除了预测特征之间的多重共线性，

+   回忆，预测特征之间的独立性是多项式回归中应用多项式基扩展的线性系统的一个假设

## **启发式算法**

聚类分析：解决困难问题的快捷方法，在速度和实用性方面对最优性和准确性做出妥协。

+   这种通用方法在机器学习、计算机科学和数学优化中很常见，例如，k 均值聚类的 $k^n$ 解空间实际上是通过启发式算法来解决的。

## **层次聚类**

聚类分析：所有聚类分组分配都是迭代确定的，与一次确定所有聚类分组的划分聚类方法相反。包括，

+   *聚合层次聚类* - 从 $n$ 个聚类开始，每个数据样本在其自己的聚类中，然后迭代地将聚类合并成更大的聚类

+   *划分层次聚类* - 从一个包含所有数据的单个聚类开始，然后迭代地划分出新的聚类

+   k 均值聚类是划分聚类，而找到解决方案的启发式方法是迭代的，但实际上解决方案是一次性完成的

+   很难更新，一旦进行了一系列的分割或合并，就很难返回并修改模型

## **直方图**

单变量分析：通过在可能值的范围内对一系列箱子的频率进行绘制来表示单变量统计分布。构建直方图的步骤如下，

1.  将可能值的连续特征范围划分为 $K$ 个相等大小的箱子，$\delta x$:

$$ \Delta x = \left( \frac{x_{max} - x_{min}}{K} \right) $$

或者使用可用的类别标签进行分类特征。

1.  计算每个箱子中的样本数（频率），$n_k$， \quad $\forall \quad k=1,\ldots,K$.

1.  绘制频率与箱标签的关系图（如果连续，则使用箱中心点）

注意，直方图通常以柱状图的形式绘制。

## **混合模型**

机器学习概念：包括确定性模型和随机模型组合的系统或过程

+   大多数地统计模型都是混合模型

+   例如，加性确定性趋势模型和随机残差模型

## **独立性**（概率）

概率概念：事件 $A$ 和 $B$ 独立当且仅当以下关系成立，

1.  $P(A \cap B) = P(A) \cdot P(B)$

1.  $P(A|B) = P(A)$

1.  $P(B|A) = P(B)$

如果其中任何一项违反，我们怀疑存在某种形式的关系。

## **指示变换**（也称为二值变换）

特征变换：将随机变量指示编码为相对于类别或阈值的概率。

如果 $i(\bf{u}:z_k)$ 是一个分类变量的指示符，

+   实现等于类别的概率是多少？

$$\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) = z_k \\ 0, & \text{if } Z(\bf{u}) \ne z_k \end{cases} \end{split}$$

例如，

+   给定阈值，$z_2 = 2\%$，以及数据在 $\bf{u}_1$ 处，$z(\bf{u}_1) = 2\%$，则 $i(\bf{u}_1; z_2) = 1$

+   给定阈值，$z_1 = 1\%$，以及一个远离数据的随机变量 $Z(\bf{u}_2)$，则计算为 $F^{-1}_{\bf{u}_2}(z_1)$ 的随机变量，$i(\bf{u}_2; z_1) = 0.23$

如果 $i(\bf{u}:z_k)$ 是一个连续变量的指示符，

+   实现小于或等于阈值的概率是多少？

$$\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) \le z_k \\ 0, & \text{if } Z(\bf{u}) > z_k \end{cases} \end{split}$$

例如，

+   给定阈值，$z_1 = 6\%$，以及数据在 $\bf{u}_1$ 处，$z(\bf{u}_1) = 8\%$，则 $i(\bf{u}_1; z_1) = 0$

+   给定阈值，$z_4 = 18\%$，以及一个远离数据的随机变量，$Z(\bf{u}_2) = N\left[\mu = 16\%,\sigma = 3\%\right]$，则 $i(\bf{u}_2; z_4) = 0.75$

指示编码可以通过在每个位置的随机变量的指示变换应用于整个随机函数。

## **指示变异图**

特征变换：从空间数据的指示变换中计算和建模的变异图，用于指示克立格。指示变异图是，

$$ \gamma_i(\mathbf{h}; z_k) = \frac{1}{2N(\mathbf{h})} \sum_{\alpha=1}^{N(\mathbf{h})} \left[ i(\mathbf{u}_\alpha; z_k) - i(\mathbf{u}_\alpha + \mathbf{h}; z_k) \right]² $$

$i(\mathbf{u}_\alpha; z_k)$ 和 $i(\mathbf{u}_\alpha + \mathbf{h}; z_k)$ 分别是尾部位置 $\mathbf{u}_\alpha$ 和头部位置 $\mathbf{u}_\alpha + \mathbf{h}$ 处的 $z_k$ 阈值的指示变换。

+   对于硬数据，指示变换 $i(\bf{u},z_k)$ 要么是 0 要么是 1，在这种情况下，当头部和尾部的值都 $\le z_k$（对于连续特征）或 $= z_k$（对于分类特征）时，$\left[ i(\mathbf{u}_\alpha; z_k) - i(\mathbf{u}_\alpha + \mathbf{h}; z_k) \right]²$ 等于 0，相对于阈值相同，或者当它们不同时为 1。

+   因此，指示变异图是变化对的比例的一半！指示变异图可以与滞后距离 $h$ 上变化的概率相关。

+   指示变异图的基台是按照以下方式计算的指示方差，

$$ \sigma_i² = p \cdot (1 - p) $$

其中 $p$ 是 1 的比例（或零，因为函数在比例上是对称的）

## **推理，推断统计学**

机器学习概念：这是一个大主题，但为了课程，我提供了这个简化的、功能性的定义，给定来自总体的随机样本，描述总体，例如，

+   给定井样，描述储层

+   给定钻孔样本，描述矿石体

## **内点**

一个回归模型准确度指标，表示在模型预测 $\hat{y}_i$ 的 $\epsilon$ 边界内的测试数据比例，

$$ I_R = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} I(y_i, \hat{y}_i) $$

给定指示变换，

$$\begin{split} I(y_i, \hat{y}_i) = \begin{cases} 1, & \text{if } |y_i - \hat{y}_i| \leq \epsilon \\ 0, & \text{otherwise} \end{cases} \end{split}$$

这是一个有用的直观准确度度量，表示具有足够好的预测的训练或测试数据比例。

+   但是，存在选择边界大小 $\epsilon$ 的选择，这可能与特定应用的准确度要求有关

## **基于实例的学习**

k-最近邻：也称为基于记忆的学习，比较新的预测问题（作为预测器集合，$𝑥_1,\ldots,𝑥_𝑚$)与训练数据中观察到的案例。

+   模型需要访问训练数据，充当观察的库

+   直接从训练数据中进行预测

+   预测复杂性随着训练数据数量 $𝑛$、邻居数量 $𝑘$ 和特征数量 $𝑚$ 的增加而增长。

+   懒惰学习的特定情况

## **事件交集**（概率）

概率概念：结果的交集，$A$ 和 $B$ 的概率表示为，

$$ P(A \cap B) = P(A,B) $$

在 $A$ 和 $B$ 独立性的假设下，$A$ 和 $B$ 的概率是，

$$ P(A,B) = P(A) \cdot P(B) $$

## **不可减少误差**

机器学习概念：是由于数据限制造成的错误，包括缺失特征和缺失样本，例如，完整的预测特征空间没有得到充分的采样

+   不可减少误差不受模型复杂性的影响，它是数据的一个限制

+   预期测试平方误差的三个组成部分之一，包括模型方差、模型偏差和不可减少误差

$$ E \left[ \left(y_0 - \hat{f}(x_1⁰, \ldots, x_m,⁰ \right)² \right] = \left(E [\hat{f}(x_1⁰, \ldots, x_m,⁰)] - f(x_1⁰, \ldots, x_m,⁰) \right)² + $$ $$ E \left[ \left( \hat{f} \left(x_1⁰, \ldots, x_m,⁰ \right) - E \left[ \hat{f}(x_1⁰, \ldots, x_m,⁰) \right] \right)² \right] + \sigma_e² $$

其中 $\sigma_e²$ 是不可减少误差。

## **惯性**（聚类）

聚类分析：k-均值聚类损失函数总结了所有组内样本之间的差异，

$$ I = \sum_{i=1}^{K} \sum_{x_j \in C_i} \| x_j - \mu_i \|² $$

其中 $K$ 是簇的总数，$C_i$ 代表第 $i$ 个簇中的样本集，$x_j$ 代表簇 $C_i$ 中的数据样本，$\mu_i$ 是簇 $C_i$ 的原型，$\| x_j - \mu_i \|²$ 是样本 $x_j$ 与簇原型 $\mu_i$ 之间的欧几里得距离的平方。在 mD 空间中，对样本和原型以及距离进行计算，具有 $1,\ldots,m$ 个特征。

+   通过最小化惯性，k-means 簇最小化组内差异，同时最大化组间差异

## **联合概率**

概率概念：考虑多个事件同时发生的概率，$A$ 和 $B$ 的概率表示为，

$$ P(A \cap B) = P(A,B) $$

或者 $A$、$B$ 和 $C$ 的概率表示为，

$$ P(A \cap B \cap C) = P(A,B,C) $$

在假设 $A$、$B$ 和 $C$ 独立的情况下，联合概率可以计算为，

$$ P(A,B,C) = P(A) \cdot P(B) \cdot P(C) $$

## **K 个区间离散化**

特征变换：将特征的取值范围划分为 K 个区间，然后对于每个样本，如果样本位于区间内则分配值为 1，如果不在区间内则分配值为 0

+   分箱策略包括均匀宽度分箱（均匀）和每个分箱中均匀数量的数据（分位数）

+   也称为独热编码

需要 K 个区间离散化的方法，

+   基础扩展以在更高维空间中工作

+   将连续特征离散化为分类特征，用于分类方法，如朴素贝叶斯分类器

+   构建直方图和卡方检验以比较分布差异

+   互信息分箱

## **K 重交叉验证**

机器学习概念：将数据划分为 K 个折叠，并在折叠上循环训练模型，使用剩余数据训练模型，并在折叠中的数据上测试模型。然后汇总所有折叠的测试准确性。

+   训练数据和测试数据的划分基于 K，例如，K = 4 时，每个折叠的测试数据为 25%，K = 5 时，每个折叠的测试数据为 20%

+   这是对仅应用一个训练和测试分割来构建单个模型的交叉验证的改进。K 重方法允许测试所有数据，并且汇总所有折叠的准确性往往可以平滑准确性与超参数的图表，从而进行更可靠的超参数调整

+   k 重交叉验证可以应用于检查模型性能以估计准确性（最常见）和模型不确定性（[Maldonado-Cruz 和 Pyrcz，2021](https://www.sciencedirect.com/science/article/pii/S0920410521006343)）

## **k-Means 聚类**

聚类分析：一种无监督的机器学习方法，用于分区聚类，将未标记数据分组，其中聚类组内的差异最小化。最小化的损失函数是，

$$ I = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i || $$

其中 $i$ 是聚类索引，$\alpha$ 是数据样本索引，$X$ 是数据样本，$\mu_i$ 是 $i$ 聚类原型，$k$ 是聚类总数，$|| X_m - \mu_m ||$ 是在 $M$ 维空间中从样本到聚类原型的欧几里得距离，计算如下，$$ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i} \right)² } $$

$$ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i} \right)² } $$

这里是 k-means 聚类的关键方面总结，

+   *k* - 作为模型超参数给出

+   *穷尽且互斥的组* - 所有数据分配到单个组

+   *原型方法* - 在特征空间中用合成案例的数量表示训练数据。对于 K-means 聚类，我们分配并迭代更新 $K$ 个原型。

+   *迭代解法* - 初始原型在特征空间中随机分配，每个训练样本的标签更新为最近的原型，然后原型调整到其分配的训练数据质心，重复进行，直到训练数据分配没有进一步更新。

+   *无监督学习* - 训练数据未标记，并根据其在特征空间中与原型的接近程度分配 $K$ 个标签。其想法是相似的事物，在特征空间中的接近度，应属于同一聚类组。

+   *特征加权* - 该过程取决于训练样本和原型在特征空间中的欧几里得距离。距离被视为相似性的“倒数”。如果特征具有显著不同的幅度，幅度和范围最大的特征将主导损失函数，聚类组将变得各向异性，与高范围特征垂直对齐。虽然常见的做法是对变量进行标准化/归一化，但可以通过不等方差应用按特征加权。注意，在本演示中，我们将特征归一化到 0.0 到 1.0 的范围。

## **k-最近邻**

k-最近邻：一种简单、可解释且灵活的非参数预测机器学习模型，基于局部加权窗口应用于 $k$ 个最近训练数据

k-最近邻方法类似于空间插值的卷积方法。卷积是两个函数的积分乘积，其中一个函数经过反转并沿 $\Delta$ 平移。

+   一种解释是使用加权函数 $𝑓(\Delta)$ 对函数进行平滑，以计算函数 $𝑔(x)$ 的加权平均值，

$$ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta $$

这很容易扩展到多维

$$ (f * g)(x, y, z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(\Delta_x, \Delta_y, \Delta_z) g(x - \Delta_x, y - \Delta_y, z - \Delta_z) \, d\Delta_x \, d\Delta_y \, d\Delta_z $$

在积分之前移动哪个函数的选择不会改变结果，卷积算子具有交换性。

$$ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta $$$$ (f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta $$

+   如果任一函数被反射，则卷积等同于交叉相关，作为位移函数的信号相似度度量。

+   对于 k 个最近邻，使用 $k$ 结果是一个局部自适应窗口大小，不同于标准卷积

K 个最近邻是一种基于实例的懒惰学习方法，模型训练被推迟到需要预测时，不需要预先计算模型。即，预测需要访问数据。

+   要进行新的预测，必须提供训练数据

超参数包括，

+   *k 个最近的数据点*用于预测

+   *数据加权*，例如使用局部训练数据平均值的均匀加权，或逆距离加权

注意，对于逆距离加权的情形，该方法类似于逆距离加权插值，通常应用于空间插值，并施加最大局部数据数量约束。

+   GeostatsPy 中提供了逆距离用于空间映射。

要找到 k 个最近的数据点，需要一个距离度量，

+   预测特征空间内的训练数据按距离排序（从近到远）

+   可以应用各种距离度量，包括：

1.  欧几里得距离

\begin{equation}

d_i = \sqrt{\sum_{\alpha = 1}^{m} \left(x_{\alpha,i} - x_{\alpha,0}\right)²} \end{equation}

1.  Minkowski 距离 - 距离的通用表达式，其中已知的曼哈顿和欧几里得距离是特殊情况，

$$ d_{(i,i')} = \left( \sum_{j=1}^{m} \left( x_{(j,i)} - x_{(j,i')} \right)^p \right)^{\frac{1}{p}} $$

+   当 $p=2$ 时，这成为欧几里得距离

+   当 $p=1$ 时，它成为曼哈顿距离

## **核技巧**（支持向量机）

支持向量机：我们可以在我们的方法中包含基础扩展，而无需将训练数据转换为这个更高维度的空间，

$$ h(x) $$

我们只需要预测特征上的内积，

$$ h(x) \left( h(x') \right)^T = \langle h(x), h(x') \rangle $$

而不是变换空间中的实际值，我们只需要该变换空间中所有可用训练数据之间的‘相似性’！

+   我们仅使用训练数据之间的相似性矩阵来训练支持向量机，这些数据将被投影到更高维度的空间

+   我们实际上永远不需要计算更高维空间中的训练数据值

## **克里金法**

数据准备：一种依赖于线性权重的空间估计方法，这些权重考虑了空间连续性、数据接近性和冗余。克里金估计为，

$$ z^*(\bf{u}) = \sum_{\alpha = 1}^{n} \lambda_{\alpha} \cdot z(\bf{u}_{\alpha}) + \left( 1.0 - \sum_{\alpha=1}^n \lambda_{\alpha} \right) \cdot m_z $$

+   正确的术语是不偏性约束，将权重之和的倒数应用于全局均值。

在去除趋势 $t(\bf{u})$ 的情况下，我们现在有一个残差，$y(\bf{u})$，

$$ y(\bf{u}) = z(\bf{u}) - t(\bf{u}) $$

剩余均值为零，因此我们可以简化我们的克里金估计为，

$$ y^*(\bf{u}) = \sum_{\alpha = 1}^{n} \lambda_{\alpha} \cdot y(\bf{u}_{\alpha}) $$

简单克里金权重是通过求解线性方程组来计算的，

$$ \sum_{j=1}^n \lambda_j C(\bf{u}_i,\bf{u}_j) = C(\bf{u},\bf{u}_i), \quad i=1,\ldots,n $$

这可以用矩阵表示，

$$\begin{split} \begin{bmatrix} C(\bf{u}_1,\bf{u}_1) & C(\bf{u}_1,\bf{u}_2) & \dots & C(\bf{u}_1,\bf{u}_n) \\ C(\bf{u}_2,\bf{u}_1) & C(\bf{u}_2,\bf{u}_2) & \dots & C(\bf{u}_2,\bf{u}_n) \\ \vdots & \vdots & \ddots & \vdots \\ C(\bf{u}_n,\bf{u}_1) & C(\bf{u}_n,\bf{u}_2) & \dots & C(\bf{u}_n,\bf{u}_n) \\ \end{bmatrix} \cdot \begin{bmatrix} \lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_n \\ \end{bmatrix} = \begin{bmatrix} C(\bf{u}_1,\bf{u}) \\ C(\bf{u}_2,\bf{u}) \\ \vdots \\ C(\bf{u}_n,\bf{u}) \\ \end{bmatrix} \end{split}$$

通过将克里金估计的方程代入估计方差的方程，然后设置关于权重的偏导数为零，可以推导出这个系统。

+   我们正在优化权重以最小化估计方差

这个系统综合了，

+   通过变异函数（以及协方差函数来计算协方差值 $C$）量化的 *空间连续性*

+   *冗余* - 所有可用数据与其自身之间的空间连续性程度，$C(\bf{u}_i,\bf{u}_j)$

+   *接近性* - 可用数据与估计位置之间的空间连续性程度，$C(\bf{u}_i,\bf{u})$

克里金提供了一个称为克里金方差的估计精度度量（估计方差的一个特例）。

$$ \sigma^{2}_{E}(\bf{u}) = C(0) - \sum^{n}_{\alpha = 1} \lambda_{\alpha} C(\bf{u}_0 - \bf{u}_{\alpha}) $$

克里金估计之所以最佳，是因为它们最小化了上述估计方差。

克里金估计的性质包括，

+   *精确插值器* - 在数据位置处的克里金估计与数据值

+   *克里金方差* - 克里金估计中的不确定性度量。可以在获取样本信息之前计算，因为克里金估计方差不依赖于数据的值或克里金估计，即克里金估计量是同方差性的。

+   *空间上下文* - 克里金综合了空间连续性、邻近性和冗余性；因此，克里金考虑了数据的配置和被估计特征的连续性结构。

+   *尺度* - 默认情况下，克里金假设估计和数据位于相同的支持点，即数学上表示为空间中的零体积的点。克里金可以推广以考虑数据和估计的支持体积，

+   *多元* - 克里金可以通过共克里金系统推广，以考虑空间估计中的多个次级数据。我们将在后面介绍这一点。

+   *平滑效应* - 克里金法可以预测为缺失方差。局部估计的缺失方差是克里金方差。

## **基于克里金的去聚类**

数据准备：一种去聚类方法，根据局部采样密度对空间样本分配权重，使得加权统计更有可能代表总体。数据权重分配如下，

+   稠密采样区域中的样本获得较少权重

+   稀疏采样区域中的样本获得更多权重

基于克里金的去聚类过程如下：

1.  计算并建模实验变异函数

1.  应用克里金计算覆盖感兴趣区域的高分辨率网格上的估计

1.  计算分配给每个数据的权重总和

1.  分配与该权重总和成比例的数据权重

权重计算如下：

$$ w(\bf{u}_j) = n \cdot \frac{\sum_{iy}^{ny} \sum_{ix}^{nx} \lambda_j}{\sum_{i=1}^n \left[ \sum_{iy}^{ny} \sum_{ix}^{nx} \lambda_{j,ix,iy} \right]} $$

其中 $nx$ 和 $ny$ 是网格中的单元格数量，$n$ 是数据数量，$\lambda_{j,ix,iy}$ 是分配给 $ix,iy$ 网格单元格中 $j$ 数据的权重。

这里是关于基于克里金去聚类的一个重要观点，

+   类似于多边形去聚类，基于克里金的去聚类对感兴趣区域的边界敏感；因此，当感兴趣区域扩展或收缩时，分配给边界附近数据的权重可能会发生根本性的变化。

此外，基于克里金的去聚类还综合了变异函数模型的空间连续性模型。考虑以下变异函数模型对去聚类权重可能产生的影响，

+   如果有 100% 的相对块状效应，则没有空间连续性，因此，所有数据都获得相等的权重。注意，对于上面的方程，这会导致除以 0.0 的错误，必须在代码中进行检查。

+   几何各向异性可能会显著影响权重，因为沿特定方位对齐的数据在协方差方面被视为更接近或更远。

## **柯尔莫哥洛夫的 3 个概率公理**

概率概念：这些是柯尔莫哥洛夫对有效概率的 3 个公理，

1.  事件发生的概率是一个非负数。

$$ P(𝐴) \ge 0 $$

1.  整个样本空间（所有可能的结果）的概率 $\Omega$ 为一（单位），也称为概率封闭。

$$ P(\Omega) = 1 $$

1.  互斥事件的并集的加法性。

$$ P\left(⋃_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i) $$

例如，$A_1$ 和 $A_2$ 互斥事件的概率是，$P(A_1 + A_2) = P(A_1) + P(A_2)$

## **$L¹$ 范数**

线性回归：称为曼哈顿范数或绝对残差和（SAR），

$$ \sum_{i=1}^n |\Delta y_i | $$

也表示为平均绝对误差（MAE），

$$ \frac{1}{n} \sum_{i=1}^n |\Delta y_i | $$

使用 $L¹$ 范数进行最小化称为最小绝对差异。

## **$L²$ 范数**

线性回归：称为平方残差和（SSR），

$$ \sum_{i=1}^n \sqrt{\Delta y_i} $$

也表示为均方误差（MSE），

$$ \frac{1}{n} \sum_{i=1}^n \left( \Delta y_i \right)² $$

以及欧几里得范数，

$$ \sqrt{ \sum_{i=1}^n \sqrt{\Delta y_i} } $$

使用 $L²$ 范数进行最小化称为最小二乘法。

## **$L¹$ 与 $L²$ 范数**

LASSO 回归：在机器学习中，$L¹$ 和 $L²$ 范数的选择很重要。为了解释这一点，让我们比较在训练模型参数时 $L¹$ 和 $L²$ 范数在损失函数中的性能。

| 属性 | 最小绝对偏差（L1） | 最小二乘（L2） |
| --- | --- | --- |
| 鲁棒性* | 鲁棒 | 非常不鲁棒 |
| 解的稳定性 | 不稳定解 | 稳定解 |
| 解的数量 | 可能存在多个解 | 总是只有一个解 |
| 特征选择 | 内置特征选择 | 无特征选择 |
| 输出稀疏性 | 稀疏输出 | 非稀疏输出 |
| 解析解 | 无解析解 | 解析解 |

这里有一些重要的要点，

+   *鲁棒* - 对异常值有抵抗力

+   *不稳定* - 对于训练模型的微小变化，预测结果可能会跳跃

+   *多重解* - 不同的解具有相似或相同的损失，导致解在训练数据微小变化时跳跃

+   *输出稀疏性* 和 *特征选择* - 模型参数趋向于 0.0

+   *解析解* - 可用解析解求解最优模型参数

## **$L¹$ 或 $L²$ 正则化器**

特征变换：在单个样本的特征上执行以约束总和

L1 范数在样本间有以下约束，

$$ \sum_{\alpha = 1}^m x^{\prime}_{i,\alpha} = 1.0, \quad i = 1, \ldots, n $$

L1 正则化变换，

$$ x^{\prime}_{i,\alpha} = \frac{x_{i,\alpha}}{\sum_{\alpha=1}^m x_{i,\alpha}} $$

L2 范数在样本间有以下约束，

$$ \sum_{\alpha = 1}^m \left( x^{\prime}_{i,\alpha} \right)² = 1.0, \quad i = 1, \ldots, n $$

L2 正则化变换，

$$ x^{\prime}_{i,\alpha} = \sqrt{\frac{(x_{i,\alpha})²}{\sum_{\alpha=1}^m (x_{i,\alpha})²}} $$

例如，应用于文本分类和聚类，以及用于组合数据的 L1（求和约束为 1.0）

## **LASSO 回归**

LASSO 回归：具有$L¹$正则化项和正则化超参数λ的线性回归，

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)² + \lambda \sum_{j=1}^m |b_{\alpha}| $$

因此，LASSO 回归训练整合了两个经常竞争的目标来寻找模型参数，

+   找到模型参数以最小化训练数据中的误差

+   将斜率参数最小化到零

LASSO 和岭回归之间的唯一区别是：

+   对于 LASSO，收缩项被表示为$\ell_1$惩罚，

$$ \lambda \sum_{\alpha=1}^m |b_{\alpha}| $$

+   对于岭回归，收缩项被表示为$\ell_2$惩罚，

$$ \lambda \sum_{\alpha=1}^m \left(b_{\alpha}\right)² $$

当岭回归和 LASSO 都将模型参数($b_{\alpha}, \alpha = 1,\ldots,m$)收缩到零时：

+   当正则化超参数λ增加时，LASSO 参数以不同的速率达到零，对于每个预测特征。

+   因此，LASSO 提供了一种特征排序和选择的方法！

λ超参数控制模型的拟合程度，可能与模型的偏差-方差权衡有关。

+   当$\lambda \rightarrow 0$时，预测模型趋近于线性回归，模型偏差较低，但模型方差较高

+   当λ增加时，模型方差降低，模型偏差增加

+   当$\lambda \rightarrow \infty$时，所有系数都变为 0.0，模型是训练数据响应特征的平均值

## **懒惰学习**

k-最近邻：模型是训练数据的一般化，计算在查询模型时才进行

+   模型是训练数据和选定的超参数，要做出新的预测，必须提供训练数据

相反的是急切学习。

## **学习率**（梯度提升）

梯度提升：控制每个新模型更新的速率。

$$ f_m = f_{m-1} - \rho_m \frac{\partial L(y_\alpha, F(X_\alpha))}{\partial F(X_\alpha)} $$

其中$\rho_m$是学习率，$\frac{\partial L(y_\alpha, F(X_\alpha))}{\partial F(X_\alpha)}$是梯度，误差，$f_{m-1}$是前一个估计，$f_m$是新的估计。

关于学习率的一些显著点，

+   没有学习率，提升模型学习得太快，模型方差会过高

+   减缓学习速度以获得更稳健的模型，平衡以确保良好的性能，过小的速率将需要非常大的模型数量才能达到收敛

## **类似删除**（MRMR）

特征排序：移除任何具有任何缺失特征值的样本

+   如果缺失的特征值不是随机缺失（MAR），这可能会在数据中引入偏差

+   将导致有效数据量减少和模型不确定性增加

## **线性回归**

线性回归：一个线性、参数化的预测模型，

$$ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 $$

对于 L2 范数损失函数，模型参数$b_1,\ldots,b_m,b_0$的解析解是可用的，误差是求和并平方的，已知为最小二乘法。

+   我们最小化训练数据上的误差，即残差平方和（RSS）：

$$ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)² $$

其中$y_i$是实际响应特征值，$\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$是模型预测，在$\alpha = 1,\ldots,n$的训练数据上。

+   这可以简化为训练数据上的平方误差之和，

$$ \sum_{i=1}^n (\Delta y_i)² $$

其中$\Delta y_i$是实际响应特征观测值$y_i$减去模型预测$\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$，在$i = 1,\ldots,n$的训练数据上。

我们的线性回归模型有一些重要的假设，

+   *无误差* - 预测变量无误差，不是随机变量

+   *线性* - 响应是特征（s）的线性组合

+   *常数方差* - 响应误差在预测值上是恒定的

+   *误差独立性* - 响应误差彼此不相关

+   *无多重共线性* - 没有特征与其他特征冗余

## **位置图**

加载数据和绘图模型：一个数据图，其中两个轴是位置，例如$X$和$Y$，东西方向和南北方向，纬度和经度等，以显示空间数据的位置和大小。

+   经常将数据点着色以表示特征的比例，以便在感兴趣的区域或体积上可视化采样特征

+   优点，可视化数据而不需要任何可能影响我们对数据印象的模型

+   缺点，可能难以可视化大型数据集和三维数据

## **损失函数**

LASSO 回归：用于训练模型参数的最小化方程。例如，线性回归的损失函数包括残差平方和、$L²$误差范数，

$$ \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)² $$

对于 LASSO 回归，损失函数包括残差平方和、$L²$误差范数，以及一个$L¹$正则化项，

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)² + \lambda \sum_{j=1}^m |b_{\alpha}| $$

对于 k-means 聚类，损失函数是，

$$ I = \sum^k_{i=1} \sum_{\alpha \in C_i} \sqrt{ \sum_{j = 1}^m X_{\alpha,m} - \mu_{i,m} } $$

最小化损失函数的方法取决于范数的类型，

+   使用 $L²$ 范数，我们对损失函数相对于模型参数进行微分，并将其设置为等于零

+   在我们的损失函数中使用 $L¹$ 范数时，我们失去了分析解的访问权限，并使用迭代优化，例如最速下降法

## **机器学习工作流程设计**

机器学习工作流程构建和编码：基于以下步骤，

1.  *指定目标* - 例如，

+   建立数值模型

+   评估不同的恢复过程

1.  *指定数据* - 可用的是什么，缺少的是什么？

1.  *设计一组步骤以实现目标* - 常见步骤包括，

+   加载数据

+   格式化、检查和清理数据

+   运行操作，包括统计计算、模型或可视化

+   转换函数

1.  *开发文档* - 包括实现细节、决策的辩护、元数据、限制和未来工作

1.  *流程* - 数据和信息流，在建模时使用分支和回环进行学习

1.  *不确定性* - 总结所有不确定性来源，包括集成不确定性的方法，辩护不确定性模型和被认为确定的因素

## **边缘**（支持向量机）

支持向量机：当训练数据包括重叠类别时，不可能也不希望开发一个完美分离这些类别的决策边界，对于这个条件将成立，

$$ y_i \left( x_i^T \beta + \beta_0 \right) \geq 0 $$

我们需要一个允许某些误分类的模型。

$$ y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i $$

我们引入了边缘的概念 $𝑀$ 和边缘的距离（误差，$𝜉_𝑖$）。

$$ \underset{\beta, \beta_0}{\text{min}} \left( \frac{1}{2M²} + C \sum_{i=1}^N \xi_i \right) $$

损失函数包括边缘项 $M$，因此试图在最小化分类错误的同时最小化边缘，而分类错误由超参数 $C$ 加权。

## **边缘概率**

概率概念：只考虑一个事件发生的概率，即 $A$ 的概率，

$$ P(A) $$

边缘概率可以通过边缘化过程从联合概率中计算得出，

$$ P(A) = \int_{-\infty}^{\infty} P(A,B) dB $$

在这里，我们对其他事件的所有情况 $B$ 进行积分，以消除其影响。对于事件 $B$ 的离散可能情况，我们可以简单地对所有可能的 $B$ 的情况求和概率，

$$ P(A) = \sum_{i=1}^{k_B} P(A,B) dB $$

## **矩阵散点图**

多元分析：包括所有特征的所有成对散点图的组合图。

+   给定 $m$ 个特征，有 $m \times m$ 个散点图

+   散点图是有序的，y 轴特征从 $X_1,\ldots,X_m$ 到行，x 轴特征从 $X_1,\ldots,X_m$ 到列

+   对角线是特征与其自身绘制的，通常被特征直方图或概率密度函数所取代

我们使用矩阵散点图来，

+   寻找二元线性或非线性结构

+   寻找二元同方差（条件方差恒定）和异方差（条件方差随值变化）

+   寻找二元约束，例如，具有成分数据的求和约束

记住，其他特征被边缘化，这不是一个完整的 m-D 可视化。

## **最大相关性最小冗余** (MRMR)

特征排序：一种基于互信息的特征排序方法，考虑了特征的相关性和冗余。

+   一个例子是相关性减去冗余的总结，

$$ MRMR = max \left[ frac{1}{|S|} \sum_{X_i \in S} I(X_i,Y) - \frac{1}{|S|²} \sum_{X_i \in S} \sum_{X_j, i \ne j} I(X_i,X_j) \right] $$

其中 $𝑆$ 是预测特征子集，而 $|𝑆|$ 是子集 $𝑆$ 中的特征数量。

## **Metropolis-Hastings MCMC 样本器**

贝叶斯线性回归：Metropolis-Hastings MCMC 样本器的基本步骤：

对于 $\ell = 1, \ldots, L$:

1.  为模型参数的初始样本分配随机值，$\beta(\ell = 1) = b_1(\ell = 1)$，$b_0(\ell = 1)$ 和 $\sigma²(\ell = 1)$。

1.  根据提议函数提出新的模型参数，$\beta^{\prime} = b_1$，$b_0$ 和 $\sigma²$.

1.  计算新提议的接受概率，即新模型参数在给定数据下的后验概率与旧模型参数在给定数据下的后验概率的比值，乘以旧步骤在给定新步骤下的概率除以新步骤在给定旧步骤下的概率。

$$ P(\beta \rightarrow \beta^{\prime}) = min\left(\frac{P(\beta^{\prime}|y,X) }{ P(\beta | y,X)} \cdot \frac{P(\beta^{\prime}|\beta) }{ P(\beta | \beta^{\prime})},1\right) $$

1.  应用蒙特卡洛模拟来条件性地接受提议，如果被接受，$\ell = \ell + 1$，并采样 $\beta(\ell) = \beta^{\prime}$

1.  转到步骤 2。

## **Minkowski 距离**

k-最近邻：距离的一般表达式，其中已知的曼哈顿和欧几里得距离是特殊情况，

$$ d_{(i,i')} = \left( \sum_{j=1}^{m} \left( x_{(j,i)} - x_{(j,i')} \right)^p \right)^{\frac{1}{p}} $$

+   当 $p=2$ 时，这变为欧几里得距离

+   当 $p=1$ 时，它变为曼哈顿距离

## **缺失特征值**

特征插补：数据表中的空值，没有所有特征值的样本

缺失特征值的原因有很多，例如，

1.  采样成本，例如，低渗透率测试耗时过长

1.  岩石流变学样本过滤器，例如，无法恢复泥岩样本

1.  通过采样减少不确定性并最大化盈利性，而不是统计代表性，双重用途样品用于信息和生产

缺失数据后果，不仅仅是减少训练和测试数据量，如果缺失数据不是完全随机，可能会产生，

+   偏差样本统计导致模型训练和测试偏差

+   偏差模型具有偏差预测，可能没有偏差的指示

## **随机缺失** (MAR)

特征插补：缺失特征值是随机分布的，在预测特征空间中均匀覆盖，即所有值都有可能缺失，并且缺失特征值之间没有相关性。

这通常不是情况，因为缺失数据通常具有混杂特征，例如，

1.  采样成本，例如，低渗透率测试耗时过长

1.  岩石流变学样品过滤器，例如，无法恢复泥岩样品

1.  通过采样减少不确定性并最大化盈利性，而不是统计代表性，双重用途样品用于信息和生产

缺失数据后果，不仅仅是减少训练和测试数据量，如果缺失数据不是完全随机，可能会产生，

+   偏差样本统计导致模型训练和测试偏差

+   偏差模型具有偏差预测，可能没有偏差的指示

## **模型偏差**

机器学习概念：是由于模型复杂性和灵活性不足，无法适应自然设置而产生的错误

+   增加模型复杂度通常会导致模型偏差减少

+   *模型偏差方差权衡* - 随着复杂性的增加，模型方差增加，模型偏差减少

+   预期测试平方误差的三个组成部分之一，包括模型方差、模型偏差和不可减少误差

$$ E \left[ \left(y_0 - \hat{f}(x_1⁰, \ldots, x_m,⁰ \right)² \right] = \left(E [\hat{f}(x_1⁰, \ldots, x_m,⁰)] - f(x_1⁰, \ldots, x_m,⁰) \right)² + $$$$ E \left[ \left( \hat{f} \left(x_1⁰, \ldots, x_m,⁰ \right) - E \left[ \hat{f}(x_1⁰, \ldots, x_m,⁰) \right] \right)² \right] + \sigma_e² $$

其中 $\left(E [\hat{f}(x_1⁰, \ldots, x_m,⁰)] - f(x_1⁰, \ldots, x_m,⁰) \right)²$ 是模型偏差。

## **模型偏差方差权衡**

机器学习概念：随着复杂性的增加，模型方差增加，模型偏差减少。

+   由于模型方差和模型偏差都是预期测试平方误差的组成部分，因此模型偏差和模型方差的平衡导致了一个最佳复杂度水平，以最小化测试误差

## **模型检查**

机器学习概念：是任何空间建模工作流程的关键最后一步。以下是模型检查的关键方面，

1.  *模型输入* - 数据和统计整合

+   检查模型以确保模型输入在模型中得到尊重，通常对所有实现进行检查，例如，输出直方图与实现中的输入直方图相匹配

1.  *精确空间估计* - 模型在可用样本数据之外，在各种配置下，以精度预测的能力

+   通过交叉验证，保留部分数据，检查模型预测的能力

+   通常，用真实值与预测交叉图和均方误差等指标来总结

$$ MSE = \frac{1}{n} \sum_{\alpha = 1}^{n} \left(z^{*}(\bf{u}_{\alpha}) - z(\bf{u}_{\alpha}) \right)² $$

1.  *精确和精确的不确定性模型* - 在给定信息量和各种不确定性来源的情况下，不确定性模型是公平的

+   也通过交叉验证，保留部分数据，但通过检查特定概率区间中的数据比例

+   用保留数据的比例在区间与概率区间之间的比例来总结

+   在 45 度线上的点表示精确和精确的不确定性模型

+   在 45 度线以上的点表示精确和不精确的不确定性模型，不确定性太宽

+   在 45 度线以下的点表示不精确的不确定性模型，不确定性太窄或模型存在偏差

## **模型复杂度或灵活性**

机器学习概念：模型拟合数据和可解释的能力。

可以使用各种概念来描述模型复杂度，

+   模型中的特征数量、预测变量，模型的维度，通常导致更多的模型参数

+   参数的数量，每个项应用的顺序，例如线性、二次、阈值

+   模型的格式，即多项式回归的紧凑方程与决策树嵌套条件语句相比，与神经网络成千上万的权重和偏差模型参数相比

+   例如，更高阶的多项式、更大的决策树等更复杂

通常，更复杂或灵活的模型更难以解释，

+   线性回归及其相关模型参数可以进行分析和甚至应用于特征排序，而具有径向基函数的支持向量机在 nD 高维空间中是一个线性模型

## **模型泛化**

机器学习概念：模型在训练数据之外预测的能力。

+   模型学习数据中的结构，而不仅仅是记住训练数据

模型泛化能力不佳，

+   过拟合模型在训练数据上具有高精度，而在训练数据之外的数据上精度较低，这通过低测试精度得到体现

+   欠拟合模型对于自然现象来说过于简单或不灵活，具有低训练和测试精度

## **模型超参数**

机器学习概念：约束模型复杂度。超参数调整以最大化保留测试数据上的准确性，以防止模型过拟合。

对于从 $4^{th}$ 到 $1^{st}$ 次的多项式模型集，

$$ y = b_4 \cdot x⁴ + b_3 \cdot x³ + b_2 \cdot x² + b_1 \cdot x + b_0 $$$$ y = b_3 \cdot x³ + b_2 \cdot x² + b_1 \cdot x + b_0 $$$$ y = b_2 \cdot x² + b_1 \cdot x + b_0 $$$$ y = b_1 \cdot x + b_0 $$

多项式阶数的选择是超参数，即，一阶模型最简单，四阶模型最复杂。

## **模型参数**

机器学习概念：机器学习模型的训练系数，用于控制对训练数据的拟合。

对于一个多项式模型，

$$ y = b_3 \cdot x³ + b_2 \cdot x² + b_1 \cdot x + b_0 $$

$b_3$, $b_2$, $b_1$ 和 $b_0$ 是模型参数。

+   *训练模型参数* - 模型参数通过优化计算，以最小化训练数据上的误差和正则化项，通过解析解或迭代解，例如梯度下降优化。

## 模型正则化

岭回归：添加信息以防止过拟合（或欠拟合），提高模型泛化。

+   这信息被称为正则化项

+   这代表了一个与正则化超参数调整的复杂度惩罚

考虑岭回归损失函数，

$$ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)² + \lambda \sum_{j=1}^m b_{\alpha}² $$

其中 $\lambda \sum_{j=1}^m b_{\alpha}²$ 是正则化项，而 $\lambda$ 是正则化超参数。

正则化的概念相当普遍，机器学习架构中的选择，例如，

+   卷积神经网络（CNNs）中感受野的使用

+   限制决策树最大层数的选择。

有几个有用的视角关于模型正则化，

+   *奥卡姆剃刀原则* - 正则化将模型复杂度调整到最简单的有效解。

+   *贝叶斯视角* - 正则化是对解施加先验。

## **模型方差**

机器学习概念：是因对数据集敏感而产生的误差

+   增加模型复杂度通常会导致模型方差增加

+   例如，集成机器学习，模型袋装通过在数据集的引导实现上平均多个估计量来减少模型方差

+   *模型偏差与方差权衡* - 随着复杂度的增加，模型方差增加，模型偏差减少

+   预期测试平方误差的三个组成部分之一，包括模型方差、模型偏差和不可减少误差

$$ E \left[ \left(y_0 - \hat{f}(x_1⁰, \ldots, x_m,⁰ \right)² \right] = \left(E [\hat{f}(x_1⁰, \ldots, x_m,⁰)] - f(x_1⁰, \ldots, x_m,⁰) \right)² + $$$$ E \left[ \left( \hat{f} \left(x_1⁰, \ldots, x_m,⁰ \right) - E \left[ \hat{f}(x_1⁰, \ldots, x_m,⁰) \right] \right)² \right] + \sigma_e² $$

其中 $E \left[ \left( \hat{f} \left(x_1⁰, \ldots, x_m,⁰ \right) - E \left[ \hat{f}(x_1⁰, \ldots, x_m,⁰) \right] \right)² \right]$ 是模型方差。

## **动量** (优化)

LASSO 回归：更新前一步骤，动量，$\lambda$，是应用于前一步骤的权重，而 $1 - \lambda$ 是应用于当前步骤的权重，

$$ \left( \left( r \cdot \nabla L \right)_{t-1} \right)^m = \lambda \cdot r \cdot \nabla L_{t-2} + (1 - \lambda) \cdot r \cdot \nabla L_{t-1} $$

+   从每个模型参数的损失函数偏导数计算出的梯度存在噪声。动量平滑可以减少这种噪声的影响。

+   动量有助于解决方案沿着损失函数的一般斜率前进，而不是在局部峡谷或凹槽中振荡

## **马尔可夫链蒙特卡洛** (MCMC)

贝叶斯线性回归：一组算法，用于从概率分布中采样，使得样本匹配分布统计信息。

+   *马尔可夫* - 屏蔽假设，下一个样本只依赖于前一个样本

+   *链* - 样本形成一个序列，通常显示出从具有不准确统计信息的烧毁链到具有准确统计信息的平衡链的过渡

+   *蒙特卡洛* - 使用蒙特卡洛模拟，从统计分布中进行随机采样

这有什么用？

+   我们通常没有目标分布，它是未知的

+   但我们可以通过其他形式的信息，如条件概率密度函数、Gibbs 采样或候选下一样本和当前样本的似然比，以正确的频率进行采样，Metropolis-Hastings

## **Metropolis-Hastings 采样** (MCMC)

贝叶斯线性回归：一组算法，用于从概率分布中采样，使得样本匹配分布统计信息，基于，

+   候选下一样本和当前样本的似然比

+   基于这个似然比的反向采样器

由于只需要似然比的比率，系统简化为证据项从贝叶斯概率中取消

这里是 Metropolis-Hastings MCMC 采样器的基本步骤：

对于 $\ell = 1, \ldots, L$:

1.  为模型参数的初始样本分配随机值，$\beta(\ell = 1) = b_1(\ell = 1)$，$b_0(\ell = 1)$ 和 $\sigma²(\ell = 1)$。

1.  根据建议函数提出新的模型参数，$\beta^{\prime} = b_1$，$b_0$ 和 $\sigma²$。

1.  计算新提议的接受概率，即新模型参数在给定数据下的后验概率与旧模型参数在给定数据下的后验概率的比值，乘以旧步骤在给定新步骤下的概率除以新步骤在给定旧步骤下的概率。

$$ P(\beta \rightarrow \beta^{\prime}) = min\left(\frac{P(\beta^{\prime}|y,X) }{ P(\beta | y,X)} \cdot \frac{P(\beta^{\prime}|\beta) }{ P(\beta | \beta^{\prime})},1\right) $$

1.  将蒙特卡洛模拟应用于条件接受提议，如果接受，$\ell = \ell + 1$，并采样 $\beta(\ell) = \beta^{\prime}$

1.  返回步骤 2。

## **蒙特卡洛模拟（MCS**）

贝叶斯线性回归：从统计分布中抽取的随机样本，随机变量。蒙特卡洛模拟（MCS）的步骤如下：

1.  建立特征累积分布函数模型，$F_x(x)$

1.  从均匀分布[0,1]中抽取随机值，这是一个随机累积概率值，称为 p 值，$p^{\ell}$

1.  应用累积分布函数的逆来计算相关的实现

$$ x^{\ell} = F_x^{-1} (p^{\ell}) $$

1.  重复计算，以获得足够多的实现用于后续分析

蒙特卡洛模拟是随机模拟工作流程的基本构建块，例如，

+   *蒙特卡洛模拟工作流程* - 将蒙特卡洛模拟应用于所有特征到传递函数，以计算决策标准的实现，重复多次以通过传递函数传播不确定性

+   *自助法* - 将蒙特卡洛模拟应用于获取数据的实现，以计算样本统计量或基于集成预测模型的集成的不确定性

+   *蒙特卡洛方法* - 将蒙特卡洛模拟应用于使用有限随机样本加速昂贵计算，随着随机样本数量的增加，这些样本收敛到解

## **蒙特卡洛模拟工作流程**

贝叶斯线性回归：一种方便的随机工作流程，通过蒙特卡洛模拟（MCS）进行采样来传播通过传递函数的不确定性。该工作流程包括以下步骤，

1.  建立所有输入特征的分布，累积分布函数，

$$ F_{x_1}(x_1), \quad F_{x_2}(x_2), \quad \dots \quad , F_{x_m}(x_m) $$

1.  对所有输入进行蒙特卡洛模拟以获得实现，

$$ x_1^{\ell}, \quad x_2^{\ell}, \quad \ldots \quad , x_m^{\ell} $$

1.  将其应用于传递函数以获得传递函数输出的实现，通常是*决策标准*

$$ y^{\ell} = f \left(x_1^{\ell},x_2^{\ell}, \quad \ldots \quad, x_m^{\ell} \right) $$

1.  重复步骤 1-3，以计算足够多的实现来模拟传递函数输出分布。

$$ F_y(y) $$

## **乘法法则**（概率）

概率概念：我们可以通过 $A$ 给定 $B$ 的条件概率与 $A$ 的边缘概率的乘积来计算 $A$ 和 $B$ 的联合概率，

$$ P(A \cup B) = P(A,B) = P(B|A) \cdot P(A) $$

乘法规则是通过条件概率定义的简单操作推导出来的，在这种情况下，

$$ P(B|A) = \frac{P(A,B)}{P(A)} $$

## **互信息**

特征排序：一种量化两个特征之间相互依赖性的通用方法。

+   量化从观察一个特征中获取关于另一个特征的信息量

+   避免对关系的形状做出任何假设（例如，没有线性关系的假设）

    单位是香农或比特

+   将联合概率与边缘概率的乘积进行比较

+   +   总结了联合 $P(x,y)$ 与边缘 $P(x)\cdot P(y)$ 乘积之间的差异，积分遍历所有 $x \in 𝑋$ 和 $y \in Y$，

对于离散或分箱的连续特征 $X$ 和 $Y$，互信息计算如下：

$$ I(X;Y) = \sum_{y \in Y} \sum_{x \in X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) $$

回想一下，给定 $X$ 和 $Y$ 之间的独立性：

$$ P_{X,Y}(x,y) = P_X(x) \cdot P_Y(y) $$

因此，如果两个特征是独立的，那么 $log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) = 0$

联合概率 $P_{X,Y}(x,y)$ 是总和上的加权项，并强制封闭。

+   联合分布中密度更大的部分对互信息度量有更大的影响

对于连续（和非分箱）特征，我们可以应用积分形式。

$$ I(X;Y) = \int_{Y} \int_{X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) dx dy $$

## **互斥事件**（概率）

概率概念：事件不相交，即没有共同的结果。我们用以下方式表示，

+   使用集合表示法，我们表示事件 $A$ 和 $B$ 是互斥的，如下，

$$ A \cap B = \{x: x \in A \text{ and } x \in B \} = \emptyset $$

+   并且，互斥事件的概率为，

$$ P(A,B) = 0.0 $$

## **多维尺度**

多维尺度：一种在信息可视化中用于探索/可视化高维数据集中个体样本之间相似性（或相反的差异）的推断统计方法。

多维尺度（MDS）将 $m$ 维数据投影到 $p$ 维，使得 $p << m$。

+   在尝试保留数据样本之间的成对差异的同时

+   理想情况下，我们能够将 $p=2$ 投影出来，以便轻松探索样本之间的关系

虽然主成分分析（PCA）使用协方差矩阵进行操作，多维尺度分析则使用距离或相似性矩阵。对于多维尺度分析，

+   你不需要知道实际的特征值，只需要知道样本之间的距离或相似性

+   就像特征空间中的任何距离一样，我们考虑特征标准化以确保具有更大方差的特征不会主导计算

+   我们可以使用各种相似性度量

多维尺度分析与主成分分析的比较，

+   主成分分析通过所有特征之间的协方差矩阵 ($m \times m$) 找到线性正交旋转，使得在有序的主成分上 *方差最大化*

+   多维尺度分析通过特征空间中所有样本之间的成对距离矩阵 ($n \times n$) 并找到非线性投影，使得 *成对距离误差最小化*

有些人认为在多维尺度空间中可视化数据或模型是可视化不确定性空间。

## **朴素贝叶斯**

朴素贝叶斯：从贝叶斯更新的角度，将条件独立性假设应用于简化分类预测问题，基于给定 $n$ 个特征 $x_1, \dots , x_n$ 的类别 $k$ 的条件概率，

$$ P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3 | x_4, \dots , x_n, C_k) \ldots P(x_{n-1} | x_n, C_k) (x_{n} | C_k) P(C_k) $$

联合条件概率的似然，从联合条件概率的角度看是困难的，可能是不可能的。它需要关于 $x_1, \dots , x_n$ 特征之间联合关系的信息。随着 $n$ 的增加，这需要大量数据来告知联合分布。

在朴素贝叶斯方法中，我们做出“朴素”的假设，即特征都是 *条件独立的**。这包括，

$$ P(x_i | x_{i+1}, \ldots , x_n, C_k) = P(x_i | C_k) $$

对于所有 $i = 1, \ldots, n$ 特征。

我们现在可以解出所需的条件概率如下：

$$ P(C_k | x_1, \dots , x_n) = \frac{P(C_k) \prod_{i=1}^{n} P(x_i | C_k)}{P(x_1, \dots , x_n)} $$

我们只需要先验概率 $P(C_k)$ 和所有预测特征的条件概率集，$P(x_i | C_k)$，对于所有预测特征 $i = 1,\ldots,n$ 和所有类别 $k = 1,\ldots,K$。

证据项 $P(x_1, \dots , x_n)$ 仅基于特征 $x_1, \dots , x_n$；因此，在类别 $k = 1,\ldots,n$ 上是一个常数。

+   它确保了封闭性 - 所有类别的概率之和为 1

+   我们只是将分子标准化，使其在类别上求和为 1

朴素贝叶斯方法如下：

+   容易理解，建立在基本的贝叶斯统计基础之上

+   即使数据集很小，也是实用的，因为有了条件独立性，我们只需要估计简单的条件分布

## **ndarray**

机器学习工作流程构建和编码：Numpy 的便捷类，用于处理 2D 或 3D 上的网格、详尽、规则间隔的数据，表示地图和模型，因为，

+   便于存储、访问、操作网格数据的便捷数据结构

+   内置方法从各种文件类型中加载，Python 类

+   内置方法用于计算多维汇总统计量

+   内置方法用于数据查询、筛选

+   内置方法用于数据处理、清理、重新格式化

+   内置属性以存储关于 nD 数组的信息，例如大小和形状

## **非参数模型**

机器学习概念：一个不假设自然设置中函数形式、形状的模型。

+   从训练数据中学习形状，以适应自然系统的各种形状具有更多灵活性

+   与参数模型相比，模型对自然设置的拟合风险更低

通常需要更多的数据来准确估计非参数模型，

+   非参数模型通常具有许多可训练的参数，即非参数模型实际上是参数丰富的！

## **范数**

线性回归：向量的范数将向量值映射到表示大小或长度的度量$[𝟎,\infty)$。

为了训练我们的模型以训练数据为目标，我们需要一个单一的不匹配度量，即训练误差。误差在每个训练数据位置观察到，

$$ \Delta y_i = y_i - \hat{y}_i, \quad \forall \quad i = 1,\ldots,n $$

作为错误向量。我们需要一个单一值来总结所有训练数据，我们可以最小化它！

## **规范化**

特征转换：一种分布缩放，可以将其视为平移、拉伸或压缩一元分布（例如，*直方图*）到最小为 0.0 和最大为 1.0。

+   这是对原始属性分布的平移和拉伸/压缩，假设没有形状变化，保持等级

$$ y_i = \frac{x_i - min(x)}{max(x) - min(x)}, \quad \forall \quad i, \ldots, n $$

需要标准化和最小/最大归一化的方法：

+   k-means 聚类、k-最近邻回归

+   $\beta$系数用于特征排序

+   人工神经网络的前向转换预测特征和后向转换响应特征以改善激活函数的敏感性

## **归一化直方图**

单变量分析：是使用在可能值范围内的一个完整集合的 bin 上的概率图来表示单变量统计分布。

1.  将可能值的连续特征范围划分为$K$个等大小的区间，$\delta x$：

$$ \Delta x = \left( \frac{x_{max} - x_{min}}{K} \right) $$

或者使用可用的类别作为分类特征的类别。

1.  计算每个箱中样本的数量（频率），$n_k$，$\forall k=1,\ldots,K$，并将每个除以数据总数，$n$，以计算每个箱的概率，

$$ p_k = \frac{n_k}{n}, \forall \quad k = 1,\ldots,L $$

1.  绘制概率与箱标签的关系图（如果连续，则使用箱中心）

注意，归一化直方图通常以柱状图的形式绘制。

## **独热编码**

特征变换：将特征的取值范围划分为 K 个箱，然后对于每个样本，如果样本在箱内，则分配值为 1，如果不在箱内，则分配值为 0

+   箱划分策略包括均匀宽度箱（均匀）和每个箱中均匀数量的数据（分位数）

+   也称为 K 箱离散化

需要 K 箱离散化的方法，

+   基础扩展以在更高维空间中工作

+   将连续特征离散化为分类特征，用于分类方法，如朴素贝叶斯分类器

+   直方图构建和分布差异的卡方检验

+   互信息箱划分

## **袋外样本**

袋装树和随机森林：通过数据重采样，可以证明大约 $\frac{2}{3}$ 的数据将被包括（在期望中）。对于基于袋装的集成预测模型，

+   因此，每个模型实现中，有 $\frac{1}{3}$ 的数据（在期望中）未被用于训练，这些被称为袋外观测值

+   对于每个响应特征观测值，$y_{\alpha}$，有 $\frac{B}{3}$ 个袋外预测，$y^{*,b}_{\alpha}$

+   我们可以汇总这个预测实现的集成，对于回归取平均值或对于分类取众数，以计算单个袋外预测，$y^{*}_{\alpha} = \sum_{\alpha = 1}^{\frac{B}{3}} y^{*,b}_{\alpha}$

+   从所有数据中的单个袋外预测中，计算袋外均方误差（MSE）如下，

$$ MSE_{OOB} = \sum_{\alpha = 1}^{\frac{B}{3}} \left[ y^{*}_{\alpha} - y_{\alpha} \right]² $$

对于基于袋装的集成预测机器学习，不需要执行训练和测试划分，可以使用袋外均方误差进行超参数调整。

+   这相当于随机划分训练集和测试集，可能不公平，难度与计划使用模型相同

+   这将测试比例冻结在大约 $\frac{1}{3}$

## **过拟合模型**

机器学习概念：一个适合数据噪声或数据特殊性的机器学习模型

+   增加的复杂性通常会在训练数据集上降低误差，但可能会在测试数据上增加误差

+   在模型复杂度上升的测试误差和下降的训练误差的区域

过拟合机器学习模型的问题，

+   模型复杂性和灵活性比可用数据、数据准确性、频率和覆盖范围所证明的更多

+   训练时高精度，但在测试时精度低，表示模型在远离训练数据案例的真实世界使用中的泛化能力较差

## **参数**（统计学）

机器学习概念：总体的一种概括性度量

+   例如，总体均值，总体标准差

我们很少有机会接触到实际的总体参数，通常我们使用可用的样本统计量来推断总体参数

## **参数**（机器学习）

机器学习概念：机器学习模型的训练系数，用于控制对训练数据的拟合

+   模型参数通过优化计算，以最小化训练数据上的误差，例如通过解析解或迭代解，如梯度下降优化

## **参数模型**

机器学习概念：对自然系统的函数形式、形状做出假设的模型。

+   我们获得简单性和只有少数参数的优势

+   对于线性模型，我们只有 $m+1$ 个模型参数

存在一种风险，即我们的模型与自然设置大相径庭，导致模型性能不佳，例如，将线性模型应用于非线性现象。

## **部分相关系数**

多元分析：一种计算 $𝑿$ 和 $𝒀$ 之间相关性的方法，在控制 $𝒁_𝟏,\ldots,𝒁_(𝒎−𝟐)$ 其他特征对 $𝑿$ 和 $𝑌$ 的影响后。注意，我使用 $m-2$ 来考虑 $X$ 和 $Y$ 被移除。

对于 $\rho_(𝑋,𝑌.𝑍_1,…,𝑍_(𝑚−2) )$，

1.  执行线性、最小二乘回归来预测 $𝑿$ 从 $𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐}$。$𝑿$ 通过预测因子进行回归以计算估计值，$𝑿^∗$。

1.  执行线性、最小二乘回归来预测 $𝒀$ 从 $𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐}$。$𝒀$ 通过预测因子进行回归以计算估计值，$𝒀^∗$

1.  在步骤 #1 中计算残差，$𝑿 − 𝑿^∗$，其中 $𝑿^∗=𝒇(𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐})$，线性回归模型

1.  在步骤 #2 中计算残差，$𝒀 − 𝒀^∗$，其中 $𝒀^∗=𝒇(𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐})$，线性回归模型

1.  计算步骤 #3 和 #4 中残差之间的相关系数，$\rho_{𝑿 −𝑿^∗,𝒀 − 𝒀^∗}$

部分相关性的假设，对于 $𝝆_(𝑿,𝒀.𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐})$，

+   $𝑿,𝒀,𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐}$ 存在线性关系，即所有成对关系都是线性的

+   对于任何单变量分布（单变量异常值）和成对关系（双变量异常值）都没有异常值。部分相关性对异常值像常规相关性一样敏感。

+   高斯分布，单变量和成对的双变量分布都是高斯分布。双变量应该是线性相关的，并且同方差。

## **划分聚类**

聚类分析：所有聚类分组分配一次确定，与从 $n$ 个聚类开始并迭代合并聚类到更大聚类的层次聚类方法相反

+   k-means 聚类是划分聚类，而找到解决方案的启发式方法是迭代的，但实际上解决方案是一次性确定的。

+   容易更新，例如，通过修改原型位置并重新计算分组分配

## **多边形去聚类**

数据准备：一种去聚类方法，根据局部采样密度对空间样本分配权重，使得加权统计更有可能代表总体。数据权重分配如下，

+   稠密采样区域中的样本获得较少权重

+   稀疏采样区域中的样本获得更多权重

多边形去聚类按以下方式进行：

1.  使用 Voronoi 多边形划分感兴趣区域。这些多边形是通过相邻数据点之间的相交垂直平分线构建的。多边形通过最近的数据点将感兴趣区域分组

1.  根据相关 Voronoi 多边形的面积按比例分配每个数据的权重

$$ w(\bf{u}_j) = n \cdot \frac{A_j}{\sum_{j=1}^n} $$

其中 $w(\bf{u}_j)$ 是 $j$ 数据的权重。注意，权重的总和是 $n$；因此，$w(\bf{u}_j)$ 是名义权重 1.0，如果数据在感兴趣区域的面积上均匀分布，则表示样本密度。

这里是一些关于多边形去聚类的亮点，

+   多边形去聚类对感兴趣区域的边界敏感；因此，当感兴趣区域扩展或收缩时，分配给感兴趣区域边界附近的数据的权重可能会发生根本性变化

+   多边形去聚类与 Alfred H. Thiessen 在 1911 年开发的用于计算降水平均值的 Theissen 多边形方法相同 []

## **多项式回归**

多项式回归：在线性回归之前将多项式基展开应用于预测特征，

$$ y = \sum_{l=1}^{k} \sum_{j=1}^{m} \beta_{j,l} h_l (X_j) + \beta_0 $$

其中 h 对训练数据进行转换，$𝑖=1,\ldots,n$,

$$ h_1(x_i) = x_i, \quad h_2(x_i) = x_i², \quad h_3(x_i) = x_i³, \quad h_4(x_i) = x_i⁴, \dots, h_k(x_i) = x_i^k $$

最高到指定的 $𝑘$ 阶。

例如，对于单个预测特征，$𝑚 = 1$，最高到 $4^{th}$ 阶，

$$ y = \beta_{1,1} X + \beta_{1,2} X² + \beta_{1,3} X³ + \beta_{1,4} X⁴ + \beta_0 $$

在 $𝑗=1,\dots,𝑚$ 个预测特征上，经过 $𝑙=1,\ldots,𝑘$ 的 $𝑙$ 转换后，我们有相同的线性方程和利用先前讨论的解析解的能力。

+   我们假设在应用基展开后是线性的。

现在模型参数，$\beta_(𝒍,𝒊)$，与初始预测特征转换后的版本相关，$𝒉_𝒍 (𝑿_𝒋)$。

+   我们失去了解释系数的能力，例如，$𝑝𝑒𝑟𝑚𝑒𝑎𝑏𝑖𝑙𝑖𝑡𝑦\(⁴$) 是什么？

+   通常，模型方差显著更高，即可能有不稳定的插值和特别是外推

多项式回归模型假设，

+   *无误差* - 预测特征基函数扩展是无误差的，不是随机变量

+   *方差恒定* - 响应误差在预测值上恒定

+   *线性* - 响应是基特征的组合

+   *多项式* - X 和 Y 之间的关系是多项式

+   *误差独立性* - 响应误差之间相互不相关

+   *无多重共线性* - 基特征扩展中没有任何一个与其他特征线性冗余

## **总体**

概率概念：在感兴趣区域内对感兴趣属性的详尽、有限列表。

+   例如，在储层内每个位置的孔隙率度量的详尽集合

通常，整个总体通常不可访问，我们使用有限的样本来对总体进行推断

## **幂律平均**

特征变换：基于尺度放大的一般形式，将较大体积中的较小尺度度量聚合为表示较大体积的单个值

$$ \overline{x}_p = \left(\frac{1}{n}\sum_{i=1}^n x_i^p \right)^{\frac{1}{p}} $$

+   有助于计算非平行也不垂直于不同渗透率层的有效渗透率

+   流体模拟可以应用于校准（计算幂律平均的适当功率）

## **精度**（分类准确度指标）

朴素贝叶斯：一个分类预测模型的准确度度量，混淆矩阵中每个 $k$ 类别的单个汇总指标。

+   真正阳性与所有阳性的比率，真正阳性 + 假阳性

$$ Precision_k = \frac{ n_{k,\text{true positives}} }{ n_{k,\text{true positives}} + n_{k,\text{false positives}}} = \frac{ n_{k,\text{true positives}} }{ n_{k, \text{all positives}} } $$

## **预测区间**

线性回归：下一次预测的不确定性表示为一个范围，下限和上限，基于一个称为置信水平的指定概率区间。

我们这样传达置信区间，

+   给定预测特征值，$𝑋_1=𝑥_1,\ldots,𝑋_𝑚=𝑥_𝑚$，有 95%的概率（或 20 次中的 19 次）表明真正储层 NTG 在 13%到 17%之间。

我们的预测不确定性，对于预测区间，我们进行积分，

+   模型 $𝐸{\hat{𝑌}|𝑋=𝑥}$ 的不确定性

+   模型的误差，条件分布 $\hat{Y}|X=x$

## **预测，预测统计**

机器学习概念：根据对总体或总体模型的假设来估计下一个样本（样本）

+   例如，给定我们的储层模型，预测下一个井（预钻评估）样本，例如，孔隙率、渗透率、产量等。

## **预测特征**

机器学习概念：预测机器学习模型的输入特征。我们可以将预测机器学习模型概括为，

$$ y = \hat{f}(x_1,\ldots,x_m) + \epsilon $$

其中响应特征是 $y$，预测特征是 $x_1,\ldots,x_m$，而 $\epsilon$ 是模型误差

+   传统统计学使用独立变量的术语

## **预测特征空间**

特征排序：指的是预测特征，不包括响应特征，即，

+   我们需要做出预测的所有预测特征的可能组合

+   可能被称为预测特征空间。

通常，我们在预测特征空间上训练和测试我们机器的预测。

+   该空间通常是超立方体，每个轴代表一个预测特征，从每个预测特征的最小值延伸到最大值

+   可能存在更复杂的预测特征空间形状，例如，我们可以屏蔽或移除数据覆盖较差的子集。

## **原始数据**

机器学习概念：感兴趣特征的数据样本，用于构建模型的目标特征，例如，

+   从岩心和测井数据中测得的孔隙率，用于构建完整的 3D 孔隙率模型。任何孔隙率样本都是原始数据

+   与次级特征相反，例如，如果我们有岩性数据来帮助预测孔隙率，则岩性数据是次级数据

## **主成分分析**

主成分分析：降维的多种方法之一，将数据转换到低维

+   对于给定的特征 $𝑋_1,\dots,𝑋_𝑚$，我们需要 ${m \choose 2}=\frac{𝑚 \cdot (𝑚−1)}{2}$ 个散点图来可视化二维散点图。

+   一旦我们有 4 个或更多变量，理解数据就变得非常困难。

+   回忆维度诅咒，影响推理、建模和可视化。

一种解决方案是找到一个好的低维 $𝑝$ 表示，以表示原始维度 $𝑚$

在降维表示中工作的好处：

1.  数据存储/计算时间

1.  更容易可视化

1.  还要注意多重共线性

主成分分析的主要观点，

+   *正交变换* - 将一组观测值转换为一组线性不相关的变量，称为主成分，这种变换保留了成对距离，即是一种旋转

+   *可用的主成分数 ($k$) - 是 min⁡($𝑛−1,𝑚$)，受变量/特征 $𝑚$ 和数据数量的限制

组件是有序的，

+   第一成分描述了最大的可能方差 / 尽可能解释了最大的可变性

+   下一个成分描述了可能剩余的最大方差

+   直到最大主成分数

基于特征值和特征向量，

计算数据协方差矩阵，特征对的协方差，然后从协方差矩阵中计算特征向量和特征值，

+   特征值是每个成分的解释方差。

+   数据协方差矩阵的特征向量是主成分。

## **概率密度函数** (PDF)

单变量分析：使用概率密度函数 $f(x)$ 的函数 $f(x)$ 表示统计分布，该函数在所有可能的特征值范围 $x$ 上，这些是 PDF 的概念，

+   非负性约束，密度不能为负，

$$ 0.0 \le f(x) $$

+   对于连续特征，密度可能大于 1.0，因为密度是似然度的度量，而不是概率的度量

+   在$x$的范围内对密度进行积分以计算概率，

$$ 0 \le \int_a^b f(x) dx = P(a \le x \le b) \le 1.0 $$

+   概率封闭，PDF 曲线下面积的总和等于 1.0，

$$ \int_{-infty}^{\infty} f(x) dx = 1.0 $$

非参数概率密度函数通过核函数（通常是一个小的高斯分布）计算，该核函数对所有数据进行求和；因此，在计算概率密度函数时存在一个隐含的尺度（平滑度）参数。

+   核函数太大将平滑掉关于单变量分布的重要信息

+   过窄会导致过于嘈杂的 PDF 文件，难以解读。

这类似于选择直方图或归一化直方图的箱大小。

参数概率密度函数是可能的，但需要将模型拟合到数据，步骤包括，

1.  选择一个参数分布，例如高斯分布、对数正态分布等。

1.  根据可用数据计算参数分布的参数，使用如最小二乘法或最大似然法等方法。

## **概率非负性，归一化**

概率概念：概率的基本约束，包括，

1.  有界，$0.0 \le P(A) \le 1.0$

1.  封闭，$P(\Omega) = 1.0$

1.  空集，$P(\emptyset) = 0.0$

## **接受概率** (MCMC)

贝叶斯线性回归：在拒绝采样中作为候选样本被添加到样本中的似然。

+   通过蒙特卡洛模拟进行条件接受，

+   依次从条件分布中进行采样

接受规则是，

+   如果 $𝑃(𝑎𝑐𝑐𝑒𝑝𝑡) \ge 1$，接受 – 接受

+   如果 $𝑃(𝑎𝑐𝑐𝑒𝑝𝑡) \lt 1$，条件接受，抽取 $𝑝 ∼ U[0,1]$，如果 $𝑝 \le 𝛼$ 则接受

## **概率算子**

概率概念：与概率和不确定性问题工作相关的常见概率算子，

*事件并集* - 结果的并集，$A$ 或 $B$ 的概率通过概率加法规则计算，

$$ P(A \cup B) = P(A) + P(B) - P(A,B) $$

*事件交集* - 结果的交集，$A$ 和 $B$ 的概率表示为，

$$ P(A \cap B) = P(A,B) $$

只有在 $A$ 和 $B$ 独立的前提下，才能从 $A$ 和 $B$ 的概率中计算出它，

$$ P(A,B) = P(A) \cdot P(B) $$

如果 $A$ 和 $B$ 之间存在依赖关系，那么我们需要条件概率 $P(A|B)$，而不是边缘概率 $P(A)$，

$$ P(A,B) = P(A|B) \cdot P(B) $$

*互补事件* - 是概率中的 NOT 操作符，如果我们定义 $A$，那么 $A$ 的补集 $A^c$ 就不是 $A$，并且我们有这个结果闭包关系，

$$ P(A) + P(A^c) = 1.0 $$

对于超越单变量问题，可以考虑互补事件，例如考虑这个双变量闭包，

$$ P(A|B) + P(A^c|B) = 1.0 $$

注意，给定的术语必须相同。

*互斥事件* - 不相交或没有共同结果的事件。我们用集合表示法表示这一点，

$$ \{x: x \in A \text{ and } x \in B \} = \emptyset $$

以及 $A$ 和 $B$ 的联合概率为，

$$ P(A \cap B) = P(A,B) = 0 $$

## **概率视角**

概率概念：计算概率的 3 个主要视角：

1.  *长期频率* - 概率作为结果的比率，需要重复观察实验。这是 *频率主义概率* 的基础。

1.  *物理趋势或倾向* - 从对系统了解或建模中得到的概率，例如，我们可以知道抛硬币得到正面的概率，而不需要实验。

1.  *信念程度* - 反映我们对结果的确定性，非常灵活，可以对任何事物分配概率，并随着新信息的更新。这是 *贝叶斯概率* 的基础。

## **原型**（聚类）

聚类分析：用特征空间中的点集表示样本数据。

+   原型通常是实际样本

+   样本数据通常被分配到最近的（欧几里得）距离原型

## **定性特征**

机器学习概念：关于无法直接测量的数量信息，需要解释测量，并用文字（而不是数字）描述，例如，

+   岩石类型 = 砂岩

+   带状分布 = 钴铜矿-黄铜矿-金 高级铜矿带

## **定量特征**

机器学习概念：可以测量并由数字表示的特征，例如，

+   年龄 = 10 Ma（百万年）

+   孔隙率 = 0.134（体积中空隙的分数）

+   饱和度 = 80.5%（体积百分比）

与 *定性特征* 一样，通常需要解释，例如，总孔隙率可以测量，但应通过解释或模型转换为有效孔隙率

## **$r²$**（也称为确定系数）

线性回归：在线性回归中模型解释的方差比例

这只适用于线性模型，其中：

$$ \sigma²_{tot} = \sigma²_{reg} + \sigma²_{res} $$

其中 $\sigma²_{tot}$ 是响应特征训练的方差，$y_i$，$\sigma²_{reg}$ 是模型预测的方差，$\hat{y}_i$，和 $\sigma²_{res}$ 是误差的方差，$\Delta y_i$。

+   对于线性回归，$r² = (\rho_{x,y})²$

对于非线性模型，这不太可能成立，那么 $\frac{\sigma²_{𝑟𝑒𝑔}}{\sigma²_{𝑡𝑜𝑡}}$ 可能超过 $[0,1]$，对于我们的非线性回归模型，我们将使用更稳健的度量，例如均方误差（MSE）

## **随机森林**

袋装树和随机森林：基于标准袋装方法的集成预测模型，具体来说，

+   使用决策树

+   通过限制每个分割只考虑 $𝑚$ 个可用预测因子中的 $p$ 个随机子集来多样化单个树

有各种方法可以从 $m$ 个可用特征中计算 $p$，

$$ p = \sqrt{m} $$

是常见的。注意，如果 $p = m$，则随机森林是树袋法。

更多关于集成模型多样化的益处的评论，

+   通过集成估计降低模型方差，如标准误差所表示的，

$$ \sigma_{\overline{x}}² = \frac{\sigma_{s}²}{n} $$

假设样本是不相关的。树袋法的一个问题是，集成中的树可能高度相关。

+   当存在一个占主导地位的预测特征时，这种情况会发生，因为它将始终应用于顶部分割（s），结果是集成中的所有树都非常相似（即相关）

+   对于高度相关的树，集成中的模型方差减少显著较少

+   这迫使集成中的每棵树以不同、去相关的模式进化

## **实现**

机器学习概念：来自一个**随机变量**的结果或来自一个**随机函数**的联合结果。

+   来自随机变量 $X$ 的结果（或来自随机函数的联合结果集）

+   用小写表示，例如，$x$

+   对于空间设置，通常包括一个位置向量 $\bf{u}$，以描述位置，例如，$x(\bf{u})$，作为 $X(\bf{u})$

+   来自模拟的结果，例如，蒙特卡洛模拟，顺序高斯模拟，从 RV（RF）中（共同）采样的方法

+   通常，我们假设所有实现都是等概率的，即具有相同的出现概率

## **实现**（不确定性）

机器学习概念：通过保持输入参数和模型选择不变，仅改变随机数种子，通过随机模拟计算出的多个空间、地下模型。

+   这些模型表示空间不确定性

+   例如，保持孔隙率平均值不变，并观察远离井的多次实现中孔隙率的变化

## **学习一些编码的原因**

机器学习工作流程构建与编码：Pyrcz 教授解释了所有科学家和工程师学习一些编码的原因，

+   *透明度* – 没有编译器会接受挥手！编码迫使你的逻辑被揭露，以便任何其他科学家或工程师进行审查。

+   *可重复性* – 运行它并得到答案，将其交给同行，他们运行它并得到相同的答案。这是科学方法的一个原则。

+   *量化* – 程序需要数字，并推动我们从定性到定量。给程序喂食，发现新的看待世界的方式。

+   *开源* – 利用世界上的智慧。查看包、片段，并惊叹于伟大思想者免费分享的内容。

+   *打破壁垒* – 不要把它扔过栅栏。和开发者坐在一起，分享更多你的专业知识，以获得更好的部署产品。

+   *部署* – 与他人分享你的代码，扩大你的影响力。性能指标或利他主义，你的好工作使许多人受益。

+   *效率* – 最小化工作中无聊的部分。构建一套脚本来自动化常见任务，并花更多时间进行科学和工程！

+   *永远都有时间再做一次!* – 你只做了一次吗？脚本和自动化工作流程可能需要 2-4 倍的时间。通常，这是值得的。

像我们一样 – 这将改变你。用户感到受限，程序员真正利用了他们的应用程序和硬件的力量。

## **召回率**（分类准确率指标）

朴素贝叶斯：一个分类预测模型的准确性度量，混淆矩阵中每个 $k$ 类别的单一汇总指标。

+   测试数据集中该类别所有案例中真实正例与所有案例的比率

$$ Recall_k = \frac{ n_{k, \text{true positives}} }{n_k} $$

## **递归特征消除**

特征排名：该方法通过递归移除特征并使用剩余特征构建模型来工作。

+   使用所有特征构建模型，计算特征排名指标，例如系数或特征重要性，具体取决于建模方法中哪些可用

+   移除特征重要性最低的特征并重新构建模型

重复此过程，直到只剩下一个特征

任何预测模型都可以使用，

+   该方法为所有特征分配 $1,\ldots,𝑚$ 的排名，按移除的逆序，即最后一个剩余的特征是最重要的，第一个移除的是最不重要的

## **储层建模工作流程**

机器学习工作流程构建与编码：以下是一个常见的地质统计学储层建模工作流程：

1.  整合所有可用信息，构建多个地下场景和实现，以采样不确定性空间

1.  将所有模型应用于传递函数以采样决策准则

1.  组装决策准则的分布

1.  考虑此不确定性模型，做出最佳储层开发决策

## **响应特征**

机器学习概念：预测机器学习模型的输出特征。我们可以将预测机器学习模型概括为，

$$ y = \hat{f}(x_1,\ldots,x_m) + \epsilon $$

其中响应特征是 $y$，预测特征是 $x_1,\ldots,x_m$，而 $\epsilon$ 是模型误差

+   传统统计学使用术语因变量

## **岭回归**（Tikhonov 正则化）

岭回归：一个线性、参数化的预测模型，

$$ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 $$

对于 L2 范数损失函数，模型参数 $b_1,\ldots,b_m,b_0$ 的解析解是可用的，误差是总和并平方，已知为最小二乘法。

+   我们最小化一个包含误差、训练数据上的残差平方和（RSS）和正则化项的损失函数：

$$ \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)² + \lambda \sum_{\alpha = 1}^m b_{\alpha}² $$

其中 $y_i$ 是实际响应特征值，$\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0$ 是模型预测，在 $\alpha = 1,\ldots,n$ 的训练数据上，而 $\lambda \sum_{\alpha = 1}^m b_{\alpha}²$ 是收缩惩罚项。

在岭回归中，我们向最小化中添加一个超参数 $\lambda$，具有收缩惩罚项 $\sum_{j=1}^m b_{\alpha}²$。

因此，岭回归训练集成了两个通常相互竞争的目标，以找到模型参数，

+   找到使训练数据误差最小的模型参数

+   将斜率参数最小化到零

注意：lambda 不包括截距，$b_0$。

$\lambda$ 是一个超参数，它控制模型的拟合程度，可能与模型的偏差-方差权衡有关。

+   当 $\lambda \rightarrow 0$ 时，解趋近于线性回归，没有偏差（相对于线性模型拟合），但模型方差可能更高

+   随着 $\lambda$ 的增加，模型方差减小，模型偏差增加，模型变得简单

+   当 $\lambda \rightarrow \infty$ 时，模型参数 $b_1,\ldots,b_m$ 收缩到 0.0，模型预测趋近于训练数据响应特征均值

## **样本**

机器学习概念：已测量的值和位置的集合

+   例如，来自储层中井的 1,000 个孔隙度测量值

+   或者在一个 1000 x 1000 2D 网格上对感兴趣的储层单元进行 1,000,000 个声阻抗测量

## **场景**（不确定性）

机器学习概念：通过改变输入参数或其他建模选择进行随机模拟，计算多个空间、地下模型，以表示模型参数和模型选择的推断不确定性

+   例如，模型三个孔隙率输入分布，孔隙率平均值低、中、高，并改变输入分布来计算新的地下模型

## **次级数据**

机器学习概念：为另一个特征的数据样本，而不是感兴趣的特征，为构建模型的目标特征，但用于提高目标特征的预测

+   需要一个主次级数据之间关系模型

例如，空间中的样本，

+   声阻抗（次级数据）用于支持孔隙率模型的计算，这是感兴趣的特征

+   孔隙率（次级数据）用于支持渗透率模型的计算，这是感兴趣的特征

## **地震数据**

机器学习概念：遥感间接测量，反射地震使用声源（s）和接收器（地震检波器）来绘制高覆盖率和一般低分辨率的声波反射图。一些更详细的说明，

+   地震反射（振幅）数据被反演为岩石属性，例如，声阻抗，与井声波测井一致并在位置上锚定

+   提供框架、边界表面，以及关于储层属性（例如，孔隙率和岩性）的软信息

## **Shapley 值**

特征排序：基于学习每个特征对预测的贡献，进行基于模型、局部（针对单个预测）和全局（针对一系列预测）的特征重要性

需要一种可解释的机器学习方法来支持复杂的模型，但这些方法通常具有较低的可解释性

提高模型可解释性的两种选择，

1.  减少模型的复杂性，但可能会降低模型精度

1.  开发改进的、无差别的（适用于任何模型）模型诊断，即 Shapley 值

Shapley 值是合作博弈论方法，

+   用于根据边际贡献的汇总来分配资源给玩家，即分配给玩家的付款

+   计算每个预测特征对推动响应预测远离响应平均值贡献的大小

+   边际贡献和 Shapley 值以响应特征的单位表示

+   在响应特征的单位中

## **辛普森悖论**

多元分析：当组合（或分离）组时，数据趋势反转或消失。在分组数据的相关分析中经常观察到，例如，

+   每组都有负相关，但整体有正相关

## **软数据**

机器学习概念：具有高度不确定性的数据，因此数据不确定性必须集成到模型中

+   例如，从声阻抗校准的局部孔隙率概率密度函数

软数据集成需要像 *指示克里金*、*指示模拟* 和 *p 场模拟* 或使用假设硬数据的标准模拟方法随机化数据的流程，如 *顺序高斯模拟*。

+   软数据集成是一个高级主题，也是当前研究的热点，但通常使用标准的、地下建模软件包来完成

## **空间抽样**（有偏差）

数据准备：样本以使样本统计量不代表总体参数。例如，

+   样本均值与总体均值不同

+   样本方差与总体方差不同

当然，人口参数是不可访问的，因此我们无法直接计算抽样偏差，即样本统计量与人口参数之间的差异。我们可以用来检查抽样偏差的方法，

+   评估样本的优先抽样、聚集、过滤等

+   应用 *去聚集* 并检查结果在汇总统计中的重大变化，这是使用去聚集进行诊断

## **空间抽样**（聚集）

数据准备：优先选择位置的空间样本，即聚集，导致统计偏差，

+   通常，空间样本在具有更高价值样本的位置聚集，例如，高孔隙率和渗透率，优质页岩用于非常规储层，低声阻抗指示更高的孔隙率等。

当然，人口参数是不可访问的，因此我们无法直接计算抽样偏差，即样本统计量与人口参数之间的差异。我们可以用来检查抽样偏差的方法，

+   评估样本的优先抽样、聚集、过滤等

+   应用 *去聚集* 并检查结果在汇总统计中的重大变化，这是使用去聚集进行诊断

## **空间抽样**（常见做法）

数据准备：选择样本位置以，

*减少不确定性* - 通过回答问题，例如，

+   污染羽流延伸多远？ – 样本外围

+   断层在哪里？ – 根据地震解释进行钻探

+   最高矿物品位是多少？ – 样本最佳部分

+   储层延伸多远？ – 偏移钻探

*直接最大化净现值* - 在收集信息时，例如，

+   最大化生产率

+   最大化提取的矿物吨数

换句话说，我们的样本通常是双用途的，例如，为勘探和评估信息而钻探的井随后被用于生产。

## **空间抽样**（代表性）

数据准备：如果我们是为了代表性进行采样，即样本集和结果样本统计量代表总体，根据采样理论，我们有 2 种选择：

*随机采样* - 从总体中每个潜在的样本被采样的可能性相等。这包括，

+   选择特定位置不会影响后续位置的选择。

+   假设总体大小远大于样本大小；因此，由于不放回采样（只能采样一个位置的约束），样本之间不存在显著的关联。注意，由于地下采样密集的总体，这通常不是问题

*规则采样* - 在等间隔的空间或时间进行采样。虽然随机采样更受欢迎，但只要满足以下条件，规则采样就足够稳健，

+   规则采样间隔与数据中的自然周期性不一致，例如，顶峰被系统地采样，导致样本统计量偏高

## **谱聚类**

谱聚类：一种利用表示数据成对关系的矩阵的谱、特征值和特征向量的划分聚类方法。

+   从数据样本的成对关系中通过图拉普拉斯矩阵进行降维

+   特征值和特征向量等同于通过线性、正交特征投影和旋转进行的主成分分析降维，以最好地描述方差

谱聚类的优势，

+   能够编码成对关系，整合专家知识。

+   特征值提供了关于聚类数量的有用信息，基于形成 k 个聚类所需的“切割”程度

+   样本数据成对关系的低维表示

+   结果的特征值和特征向量可以解释，特征值描述了每个组数之间的连接量，而特征向量被分组形成聚类

## **标准化**

特征变换：一种分布缩放，可以将其视为平移、拉伸或压缩单变量分布（例如，*直方图*）到均值为 0.0 和方差为 1.0。

$$ y_i = \frac{1}{\sigma_x}(x_i - \overline{x}), \quad \forall \quad i, \ldots, n $$

$\overline{x}$ 和 $\sigma_x$ 分别是原始均值和方差。

+   这是对原始属性分布的平移和拉伸/压缩

+   假设没有形状变化，保持秩不变

## **基于随机梯度的优化**

LASSO 回归：通过迭代最小化损失函数来求解模型参数的方法。通过使用批量，将随机性和提高计算效率添加到梯度下降中，

+   批量是具有指定大小 $n_{batch}$ 的训练数据的随机子集

+   导致损失函数梯度的随机近似，计算速度更快

+   批量在梯度下降中降低了准确性，但加快了计算速度，可以执行更多步骤，通常比梯度下降更快

+   为了提高梯度估计的准确性，增加 $𝑛_{𝑏𝑎𝑡𝑐ℎ}$，为了加快步骤，减少 $𝑛_{𝑏𝑎𝑡𝑐ℎ}$

Robbins-Siegmund (1971) 定理 - 对于凸损失函数收敛到全局最小值，对于非凸损失函数收敛到全局或局部最小值。

步骤包括，

1.  从随机的模型参数开始

1.  选择一个随机子集的训练数据，$n_{batch}$

1.  计算模型参数在随机批量上的损失函数和损失函数梯度

$$ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) = \frac{L(y_{\alpha}, F(X_{\alpha}, b_1 - \epsilon)) - L(y_{\alpha}, F(X_{\alpha}, b_1 + \epsilon))}{2\epsilon} $$

1.  通过沿斜坡/梯度下降来更新参数估计

$$ \hat{b}_{1,t+1} = \hat{b}_{1,t} - r \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) $$

其中 $r$ 是学习率/步长，$\hat{b}(1,𝑡)$ 是当前模型参数估计，$\hat{b}(1,𝑡+1)$ 是更新的参数估计。

## **随机模型**

机器学习概念：不确定的系统或过程，由多个模型、*实现*和受统计约束的*情景*表示，

+   例如，像地质统计模拟模型这样的数据驱动模型，它们整合了不确定性

优点：

+   速度

+   不确定性评估

+   报告显著性、置信/预测区间

+   尊重许多类型的数据

+   数据驱动方法

缺点：

+   使用的物理限制

+   统计模型假设/简化

对于随机模型的替代方案，请参阅*确定性模型*。

## **统计学**（实践）

机器学习概念：收集、组织、解释数据以及得出结论和做出决策的理论和实践。

## **统计学**（测量）

机器学习概念：样本的汇总度量，例如，

+   样本均值 - $\overline{x}$

+   样本标准差 - $s$,

我们使用统计作为模型参数的估计，这些估计总结了总体（*推断*）

## **统计分布**

单变量分析：对于一个特征，描述其在可能值范围内的发生概率。我们用以下方式表示单变量统计分布，

+   *直方图*

+   *归一化直方图*

+   *概率密度函数* (PDF)

+   *累积分布函数* (CDF)

我们从统计分布中学到了什么？例如，

+   最小值和最大值是什么？

+   我们是否有大量的低值？

+   我们是否有大量的高值？

+   我们是否有异常值，以及任何其他不合理且需要解释的值？

## **支持向量** (支持向量机)

支持向量机：在边缘内的训练数据或被错误分类，并更新支持向量机分类模型。

+   在支持向量机模型中，训练数据很好地位于正确区域，不是支持向量，对模型没有影响

## **支持向量机**

支持向量机：一种预测性的二分类机器学习方法，当组别分离不良时是一种很好的分类方法。

+   将原始预测特征投影到高维空间，然后应用线性、平面或超平面，

$$ 𝑓(𝑥) = 𝑥^𝑇 \beta +\beta_0 $$

其中 $\beta$ 是一个向量，与 $\beta$ 一起是超平面模型参数，而 $x$ 是预测特征矩阵，所有这些都在高维空间中。

$$ 𝐺(𝑥)=\text{𝑠𝑖𝑔𝑛}\left( 𝑓(𝑥) \right) $$

$𝑓(𝑥)$ 与决策边界的符号距离成正比，而 $𝐺(𝑥)$ 是决策边界的侧面，$-$ 一侧和 $+$ 另一侧，$f(x) = 0$ 在决策边界上。

我们通过以下方式表示约束，每个类别的所有数据都必须位于边界的正确一侧，

$$ y_i \left( x_i^T \beta + \beta_0 \right) \geq 0 $$

其中如果类别 $y_i$ 是 -1 或 1，则此条件成立。我们需要一个允许某些错误分类的模型，

$$ y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i $$

我们引入了边缘的概念 $𝑀$ 和从边缘的距离，误差为 $\xi_i$。现在我们可以将我们的损失函数表示为，

$$ \underset{\beta, \beta_0}{\text{min}} \left( \frac{1}{2M²} + C \sum_{i=1}^N \xi_i \right) $$

受限于，$\xi_i \geq 0, \quad y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i$。

这是在高维空间中的支持向量机损失函数，其中 𝛽,𝛽_0 是多线性模型参数。

通过找到最大化边缘 $M$ 并最小化误差 $\sum_{i=1}^N \xi_i$ 的模型参数来训练支持向量机。

+   $𝑪$ 超参数加权误差总和 $xi_𝑖$，更高的 $𝐶$ 将导致较小的边缘 $M$，并可能导致过拟合

+   较小的边缘，使用较少的数据来约束边界，称为支持向量

+   在边界正确一侧的训练数据没有影响

这里是支持向量机的几个关键方面，

+   被称为支持向量机，而不是机器，因为使用新的核可以得到一个新的机器

+   有许多核可用，包括多项式和径向基函数

主要超参数是 $C$，其成本为

超参数与核的选择有关，例如，

+   *多项式* - 多项式阶数

+   *径向基函数* - $\gamma$ 与训练数据的距离影响成反比

## **表格数据**

机器学习工作流程构建和编码：具有每行代表每个样本和每列代表每个特征的表格数据

由于 Pandas 的 DataFrame 是处理表格数据的便捷类，

+   方便的数据结构用于存储、访问、操作表格数据

+   内置方法用于从各种文件类型、Python 类甚至直接从 Excel 加载数据

+   内置方法用于计算汇总统计和可视化数据

+   内置方法用于数据查询、排序、数据筛选

+   内置方法用于数据处理、清理、重新格式化

+   内置属性用于存储有关数据的信息，例如大小、空值数量和空值

## **训练和测试分割**

机器学习概念：在预测模型训练之前，为模型交叉验证保留一部分数据作为测试数据。

+   训练数据用于训练模型参数，而保留的测试数据用于调整模型超参数

+   超参数调整是选择最小化保留测试数据上误差范数的超参数组合

最常见的方法是随机选择，这可能不是公平的测试，

+   测试难度的范围与模型在现实世界中的应用相似

+   太简单了 - 测试案例与训练案例相同或几乎相同，随机抽样通常太简单

+   太难了 - 测试案例与训练案例非常不同，模型预计将严重外推

如 k 折交叉验证等替代方法提供了对所有可用数据进行测试的机会，但需要，

+   在超参数组合上训练 k 个预测机器学习模型

+   对 k 个模型的测试错误进行聚合，以选择最佳超参数，超参数调整

此外，还有包括训练、验证和测试数据子集的替代工作流程

## **转移函数**（储层建模工作流程）

机器学习概念：应用于空间、地下模型实现和场景的计算，以计算决策标准，这是一个用于支持决策、表示价值和健康、环境和安全的指标。一些示例转移函数包括，

+   *运输和生物衰减* - 数值模拟以模拟泵和治处理操作期间随时间推移的土壤污染物浓度

+   *体积计算* - 用于计算原地总油量以计算原地资源

+   *异质性指标* - 作为采收率指标，用于从资源估计储量

+   *流动模拟* - 用于计划井的预钻生产预测

+   *惠特尔的矿坑优化* - 用于计算矿产资源及其最终矿坑壳体

## **不确定性建模**

机器学习概念：在样本时间对位置或多个位置的可能的特征值范围的计算。一些考虑因素，

+   对我们的样本和模型预测精度的限制进行量化

+   不确定性是一个模型，没有客观的不确定性

+   不确定性是由我们的无知引起的

+   不确定性是由稀疏采样、测量误差和偏差以及异质性引起的

我们通过多个模型、场景和实现来表示不确定性：

+   场景 - 通过改变输入参数或其他建模选择，通过随机模拟计算多个空间、地下模型来表示由于模型参数和模型选择的推断引起的不确定性

+   实现 - 通过保持输入参数和模型选择不变，仅改变随机数种子来计算多个空间、地下模型

## **欠拟合模型**

机器学习概念：一个过于简单、复杂性和灵活性太低，无法拟合自然现象的机器学习模型，导致模型偏差非常高。

+   欠拟合模型通常接近响应特征的全球均值

+   过拟合模型在训练和测试数据上具有高误差

+   增加的复杂性通常会在训练和测试数据集上降低错误

+   在模型复杂度下降的训练和测试错误区域

过拟合机器学习模型的问题，

+   在给定的数据、数据精度、频率和覆盖范围下，更多的模型复杂性和灵活性是不够的

+   训练和测试中的低精度表示远离训练数据案例的现实世界使用，表明模型泛化能力差

## **事件的并集**（概率）

概率概念：结果的并集，$A$ 或 $B$ 的概率通过概率加法规则计算，

$$ P(A \cup B) = P(A) + P(B) - P(A,B) $$

## **单变量参数**

单变量分析：基于对总体中一个特征测量的汇总度量

## **单变量统计**

单变量分析：基于对样本中一个特征测量的汇总度量

## **无监督学习**

聚类分析：从未标记数据中学习数据模式。

+   没有响应特征 $𝑌$，而是只有预测特征 $𝑋_1,ldots,𝑋_𝑚$

+   机器通过模仿数据的紧凑表示来学习

+   将模式捕捉为特征投影、分组分配、神经网络潜在特征等。

+   专注于对总体、自然系统的推断，而不是响应特征的预测

在本课程中，我们使用推断和预测机器学习的术语，所有涵盖的推断机器学习方法都是无监督的。

## **变量**（也称为特征）

机器学习概念：在研究中测量或观察到的任何属性，例如，

+   孔隙率、渗透率、矿物浓度、饱和度、污染物浓度

+   在数据挖掘/机器学习中，这被称为 *特征*

+   测量通常需要大量的分析、解释等。

## **方差膨胀因子** (VIF)

特征排序：衡量预测特征（$X_i$）与所有其他预测特征（$X_j, \forall j \ne i$）之间的线性多重共线性。

首先，我们针对所有其他预测特征计算给定预测特征的线性回归。

$$ X_i = \sum_{j, j \ne i}^m X_j + \epsilon $$

从这个模型中，我们确定确定系数 $R²$，称为方差解释。

然后我们计算方差膨胀因子（VIF）如下：

$$ VIF = \frac{1}{1 - R²} $$

## **体积-方差关系**

特征变换：当 *体积支撑*（尺度）增加时，方差减少

预测体积-方差关系是处理数据和多尺度模型的关键。一些一般观察和假设，

+   当体积支撑、尺度变化时，均值不会改变。只有方差会改变

+   可能会有形状变化（我们在这里不会处理这一点）。最佳实践是经验性地检查形状变化。通常假设没有形状变化（*仿射校正*）或使用形状变化模型（间接对数正态校正）。

+   分布中的方差减少与空间连续性的范围成反比。对于较短的空间连续性范围，方差减少得更快（在较小的体积增加上）。

在常见的尺度变化中，这种影响可能是显著的；因此，忽略体积-方差关系是不合适的，

+   我们不进行这种尺度放大，体积支撑的变化完美无缺，这就是为什么它仍然被称为缺失尺度。我们很少拥有足够的数据来严格地模拟这一点

+   我们需要一个模型来预测这种方差随体积支撑变化的变化

存在一些体积支撑、尺度模型的变化，

*经验性* - 建立一个小尺度、高分辨率模型，并对其进行数值放大。例如，计算渗透率的高分辨率模型，应用流动模拟来计算 $v$ 尺度块上的有效渗透率

*幂律平均* - 有一种称为幂律平均的灵活方法。

$$ z_V = \left[ \frac{1}{n} \sum z_v^{\omega} \right] ^{\frac{1}{\omega}} $$

其中 $\omega$ 是平均的幂。例如：

+   $\omega = 1$ 是常规线性平均

+   $\omega = -1$ 是谐波平均

+   $\omega = 0$ 是几何平均（这已在 $\omega \rightarrow 0$ 的极限中得到证明）

如何计算 $\omega$？

+   对于某些情况，我们从理论中知道正确的 $\omega$ 值，例如，对于与床层正交的流动，我们选择 $\omega = -1.0$ 来放大渗透率

+   流动模拟可以应用于数值放大渗透率，然后反向计算校准的 $\omega$

*模型* - 直接调整统计量以改变尺度。例如，在假设线性平均和静态变差函数及方差的情况下：

$$ f = 1 - \frac{\overline{\gamma}(v,v)}{\sigma²} $$

其中 $f$ 是方差减少因子，

$$ f = \frac{D²(v,V)}{D²(\cdot,V)} = \frac{D²(v,V)}{\sigma²} $$

换句话说，$f$ 是在 $v$ 尺度上的方差与基于原始数据点支持尺度的原始数据方差之比

+   变差函数模型

+   数据的尺度，$\cdot$ 和 $v$ 的尺度

## **维恩图**

概率概念：一种图表，用于传达概率的视觉工具。我们从维恩图中能学到什么？

+   区域的大小 $\propto$ 发生的概率

+   $\Omega$ 的比例，所有可能的结果都由一个方框表示，即 $1.0$ 的概率

+   重叠 $\propto$ 联合发生的概率

维恩图是可视化边缘、联合和条件概率的一个优秀工具。

## **测井数据**

机器学习概念：作为一种更便宜的采样井的方法，它不会中断钻井作业，测井数据在井中非常常见。通常所有井都有各种测井数据可用。例如，

+   在试验垂直井中的伽马射线用于评估页岩的位置和质量，以定位（着陆）水平井

+   中子孔隙率用于评估高孔隙率储层砂岩的位置

+   在钻孔中的伽马射线用于绘制钍矿化

岩心测井数据对于支持地下资源解释至关重要。一旦由岩心数据锚定，它们提供了建模整个储层概念/框架进行预测所必需的覆盖范围和分辨率，例如，

+   与测井数据同位校准的岩心数据用于绘制关键的地层层位，包括储层和封堵单元

+   测井数据应用于深度校正由于在感兴趣体积内岩石速度的不确定性而位置不精确的地震反演特征

## **弱学习器**

梯度提升：预测模型的表现仅略好于随机

$$ 𝑌 = \hat{f}_𝑘(𝑋_1,\ldots,𝑋_𝑚) $$

其中 $\hat{f}_𝑘$ 是第 $𝑘$ 个弱学习器，$𝑋_1,\ldots,𝑋_𝑚$ 是预测特征，$\hat{Y}$ 是响应特征的预测。

弱预测器这个术语经常被使用，并且具体来说，对于分类模型的情况，术语是弱分类器。

## **测井数据，图像测井**

机器学习概念：*测井数据*的一种特殊情况，其中测井数据在井孔内的各种方位角间隔内重复，从而产生一个二维（展开）图像，而不是沿井孔的 1D 线。例如，全孔径地层微成像仪（FMI）具有：

+   以 80% 的钻孔覆盖率

+   0.2 英寸（0.5 厘米）的垂直和水平分辨率

+   30 英寸（79 厘米）的探测深度

可以应用于观察岩性变化、层倾角和沉积结构。

## 评论

这是对地统计学的基本介绍。如果您想了解更多关于这些基本概念，我建议您阅读我的教科书《地统计学储层建模》中的介绍、建模原理和建模先决条件章节。[Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446){cite}`pyrcz2014’。

希望这有所帮助，

*迈克尔*

## 作者：

迈克尔·皮尔奇（Michael Pyrcz），教授，德克萨斯大学奥斯汀分校 *新型数据分析、地统计学和机器学习地下解决方案*

在地下咨询、研究和开发领域拥有超过 17 年经验，迈克尔（Michael）因对教学的热情和对增强工程师和地球科学家在地下资源开发中影响力的热情，重返学术界。

更多关于迈克尔的信息，请查看以下链接：

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [网站](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [地统计学书籍](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python 中应用地统计学电子书](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python 中应用机器学习电子书](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## 想一起工作吗？

希望这些内容对那些想了解更多关于地下建模、数据分析和机器学习的人有所帮助。学生和在职专业人士都欢迎参加。

+   想邀请我到贵公司进行培训、辅导、项目审查、工作流程设计和/或咨询吗？我很乐意拜访并与您合作！

+   感兴趣合作、支持我的研究生研究或我的地下数据分析和机器学习联盟（共同负责人包括 Foster 教授、Torres-Verdin 教授和 van Oort 教授）吗？我的研究将数据分析、随机建模和机器学习理论与实践相结合，以开发新的方法和工作流程，增加价值。我们正在解决具有挑战性的地下问题！

+   您可以通过 mpyrcz@austin.utexas.edu 联系我。

我总是乐于讨论，

*迈克尔*

迈克尔·皮尔奇，博士，P.Eng. 教授，德克萨斯大学奥斯汀分校 Cockrell 工程学院和 Jackson 地球科学学院

更多资源可在以下链接找到：[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [网站](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [地统计学书籍](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python 中应用地统计学电子书](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python 中应用机器学习电子书](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
