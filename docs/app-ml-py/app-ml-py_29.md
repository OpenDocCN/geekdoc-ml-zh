# æ”¯æŒå‘é‡æœº

> åŸæ–‡ï¼š[`geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_support_vector_machines.html`](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_support_vector_machines.html)

è¿ˆå…‹å°”Â·JÂ·çš®å°”å¥‡ï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

ç”µå­ä¹¦â€œPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šåŠ¨æ‰‹æŒ‡å—åŠä»£ç â€çš„ç« èŠ‚ã€‚

è¯·å°†æ­¤ç”µå­ä¹¦å¼•ç”¨å¦‚ä¸‹ï¼š

Pyrcz, M. J., 2024, ã€ŠPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šåŠ¨æ‰‹æŒ‡å—åŠä»£ç ã€‹ã€‚GitHub ä»“åº“ã€‚Zenodoã€‚DOIï¼š10.5281/zenodo.15169138 ![DOI](https://doi.org/10.5281/zenodo.15169138)

æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠæ›´å¤šå†…å®¹åœ¨æ­¤å¤„å¯ç”¨ï¼š

è¯·å°† MachineLearningDemos GitHub ä»“åº“å¼•ç”¨å¦‚ä¸‹ï¼š

Pyrcz, M.J., 2024, MachineLearningDemosï¼šPython æœºå™¨å­¦ä¹ æ¼”ç¤ºå·¥ä½œæµç¨‹ä»“åº“ï¼ˆ0.0.1ï¼‰ã€‚Zenodoã€‚DOIï¼š10.5281/zenodo.13835312 ![DOI](https://zenodo.org/doi/10.5281/zenodo.13835312)

ä½œè€…ï¼šè¿ˆå…‹å°”Â·JÂ·çš®å°”å¥‡

Â© ç‰ˆæƒæ‰€æœ‰ 2024ã€‚

æœ¬ç« æ˜¯å…³äº/æ¼”ç¤º **æ”¯æŒå‘é‡æœº** çš„æ•™ç¨‹ã€‚

**YouTube è®²åº§**ï¼šæŸ¥çœ‹ä»¥ä¸‹è®²åº§ï¼š

+   [æœºå™¨å­¦ä¹ ç®€ä»‹](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)

+   [å¤šé¡¹å¼å›å½’](https://youtu.be/z19Hs2HfO88?si=U2eAMJcMXRMwHG0C)

+   [æ”¯æŒå‘é‡æœº](https://youtu.be/UpN6TLMJiGg?si=-aevKAWNqk_sXxYO)

è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘ YouTube ä¸Šçš„ [æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI) çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«æœ‰è‰¯å¥½æ–‡æ¡£è®°å½•çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºè·å–ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœä½ æƒ³çŸ¥é“æˆ‘çš„åŠ¨æœºï¼Œè¯·æŸ¥çœ‹ [è¿ˆå…‹å°”çš„æ•…äº‹](https://michaelpyrcz.com/my-story)ã€‚

## æ”¯æŒå‘é‡æœºçš„åŠ¨æœº

ä¸€ç§äºŒå…ƒåˆ†ç±»æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå½“ç»„é—´åˆ†ç¦»ä¸è‰¯æ—¶æ˜¯ä¸€ä¸ªå¥½çš„åˆ†ç±»æ–¹æ³•ã€‚

+   å°†åŸå§‹é¢„æµ‹ç‰¹å¾æŠ•å½±åˆ°æ›´é«˜ç»´ç©ºé—´ï¼Œç„¶ååº”ç”¨çº¿æ€§ã€å¹³é¢æˆ–è¶…å¹³é¢ï¼Œ

$$ ğ‘“(ğ‘¥) = ğ‘¥^ğ‘‡ \beta +\beta_0 $$

å…¶ä¸­ $\beta$ æ˜¯ä¸€ä¸ªå‘é‡ï¼Œä¸ $\beta$ ä¸€èµ·æ˜¯è¶…å¹³é¢æ¨¡å‹å‚æ•°ï¼Œè€Œ $x$ æ˜¯é¢„æµ‹ç‰¹å¾çŸ©é˜µï¼Œæ‰€æœ‰è¿™äº›éƒ½åœ¨é«˜ç»´ç©ºé—´ä¸­ã€‚

+   $ğ‘“(ğ‘¥)$ ä¸å†³ç­–è¾¹ç•Œçš„ç¬¦å·è·ç¦»æˆæ­£æ¯”ï¼Œè€Œ $ğº(ğ‘¥)$ æ˜¯å†³ç­–è¾¹ç•Œçš„ä¸¤ä¾§ï¼Œ$-$ ä¸€ä¾§å’Œ $+$ å¦ä¸€ä¾§ï¼Œ$f(x) = 0$ åœ¨å†³ç­–è¾¹ç•Œä¸Šï¼Œ

$$ ğº(ğ‘¥)=\text{ğ‘ ğ‘–ğ‘”ğ‘›}\left( ğ‘“(ğ‘¥) \right) $$

æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼è¡¨ç¤ºçº¦æŸï¼Œæ¯ä¸ªç±»åˆ«çš„æ‰€æœ‰æ•°æ®éƒ½å¿…é¡»ä½äºè¾¹ç•Œçš„æ­£ç¡®ä¸€ä¾§ï¼Œ

$$ y_i \left( x_i^T \beta + \beta_0 \right) \geq 0 $$

å…¶ä¸­ï¼Œå¦‚æœç±»åˆ« $y_i$ æ˜¯ -1 æˆ– 1ï¼Œåˆ™æˆç«‹ã€‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå…è®¸æŸäº›è¯¯åˆ†ç±»çš„æ¨¡å‹ï¼Œ

$$ y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i $$

æˆ‘ä»¬å¼•å…¥äº†é—´éš” $ğ‘€$ å’Œä»é—´éš”çš„è·ç¦»çš„æ¦‚å¿µï¼Œå³è¯¯å·® $\xi_i$ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬çš„æŸå¤±å‡½æ•°è¡¨ç¤ºä¸ºï¼Œ

$$ \underset{\beta, \beta_0}{\text{min}} \left( \frac{1}{2MÂ²} + C \sum_{i=1}^N \xi_i \right) $$

æ¡ä»¶æ˜¯ï¼Œ$\xi_i \geq 0, \quad y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i$.

è¿™æ˜¯æ›´é«˜ç»´ç©ºé—´ä¸­çš„æ”¯æŒå‘é‡æœºæŸå¤±å‡½æ•°ï¼Œå…¶ä¸­ ğ›½,ğ›½_0 æ˜¯å¤šçº¿æ€§æ¨¡å‹å‚æ•°ã€‚

é€šè¿‡å¯»æ‰¾æœ€å¤§åŒ–é—´éš” $M$ çš„å¹³é¢æ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶æœ€å°åŒ–è¯¯å·® $\sum_{i=1}^N \xi_i$ æ¥è®­ç»ƒæ”¯æŒå‘é‡æœº

+   $ğ‘ª$ è¶…å‚æ•°åŠ æƒè¯¯å·®æ€»å’Œ $xi_ğ‘–$ï¼Œè¾ƒé«˜çš„ $ğ¶$ å°†å¯¼è‡´é—´éš” $M$ å‡å°‘ï¼Œå¹¶å¯¼è‡´è¿‡æ‹Ÿåˆ

+   è¾ƒå°çš„é—´éš”ï¼Œä½¿ç”¨è¾ƒå°‘çš„æ•°æ®æ¥çº¦æŸè¾¹ç•Œï¼Œç§°ä¸ºæ”¯æŒå‘é‡

+   è®­ç»ƒæ•°æ®å®Œå…¨ä½äºè¾¹ç•Œæ­£ç¡®ä¸€ä¾§çš„æ²¡æœ‰å½±å“

è¿™é‡Œæ˜¯æ”¯æŒå‘é‡æœºçš„å‡ ä¸ªå…³é”®æ–¹é¢ï¼Œ

+   è¢«ç§°ä¸ºæ”¯æŒå‘é‡æœºï¼Œè€Œä¸æ˜¯æœºå™¨ï¼Œå› ä¸ºä½¿ç”¨æ–°çš„æ ¸å¯ä»¥å¾—åˆ°æ–°çš„æœºå™¨

+   æœ‰è®¸å¤šæ ¸å¯ç”¨ï¼ŒåŒ…æ‹¬å¤šé¡¹å¼å’Œå¾„å‘åŸºå‡½æ•°

ä¸»è¦è¶…å‚æ•°æ˜¯ $C$ï¼Œå³

è¶…å‚æ•°ä¸æ ¸çš„é€‰æ‹©æœ‰å…³ï¼Œä¾‹å¦‚ï¼Œ

+   *å¤šé¡¹å¼* - å¤šé¡¹å¼é˜¶æ•°

+   *å¾„å‘åŸºå‡½æ•°* - $\gamma$ ä¸è®­ç»ƒæ•°æ®çš„è·ç¦»å½±å“æˆåæ¯”

## **æ ¸æŠ€å·§**

æˆ‘ä»¬å¯ä»¥åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­åŒ…å«åŸºå‡½æ•°å±•å¼€ï¼Œè€Œæ— éœ€å°†è®­ç»ƒæ•°æ®è½¬æ¢åˆ°è¿™ä¸ªæ›´é«˜ç»´ç©ºé—´ï¼Œ

$$ h(x) $$

æˆ‘ä»¬åªéœ€è¦é¢„æµ‹ç‰¹å¾çš„å†…ç§¯ï¼Œ

$$ h(x) \left( h(x') \right)^T = \langle h(x), h(x') \rangle $$

æˆ‘ä»¬åªéœ€è¦åœ¨å˜æ¢ç©ºé—´ä¸­æ‰€æœ‰å¯ç”¨è®­ç»ƒæ•°æ®ä¹‹é—´çš„â€œç›¸ä¼¼æ€§â€ï¼

+   æˆ‘ä»¬ä»…ä½¿ç”¨è®­ç»ƒæ•°æ®ä¹‹é—´çš„ç›¸ä¼¼æ€§çŸ©é˜µæ¥è®­ç»ƒæ”¯æŒå‘é‡æœºï¼Œè¿™äº›æ•°æ®å°†è¢«æŠ•å½±åˆ°æ›´é«˜ç»´çš„ç©ºé—´

+   æˆ‘ä»¬å®é™…ä¸Šå¹¶ä¸éœ€è¦è®¡ç®—æ›´é«˜ç»´ç©ºé—´ä¸­çš„è®­ç»ƒæ•°æ®å€¼

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = True
import os                                                     # to set current working directory 
import math                                                   # square root operator
import numpy as np                                            # arrays and matrix math
import scipy.stats as st                                      # statistical methods
import pandas as pd                                           # DataFrames
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter,NullLocator) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import seaborn as sns                                         # for matrix scatter plots
from sklearn.svm import SVC                                   # support vector machine methods
from sklearn import metrics                                   # measures to check our models
from sklearn.metrics import confusion_matrix                  # for summarizing model performance
from sklearn.preprocessing import StandardScaler              # standardize the features
from sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,StratifiedShuffleSplit) # model tuning
from sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline
from sklearn import metrics                                   # measures to check our models
from sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation
from sklearn.model_selection import train_test_split          # train and test split
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias
binary_cmap = ListedColormap(['grey', 'gold'])                # custom binary categorical colormap
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # suppress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœä½ é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½é¦–å…ˆéœ€è¦å®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æ›´å¤šå¸®åŠ©å¯ä»¥åœ¨ç›¸åº”åŒ…çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰å‡ ä¸ªå‡½æ•°æ¥ç®€åŒ–ç›¸å…³çŸ©é˜µçš„ç»˜åˆ¶å’Œå†³ç­–æ ‘å›å½’æ¨¡å‹çš„å¯è§†åŒ–ã€‚

```py
def comma_format(x, pos):
    return f'{int(x):,}'

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def plot_CDF(data,color,alpha=1.0,lw=1,ls='solid',label='none'):
    cumprob = (np.linspace(1,len(data),len(data)))/(len(data)+1)
    plt.scatter(np.sort(data),cumprob,c=color,alpha=alpha,edgecolor='black',lw=lw,ls=ls,label=label,zorder=10)
    plt.plot(np.sort(data),cumprob,c=color,alpha=alpha,lw=lw,ls=ls,zorder=8)

def visualize_SVM(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,xlabel,ylabel,title,cat,label,cmap,plot_support): 
    xplot_step = (x_max - x_min)/300.0; yplot_step = (y_max - y_min)/300.0 # resolution of the model visualization
    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step), # set up the mesh
                     np.arange(y_min, y_max, yplot_step))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])          # predict with our trained model over the mesh
    Z = Z.reshape(xx.shape)
    cs = plt.contourf(xx, yy,Z,cmap=cmap,vmin=z_min,vmax=z_max,levels = 50,alpha=0.6) # plot the predictions
    for i in range(len(cat)):
        im = plt.scatter(xfeature[response==cat[i]],yfeature[response==cat[i]],s=None,c=response[response==cat[i]], 
                    marker=None, cmap=cmap, norm=None,vmin=z_min,vmax=z_max,alpha=0.8,linewidths=0.3, edgecolors="black")
    plt.scatter(-9999,-9999,marker='s',c = cat[0],label=label[0],cmap=cmap,vmin=z_min,vmax=z_max) # custom legend
    plt.scatter(-9999,-9999,marker='s',c = cat[1],label=label[1],cmap=cmap,vmin=z_min,vmax=z_max)
    plt.scatter(-999,-999,s=80,marker='o',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Train')
    if plot_support:                                          # modified from Jake VanderPlas's Python Data Science Handbook 
        sv = model.support_vectors_                           # retrieve the support vectors
        plt.scatter(sv[:, 0],sv[:, 1],s=3,linewidth=8,alpha = 0.6,facecolors='black',label='Support Vector');
    plt.legend(loc='upper right'); plt.title(title)                    
    plt.xlabel(xlabel); plt.ylabel(ylabel)
    plt.xlim([x_min,x_max]); plt.ylim([y_min,y_max]); add_grid()

def display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)
    html_str = ''
    for df in args:
        html_str += df.head().to_html()                       # Using .head() for the first few rows
    display(HTML(f'<div style="display: flex;">{html_str}</div>')) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/PGE383")                                        # set the working directory 
```

## åŠ è½½æ•°æ®

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›†â€˜12_sample_data.csvâ€™ã€‚å®ƒæ˜¯ä¸€ä¸ªä»¥é€—å·åˆ†éš”çš„æ–‡ä»¶ï¼ŒåŒ…å«ï¼š

+   X å’Œ Y åæ ‡ï¼ˆ$m$ï¼‰

+   åœ°å±‚ 0 å’Œ 1

+   å­”éš™ç‡ï¼ˆåˆ†æ•°ï¼‰

+   æ¸—é€ç‡ï¼ˆ$mD$ï¼‰

+   å£°æ³¢é˜»æŠ—ï¼ˆ$kg/mÂ³ \cdot m/s \cdot 10Â³$ï¼‰ã€‚

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬ç§°ä¸ºâ€˜dfâ€™çš„æ•°æ®æ¡†ä¸­ï¼Œç„¶åé¢„è§ˆå®ƒä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

**Python æŠ€å·§ï¼šä½¿ç”¨åŒ…ä¸­çš„å‡½æ•°**åªéœ€è¾“å…¥æˆ‘ä»¬åœ¨å¼€å¤´å£°æ˜çš„åŒ…çš„æ ‡ç­¾ï¼š

```py
import pandas as pd 
```

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®¿é—® pandas å‡½æ•°â€˜read_csvâ€™ï¼š

```py
pd.read_csv() 
```

ä½†æ˜¯ï¼Œread_csv éœ€è¦è¾“å…¥å‚æ•°ã€‚æœ€é‡è¦çš„ä¸€ä¸ªæ˜¯æ–‡ä»¶åã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–é»˜è®¤å‚æ•°éƒ½å¾ˆå¥½ã€‚å¦‚æœä½ æƒ³æŸ¥çœ‹æ­¤å‡½æ•°çš„æ‰€æœ‰å¯èƒ½å‚æ•°ï¼Œè¯·å‚é˜…[è¿™é‡Œ](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)çš„æ–‡æ¡£ã€‚

+   æ–‡æ¡£æ€»æ˜¯å¾ˆæœ‰å¸®åŠ©

+   Python å‡½æ•°é€šå¸¸æœ‰å¾ˆå¤šçµæ´»æ€§ï¼Œè¿™æ˜¯é€šè¿‡ä½¿ç”¨å„ç§è¾“å…¥å‚æ•°å®ç°çš„

æ­¤å¤–ï¼Œç¨‹åºæœ‰ä¸€ä¸ªè¾“å‡ºï¼Œä¸€ä¸ªä»æ•°æ®åŠ è½½çš„ pandas DataFrameã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šä»£è¡¨è¯¥æ–°å¯¹è±¡çš„åå­—/å˜é‡ã€‚

```py
df = pd.read_csv("12_sample_data.csv") 
```

## æ ‡å‡†åŒ–é¢„æµ‹ç‰¹å¾

æ”¯æŒå‘é‡æœºæœ€å°åŒ–è¯¯å·®ï¼Œå³è®­ç»ƒæ•°æ®ä¸è¾¹ç•Œçš„è·ç¦»ã€‚å› æ­¤ï¼Œè¿™ç§æ–¹æ³•å¯¹é¢„æµ‹ç‰¹å¾çš„ç›¸å¯¹èŒƒå›´æ•æ„Ÿã€‚

+   å¦‚æœä¸€ä¸ªé¢„æµ‹ç‰¹å¾çš„èŒƒå›´å¤§å¾—å¤šï¼Œå®ƒå°†ä¸»å¯¼æ¨¡å‹ï¼Œæ¨¡å‹å°†åªåœ¨è¯¥ç‰¹å¾ä¸Šåˆ†ç¦»ï¼ç»“æœæ˜¯æ¨¡å‹ä¸è¯¥ç‰¹å¾æ­£äº¤ï¼Œå³åªåœ¨è¯¥ç‰¹å¾ä¸Šåˆ†å‰²ã€‚

```py
df = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv")

yname = 'Facies'; Xname = ['Porosity','AI']                   # specify the predictor features (x2) and response feature (x1)
Xmin = [0.1,1500.0]; Xmax = [0.3,6500.0]                      # set minimums and maximums for visualization 
ymin = 0.0; ymax = 1.0
Xlabel = ['Porosity','Acoustic Impedance']; ylabel = 'Facies' # specify the feature labels for plotting
Xunit = ['Fraction',r'$\frac{kg}{mÂ³} \cdot \frac{m}{s} \cdot 10Â³$']; yunit = 'MCFPD'
Xlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']
ylabelunit = ylabel + ' (' + yunit + ')'

y = pd.DataFrame(df[yname])                                   # extract selected features as X and y DataFrames
X = df[Xname]

ysname = 's' + yname; Xsname = ['s' + element for element in Xname] # standardized predictor names
Xsmin = [-3.0,-3.0]; Xsmax = [3.0,3.0]                        # set minimums and maximums for standardized features
Xslabel = ['Standardized ' + element for element in Xlabel]   # standardized predictor names
Xsunit = ['S[' + element + ']' for element in Xunit]          # standardized predictor names
Xslabelunit = [Xslabel[0] + ' (' + Xsunit[0] + ')',Xslabel[1] + ' (' + Xsunit[1] + ')']

transform = StandardScaler();                                 # instantiate feature standardization method
Xs = transform.fit_transform(X)                               # standardize the data features to mean = 0, var = 1.0
X[Xsname] = Xs                                                # add standardized features to the predictor feature DataFrame 
```

## è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²

ä¸ºäº†æ–¹ä¾¿å’Œç®€å•ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn çš„éšæœºè®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²ã€‚

+   æˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„ random_state å‚æ•°ï¼Œå› æ­¤åŸå§‹ç‰¹å¾å’Œæ ‡å‡†åŒ–ç‰¹å¾ä¸Šçš„è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²æ˜¯ç›¸åŒçš„ã€‚

+   æˆ‘æœ¬å¯ä»¥åªåå‘è½¬æ¢æ ‡å‡†åŒ–åçš„æ•°æ®ï¼ˆå‰§é€ï¼šæˆ‘å°†å±•ç¤ºä¸æ ‡å‡†åŒ–å¯¹æ¨¡å‹çš„å½±å“ï¼‰ã€‚

+   é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸éœ€è¦å¯¹é¢„æµ‹ç‰¹å¾è¿›è¡Œåå‘è½¬æ¢ï¼Œå¯¹äºæˆ‘ä»¬çš„é¢„æµ‹å·¥ä½œæµç¨‹æ¥è¯´ï¼Œé¢„æµ‹ç‰¹å¾æ˜¯ä¸€æ¬¡æ€§æ—…ç¨‹ã€‚

```py
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split
df_train = pd.concat([X_train,y_train],axis=1)                # make one train and test DataFrame with both X and y
df_test = pd.concat([X_test,y_test],axis=1) 
```

## å¯è§†åŒ– DataFrame

åœ¨æˆ‘ä»¬æ„å»ºæ¨¡å‹ä¹‹å‰ï¼Œå¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯• DataFrame æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ DataFrame çš„â€˜headâ€™æˆå‘˜å‡½æ•°æ¥é¢„è§ˆï¼ˆæ ¼å¼æ•´æ´ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚

```py
print('       Training DataFrame          Testing DataFrame')
display_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display 
```

```py
 Training DataFrame          Testing DataFrame 
```

|  | å­”éš™åº¦ | AI | s å­”éš™åº¦ | sAI | ç›¸ |
| --- | --- | --- | --- | --- | --- |
| 340 | 0.204313 | 4373.187870 | 0.469659 | 0.788406 | 1 |
| 159 | 0.167316 | 3088.482947 | -0.698603 | -0.860390 | 0 |
| 315 | 0.219801 | 2983.326185 | 0.958720 | -0.995349 | 1 |
| 365 | 0.216819 | 2543.772663 | 0.864542 | -1.559474 | 1 |
| 385 | 0.191565 | 3670.457907 | 0.067120 | -0.113481 | 1 |
|  | å­”éš™åº¦ | AI | s å­”éš™åº¦ | sAI | ç›¸ |
| --- | --- | --- | --- | --- | --- |
| 72 | 0.139637 | 4747.274043 | -1.572630 | 1.268510 | 0 |
| 153 | 0.170732 | 4535.625583 | -0.590742 | 0.996879 | 0 |
| 258 | 0.244345 | 2696.102930 | 1.733756 | -1.363972 | 1 |
| 56 | 0.167125 | 5500.997419 | -0.704644 | 2.235841 | 0 |
| 303 | 0.216253 | 3959.934912 | 0.846677 | 0.258035 | 1 |

## è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡

åœ¨ DataFrames ä¸­ï¼Œæœ‰è®¸å¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡ã€‚

+   æè¿°å‘½ä»¤æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„æ•°æ®è¡¨ï¼Œå…¶ä¸­åŒ…æ‹¬è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€‚

```py
print('            Training DataFrame                      Testing DataFrame') # custom function for side-by-side summary statistics
display_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) 
```

```py
 Training DataFrame                      Testing DataFrame 
```

|  | å­”éš™åº¦ | AI | s å­”éš™åº¦ | sAI | ç›¸ |
| --- | --- | --- | --- | --- | --- |
| count | 360.000000 | 360.000000 | 360.000000 | 360.000000 | 360.000000 |
| mean | 0.189150 | 3767.451286 | -0.009167 | 0.011001 | 0.602778 |
| std | 0.031636 | 786.394126 | 0.998983 | 1.009262 | 0.490004 |
| min | 0.117562 | 1746.387548 | -2.269691 | -2.582841 | 0.000000 |
| max | 0.261091 | 5957.162150 | 2.262519 | 2.821285 | 1.000000 |
|  | å­”éš™åº¦ | AI | s å­”éš™åº¦ | sAI | ç›¸ |
| --- | --- | --- | --- | --- | --- |
| count | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 |
| mean | 0.190311 | 3733.164755 | 0.027500 | -0.033003 | 0.658333 |
| std | 0.032014 | 763.117871 | 1.010903 | 0.979389 | 0.476257 |
| min | 0.131230 | 1961.600397 | -1.838105 | -2.306636 | 0.000000 |
| max | 0.256172 | 6194.573653 | 2.107198 | 3.125980 | 1.000000 |

å¾ˆå¥½ï¼Œæˆ‘ä»¬å·²ç»æ£€æŸ¥äº†æ±‡æ€»ç»Ÿè®¡ã€‚

+   æ²¡æœ‰æ˜æ˜¾çš„é”™è¯¯

+   æ£€æŸ¥æ¯ä¸ªç‰¹å¾çš„å€¼èŒƒå›´ï¼Œä»¥è®¾ç½®å’Œè°ƒæ•´ç»˜å›¾é™åˆ¶ã€‚è§ä¸Šæ–‡ã€‚

## å¯è§†åŒ–è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ’åˆ†

è®©æˆ‘ä»¬é€šè¿‡ç›´æ–¹å›¾å’Œæ•£ç‚¹å›¾æ£€æŸ¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ä¸€è‡´æ€§å’Œè¦†ç›–ç‡ã€‚

+   æ£€æŸ¥ä»¥ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¦†ç›–äº†å¯èƒ½çš„ç‰¹å¾ç»„åˆèŒƒå›´

+   ç¡®ä¿æˆ‘ä»¬ä½¿ç”¨æµ‹è¯•æ¡ˆä¾‹ä¸ä¼šè¶…å‡ºè®­ç»ƒæ•°æ®èŒƒå›´è¿›è¡Œå¤–æ¨

```py
nbins = 20                                                    # number of histogram bins

plt.subplot(231)                                              # predictor feature #1 histogram
freq1,_,_ = plt.hist(x=X_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=X_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[0]); add_grid()  
plt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   

plt.subplot(232)                                              # predictor feature #2 histogram
freq1,_,_ = plt.hist(x=X_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=X_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[1]); add_grid()  
plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
plt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   

plt.subplot(233)                                              # predictor features #1 and #2 scatter plot
plt.scatter(X_train[Xname[0]],X_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(X_test[Xname[0]],X_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.title(Xlabel[0] + ' vs ' +  Xlabel[1])
plt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])
plt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplot(234)                                              # predictor feature #1 histogram
freq1,_,_ = plt.hist(x=X_train[Xsname[0]],weights=None,bins=np.linspace(Xsmin[0],Xsmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=X_test[Xsname[0]],weights=None,bins=np.linspace(Xsmin[0],Xsmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xslabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xslabel[0]); add_grid()  
plt.xlim([Xsmin[0],Xsmax[0]]); plt.legend(loc='upper right')   

plt.subplot(235)                                              # predictor feature #2 histogram
freq1,_,_ = plt.hist(x=X_train[Xsname[1]],weights=None,bins=np.linspace(Xsmin[1],Xsmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=X_test[Xsname[1]],weights=None,bins=np.linspace(Xsmin[1],Xsmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xslabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xslabel[1]); add_grid()  
plt.xlim([Xsmin[1],Xsmax[1]]); plt.legend(loc='upper right')   

plt.subplot(236)                                              # predictor features #1 and #2 scatter plot
plt.scatter(X_train[Xsname[0]],X_train[Xsname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(X_test[Xsname[0]],X_test[Xsname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.title(Xslabel[0] + ' vs ' +  Xslabel[1])
plt.xlabel(Xslabelunit[0]); plt.ylabel(Xslabelunit[1])
plt.legend(); add_grid(); plt.xlim([Xsmin[0],Xsmax[0]]); plt.ylim([Xsmin[1],Xsmax[1]])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=1.6, wspace=0.3, hspace=0.25)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![_images/c96849d9ac15251e0c63378a9f425aa0f734d87c0a250b8ce44f189349db09c5.png](img/acefe3d0111ef4c4aadff2177c5a7676.png)

æœ‰æ—¶æˆ‘å‘ç°é€šè¿‡æŸ¥çœ‹ CDF è€Œä¸æ˜¯ç›´æ–¹å›¾æ¥æ¯”è¾ƒåˆ†å¸ƒæ›´æ–¹ä¾¿ã€‚

+   æˆ‘ä»¬é¿å…ä»»æ„é€‰æ‹©ç›´æ–¹å›¾æŸ±çŠ¶å¤§å°ï¼Œå› ä¸º CDF ä¸æ•°æ®åˆ†è¾¨ç‡ä¸€è‡´ã€‚

```py
plt.subplot(131)                                              # predictor feature #1 CDF
plot_CDF(X_train[Xsname[0]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(X_test[Xsname[0]],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(Xslabelunit[0]); plt.xlim(Xsmin[0],Xsmax[0]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(Xslabel[0] + ' Train and Test CDFs')

plt.subplot(132)                                              # predictor feature #2 CDF
plot_CDF(X_train[Xsname[1]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(X_test[Xsname[1]],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(Xslabelunit[1]); plt.xlim(Xsmin[1],Xsmax[1]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(Xslabel[1] + ' Train and Test CDFs')

plt.subplot(133)                                              # categorical response feature grouped histogram
plt.bar([-0.125],len(y_train[yname][y_train[yname]==0]),width=0.25,color=['darkorange'],edgecolor='black',label='Train')
plt.bar([0.125],len(y_test[yname][y_test[yname]==0]),width=0.25,color=['red'],edgecolor='black',label='Test')
plt.bar([0.875],len(y_train[yname][y_train[yname]==1]),width=0.25,color=['darkorange'],edgecolor='black')
plt.bar([1.125],len(y_test[yname][y_test[yname]==1]),width=0.25,color=['red'],edgecolor='black')
x_ticks = [0, 1]; x_labels = ['Shale', 'Sand']; plt.xticks(x_ticks,x_labels)
plt.ylim([0.,250.0]); plt.xlim([-0.5,1.5]); add_grid(); plt.legend(loc='upper left')
ax = plt.gca(); ax.xaxis.set_minor_locator(NullLocator())
plt.title(ylabel + ' Train and Test Categorical Response Frequencies'); plt.xlabel('Facies'); plt.ylabel('Frequency')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=0.8, wspace=0.3, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![_images/7f4199220c2686c54fc8e8ceea1351ff88ab7bba2333e7441c6d268c034adbed.png](img/2bbf0a8662041cb7a556a37197370bc0.png)

è¿™çœ‹èµ·æ¥ä¸é”™ï¼Œ

+   åˆ†å¸ƒè¡¨ç°è‰¯å¥½ï¼Œæˆ‘ä»¬æ— æ³•è§‚å¯Ÿåˆ°æ˜æ˜¾çš„ç¼ºå£ã€å¼‚å¸¸å€¼æˆ–æˆªæ–­

+   æµ‹è¯•å’Œè®­ç»ƒæ¡ˆä¾‹æœ‰ç±»ä¼¼çš„è¦†ç›–èŒƒå›´

+   åœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä¸­ï¼Œåˆ†ç±»å“åº”çš„ç›¸å¯¹é¢‘ç‡ç›¸ä¼¼ï¼Œå³ï¼Œè®­ç»ƒå’Œæµ‹è¯•å¹³è¡¡è‰¯å¥½ã€‚

## å¯è§†åŒ–é¢„æµ‹ç‰¹å¾ç©ºé—´

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªç®€åŒ–çš„å›¾è¡¨æ¥å¯è§†åŒ–äºŒç»´é¢„æµ‹ç‰¹å¾ç©ºé—´ä¸­çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚

+   æˆ‘ä»¬æå‡ºé—®é¢˜ï¼Œæˆ‘ä»¬èƒ½å¦å»ºæ¨¡åˆ†ç±»è¾¹ç•Œï¼Ÿæ•°æ®é‡å å¾ˆå¤šå—ï¼Ÿè¾¹ç•Œç®€å•ï¼ˆå³ï¼Œçº¿æ€§ï¼‰è¿˜æ˜¯æ›´å¤æ‚ï¼Ÿ

```py
plt.subplot(111)                                              # plot train and test data in predictor feature space
plt.scatter(X_train[Xsname[0]][y_train[yname]==1],X_train[Xsname[1]][y_train[yname]==1],s=80,
            marker='o',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,label='Sand')
plt.scatter(X_train[Xsname[0]][y_train[yname]==0],X_train[Xsname[1]][y_train[yname]==0],s=80,
            marker='o',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,label='Shale')

plt.scatter(X_test[Xsname[0]][y_test[yname]==1],X_test[Xsname[1]][y_test[yname]==1],s=80,
            marker='s',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,)
plt.scatter(X_test[Xsname[0]][y_test[yname]==0],X_test[Xsname[1]][y_test[yname]==0],s=80,
            marker='s',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,)

plt.scatter([-999],[-999],s=80,marker='o',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Train')
plt.scatter([-999],[-999],s=80,marker='s',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Test')

plt.legend(loc = 'upper right')
plt.title('Training and Testing ' + ylabel + ' vs. ' + Xslabel[1] + ' and ' + Xlabel[0])
plt.xlabel(Xslabelunit[0]); plt.ylabel(Xslabelunit[1]); add_grid(); plt.xlim([Xsmin[0],Xsmax[0]]); plt.ylim([Xsmin[1],Xsmax[1]])
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![b3c308d6eca35509ace9645fe2c47910d318c23c40b049222f1c1f89bfeeea5b.png](img/56dc165df133e2ddbce54c0beda4915c.png)

è¿™å°†æ˜¯ä¸€ä¸ªå›°éš¾çš„åˆ†ç±»ï¼Œ

+   å½“ç„¶æœ‰å¾ˆå¤šé‡å 

+   è¾¹ç•Œå¯èƒ½æ˜¯éçº¿æ€§çš„ã€‚

ä½†æ˜¯ï¼Œæœ‰å¥½çš„æ¶ˆæ¯ï¼Œ

+   æ€»ä½“è€Œè¨€ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¦†ç›–èŒƒå›´çœ‹èµ·æ¥ä¸é”™ï¼Œæ³¨æ„æœ‰å‡ ä¸ªæµ‹è¯•æ¡ˆä¾‹å°†æµ‹è¯•æ¨¡å‹å¤–æ¨ã€‚

+   æ”¯æŒå‘é‡æœºè¢«è®¾è®¡ç”¨æ¥å¤„ç†è¿™ç§é‡å ï¼

## çº¿æ€§æ ¸æ”¯æŒå‘é‡æœºæ¨¡å‹

è®©æˆ‘ä»¬ä»ç®€å•å¼€å§‹ï¼Œåœ¨æˆ‘ä»¬çš„ç‰¹å¾ç©ºé—´ä¸­è®­ç»ƒå¹¶å¯è§†åŒ–çº¿æ€§æ”¯æŒå‘é‡æœºæ¨¡å‹ã€‚

+   è¿™å°†ä¸º facies 0 å’Œ 1 æä¾›ä¸€ä¸ªåŸºäº AI å’Œå­”éš™ç‡çš„çº¿æ€§ç©ºé—´åˆ†ç±»æ¨¡å‹ã€‚

æˆ‘ä»¬ä½¿ç”¨ scikit-learn å‡½æ•° $SVC$ æ¥è¯å®æ”¯æŒå‘é‡æœºï¼š

```py
svm_linear = SVC() 
```

å‚æ•°åŒ…æ‹¬ï¼š

+   **kernel** åº”ç”¨åˆ°æ•°æ®ä»¥å°†å…¶æŠ•å½±åˆ°å¯èƒ½æ›´é«˜ç»´ç©ºé—´ä¸­çš„æ ¸ç±»å‹

+   **$C$** è¯¯åˆ†ç±»çš„æƒ©ç½š

+   **random_state** éšæœºæ•°ç”¨äºéšæœºæ‰“ä¹±æ•°æ®ä»¥è¿›è¡Œæ¦‚ç‡ä¼°è®¡

æˆ‘ä»¬æ¥ç€ä½¿ç”¨å‘½ä»¤ï¼Œ

```py
svm_linear.fit() 
```

ä»¥è®­ç»ƒæ•°æ®é›†æ¥è®­ç»ƒæ¨¡å‹ã€‚

fit å‡½æ•°çš„è¾“å…¥åŒ…æ‹¬ï¼š

+   **X** - ç”¨äºè®­ç»ƒæ•°æ®é›†çš„é¢„æµ‹ç‰¹å¾çš„ $n \times m$ æ•°ç»„

+   **y** - ç”¨äºè®­ç»ƒæ•°æ®é›†çš„å“åº”ç‰¹å¾çš„ $n \times 1$ æ•°ç»„

è®©æˆ‘ä»¬å°è¯•ä¸¤ä¸ªä¸åŒçš„ $C$ æƒ©ç½šè¶…å‚æ•°æ¥å¯è§†åŒ–è¯¯åˆ†ç±»æƒ©ç½šçš„å½±å“ã€‚

```py
C1_list = [0.01,100]                                          # set hyperparameters
SVM1_list = []

for C in C1_list:                                             # train the models
    SVM1_list.append(SVC(kernel = 'linear',C = C, random_state = seed).fit(X_train[Xsname],y_train)) # instantiate and train 
```

çœ‹èµ·æ¥è¿è¡Œå¾—å¾ˆå¥½ï¼è®©æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„æ–¹ä¾¿çš„å¯è§†åŒ–å‡½æ•°æ¥å¯è§†åŒ–ç»“æœã€‚

```py
for iC, C in enumerate(C1_list):                              # visualize the training data and model
    plt.subplot(1,2,iC+1)
    visualize_SVM(SVM1_list[iC],X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,
                Xslabelunit[0],Xslabelunit[1],r'Training Data and Linear Support Vector Machine, $C$ = ' + str(C),[0,1],['Shale','Sand'],
                binary_cmap,True)
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![0dec98b51eac972aa8541bbcf4d2742f7a09bcb6b1b19ee80196793e988428ba.png](img/f9599d00749b4f00b75cb1eeb40f7f47.png)

ä¸Šå›¾æ˜¾ç¤ºäº†çº¿æ€§æ ¸æ”¯æŒå‘é‡æœºåˆ†ç±»æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®é›†ä»¥åŠç”¨ç²—åœ†åœˆè¡¨ç¤ºçš„ç»“æœæ”¯æŒå‘é‡ã€‚

+   çº¿æ€§æ ¸åªæä¾›ä¸€æ¡ç›´çš„å†³å®šè¾¹ç•Œã€‚

+   è°ƒæ•´æ¨¡å‹ä»¥é€‚åº”æ›´å¤æ‚çš„æƒ…å†µæ˜¯å›°éš¾çš„ã€‚

æ³¨æ„ï¼Œéšç€ C å€¼çš„å¢åŠ ï¼Œé”™è¯¯æ€»å’Œçš„åŠ æƒï¼Œæˆ‘ä»¬å¾—åˆ°æ›´å°çš„è¾¹ç•Œã€‚çº¿æ€§æ¨¡å‹ä¸æ›´å°‘çš„æ”¯æŒå‘é‡ï¼ˆè¾¹ç•Œå†…çš„è®­ç»ƒæ•°æ®ï¼‰æ‹Ÿåˆã€‚è®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹å…³äºç®€å•å’Œå¤æ‚æ¨¡å‹ã€æ½œåœ¨æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆä»¥åŠæ¨¡å‹åå·®å’Œæ–¹å·®æƒè¡¡çš„è¯¥è¶…å‚æ•°çš„ç«¯æˆå‘˜ã€‚

+   **ç®€å•æ¨¡å‹/æ¬ æ‹Ÿåˆæ¨¡å‹** - å½“ C å€¼è¾ƒå°æ—¶ï¼Œåˆ†ç±»å™¨å¯¹è¯¯åˆ†ç±»çš„æ•°æ®ç‚¹æ›´å®½å®¹ï¼ˆæ›´é«˜çš„æ¨¡å‹åå·®ï¼Œæ›´ä½çš„æ¨¡å‹æ–¹å·®ï¼‰ã€‚

+   **å¤æ‚æ¨¡å‹ \ è¿‡æ‹Ÿåˆæ¨¡å‹** - å½“ C å€¼è¾ƒå¤§æ—¶ï¼Œåˆ†ç±»å™¨å¯¹è¯¯åˆ†ç±»çš„æ•°æ®ç‚¹æ›´æ•æ„Ÿï¼ˆæ¨¡å‹åå·®æ›´ä½ï¼Œæ¨¡å‹æ–¹å·®æ›´é«˜ï¼‰ã€‚

æ¢å¥è¯è¯´ï¼Œ$C$ æ˜¯ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œå°±åƒå²­å›å½’ä¸­çš„æ”¶ç¼©é¡¹ã€‚è®©æˆ‘ä»¬å°è¯•ä¸€äº›å…·æœ‰ä¸åŒæ ¸çš„æ›´çµæ´»çš„åˆ†ç±»å™¨ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚

## åŸºäºå¤šé¡¹å¼æ ¸çš„æ”¯æŒå‘é‡æœºæ¨¡å‹

å¤šé¡¹å¼æ ¸å®šä¹‰ä¸º

\begin{equation} K(x,xâ€™) = (x^Tx)^d, \end{equation}

å…¶ä¸­ $d$ æ˜¯å¤šé¡¹å¼çš„æ¬¡æ•°ã€‚

è¶…å‚æ•° $degree$ æ˜¯å¤šé¡¹å¼æ ¸å‡½æ•°çš„é˜¶æ•°ã€‚

å¦‚å‰æ‰€è¿°ï¼Œè®©æˆ‘ä»¬å°è¯•ä¸åŒçš„ $C$ï¼Œé”™è¯¯æƒ©ç½šï¼Œä½¿ç”¨å•ä¸ªå¤šé¡¹å¼é˜¶æ•°è§‚å¯Ÿç»“æœã€‚

```py
C2_list = [0.01,0.1,1,10]                                     # set hyperparameters
order = np.full((len(C2_list)),3)       
SVM2_list = []

for iC, C in enumerate(C2_list):                              # train the model and visualize the training data and model
    SVM2_list.append(SVC(kernel = 'poly',degree=order[iC],C = C,random_state = seed).fit(X_train[Xsname],y_train)) # instantiate and train
    plt.subplot(2,2,iC+1)
    visualize_SVM(SVM2_list[iC-1],X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,
                Xslabelunit[0],Xslabelunit[1],'Polynomial Support Vector Machine, Order = ' + str(order[iC]) + r', $C$ = ' + str(C),
                [0,1],['Shale','Sand'],binary_cmap,True)
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=1.6, wspace=0.3, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/00a2d3685850074a3a660b3f3a1259f2.png)

éšç€ C è¶…å‚æ•°çš„å¢åŠ ï¼Œé—´éš”ç¼©å°ï¼Œæ”¯æŒå‘é‡çš„æ•°é‡å‡å°‘ï¼Œæ¨¡å‹å¤æ‚åº¦å¢åŠ ã€‚

## åŸºäºå¾„å‘åŸºå‡½æ•°æ ¸çš„æ”¯æŒå‘é‡æœºæ¨¡å‹

å¾„å‘åŸºå‡½æ•°ï¼ˆRBFï¼‰æ˜¯ SVC ä¸­å¸¸ç”¨çš„å¦ä¸€ä¸ªæ ¸å‡½æ•°ï¼š

\begin{equation} K(x,xâ€™) = e^{- \gamma ||x-xâ€™||Â²}, \end{equation}

å…¶ä¸­ $||x-x'||Â²$ æ˜¯ä¸¤ä¸ªæ•°æ®ç‚¹ x å’Œ xâ€™ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»çš„å¹³æ–¹ã€‚

é«˜æ–¯æ ¸æ˜¯ RBF çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå…¶ä¸­ï¼š

\begin{equation} K(x,xâ€™) = e^{- \frac {||x-xâ€™||Â²} {2 \sigmaÂ²}}. \end{equation}

é€šè¿‡æ”¹å˜ $\gamma$ å’Œ C çš„å€¼ï¼Œå…·æœ‰ RBF æ ¸çš„åˆ†ç±»å™¨å¯ä»¥è¢«è°ƒæ•´ã€‚

$\gamma$ å¯ä»¥è¢«è§†ä¸ºæ ¸çš„æ‰©æ•£ã€‚

+   å½“ $\gamma$ è¾ƒä½æ—¶ï¼Œå†³ç­–è¾¹ç•Œçš„æ›²ç‡è¾ƒä½ï¼Œå¯¼è‡´å†³ç­–åŒºåŸŸè¾ƒå®½ï¼ˆä½æ–¹å·®ï¼Œé«˜åå·®ï¼‰ï¼Œä½å¤æ‚åº¦ã€‚

+   $\gamma$ å‚æ•°å¯ä»¥è§£é‡Šä¸ºæ¨¡å‹é€‰ä¸ºæ”¯æŒå‘é‡çš„æ ·æœ¬çš„å½±å“åŠå¾„çš„å€’æ•°ï¼Œå³ä½ gamma å€¼ä¼šæ•´åˆæ›´å¤šæ•°æ®ä»¥å®ç°æ›´å¹³æ»‘çš„æ¨¡å‹ã€‚

```py
C3_list = [1e-1, 1, 1e2]                                      # set hyperparameters
gamma1_list = [1e-1, 1, 1e1]

index= 1
for C in C3_list:
    for gamma in gamma1_list:                                 # train the models, visualize the training data and models
        svc = SVC(kernel='rbf',gamma=gamma,C=C,random_state = seed).fit(X_train[Xsname],y_train) # instantiate and train
        plt.subplot(3,3,index)
        visualize_SVM(svc,X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,
                Xslabelunit[0],Xslabelunit[1],r'RBF Support Vector Machine, $\gamma$ = ' + str(gamma) + r', $C$ = ' + str(C),
                [0,1],['Shale','Sand'],binary_cmap,True)
        index = index + 1

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.4, top=2.4, wspace=0.3, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/6f78098f6348e837aa484bf3c227abf8.png)

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨ C è¶…å‚æ•°è¿›è¡Œæ­£åˆ™åŒ–çš„å½±å“éå¸¸æ˜æ˜¾ï¼Œ

+   æ›´é«˜çš„ C å€¼ï¼Œæ›´å°çš„é—´éš”ï¼Œæ›´å°‘çš„æ”¯æŒå‘é‡ï¼Œå€¾å‘äºè¿‡æ‹Ÿåˆã€‚

+   æ›´ä½çš„ C å€¼ï¼Œæ›´å¤§çš„é—´éš”ï¼Œæ›´å¤šçš„æ”¯æŒå‘é‡ï¼Œå€¾å‘äºæ¬ æ‹Ÿåˆã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œgamma çš„å½±å“ä¹Ÿéå¸¸æ˜æ˜¾ï¼Œ

+   æ›´é«˜çš„ gamma å€¼å¯¼è‡´æ›´å¤æ‚ã€é«˜æ›²ç‡çš„å†³ç­–è¾¹ç•Œã€‚

+   æ›´ä½çš„ gamma å€¼å¯¼è‡´æ›´ç®€å•ã€ä½æ›²ç‡ã€å¹³æ»‘çš„å†³ç­–è¾¹ç•Œã€‚

å°½ç®¡åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¸¤ä¸ªå²©ç›¸ä¼¼ä¹è¢«æ­£ç¡®åˆ†ç±»ï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé«˜ gamma å’Œé«˜ C çš„ä¾‹å­ã€‚

## æœªå¯¹é¢„æµ‹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–çš„æ”¯æŒå‘é‡æœº

å¦‚æ‰¿è¯ºçš„é‚£æ ·ï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªæœªå¯¹é¢„æµ‹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–çš„æ¨¡å‹ã€‚

+   è¿™å°†æ˜¯æœ‰è¯´æ˜æ€§çš„ï¼Œå› ä¸ºåŸå§‹é¢„æµ‹ç‰¹å¾çš„èŒƒå›´å·®å¼‚å¾ˆå¤§ï¼

```py
order = 3; C = 0.01                                           # set the hyperparameters
svc_test1 = SVC(kernel='poly',degree=order,C=C,random_state = seed).fit(X_train[Xname],y_train) # fit with original features, not standardized
gamma = 1.0; C = 1.0                                          # set the hyperparameters
svc_test2 = SVC(kernel='rbf',gamma=gamma,C=C,random_state = seed).fit(X_train[Xname],y_train) # fit with original features, not standardized

plt.subplot(121)                                              # visualize the training data and models
visualize_SVM(svc_test1,X_train[Xname[0]],Xmin[0],Xmax[0],X_train[Xname[1]],Xmin[1],Xmax[1],y_train[yname],0.0,1.0,
                Xlabelunit[0],Xlabelunit[1],'Polynomial Support Vector Machine, Order = ' + str(order) + r', $C$ = ' + str(C),
                [0,1],['Shale','Sand'],binary_cmap,True)
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplot(122)
visualize_SVM(svc_test2,X_train[Xname[0]],Xmin[0],Xmax[0],X_train[Xname[1]],Xmin[1],Xmax[1],y_train[yname],0.0,1.0,
                Xlabelunit[0],Xlabelunit[1],r'RBF Support Vector Machine, $\gamma$ = ' + str(gamma) + r', $C$ = ' + str(C),
                [0,1],['Shale','Sand'],binary_cmap,True)
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.3, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/6284d1148a1bd492bb2d8429f0b4cd96.png)

å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

+   å…·æœ‰å¤šé¡¹å¼æ ¸çš„æ”¯æŒå‘é‡æœºåœ¨èŒƒå›´æœ€å¤§çš„ç‰¹å¾ä¸Šåˆ†å‰²ï¼Œå³å£°é˜»æŠ—ã€‚åœ¨èŒƒå›´éå¸¸å°çš„ç‰¹å¾ï¼ˆè¿™é‡ŒæŒ‡å­”éš™ç‡ï¼‰ä¸Šçš„å·®å¼‚å¯¹æ¨¡å‹æ²¡æœ‰æ˜¾è‘—å½±å“ã€‚

+   å…·æœ‰å¾„å‘åŸºå‡½æ•°çš„æ”¯æŒå‘é‡æœºåœ¨å£°é˜»æŠ—ä¸Šæœ‰è–„é¡µå²©å’Œç ‚å²©å±‚ã€‚

æˆ‘ä»¬å¿…é¡»æ ‡å‡†åŒ–æˆ‘ä»¬çš„é¢„æµ‹ç‰¹å¾ä»¥åº”ç”¨æ”¯æŒå‘é‡æœºã€‚

## è¶…å‚æ•°è°ƒä¼˜

è®©æˆ‘ä»¬ä½¿ç”¨å¸¦æœ‰åˆ†å±‚æ´—ç‰Œæ‹†åˆ†çš„æš´åŠ›ç½‘æ ¼æœç´¢æ¥è¿­ä»£å¤šä¸ªè¶…å‚æ•°å¹¶æ‰¾åˆ°æœ€ä½³æ¨¡å‹å¤æ‚åº¦ã€‚

+   **ç½‘æ ¼æœç´¢äº¤å‰éªŒè¯** - ä¸ºæ‰€æœ‰è¶…å‚æ•°çš„ç»„åˆæ„å»ºæ¨¡å‹

+   **åˆ†å±‚æ´—ç‰Œæ‹†åˆ†** - ç¡®ä¿åœ¨æ‹†åˆ†ä¸­ä¿æŒåˆ†ç±»æ¡ˆä¾‹çš„å¹³è¡¡ï¼Œå¹¶éšæœºåŒ–æ‹†åˆ†ä»¥ç¡®ä¿æ¨¡å‹æ¯æ¬¡éƒ½ä½¿ç”¨ç›¸åŒçš„æ•°æ®é¡ºåº

+   è­¦å‘Šï¼šåœ¨æ™®é€š PC ä¸Šè¿è¡Œæ­¤æ“ä½œå°†å¤§çº¦éœ€è¦ 2 åˆ†é’Ÿ

```py
C_range = np.logspace(-2, 7, 10)                              # set hyperparameter cases
gamma_range = np.logspace(-6, 3, 10)
param_grid = dict(gamma=gamma_range, C=C_range)               # store hyperparameter cases in a dictionary
cv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=seed) # instantiate the cross validation method
grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv,n_jobs=5).fit(X[Xsname],y) # brute force, full combinatorial search with cross validation 
scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range)) # retrieve average accuracy and shape as a 2D array for plot 
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥å¯è§†åŒ–æ‰€æœ‰è¶…å‚æ•°ç»„åˆçš„äº¤å‰éªŒè¯å‡†ç¡®ç‡ã€‚

+   æ³¨æ„ï¼Œè¾“å‡ºæ˜¯æ‰€æœ‰åˆ†å±‚æ´—ç‰Œæ‹†åˆ†çš„å¹³å‡å‡†ç¡®ç‡ï¼Œå…¶ä¸­å‡†ç¡®ç‡æ˜¯ï¼Œ

$$ å‡†ç¡®ç‡ = \frac{n_{\text{æ­£ç¡®åˆ†ç±»çš„}}}{n} $$

```py
plt.subplot(111)                                              # plot results of hyperparameter tuning 
im = plt.imshow(scores,vmin=0.6,vmax=0.95,cmap=cmap,alpha=1.0)
plt.xlabel(r'$\gamma$ Hyperparameter')
plt.ylabel(r'$C$ Hyperparameter')
cbar = plt.colorbar(im, orientation = 'vertical')
cbar.set_label('Average Classification Accuracy Over Splits', rotation=270, labelpad=20)
plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)
plt.yticks(np.arange(len(C_range)), C_range)
plt.title('SVC Hyperparameter Tuning, Cross Validation Accuracy');

plt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8, wspace=0.3, hspace=0.3); plt.show() 
```

![_images/c27ee9a9f09064a90ce366edd51e5c5e6693ed7e75b913239500a89b85839743.png](img/a7a596942f23b8c59494c817735ad48e.png)

æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°åœ¨ $C$ ä¸º 100 å’Œ $\gamma$ ä¸º 0.1 çš„ $C$ å’Œ $\gamma$ åŒºåŸŸå‘¨å›´å…·æœ‰æœ€ä½³æ¨¡å‹äº¤å‰éªŒè¯å‡†ç¡®ç‡ã€‚

## å¯è§†åŒ–é«˜ã€ä¸­ã€ä½æ€§èƒ½æ¨¡å‹

ç°åœ¨æˆ‘ä»¬å±•ç¤ºåŸºäºä¸Šè¿°æ¼”ç¤ºçš„éªŒè¯å‡†ç¡®ç‡çš„é«˜ã€ä¸­ã€ä½æ€§èƒ½æ¨¡å‹ç¤ºä¾‹ã€‚

+   æˆ‘ä»¬å°†ä½¿ç”¨ä¸Šå›¾ä¸­çš„å‚æ•°ç»„åˆï¼Œ$C$ å’Œ $\gamma$ï¼Œæ¥é€‰æ‹©å¹¶é‡æ–°è¿è¡Œæ¡ˆä¾‹ã€‚

```py
cases = ['Poor','OK','Good']                                  # selected hyperparameter cases for visualization
C_list = [100,100,1e6]
gamma_list = [100,10,0.01]
model_cases = []

for icase, case in enumerate(cases):                          # visualize the training data and model
    model_cases.append(SVC(kernel='rbf',C=C_list[icase],gamma=gamma_list[icase]).fit(X[Xsname],y)) # train on all the data
    plt.subplot(1,3,icase+1)                                  # visualize model cases and all data
    visualize_SVM(model_cases[icase],X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,
        Xslabelunit[0],Xslabelunit[1],r'RBF Support Vector Machine, ' + str(cases[icase]) + ' Model',[0,1],['Shale','Sand'],binary_cmap,False)
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.3, hspace=0.3); plt.show() 
```

![_images/72e5e586c8cb2a4e90eea81cd6276fdc2175a66c300652b076105e3cc3ba6079.png](img/e3b9d37c4cafc9bc078800f977947926.png)

é€šè¿‡ä»æˆ‘ä»¬çš„è¶…å‚æ•°è°ƒä¼˜äº¤å‰éªŒè¯å‡†ç¡®ç‡ä¸­é€‰æ‹©ä½ã€ä¸­ã€é«˜å‡†ç¡®ç‡è¶…å‚æ•°æ¡ˆä¾‹ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ä¸ªè¿‡åº¦æ‹Ÿåˆåˆ°è‰¯å¥½æ‹Ÿåˆæ¨¡å‹çš„è‰¯å¥½ç¤ºä¾‹ã€‚

## è¯„è®º

å¸Œæœ›æ‚¨è§‰å¾—è¿™ä¸€ç« æœ‰å¸®åŠ©ã€‚è¿˜æœ‰å¾ˆå¤šå¯ä»¥åšçš„å’Œè®¨è®ºçš„ï¼Œæˆ‘æœ‰å¾ˆå¤šèµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ï¼Œ

*è¿ˆå…‹å°”*

## ä½œè€…ï¼š

è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ *æ–°å‹æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ åœ°ä¸‹è§£å†³æ–¹æ¡ˆ*

è¿ˆå…‹å°”åœ¨åœ°ä¸‹å’¨è¯¢ã€ç ”ç©¶å’Œå¼€å‘æ–¹é¢æ‹¥æœ‰è¶…è¿‡ 17 å¹´çš„ç»éªŒï¼Œä»–å›åˆ°å­¦æœ¯ç•Œï¼Œå—å…¶æ•™å­¦çƒ­æƒ…å’Œå¯¹å¢å¼ºå·¥ç¨‹å¸ˆå’Œåœ°çƒç§‘å­¦å®¶åœ¨åœ°ä¸‹èµ„æºå¼€å‘ä¸­å½±å“çš„çƒ­æƒ…æ‰€é©±åŠ¨ã€‚

æ›´å¤šå…³äºè¿ˆå…‹å°”çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹é“¾æ¥ï¼š

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åœ°ç»Ÿè®¡å­¦åº”ç”¨ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ

å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚åŠ ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æ„Ÿå…´è¶£åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶ç»“åˆæ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºåŠå®è·µï¼Œå¼€å‘æ–°é¢–çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ä»¥å¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°æˆ‘ã€‚

æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°è´¨å­¦é™¢

## æ›´å¤šèµ„æºè¯·è®¿é—®ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python åœ°ç»Ÿè®¡å­¦åº”ç”¨ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## æ”¯æŒå‘é‡æœºçš„åŠ¨æœº

å½“ç»„é—´åˆ†ç¦»ä¸ä½³æ—¶ï¼Œæ˜¯ä¸€ç§è‰¯å¥½çš„åˆ†ç±»æ–¹æ³•çš„äºŒå…ƒåˆ†ç±»æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚

+   å°†åŸå§‹é¢„æµ‹ç‰¹å¾æŠ•å½±åˆ°æ›´é«˜ç»´çš„ç©ºé—´ï¼Œç„¶ååº”ç”¨çº¿æ€§ã€å¹³é¢æˆ–è¶…å¹³é¢ï¼Œ

$$ ğ‘“(ğ‘¥) = ğ‘¥^ğ‘‡ \beta +\beta_0 $$

å…¶ä¸­ $\beta$ æ˜¯ä¸€ä¸ªå‘é‡ï¼Œä¸ $\beta$ ä¸€èµ·æ˜¯è¶…å¹³é¢æ¨¡å‹å‚æ•°ï¼Œè€Œ $x$ æ˜¯é¢„æµ‹ç‰¹å¾çŸ©é˜µï¼Œæ‰€æœ‰è¿™äº›éƒ½åœ¨é«˜ç»´ç©ºé—´ä¸­ã€‚

+   $ğ‘“(ğ‘¥)$ ä¸å†³ç­–è¾¹ç•Œçš„ç¬¦å·è·ç¦»æˆæ­£æ¯”ï¼Œè€Œ $ğº(ğ‘¥)$ æ˜¯å†³ç­–è¾¹ç•Œçš„ä¾§é¢ï¼Œ$âˆ’$ è¡¨ç¤ºä¸€è¾¹ï¼Œ$+$ è¡¨ç¤ºå¦ä¸€è¾¹ï¼Œ$f(x) = 0$ åœ¨å†³ç­–è¾¹ç•Œä¸Šï¼Œ

$$ ğº(ğ‘¥)=\text{ğ‘ ğ‘–ğ‘”ğ‘›}\left( ğ‘“(ğ‘¥) \right) $$

æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼è¡¨ç¤ºçº¦æŸï¼Œæ¯ä¸ªç±»åˆ«çš„æ‰€æœ‰æ•°æ®éƒ½å¿…é¡»ä½äºè¾¹ç•Œçš„æ­£ç¡®ä¸€ä¾§ï¼Œ

$$ y_i \left( x_i^T \beta + \beta_0 \right) \geq 0 $$

å…¶ä¸­ï¼Œå¦‚æœç±»åˆ« $y_i$ ä¸º -1 æˆ– 1ï¼Œåˆ™æ­¤æ¡ä»¶æˆç«‹ã€‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå…è®¸æŸäº›è¯¯åˆ†ç±»çš„æ¨¡å‹ï¼Œ

$$ y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i $$

æˆ‘ä»¬å¼•å…¥äº†è¾¹è·çš„æ¦‚å¿µï¼Œ$ğ‘€$ï¼Œä»¥åŠè¾¹è·çš„è·ç¦»ï¼Œè¯¯å·®ä¸º $\xi_i$ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬çš„æŸå¤±å‡½æ•°è¡¨ç¤ºä¸ºï¼Œ

$$ \underset{\beta, \beta_0}{\text{min}} \left( \frac{1}{2MÂ²} + C \sum_{i=1}^N \xi_i \right) $$

æ»¡è¶³ï¼Œ$\xi_i \geq 0, \quad y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i$.

è¿™æ˜¯æ›´é«˜ç»´ç©ºé—´ä¸­çš„æ”¯æŒå‘é‡æœºæŸå¤±å‡½æ•°ï¼Œå…¶ä¸­ ğ›½,ğ›½_0 æ˜¯å¤šçº¿æ€§æ¨¡å‹å‚æ•°ã€‚

é€šè¿‡æ‰¾åˆ°æœ€å¤§åŒ–è¾¹è· $M$ å¹¶æœ€å°åŒ–è¯¯å·® $\sum_{i=1}^N \xi_i$ çš„æ¨¡å‹å‚æ•°æ¥è®­ç»ƒæ”¯æŒå‘é‡æœº

+   $ğ‘ª$ è¶…å‚æ•°å¯¹è¯¯å·®æ€»å’Œ $xi_ğ‘–$ è¿›è¡ŒåŠ æƒï¼Œæ›´é«˜çš„ $ğ¶$ å°†å¯¼è‡´è¾¹è· $M$ å‡å°‘ï¼Œå¹¶å¯¼è‡´è¿‡æ‹Ÿåˆ

+   è¾ƒå°çš„è¾¹è·ï¼Œä½¿ç”¨è¾ƒå°‘çš„æ•°æ®æ¥çº¦æŸè¾¹ç•Œï¼Œç§°ä¸ºæ”¯æŒå‘é‡

+   è®­ç»ƒæ•°æ®å¾ˆå¥½åœ°ä½äºè¾¹ç•Œçš„æ­£ç¡®ä¸€ä¾§æ²¡æœ‰å½±å“

è¿™é‡Œæœ‰ä¸€äº›æ”¯æŒå‘é‡æœºçš„å…³é”®æ–¹é¢ï¼Œ

+   è¢«ç§°ä¸ºæ”¯æŒå‘é‡æœºï¼Œè€Œä¸æ˜¯æœºå™¨ï¼Œå› ä¸ºä½¿ç”¨æ–°çš„æ ¸å¯ä»¥å¾—åˆ°æ–°çš„æœºå™¨

+   æœ‰è®¸å¤šæ ¸å¯ç”¨ï¼ŒåŒ…æ‹¬å¤šé¡¹å¼å’Œå¾„å‘åŸºå‡½æ•°

ä¸»è¦è¶…å‚æ•°æ˜¯ $C$ï¼Œè¡¨ç¤ºå¤šé¡¹å¼çš„æˆæœ¬

è¶…å‚æ•°ä¸æ ¸çš„é€‰æ‹©æœ‰å…³ï¼Œä¾‹å¦‚ï¼Œ

+   *å¤šé¡¹å¼* - å¤šé¡¹å¼é˜¶æ•°

+   *å¾„å‘åŸºå‡½æ•°* - $\gamma$ ä¸è®­ç»ƒæ•°æ®çš„è·ç¦»å½±å“æˆåæ¯”

## **æ ¸æŠ€å·§**

æˆ‘ä»¬å¯ä»¥åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­åŒ…å«åŸºå‡½æ•°å±•å¼€ï¼Œè€Œæ— éœ€å°†è®­ç»ƒæ•°æ®è½¬æ¢ä¸ºè¿™ä¸ªæ›´é«˜ç»´çš„ç©ºé—´ï¼Œ

$$ h(x) $$

æˆ‘ä»¬åªéœ€è¦é¢„æµ‹ç‰¹å¾çš„å†…ç§¯ï¼Œ

$$ h(x) \left( h(x') \right)^T = \langle h(x), h(x') \rangle $$

æˆ‘ä»¬ä¸éœ€è¦å˜æ¢ç©ºé—´ä¸­çš„å®é™…å€¼ï¼Œæˆ‘ä»¬åªéœ€è¦è¯¥å˜æ¢ç©ºé—´ä¸­æ‰€æœ‰å¯ç”¨è®­ç»ƒæ•°æ®ä¹‹é—´çš„â€œç›¸ä¼¼æ€§â€ï¼

+   æˆ‘ä»¬ä»…ä½¿ç”¨è®­ç»ƒæ•°æ®ä¹‹é—´çš„ç›¸ä¼¼æ€§çŸ©é˜µæ¥è®­ç»ƒæ”¯æŒå‘é‡æœºï¼Œè¿™äº›æ•°æ®å°†è¢«æŠ•å½±åˆ°æ›´é«˜ç»´çš„ç©ºé—´

+   æˆ‘ä»¬å®é™…ä¸Šä»æœªéœ€è¦è®¡ç®—æ›´é«˜ç»´ç©ºé—´ä¸­çš„è®­ç»ƒæ•°æ®å€¼

## åŠ è½½æ‰€éœ€çš„åº“

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸ Anaconda 3 ä¸€èµ·å®‰è£…ã€‚

```py
%matplotlib inline                                         
suppress_warnings = True
import os                                                     # to set current working directory 
import math                                                   # square root operator
import numpy as np                                            # arrays and matrix math
import scipy.stats as st                                      # statistical methods
import pandas as pd                                           # DataFrames
import matplotlib.pyplot as plt                               # for plotting
from matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter,NullLocator) # control of axes ticks
from matplotlib.colors import ListedColormap                  # custom color maps
import seaborn as sns                                         # for matrix scatter plots
from sklearn.svm import SVC                                   # support vector machine methods
from sklearn import metrics                                   # measures to check our models
from sklearn.metrics import confusion_matrix                  # for summarizing model performance
from sklearn.preprocessing import StandardScaler              # standardize the features
from sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,StratifiedShuffleSplit) # model tuning
from sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline
from sklearn import metrics                                   # measures to check our models
from sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation
from sklearn.model_selection import train_test_split          # train and test split
from IPython.display import display, HTML                     # custom displays
cmap = plt.cm.inferno                                         # default color bar, no bias
binary_cmap = ListedColormap(['grey', 'gold'])                # custom binary categorical colormap
plt.rc('axes', axisbelow=True)                                # grid behind plotting elements
if suppress_warnings == True:  
    import warnings                                           # suppress any warnings for this demonstration
    warnings.filterwarnings('ignore') 
seed = 13                                                     # random number seed for workflow repeatability 
```

å¦‚æœä½ é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½é¦–å…ˆéœ€è¦å®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ Windows ä¸Šæ‰“å¼€å‘½ä»¤çª—å£ï¼Œç„¶åè¾“å…¥â€˜python -m pip install [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£ï¼Œå¯ä»¥è·å¾—æ›´å¤šå¸®åŠ©ã€‚

## å£°æ˜å‡½æ•°

è®©æˆ‘ä»¬å®šä¹‰å‡ ä¸ªå‡½æ•°æ¥ç®€åŒ–ç»˜åˆ¶ç›¸å…³çŸ©é˜µå’Œå†³ç­–æ ‘å›å½’æ¨¡å‹çš„å¯è§†åŒ–ã€‚

```py
def comma_format(x, pos):
    return f'{int(x):,}'

def add_grid():
    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids
    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)
    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks 

def plot_CDF(data,color,alpha=1.0,lw=1,ls='solid',label='none'):
    cumprob = (np.linspace(1,len(data),len(data)))/(len(data)+1)
    plt.scatter(np.sort(data),cumprob,c=color,alpha=alpha,edgecolor='black',lw=lw,ls=ls,label=label,zorder=10)
    plt.plot(np.sort(data),cumprob,c=color,alpha=alpha,lw=lw,ls=ls,zorder=8)

def visualize_SVM(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,xlabel,ylabel,title,cat,label,cmap,plot_support): 
    xplot_step = (x_max - x_min)/300.0; yplot_step = (y_max - y_min)/300.0 # resolution of the model visualization
    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step), # set up the mesh
                     np.arange(y_min, y_max, yplot_step))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])          # predict with our trained model over the mesh
    Z = Z.reshape(xx.shape)
    cs = plt.contourf(xx, yy,Z,cmap=cmap,vmin=z_min,vmax=z_max,levels = 50,alpha=0.6) # plot the predictions
    for i in range(len(cat)):
        im = plt.scatter(xfeature[response==cat[i]],yfeature[response==cat[i]],s=None,c=response[response==cat[i]], 
                    marker=None, cmap=cmap, norm=None,vmin=z_min,vmax=z_max,alpha=0.8,linewidths=0.3, edgecolors="black")
    plt.scatter(-9999,-9999,marker='s',c = cat[0],label=label[0],cmap=cmap,vmin=z_min,vmax=z_max) # custom legend
    plt.scatter(-9999,-9999,marker='s',c = cat[1],label=label[1],cmap=cmap,vmin=z_min,vmax=z_max)
    plt.scatter(-999,-999,s=80,marker='o',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Train')
    if plot_support:                                          # modified from Jake VanderPlas's Python Data Science Handbook 
        sv = model.support_vectors_                           # retrieve the support vectors
        plt.scatter(sv[:, 0],sv[:, 1],s=3,linewidth=8,alpha = 0.6,facecolors='black',label='Support Vector');
    plt.legend(loc='upper right'); plt.title(title)                    
    plt.xlabel(xlabel); plt.ylabel(ylabel)
    plt.xlim([x_min,x_max]); plt.ylim([y_min,y_max]); add_grid()

def display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)
    html_str = ''
    for df in args:
        html_str += df.head().to_html()                       # Using .head() for the first few rows
    display(HTML(f'<div style="display: flex;">{html_str}</div>')) 
```

## è®¾ç½®å·¥ä½œç›®å½•

æˆ‘æ€»æ˜¯å–œæ¬¢è¿™æ ·åšï¼Œè¿™æ ·æˆ‘å°±ä¸ä¼šä¸¢å¤±æ–‡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ç®€åŒ–åç»­çš„è¯»å–å’Œå†™å…¥ï¼ˆé¿å…æ¯æ¬¡éƒ½åŒ…å«å®Œæ•´åœ°å€ï¼‰ã€‚

```py
#os.chdir("c:/PGE383")                                        # set the working directory 
```

## åŠ è½½æ•°æ®

è®©æˆ‘ä»¬åŠ è½½æä¾›çš„å¤šå…ƒã€ç©ºé—´æ•°æ®é›†â€˜12_sample_data.csvâ€™ã€‚å®ƒæ˜¯ä¸€ä¸ªé€—å·åˆ†éš”çš„æ–‡ä»¶ï¼ŒåŒ…å«ï¼š

+   X å’Œ Y åæ ‡ ($m$)

+   åœ°å±‚ 0 å’Œ 1

+   å­”éš™ç‡ï¼ˆåˆ†æ•°ï¼‰

+   æ¸—é€ç‡ ($mD$)

+   å£°é˜»æŠ— ($\frac{kg}{mÂ³} \cdot \frac{m}{s} \cdot 10Â³$)

æˆ‘ä»¬ä½¿ç”¨ pandas çš„â€˜read_csvâ€™å‡½æ•°å°†å…¶åŠ è½½åˆ°åä¸ºâ€˜dfâ€™çš„æ•°æ®æ¡†ä¸­ï¼Œç„¶åé¢„è§ˆä»¥ç¡®ä¿æ­£ç¡®åŠ è½½ã€‚

**Python å°è´´å£«ï¼šä½¿ç”¨åŒ…ä¸­çš„å‡½æ•°**åªéœ€è¾“å…¥æˆ‘ä»¬åœ¨å¼€å¤´å£°æ˜çš„åŒ…çš„æ ‡ç­¾ï¼š

```py
import pandas as pd 
```

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‘½ä»¤è®¿é—® pandas å‡½æ•°â€˜read_csvâ€™ï¼š

```py
pd.read_csv() 
```

ä½†æ˜¯è¯»å– csv æ–‡ä»¶éœ€è¦è¾“å…¥å‚æ•°ã€‚å…¶ä¸­æœ€é‡è¦çš„æ˜¯æ–‡ä»¶åã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–é»˜è®¤å‚æ•°éƒ½å¾ˆå¥½ã€‚å¦‚æœä½ æƒ³æŸ¥çœ‹æ­¤å‡½æ•°çš„æ‰€æœ‰å¯èƒ½å‚æ•°ï¼Œè¯·è®¿é—®[è¿™é‡Œ](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)ã€‚

+   æ–‡æ¡£æ€»æ˜¯å¾ˆæœ‰å¸®åŠ©

+   Python å‡½æ•°é€šå¸¸æœ‰å¾ˆå¤šçµæ´»æ€§ï¼Œè¿™æ˜¯é€šè¿‡ä½¿ç”¨å„ç§è¾“å…¥å‚æ•°å®ç°çš„ã€‚

æ­¤å¤–ï¼Œç¨‹åºè¿˜æœ‰ä¸€ä¸ªè¾“å‡ºï¼Œä¸€ä¸ªä»æ•°æ®åŠ è½½çš„ pandas DataFrameã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šä»£è¡¨è¯¥æ–°å¯¹è±¡çš„åç§°/å˜é‡ã€‚

```py
df = pd.read_csv("12_sample_data.csv") 
```

## æ ‡å‡†åŒ–é¢„æµ‹ç‰¹å¾

æ”¯æŒå‘é‡æœºæœ€å°åŒ–è¯¯å·®ï¼Œå³è®­ç»ƒæ•°æ®ä¸è¾¹ç•Œçš„è·ç¦»ã€‚å› æ­¤ï¼Œè¿™ç§æ–¹æ³•å¯¹é¢„æµ‹ç‰¹å¾çš„ç›¸å¯¹èŒƒå›´æ•æ„Ÿã€‚

+   å¦‚æœä¸€ä¸ªé¢„æµ‹ç‰¹å¾çš„èŒƒå›´å¤§å¾—å¤šï¼Œå®ƒå°†ä¸»å¯¼æ¨¡å‹ï¼Œæ¨¡å‹å°†åªåœ¨è¯¥ç‰¹å¾ä¸Šåˆ†ç¦»ï¼ç»“æœæ˜¯æ¨¡å‹ä¸è¯¥ç‰¹å¾æ­£äº¤ï¼Œå³ä»…åœ¨ç‰¹å¾ä¸Šåˆ†å‰²ã€‚

```py
df = pd.read_csv(r"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv")

yname = 'Facies'; Xname = ['Porosity','AI']                   # specify the predictor features (x2) and response feature (x1)
Xmin = [0.1,1500.0]; Xmax = [0.3,6500.0]                      # set minimums and maximums for visualization 
ymin = 0.0; ymax = 1.0
Xlabel = ['Porosity','Acoustic Impedance']; ylabel = 'Facies' # specify the feature labels for plotting
Xunit = ['Fraction',r'$\frac{kg}{mÂ³} \cdot \frac{m}{s} \cdot 10Â³$']; yunit = 'MCFPD'
Xlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']
ylabelunit = ylabel + ' (' + yunit + ')'

y = pd.DataFrame(df[yname])                                   # extract selected features as X and y DataFrames
X = df[Xname]

ysname = 's' + yname; Xsname = ['s' + element for element in Xname] # standardized predictor names
Xsmin = [-3.0,-3.0]; Xsmax = [3.0,3.0]                        # set minimums and maximums for standardized features
Xslabel = ['Standardized ' + element for element in Xlabel]   # standardized predictor names
Xsunit = ['S[' + element + ']' for element in Xunit]          # standardized predictor names
Xslabelunit = [Xslabel[0] + ' (' + Xsunit[0] + ')',Xslabel[1] + ' (' + Xsunit[1] + ')']

transform = StandardScaler();                                 # instantiate feature standardization method
Xs = transform.fit_transform(X)                               # standardize the data features to mean = 0, var = 1.0
X[Xsname] = Xs                                                # add standardized features to the predictor feature DataFrame 
```

## è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²

ä¸ºäº†æ–¹ä¾¿å’Œç®€å•ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn çš„éšæœºè®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²ã€‚

+   æˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„ random_state å‚æ•°ï¼Œä»¥ç¡®ä¿åŸå§‹å’Œæ ‡å‡†åŒ–ç‰¹å¾ä¸Šçš„è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²ç›¸åŒã€‚

+   æˆ‘æœ¬å¯ä»¥åªé€†å˜æ¢æ ‡å‡†åŒ–åçš„æ•°æ®ï¼ˆå‰§é€è­¦å‘Šï¼Œæˆ‘å°†å±•ç¤ºä¸æ ‡å‡†åŒ–çš„æ¨¡å‹å½±å“ï¼‰ã€‚

+   é€šå¸¸æˆ‘ä»¬ä¸éœ€è¦å¯¹é¢„æµ‹ç‰¹å¾è¿›è¡Œé€†å˜æ¢ï¼Œå¯¹äºæˆ‘ä»¬çš„é¢„æµ‹å·¥ä½œæµç¨‹æ¥è¯´ï¼Œé¢„æµ‹ç‰¹å¾æ˜¯ä¸€æ¬¡æ€§çš„ã€‚

```py
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split
df_train = pd.concat([X_train,y_train],axis=1)                # make one train and test DataFrame with both X and y
df_test = pd.concat([X_test,y_test],axis=1) 
```

## å¯è§†åŒ– DataFrame

åœ¨æˆ‘ä»¬æ„å»ºæ¨¡å‹ä¹‹å‰ï¼Œå¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯• DataFrame æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ£€æŸ¥ã€‚

+   è®¸å¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ è½½äº†é”™è¯¯çš„æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰åŠ è½½ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ©ç”¨â€˜headâ€™ DataFrame æˆå‘˜å‡½æ•°ï¼ˆæ ¼å¼æ•´æ´ã€ç¾è§‚ï¼Œè§ä¸‹æ–‡ï¼‰æ¥é¢„è§ˆã€‚

```py
print('       Training DataFrame          Testing DataFrame')
display_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display 
```

```py
 Training DataFrame          Testing DataFrame 
```

|  | å­”éš™ç‡ | AI | s å­”éš™ç‡ | sAI | å²©æ€§ |
| --- | --- | --- | --- | --- | --- |
| 340 | 0.204313 | 4373.187870 | 0.469659 | 0.788406 | 1 |
| 159 | 0.167316 | 3088.482947 | -0.698603 | -0.860390 | 0 |
| 315 | 0.219801 | 2983.326185 | 0.958720 | -0.995349 | 1 |
| 365 | 0.216819 | 2543.772663 | 0.864542 | -1.559474 | 1 |
| 385 | 0.191565 | 3670.457907 | 0.067120 | -0.113481 | 1 |
|  | å­”éš™ç‡ | AI | s å­”éš™ç‡ | sAI | å²©æ€§ |
| --- | --- | --- | --- | --- | --- |
| 72 | 0.139637 | 4747.274043 | -1.572630 | 1.268510 | 0 |
| 153 | 0.170732 | 4535.625583 | -0.590742 | 0.996879 | 0 |
| 258 | 0.244345 | 2696.102930 | 1.733756 | -1.363972 | 1 |
| 56 | 0.167125 | 5500.997419 | -0.704644 | 2.235841 | 0 |
| 303 | 0.216253 | 3959.934912 | 0.846677 | 0.258035 | 1 |

## è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡

åœ¨ DataFrames ä¸­ï¼Œæœ‰è®¸å¤šé«˜æ•ˆçš„æ–¹æ³•å¯ä»¥è®¡ç®—è¡¨æ ¼æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡ã€‚

+   describe å‘½ä»¤ä»¥æ•´æ´çš„æ•°æ®è¡¨å½¢å¼æä¾›è®¡æ•°ã€å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€‚

```py
print('            Training DataFrame                      Testing DataFrame') # custom function for side-by-side summary statistics
display_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) 
```

```py
 Training DataFrame                      Testing DataFrame 
```

|  | å­”éš™ç‡ | AI | s å­”éš™ç‡ | sAI | å²©æ€§ |
| --- | --- | --- | --- | --- | --- |
| count | 360.000000 | 360.000000 | 360.000000 | 360.000000 | 360.000000 |
| mean | 0.189150 | 3767.451286 | -0.009167 | 0.011001 | 0.602778 |
| std | 0.031636 | 786.394126 | 0.998983 | 1.009262 | 0.490004 |
| min | 0.117562 | 1746.387548 | -2.269691 | -2.582841 | 0.000000 |
| max | 0.261091 | 5957.162150 | 2.262519 | 2.821285 | 1.000000 |
|  | å­”éš™ç‡ | AI | s å­”éš™ç‡ | sAI | å²©æ€§ |
| --- | --- | --- | --- | --- | --- |
| count | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 |
| mean | 0.190311 | 3733.164755 | 0.027500 | -0.033003 | 0.658333 |
| std | 0.032014 | 763.117871 | 1.010903 | 0.979389 | 0.476257 |
| min | 0.131230 | 1961.600397 | -1.838105 | -2.306636 | 0.000000 |
| max | 0.256172 | 6194.573653 | 2.107198 | 3.125980 | 1.000000 |

æ£€æŸ¥æ±‡æ€»ç»Ÿè®¡æ˜¯ä»¶å¥½äº‹ã€‚

+   æ²¡æœ‰æ˜æ˜¾çš„é”™è¯¯

+   æ£€æŸ¥æ¯ä¸ªç‰¹å¾çš„å€¼èŒƒå›´ï¼Œä»¥è®¾ç½®å’Œè°ƒæ•´ç»˜å›¾é™åˆ¶ã€‚è§ä¸Šæ–‡ã€‚

## å¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²

è®©æˆ‘ä»¬é€šè¿‡ç›´æ–¹å›¾å’Œæ•£ç‚¹å›¾æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•çš„ä¸€è‡´æ€§å’Œè¦†ç›–ç‡ã€‚

+   æ£€æŸ¥ä»¥ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•æ¶µç›–äº†å¯èƒ½çš„ç‰¹å¾ç»„åˆèŒƒå›´

+   ç¡®ä¿æµ‹è¯•æ¡ˆä¾‹ä¸ä¼šè¶…å‡ºè®­ç»ƒæ•°æ®èŒƒå›´è¿›è¡Œå¤–æ¨

```py
nbins = 20                                                    # number of histogram bins

plt.subplot(231)                                              # predictor feature #1 histogram
freq1,_,_ = plt.hist(x=X_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=X_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[0]); add_grid()  
plt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   

plt.subplot(232)                                              # predictor feature #2 histogram
freq1,_,_ = plt.hist(x=X_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=X_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[1]); add_grid()  
plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))
plt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   

plt.subplot(233)                                              # predictor features #1 and #2 scatter plot
plt.scatter(X_train[Xname[0]],X_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(X_test[Xname[0]],X_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.title(Xlabel[0] + ' vs ' +  Xlabel[1])
plt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])
plt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplot(234)                                              # predictor feature #1 histogram
freq1,_,_ = plt.hist(x=X_train[Xsname[0]],weights=None,bins=np.linspace(Xsmin[0],Xsmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=X_test[Xsname[0]],weights=None,bins=np.linspace(Xsmin[0],Xsmax[0],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xslabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xslabel[0]); add_grid()  
plt.xlim([Xsmin[0],Xsmax[0]]); plt.legend(loc='upper right')   

plt.subplot(235)                                              # predictor feature #2 histogram
freq1,_,_ = plt.hist(x=X_train[Xsname[1]],weights=None,bins=np.linspace(Xsmin[1],Xsmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='darkorange',density=False,label='Train')
freq2,_,_ = plt.hist(x=X_test[Xsname[1]],weights=None,bins=np.linspace(Xsmin[1],Xsmax[1],nbins),alpha = 0.6,
                     edgecolor='black',color='red',density=False,label='Test')
max_freq = max(freq1.max()*1.10,freq2.max()*1.10)
plt.xlabel(Xslabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xslabel[1]); add_grid()  
plt.xlim([Xsmin[1],Xsmax[1]]); plt.legend(loc='upper right')   

plt.subplot(236)                                              # predictor features #1 and #2 scatter plot
plt.scatter(X_train[Xsname[0]],X_train[Xsname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')
plt.scatter(X_test[Xsname[0]],X_test[Xsname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')
plt.title(Xslabel[0] + ' vs ' +  Xslabel[1])
plt.xlabel(Xslabelunit[0]); plt.ylabel(Xslabelunit[1])
plt.legend(); add_grid(); plt.xlim([Xsmin[0],Xsmax[0]]); plt.ylim([Xsmin[1],Xsmax[1]])

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=1.6, wspace=0.3, hspace=0.25)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![_images/c96849d9ac15251e0c63378a9f425aa0f734d87c0a250b8ce44f189349db09c5.png](img/acefe3d0111ef4c4aadff2177c5a7676.png)

æœ‰æ—¶æˆ‘å‘ç°é€šè¿‡æŸ¥çœ‹ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰è€Œä¸æ˜¯ç›´æ–¹å›¾æ¥æ¯”è¾ƒåˆ†å¸ƒæ›´æ–¹ä¾¿ã€‚

+   æˆ‘ä»¬é¿å…ä»»æ„é€‰æ‹©ç›´æ–¹å›¾æŸ±çŠ¶å¤§å°ï¼Œå› ä¸ºç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰ä¸æ•°æ®åˆ†è¾¨ç‡ä¸€è‡´ã€‚

```py
plt.subplot(131)                                              # predictor feature #1 CDF
plot_CDF(X_train[Xsname[0]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(X_test[Xsname[0]],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(Xslabelunit[0]); plt.xlim(Xsmin[0],Xsmax[0]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(Xslabel[0] + ' Train and Test CDFs')

plt.subplot(132)                                              # predictor feature #2 CDF
plot_CDF(X_train[Xsname[1]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')
plot_CDF(X_test[Xsname[1]],'red',alpha=0.6,lw=1,ls='solid',label='Test')
plt.xlabel(Xslabelunit[1]); plt.xlim(Xsmin[1],Xsmax[1]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')
plt.title(Xslabel[1] + ' Train and Test CDFs')

plt.subplot(133)                                              # categorical response feature grouped histogram
plt.bar([-0.125],len(y_train[yname][y_train[yname]==0]),width=0.25,color=['darkorange'],edgecolor='black',label='Train')
plt.bar([0.125],len(y_test[yname][y_test[yname]==0]),width=0.25,color=['red'],edgecolor='black',label='Test')
plt.bar([0.875],len(y_train[yname][y_train[yname]==1]),width=0.25,color=['darkorange'],edgecolor='black')
plt.bar([1.125],len(y_test[yname][y_test[yname]==1]),width=0.25,color=['red'],edgecolor='black')
x_ticks = [0, 1]; x_labels = ['Shale', 'Sand']; plt.xticks(x_ticks,x_labels)
plt.ylim([0.,250.0]); plt.xlim([-0.5,1.5]); add_grid(); plt.legend(loc='upper left')
ax = plt.gca(); ax.xaxis.set_minor_locator(NullLocator())
plt.title(ylabel + ' Train and Test Categorical Response Frequencies'); plt.xlabel('Facies'); plt.ylabel('Frequency')

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=0.8, wspace=0.3, hspace=0.2)
#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') 
plt.show() 
```

![_images/7f4199220c2686c54fc8e8ceea1351ff88ab7bba2333e7441c6d268c034adbed.png](img/2bbf0a8662041cb7a556a37197370bc0.png)

è¿™çœ‹èµ·æ¥ä¸é”™ï¼Œ

+   åˆ†å¸ƒè¡¨ç°è‰¯å¥½ï¼Œæˆ‘ä»¬æ— æ³•è§‚å¯Ÿåˆ°æ˜æ˜¾çš„ç¼ºå£ã€å¼‚å¸¸å€¼æˆ–æˆªæ–­

+   æµ‹è¯•å’Œè®­ç»ƒæ¡ˆä¾‹å…·æœ‰ç›¸ä¼¼çš„è¦†ç›–èŒƒå›´

+   ç±»åˆ«å“åº”çš„ç›¸å¯¹é¢‘ç‡åœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä¸­ç›¸ä¼¼ï¼Œå³ï¼Œè‰¯å¥½çš„è®­ç»ƒå’Œæµ‹è¯•å¹³è¡¡ã€‚

## å¯è§†åŒ–é¢„æµ‹ç‰¹å¾ç©ºé—´

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªç®€åŒ–çš„å›¾è¡¨æ¥å¯è§†åŒ– 2D é¢„æµ‹ç‰¹å¾ç©ºé—´ä¸­çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚

+   æˆ‘ä»¬æå‡ºé—®é¢˜ï¼Œæˆ‘ä»¬èƒ½å¦å¯¹åˆ†ç±»è¾¹ç•Œè¿›è¡Œå»ºæ¨¡ï¼Ÿæ•°æ®é‡å å¾ˆå¤šå—ï¼Ÿè¾¹ç•Œç®€å•ï¼ˆå³ï¼Œçº¿æ€§ï¼‰è¿˜æ˜¯æ›´å¤æ‚ï¼Ÿ

```py
plt.subplot(111)                                              # plot train and test data in predictor feature space
plt.scatter(X_train[Xsname[0]][y_train[yname]==1],X_train[Xsname[1]][y_train[yname]==1],s=80,
            marker='o',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,label='Sand')
plt.scatter(X_train[Xsname[0]][y_train[yname]==0],X_train[Xsname[1]][y_train[yname]==0],s=80,
            marker='o',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,label='Shale')

plt.scatter(X_test[Xsname[0]][y_test[yname]==1],X_test[Xsname[1]][y_test[yname]==1],s=80,
            marker='s',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,)
plt.scatter(X_test[Xsname[0]][y_test[yname]==0],X_test[Xsname[1]][y_test[yname]==0],s=80,
            marker='s',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,)

plt.scatter([-999],[-999],s=80,marker='o',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Train')
plt.scatter([-999],[-999],s=80,marker='s',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Test')

plt.legend(loc = 'upper right')
plt.title('Training and Testing ' + ylabel + ' vs. ' + Xslabel[1] + ' and ' + Xlabel[0])
plt.xlabel(Xslabelunit[0]); plt.ylabel(Xslabelunit[1]); add_grid(); plt.xlim([Xsmin[0],Xsmax[0]]); plt.ylim([Xsmin[1],Xsmax[1]])
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/56dc165df133e2ddbce54c0beda4915c.png)

è¿™å°†æ˜¯ä¸€ä¸ªå›°éš¾çš„åˆ†ç±»ï¼Œ

+   å½“ç„¶æœ‰å¾ˆå¤šé‡å 

+   è¾¹ç•Œå¯èƒ½æ˜¯éçº¿æ€§çš„ã€‚

ä½†æ˜¯ï¼Œæœ‰å¥½æ¶ˆæ¯ï¼Œ

+   æ€»ä½“è€Œè¨€ï¼Œè®­ç»ƒå’Œæµ‹è¯•è¦†ç›–èŒƒå›´çœ‹èµ·æ¥ä¸é”™ï¼Œæ³¨æ„æœ‰å‡ ä¸ªæµ‹è¯•æ¡ˆä¾‹å°†æµ‹è¯•æ¨¡å‹å¤–æ¨ã€‚

+   æ”¯æŒå‘é‡æœºè¢«è®¾è®¡ç”¨æ¥å¤„ç†è¿™ç§é‡å ï¼

## çº¿æ€§æ ¸æ”¯æŒå‘é‡æœºæ¨¡å‹

è®©æˆ‘ä»¬ä»ç®€å•å¼€å§‹ï¼Œåœ¨æˆ‘ä»¬çš„ç‰¹å¾ç©ºé—´ä¸Šè®­ç»ƒå¹¶å¯è§†åŒ–çº¿æ€§æ”¯æŒå‘é‡æœºæ¨¡å‹ã€‚

+   è¿™å°†ä¸º 0 å’Œ 1 ç›¸æ€§æä¾›ä¸€ä¸ªçº¿æ€§ç©ºé—´åˆ†ç±»æ¨¡å‹ï¼Œä½œä¸ºäººå·¥æ™ºèƒ½å’Œå­”éš™ç‡çš„å‡½æ•°ã€‚

æˆ‘ä»¬ä½¿ç”¨ scikit-learn å‡½æ•° $SVC$ æ¥å®ç°æ”¯æŒå‘é‡æœºï¼š

```py
svm_linear = SVC() 
```

å‚æ•°åŒ…æ‹¬ï¼š

+   **æ ¸** åº”ç”¨åˆ°æ•°æ®ä»¥æŠ•å½±åˆ°å¯èƒ½æ›´é«˜ç»´ç©ºé—´ä¸­çš„æ ¸ç±»å‹

+   **$C$** è¯¯åˆ†ç±»æƒ©ç½š

+   **random_state** éšæœºæ•°ç”¨äºéšæœºæ´—ç‰Œæ•°æ®ä»¥è¿›è¡Œæ¦‚ç‡ä¼°è®¡

ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼Œ

```py
svm_linear.fit() 
```

ä½¿ç”¨è®­ç»ƒæ•°æ®é›†æ¥è®­ç»ƒæ¨¡å‹ã€‚

fit å‡½æ•°çš„è¾“å…¥åŒ…æ‹¬ï¼š

+   **X** - è®­ç»ƒæ•°æ®é›†çš„é¢„æµ‹ç‰¹å¾ $n \times m$ æ•°ç»„

+   **y** - è®­ç»ƒæ•°æ®é›†çš„å“åº”ç‰¹å¾ $n \times 1$ æ•°ç»„

è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨ 2 ä¸ªä¸åŒçš„ $C$ æƒ©ç½šè¶…å‚æ•°æ¥å¯è§†åŒ–è¯¯åˆ†ç±»æƒ©ç½šçš„å½±å“ã€‚

```py
C1_list = [0.01,100]                                          # set hyperparameters
SVM1_list = []

for C in C1_list:                                             # train the models
    SVM1_list.append(SVC(kernel = 'linear',C = C, random_state = seed).fit(X_train[Xsname],y_train)) # instantiate and train 
```

çœ‹èµ·æ¥è¿è¡Œå¾—å¾ˆå¥½ï¼è®©æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„æ–¹ä¾¿çš„å¯è§†åŒ–å‡½æ•°æ¥å¯è§†åŒ–ç»“æœã€‚

```py
for iC, C in enumerate(C1_list):                              # visualize the training data and model
    plt.subplot(1,2,iC+1)
    visualize_SVM(SVM1_list[iC],X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,
                Xslabelunit[0],Xslabelunit[1],r'Training Data and Linear Support Vector Machine, $C$ = ' + str(C),[0,1],['Shale','Sand'],
                binary_cmap,True)
plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() 
```

![å›¾ç‰‡](img/f9599d00749b4f00b75cb1eeb40f7f47.png)

ä¸Šå›¾æ˜¾ç¤ºäº†çº¿æ€§æ ¸æ”¯æŒå‘é‡æœºåˆ†ç±»æ¨¡å‹ã€è®­ç»ƒæ•°æ®é›†ä»¥åŠç”¨ç²—åœ†åœˆè¡¨ç¤ºçš„ç»“æœæ”¯æŒå‘é‡ã€‚

+   çº¿æ€§æ ¸ä»…æä¾›ç›´çº¿å†³ç­–è¾¹ç•Œã€‚

+   è°ƒæ•´æ¨¡å‹ä»¥é€‚åº”æ›´å¤æ‚çš„æƒ…å†µæ˜¯å›°éš¾çš„ã€‚

æ³¨æ„ï¼Œéšç€ C å€¼çš„å¢åŠ ï¼Œé”™è¯¯æ€»å’Œçš„åŠ æƒï¼Œæˆ‘ä»¬å¾—åˆ°æ›´å°çš„é—´éš”ã€‚çº¿æ€§æ¨¡å‹ä½¿ç”¨è¾ƒå°‘çš„æ”¯æŒå‘é‡ï¼ˆè®­ç»ƒæ•°æ®åœ¨é—´éš”å†…ï¼‰ã€‚è®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹å…³äºç®€å•å’Œå¤æ‚æ¨¡å‹ã€æ½œåœ¨æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆä»¥åŠæ¨¡å‹åå·®å’Œæ–¹å·®æƒè¡¡çš„è¯¥è¶…å‚æ•°çš„ç«¯æˆå‘˜ã€‚

+   **ç®€å•æ¨¡å‹/æ¬ æ‹Ÿåˆæ¨¡å‹** - å½“ C å€¼è¾ƒå°æ—¶ï¼Œåˆ†ç±»å™¨å¯¹è¯¯åˆ†ç±»çš„æ•°æ®ç‚¹æ›´åŠ å®½å®¹ï¼ˆæ›´é«˜çš„æ¨¡å‹åå·®ï¼Œæ›´ä½çš„æ¨¡å‹æ–¹å·®ï¼‰ã€‚

+   **å¤æ‚æ¨¡å‹** > **è¿‡æ‹Ÿåˆæ¨¡å‹** - å½“ C å€¼è¾ƒå¤§æ—¶ï¼Œåˆ†ç±»å™¨å¯¹è¯¯åˆ†ç±»çš„æ•°æ®ç‚¹æ›´æ•æ„Ÿï¼ˆæ¨¡å‹åå·®è¾ƒä½ï¼Œæ¨¡å‹æ–¹å·®è¾ƒé«˜ï¼‰ã€‚

æ¢å¥è¯è¯´ï¼Œ$C$æ˜¯ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œå°±åƒå²­å›å½’ä¸­çš„æ”¶ç¼©é¡¹ä¸€æ ·ã€‚è®©æˆ‘ä»¬å°è¯•ä¸€äº›å…·æœ‰ä¸åŒæ ¸çš„æ›´çµæ´»çš„åˆ†ç±»å™¨ï¼Œä»¥ä¾¿æˆ‘ä»¬æ›´å¥½åœ°è§‚å¯Ÿè¿™ä¸€ç‚¹ã€‚

## æ”¯æŒå‘é‡æœºæ¨¡å‹ï¼ˆå¤šé¡¹å¼æ ¸ï¼‰

å¤šé¡¹å¼æ ¸å®šä¹‰ä¸º

$$ K(x,xâ€™) = (x^Tx)^d, $$

å…¶ä¸­$d$æ˜¯å¤šé¡¹å¼çš„æ¬¡æ•°ã€‚

è¶…å‚æ•°$degree$æ˜¯å¤šé¡¹å¼æ ¸å‡½æ•°çš„é˜¶æ•°ã€‚

å¦‚å‰æ‰€è¿°ï¼Œè®©æˆ‘ä»¬å°è¯•ä¸åŒçš„$C$ï¼Œå³è¯¯å·®æƒ©ç½šï¼Œä½¿ç”¨å•ä¸ªå¤šé¡¹å¼é˜¶æ•°è§‚å¯Ÿç»“æœã€‚

```py
C2_list = [0.01,0.1,1,10]                                     # set hyperparameters
order = np.full((len(C2_list)),3)       
SVM2_list = []

for iC, C in enumerate(C2_list):                              # train the model and visualize the training data and model
    SVM2_list.append(SVC(kernel = 'poly',degree=order[iC],C = C,random_state = seed).fit(X_train[Xsname],y_train)) # instantiate and train
    plt.subplot(2,2,iC+1)
    visualize_SVM(SVM2_list[iC-1],X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,
                Xslabelunit[0],Xslabelunit[1],'Polynomial Support Vector Machine, Order = ' + str(order[iC]) + r', $C$ = ' + str(C),
                [0,1],['Shale','Sand'],binary_cmap,True)
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=1.6, wspace=0.3, hspace=0.3); plt.show() 
```

![å›¾ç‰‡é“¾æ¥](img/00a2d3685850074a3a660b3f3a1259f2.png)

éšç€ C è¶…å‚æ•°çš„å¢åŠ ï¼Œè¾¹ç•Œç¼©å°ï¼Œæ”¯æŒå‘é‡æ•°é‡å‡å°‘ï¼Œæ¨¡å‹å¤æ‚æ€§å¢åŠ ã€‚

## æ”¯æŒå‘é‡æœºæ¨¡å‹ï¼ˆå¾„å‘åŸºå‡½æ•°æ ¸ï¼‰

å¾„å‘åŸºå‡½æ•°ï¼ˆRBFï¼‰æ˜¯ SVC ä¸­å¸¸ç”¨çš„å¦ä¸€ä¸ªæ ¸ï¼š

$$ K(x,xâ€™) = e^{- \gamma ||x-xâ€™||Â²}, $$

å…¶ä¸­$\||x-xâ€™\|Â²$æ˜¯ä¸¤ä¸ªæ•°æ®ç‚¹ x å’Œ xâ€™ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»çš„å¹³æ–¹ã€‚

é«˜æ–¯æ ¸æ˜¯ RBF çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå…¶ä¸­ï¼š

$$ K(x,xâ€™) = e^{- \frac {||x-xâ€™||Â²} {2 \sigmaÂ²}}. $$

é€šè¿‡æ”¹å˜$\gamma$å’Œ C çš„å€¼ï¼Œå…·æœ‰ RBF æ ¸çš„åˆ†ç±»å™¨å¯ä»¥è¢«è°ƒæ•´ã€‚

$\gamma$å¯ä»¥è¢«è®¤ä¸ºæ˜¯æ ¸çš„æ‰©æ•£ã€‚

+   å½“$\gamma$è¾ƒä½æ—¶ï¼Œå†³ç­–è¾¹ç•Œçš„æ›²ç‡è¾ƒä½ï¼Œå¯¼è‡´å†³ç­–åŒºåŸŸè¾ƒå®½ï¼ˆæ–¹å·®ä½ï¼Œåå·®é«˜ï¼‰ï¼Œå¤æ‚æ€§ä½

+   $\gamma$å‚æ•°å¯ä»¥è§£é‡Šä¸ºæ¨¡å‹é€‰å®šçš„æ”¯æŒå‘é‡çš„å½±å“åŠå¾„çš„å€’æ•°ï¼Œå³ä½ä¼½é©¬å€¼ä½¿æ¨¡å‹æ›´å¹³æ»‘åœ°æ•´åˆæ›´å¤šæ•°æ®ã€‚

```py
C3_list = [1e-1, 1, 1e2]                                      # set hyperparameters
gamma1_list = [1e-1, 1, 1e1]

index= 1
for C in C3_list:
    for gamma in gamma1_list:                                 # train the models, visualize the training data and models
        svc = SVC(kernel='rbf',gamma=gamma,C=C,random_state = seed).fit(X_train[Xsname],y_train) # instantiate and train
        plt.subplot(3,3,index)
        visualize_SVM(svc,X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,
                Xslabelunit[0],Xslabelunit[1],r'RBF Support Vector Machine, $\gamma$ = ' + str(gamma) + r', $C$ = ' + str(C),
                [0,1],['Shale','Sand'],binary_cmap,True)
        index = index + 1

plt.subplots_adjust(left=0.0, bottom=0.0, right=2.4, top=2.4, wspace=0.3, hspace=0.3); plt.show() 
```

![å›¾ç‰‡é“¾æ¥](img/6f78098f6348e837aa484bf3c227abf8.png)

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ­£åˆ™åŒ–ä¸ C è¶…å‚æ•°çš„å½±å“éå¸¸æ˜æ˜¾ï¼Œ

+   è¾ƒé«˜çš„ Cï¼Œè¾ƒå°çš„è¾¹ç•Œï¼Œè¾ƒå°‘çš„æ”¯æŒå‘é‡ï¼Œå€¾å‘äºè¿‡æ‹Ÿåˆ

+   è¾ƒä½çš„ Cï¼Œè¾ƒå¤§çš„è¾¹ç•Œï¼Œæ›´å¤šçš„æ”¯æŒå‘é‡ï¼Œå€¾å‘äºæ¬ æ‹Ÿåˆ

åœ¨è¿™ä¸ªæƒ…å†µä¸‹ï¼Œ$\gamma$çš„å½±å“ä¹Ÿéå¸¸æ˜æ˜¾ï¼Œ

+   é«˜ä¼½é©¬å€¼å¯¼è‡´æ›´å¤æ‚ã€é«˜æ›²ç‡çš„å†³ç­–è¾¹ç•Œ

+   ä½ä¼½é©¬å€¼å¯¼è‡´æ›´ç®€å•ã€ä½æ›²ç‡çš„å¹³æ»‘å†³ç­–è¾¹ç•Œ

å°½ç®¡åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¸¤ç§å²©ç›¸ä¼¼ä¹è¢«æ­£ç¡®åˆ†ç±»ï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé«˜ä¼½é©¬å’Œé«˜ C çš„ä¾‹å­ã€‚

## æ”¯æŒå‘é‡æœºï¼ˆæœªæ ‡å‡†åŒ–é¢„æµ‹ç‰¹å¾ï¼‰

å¦‚æ‰¿è¯ºçš„é‚£æ ·ï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªæ²¡æœ‰æ ‡å‡†åŒ–é¢„æµ‹ç‰¹å¾çš„æ¨¡å‹ã€‚

+   è¿™å°†å¾ˆæœ‰è¯´æ˜æ€§ï¼Œå› ä¸ºåŸå§‹é¢„æµ‹ç‰¹å¾çš„èŒƒå›´å·®å¼‚å¾ˆå¤§ï¼

```py
order = 3; C = 0.01                                           # set the hyperparameters
svc_test1 = SVC(kernel='poly',degree=order,C=C,random_state = seed).fit(X_train[Xname],y_train) # fit with original features, not standardized
gamma = 1.0; C = 1.0                                          # set the hyperparameters
svc_test2 = SVC(kernel='rbf',gamma=gamma,C=C,random_state = seed).fit(X_train[Xname],y_train) # fit with original features, not standardized

plt.subplot(121)                                              # visualize the training data and models
visualize_SVM(svc_test1,X_train[Xname[0]],Xmin[0],Xmax[0],X_train[Xname[1]],Xmin[1],Xmax[1],y_train[yname],0.0,1.0,
                Xlabelunit[0],Xlabelunit[1],'Polynomial Support Vector Machine, Order = ' + str(order) + r', $C$ = ' + str(C),
                [0,1],['Shale','Sand'],binary_cmap,True)
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplot(122)
visualize_SVM(svc_test2,X_train[Xname[0]],Xmin[0],Xmax[0],X_train[Xname[1]],Xmin[1],Xmax[1],y_train[yname],0.0,1.0,
                Xlabelunit[0],Xlabelunit[1],r'RBF Support Vector Machine, $\gamma$ = ' + str(gamma) + r', $C$ = ' + str(C),
                [0,1],['Shale','Sand'],binary_cmap,True)
plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.3, hspace=0.3); plt.show() 
```

![å›¾ç‰‡é“¾æ¥](img/6284d1148a1bd492bb2d8429f0b4cd96.png)

å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

+   å…·æœ‰å¤šé¡¹å¼æ ¸çš„æ”¯æŒå‘é‡æœºåœ¨å…·æœ‰æœ€å¤§èŒƒå›´çš„ç‰¹æ€§ï¼ˆå£°é˜»æŠ—ï¼‰ä¸Šåˆ†å‰²ã€‚éå¸¸å°çš„èŒƒå›´ç‰¹æ€§ï¼ˆè¿™é‡Œçš„å­”éš™ç‡ï¼‰çš„å·®å¼‚å¯¹æ¨¡å‹æ²¡æœ‰æ˜¾è‘—å½±å“ã€‚

+   åŸºäºå¾„å‘åŸºå‡½æ•°çš„æ”¯æŒå‘é‡æœºåœ¨å£°é˜»æŠ—å±‚ä¸Šå…·æœ‰è–„é¡µå²©å’Œç ‚å²©å±‚ã€‚

æˆ‘ä»¬å¿…é¡»æ ‡å‡†åŒ–æˆ‘ä»¬çš„é¢„æµ‹ç‰¹å¾æ‰èƒ½åº”ç”¨æ”¯æŒå‘é‡æœºã€‚

## è¶…å‚æ•°è°ƒæ•´

è®©æˆ‘ä»¬ä½¿ç”¨ç©·ä¸¾ç½‘æ ¼æœç´¢å’Œåˆ†å±‚æ´—ç‰Œåˆ†å‰²æ¥è¿­ä»£å¤šä¸ªè¶…å‚æ•°ï¼Œå¹¶æ‰¾åˆ°æœ€ä½³æ¨¡å‹å¤æ‚åº¦ã€‚

+   **ç½‘æ ¼æœç´¢äº¤å‰éªŒè¯** - ä¸ºè¶…å‚æ•°çš„å®Œæ•´ç»„åˆæ„å»ºæ¨¡å‹

+   **åˆ†å±‚æ´—ç‰Œåˆ†å‰²** - ç¡®ä¿åœ¨åˆ†å‰²ä¸­ä¿ç•™åˆ†ç±»æ¡ˆä¾‹çš„å¹³è¡¡ï¼Œå¹¶éšæœºåŒ–åˆ†å‰²ä»¥ç¡®ä¿æ¨¡å‹æ¯æ¬¡ä½¿ç”¨ç›¸åŒçš„æ•°æ®é¡ºåº

+   è­¦å‘Šï¼šåœ¨æ™®é€š PC ä¸Šè¿è¡Œæ­¤æ“ä½œå°†å¤§çº¦éœ€è¦ 2 åˆ†é’Ÿ

```py
C_range = np.logspace(-2, 7, 10)                              # set hyperparameter cases
gamma_range = np.logspace(-6, 3, 10)
param_grid = dict(gamma=gamma_range, C=C_range)               # store hyperparameter cases in a dictionary
cv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=seed) # instantiate the cross validation method
grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv,n_jobs=5).fit(X[Xsname],y) # brute force, full combinatorial search with cross validation 
scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range)) # retrieve average accuracy and shape as a 2D array for plot 
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥å¯è§†åŒ–æ‰€æœ‰è¶…å‚æ•°ç»„åˆçš„äº¤å‰éªŒè¯å‡†ç¡®ç‡ã€‚

+   æ³¨æ„ï¼Œè¾“å‡ºæ˜¯æ‰€æœ‰åˆ†å±‚æ´—ç‰Œåˆ†å‰²çš„å¹³å‡å‡†ç¡®ç‡ï¼Œå…¶ä¸­å‡†ç¡®ç‡æ˜¯ï¼Œ

$$ å‡†ç¡®ç‡ = \frac{\text{æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ•°}}{\text{æ€»æ ·æœ¬æ•°}} $$

```py
plt.subplot(111)                                              # plot results of hyperparameter tuning 
im = plt.imshow(scores,vmin=0.6,vmax=0.95,cmap=cmap,alpha=1.0)
plt.xlabel(r'$\gamma$ Hyperparameter')
plt.ylabel(r'$C$ Hyperparameter')
cbar = plt.colorbar(im, orientation = 'vertical')
cbar.set_label('Average Classification Accuracy Over Splits', rotation=270, labelpad=20)
plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)
plt.yticks(np.arange(len(C_range)), C_range)
plt.title('SVC Hyperparameter Tuning, Cross Validation Accuracy');

plt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8, wspace=0.3, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/a7a596942f23b8c59494c817735ad48e.png)

æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ° $C$ å¤§çº¦ä¸º 100 å’Œ $\gamma$ å¤§çº¦ä¸º 0.1 çš„æœ€ä½³æ¨¡å‹äº¤å‰éªŒè¯å‡†ç¡®ç‡åŒºåŸŸã€‚

## å¯è§†åŒ–é«˜ã€ä¸­ã€ä½æ€§èƒ½æ¨¡å‹

ç°åœ¨æˆ‘ä»¬å±•ç¤ºåŸºäºä¸Šè¿°æ¼”ç¤ºçš„éªŒè¯å‡†ç¡®ç‡çš„é«˜ã€ä¸­ã€ä½æ€§èƒ½æ¨¡å‹ç¤ºä¾‹ã€‚

+   æˆ‘ä»¬å°†ä½¿ç”¨ä¸Šå›¾ä¸­çš„å‚æ•°ç»„åˆ $C$ å’Œ $\gamma$ æ¥é€‰æ‹©å¹¶é‡æ–°è¿è¡Œæ¡ˆä¾‹ã€‚

```py
cases = ['Poor','OK','Good']                                  # selected hyperparameter cases for visualization
C_list = [100,100,1e6]
gamma_list = [100,10,0.01]
model_cases = []

for icase, case in enumerate(cases):                          # visualize the training data and model
    model_cases.append(SVC(kernel='rbf',C=C_list[icase],gamma=gamma_list[icase]).fit(X[Xsname],y)) # train on all the data
    plt.subplot(1,3,icase+1)                                  # visualize model cases and all data
    visualize_SVM(model_cases[icase],X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,
        Xslabelunit[0],Xslabelunit[1],r'RBF Support Vector Machine, ' + str(cases[icase]) + ' Model',[0,1],['Shale','Sand'],binary_cmap,False)
    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))

plt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.3, hspace=0.3); plt.show() 
```

![å›¾ç‰‡](img/e3b9d37c4cafc9bc078800f977947926.png)

é€šè¿‡ä»æˆ‘ä»¬çš„è¶…å‚æ•°è°ƒæ•´äº¤å‰éªŒè¯å‡†ç¡®ç‡ä¸­é€‰æ‹©ä½ã€ä¸­ã€é«˜å‡†ç¡®ç‡è¶…å‚æ•°æ¡ˆä¾‹ï¼Œæˆ‘ä»¬è·å¾—äº†è¿‡åº¦æ‹Ÿåˆåˆ°è‰¯å¥½æ‹Ÿåˆæ¨¡å‹çš„è‰¯å¥½ç¤ºä¾‹ã€‚

## è¯„è®º

å¸Œæœ›æ‚¨è§‰å¾—è¿™ä¸€ç« æœ‰å¸®åŠ©ã€‚è¿˜æœ‰å¾ˆå¤šå¯ä»¥åšçš„å’Œè®¨è®ºçš„ï¼Œæˆ‘æœ‰å¾ˆå¤šèµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ï¼Œ

*Michael*

## ä½œè€…ï¼š

Michael Pyrczï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ *æ–°å‹æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ åœ°ä¸‹è§£å†³æ–¹æ¡ˆ*

Michael åœ¨åœ°ä¸‹å’¨è¯¢ã€ç ”ç©¶å’Œå¼€å‘é¢†åŸŸæ‹¥æœ‰è¶…è¿‡ 17 å¹´çš„ç»éªŒï¼Œä»–å› å¯¹æ•™å­¦çš„çƒ­æƒ…å’Œå¯¹å¢å¼ºå·¥ç¨‹å¸ˆå’Œåœ°çƒç§‘å­¦å®¶åœ¨åœ°ä¸‹èµ„æºå¼€å‘ä¸­å½±å“çš„çƒ­æƒ…è€Œé‡è¿”å­¦æœ¯ç•Œã€‚

æƒ³äº†è§£æ›´å¤šå…³äº Michael çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹é“¾æ¥ï¼š

[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)

## æƒ³è¦ä¸€èµ·å·¥ä½œå—ï¼Ÿ

æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³è¦äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æä»¥åŠæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚ä¸ã€‚

+   æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼

+   æƒ³è¦åˆä½œï¼Œæ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼

+   æ‚¨å¯ä»¥é€šè¿‡ mpyrcz@austin.utexas.edu è”ç³»åˆ°æˆ‘ã€‚

æˆ‘æ€»æ˜¯ä¹äºè®¨è®ºï¼Œ

*è¿ˆå…‹å°”*

è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼Œæ³¨å†Œå·¥ç¨‹å¸ˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢æ•™æˆ

## æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
