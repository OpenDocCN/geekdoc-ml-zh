# 约定和符号

> 原文：[`dafriedman97.github.io/mlbook/content/conventions_notation.html`](https://dafriedman97.github.io/mlbook/content/conventions_notation.html)

本书将使用以下术语。

+   变量可以分为两种类型：我们打算建模的变量被称为**目标**或**输出**变量，而用于建模目标变量的变量被称为**预测变量**、**特征**或**输入**变量。这些也分别被称为*因变量*和*自变量*。

+   **观测**是单个预测变量和目标变量的集合。具有相同变量的多个观测组合形成一个**数据集**。

+   **训练**数据集是用于构建机器学习模型的。**验证**数据集是用于比较基于同一训练数据集但参数不同的多个模型的。**测试**数据集是用于评估最终模型的。

+   变量，无论是预测变量还是目标变量，可以是**定量**的或**分类**的。定量变量遵循连续或近似连续的尺度（例如英寸或美元的收入）。分类变量属于一组离散的组之一（例如出生国家或物种类型）。虽然分类变量的值可能遵循某种自然顺序（例如衬衫尺寸），但这并不假设。

+   如果目标是定量的，建模任务被称为**回归**；如果目标是分类的，则称为**分类**。请注意，回归不一定指普通最小二乘（OLS）线性回归。

除非另有说明，以下约定用于表示数据和数据集。

+   训练数据集假设有 $N$ 个观测值和 $D$ 个预测变量。

+   第 $n$ 个观测的特征向量由 $\bx_n$ 给出。请注意，$\bx_n$ 可能包括通过特征工程对原始预测变量的函数。当目标变量是单维的（即每个观测只有一个目标变量）时，它由 $y_n$ 给出；当每个观测有多个目标变量时，目标变量的向量由 $\by_n$ 给出。

+   整个输入和输出数据集通常用 $\{\bx_n, y_n\}_{n = 1}^N$ 表示，这表示观测 $n$ 有一个多维预测变量向量 $\bx_n$ 和一个目标变量 $y_n$，其中 $n = 1, 2, \dots, N$。

+   许多模型，例如普通线性回归，都会在预测变量向量中添加一个截距项。当这种情况发生时，$\bx_n$ 将定义为

    $$ \bx_n = \begin{pmatrix} 1 & x_{n1} & x_{n2} & ... & x_{nD} \end{pmatrix}. $$

+   *特征矩阵* 或 *数据框* 是通过将观察到的特征向量连接起来创建的。在一个矩阵中，特征向量是行向量，其中 $\bx_n$ 代表矩阵的第 $n$ 行。这些矩阵由 $\bX$ 给出。如果将每个 $\bx_n$ 的前导 1 添加到对应特征矩阵 $\bX$ 的第一列，那么该特征矩阵的第一列将只包含 1。

最后，以下数学和符号约定被使用。

+   标量值将不带粗体且为小写，随机变量将不带粗体且为大写，向量将带粗体且为小写，而矩阵将带粗体且为大写。例如，$b$ 是一个标量，$B$ 是一个随机变量，$\mathbf{b}$ 是一个向量，而 $\mathbf{B}$ 是一个矩阵。

+   除非另有说明，所有向量都被假定为列向量。由于特征向量（如上面的 $\bx_n$ 和 $\bphi_n$）作为行输入到数据框中，它们有时会被当作行向量处理，即使在数据框之外也是如此。

+   矩阵或向量导数，在 数学附录 中介绍，将使用分子 [布局约定](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions)。令 $\by \in \R^a$ 和 $\bx \in \R^b$；根据此约定，导数 $\partial\by/\partial\bx$ 被写成

$$\begin{split} \dadb{\by}{\bx} = \begin{pmatrix} \dadb{y_1}{x_1} & ... & \dadb{y_1}{x_b} \\ \dadb{y_2}{x_1} & ... & \dadb{y_2}{x_b} \\ & ... & \\ \dadb{y_a}{x_1} & ... & \dadb{y_a}{x_b} \\ \end{pmatrix}. \end{split}$$

+   给定数据 $\{x_n\}_{n = 1}^N$ 的参数 $\theta$ 的似然由 $\mathcal{L}\left(\theta; \{x_n\}_{n = 1}^N\right)$ 表示。如果我们考虑数据是随机的（即尚未观察到的），它将被写成 $\{X_n\}_{n = 1}^N$。如果考虑的数据是明显的，我们可以将似然简单地写成 $\mathcal{L}(\theta)$。
