# 一、引言

> 译者：[@PEGASUS1993](https://github.com/PEGASUS1993)

## 1.1 机器学习是什么？   

机器学习是一个与构建算法有关的计算机科学的子领域，这些算法很有用，它依赖于某些现象的实例收集是有用的。这些实例可以来自自然，由人类手工设计或由另一种算法产生。  

机器学习也可以定义为通过收集数据集来解决实际问题的过程。假设这个统计模型是用来解决实际问题的。  

本书交替使用“学习”和“机器学习”这两个术语。  
## 1.2 学习类型  
学习可以是监督的、半监督的、非监督的和强化的。  

### 1.2.1 监督学习  

在**监督学习**中，**数据集**是**标签示例**{（x<sub>i</sub>，y<sub>i</sub>）} <sup>N</sup><sub>i=1</sub>的集合。(如果术语以**粗体**显示，则表示该术语可以在本书末尾的索引中找到。)

N 中的每个元素 x<sub>i</sub>称为**特征向量**。特征向量是这样的一种向量，它每个维度 j = 1，...D 包含一个以某种方式描述该样本的值。这些值被称为**特征**，表示为 x<sup>(j)</sup>。例如，如果每个样本 x 在我们的集合代表一个人，然后第一个特征 x<sup>(1)</sup>，可以包含高度（厘米），第二个特征 x<sup>(2)</sup>，可以包含体重（千克），第三个特征 x<sup>(3)</sup>可以包含性别等等。  

数据集中的所有样本，特征向量中位置 j 处的特征始终包含同样的信息。这意味着在某一样本 x<sub>i</sub>中,如果 x<sup>(2)</sup><sub>i</sub>包含重量（千克）的信息，那么，当 k = 1 ,. 。 。 ，N 时，在每个样本 x<sub>k</sub>中的 x<sup>(2)</sup><sub>k</sub>都会包含重量（千克）的信息。
要么是属于有限的一组类{1,2，...的元素。 。 。 ，C}，或实数，或更复杂的结构，如矢量，矩阵，树或图形。除非另有说明，否则本书 yi 是一组有限的类或实数 2。你可以看到一个班级示例所属的类别。例如，如果您的示例是电子邮件并且您的问题是垃圾邮件检测，那么您有两个类{spam，not_spam}。

监督学习算法的目标是使用数据集生成一个模型，以特征向量 x 作为输入和输出信息以便推断标签特征向量。例如，使用人员数据集创建的模型可以视为输入描述一个人的特征向量，并输出该人患癌症的概率。


### 1.2.2 无监督学习   

在无监督学习中，数据集是未标记示例的集合 {Xi}<sup>N</sup>i=1
x 是一个特征向量，无监督学习算法的目标是创建一个以特征向量 x 作为输入并将它转换为另一个向量或数值来用于解决实际问题的模型。例如，在聚类中，模型返回数据集中每个特征向量的群集 ID。在维度减少中，模型的输出是一个特征向量，其特征比输入的 x 要少;其输出是一个实数，预测 x 与数据集中的"典型"示例是如何不同的。

### 1.2.3 半监督学习   

在半监督学习中，数据集包含标记和未标记的示例。通常，未标记示例的数量远远高于标记示例的数量。半监督学习算法的目标与监督学习算法相同。希望使用许多未标记的示例可以帮助学习算法找到更好的模型（我们可以说"生成"或"计算"）。
学习可能因为添加更多未标记的示例而受益，这似乎有悖常理。似乎我们给这个问题增加了更多的不确定性。但是，当您添加未标记的示例时，会添加有关问题的详细信息：较大的示例反映了我们标记数据更好的概率分布。从理论上讲，学习算法应该能够利用这些附加信息。

### 1.2.4 强化学习  

强化学习是机器学习的一个子领域，机器"生活在"环境中，能够感知该环境的状态作为特征的载体。计算机可以在每种状态下执行操作。不同的行动带来不同的奖励，也可以将机器移动到环境的另一种状态。强化学习算法的目标是学习策略。
策略是一个函数（类似于监督学习中的模型），将状态的特征向量作为输入并输出，以该状态执行的最佳操作。如果操作是最佳的，则为最佳操作最大化预期的平均回报。
强化学习解决了一种特殊的问题，其中决策是连续的，目标是长期的，例如游戏、机器人、资源管理或物流。在此书，我强调一次性决策，其中输入实例是彼此独立的和过去式预测。我把强化学习排除在本书的讨论范围之内。  

## 1.3 监督学习的工作原理  

在这一部分中，我简要地解释了有监督的学习是如何工作的，这样在我们详细讨论之前的整个过程你脑海中就有了一定的画面。我决定把监督学习作为
例子，因为它是在实际中最常用的机器学习类型。  

监督学习的过程从收集数据开始。监督学习的数据是成对的集合（输入、输出）。输入可以是任何内容，例如电子邮件，信息，图片或传感器测量。输出通常是实数或标签（例如“垃圾邮件”、“非垃圾邮件”、“猫”、“狗”、“老鼠”等）。在某些情况下，输出是向量（例如，图片上一个人周围矩形的四个坐标），序列（例如[“形容词”，“形容词”、“名词”]表示输入“大且漂亮的车”），或有其他结构。  

假设您希望使用监督学习解决的问题是垃圾邮件检测。你收集数据，例如，10000 封电子邮件，每个邮件都有一个标签“垃圾邮件”或“非垃圾邮件”（你可以手动添加这些标签，也可以花钱请人为我们添加）。现在，您必须将每个电子邮件转换为特征向量。  

数据分析师根据他们的经验决定如何将一个真实的实体转换一个特征向量，例如一封电子邮件。词袋模型是一种将文本转换为特征向量的常见方法，它是取一本英语单词词典（假设它包含 20000 个单词，其中单词是通过字母顺序划分的），并在我们的特征向量中规定：  

如果电子邮件中包含单词“a”，则第一个特征等于 1;否则,这特征是 0;  
  
如果电子邮件中包含单词“aaron”，则第二个特征等于 1;否则，该特征值为 0;  

**......**  

如果电子邮件中包含单词“zulu”，则第 20000 个特征等于 1;否则，该特征值为 0。  

您可以对我们集合中的每一封电子邮件重复上述步骤，我们的集合有 10000 个特征向量(每个向量的维数为 20000)和一个标签(“垃圾邮件”/“非垃圾邮件”)。  

现在您有了一个机器可读的输入数据，但是输出标签仍然是人类可读的文本。一些学习算法需要把标签转换成数字。例如，有些算法需要像 0 这样的数字(表示标签“非垃圾邮件”)和 1(表示“垃圾邮件”标签)。我用来解释监督学习的这种算法称为支持向量机(SVM)。该算法要求正标签(在此例中是“垃圾邮件”)，数值为+1(1)，负标签(“非垃圾邮件”)值为- 1。  

此时，您已经有了一个数据集和一个学习算法，所以您已经准备好应用学习算法，得到数据集的模型。  

SVM 将每个特征向量看作高维空间中的一个点(在我们的例子中是高维空间是 20000 -维)。该算法将所有特征向量放在一个假想的 20,000-高维图，并绘制一个假想的 19,999 维的线(超平面)，它将带有正负面标签的例子分开。在机器学习中,将不同类的示例分隔开的边界称为决策边界。

超平面的方程由两个参数给出，一个实值向量 w,它和输入特征向量 x 维数相同，和实数 b。  

　　　　　　　　　　　　wx − b = 0
表示 wx 意味着 w<sup>（１）</sup>x<sup>（１）</sup> + w<sup>（２）</sup>x<sup>（２）</sup> +…+ w<sup>（Ｄ）</sup>x<sup>（Ｄ）</sup>和 D 是代表特征向量 x 维度的数字。　　

（如果一些方程现在不清楚，在第 2 章中，我们重新审视数学和统计，理解它们所必需的概念。就目前而言，试着去了解此处是什么含义。阅读下一章后，一切都变得更加清晰。）　　

现在，一些输入的特征向量 x 的预测标签如下所示：　　

　　　　　　　　　　y = sign(wx − b),　　

其中*sign*是一个数学运算符，该运算符将任何值作为输入，如果输入为正例，返回+1。如果输入为负例，返回-1。  

学习算法 (本例中为 SVM)的目标是利用数据集并查找参数 w<sup>星号</sup>和 b<sup>*</sup>的最佳值 w 和 b。  

一旦学习算法确定然后，将模型 f(x)定义为：  
　　　　　　　　　　f(x) = sign(w<sup>*</sup>x − b<sup>＊</sup>)



因此，要使用 SVM 模型预测电子邮件是垃圾邮件还是非垃圾邮件，您必须获取消息的文本，将其转换为特征向量，然后将其相乘
w<sup>*</sup>的向量，减去 b<sup>＊</sup>，并取符号作为结果。 这将给我们预测（+1 表示“垃圾邮件”，-1 表示“ 非垃圾邮件”）。   

现在，机器如何找到 w<sup>*</sup>和 b<sup>＊</sup>？ 它解决了优化问题。 机器是擅长在约束下处理优化问题的。  

那么我们要在这里满足哪些约束？ 首先，我们希望模型能够正确预测 10,000 个示例的标签。 请记住，每个示例 i = 1。 。 。 ，10000 由一对（x<sub>i</sub>，y<sub>i</sub>）给出，其中 x<sub>i</sub>是示例 i 的特征向量，而 y<sub>i</sub>是其标签，其标签使用值-1 或+1。 因此，约束自然是：

wx<sub>i</sub> − b ≥ +1  当  y<sub>i</sub> = +1,
wx<sub>i</sub> − b ≤ −1  当  y<sub>i</sub> = −1。  

我们也希望超平面将具有最大限量的正例与负例分开。 **决策**距离是两个类别的最接近示例之间的距离，由决策边界定义。 较大的决策距离有助于更好的泛化，这就是模型将来对新示例进行分类的程度。为此，我们需要
最小化 w 的欧几里得范数，表示为||w||，由![](https://i.imgur.com/RoHUALP.png)计算得来。

因此，我们要机器解决的优化问题看起来像这样：  

最小化||w||， 服从 y<sub>i</sub>(wx<sub>i</sub>-b)≥1,i=1,...N。表达式 y<sub>i</sub>(wx<sub>i</sub>-b)≥1 只是上面两个约束的一种紧凑方式。

由 w<sup>*</sup>和ｂ<sup>＊</sup>给出的此优化问题的解决方案称为统计模型，或简称为模型。 建立模型的过程称为训练。

对于二维特征向量，问题和解决方案如图 1.1 所示。蓝色和橙色圆圈分别表示正例和负例，并且 wx-b = 0 给出的线是决策边界。

为什么通过最小化 w 的范数来找到两个类别之间的最大边界范围？如图 1.1 所示，从几何上讲，方程 wx-b = 1 和 wx-b = -1 定义了两个平行的超平面，这些超平面之间的距离为 2/||w||，因此，范数||w||越小，这两个超平面之间的距离越大。

支持向量机就是这样工作的。该算法的特定版本建立了所谓的线性模型。之所以称为线性，是因为决策边界是直线（或平面或超平面）。 SVM 还可以合并内核，这些内核可以使决策边界任意非线性化。在某些情况下，由于数据中的噪声，标注错误或离群值（示例与数据集中的“典型”示例截然不同），可能无法完全分离两组点。 SVM 的另一个版本也可以包含惩罚超参数，用于对特定类别的训练示例进行错误分类。我们将在第 3 章中更详细地研究 SVM 算法。  

此时，您应该保留以下内容：任何隐式或显式构建模型的分类学习算法都会创建决策边界。决策边界可以是直的或弯曲的，或者可以具有复杂的形式，也可以是某些几何图形的叠加。决策边界的形式决定了模型的准确性（即正确预测了标签示例的比例）。决策边界的形式（基于训练数据进行算法或数学计算的方式）将一种学习算法与另一种学习算法区分开。

在实践中，还需要考虑学习算法的其他两个基本区别：模型构建的速度和预测处理时间。 在许多实际情况下，您可能希望使用一种学习算法，该算法可以快速建立不太准确的模型。 此外，您可能更喜欢精度较低的模型，该模型可以更快地进行预测。

## 1.4 模型为什么适用于新数据  

为什么机器学习模型能够正确预测以前新示例看不到的标签？要理解这一点，请查看图 1.1 中的图。如果两个类通过决策边界彼此分离，则属于每个类的示例显然位于决策边界创建的两个不同的子空间中。
如果用于训练的示例是随机选择的，彼此独立，并遵循相同的过程，那么，从统计上看，新的负例更有可能位于离其他负例不远的图上。同样，新的正例它很可能围绕在自其他正例。在这种情况下，我们的决定边界仍然极有可能将新的正负两方面的例子分开。对于其他不太可能的情况，我们的模型会出错，但由于此类情况的可能性较小，因此错误数可能小于正确预测的数量。
直观地说，训练示例集越大，**新示例与用于训练的示例，相同的可能性就越大。**(还需校对)。  

为了尽可能降低在新示例中出错的可能性，SVM 算法通过查找最大的边距，精确试图绘制决策边界，使它尽可能远离从两个类的示例。
有兴趣更多地了解可学习性和了解模型误差之间的密切关系，训练集的大小，数学方程的形式定义模型，构建模型所需的时间的读者，鼓励阅读有关 PAC 的内容。PAC（"可能近似正确"）学习理论有助于分析学习算法在什么条件下或者说可能在什么条件下输出一个近似正确的分类器。
