# 十一、总结

> 译者：[kjlintong](https://github.com/kjlintong)

哇，太快了！如果你到了这里并且设法理解了大部分的好的材料，那你真的很厉害。

如果你注意到本页的底部的数字，你会发现本书远超百页，这意味着书名有点误导。我希望你能原谅这个小小的营销手法。毕竟，如果我想要使这本书正好百页，我可以减少字体大小，白边距和行间距，或删除 UMAP 上的部分，让你自己去找它们。相信我：你不希望这些内容被放在 UMAP 中！

但是，先停下来，我可以自信地说你已经拥有成为一名优秀的现代数据分析师或机器学习工程师所需的一切。这并不意味着我把所有的需要的东西都包括在内，而是我在这本书中所提到的内容，你可以扩展学习很多资料。我所强调的的大部分内容根本不局限于书中：典型的机器学习书籍是保守的和学术性的，而我强调的那些将在日常工作中发挥作用的算法以及方法。

而那些我没提到的，将会在千页机器学习中提到吧？

## 11.1 主题模型 （Topic Modeling）

在文本分析中，主题模型是一种普遍的无监督学习问题。您有一组文本文档，并且您希望发现每个文档中存在的主题。Latent Dirichlet Allocation（LDA）是一种非常有效的主题发现算法。您可以决定文档集合中存在多少主题，算法会为此集合中的每个单词指定一个主题。然后，要从文档中提取主题，您只需计算该文档中每个主题的单词数。

## 11.2 高斯过程 （Gaussian Processes）

高斯过程（GP）是一种与核回归不相上下的监督学习方法。它比后者有一些优点。例如，它为每个点中的回归线提供置信区间。我决定不解释 GP，因为我无法通过一种简单的方式来解释它们，但你可以花一些时间来学习 GP。这将花费一些时间。

## 11.3 广义线性模型 （Generalized Linear Models）

广义线性模型（GLM）是线性回归的一般化，用于对输入特征向量和目标之间的各种形式的依赖性进行建模。例如，逻辑回归是 GLM 的一种形式。如果您对回归感兴趣并且寻找简单且可解释的模型，那么您应该阅读更多 GLM 方面的内容。

## 11.4 概率图模型 （Probabilistic Graphical Models）

我们在第 7 章中提到了概率图形模型（PGM）的一个例子：条件随机场（CRF）。使用 CRF，我们可以将单词的输入序列以及此序列中的要素和标签之间的关系建模为顺序依赖图。更一般地，PGM 可以是任何图形。图是由连接一对节点的节点和边的集合组成的结构。PGM 中的每个节点表示一些随机变量（其值可以被观察或未观察到），并且边表示一个随机变量对另一个随机变量的条件依赖性。例如，随机变量“人行道湿度”取决于随机变量“天气条件”。通过观察一些随机变量的值，优化算法可以从数据中学习观测变量和未观测变量之间的依赖性。

PGM 允许数据分析师查看一个特征的值是如何依赖于其他特征的值。如果依赖图的边缘是定向的，则可以推断因果关系。不幸的是，手工构建这样的模型需要大量的领域专业知识和对概率论和统计学的深刻理解。后者通常是许多领域专家的问题。一些算法允许从数据中学习依赖图的结构，但是学习的模型通常难以被人解释，因此它们对于理解生成数据的复杂概率过程不是有益的。CRF 是目前使用最多的 PGM，主要用于文本和图像处理。然而，在这两个领域中，它们被神经网络所超越。另一种图形模型，隐马尔可夫模型或 HMM，在过去经常被用于语音识别，时间序列分析和其他时间推理任务，但是，HMM 再次败给了神经网络。

PGM 的名气也在贝叶斯网络，信念网络和概率独立网络之下。

## 11.5 马尔可夫链蒙特卡洛方法 （Markov Chain Monte Carlo）

如果您使用图形模型并希望从依赖图定义的非常复杂的分布中抽样示例，则可以使用马尔可夫链蒙特卡罗（MCMC）算法。MCMC 是一类算法，用于从数学上定义的任何概率分布中进行采样。记得当我们讨论去噪自动编码器时，我们从正态分布中采样噪声。从标准分布中取样，例如正常分布或均匀分布，相对容易，因为它们的属性是众所周知的。然而，当概率分布可以具有由从数据学习的依赖图定义的任意形式时，采样任务变得显着更复杂。

## 11.6 遗传算法 (Genetic Algorithms)

遗传算法（GA）是一种用于优化不可区分的优化目标函数的数值优化技术。他们使用来自进化生物学的概念，通过模拟进化生物过程来寻找优化问题的全局最优（最小或最大）。

GA 工作从初始生成候选解决方案开始。如果我们寻找模型参数的最佳值，我们首先会随机生成多个参数值组合。然后，我们针对目标函数测试参数值的每个组合。想象一下参数值的每个组合作为多维空间中的一个点。然后，我们通过应用“选择”，“交叉”和“突变”等概念，从上一代生成后续一代点。

简而言之，这导致每个新一代保持更多的点，类似于上一代中针对目标执行的那些点。在新一代中，在上一代中表现最差的点被表现最好的点的“突变”和“交叉”所取代。点的突变是通过随机失真的一些原点属性获得的。交叉是几个点的某种组合（例如，平均值）。

遗传算法允许找到任何可测量的优化标准的解决方案。例如，G A 可用于优化学习算法的超参数。它们通常比基于梯度的优化技术慢得多。

## 11.7 强化学习 （Reinforcement Learning）

正如我们已经讨论过的那样，强化学习（RL）解决了一个非常具体的问题，即决策是连续的。通常，代理人在未知环境中行事。每个动作都会带来奖励并将代理移动到另一个环境状态（通常是由于某些具有未知属性的随机过程）。代理的目标是优化其长期奖励。

强制学习算法，如 Q-learning，以及基于神经网络的对应物，用于学习玩视频游戏，机器人导航和协调，库存和供应连锁管理，复杂电力系统（电网）优化和学习金融交易策略。

                                 ***

这本书在这里结束了。不要忘记偶尔看看本书的配套维基网页，以便及时了解本书中提到的每个机器学习领域的新发展。正如我在序言中所说的那样，这本书就像你去购买后的葡萄酒一样越来越好，当然这要感谢维基。哦，不要忘记这本书是先阅读，然后再买的原则。这意味着如果在阅读这些文字时你看到电子屏幕，那么你可能是购买这本书的合适人选。
